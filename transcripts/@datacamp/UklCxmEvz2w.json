{
  "episodeId": "UklCxmEvz2w",
  "channelSlug": "@datacamp",
  "title": "LangGraph Tutorial for Beginners: Build Your First AI Agent",
  "publishedAt": "2025-06-27T16:45:02.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "What's up everybody? Adele here. Today",
      "offset": 0.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we're going to get our hands dirty with",
      "offset": 2.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Lang Graph, a framework that lets you",
      "offset": 4.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "define AI agents as graphs and nodes.",
      "offset": 6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "We're going to learn how to build AI",
      "offset": 8.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "agents that can think, reason, and act",
      "offset": 10.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "with tools. Our instructor, VBA, will",
      "offset": 13.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "walk you exactly how to build your AI",
      "offset": 15.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "agents using Langraph. So, make sure to",
      "offset": 17.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "stay tuned for that. Now, if you want to",
      "offset": 19.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "jump directly to the coding section,",
      "offset": 21.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "make sure to check out the timestamp",
      "offset": 22.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "below. And make sure to check out the",
      "offset": 24.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "data lab notebook that's accompanied in",
      "offset": 26,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the description so you can code along",
      "offset": 28.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with the video. If you want to take your",
      "offset": 30,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "skills to the next level and learn how",
      "offset": 32.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to build multi- aent systems using",
      "offset": 33.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Langraph, make sure to check out the",
      "offset": 35.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "course that you can see in the",
      "offset": 37.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "description here and learn how to build",
      "offset": 38.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "an AI agent that can take data from",
      "offset": 41.12,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Wikipedia, analyze stock data, and",
      "offset": 43.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "provide you with sound analysis on the",
      "offset": 46.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "performance of a particular stock. Now",
      "offset": 48.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "with that said, let's get started.\n Hey",
      "offset": 50.079,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "guys, welcome to data cam. In this",
      "offset": 53.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tutorial, I'm going to be teaching you",
      "offset": 56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "about the fundamentals of Langraph. Now",
      "offset": 57.68,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "this is a beginner level tutorial. So I",
      "offset": 59.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "assume you know little to no knowledge",
      "offset": 62.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "in langraph. From this tutorial, we're",
      "offset": 63.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "going to go right from the basics and",
      "offset": 66.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "build ourselves up so that by the end of",
      "offset": 67.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it, you can build almost any AI agent",
      "offset": 69.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "out there. Now, if that sounds good to",
      "offset": 72.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you, let's begin with this tutorial. So",
      "offset": 74.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I'm going to ask you two simple",
      "offset": 76.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "questions. What is the difference",
      "offset": 78.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "between an LLM and an AI agent? You can",
      "offset": 80.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "think of an LLM as a model that",
      "offset": 83.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "understands and generates humanlike text",
      "offset": 86.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "based on the patterns it received in its",
      "offset": 88.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "training data. An AI agent, on the other",
      "offset": 90.799,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "hand, it uses this LLM to reason, plan,",
      "offset": 92.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and use tools and overall take the best",
      "offset": 96.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "respective actions over time. Now, I've",
      "offset": 99.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "put the word tools in bold yet because",
      "offset": 101.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it will be coming up quite often soon.",
      "offset": 103.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "My next question is this. What even is",
      "offset": 106.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Langraph? Langraph is a framework that",
      "offset": 108.88,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "lets us build AI agents as graphs where",
      "offset": 111.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "each node does a task and edges decide",
      "offset": 114.799,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "what happens next. Now, don't worry if",
      "offset": 117.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you don't know what a node and an edge",
      "offset": 119.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is. We will discuss them shortly. You",
      "offset": 120.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "can think of it like having a flowchart",
      "offset": 123.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "which I've shown here. This flowchart",
      "offset": 125.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "basically represents an AI agent and it",
      "offset": 127.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "shows how it thinks, how it acts, and",
      "offset": 129.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "all of the internal decision-m. For",
      "offset": 132.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "example, it has the start and the end",
      "offset": 134.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "point here and then you have the input",
      "offset": 136.239,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "query. So the input query could be from",
      "offset": 138.239,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "me and I can say give me this this this",
      "offset": 140.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and then it gets passed to the tool",
      "offset": 144.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "selector part. Its job is to be able to",
      "offset": 146.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "select well the correct tool. For",
      "offset": 148.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "example, if I ask it to do a very simple",
      "offset": 150.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "query like what is 2 + 3? It should be",
      "offset": 152.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "able to pick the maths tool not the web",
      "offset": 155.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "search tool because well why would you",
      "offset": 157.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "search what the answer for 2 plus 3 is",
      "offset": 159.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right? So the tool selector is quite an",
      "offset": 161.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "important part of an AI agent. Then",
      "offset": 163.84,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "based on whatever tool it picks, it then",
      "offset": 166.08,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "outputs the result and then well the",
      "offset": 168.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "query finishes, right? Hopefully that",
      "offset": 170.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "flowchart analogy helps you to",
      "offset": 172.959,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "understand what an AI agent is. What is",
      "offset": 174.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this tutorial's overall aim? It really",
      "offset": 176.879,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "is to be able to understand and master",
      "offset": 179.28,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the architecture of Langraph. If you can",
      "offset": 182.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "understand and master the architecture,",
      "offset": 184.879,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "well, you can build any AI agent in from",
      "offset": 186.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Langraph, right? So let's begin by pip",
      "offset": 189.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "installing all of the libraries we'll",
      "offset": 192.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "need. So for this tutorial you need to",
      "offset": 193.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "install lang graph langchain core lang",
      "offset": 196.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "openai duck go search lang chain",
      "offset": 199.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "community and you also will need to",
      "offset": 202,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "install models. The ammo models I will",
      "offset": 203.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "discuss in the end of the tutorial. For",
      "offset": 206.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the majority of this tutorial I'm going",
      "offset": 208.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to be using openAI and you can get its",
      "offset": 209.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "openi API key through their website.",
      "offset": 212.08,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "It's very very cheap. So don't worry it",
      "offset": 214.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "will be in the matter of pennies or",
      "offset": 216.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "cents. So the price is very cheap. So",
      "offset": 218.799,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "don't worry about that. So let's go",
      "offset": 221.44,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "ahead and pip install all of this. And",
      "offset": 223.599,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "perfect. Let's get started with coding",
      "offset": 226.959,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "right away. I want to build this very",
      "offset": 229.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "simple agent. Agent one. The aim of this",
      "offset": 231.599,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "very simple agent is this. I want to be",
      "offset": 234.08,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "able to teach you what states, edges,",
      "offset": 236.879,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "nodes, graph, and state graph is. I also",
      "offset": 240.879,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "want to teach you about type dictionary.",
      "offset": 243.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Now that's a type annotation which we're",
      "offset": 246.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "going to uh learn about very shortly.",
      "offset": 248.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Third thing is how do we even integrate",
      "offset": 250.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "an LLM using langraph specifically again",
      "offset": 252.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the open AAI model. Now please note you",
      "offset": 255.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "can use any model you want. You can use",
      "offset": 257.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the Gemini model, you can use anthropics",
      "offset": 259.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "model. I personally use OpenAI model",
      "offset": 261.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "because I've seen their performance is",
      "offset": 263.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "quite good and also because well they're",
      "offset": 265.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "quite cheap as well. Obviously they're",
      "offset": 268.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "not free completely. That's why I've",
      "offset": 270.08,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "also used Olama at the end of this",
      "offset": 272,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "tutorial. But all in all, OpenAI is a",
      "offset": 273.759,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "good choice. Now, I also want to teach",
      "offset": 276.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you about the basic syntax of Langraph.",
      "offset": 278.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Let's build our intuition on the",
      "offset": 281.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "fundamental building blocks in Langraph.",
      "offset": 283.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And I'm going to be using a game analogy",
      "offset": 285.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "for this. So, let's start off with the",
      "offset": 288,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "first fundamental building block, which",
      "offset": 290.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is the state. Now think of the state as",
      "offset": 292.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "a data structure that is shared and it",
      "offset": 295.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "holds the current information or context",
      "offset": 298,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "of the entire application. So in terms",
      "offset": 300.639,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "of a game analogy you can think of it as",
      "offset": 304.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "a file in a game which saves everything",
      "offset": 306.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "which you have in the game such as your",
      "offset": 309.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "health, your score, your weapons, you",
      "offset": 310.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "get the point. So it basically stores",
      "offset": 313.759,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "everything the game needs to remember.",
      "offset": 315.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "Hence this uh asset which I've drawn",
      "offset": 317.919,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that is where state is. Now let's move",
      "offset": 320.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "to a node. A node is a self-contained",
      "offset": 322.639,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "function or a step and it carries out",
      "offset": 326.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "really specific tasks within the graph.",
      "offset": 329.52,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "For example, in a game analogy, it could",
      "offset": 332.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "be a particular task or a particular",
      "offset": 334.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "scene like a battle scene or a puzzle",
      "offset": 336.479,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "scene or a cutscene that happens when",
      "offset": 339.28,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "you reach a particular point on the map.",
      "offset": 341.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Hence this puzzle piece. Now an edge",
      "offset": 344.479,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "remember edge and node was something we",
      "offset": 347.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "just briefly talked about and mentioned",
      "offset": 349.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "here. But an edge it's defined as the",
      "offset": 351.36,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "how the process moves from one node to",
      "offset": 354.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "another. Remember in a graph you have",
      "offset": 357.759,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "nodes and to connect these nodes you",
      "offset": 360.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "have edges. You have edges and",
      "offset": 362.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "conditional edges. Conditional edges are",
      "offset": 363.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "something we will be covering later. But",
      "offset": 365.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "remember you need something to connect",
      "offset": 368.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these nodes or functions together right",
      "offset": 370,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and that is what an edge is and it",
      "offset": 372,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "controls the direction and logic of",
      "offset": 374.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "execution. You can think of these as a",
      "offset": 376.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "door or a road or choices that connect",
      "offset": 378.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "scenes. Essentially they decide where",
      "offset": 382,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the game goes next based on what has",
      "offset": 384.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "already happened. The next thing is well",
      "offset": 386.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "graph lang graph the most probably the",
      "offset": 388.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "most important uh fundamental building",
      "offset": 391.919,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "block in langraph. So, it's the main",
      "offset": 394.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "structure that defines how tasks aka",
      "offset": 396.319,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "nodes are linked together and carried",
      "offset": 398.8,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "out. Like I just said, edges and nodes",
      "offset": 401.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "make up a graph. The game analogy for",
      "offset": 403.919,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "this is the complete overall design",
      "offset": 406.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "showing all the levels, all the possible",
      "offset": 409.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "paths, and all the decision points. Kind",
      "offset": 411.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of like how a full map of how a game",
      "offset": 414.319,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "unfolds. The last thing is a state",
      "offset": 417.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "graph. Now a state graph essentially",
      "offset": 419.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "think of it as merging the state and",
      "offset": 422.08,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "merging the graph together. So it builds",
      "offset": 424.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and manages the entire graph. It uh has",
      "offset": 427.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the nodes, it has the edges, it has the",
      "offset": 430.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "shared state and its main job is to",
      "offset": 432.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "ensure the smooth data flow and",
      "offset": 435.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "coordinated execution. In terms of the",
      "offset": 438.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "game analogy again, it's like the",
      "offset": 440.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "current live game you're playing. It's",
      "offset": 442.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "keeping track of your location, your",
      "offset": 445.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "inventory, your choices, and progress on",
      "offset": 446.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the map. Remember, it is very, very",
      "offset": 449.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "similar to state and graph. It's like",
      "offset": 451.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the fusion. Now, if you didn't",
      "offset": 453.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "understand this, don't worry. We will",
      "offset": 455.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "consolidate this using code. Let's start",
      "offset": 457.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "off with this. So, this is all of the",
      "offset": 459.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "imports we will need for creating our",
      "offset": 462.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "very first AI agent, the simple agent.",
      "offset": 464.4,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "So, you can see I've done from typing",
      "offset": 467.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "import type dictionary. And this is the",
      "offset": 469.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "add data annotation which we're about to",
      "offset": 472,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "cover. Then we have lang chain code on",
      "offset": 474,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "messages import human message. And a",
      "offset": 476.479,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "human message is a message type which",
      "offset": 478.639,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "represents the input from a user. So",
      "offset": 481.599,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "back into the flowchart analogy where I",
      "offset": 485.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "said input query. The input is again a",
      "offset": 488.319,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "human message. Now there's loads of",
      "offset": 491.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "types of uh messages in LN graph. For",
      "offset": 493.36,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "example, there is an AI message, human",
      "offset": 495.84,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "message, tool message. We will cover",
      "offset": 498.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "them all shortly. Don't worry. Back to",
      "offset": 500.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the imports though. We have lang chain",
      "offset": 502.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "openai, import chat openai. And well,",
      "offset": 504.639,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you've guessed it. This is to import the",
      "offset": 507.44,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "LLM or use the LLM. And the last point",
      "offset": 509.84,
      "duration": 5.999
    },
    {
      "lang": "en",
      "text": "is from langraph.graph import state",
      "offset": 513.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "graph start and end. Essentially, this",
      "offset": 515.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "will help us build the overall graph",
      "offset": 517.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "structure. So, this is obviously quite",
      "offset": 519.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "important. Now, back to this type",
      "offset": 521.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "dictionary. If you don't know what a",
      "offset": 523.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "type dictionary, it's essentially a data",
      "offset": 524.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "annotation. And it's like a dictionary",
      "offset": 526.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "but type the data type is quite",
      "offset": 529.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "important to define a type dictionary in",
      "offset": 532.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Python. You use it like this. So you",
      "offset": 533.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "define a class and then obviously the",
      "offset": 536.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "name of the class and then you assign",
      "offset": 539.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "type dictionary as a superass. Now",
      "offset": 541.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "within this type dictionary I have three",
      "offset": 544.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "keys. I have the name, the age and if",
      "offset": 546,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "this person is a student or not. So for",
      "offset": 549.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the name well it can't be an integer",
      "offset": 552.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "right? How many people do you know who's",
      "offset": 554.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "called like one or two for example they",
      "offset": 556.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "are a string for example the age also",
      "offset": 558.88,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "needs to be an integer and in if it's is",
      "offset": 561.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "student it can either be true or false",
      "offset": 564.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right it can't be something in between",
      "offset": 566.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to create our own type dictionary or an",
      "offset": 568.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "instance of this type dictionary we use",
      "offset": 571.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this syntax we use curly brackets we",
      "offset": 573.36,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "define the key with the respective value",
      "offset": 576.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "but because we've used a type dictionary",
      "offset": 579.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Python will automatically throw an error",
      "offset": 581.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "if it's the wrong data type. Now, we",
      "offset": 583.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "could have obviously used a simple",
      "offset": 585.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "dictionary and that's fine, but in",
      "offset": 587.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "industry or in very very large",
      "offset": 589.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "applications, type annotations are quite",
      "offset": 591.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "a huge problem. Data types in general",
      "offset": 594.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "because well, data types cause a lot of",
      "offset": 598.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "logic errors. And to evade all of those",
      "offset": 600.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "logic errors, that's why type",
      "offset": 603.36,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "dictionaries are used. It just makes",
      "offset": 604.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "everything more robust. And in Langraph,",
      "offset": 606.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "robustness is everything. So, that's why",
      "offset": 608.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "we use type dictionary here. So now that",
      "offset": 610.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you know what a type dictionary is, oh",
      "offset": 612.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and we've also printed the person and",
      "offset": 614.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you can see the printed statement is",
      "offset": 616.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "exactly what we had uh initialized here",
      "offset": 618.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "instantiated. We're now going to define",
      "offset": 620.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the state of our agent. So to define a",
      "offset": 622.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "state you use again the type dictionary",
      "offset": 625.76,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "what we just used here and we use the",
      "offset": 628.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "same syntax. So we define a class the",
      "offset": 632.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "name of the state. Now the name of the",
      "offset": 634.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "state can be absolutely anything. I've",
      "offset": 636.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "just kept it as agent state here because",
      "offset": 638.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "well it's easier to follow that's why",
      "offset": 640.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "but you could name it whatever you want",
      "offset": 643.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you can name it state you could name it",
      "offset": 644.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Bob whatever you want and then obviously",
      "offset": 647.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you pass in type dictionary because it",
      "offset": 650,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "acts as the superass now for this agent",
      "offset": 651.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what did we want let's go back to the",
      "offset": 654.079,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "aims so the aim was here how to",
      "offset": 656,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "integrate an LLM and understand the",
      "offset": 658.959,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "basic syntax of langraph so essentially",
      "offset": 660.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "what I want is I want to be able to say",
      "offset": 662.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "something and our AI agent should be",
      "offset": 664.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "able to reply it",
      "offset": 666.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "So realistically the only thing which I",
      "offset": 668,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "need to store is my my own input",
      "offset": 670,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "message. And what will the input message",
      "offset": 673.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "be? Well, we just spoke about that it",
      "offset": 675.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "will be in the form of a human message",
      "offset": 678.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "and that's why we have created a key",
      "offset": 680.56,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "which is user message and it will be the",
      "offset": 683.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "type human message. Remember human",
      "offset": 686.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "message, AI message, all of these",
      "offset": 688.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "message types in langraph are data types",
      "offset": 690.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and that's why I referred to it as human",
      "offset": 692.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "message like this syntax.",
      "offset": 694.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So remember this is quite a very very",
      "offset": 697.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "basic AI agent that's why I only have",
      "offset": 699.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "one key but later on we will have",
      "offset": 701.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "multiple keys. So now we define an LLM",
      "offset": 703.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "in langraph and we use chat openai. Now",
      "offset": 706,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "if you know lang chain or know a little",
      "offset": 709.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "bit about lang chain this is exactly how",
      "offset": 711.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you define an LLM in lang chain as well.",
      "offset": 713.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Remember langraph is built on lang",
      "offset": 716.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "chain. They're built by the same sort of",
      "offset": 718.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "developers. So a lot of the tools and",
      "offset": 720.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "everything which we uh use in lang chain",
      "offset": 722.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "have also been implemented in langraph.",
      "offset": 725.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "It just makes things much more",
      "offset": 727.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "compatible and easier. Right? So for",
      "offset": 729.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "this example I'm going to be using the",
      "offset": 731.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "GPT40 mini. You can use any open AI",
      "offset": 733.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "model. You can use any LLM model if you",
      "offset": 737.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "want. You just need to change this. You",
      "offset": 739.68,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "can have chat gemini chat perplexity",
      "offset": 741.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "chat anthropic whatever. You can refer",
      "offset": 744.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "to the lang chain langraph documentation",
      "offset": 746.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and lang chain documentation. We will",
      "offset": 749.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "talk about the documentation in a bit.",
      "offset": 751.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "But going back to this example, I've",
      "offset": 752.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "also set this attribute here which is",
      "offset": 754.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "temperature. Now temperature is an",
      "offset": 756.639,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "attribute which controls the creativity",
      "offset": 758.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of our AI agent. So it's between 0 and",
      "offset": 761.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "one for open AI models. If it's towards",
      "offset": 763.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "one, it means the LLM will be more",
      "offset": 766.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "creative. And if it's towards zero,",
      "offset": 768.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "well, it would be more it will be less",
      "offset": 770.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "stochastic. So I've kept it at 0.7",
      "offset": 772.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "because I want a little bit more",
      "offset": 775.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "creativity than 0.5. So this is",
      "offset": 776.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "arbitrary. You can set whatever you",
      "offset": 779.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "want. Uh but that's not really",
      "offset": 781.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "important. I've just kept it here just",
      "offset": 783.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to show you that chat open AAI has a lot",
      "offset": 784.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of attributes which we can tweak. It can",
      "offset": 786.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you can change the top K parameters, top",
      "offset": 788.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "P parameters. If you don't know what",
      "offset": 790.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that is, that is completely fine. The",
      "offset": 792.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "most important thing which you need to",
      "offset": 794,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "know is that this you need to set the",
      "offset": 795.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "model here. In terms of the API key, by",
      "offset": 797.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the way, I've set it as an environment",
      "offset": 800.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "variable. If you using this locally, you",
      "offset": 802.079,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "can store it in av file and then you can",
      "offset": 805.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "uh import it using the from.env load.env",
      "offset": 808.399,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "uh syntax. I'm using data lab and it",
      "offset": 812.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "just makes things easier because",
      "offset": 814.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "everything's in the environment. So that",
      "offset": 815.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is how you create the agent state and",
      "offset": 817.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "how you initialize a large language",
      "offset": 820.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "model. Now I'm going to be creating a",
      "offset": 822.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "node. So the purpose of this node is to",
      "offset": 824.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "pass the user's message to the LLM. So",
      "offset": 826.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to be able to do this I define a",
      "offset": 829.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "function. So it will take in the state",
      "offset": 830.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "which will be agent state which is the",
      "offset": 833.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "state of our agent which we just uh",
      "offset": 835.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "created and it will output the exact",
      "offset": 838.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "same agent state. Remember a node has to",
      "offset": 840.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "be able to in uh has to be able to",
      "offset": 843.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "intake a state and then output the",
      "offset": 845.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "updated state. The state in any graph is",
      "offset": 848.32,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the most important thing in there. If",
      "offset": 852,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you don't have the state well your",
      "offset": 854.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "entire robustness completely falls",
      "offset": 856.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "apart. So remember a rule of thumb is",
      "offset": 858.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "always always always always output the",
      "offset": 860.959,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "agent state. Now this is Python 3 uh",
      "offset": 863.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "syntax but this really just shows the",
      "offset": 866.639,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "reader that you're inputting an agent",
      "offset": 870.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "state and you're outputting the same",
      "offset": 872.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "state or the updated state. So within",
      "offset": 874,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this function you can see that I have",
      "offset": 876.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this line of code. It's saying it's",
      "offset": 877.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "using llm.invoke state user message. Now",
      "offset": 880.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "what does this line even mean? Well",
      "offset": 883.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "llm.invoke Invoke is a really fancy way",
      "offset": 885.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "of just saying of you of running this",
      "offset": 888.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "LLM. So whatever I pass here, it will",
      "offset": 890.959,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "get passed to this LLM or the chat open",
      "offset": 893.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "AI model which we uh defined the state",
      "offset": 896.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "user message this will contain our",
      "offset": 899.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "query. Now I haven't passed any query",
      "offset": 901.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "yet but overall this line is essentially",
      "offset": 904.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "saying that whatever I have that gets",
      "offset": 906.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "passed to the lm using this invoke",
      "offset": 909.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "method and the invoke method is an",
      "offset": 911.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "inbuilt method. And then this is just me",
      "offset": 913.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "printing what the response is. Now the",
      "offset": 916.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "reason I haven't just printed response",
      "offset": 918.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "by itself and I've used response.content",
      "offset": 920.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is because if you play around with this",
      "offset": 923.199,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and you just print response, well what",
      "offset": 925.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "happens is you get a lot of unnecessary",
      "offset": 927.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "metadata. For example, you get the",
      "offset": 930.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "number of input tokens, the number of",
      "offset": 932.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "output tokens, you get a lot of",
      "offset": 934.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "unnecessary stuff. So if you just want",
      "offset": 936.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the clean AI output result, you can just",
      "offset": 938.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "use response.content. And finally, I'm",
      "offset": 941.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "returning the state because that is what",
      "offset": 943.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "my function uh wants. I want to return",
      "offset": 945.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the updated state. In this case, I'm not",
      "offset": 947.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "updating the state at all. But overall,",
      "offset": 949.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "this node is really just to pass the",
      "offset": 952.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "user's message to the LLM. It's really",
      "offset": 954.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "dead simple, but it shows you how we",
      "offset": 957.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "create the underlying function behind a",
      "offset": 960,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "node. So, now that we've covered the the",
      "offset": 962.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "stuff, let's actually build the graph",
      "offset": 965.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "itself. So, how do we do this? Well, you",
      "offset": 968.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "start off by saying graph is equal to",
      "offset": 971.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "state graph agent state. And this state",
      "offset": 973.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "graph was remember we just covered it",
      "offset": 975.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "previously here. It builds and manages",
      "offset": 977.759,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the entire graph. We have to pass in",
      "offset": 980.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "what the shared state is. In this case,",
      "offset": 983.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "the shared state is this the agent state",
      "offset": 986,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "which we had just defined. Remember the",
      "offset": 988.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "state is really just it's a shared data",
      "offset": 991.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "structure which holds record of",
      "offset": 994.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "everything you want. In this case, it's",
      "offset": 996.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just uh storing the human message. But",
      "offset": 998.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "in more complicated AI agents which",
      "offset": 1001.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we're going to build, it will store a",
      "offset": 1002.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "lot more stuff. Now, how do we create a",
      "offset": 1004.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "node and an edge? This is the underlying",
      "offset": 1006.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "function of the node. So, if I go to",
      "offset": 1009.199,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "this line, it says graph.add node. And I",
      "offset": 1011.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "have two parameters here. Node one and",
      "offset": 1014.959,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "first node. So, this part is the name of",
      "offset": 1017.279,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the node of the graph. And then the",
      "offset": 1020.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "other part is the name of the underlying",
      "offset": 1022.959,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "function aka action. This is the",
      "offset": 1025.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "underlying function and this overall now",
      "offset": 1027.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "is the node along with its name. Then we",
      "offset": 1030.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "also to create an edge we use graph.add",
      "offset": 1033.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "edge and we use the start and the end",
      "offset": 1036.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "point. Start and end point well they're",
      "offset": 1038.72,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "quite self-explanatory right but an edge",
      "offset": 1040.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "has a start point and an endpoint",
      "offset": 1043.439,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "itself. So we want our graph to look",
      "offset": 1045.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "something like this. We want it to",
      "offset": 1048.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "start. We want it to be passed to our",
      "offset": 1050.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "node which was uh this. And then we want",
      "offset": 1052.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "it to end. Uh these two lines of code is",
      "offset": 1056,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "an in-built function in langraph where",
      "offset": 1058.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "you can uh where you're able to",
      "offset": 1061.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "visualize your uh graphs quite well. You",
      "offset": 1063.039,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "don't need to use any external Python",
      "offset": 1066.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "libraries. You can just use this using",
      "offset": 1068.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "ipython. So it's really a great",
      "offset": 1070.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "visualization tool in case you're messed",
      "offset": 1072.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "up somewhere. Uh a rule of thumb here",
      "offset": 1075.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "though is this. If the graph does",
      "offset": 1077.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "compile fully and you get a result and",
      "offset": 1079.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it's uh it looks all fine, chances are",
      "offset": 1082.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "you might have messed up internally and",
      "offset": 1085.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it doesn't always show you the internal",
      "offset": 1087.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "mechanics. So that's just a heads up.",
      "offset": 1088.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Now going back to here, I created",
      "offset": 1090.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "another edge which is this edge here. Uh",
      "offset": 1092.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "it goes from node one and it ends. So to",
      "offset": 1095.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "summarize everything again, we create",
      "offset": 1098.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the add node which is this node. This",
      "offset": 1100.72,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "node's underlying function is this.",
      "offset": 1103.28,
      "duration": 7.759
    },
    {
      "lang": "en",
      "text": "And the this edge is from this start",
      "offset": 1107.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "point to node one. And this edge is from",
      "offset": 1111.039,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "node one to end point. Lastly, I just",
      "offset": 1114.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "compile the entire graph and I store it",
      "offset": 1117.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in an agent variable. Now, another heads",
      "offset": 1120.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "up is this. Notice how I written",
      "offset": 1122.559,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "agent.getgraphth dot draw mermaid png so",
      "offset": 1125.919,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "on and I've not written graph. The",
      "offset": 1129.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "reason I've done agent instead of graph",
      "offset": 1132.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is because if I did graph, it wouldn't",
      "offset": 1134.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "work. Why wouldn't it work is because",
      "offset": 1136.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "graph isn't compiled yet. I need to be",
      "offset": 1139.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "able to send to this function the",
      "offset": 1142.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "compiled version of the graph and the",
      "offset": 1144.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "compiled version of the graph is well",
      "offset": 1146.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Asian. That's why we did graph to",
      "offset": 1148.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "compile. So that's how you define uh a",
      "offset": 1149.76,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "very very simple graph. Now let's",
      "offset": 1154.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "actually test it. So let me just make",
      "offset": 1156.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "sure everything is running. 2 3 Perfect.",
      "offset": 1158.16,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "Now let's run this. Okay. So this",
      "offset": 1162.24,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "function is an infinite loop where I",
      "offset": 1166.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "input something it um unless if it's",
      "offset": 1169.039,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "exit it just keeps going. So I can say",
      "offset": 1172.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "hi there.",
      "offset": 1174.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "There you go. The AI responds how can I",
      "offset": 1177.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "assist you today. Now if you don't",
      "offset": 1179.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "believe me uh I can write something like",
      "offset": 1181.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "who made you and it should say something",
      "offset": 1183.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like open AI. Right there we go. I was",
      "offset": 1186.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "created by OpenAI an AI research",
      "offset": 1188.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "organization.",
      "offset": 1190.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Now notice and I can also obviously exit",
      "offset": 1192,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "but I'm now about to do something here.",
      "offset": 1195.12,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "So I'm going to say uh my name is Vav.",
      "offset": 1197.28,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "So nice to meet you Vav. How can I",
      "offset": 1203.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "assist you today? So where's the",
      "offset": 1205.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "problem? The problem is this. What is my",
      "offset": 1207.12,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "name? Do you think you will be able to",
      "offset": 1211.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "recall my name?",
      "offset": 1214,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "I'm sorry but I don't have access to",
      "offset": 1216.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "personal information blah blah blah. So",
      "offset": 1218.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "let me just exit. Perfect. That's",
      "offset": 1221.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "exited. So now you can clearly see what",
      "offset": 1223.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the problem is, right? It doesn't recall",
      "offset": 1225.12,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "the previous messages. Why? Well, it's",
      "offset": 1227.679,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "because we never created our state that",
      "offset": 1231.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "way. All we did in our state was to",
      "offset": 1234.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "store the human message. And the human",
      "offset": 1238,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "message, for example, was hi there or uh",
      "offset": 1240.24,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "who made you or my name is Vav or what",
      "offset": 1244.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is my name? These queries were the human",
      "offset": 1247.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "messages. I send these messages. That's",
      "offset": 1249.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "why they're human messages. Right? So",
      "offset": 1251.919,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "this is quite a big problem, right? So",
      "offset": 1254.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "how can we fix this? Well, let's move to",
      "offset": 1257.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the next section which is this. So we",
      "offset": 1259.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "have a problem. Our agent doesn't have",
      "offset": 1262.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "any recollection of what the previous",
      "offset": 1264.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "message was. I.e. it doesn't have a",
      "offset": 1266.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "context. Context is written in bold here",
      "offset": 1268.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because that's the fancy way of saying",
      "offset": 1271.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "that it doesn't have any prior",
      "offset": 1273.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "knowledge. This is because each time",
      "offset": 1274.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we're running the this program, it's a",
      "offset": 1276.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "separate API call. So what happens is",
      "offset": 1279.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "when I send a request the AI model or",
      "offset": 1282.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the open air from the open AI server, it",
      "offset": 1285.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "gives me the request back. But that's",
      "offset": 1287.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "it. Then I ask it again. If I go back to",
      "offset": 1289.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "here, I say hi there, it sends a",
      "offset": 1292.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "response which is hello, how can I",
      "offset": 1295.039,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "assist you today? That's the end of the",
      "offset": 1296.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "conversation. If I ask who made you,",
      "offset": 1297.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that's a new API request. it doesn't",
      "offset": 1300.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "recall anything uh from previously. So",
      "offset": 1302.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "we need to somehow integrate memory into",
      "offset": 1306,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "this. Now luckily there is a lot of",
      "offset": 1308.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "different ways to be able to integrate",
      "offset": 1310.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "memory into Langraph agents. And these",
      "offset": 1311.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "are some of the multiple ways you can do",
      "offset": 1314,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it. For example, you can combine with",
      "offset": 1315.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "the current request with previous",
      "offset": 1318.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "context. You can save it in a text file",
      "offset": 1320.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "or in any external file. It could be a",
      "offset": 1323.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "CSV, a JSON, whatever. You can use",
      "offset": 1324.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "inbuilt lang chain memory tools like",
      "offset": 1326.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "checkpointter memory for example. You",
      "offset": 1329.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "can use rag for larger data storage or",
      "offset": 1331.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you can use external memory tools such",
      "offset": 1334.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "as mem zero. These tools are also",
      "offset": 1335.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "getting quite popular but you'll need to",
      "offset": 1338.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "understand the basic documentation of uh",
      "offset": 1340,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "different libraries. Mem is a different",
      "offset": 1343.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "library. There's multiple different uh",
      "offset": 1345.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "such libraries. The same goes for rag.",
      "offset": 1347.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "You can use for example pine cone chroma",
      "offset": 1349.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "database. There's many but you need to",
      "offset": 1352.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "understand slight fundamentals of it. If",
      "offset": 1354.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you just want to stick to langraph you",
      "offset": 1356.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "can use checkpoint memory. These are all",
      "offset": 1358.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "slightly beyond the scope of this",
      "offset": 1360.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "tutorial. I'm going to still integrate",
      "offset": 1361.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "memory somehow and that is using the",
      "offset": 1363.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "first option. The second option is also",
      "offset": 1365.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "quite easy. You can just use a simple",
      "offset": 1367.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "text file but the first one is the",
      "offset": 1369.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "easiest and this moves uh this brings us",
      "offset": 1371.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "to the next part which is agent 2. And",
      "offset": 1374.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "this agent will essentially be our",
      "offset": 1377.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "simple agent but infused with context.",
      "offset": 1380.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "So agent with context feeding. So it",
      "offset": 1383.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "will have some form of basic memory. Now",
      "offset": 1385.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the aim of this agent is to understand",
      "offset": 1388.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the different messages in lang. Remember",
      "offset": 1390.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "I told you there were loads of different",
      "offset": 1393.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "messages not just human message there's",
      "offset": 1395.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "AI message tool message system message",
      "offset": 1397.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "lot of different messages coming up.",
      "offset": 1399.6,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Then we're also going to understand the",
      "offset": 1401.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "union type annotation and we're also",
      "offset": 1402.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "going to feed context back into the LLM",
      "offset": 1404.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "aka the whole overall aim of this agent.",
      "offset": 1407.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "These are all the imports we're going to",
      "offset": 1410.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "need this time around. We're going to",
      "offset": 1411.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "import OS. We're going to from the",
      "offset": 1414.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "typing library import type dictionary,",
      "offset": 1416.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "list and union. We've covered what a",
      "offset": 1418.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "type dictionary is. We know what a list",
      "offset": 1420.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is. We don't know what a union is. Uh",
      "offset": 1422.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "before I explain that, there's also this",
      "offset": 1424.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "these two uh imports which we've already",
      "offset": 1427.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "covered and this new one. We've we",
      "offset": 1429.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "already know what a human message, but",
      "offset": 1432.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "what is an AI message and a system",
      "offset": 1433.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "message? Well, an AI message is well the",
      "offset": 1435.84,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "response of the AI model. In this case,",
      "offset": 1438.72,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "the AI message was this. Now, why didn't",
      "offset": 1442.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "I use AI message in the state before?",
      "offset": 1445.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Well, because we didn't need to store",
      "offset": 1448.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it. We never stored it. We just wanted",
      "offset": 1450.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to store the human message and that's",
      "offset": 1451.84,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "it. But now, we're going to also store",
      "offset": 1453.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the AI message. And then there's also",
      "offset": 1455.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "this system message. This one, the",
      "offset": 1457.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "system message is essentially telling",
      "offset": 1459.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "our large language model to be able to",
      "offset": 1461.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "it it gives instructions to it. If you",
      "offset": 1463.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "don't want to give instructions via the",
      "offset": 1466.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "human message and you just want it to be",
      "offset": 1468.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "infused, uh we use a system message. Uh",
      "offset": 1470.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "don't worry if you didn't understand",
      "offset": 1473.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that, it will make much more sense when",
      "offset": 1474.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we actually code this up. But back to",
      "offset": 1476.24,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "this union type annotation. So union is",
      "offset": 1478.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "a type annotation that allows for a",
      "offset": 1481.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "variable to be more than one data type.",
      "offset": 1484,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "For example, in this very very uh dead",
      "offset": 1486.799,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "simple function which I created, it uses",
      "offset": 1489.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "a a variable called value and I used a",
      "offset": 1492.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "union type annotation and it allows this",
      "offset": 1495.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "variable value to be either an integer",
      "offset": 1498.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "or a float. So like I said, uh this",
      "offset": 1500.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "variable value can only be an integer or",
      "offset": 1504.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "float. So it can't be anything like a",
      "offset": 1507.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "string or a boolean or any custom data",
      "offset": 1509.279,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "type I make only these two. So hopefully",
      "offset": 1511.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you now understand what a union is. So",
      "offset": 1514.96,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "if we go back to uh our code, we create",
      "offset": 1517.76,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "a new state class agent state type",
      "offset": 1521.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "dictionary again and we again only have",
      "offset": 1524.159,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "one single key messages. Last time we",
      "offset": 1527.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "had user message but this time we're",
      "offset": 1530.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "storing every single message. We've",
      "offset": 1532.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "stored this as a union. This could be",
      "offset": 1534.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "human message, AI message, and system",
      "offset": 1536.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "message. These are the three message",
      "offset": 1537.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "types we'll be covering in this agent.",
      "offset": 1539.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "And we're going to be storing this in a",
      "offset": 1541.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "list format. So again, I've used the",
      "offset": 1543.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "same LLM and defined it here. Uh we've",
      "offset": 1545.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "mentioned what uh model is. We've",
      "offset": 1548.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "mentioned what a temperature is. And now",
      "offset": 1550.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I'm going to be I'm going to be creating",
      "offset": 1552.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "this variable. And this is called a",
      "offset": 1554.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "conversation history. So this is where",
      "offset": 1556.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the system message comes in. So it says",
      "offset": 1559.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you are an AI assistant that speaks like",
      "offset": 1561.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a pirate. Answer all of my questions",
      "offset": 1563.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "properly. Now the reason I've said speak",
      "offset": 1565.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "like a pirate is really to show you the",
      "offset": 1568.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "value of system message. The system",
      "offset": 1570.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "message is slightly different to the",
      "offset": 1572.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "human message in the sense that the",
      "offset": 1574.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "system message is purely just",
      "offset": 1576.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "instructions which the uh AI agent has",
      "offset": 1578,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to follow every single time. But whereas",
      "offset": 1581.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a human message well that's dependent on",
      "offset": 1583.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "me right in this case I want the AI",
      "offset": 1585.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "agent to be able to speak like a pirate",
      "offset": 1588.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "every single time it answers back. So it",
      "offset": 1590.48,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "will craft its AI message in that way in",
      "offset": 1594,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "this case like a pirate. So we're now",
      "offset": 1597.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "going to uh define another node in this",
      "offset": 1600.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "case our processing node. So this node",
      "offset": 1602.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh this underlying function uh of the",
      "offset": 1605.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "node is slightly more complicated in the",
      "offset": 1607.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "sense that although we've uh sent the",
      "offset": 1610.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "response again this time we're not uh",
      "offset": 1612.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "sending just the human message we're",
      "offset": 1615.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "sending the uh her message the system",
      "offset": 1617.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "message the AI message everything. Why?",
      "offset": 1619.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because well in our state we've defined",
      "offset": 1622.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "it like that. We've defined it as a list",
      "offset": 1624.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "of either human message, AI message or",
      "offset": 1626.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "system message, right? All of that is",
      "offset": 1628.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "going to get passed into our LLM. Now we",
      "offset": 1630.24,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "update this uh state and this is where",
      "offset": 1633.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "the context or the memory comes in",
      "offset": 1637.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "because of this line. It appends this",
      "offset": 1639.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "state and that allows for the AI agent",
      "offset": 1642.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to essentially recall what's happened in",
      "offset": 1645.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the previous queries in the",
      "offset": 1647.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "conversation. So although we're still",
      "offset": 1648.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "sending separate API requests, it kind",
      "offset": 1651.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "of it will act like it know it's like a",
      "offset": 1653.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "flowing conversation. Now obviously",
      "offset": 1656.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "there are pros and cons to this which",
      "offset": 1657.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we'll discuss in just a bit. But going",
      "offset": 1659.279,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "back to the code this again I print the",
      "offset": 1661.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "response or content and I finally return",
      "offset": 1663.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the updated state. Now this time",
      "offset": 1666,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "remember we've also updated the state",
      "offset": 1667.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "because of this line because we've",
      "offset": 1669.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "appended it and that's why I had also",
      "offset": 1670.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "created this as a list to be able to",
      "offset": 1673.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "append our new uh new state. If we go to",
      "offset": 1675.279,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "this line now again it's the exact same",
      "offset": 1678.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "syntax which was in the simple agent.",
      "offset": 1682.399,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "The overall graph structure hasn't",
      "offset": 1684.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "changed at all. You can see that we have",
      "offset": 1686.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the same start point. We have an end",
      "offset": 1688.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "point. I have changed the name of the",
      "offset": 1690.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "function to help you get familiar with",
      "offset": 1692.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "uh the syntax. The add note I can save",
      "offset": 1694.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this as whatever I want. I can name this",
      "offset": 1697.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "as another new node or something. And",
      "offset": 1699.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "this is the underlying function which is",
      "offset": 1702.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "our processing node. Right? And again we",
      "offset": 1704.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "have the start and end point. But a very",
      "offset": 1706.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "very common mistake is this in a lang",
      "offset": 1708.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "graph which is we mess up the names of",
      "offset": 1711.279,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "our nodes. So that's why I like to keep",
      "offset": 1714.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "these nodes quite simple and intuitive.",
      "offset": 1716.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Don't try to name it something like",
      "offset": 1719.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "completely out of the blue. Try to name",
      "offset": 1722.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it like um something intuitive something",
      "offset": 1723.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "easy to remember and something which",
      "offset": 1726.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "makes sense. In this case LLM node does",
      "offset": 1727.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "make sense. So that's why I've kept it.",
      "offset": 1730.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And let's now run this. So we can run",
      "offset": 1732.24,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "this, run that,",
      "offset": 1736.08,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "that. And we're back. Perfect. So let's",
      "offset": 1740.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "try this simple agent. So now I ask it,",
      "offset": 1744,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "hi, who are you?",
      "offset": 1746.559,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "So what do you think it will output this",
      "offset": 1750.72,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "time? Think about it.",
      "offset": 1753.2,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "Okay. So, obviously you're not going to",
      "offset": 1757.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "guess it uh word by word, but I hope you",
      "offset": 1759.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "said something like it will uh give us",
      "offset": 1762,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "an answer in a pirate themed manner. And",
      "offset": 1764.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "again, it does because we set the system",
      "offset": 1767.2,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "message or instructions here.",
      "offset": 1769.76,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "If we go here, it says, &quot;Ahoy matey, I",
      "offset": 1773.279,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "be your trusty AI assistant here to help",
      "offset": 1776.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "you.&quot; Whatever, whatever. Okay. Now let",
      "offset": 1779.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "me remove this break line and let's run",
      "offset": 1781.6,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "this again. So I'll again ask it hi.",
      "offset": 1784.96,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "Perfect. Now I'm going to ask say my",
      "offset": 1789.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "name again. So my",
      "offset": 1791.679,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "name is Okay. Perfect. Still all good.",
      "offset": 1794.48,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "And now let me ask it what is my",
      "offset": 1797.76,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "name? If I can spell.",
      "offset": 1802.08,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Perfect. Ye be called Va Savvi. It still",
      "offset": 1805.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "speaks in a pythine manner and it still",
      "offset": 1808.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is able to remember what I just asked",
      "offset": 1811.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "him. Why? Because every single time in",
      "offset": 1813.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "our node we are appending the uh update.",
      "offset": 1816.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "We're appending the new state. So we're",
      "offset": 1820.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "getting an updated state. I can also",
      "offset": 1822.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "print the current how the current state",
      "offset": 1824.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "looks like to be able to give you a",
      "offset": 1826.799,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "better intuition. So if I stop this.",
      "offset": 1828.799,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "Okay, that's just a keyboard interrupt.",
      "offset": 1832.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Don't worry about that. Let's run this",
      "offset": 1834.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "again. Let me actually break this so it",
      "offset": 1836.559,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "keeps everything simple.",
      "offset": 1839.52,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "Actually, let's do this.",
      "offset": 1842.159,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "Stop and run. Okay, now look at how the",
      "offset": 1846.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "state gets updated. So again, I'm going",
      "offset": 1850.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to write hi",
      "offset": 1852.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "ao mi whatever. But our current state",
      "offset": 1854.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "looks like this. This new print",
      "offset": 1857.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "statement was what I've just added here.",
      "offset": 1859.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And this is again to show you how the",
      "offset": 1862,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "state changes. So if you look here uh it",
      "offset": 1863.76,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "says uh the system message which is you",
      "offset": 1867.2,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "are an AI assistant then the human",
      "offset": 1870.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "message which is hi and then it also",
      "offset": 1873.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "says the AI message. And why does it",
      "offset": 1875.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "store all of these three message types?",
      "offset": 1878.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Well it's because we had defined them",
      "offset": 1880.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "here. Okay. So hopefully everything is",
      "offset": 1881.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "like piecing together now. And now look",
      "offset": 1885.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "at how we change the how the state gets",
      "offset": 1888.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "updated. So again I'll write uh I can",
      "offset": 1891.12,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "write something like who are you?",
      "offset": 1893.919,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Okay it's now given me the new response",
      "offset": 1897.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "but now look at how the current state is",
      "offset": 1899.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the state is content you are an AI",
      "offset": 1902.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "assistant that stays the same. Then we",
      "offset": 1904.399,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "have the human message which is hi which",
      "offset": 1906.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "was the first message we we've ever",
      "offset": 1909.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "written. And then we have the its",
      "offset": 1911.12,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "respective uh AI message which is here",
      "offset": 1914.24,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "and then we have the human message again",
      "offset": 1918.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the new human message which is who are",
      "offset": 1920.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "you which I just asked it and then this",
      "offset": 1923.279,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "new AI response is right here. And also",
      "offset": 1925.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "one more thing remember uh earlier I had",
      "offset": 1929.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "said we do response.content content is",
      "offset": 1931.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because look at all this unnecessary",
      "offset": 1934.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "metadata. For example, this additional",
      "offset": 1936.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "keyword arguments, response metadata. It",
      "offset": 1938.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "can also sometimes output the input",
      "offset": 1941.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tokens, output tokens, which we just",
      "offset": 1943.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "don't want. We want a clean content and",
      "offset": 1944.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "that's why we just use the content uh",
      "offset": 1947.12,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "method response.content. Hopefully, you",
      "offset": 1950.159,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "started to understand how this state",
      "offset": 1953.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "works and why it gets updated like this.",
      "offset": 1955.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to summarize it stores in this case it",
      "offset": 1957.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "stores the human message the AI message",
      "offset": 1960.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and the system message and every time",
      "offset": 1963.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "we've run it again or we ask a query it",
      "offset": 1965.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "gets appended why it it gets appended",
      "offset": 1968.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "because we uh wrote append right and it",
      "offset": 1971.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "gets stored in a list format and it can",
      "offset": 1974.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "take either a human message AI message",
      "offset": 1976,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "or system message and that's why because",
      "offset": 1977.519,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "we're appending it it gets sent to the",
      "offset": 1980.32,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "uh back of the list so we can now stop",
      "offset": 1983.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "But cool. That's a keyword argument.",
      "offset": 1986.399,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "Perfect. But but I had said there are",
      "offset": 1989.679,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "pros and cons to this agent. So we have",
      "offset": 1992.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "a problem. As our conversation increases",
      "offset": 1996.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "in size, we will use more input tokens.",
      "offset": 1998.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "And because we are using a paid model,",
      "offset": 2002,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "an open AI model, it means more cost per",
      "offset": 2004.559,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "API call gradually. So for example,",
      "offset": 2007.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "right now we are working in like tenth",
      "offset": 2010.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "of a cent but gradually as a",
      "offset": 2013.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "conversation builds builds builds we",
      "offset": 2015.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "might be at the point where we're",
      "offset": 2017.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "sending thousands and thousands and",
      "offset": 2019.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "thousands of tokens every single API",
      "offset": 2020.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "call and that keeps getting appended",
      "offset": 2023.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "because we haven't set any hard limit.",
      "offset": 2025.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "So the simplest simplest way to reduce",
      "offset": 2027.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "the cost if you're using this memory uh",
      "offset": 2030,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "this way of memory is to set a",
      "offset": 2032.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "restriction of the number of messages",
      "offset": 2035.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you store. So you could limit it to the",
      "offset": 2037.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "last five messages. The reason I've said",
      "offset": 2040.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "last five messages is because of recency",
      "offset": 2042.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "bias because you're more likely to be",
      "offset": 2044.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "sending new or the more relevant",
      "offset": 2047.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "important information in the previous",
      "offset": 2049.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "end number of messages. The number five",
      "offset": 2052.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I've just set it as arbitrary. You can",
      "offset": 2054.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "set it as seven. You can set it as 10.",
      "offset": 2056.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "You can set whatever you want if you go",
      "offset": 2058.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with this technique. But yeah, this is a",
      "offset": 2060.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "very very very easy way of being able to",
      "offset": 2062.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "create uh a form of memory for your AI",
      "offset": 2065.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "agent with very very limited number of",
      "offset": 2068.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "code lines. That's the second agent",
      "offset": 2071.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "done. So well done. We're making good",
      "offset": 2073.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "progress. But now it's time to take our",
      "offset": 2075.2,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "AI agents to a whole new level. Remember",
      "offset": 2077.919,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "at the start where I asked you the two",
      "offset": 2080.879,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "questions here,",
      "offset": 2082.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "I had put tools as uh in bold. Now we",
      "offset": 2084.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "actually talk about it. Agent three is",
      "offset": 2088.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "agent with inbuilt tools. Now there's",
      "offset": 2091.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "two types of tools in Langraph. There's",
      "offset": 2093.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the in-built tools and there are your",
      "offset": 2094.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "own custom tools. For this agent, we",
      "offset": 2097.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "will be using the in-built tools and I",
      "offset": 2099.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "want to talk about them because they are",
      "offset": 2101.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "important and they can save you a lot of",
      "offset": 2102.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "time as well. So until now what we've",
      "offset": 2104.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "built is really just the LLM, right?",
      "offset": 2107.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "There's no clear distinction between an",
      "offset": 2109.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "LLM and an AI agent. Everything which",
      "offset": 2111.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "we've done so far, well, you could have",
      "offset": 2114,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "just asked an LLM simply to do without",
      "offset": 2115.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "needing the overall the overhanging",
      "offset": 2118.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Langro code, right? But now, here's",
      "offset": 2121.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "where Langroth gets really like useful.",
      "offset": 2123.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "But so now we're going to upgrade. So",
      "offset": 2126.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we're going to be creating an AI agent,",
      "offset": 2128,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a true AI agent this time, which has",
      "offset": 2130.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "access to different tools. So the aim is",
      "offset": 2132.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "to be able to understand what tools and",
      "offset": 2135.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "conditional edges are to be able to",
      "offset": 2137.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "understand what the base message is to",
      "offset": 2138.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "be able to understand the annotated type",
      "offset": 2141.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "annotation a new type annotation which",
      "offset": 2143.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we're going to learn and also how to",
      "offset": 2144.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "access inbuilt tools in langraph. Okay,",
      "offset": 2147.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so let's get started with what tools and",
      "offset": 2150.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "conditional edges are. And these are the",
      "offset": 2152.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "last two fundamental building blocks",
      "offset": 2154.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which you'll need. And if you master",
      "offset": 2156.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "these five uh building blocks here,",
      "offset": 2158.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "state, nodes, edges, graph, state graph,",
      "offset": 2162.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then these last two which are tools",
      "offset": 2165.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and conditional edge. You can really",
      "offset": 2168,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "build any robust AI agent out there.",
      "offset": 2170.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Almost any AI agent out there using",
      "offset": 2173.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Langraph only if you are able to master",
      "offset": 2174.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "these uh certain tools uh certain",
      "offset": 2177.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "fundamental building blocks. Okay. So",
      "offset": 2179.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "what is a tool? Think of a tool as a",
      "offset": 2181.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "specialized function that the agent can",
      "offset": 2184.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "utilize to perform specific tasks. It",
      "offset": 2186.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "could be to fetch data from an API. For",
      "offset": 2189.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "example, from a weather API, it can uh",
      "offset": 2192.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "get the weather and that would be your",
      "offset": 2195.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "weather tool. So going back to the game",
      "offset": 2197.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "analogy, it could be something the",
      "offset": 2200.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "player aka our agent can use like a",
      "offset": 2202.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "weapon or a healing potion which I've",
      "offset": 2205.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "created here to perform a specific",
      "offset": 2207.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "action. This is very important. The aim",
      "offset": 2210.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of a tool is to be able to allow our LLM",
      "offset": 2213.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "to be able to perform a specific action.",
      "offset": 2216.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So what is a conditional edge? So these",
      "offset": 2218.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "are specialized connections that decide",
      "offset": 2221.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the next node to execute based on the",
      "offset": 2223.599,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "specific conditions or logic applied to",
      "offset": 2226.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the current state. Damn, that's a",
      "offset": 2228.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "mouthful, right? So a good analogy for",
      "offset": 2229.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "this is going back to a video game",
      "offset": 2232.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "analogy and it's thinking of like a path",
      "offset": 2234.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "that only opens if a condition is met.",
      "offset": 2236.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Kind of like an if else statement. We're",
      "offset": 2239.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "going to understand these two in more",
      "offset": 2241.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "depth right here. So these are all the",
      "offset": 2244,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "imports which we're going to need. And",
      "offset": 2246.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it's a lot more import than last time,",
      "offset": 2248.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "right? Really, we've only added this",
      "offset": 2250,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this line which is annotated. Then we've",
      "offset": 2252.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "used from langching.cord on messages.",
      "offset": 2255.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And we've imported all of these new",
      "offset": 2257.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "stuff, particularly base message. We're",
      "offset": 2258.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "going to talk about what a base message",
      "offset": 2261.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is soon. But going to the imports, we",
      "offset": 2262.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "already know chat openai. And then",
      "offset": 2265.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "there's this new line of code from",
      "offset": 2267.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "langro.graph message import add",
      "offset": 2270,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "messages. So this is called a reducer",
      "offset": 2272.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "function. You don't need to know what a",
      "offset": 2275.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "reducer function is in depth, but think",
      "offset": 2277.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of a reducer function as basically being",
      "offset": 2280.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "able to add your messages without them",
      "offset": 2282.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "being overwritten. That's a very very",
      "offset": 2284.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "simple way to put what add messages",
      "offset": 2287.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "reducer function does. We will code this",
      "offset": 2289.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "up in just a bit. But there's also these",
      "offset": 2291.359,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "two. So the duck go search run is a",
      "offset": 2294.16,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "inbuilt tool uh which lang chain has.",
      "offset": 2298.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "We're going to be talking about where",
      "offset": 2301.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "you can get access to these tools. And",
      "offset": 2302.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "then there's this tool node which is",
      "offset": 2304.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "essentially just the fusion of a tool",
      "offset": 2306.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and a node. Kind of like how the state",
      "offset": 2308.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "graph was the fusion of the state and",
      "offset": 2310.88,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "the graph. So the annotated is a type",
      "offset": 2313.599,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "annotation. So it allows for a variable",
      "offset": 2317.119,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "to be more than one data type. Now it's",
      "offset": 2320.079,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "very similar to what a union was right.",
      "offset": 2322.88,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "So it allows for uh for in example in",
      "offset": 2325.92,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "this case it's an integer and an age",
      "offset": 2329.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "which which is x. and I've set it to as",
      "offset": 2332.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "20. So it basically allows us to provide",
      "offset": 2334.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "extra information about the type of",
      "offset": 2337.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "value. In this case, we're providing age",
      "offset": 2339.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "as extra information about the variable",
      "offset": 2342,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "x. Now sequence is another type",
      "offset": 2344.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "annotation which allows a variable to be",
      "offset": 2346.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "any ordered collection. So it could be",
      "offset": 2348.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like a list or a pupil or a string or",
      "offset": 2350.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "whatever. There's a slight difference",
      "offset": 2353.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "between what a list and a sequence is.",
      "offset": 2355.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "You don't need to know about this in",
      "offset": 2357.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "depth. The reason I've included sequence",
      "offset": 2359.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "here is because we've used that in the",
      "offset": 2361.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "code. So if we go to here from typing",
      "offset": 2362.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "import sequence, uh it's really similar",
      "offset": 2365.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "to what a list is. Uh but there's a",
      "offset": 2368.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "slight difference which is a sequence is",
      "offset": 2370.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "a read only access to any order data",
      "offset": 2372.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "type. But a list is a mutable ordered",
      "offset": 2375.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "collection. So a sequence is only used",
      "offset": 2377.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "when you only need to read the data, not",
      "offset": 2380.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "modify it. And a list is you can add,",
      "offset": 2382.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "remove or change items in it. That's why",
      "offset": 2385.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we've used a reducer function for the",
      "offset": 2386.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "sequence which you're going to see in a",
      "offset": 2388.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "minute. Now, what is a base message?",
      "offset": 2390.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Remember, we just imported what what the",
      "offset": 2392.8,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "base message was uh right here. So, what",
      "offset": 2395.359,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "exactly is it? Think of the base message",
      "offset": 2398.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "as the parent class of all of the",
      "offset": 2401.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "different types of messages such as the",
      "offset": 2404,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "human message, AI message, system",
      "offset": 2406.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "message. Now there's so many different",
      "offset": 2408.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "message types in langraph and lang chain",
      "offset": 2410.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that it could be quite cumbersome to",
      "offset": 2413.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "write all of them and to make sure we've",
      "offset": 2415.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "got all of them. So we can just simply",
      "offset": 2417.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "put base message. So again think of base",
      "offset": 2419.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "message as the parent class and human",
      "offset": 2422.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "message AI message as the children",
      "offset": 2424.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "classes which inherit all the properties",
      "offset": 2426.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "of the base message and also have their",
      "offset": 2428.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "own uh properties respectively. Right?",
      "offset": 2431.119,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "So that is what a base message is. Now",
      "offset": 2433.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "going back to the inbuilt tools in",
      "offset": 2436.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Langra and Langraph have a lot of",
      "offset": 2439.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "inbuilt tools. So this is where the",
      "offset": 2440.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "documentation is. So if I follow the",
      "offset": 2443.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "link, we come to this. Now this is a",
      "offset": 2445.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "really well-written documentation and",
      "offset": 2448.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "they have a lot of tools um inbuilt",
      "offset": 2450.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "already. For example, there's searches,",
      "offset": 2453.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "there's for code interpreters, there's",
      "offset": 2455.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for productivity, web browsing,",
      "offset": 2457.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "databases, etc., etc. You get the point.",
      "offset": 2460,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "There's a lot of stuff here. So you can",
      "offset": 2463.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "even click on anything. For example, if",
      "offset": 2466.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "we go to search and duck.go, which is",
      "offset": 2467.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "what we're going to be using for this",
      "offset": 2469.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "agent, you can see all of the uh",
      "offset": 2471.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "documentation and it's very very very",
      "offset": 2473.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "well organized. So going back to this,",
      "offset": 2475.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we I want to show you how we can use",
      "offset": 2478.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "this duck.go search run. So for example,",
      "offset": 2480.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "I've asked it a query who won the",
      "offset": 2484,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Champions League final 2025, which",
      "offset": 2486.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "recently happened. our AI agent. Well, I",
      "offset": 2488.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "can show you here",
      "offset": 2491.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "if I run this.",
      "offset": 2493.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Okay, so I've printed two different",
      "offset": 2496.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "types here. I've printed the AI response",
      "offset": 2498.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "and I've printed the duck.go search runs",
      "offset": 2500.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh response aka the search tools. You",
      "offset": 2503.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "can see that GPD40 minis says I'm sorry,",
      "offset": 2505.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "but I don't have information on events",
      "offset": 2509.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that occurred after October 2021. Why?",
      "offset": 2511.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Because these models, well, their",
      "offset": 2514.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "training data was up to this set date.",
      "offset": 2516,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "So obviously it won't have access to um",
      "offset": 2518.64,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "uh 2025 Champions League final, right?",
      "offset": 2522.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "It we're lucky that it was able to say",
      "offset": 2525.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that it didn't because a lot of the",
      "offset": 2527.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "times the model can um it can",
      "offset": 2530,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "hallucinate and obviously that's quite",
      "offset": 2531.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "bad. If you don't know what",
      "offset": 2533.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "hallucination is, hallucination is",
      "offset": 2535.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "essentially when an AI model just spits",
      "offset": 2536.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "out garbage because it just needs to",
      "offset": 2539.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "spit out something, right? Uh so that's",
      "offset": 2541.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "what hallucination is. So it could have",
      "offset": 2544.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "said something like uh the winners of",
      "offset": 2546,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Champions League final was like Real",
      "offset": 2548.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Madrid or or some something which is",
      "offset": 2550.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "completely wrong or uh infactual. But",
      "offset": 2552.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "look at what the search tool print did.",
      "offset": 2555.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "It said PSG and Inter Milan locked horns",
      "offset": 2557.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in the final of the UFA Champions League",
      "offset": 2559.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "in 2025 and it eventually says that PSG",
      "offset": 2561.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "achieved a historic milestone by",
      "offset": 2564.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "winning. So it was able to uh show us",
      "offset": 2566.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "all of the details fully. It showed when",
      "offset": 2570.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the date was, what the time was, who",
      "offset": 2572.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "won, who it was between, what date it",
      "offset": 2574.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "was, everything. And you can now see if",
      "offset": 2577.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "we were able to connect the tool and an",
      "offset": 2580.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "AI agent together, sorry, a tool or an",
      "offset": 2583.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "LLM together, it could be quite",
      "offset": 2586.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "powerful, right? So that is the whole",
      "offset": 2588.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "point of AI agents. And that's exactly",
      "offset": 2590.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what we're about to do right now. If we",
      "offset": 2592.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "go and define our class agent state",
      "offset": 2594.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "here, it's the same way how we've been",
      "offset": 2596.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "defining the state in our previous",
      "offset": 2598.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "agents. But this time the messages key",
      "offset": 2600.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is slightly different. We've used",
      "offset": 2603.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "annotated which we just talked about",
      "offset": 2606,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "which is over here. It allows us to",
      "offset": 2608.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "provide extra information about the type",
      "offset": 2611.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of value. In this case, we're providing",
      "offset": 2613.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "age as extra information about the",
      "offset": 2615.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "variable X. So that's what annotated",
      "offset": 2616.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "does. It literally is just a way to be",
      "offset": 2619.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "able to provide extra information. And",
      "offset": 2621.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "over here that's what we've done. So",
      "offset": 2624.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "this essentially says this part is what",
      "offset": 2627.28,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "data type the um your uh message is. So",
      "offset": 2629.839,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "it could be either a tool message, an AI",
      "offset": 2633.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "message, etc. And this all stems from",
      "offset": 2637.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the base message. Remember what is the",
      "offset": 2639.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "base message? The base message is the",
      "offset": 2640.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "parent class of all of the different",
      "offset": 2643.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "types of messages. So and this add",
      "offset": 2644.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "messages is the reducer function. And",
      "offset": 2647.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "I've written in comments here. It",
      "offset": 2649.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "essentially allows for the addition of",
      "offset": 2651.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "messages without getting deleted. So",
      "offset": 2652.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that's all you need to know for this.",
      "offset": 2654.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Now a tip is you can use this line of",
      "offset": 2657.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "these two lines of code in almost any AI",
      "offset": 2660.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "agent where you just want to store the",
      "offset": 2663.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "messages. It will work completely fine",
      "offset": 2665.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because you're storing the messages",
      "offset": 2667.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "without any overwriting and you don't",
      "offset": 2669.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "need to work uh you don't need to worry",
      "offset": 2670.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "about the different message types like",
      "offset": 2672.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "uh human message, AI message and all",
      "offset": 2674.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that. It covers everything. So there you",
      "offset": 2676.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "go. Now we create a list of tools which",
      "offset": 2678.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we have. In this case, I will only give",
      "offset": 2680.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "our AI agent the access to the doctor go",
      "offset": 2682.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "search run tool. So you can have as many",
      "offset": 2685.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "tools as you want. You can write",
      "offset": 2687.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "something like whatever tool you want.",
      "offset": 2689.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "But obviously make sure it's a proper",
      "offset": 2691.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "tool. But because we're only using",
      "offset": 2693.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "inbuilt tools and I want to keep this",
      "offset": 2694.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "very simple. I'm only going for Dr. Go",
      "offset": 2696.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "searchron tool. So now we go with the",
      "offset": 2699.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "model uh definition here. I've upgraded",
      "offset": 2701.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to GPT40 here because I personally feel",
      "offset": 2704.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "like GBT40 is the best LLM uh which is",
      "offset": 2707.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "also reasonable in pricing uh for tool",
      "offset": 2710.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "calling from the OpenAI uh services and",
      "offset": 2713.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "it's also fast. I could have also used a",
      "offset": 2716.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "reasoning model but obviously that would",
      "offset": 2718.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "have taken more time to uh give an",
      "offset": 2719.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "answer right. So how am I able to",
      "offset": 2721.359,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "connect my LLM to uh these tools? So the",
      "offset": 2724.4,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "way to do it is through this dobbind",
      "offset": 2729.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "tools. So this is really how to give our",
      "offset": 2731.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "LLM knowledge about these tools. Gives",
      "offset": 2735.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "basically the access for uh it gives the",
      "offset": 2737.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "LLM uh knowledge about the existence of",
      "offset": 2740.24,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "these tools. So if I run this perfect uh",
      "offset": 2743.04,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "now I've defined another node and this",
      "offset": 2748.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "is model call. So essentially this is",
      "offset": 2750.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "quite similar to what we've been doing",
      "offset": 2753.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "previously. We just use uh we create",
      "offset": 2754.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "system message and I'm no more pirate",
      "offset": 2758.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "stuff. I have just kept it very simple",
      "offset": 2761.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "saying you are my AI assistant please",
      "offset": 2763.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "answer my query to the best of your",
      "offset": 2765.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "ability. So it should there's no pirate",
      "offset": 2766.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "stuff there's nothing just a clearcut",
      "offset": 2769.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "answer to my request and then I invoke",
      "offset": 2771.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the model and now I pass the system",
      "offset": 2775.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "prompt which is the system message here",
      "offset": 2778,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "as well as all of the state um which we",
      "offset": 2780.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "had before and then we just return the",
      "offset": 2783.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "messages the response of the messages.",
      "offset": 2785.28,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "Perfect. Okay, now here's where it gets",
      "offset": 2788.8,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "slightly tricky. And um so pay attention",
      "offset": 2792.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "here because now we're going to be",
      "offset": 2795.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "defining the conditional edge. So if I",
      "offset": 2796.88,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "quickly run this, okay, and run this.",
      "offset": 2799.68,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "Okay, so let's go with this function. So",
      "offset": 2804.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "this should continue function is the",
      "offset": 2808.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "underlying action behind the conditional",
      "offset": 2810.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "edge. The conditional edge is a decision",
      "offset": 2813.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "element at the end of the day. So it in",
      "offset": 2816.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this line of uh in this function what",
      "offset": 2818.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we've done is we take the messages. So",
      "offset": 2820.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we copy the latest state into the",
      "offset": 2823.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "messages variable and then we look at",
      "offset": 2825.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the last message. Now there's a reason",
      "offset": 2827.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "we look at the last message and it's",
      "offset": 2829.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "mainly to look for if there's any",
      "offset": 2831.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "further tool calling. Why? Because if",
      "offset": 2834,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "there's no further tool calling required",
      "offset": 2837.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "in the last message, well we can just",
      "offset": 2839.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "end. That's what this line of code says.",
      "offset": 2841.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "If not last message tool cause. So if it",
      "offset": 2843.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "doesn't need any more tools, it just",
      "offset": 2846.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "ends the thing. Else we return and uh",
      "offset": 2848.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "else we return continue. So continue is",
      "offset": 2851.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "an edge and end is an edge as well. And",
      "offset": 2854.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "this will make a lot more sense in just",
      "offset": 2856.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a minute. Just bear with me. I want to",
      "offset": 2858.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "create the graph itself first. So we",
      "offset": 2861.2,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "again use state graph agent state. We",
      "offset": 2864.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "use the add node method which we've done",
      "offset": 2867.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "to uh add this node which is the model",
      "offset": 2869.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "calling which is very very similar to",
      "offset": 2871.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "our previous two AI agents. And then we",
      "offset": 2873.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "create a tool node. Now a tool node is",
      "offset": 2875.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "essentially just a node that neatly",
      "offset": 2878.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "contains all of the tools we want our",
      "offset": 2880.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "agent to have. So it basically is one",
      "offset": 2882.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "single node which will have all of these",
      "offset": 2885.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "tools neatly arranged. So we don't need",
      "offset": 2887.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "to worry about all of the internal",
      "offset": 2889.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "mechanics. So that's what this line of",
      "offset": 2891.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "code does. And then we add this node as",
      "offset": 2893.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "tools in our graph. So visually speaking",
      "offset": 2895.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "we have created this this tool node",
      "offset": 2898.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "here. Here's the uh the obviously the",
      "offset": 2901.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "set entry point is our agent. So this is",
      "offset": 2903.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "another way you can define a start and",
      "offset": 2905.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "an end point in uh langraph. You can",
      "offset": 2908.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "either use end or end start or you can",
      "offset": 2911.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "use dot set entry point and dot set",
      "offset": 2913.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "finish point or dot set end point in",
      "offset": 2916,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "langraph. There's multiple ways. I don't",
      "offset": 2918.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "personally have a preference but you can",
      "offset": 2920.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "use both ways. So that's why I've",
      "offset": 2921.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "written it like this. At the end of the",
      "offset": 2923.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "day, you need to define your start and",
      "offset": 2925.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "your end point somehow. And there's",
      "offset": 2928,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "multiple ways to do it. That said, but",
      "offset": 2929.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "how do we create this conditional edge?",
      "offset": 2932.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "If we go to this uh line of code, it",
      "offset": 2934.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "says graph.add conditional edges. So",
      "offset": 2936.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this internal method essentially creates",
      "offset": 2938.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the conditional edge for us. Now you can",
      "offset": 2941.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "have many many many conditional edges",
      "offset": 2943.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "within your uh langraph or within your",
      "offset": 2945.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "graph, but in this case, we're just",
      "offset": 2947.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "sticking with one. So for this method",
      "offset": 2950.079,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "the add conditional edges we have the",
      "offset": 2953.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "first parameter needs to be the origin",
      "offset": 2956,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and this is our agent. Now what is our",
      "offset": 2958,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "agent? Our agent is the node which we",
      "offset": 2960.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just created. This is where all the",
      "offset": 2962.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "model calling gets passed. Then we have",
      "offset": 2964.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the underlying action. So similar to how",
      "offset": 2966.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "the node needed the name of the node and",
      "offset": 2969.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the underlying action aka the function",
      "offset": 2972.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "behind this node which in this case was",
      "offset": 2974.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "a model call. Similarly, our underlying",
      "offset": 2976.559,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "action is should continue which is this",
      "offset": 2979.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "function or this underlying logic. It",
      "offset": 2982.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "returns two parts. It returns either an",
      "offset": 2985.04,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "end or it returns a continue. So if it",
      "offset": 2987.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "returns end, we go to the end part. Then",
      "offset": 2990.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you can see this dotted edge line here",
      "offset": 2993.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "that it returns end. It goes to the end",
      "offset": 2995.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "point. But if our underlying function,",
      "offset": 2998.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the should continue function return",
      "offset": 3000.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "continue. Well, that means it still",
      "offset": 3003.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "needs to run some more tools and that's",
      "offset": 3004.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "why it goes back to the tool node. So",
      "offset": 3006.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "from this it goes our agent continue and",
      "offset": 3009.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "this goes to the end if there's no more",
      "offset": 3012.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "tool calling left. But the graph would",
      "offset": 3014.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "still not be complete if we had left it",
      "offset": 3017.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "by this point because if you go to your",
      "offset": 3019.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tools there's no way to go back to your",
      "offset": 3022,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "agent and that's where you create this",
      "offset": 3024.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "new edge as well. And this this edge",
      "offset": 3027.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "goes from tools to our agent. Uh the",
      "offset": 3029.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "direction is from the start point tools",
      "offset": 3032.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and the end point our agent. Now that",
      "offset": 3034.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "was quite a lot. So I'm going to",
      "offset": 3036.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "summarize it one more time which is this",
      "offset": 3038.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the it starts with the start point with",
      "offset": 3040.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this set entry point. Then we define our",
      "offset": 3042.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "node our agent which we've been doing",
      "offset": 3045.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for the past uh three AI agents now and",
      "offset": 3047.119,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "that was through this. Then we created",
      "offset": 3050,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "our conditional edge which was either",
      "offset": 3053.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "continue",
      "offset": 3055.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "or end. Uh and the underlying function",
      "offset": 3056.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "behind that was this should continue",
      "offset": 3059.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "function. And lastly we needed to create",
      "offset": 3061.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "an edge which goes from tools to our",
      "offset": 3064.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "agent which was through this. And then",
      "offset": 3066.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "we compile the graph and then we send",
      "offset": 3069.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "this to uh visualize it. So let's just",
      "offset": 3071.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "run this to make sure everything is",
      "offset": 3073.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "working. And now let's actually try",
      "offset": 3075.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this. So this fancy piece of code which",
      "offset": 3077.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "I've written here is really just a way",
      "offset": 3080.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "of making the print statements or the",
      "offset": 3083.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "output look better. So all it says is",
      "offset": 3086.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "basically u if it's a human message you",
      "offset": 3088.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "write human message. If it's an AI",
      "offset": 3091.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "message write a AI message. If it's a",
      "offset": 3093.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "tool message write a tool message. You",
      "offset": 3094.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "get the point. It's just a way to neatly",
      "offset": 3096.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "organize it. So you can steal this piece",
      "offset": 3098.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of code if you uh want to um uh",
      "offset": 3100.48,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "visualize your uh outputs in a much",
      "offset": 3103.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "neater way. So now I pass in the query",
      "offset": 3106.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "who won the Champions League final in",
      "offset": 3109.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "2025. Uh the same query I had passed",
      "offset": 3111.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "before. And notice the tool message",
      "offset": 3114.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "which was this massive piece of um",
      "offset": 3117.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "output and then the AI message the final",
      "offset": 3120.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "AI message which was PSG won in 2025",
      "offset": 3123.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "defeating Inter Milan with a final",
      "offset": 3126.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "victory at this display this place. What",
      "offset": 3127.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "exactly happened here? Let's go through",
      "offset": 3130.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the workflow. So we passed in a query",
      "offset": 3132,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "which was at the start and then we",
      "offset": 3134.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "passed it to our agent. Then AI agent",
      "offset": 3136.4,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "decided okay the user wants to do a",
      "offset": 3139.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "searching. How did it decide that? Well,",
      "offset": 3143.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the an internal LLM GPD40 decided okay",
      "offset": 3145.52,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "the user wants to search. So it used",
      "offset": 3148.96,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "tool calling search aka this over here.",
      "offset": 3151.599,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "It used the duck.go search here uh with",
      "offset": 3155.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "it uh with his respective call ID and",
      "offset": 3158.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "with the respective query and the query",
      "offset": 3160.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "which was passed to the actual duck.go",
      "offset": 3162.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "search was Champions League final 2025",
      "offset": 3165.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "winner. It's slightly different with the",
      "offset": 3168.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "human message. So you can see that the",
      "offset": 3170.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "AI the LLM in the background decided",
      "offset": 3173.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "what to pass or what the argument to",
      "offset": 3176.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "pass to this Dr. Go search was and the",
      "offset": 3178.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Dr. Go search returned the tool uh",
      "offset": 3182.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "returned this output. How did it uh",
      "offset": 3184.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "return it? It went through this edge",
      "offset": 3187.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "over here and passed all of the output",
      "offset": 3189.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "aka all of this output back to our",
      "offset": 3192.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "agent. Then our agent decided whether we",
      "offset": 3195.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "need any further tool calling because we",
      "offset": 3198.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "only asked for one thing. We don't need",
      "offset": 3200.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "any further more further tool calling",
      "offset": 3202.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "right. Uh and because of this logic",
      "offset": 3205.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "which we defined in this we just return",
      "offset": 3207.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "end. So it goes to the endpoint and it",
      "offset": 3209.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "outputs the result which was PSG won the",
      "offset": 3212.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Champions League final in 2025. So that",
      "offset": 3214.96,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "was how we can create a robust AI agent",
      "offset": 3218.4,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "by connecting an LLM with a tool. So",
      "offset": 3222.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "great job so far. That was quite a lot.",
      "offset": 3225.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "But remember, practice makes perfect. So",
      "offset": 3228.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "try to create your own AI tools. Try to",
      "offset": 3230.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "go to this documentation and try to",
      "offset": 3232.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "create uh try to pick up any of these",
      "offset": 3235.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "tools and just build. There are also",
      "offset": 3237.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "free and paid. So be mindful of that. I",
      "offset": 3240,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "used a free tool, but there's a lot of",
      "offset": 3242.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "uh there's a lot of options. So feel",
      "offset": 3244.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "free have at it. Now I'm going to",
      "offset": 3247.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "introduce you to a new piece of",
      "offset": 3248.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "terminology which is a react agent. So a",
      "offset": 3250.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "react agent stands for reasoning and",
      "offset": 3255.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "acting agent. Uh they look like this in",
      "offset": 3257.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a nutshell. So you have your start and",
      "offset": 3260.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "your uh endpoint. Then you have your",
      "offset": 3262.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "agent in the middle and then you have a",
      "offset": 3264.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "loop which uses uh which keeps going",
      "offset": 3266.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "with the tools and back and forth. But",
      "offset": 3270.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hold on. Doesn't this look a bit",
      "offset": 3272.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "familiar? If you if you scroll slightly",
      "offset": 3274.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "up, if I hide this output, you can see",
      "offset": 3276.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that this is almost exactly, if not",
      "offset": 3278.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "exactly the same as this, the these",
      "offset": 3281.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "graphs are isomeorphic. So, they're just",
      "offset": 3284,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "translated differently. The tools in the",
      "offset": 3285.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "endpoint have switched uh positions, but",
      "offset": 3287.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you can see that this and this are",
      "offset": 3289.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "exactly the same. So, what we just did",
      "offset": 3291.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "was create a React agent. And React",
      "offset": 3294.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "agents are really, really, really common",
      "offset": 3297.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "in industry and in personal projects.",
      "offset": 3300,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Now because they are so common and they",
      "offset": 3302.559,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "first of all they're common because",
      "offset": 3305.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "they're quite robust. Because they're so",
      "offset": 3306.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "common, Langraph implemented its own",
      "offset": 3308.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "custom method uh within its library. Now",
      "offset": 3311.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this will make things a lot easier.",
      "offset": 3314.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "After this you will have one question",
      "offset": 3316.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "which is why did I go through all of",
      "offset": 3318.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "this line of code and it was really to",
      "offset": 3320.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "be able to teach you about conditional",
      "offset": 3323.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "edges and tools. So if you've mastered",
      "offset": 3325.68,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "that uh you can now create the same",
      "offset": 3328.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "react agent the exact same react agent",
      "offset": 3332.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in very few lines of code and this is",
      "offset": 3334.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "how you do it. So agent 4 react agent",
      "offset": 3336.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the easy way to create these agents. So",
      "offset": 3339.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "the aim of this is really how to create",
      "offset": 3343.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "react agents fast. So these are all of",
      "offset": 3345.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "the new libraries uh the meth all the",
      "offset": 3348.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "imports you need to make and there's",
      "offset": 3350.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "only one new input which you need to",
      "offset": 3352.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "make which is create react agent. This",
      "offset": 3353.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "will be the uh method in the inbuilt",
      "offset": 3356.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "method which will be able to create",
      "offset": 3358.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "these react agents for us. So we again",
      "offset": 3360.559,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "define the list of tools and we have our",
      "offset": 3364.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "LLM chat openai and again we're using",
      "offset": 3367.04,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "GPT40 shock and this is how we create",
      "offset": 3370,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "the exact same react agent through very",
      "offset": 3374,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "few lines of code. So we use the inbuilt",
      "offset": 3377.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "method. We pass whatever model we want.",
      "offset": 3380.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "In this case, we're going to be using",
      "offset": 3383.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "chat GBD40 from OpenAI. We are going to",
      "offset": 3384.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "pass in the list of tools. We're going",
      "offset": 3386.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to name our agent search agent because",
      "offset": 3388.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "it's spitting, right? It only has access",
      "offset": 3391.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "to the doctor go search run tool and our",
      "offset": 3393.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "prompt aka our system message. Uh so you",
      "offset": 3396.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "are my AM you are my AI system is the",
      "offset": 3398.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "exact same thing. You don't need to",
      "offset": 3401.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "specify that this is a system message",
      "offset": 3402.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "because uh the internal mechanics",
      "offset": 3404.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "handles that for you. So if I run this",
      "offset": 3406.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "and this perfect, it should work. Okay,",
      "offset": 3409.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "awesome. And if I print this again,",
      "offset": 3412.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "you can see the almost the exact same",
      "offset": 3416,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "thing was outputed in very few lines of",
      "offset": 3418.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "code. The tool calling we did was",
      "offset": 3421.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "duck.go search with the uh this query",
      "offset": 3423.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and the final answer was this. So in",
      "offset": 3426.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "what few like three or four lines of",
      "offset": 3429.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "code, we were able to completely",
      "offset": 3432.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "replicate what we had done in the",
      "offset": 3433.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "previous AI agents. So if you need to",
      "offset": 3435.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "create React agents files or just create",
      "offset": 3437.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "React AI agents in general after you've",
      "offset": 3439.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "mastered what a conditional edge and a",
      "offset": 3442.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tool is, you can just create this uh you",
      "offset": 3444.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "can just use this inbuilt method. It",
      "offset": 3446.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "makes things so much easier. Yeah. So",
      "offset": 3447.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we've achieved the same result in much",
      "offset": 3449.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "fewer lines. But now we go to agent 5.",
      "offset": 3451.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "And your question might be this. How do",
      "offset": 3454.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "we is there a way to create our agents",
      "offset": 3456.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "using our own tools? And the short",
      "offset": 3459.599,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "answer is yes, obviously. So what if we",
      "offset": 3462,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "want to make our own tools? After all,",
      "offset": 3465.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Langro would have all the inbuilt tools",
      "offset": 3467.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we want, right? So that's the aim of",
      "offset": 3469.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "this agent. Also, please note that we're",
      "offset": 3471.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "going to be using the inbuilt method to",
      "offset": 3473.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "create these React agents from now on.",
      "offset": 3475.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Uh the reason is well, if I write all of",
      "offset": 3477.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "this unnecessary code again, it will",
      "offset": 3480,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just get messy and it will just be",
      "offset": 3481.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "unreadable. If we import this, oh that's",
      "offset": 3484,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "agent 4. If we import this, perfect. So",
      "offset": 3486.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this is the new line of code and this",
      "offset": 3490.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "line of code essentially tells langraph",
      "offset": 3492.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that this function is a tool or a",
      "offset": 3495.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "specialized function. So how do we",
      "offset": 3498.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "actually define a tool in langraph our",
      "offset": 3500.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "own tool. So you use at tool which is a",
      "offset": 3503.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "decorator and this decorator basically",
      "offset": 3506.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "tells langraph that this is a special",
      "offset": 3509.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "function aka in this case it's a tool",
      "offset": 3510.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and in this case I have just created a",
      "offset": 3513.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "very very dead simple tool which is the",
      "offset": 3515.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "weather tool. So it basically gives the",
      "offset": 3517.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "weather in a given city. So the two most",
      "offset": 3519.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "important things which you need to know",
      "offset": 3522.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "uh which you need to be mindful of when",
      "offset": 3524.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "creating your own tools is using this",
      "offset": 3526.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "decorator and also this dock string.",
      "offset": 3529.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Possibly the most important thing if you",
      "offset": 3531.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "don't have a dock string your entire",
      "offset": 3534.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "graph will not compile. And the reason",
      "offset": 3536.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "for this dock string is it tells our",
      "offset": 3538.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "agent what the tool is form. In the next",
      "offset": 3540.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "agent, I will tell you how to structure",
      "offset": 3543.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the docs training in a better manner.",
      "offset": 3545.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "But for now, to keep it very simple,",
      "offset": 3547.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "I've just said it gets the weather in a",
      "offset": 3549.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "given city. So our underlying LLM will",
      "offset": 3551.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "know, okay, this tool, this weather tool",
      "offset": 3553.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "uh has is for getting the weather in a",
      "offset": 3556.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "given city. Now your question could be",
      "offset": 3558.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the previous doctor go search tool.",
      "offset": 3560.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Where's the dock string in that? There",
      "offset": 3562.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "is a dock string. If you go within the",
      "offset": 3564.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "documentation, all of the dock strings",
      "offset": 3566.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "are uh written by the Langraph team. So",
      "offset": 3568,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's why you didn't need to write any",
      "offset": 3570.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "dock strings for that. So let's run this",
      "offset": 3572.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "as well.",
      "offset": 3574.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "And again we create the same list of",
      "offset": 3577.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "tools and we use the create react agent.",
      "offset": 3578.799,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "And in this case I've named it weather",
      "offset": 3581.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "agent. Why? Because well we're using the",
      "offset": 3583.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "weather tool our own custom tool.",
      "offset": 3585.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "Perfect. And now let's just run this. So",
      "offset": 3588.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "we are invoking the tool by setting the",
      "offset": 3591.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "messages uh as what is the weather in",
      "offset": 3594.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "London. So the hard-coded message is the",
      "offset": 3597.52,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "weather in this city is sunny. And if",
      "offset": 3601.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you look, the weather in London is",
      "offset": 3603.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "currently sunny. You can see that it",
      "offset": 3605.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "works completely as if we were using an",
      "offset": 3607.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "inbuilt tool. It uses the weather tool.",
      "offset": 3610.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "It sets the arguments and everything",
      "offset": 3612.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "works flawlessly. Now what if we wanted",
      "offset": 3614.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "in multiple tools? Well, you can do that",
      "offset": 3616.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "as well in Langraph. You can. In this",
      "offset": 3618.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "case, I have set the uh another created",
      "offset": 3621.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "another tool which is the social media",
      "offset": 3624.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "follower account. So you pass in the",
      "offset": 3626.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "social media as obviously as a string",
      "offset": 3629.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and this is how you write dock strings.",
      "offset": 3631.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "So you have the explanation, you have",
      "offset": 3634.319,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "your arguments, you have what's returned",
      "offset": 3637.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and you also provide an example. All of",
      "offset": 3640.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this dock string is passed into the LLM.",
      "offset": 3642.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So obviously the more stuff you have",
      "offset": 3645.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "here, the better the LLM will",
      "offset": 3647.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "understand. Okay, what this person or",
      "offset": 3649.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "what this tool needs me to pass in as",
      "offset": 3652,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "arguments. The very simple return",
      "offset": 3654.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "statement is you have 9876 followers on",
      "offset": 3656.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "social media. Something random I",
      "offset": 3659.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "completely make made up. But this is how",
      "offset": 3661.119,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "you create another tool. And now look at",
      "offset": 3664.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "how we've created our tools list. It's",
      "offset": 3667.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "increased by one because now we're",
      "offset": 3669.68,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "giving our AI agent our AI agent uh more",
      "offset": 3671.28,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "tools. So again, we're using the GPT40",
      "offset": 3675.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "tool, a GPD40 LN, and we're giving it",
      "offset": 3678.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "access to the tools. And now I've set it",
      "offset": 3681.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "as a different name, aka the general",
      "offset": 3683.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "agent. The same prompt though, and let's",
      "offset": 3685.359,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "invoke this.",
      "offset": 3688.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "So this time I've asked what is the",
      "offset": 3690.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "weather in London and how many followers",
      "offset": 3692.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "do I have on Instagram? And then what",
      "offset": 3694.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "about Twitter? So there's quite a lot of",
      "offset": 3696.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh tool calling which will be necessary.",
      "offset": 3699.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "But look at why Langro is so robust and",
      "offset": 3701.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "it's the number one upcoming AI agent",
      "offset": 3704.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "framework now. It's because in very few",
      "offset": 3706.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "lines of code I was able to call the",
      "offset": 3709.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "weather the social media follow account",
      "offset": 3711.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "twice. One for Instagram and one for",
      "offset": 3713.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Twitter. And look at the final response.",
      "offset": 3716.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "The weather in London is sunny which is",
      "offset": 3718.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "true. And you have 9876 followers on",
      "offset": 3720.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "Instagram and on Twitter as well. So in",
      "offset": 3723.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "very very very few lines of code I was",
      "offset": 3726.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "able to create a very robust AI agent",
      "offset": 3728.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and I can even increase by asking what",
      "offset": 3732.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "about on Facebook or what about on",
      "offset": 3734.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Snapchat. You get the point. I could",
      "offset": 3736.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have also asked what is the weather in",
      "offset": 3738.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "front uh what's the weather in Paris and",
      "offset": 3740.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it would have still worked flawlessly as",
      "offset": 3742.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "well. So that is why AI agents in",
      "offset": 3744.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "langraph are so robust because of that",
      "offset": 3747.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "graph structure and this create react",
      "offset": 3749.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "agent is why is also why it's so",
      "offset": 3751.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "commonly used in industry and in uh",
      "offset": 3754.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "personal projects. I made a promise at",
      "offset": 3757.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the start of this course that I will be",
      "offset": 3759.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "using models as well and this is where",
      "offset": 3760.96,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "it comes in models our sixth and final",
      "offset": 3763.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "agent for this course. For this agent",
      "offset": 3767.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "we're going to explore models. So we're",
      "offset": 3769.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "going to integrate Olama models into",
      "offset": 3772,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "langraph and discuss the pros and cons",
      "offset": 3773.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "of Olama models. So for running these",
      "offset": 3776.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Olama models, I will run these locally,",
      "offset": 3779.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "not in this data lab file, but in my own",
      "offset": 3781.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "local file. So let's jump into that.",
      "offset": 3783.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Okay people. So now I'm on VS Code and",
      "offset": 3786.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I'm going to be running my Olabo models",
      "offset": 3788.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "through here. In particular, I'm going",
      "offset": 3790.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to be using Quen 2.5. I personally like",
      "offset": 3792.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the Quen model series. I think they're",
      "offset": 3794.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "great, but you can use any model. You",
      "offset": 3796.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "can download any models. You can",
      "offset": 3798.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "download Deep Seeks. you can download",
      "offset": 3799.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "llama etc. In this particular case, I",
      "offset": 3801.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "want to compare the performance of",
      "offset": 3804.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "OpenAI and O Lama models and really show",
      "offset": 3806.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you this comparison and also show you",
      "offset": 3809.119,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "why I used OpenAI models throughout this",
      "offset": 3811.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "tutorial rather than Olama. So the only",
      "offset": 3815.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "difference of imports is this one line",
      "offset": 3817.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of code and it's quite self-explanatory.",
      "offset": 3819.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "It's just chat oama very similar to",
      "offset": 3821.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "OpenAI. It just allows us to use Olama",
      "offset": 3823.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "models. In terms of the tool, this is",
      "offset": 3825.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "our own custom tool which we've already",
      "offset": 3827.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "created before. It's the weather tool",
      "offset": 3829.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and this is the duck.go search run uh",
      "offset": 3831.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "tool which is the inbuilt tool and now",
      "offset": 3833.839,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "we pass this to the uh both open AAI",
      "offset": 3836.559,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "agent uh and the OAMA agent which is",
      "offset": 3839.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "over here.",
      "offset": 3843.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Uh in terms of the prompt I have set the",
      "offset": 3845.119,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "exact same prompt in um for both in this",
      "offset": 3848,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "case it will be you are my AI assistant",
      "offset": 3851.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that has access to certain tools uh use",
      "offset": 3853.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the tools to help me with my task.",
      "offset": 3855.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "That's the uh prompt, the system",
      "offset": 3857.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "message. And in terms of my query, I've",
      "offset": 3858.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "also said it the same. So I've asked",
      "offset": 3861.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "both uh LLMs or both AI agents, what is",
      "offset": 3864,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the weather in San Francisco and tell me",
      "offset": 3866.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the latest news about the stock of Apple",
      "offset": 3869.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of today and the date today please date.",
      "offset": 3870.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I've also asked it one more thing which",
      "offset": 3874.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we haven't actually asked before which",
      "offset": 3876.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "is also tell me a list of tools you have",
      "offset": 3878.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "please. So the correct answer should be",
      "offset": 3880.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "uh that it has access to these two",
      "offset": 3883.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tools, right? Okay, so let's see how",
      "offset": 3885.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "they perform. So again, it's the same",
      "offset": 3887.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "create react agent. I'm just going to",
      "offset": 3891.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "show you the uh the comparison between",
      "offset": 3892.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the performance of Olama and OpenAI. So",
      "offset": 3894.799,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "I've already ran this code once and",
      "offset": 3897.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "here's what the results are. So this was",
      "offset": 3899.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "OpenAI Asians results. It used the",
      "offset": 3901.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "following tools with this respective",
      "offset": 3904.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "arguments and the final answer was the",
      "offset": 3906.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "weather in San Francisco was sunny. Uh",
      "offset": 3908.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the latest news about Apple stock is",
      "offset": 3910.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "this is this and it gives detail",
      "offset": 3912.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "regarding the date. Today is June 12th",
      "offset": 3914.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "which we also asked for. Remember we",
      "offset": 3916.559,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "asked for what the date is uh here. Uh",
      "offset": 3918.64,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "and the date today please.",
      "offset": 3922.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "And if we go back and it also said as",
      "offset": 3925.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for the list of tools I have they",
      "offset": 3928.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "include functions weather to get the",
      "offset": 3930.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "weather in a given city and duck.go",
      "offset": 3932.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "search to perform a search query using",
      "offset": 3934.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "duck.go. Uh and that is on obviously",
      "offset": 3936.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "like fully correct right. I don't really",
      "offset": 3940,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "have any problem with it. It used the",
      "offset": 3941.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "right number of tools. It answered every",
      "offset": 3943.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "single aspect of my query properly. Now",
      "offset": 3945.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "look at the Olama uh result. So if I",
      "offset": 3947.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "press enter a few times here, perfect.",
      "offset": 3951.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "You should be able to see that. Okay.",
      "offset": 3953.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Over here it's used the weather tool",
      "offset": 3955.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which is correct. It said latest news",
      "offset": 3957.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "about uh stock today which is 12th of",
      "offset": 3959.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "June 2025 which is also correct. But",
      "offset": 3962,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "then it started looking for date today",
      "offset": 3964.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "please 12th June which doesn't actually",
      "offset": 3966.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "make any sense. And then if we look at",
      "offset": 3968.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the final answer, it says the latest",
      "offset": 3971.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "news in the latest with the weather in",
      "offset": 3973.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "San Francisco is sunny. The latest news",
      "offset": 3975.68,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "is this. And then look here, it says the",
      "offset": 3978.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "today's date is Tuesday, June 10th,",
      "offset": 3981.039,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "2025,",
      "offset": 3983.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "which is wrong even though it correctly",
      "offset": 3985.599,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "uh mentioned uh what the actual date is",
      "offset": 3988.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "12th June correctly several times",
      "offset": 3991.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "before, but it did say uh it did",
      "offset": 3993.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "correctly give the list of tools uh",
      "offset": 3995.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "properly. It said weather inductor go",
      "offset": 3998.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "search. So there's obviously clear flaws",
      "offset": 4000.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "with the Olama model specifically quen",
      "offset": 4003.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "2.5. In particular I want to show you",
      "offset": 4005.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this piece of uh leaderboard. So this",
      "offset": 4007.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "leaderboard is essentially in a nutshell",
      "offset": 4010.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "a way to classify how good a particular",
      "offset": 4013.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "model is when it comes to tool calling.",
      "offset": 4016.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So if you can see that GPD 40 ranks",
      "offset": 4020,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "quite high. All of the GPD models rank",
      "offset": 4022.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "quite relatively quite high. If I search",
      "offset": 4025.119,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "up for quen 2.5 quen 2.5",
      "offset": 4027.359,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "you can see they place this is the 72",
      "offset": 4032.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "billion one 32 billion one 72 billion 32",
      "offset": 4034.48,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "14 14 77 73 so in my case I was using",
      "offset": 4037.76,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "either the 3B the 7B or the 3B one of",
      "offset": 4042.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "these two but you can see look at the",
      "offset": 4045.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "ranks 66 and 81st compared to GPT's one",
      "offset": 4047.2,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "which was quite highly ranked uh this is",
      "offset": 4051.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "this is also in the data lab file if you",
      "offset": 4054.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "access to this. This is the URL as well.",
      "offset": 4056.64,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "But let's go back to the uh model uh",
      "offset": 4059.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "model results. The point I'm trying to",
      "offset": 4063.599,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "say state is this. I would personally",
      "offset": 4065.599,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "use OpenAI models or Gemini models or uh",
      "offset": 4069.039,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "these APIs even though they are paid. If",
      "offset": 4072.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you want some robust tool calling, I",
      "offset": 4075.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "would highly recommend those unless if",
      "offset": 4077.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "privacy is an issue. If privacy is an",
      "offset": 4080.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "issue, then obviously you need to go for",
      "offset": 4082.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "models, right? You could also make a",
      "offset": 4084.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "counterargument here that you can't",
      "offset": 4086.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "really compare GPT40 which I used here",
      "offset": 4088.799,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "versus quen 2.5 which is like in few",
      "offset": 4092.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "billion of parameters versus a trillion",
      "offset": 4095.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "parameter model and that is fair but the",
      "offset": 4097.679,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "counterargument for that is that your uh",
      "offset": 4100.4,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "models are dependent on your computer",
      "offset": 4103.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "specification right if you could run",
      "offset": 4105.839,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "very very large model like deepsek 650",
      "offset": 4108.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "billion parameter model then obviously",
      "offset": 4110.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the performance would be quite similar",
      "offset": 4112.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "but to be able to run those you need so",
      "offset": 4114.319,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "much compute and at that rate if you if",
      "offset": 4116.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "privacy isn't a concern and you want",
      "offset": 4119.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "something which is relatively",
      "offset": 4121.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "inexpensive and still robust I would",
      "offset": 4123.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "highly recommend um either OpenAI models",
      "offset": 4125.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Gemini models um these sort of models uh",
      "offset": 4128.08,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "cuz you can see from the results they",
      "offset": 4131.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "are quite a lot more robust and that",
      "offset": 4133.759,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "brings us to the end of this tutorial",
      "offset": 4136.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and well done for completing this",
      "offset": 4138.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "tutorial we've learned so much in 1 hour",
      "offset": 4140,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "uh to quickly summarize what we've",
      "offset": 4142.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "learned We learned how to create our own",
      "offset": 4144.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "custom tools. We learned what React",
      "offset": 4146.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "agents are. We learned how to use uh",
      "offset": 4148.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "inbuilt tools. We learned how to uh",
      "offset": 4150.319,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "create React agents. We learned what all",
      "offset": 4153.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the different type annotations are, the",
      "offset": 4155.199,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "different types of messages are, the",
      "offset": 4157.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "different types of uh fundamental",
      "offset": 4159.359,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "building blocks are such as conditional",
      "offset": 4161.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "edges, tools, etc. We built six agents",
      "offset": 4162.719,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "in together. We showed how to visualize",
      "offset": 4165.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "uh agents. Uh we we showed how to",
      "offset": 4168.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "visualize graphs. We learned a ton of",
      "offset": 4170.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "stuff. So massive massive",
      "offset": 4172.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "congratulations for completing this. But",
      "offset": 4174.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "remember practice makes perfect. So now",
      "offset": 4176.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "go ahead and actually build these AI",
      "offset": 4178.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "agents and graphs by yourself. Uh try to",
      "offset": 4180.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "build any AI agent. Try to solve your",
      "offset": 4183.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "own problem because remember practicing",
      "offset": 4185.359,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is how you're going to actually solidify",
      "offset": 4187.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "your fundamentals and understanding. If",
      "offset": 4190.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you have any further questions or want",
      "offset": 4192.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to state your opinion or anything, you",
      "offset": 4194.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "can write them in the comments in uh",
      "offset": 4196.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "under this video. With that being said,",
      "offset": 4198,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that brings us to the end of this",
      "offset": 4200.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "tutorial and I'll see you in another",
      "offset": 4201.44,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "tutorial. Goodbye.",
      "offset": 4203.36,
      "duration": 3.96
    }
  ],
  "cleanText": "What's up everybody? Adele here. Today we're going to get our hands dirty with LangGraph, a framework that lets you define AI agents as graphs and nodes. We're going to learn how to build AI agents that can think, reason, and act with tools. Our instructor, VBA, will walk you exactly how to build your AI agents using LangGraph. So, make sure to stay tuned for that. Now, if you want to jump directly to the coding section, make sure to check out the timestamp below. And make sure to check out the data lab notebook that's accompanied in the description so you can code along with the video. If you want to take your skills to the next level and learn how to build multi-agent systems using LangGraph, make sure to check out the course that you can see in the description here and learn how to build an AI agent that can take data from Wikipedia, analyze stock data, and provide you with sound analysis on the performance of a particular stock. Now with that said, let's get started.\n Hey guys, welcome to Datacamp. In this tutorial, I'm going to be teaching you about the fundamentals of LangGraph. Now this is a beginner level tutorial. So I assume you know little to no knowledge in LangGraph. From this tutorial, we're going to go right from the basics and build ourselves up so that by the end of it, you can build almost any AI agent out there. Now, if that sounds good to you, let's begin with this tutorial. So I'm going to ask you two simple questions. What is the difference between an LLM and an AI agent? You can think of an LLM as a model that understands and generates humanlike text based on the patterns it received in its training data. An AI agent, on the other hand, it uses this LLM to reason, plan, and use tools and overall take the best respective actions over time. Now, I've put the word tools in bold yet because it will be coming up quite often soon. My next question is this. What even is LangGraph? LangGraph is a framework that lets us build AI agents as graphs where each node does a task and edges decide what happens next. Now, don't worry if you don't know what a node and an edge is. We will discuss them shortly. You can think of it like having a flowchart which I've shown here. This flowchart basically represents an AI agent and it shows how it thinks, how it acts, and all of the internal decision-making. For example, it has the start and the end point here and then you have the input query. So the input query could be from me and I can say give me this this this and then it gets passed to the tool selector part. Its job is to be able to select well the correct tool. For example, if I ask it to do a very simple query like what is 2 + 3? It should be able to pick the maths tool not the web search tool because well why would you search what the answer for 2 plus 3 is right? So the tool selector is quite an important part of an AI agent. Then based on whatever tool it picks, it then outputs the result and then well the query finishes, right? Hopefully that flowchart analogy helps you to understand what an AI agent is. What is this tutorial's overall aim? It really is to be able to understand and master the architecture of LangGraph. If you can understand and master the architecture, well, you can build any AI agent in from LangGraph, right? So let's begin by pip installing all of the libraries we'll need. So for this tutorial you need to install lang graph, langchain core, lang openAI, DuckDuckGo search, LangChain community, and you also will need to install models. The ammo models I will discuss in the end of the tutorial. For the majority of this tutorial I'm going to be using OpenAI and you can get its OpenAI API key through their website. It's very very cheap. So don't worry it will be in the matter of pennies or cents. So the price is very cheap. So don't worry about that. So let's go ahead and pip install all of this. And perfect. Let's get started with coding right away. I want to build this very simple agent. Agent one. The aim of this very simple agent is this. I want to be able to teach you what states, edges, nodes, graph, and state graph is. I also want to teach you about type dictionary. Now that's a type annotation which we're going to learn about very shortly. Third thing is how do we even integrate an LLM using LangGraph, specifically again the OpenAI model. Now please note you can use any model you want. You can use the Gemini model, you can use Anthropic's model. I personally use OpenAI model because I've seen their performance is quite good and also because well they're quite cheap as well. Obviously they're not free completely. That's why I've also used Ollama at the end of this tutorial. But all in all, OpenAI is a good choice. Now, I also want to teach you about the basic syntax of LangGraph. Let's build our intuition on the fundamental building blocks in LangGraph. And I'm going to be using a game analogy for this. So, let's start off with the first fundamental building block, which is the state. Now think of the state as a data structure that is shared and it holds the current information or context of the entire application. So in terms of a game analogy you can think of it as a file in a game which saves everything which you have in the game such as your health, your score, your weapons, you get the point. So it basically stores everything the game needs to remember. Hence this asset which I've drawn that is where state is. Now let's move to a node. A node is a self-contained function or a step and it carries out really specific tasks within the graph. For example, in a game analogy, it could be a particular task or a particular scene like a battle scene or a puzzle scene or a cutscene that happens when you reach a particular point on the map. Hence this puzzle piece. Now an edge remember edge and node was something we just briefly talked about and mentioned here. But an edge it's defined as the how the process moves from one node to another. Remember in a graph you have nodes and to connect these nodes you have edges. You have edges and conditional edges. Conditional edges are something we will be covering later. But remember you need something to connect these nodes or functions together right and that is what an edge is and it controls the direction and logic of execution. You can think of these as a door or a road or choices that connect scenes. Essentially they decide where the game goes next based on what has already happened. The next thing is well graph LangGraph the most probably the most important fundamental building block in LangGraph. So, it's the main structure that defines how tasks aka nodes are linked together and carried out. Like I just said, edges and nodes make up a graph. The game analogy for this is the complete overall design showing all the levels, all the possible paths, and all the decision points. Kind of like how a full map of how a game unfolds. The last thing is a state graph. Now a state graph essentially think of it as merging the state and merging the graph together. So it builds and manages the entire graph. It has the nodes, it has the edges, it has the shared state and its main job is to ensure the smooth data flow and coordinated execution. In terms of the game analogy again, it's like the current live game you're playing. It's keeping track of your location, your inventory, your choices, and progress on the map. Remember, it is very, very similar to state and graph. It's like the fusion. Now, if you didn't understand this, don't worry. We will consolidate this using code. Let's start off with this. So, this is all of the imports we will need for creating our very first AI agent, the simple agent. So, you can see I've done from typing import type dictionary. And this is the add data annotation which we're about to cover. Then we have LangChain core messages import human message. And a human message is a message type which represents the input from a user. So back into the flowchart analogy where I said input query. The input is again a human message. Now there's loads of types of messages in LangGraph. For example, there is an AI message, human message, tool message. We will cover them all shortly. Don't worry. Back to the imports though. We have LangChain OpenAI, import chat OpenAI. And well, you've guessed it. This is to import the LLM or use the LLM. And the last point is from LangGraph.graph import state graph start and end. Essentially, this will help us build the overall graph structure. So, this is obviously quite important. Now, back to this type dictionary. If you don't know what a type dictionary, it's essentially a data annotation. And it's like a dictionary but type the data type is quite important to define a type dictionary in Python. You use it like this. So you define a class and then obviously the name of the class and then you assign type dictionary as a superclass. Now within this type dictionary I have three keys. I have the name, the age and if this person is a student or not. So for the name well it can't be an integer right? How many people do you know who's called like one or two for example they are a string for example the age also needs to be an integer and in if it's is student it can either be true or false right it can't be something in between to create our own type dictionary or an instance of this type dictionary we use this syntax we use curly brackets we define the key with the respective value but because we've used a type dictionary Python will automatically throw an error if it's the wrong data type. Now, we could have obviously used a simple dictionary and that's fine, but in industry or in very very large applications, type annotations are quite a huge problem. Data types in general because well, data types cause a lot of logic errors. And to evade all of those logic errors, that's why type dictionaries are used. It just makes everything more robust. And in LangGraph, robustness is everything. So, that's why we use type dictionary here. So now that you know what a type dictionary is, oh and we've also printed the person and you can see the printed statement is exactly what we had initialized here instantiated. We're now going to define the state of our agent. So to define a state you use again the type dictionary what we just used here and we use the same syntax. So we define a class the name of the state. Now the name of the state can be absolutely anything. I've just kept it as agent state here because well it's easier to follow that's why but you could name it whatever you want you can name it state you could name it Bob whatever you want and then obviously you pass in type dictionary because it acts as the superclass now for this agent what did we want let's go back to the aims so the aim was here how to integrate an LLM and understand the basic syntax of LangGraph so essentially what I want is I want to be able to say something and our AI agent should be able to reply it.\nSo realistically the only thing which I need to store is my own input message. And what will the input message be? Well, we just spoke about that it will be in the form of a human message and that's why we have created a key which is user message and it will be the type human message. Remember human message, AI message, all of these message types in LangGraph are data types and that's why I referred to it as human message like this syntax.\nSo remember this is quite a very very basic AI agent that's why I only have one key but later on we will have multiple keys. So now we define an LLM in LangGraph and we use chat OpenAI. Now if you know LangChain or know a little bit about LangChain this is exactly how you define an LLM in LangChain as well. Remember LangGraph is built on LangChain. They're built by the same sort of developers. So a lot of the tools and everything which we use in LangChain have also been implemented in LangGraph. It just makes things much more compatible and easier. Right? So for this example I'm going to be using the GPT-4o mini. You can use any open AI model. You can use any LLM model if you want. You just need to change this. You can have chat Gemini, chat perplexity, chat anthropic whatever. You can refer to the LangChain LangGraph documentation and LangChain documentation. We will talk about the documentation in a bit. But going back to this example, I've also set this attribute here which is temperature. Now temperature is an attribute which controls the creativity of our AI agent. So it's between 0 and one for open AI models. If it's towards one, it means the LLM will be more creative. And if it's towards zero, well, it would be more it will be less stochastic. So I've kept it at 0.7 because I want a little bit more creativity than 0.5. So this is arbitrary. You can set whatever you want. Uh but that's not really important. I've just kept it here just to show you that chat OpenAI has a lot of attributes which we can tweak. It can you can change the top K parameters, top P parameters. If you don't know what that is, that is completely fine. The most important thing which you need to know is that this you need to set the model here. In terms of the API key, by the way, I've set it as an environment variable. If you using this locally, you can store it in a .env file and then you can uh import it using the from.env load.env uh syntax. I'm using data lab and it just makes things easier because everything's in the environment. So that is how you create the agent state and how you initialize a large language model. Now I'm going to be creating a node. So the purpose of this node is to pass the user's message to the LLM. So to be able to do this I define a function. So it will take in the state which will be agent state which is the state of our agent which we just uh created and it will output the exact same agent state. Remember a node has to be able to in uh has to be able to intake a state and then output the updated state. The state in any graph is the most important thing in there. If you don't have the state well your entire robustness completely falls apart. So remember a rule of thumb is always always always always output the agent state. Now this is Python 3 uh syntax but this really just shows the reader that you're inputting an agent state and you're outputting the same state or the updated state. So within this function you can see that I have this line of code. It's saying it's using llm.invoke state user message. Now what does this line even mean? Well llm.invoke Invoke is a really fancy way of just saying of you of running this LLM\n\n\nSo, whatever I pass here, it will get passed to this LLM or the chat OpenAI model, which we defined the state user message. This will contain our query. Now, I haven't passed any query yet, but overall, this line is essentially saying that whatever I have that gets passed to the LLM using this invoke method, and the invoke method is an inbuilt method. And then this is just me printing what the response is. Now, the reason I haven't just printed response by itself, and I've used response.content is because if you play around with this and you just print response, well, what happens is you get a lot of unnecessary metadata. For example, you get the number of input tokens, the number of output tokens, you get a lot of unnecessary stuff. So, if you just want the clean AI output result, you can just use response.content. And finally, I'm returning the state because that is what my function wants. I want to return the updated state. In this case, I'm not updating the state at all. But overall, this node is really just to pass the user's message to the LLM. It's really dead simple, but it shows you how we create the underlying function behind a node. So, now that we've covered the stuff, let's actually build the graph itself. So, how do we do this? Well, you start off by saying graph is equal to state graph agent state. And this state graph was, remember, we just covered it previously here. It builds and manages the entire graph. We have to pass in what the shared state is. In this case, the shared state is this, the agent state, which we had just defined. Remember, the state is really just, it's a shared data structure which holds record of everything you want. In this case, it's just storing the human message. But in more complicated AI agents which we're going to build, it will store a lot more stuff. Now, how do we create a node and an edge? This is the underlying function of the node. So, if I go to this line, it says graph.add node. And I have two parameters here: node one and first node. So, this part is the name of the node of the graph. And then the other part is the name of the underlying function, aka action. This is the underlying function, and this overall now is the node along with its name. Then we also to create an edge, we use graph.add edge, and we use the start and the end point. Start and end point, well, they're quite self-explanatory, right? But an edge has a start point and an endpoint itself. So we want our graph to look something like this. We want it to start. We want it to be passed to our node, which was this. And then we want it to end. These two lines of code is an in-built function in LangGraph where you can where you're able to visualize your graphs quite well. You don't need to use any external Python libraries. You can just use this using IPython. So it's really a great visualization tool in case you're messed up somewhere. A rule of thumb here though is this. If the graph does compile fully and you get a result and it's it looks all fine, chances are you might have messed up internally, and it doesn't always show you the internal mechanics. So that's just a heads up. Now going back to here, I created another edge, which is this edge here. It goes from node one and it ends. So to summarize everything again, we create the add node, which is this node. This node's underlying function is this. And the this edge is from this start point to node one. And this edge is from node one to end point. Lastly, I just compile the entire graph and I store it in an agent variable. Now, another heads up is this. Notice how I written agent.getgraphth dot draw mermaid png so on, and I've not written graph. The reason I've done agent instead of graph is because if I did graph, it wouldn't work. Why wouldn't it work is because graph isn't compiled yet. I need to be able to send to this function the compiled version of the graph, and the compiled version of the graph is well, Asian. That's why we did graph to compile. So that's how you define a very, very simple graph. Now let's actually test it. So let me just make sure everything is running. 2, 3. Perfect. Now let's run this. Okay. So this function is an infinite loop where I input something, it um, unless if it's exit, it just keeps going. So I can say hi there.\nThere you go. The AI responds, how can I assist you today? Now if you don't believe me, I can write something like who made you, and it should say something like OpenAI. Right there, we go. I was created by OpenAI, an AI research organization.\nNow notice, and I can also obviously exit, but I'm now about to do something here. So I'm going to say, uh, my name is Vav.\nSo nice to meet you, Vav. How can I assist you today? So where's the problem? The problem is this. What is my name? Do you think you will be able to recall my name?\nI'm sorry, but I don't have access to personal information, blah, blah, blah. So let me just exit. Perfect. That's exited. So now you can clearly see what the problem is, right? It doesn't recall the previous messages. Why? Well, it's because we never created our state that way. All we did in our state was to store the human message. And the human message, for example, was hi there or who made you or my name is Vav or what is my name? These queries were the human messages. I send these messages. That's why they're human messages, right? So this is quite a big problem, right? So how can we fix this? Well, let's move to the next section, which is this. So we have a problem. Our agent doesn't have any recollection of what the previous message was. I.e. it doesn't have a context. Context is written in bold here because that's the fancy way of saying that it doesn't have any prior knowledge. This is because each time we're running the this program, it's a separate API call. So what happens is when I send a request, the AI model or the open air from the OpenAI server, it gives me the request back. But that's it. Then I ask it again. If I go back to here, I say hi there, it sends a response, which is hello, how can I assist you today? That's the end of the conversation. If I ask who made you, that's a new API request. It doesn't recall anything from previously. So we need to somehow integrate memory into this. Now luckily, there is a lot of different ways to be able to integrate memory into LangGraph agents. And these are some of the multiple ways you can do it. For example, you can combine with the current request with previous context. You can save it in a text file or in any external file. It could be a CSV, a JSON, whatever. You can use inbuilt LangChain memory tools like checkpointter memory, for example. You can use RAG for larger data storage, or you can use external memory tools such as Mem Zero. These tools are also getting quite popular, but you'll need to understand the basic documentation of different libraries. Mem is a different library. There's multiple different such libraries. The same goes for RAG. You can use, for example, Pinecone, Chroma database. There's many, but you need to understand slight fundamentals of it. If you just want to stick to LangGraph, you can use checkpoint memory. These are all slightly beyond the scope of this tutorial. I'm going to still integrate memory somehow, and that is using the first option. The second option is also quite easy. You can just use a simple text file, but the first one is the easiest, and this moves this brings us to the next part, which is agent 2. And this agent will essentially be our simple agent, but infused with context. So agent with context feeding. So it will have some form of basic memory. Now the aim of this agent is to understand the different messages in Lang. Remember, I told you there were loads of different messages, not just human message, there's AI message, tool message, system message, a lot of different messages coming up. Then we're also going to understand the union type annotation, and we're also going to feed context back into the LLM, aka the whole overall aim of this agent. These are all the imports we're going to need this time around. We're going to import OS. We're going to from the typing library import type dictionary, list, and union. We've covered what a type dictionary is. We know what a list is. We don't know what a union is. Uh, before I explain that, there's also these two imports which we've already covered, and this new one. We've we already know what a human message, but what is an AI message and a system message? Well, an AI message is well, the response of the AI model. In this case, the AI message was this. Now, why didn't I use AI message in the state before? Well, because we didn't need to store it. We never stored it. We just wanted to store the human message, and that's it. But now, we're going to also store the AI message. And then there's also this system message. This one, the system message is essentially telling our large language model to be able to it it gives instructions to it. If you don't want to give instructions via the human message and you just want it to be infused, we use a system message. Uh, don't worry if you didn't understand that, it will make much more sense when we actually code this up. But back to this union type annotation. So union is a type annotation that allows for a variable to be more than one data type. For example, in this very, very dead simple function which I created, it uses a a variable called value, and I used a union type annotation, and it allows this variable value to be either an integer or a float. So like I said, uh, this variable value can only be an integer or float. So it can't be anything like a string or a boolean or any custom data type I make, only these two. So hopefully you now understand what a union is. So if we go back to our code, we create a new state class agent state type dictionary again, and we again only have one single key messages. Last time we had user message, but this time we're storing every single message. We've stored this as a union. This could be human message, AI message, and system message. These are the three message types we'll be covering in this agent. And we're going to be storing this in a list format. So again, I've used the same LLM and defined it here. Uh, we've mentioned what model is. We've mentioned what a temperature is. And now I'm going to be I'm going to be creating this variable. And this is called a conversation history. So this is where the system message comes in. So it says, you are an AI assistant that speaks like a pirate. Answer all of my questions properly. Now the reason I've said speak like a pirate is really to show you the value of system message. The system message is slightly different to the human message in the sense that the system message is purely just instructions which the AI agent has to follow every single time. But whereas a human message, well, that's dependent on me, right? In this case, I want the AI agent to be able to speak like a pirate every single time it answers back. So it will craft its AI message in that way, in this case, like a pirate. So we're now going to define another node, in this case, our processing node. So this node, uh, this underlying function of the node is slightly more complicated in the sense that although we've sent the response again, this time we're not sending just the human message, we're sending the her message, the system message, the AI message, everything. Why? Because well, in our state, we've defined it like that. We've defined it as a list of either human message, AI message, or system message, right? All of that is going to get passed into our LLM. Now we update this state, and this is where the context or the memory comes in because of this line. It appends this state, and that allows for the AI agent to essentially recall what's happened in the previous queries in the conversation. So although we're still sending separate API requests, it kind of it will act like it know it's like a flowing conversation. Now obviously, there are pros and cons to this, which we'll discuss in just a bit. But going back to the code, this again, I print the response or content, and I finally return the updated state. Now this time, remember, we've also updated the state because of this line, because we've appended it, and that's why I had also created this as a list to be able to append our new new state. If we go to this line now, again, it's the exact same syntax which was in the simple agent. The overall graph structure hasn't changed at all. You can see that we have the same start point. We have an end point. I have changed the name of the function to help you get familiar with the syntax. The add note, I can save this as whatever I want. I can name this as another new node or something. And this is the underlying function, which is our processing node, right? And again, we have the start and end point. But a very, very common mistake is this in a LangGraph, which is we mess up the names of our nodes. So that's why I like to keep these nodes quite simple and intuitive. Don't try to name it something like completely out of the blue. Try to name it like something intuitive, something easy to remember, and something which makes sense. In this case, LLM node does make sense. So that's why I've kept it. And let's now run this. So we can run this, run that, that. And we're back. Perfect. So let's try this simple agent. So now I ask it, hi, who are you?\nSo what do you think it will output this time? Think about it.\nOkay. So, obviously you're not going to guess it word by word, but I hope you said something like it will give us an answer in a pirate themed manner. And again, it does because we set the system message or instructions here.\nIf we go here, it says, \"Ahoy matey, I be your trusty AI assistant here to help you.\" Whatever, whatever. Okay. Now let me remove this break line and let's run this again. So I'll again ask it hi.\nPerfect. Now I'm going to ask say my name again. So my name is Okay. Perfect. Still all good. And now let me ask it what is my name? If I can spell.\nPerfect. Ye be called Va Savvi. It still speaks in a pythine manner, and it still is able to remember what I just asked him. Why? Because every single time in our node, we are appending the update. We're appending the new state. So we're getting an updated state. I can also print the current\n\n\nHow the current state looks like to be able to give you a better intuition.\nSo if I stop this.\nOkay, that's just a keyboard interrupt.\nDon't worry about that.\nLet's run this again.\nLet me actually break this so it keeps everything simple.\nActually, let's do this.\nStop and run.\nOkay, now look at how the state gets updated.\nSo again, I'm going to write hi ao mi whatever.\nBut our current state looks like this.\nThis new print statement was what I've just added here.\nAnd this is again to show you how the state changes.\nSo if you look here, uh, it says uh the system message, which is you are an AI assistant, then the human message, which is hi, and then it also says the AI message.\nAnd why does it store all of these three message types?\nWell, it's because we had defined them here.\nOkay.\nSo hopefully everything is like piecing together now.\nAnd now look at how we change the how the state gets updated.\nSo again, I'll write uh I can write something like who are you?\nOkay, it's now given me the new response, but now look at how the current state is.\nThe state is content, you are an AI assistant, that stays the same.\nThen we have the human message, which is hi, which was the first message we we've ever written.\nAnd then we have the its respective uh AI message, which is here, and then we have the human message again, the new human message, which is who are you, which I just asked it, and then this new AI response is right here.\nAnd also one more thing, remember uh earlier I had said we do response.content.\nContent is because look at all this unnecessary metadata.\nFor example, this additional keyword arguments, response metadata.\nIt can also sometimes output the input tokens, output tokens, which we just don't want.\nWe want a clean content, and that's why we just use the content uh method response.content.\nHopefully, you started to understand how this state works and why it gets updated like this.\nTo summarize, it stores, in this case, it stores the human message, the AI message, and the system message, and every time we've run it again or we ask a query, it gets appended.\nWhy it it gets appended?\nBecause we uh wrote append, right?\nAnd it gets stored in a list format, and it can take either a human message, AI message, or system message, and that's why because we're appending it, it gets sent to the uh back of the list, so we can now stop.\nBut cool.\nThat's a keyword argument.\nPerfect.\nBut but I had said there are pros and cons to this agent.\nSo we have a problem.\nAs our conversation increases in size, we will use more input tokens.\nAnd because we are using a paid model, an OpenAI model, it means more cost per API call gradually.\nSo for example, right now we are working in like tenth of a cent, but gradually as a conversation builds, builds, builds, we might be at the point where we're sending thousands and thousands and thousands of tokens every single API call, and that keeps getting appended because we haven't set any hard limit.\nSo the simplest, simplest way to reduce the cost if you're using this memory uh this way of memory is to set a restriction of the number of messages you store.\nSo you could limit it to the last five messages.\nThe reason I've said last five messages is because of recency bias, because you're more likely to be sending new or the more relevant important information in the previous end number of messages.\nThe number five I've just set it as arbitrary.\nYou can set it as seven.\nYou can set it as 10.\nYou can set whatever you want if you go with this technique.\nBut yeah, this is a very, very, very easy way of being able to create uh a form of memory for your AI agent with very, very limited number of code lines.\nThat's the second agent done.\nSo well done.\nWe're making good progress.\nBut now it's time to take our AI agents to a whole new level.\nRemember at the start where I asked you the two questions here, I had put tools as uh in bold.\nNow we actually talk about it.\nAgent three is agent with inbuilt tools.\nNow there's two types of tools in LangGraph.\nThere's the in-built tools and there are your own custom tools.\nFor this agent, we will be using the in-built tools and I want to talk about them because they are important and they can save you a lot of time as well.\nSo until now what we've built is really just the LLM, right?\nThere's no clear distinction between an LLM and an AI agent.\nEverything which we've done so far, well, you could have just asked an LLM simply to do without needing the overall the overhanging LangGraph code, right?\nBut now, here's where LangGraph gets really like useful.\nBut so now we're going to upgrade.\nSo we're going to be creating an AI agent, a true AI agent this time, which has access to different tools.\nSo the aim is to be able to understand what tools and conditional edges are, to be able to understand what the base message is, to be able to understand the annotated type annotation, a new type annotation which we're going to learn, and also how to access inbuilt tools in LangGraph.\nOkay, so let's get started with what tools and conditional edges are.\nAnd these are the last two fundamental building blocks which you'll need.\nAnd if you master these five uh building blocks here, state, nodes, edges, graph, state graph, and then these last two which are tools and conditional edge, you can really build any robust AI agent out there.\nAlmost any AI agent out there using LangGraph only if you are able to master these uh certain tools uh certain fundamental building blocks.\nOkay.\nSo what is a tool?\nThink of a tool as a specialized function that the agent can utilize to perform specific tasks.\nIt could be to fetch data from an API.\nFor example, from a weather API, it can uh get the weather and that would be your weather tool.\nSo going back to the game analogy, it could be something the player aka our agent can use like a weapon or a healing potion which I've created here to perform a specific action.\nThis is very important.\nThe aim of a tool is to be able to allow our LLM to be able to perform a specific action.\nSo what is a conditional edge?\nSo these are specialized connections that decide the next node to execute based on the specific conditions or logic applied to the current state.\nDamn, that's a mouthful, right?\nSo a good analogy for this is going back to a video game analogy and it's thinking of like a path that only opens if a condition is met.\nKind of like an if else statement.\nWe're going to understand these two in more depth right here.\nSo these are all the imports which we're going to need.\nAnd it's a lot more import than last time, right?\nReally, we've only added this this line which is annotated.\nThen we've used from langchain.cord on messages.\nAnd we've imported all of these new stuff, particularly base message.\nWe're going to talk about what a base message is soon.\nBut going to the imports, we already know chat openai.\nAnd then there's this new line of code from langgraph.graph message import add messages.\nSo this is called a reducer function.\nYou don't need to know what a reducer function is in depth, but think of a reducer function as basically being able to add your messages without them being overwritten.\nThat's a very, very simple way to put what add messages reducer function does.\nWe will code this up in just a bit.\nBut there's also these two.\nSo the DuckDuckGo search run is a inbuilt tool uh which LangChain has.\nWe're going to be talking about where you can get access to these tools.\nAnd then there's this tool node which is essentially just the fusion of a tool and a node.\nKind of like how the state graph was the fusion of the state and the graph.\nSo the annotated is a type annotation.\nSo it allows for a variable to be more than one data type.\nNow it's very similar to what a union was, right?\nSo it allows for uh for in example in this case it's an integer and an age which which is x.\nAnd I've set it to as 20.\nSo it basically allows us to provide extra information about the type of value.\nIn this case, we're providing age as extra information about the variable x.\nNow sequence is another type annotation which allows a variable to be any ordered collection.\nSo it could be like a list or a pupil or a string or whatever.\nThere's a slight difference between what a list and a sequence is.\nYou don't need to know about this in depth.\nThe reason I've included sequence here is because we've used that in the code.\nSo if we go to here from typing import sequence, uh it's really similar to what a list is.\nUh but there's a slight difference which is a sequence is a read only access to any order data type.\nBut a list is a mutable ordered collection.\nSo a sequence is only used when you only need to read the data, not modify it.\nAnd a list is you can add, remove or change items in it.\nThat's why we've used a reducer function for the sequence which you're going to see in a minute.\nNow, what is a base message?\nRemember, we just imported what what the base message was uh right here.\nSo, what exactly is it?\nThink of the base message as the parent class of all of the different types of messages such as the human message, AI message, system message.\nNow there's so many different message types in LangGraph and LangChain that it could be quite cumbersome to write all of them and to make sure we've got all of them.\nSo we can just simply put base message.\nSo again think of base message as the parent class and human message AI message as the children classes which inherit all the properties of the base message and also have their own uh properties respectively.\nRight?\nSo that is what a base message is.\nNow going back to the inbuilt tools in LangGraph and LangGraph have a lot of inbuilt tools.\nSo this is where the documentation is.\nSo if I follow the link, we come to this.\nNow this is a really well-written documentation and they have a lot of tools um inbuilt already.\nFor example, there's searches, there's for code interpreters, there's for productivity, web browsing, databases, etc., etc.\nYou get the point.\nThere's a lot of stuff here.\nSo you can even click on anything.\nFor example, if we go to search and duck.go, which is what we're going to be using for this agent, you can see all of the uh documentation and it's very very very well organized.\nSo going back to this, we I want to show you how we can use this duck.go search run.\nSo for example, I've asked it a query who won the Champions League final 2025, which recently happened.\nOur AI agent.\nWell, I can show you here if I run this.\nOkay, so I've printed two different types here.\nI've printed the AI response and I've printed the duck.go search runs uh response aka the search tools.\nYou can see that GPT-4o minis says I'm sorry, but I don't have information on events that occurred after October 2021.\nWhy?\nBecause these models, well, their training data was up to this set date.\nSo obviously it won't have access to um uh 2025 Champions League final, right?\nIt we're lucky that it was able to say that it didn't because a lot of the times the model can um it can hallucinate and obviously that's quite bad.\nIf you don't know what hallucination is, hallucination is essentially when an AI model just spits out garbage because it just needs to spit out something, right?\nUh so that's what hallucination is.\nSo it could have said something like uh the winners of Champions League final was like Real Madrid or or some something which is completely wrong or uh infactual.\nBut look at what the search tool print did.\nIt said PSG and Inter Milan locked horns in the final of the UFA Champions League in 2025 and it eventually says that PSG achieved a historic milestone by winning.\nSo it was able to uh show us all of the details fully.\nIt showed when the date was, what the time was, who won, who it was between, what date it was, everything.\nAnd you can now see if we were able to connect the tool and an AI agent together, sorry, a tool or an LLM together, it could be quite powerful, right?\nSo that is the whole point of AI agents.\nAnd that's exactly what we're about to do right now.\nIf we go and define our class agent state here, it's the same way how we've been defining the state in our previous agents.\nBut this time the messages key is slightly different.\nWe've used annotated which we just talked about which is over here.\nIt allows us to provide extra information about the type of value.\nIn this case, we're providing age as extra information about the variable X.\nSo that's what annotated does.\nIt literally is just a way to be able to provide extra information.\nAnd over here that's what we've done.\nSo this essentially says this part is what data type the um your uh message is.\nSo it could be either a tool message, an AI message, etc.\nAnd this all stems from the base message.\nRemember what is the base message?\nThe base message is the parent class of all of the different types of messages.\nSo and this add messages is the reducer function.\nAnd I've written in comments here.\nIt essentially allows for the addition of messages without getting deleted.\nSo that's all you need to know for this.\nNow a tip is you can use this line of these two lines of code in almost any AI agent where you just want to store the messages.\nIt will work completely fine because you're storing the messages without any overwriting and you don't need to work uh you don't need to worry about the different message types like uh human message, AI message and all that.\nIt covers everything.\nSo there you go.\nNow we create a list of tools which we have.\nIn this case, I will only give our AI agent the access to the doctor go search run tool.\nSo you can have as many tools as you want.\nYou can write something like whatever tool you want.\nBut obviously make sure it's a proper tool.\nBut because we're only using inbuilt tools and I want to keep this very simple.\nI'm only going for Dr. Go searchron tool.\nSo now we go with the model uh definition here.\nI've upgraded to GPT-4o here because I personally feel like GPT-4o is the best LLM uh which is also reasonable in pricing uh for tool calling from the OpenAI uh services and it's also fast.\nI could have also used a reasoning model but obviously that would have taken more time to uh give an answer right.\nSo how am I able to connect my LLM to uh these tools?\nSo the way to do it is through this dobbind tools.\nSo this is really how to give our LLM knowledge about these tools.\nGives basically the access for uh it gives the LLM uh knowledge about the existence of these tools.\nSo if I run this perfect uh now I've defined another node and this is model call.\nSo essentially this is quite similar to what we've been doing previously.\nWe just use\n\n\nUh, we create a system message, and I'm no more pirate stuff. I have just kept it very simple, saying, \"You are my AI assistant. Please answer my query to the best of your ability.\" So it should, there's no pirate stuff, there's nothing, just a clear-cut answer to my request. And then I invoke the model, and now I pass the system prompt, which is the system message here, as well as all of the state, um, which we had before, and then we just return the messages, the response of the messages. Perfect. Okay, now here's where it gets slightly tricky. And, um, so pay attention here because now we're going to be defining the conditional edge. So if I quickly run this, okay, and run this. Okay, so let's go with this function. So this should continue function is the underlying action behind the conditional edge. The conditional edge is a decision element at the end of the day. So it, in this line of, uh, in this function, what we've done is we take the messages. So we copy the latest state into the messages variable, and then we look at the last message. Now there's a reason we look at the last message, and it's mainly to look for if there's any further tool calling. Why? Because if there's no further tool calling required in the last message, well, we can just end. That's what this line of code says. If not last message tool calls. So if it doesn't need any more tools, it just ends the thing. Else, we return and, uh, else we return continue. So continue is an edge, and end is an edge as well. And this will make a lot more sense in just a minute. Just bear with me. I want to create the graph itself first. So we again use state graph agent state. We use the add node method, which we've done to, uh, add this node, which is the model calling, which is very, very similar to our previous two AI agents. And then we create a tool node. Now a tool node is essentially just a node that neatly contains all of the tools we want our agent to have. So it basically is one single node which will have all of these tools neatly arranged. So we don't need to worry about all of the internal mechanics. So that's what this line of code does. And then we add this node as tools in our graph. So visually speaking, we have created this, this tool node here. Here's the, uh, the obviously the set entry point is our agent. So this is another way you can define a start and an end point in, uh, LangGraph. You can either use end or end start, or you can use dot set entry point and dot set finish point or dot set end point in LangGraph. There's multiple ways. I don't personally have a preference, but you can use both ways. So that's why I've written it like this. At the end of the day, you need to define your start and your end point somehow, and there's multiple ways to do it. That said, but how do we create this conditional edge? If we go to this, uh, line of code, it says graph.add conditional edges. So this internal method essentially creates the conditional edge for us. Now you can have many, many, many conditional edges within your, uh, LangGraph or within your graph, but in this case, we're just sticking with one. So for this method, the add conditional edges, we have the first parameter needs to be the origin, and this is our agent. Now what is our agent? Our agent is the node which we just created. This is where all the model calling gets passed. Then we have the underlying action. So similar to how the node needed the name of the node and the underlying action, aka the function behind this node, which in this case was a model call. Similarly, our underlying action is should continue, which is this function or this underlying logic. It returns two parts. It returns either an end or it returns a continue. So if it returns end, we go to the end part. Then you can see this dotted edge line here that it returns end. It goes to the end point. But if our underlying function, the should continue function, returns continue, well, that means it still needs to run some more tools, and that's why it goes back to the tool node. So from this, it goes our agent, continue, and this goes to the end if there's no more tool calling left. But the graph would still not be complete if we had left it by this point, because if you go to your tools, there's no way to go back to your agent, and that's where you create this new edge as well. And this, this edge goes from tools to our agent. Uh, the direction is from the start point, tools, and the end point, our agent. Now that was quite a lot. So I'm going to summarize it one more time, which is this: the it starts with the start point with this set entry point. Then we define our node, our agent, which we've been doing for the past, uh, three AI agents now, and that was through this. Then we created our conditional edge, which was either continue or end. Uh, and the underlying function behind that was this should continue function. And lastly, we needed to create an edge which goes from tools to our agent, which was through this. And then we compile the graph, and then we send this to, uh, visualize it. So let's just run this to make sure everything is working. And now let's actually try this. So this fancy piece of code which I've written here is really just a way of making the print statements or the output look better. So all it says is basically, u, if it's a human message, you write human message. If it's an AI message, write a AI message. If it's a tool message, write a tool message. You get the point. It's just a way to neatly organize it. So you can steal this piece of code if you, uh, want to, um, uh, visualize your, uh, outputs in a much neater way. So now I pass in the query, \"Who won the Champions League final in 2025?\" Uh, the same query I had passed before. And notice the tool message, which was this massive piece of, um, output, and then the AI message, the final AI message, which was \"PSG won in 2025, defeating Inter Milan with a final victory at this display, this place.\" What exactly happened here? Let's go through the workflow. So we passed in a query, which was at the start, and then we passed it to our agent. Then AI agent decided, \"Okay, the user wants to do a searching.\" How did it decide that? Well, the an internal LLM GPT-4o decided, \"Okay, the user wants to search.\" So it used tool calling search, aka this over here. It used the DuckDuckGo search here, uh, with it, uh, with his respective call ID and with the respective query, and the query which was passed to the actual DuckDuckGo search was \"Champions League final 2025 winner.\" It's slightly different with the human message. So you can see that the AI, the LLM in the background decided what to pass or what the argument to pass to this DuckDuckGo search was, and the DuckDuckGo search returned the tool, uh, returned this output. How did it, uh, return it? It went through this edge over here and passed all of the output, aka all of this output, back to our agent. Then our agent decided whether we need any further tool calling, because we only asked for one thing. We don't need any further more further tool calling, right? Uh, and because of this logic, which we defined in this, we just return end. So it goes to the endpoint, and it outputs the result, which was \"PSG won the Champions League final in 2025.\" So that was how we can create a robust AI agent by connecting an LLM with a tool. So great job so far. That was quite a lot. But remember, practice makes perfect. So try to create your own AI tools. Try to go to this documentation and try to create, uh, try to pick up any of these tools and just build. There are also free and paid. So be mindful of that. I used a free tool, but there's a lot of, uh, there's a lot of options. So feel free, have at it. Now I'm going to introduce you to a new piece of terminology, which is a ReAct agent. So a ReAct agent stands for reasoning and acting agent. Uh, they look like this in a nutshell. So you have your start and your, uh, endpoint. Then you have your agent in the middle, and then you have a loop which uses, uh, which keeps going with the tools and back and forth. But hold on. Doesn't this look a bit familiar? If you, if you scroll slightly up, if I hide this output, you can see that this is almost exactly, if not exactly, the same as this, the these graphs are isomorphic. So, they're just translated differently. The tools in the endpoint have switched, uh, positions, but you can see that this and this are exactly the same. So, what we just did was create a ReAct agent. And ReAct agents are really, really, really common in industry and in personal projects. Now because they are so common and they, first of all, they're common because they're quite robust. Because they're so common, LangGraph implemented its own custom method, uh, within its library. Now this will make things a lot easier. After this, you will have one question, which is why did I go through all of this line of code, and it was really to be able to teach you about conditional edges and tools. So if you've mastered that, uh, you can now create the same ReAct agent, the exact same ReAct agent, in very few lines of code, and this is how you do it. So agent 4, ReAct agent, the easy way to create these agents. So the aim of this is really how to create ReAct agents fast. So these are all of the new libraries, uh, the meth, all the imports you need to make, and there's only one new input which you need to make, which is create ReAct agent. This will be the, uh, method in the inbuilt method which will be able to create these ReAct agents for us. So we again define the list of tools, and we have our LLM, chat OpenAI, and again we're using GPT-4o. Shock, and this is how we create the exact same ReAct agent through very few lines of code. So we use the inbuilt method. We pass whatever model we want. In this case, we're going to be using chat GBD-4o from OpenAI. We are going to pass in the list of tools. We're going to name our agent search agent because it's spitting, right? It only has access to the DuckDuckGo search run tool and our prompt, aka our system message. Uh, so you are my AM, you are my AI system is the exact same thing. You don't need to specify that this is a system message because, uh, the internal mechanics handles that for you. So if I run this and this perfect, it should work. Okay, awesome. And if I print this again, you can see the almost the exact same thing was outputted in very few lines of code. The tool calling we did was DuckDuckGo search with the, uh, this query, and the final answer was this. So in what few, like three or four lines of code, we were able to completely replicate what we had done in the previous AI agents. So if you need to create ReAct agents files or just create ReAct AI agents in general, after you've mastered what a conditional edge and a tool is, you can just create this, uh, you can just use this inbuilt method. It makes things so much easier. Yeah. So we've achieved the same result in much fewer lines. But now we go to agent 5. And your question might be this: How do we, is there a way to create our agents using our own tools? And the short answer is yes, obviously. So what if we want to make our own tools? After all, LangGraph would have all the inbuilt tools we want, right? So that's the aim of this agent. Also, please note that we're going to be using the inbuilt method to create these ReAct agents from now on. Uh, the reason is, well, if I write all of this unnecessary code again, it will just get messy and it will just be unreadable. If we import this, oh, that's agent 4. If we import this, perfect. So this is the new line of code, and this line of code essentially tells LangGraph that this function is a tool or a specialized function. So how do we actually define a tool in LangGraph, our own tool? So you use @tool, which is a decorator, and this decorator basically tells LangGraph that this is a special function, aka, in this case, it's a tool, and in this case, I have just created a very, very dead simple tool, which is the weather tool. So it basically gives the weather in a given city. So the two most important things which you need to know, uh, which you need to be mindful of when creating your own tools is using this decorator and also this doc string. Possibly the most important thing, if you don't have a doc string, your entire graph will not compile. And the reason for this doc string is it tells our agent what the tool is for. In the next agent, I will tell you how to structure the docs training in a better manner. But for now, to keep it very simple, I've just said it gets the weather in a given city. So our underlying LLM will know, \"Okay, this tool, this weather tool, uh, has is for getting the weather in a given city.\" Now your question could be, the previous DuckDuckGo search tool, where's the doc string in that? There is a doc string. If you go within the documentation, all of the doc strings are, uh, written by the LangGraph team. So that's why you didn't need to write any doc strings for that. So let's run this as well. And again, we create the same list of tools, and we use the create ReAct agent. And in this case, I've named it weather agent. Why? Because, well, we're using the weather tool, our own custom tool. Perfect. And now let's just run this. So we are invoking the tool by setting the messages, uh, as \"What is the weather in London?\" So the hard-coded message is \"The weather in this city is sunny.\" And if you look, \"The weather in London is currently sunny.\" You can see that it works completely as if we were using an inbuilt tool. It uses the weather tool. It sets the arguments, and everything works flawlessly. Now what if we wanted in multiple tools? Well, you can do that as well in LangGraph. You can. In this case, I have set the, uh, another created another tool, which is the social media follower account. So you pass in the social media as obviously as a string, and this is how you write doc strings. So you have the explanation, you have your arguments, you have what's returned, and you also provide an example. All of this doc string is passed into the LLM. So obviously the more stuff you have here, the better the LLM will understand, \"Okay, what this person or what this tool needs me to pass in as arguments.\" The very simple return statement is \"You have 9876 followers on social media.\" Something random I completely make made up. But this is how you create another tool. And now look at how we've created our tools list. It's increased by one because now we're giving our AI agent, our AI agent, uh, more tools. So again, we're using the GPT-4o tool, a GPD-4o LLM, and we're giving it access to the tools. And now I've set it as a different name, aka the general agent. The same prompt though.\n\n\nOkay, and let's invoke this.\n\nSo this time, I've asked, \"What is the weather in London and how many followers do I have on Instagram?\" And then, \"What about Twitter?\" So there's quite a lot of tool calling which will be necessary. But look at why LangGraph is so robust, and it's the number one upcoming AI agent framework now. It's because in very few lines of code, I was able to call the weather and the social media follow account twice: one for Instagram and one for Twitter. And look at the final response. The weather in London is sunny, which is true. And you have 9,876 followers on Instagram and on Twitter as well. So in very, very, very few lines of code, I was able to create a very robust AI agent, and I can even increase by asking, \"What about on Facebook?\" or \"What about on Snapchat?\" You get the point. I could have also asked, \"What is the weather in Paris?\" and it would have still worked flawlessly as well. So that is why AI agents in LangGraph are so robust because of that graph structure, and this create ReAct agent is why it's also why it's so commonly used in industry and in personal projects.\n\nI made a promise at the start of this course that I will be using models as well, and this is where it comes in: models, our sixth and final agent for this course. For this agent, we're going to explore models. So we're going to integrate Ollama models into LangGraph and discuss the pros and cons of Ollama models. So for running these Ollama models, I will run these locally, not in this Datacamp file, but in my own local file. So let's jump into that.\n\nOkay, people. So now I'm on VS Code, and I'm going to be running my Ollama models through here. In particular, I'm going to be using Qwen 2.5. I personally like the Qwen model series. I think they're great, but you can use any model. You can download any models. You can download DeepSeek. You can download Llama, etc. In this particular case, I want to compare the performance of OpenAI and Ollama models and really show you this comparison and also show you why I used OpenAI models throughout this tutorial rather than Ollama. So the only difference of imports is this one line of code, and it's quite self-explanatory. It's just chat_Ollama, very similar to OpenAI. It just allows us to use Ollama models. In terms of the tool, this is our own custom tool which we've already created before. It's the weather tool, and this is the DuckDuckGo search run tool, which is the inbuilt tool. And now we pass this to the both OpenAI agent and the Ollama agent, which is over here.\n\nIn terms of the prompt, I have set the exact same prompt in for both. In this case, it will be, \"You are my AI assistant that has access to certain tools. Use the tools to help me with my task.\" That's the prompt, the system message. And in terms of my query, I've also said it the same. So I've asked both LLMs or both AI agents, \"What is the weather in San Francisco and tell me the latest news about the stock of Apple of today and the date today, please date.\" I've also asked it one more thing which we haven't actually asked before, which is also, \"Tell me a list of tools you have, please.\" So the correct answer should be that it has access to these two tools, right? Okay, so let's see how they perform. So again, it's the same create ReAct agent. I'm just going to show you the comparison between the performance of Ollama and OpenAI. So I've already ran this code once, and here's what the results are. So this was OpenAI's results. It used the following tools with these respective arguments, and the final answer was, \"The weather in San Francisco was sunny. The latest news about Apple stock is this,\" and it gives detail regarding the date. \"Today is June 12th,\" which we also asked for. Remember, we asked for what the date is here and the date today, please. And if we go back, and it also said, \"As for the list of tools I have, they include functions: weather to get the weather in a given city and DuckDuckGo search to perform a search query using DuckDuckGo.\" And that is obviously like fully correct, right? I don't really have any problem with it. It used the right number of tools. It answered every single aspect of my query properly.\n\nNow look at the Ollama result. So if I press enter a few times here, perfect. You should be able to see that. Okay. Over here, it's used the weather tool, which is correct. It said, \"Latest news about stock today,\" which is 12th of June 2025, which is also correct. But then it started looking for \"date today, please 12th June,\" which doesn't actually make any sense. And then if we look at the final answer, it says, \"The latest news in the latest with the weather in San Francisco is sunny. The latest news is this.\" And then look here, it says, \"The today's date is Tuesday, June 10th, 2025,\" which is wrong, even though it correctly mentioned what the actual date is, 12th June, correctly several times before, but it did say, it did correctly give the list of tools properly. It said weather and DuckDuckGo search. So there's obviously clear flaws with the Ollama model, specifically Qwen 2.5.\n\nIn particular, I want to show you this piece of leaderboard. So this leaderboard is essentially, in a nutshell, a way to classify how good a particular model is when it comes to tool calling. So if you can see that GPT-4o ranks quite high. All of the GPT models rank quite relatively quite high. If I search up for Qwen 2.5, you can see they place, this is the 72 billion one, 32 billion one, 72 billion, 32, 14, 14, 77, 73. So in my case, I was using either the 3B, the 7B, or the 3B, one of these two, but you can see, look at the ranks, 66 and 81st compared to GPT's one, which was quite highly ranked. This is also in the Datacamp file if you have access to this. This is the URL as well. But let's go back to the model results. The point I'm trying to state is this: I would personally use OpenAI models or Gemini models or these APIs, even though they are paid. If you want some robust tool calling, I would highly recommend those unless if privacy is an issue. If privacy is an issue, then obviously you need to go for models, right? You could also make a counterargument here that you can't really compare GPT-4o, which I used here, versus Qwen 2.5, which is like in a few billion of parameters versus a trillion parameter model, and that is fair, but the counterargument for that is that your models are dependent on your computer specification, right? If you could run a very, very large model like DeepSeek 650 billion parameter model, then obviously the performance would be quite similar, but to be able to run those, you need so much compute, and at that rate, if you, if privacy isn't a concern and you want something which is relatively inexpensive and still robust, I would highly recommend either OpenAI models, Gemini models, these sort of models, because you can see from the results, they are quite a lot more robust.\n\nAnd that brings us to the end of this tutorial, and well done for completing this tutorial. We've learned so much in 1 hour. To quickly summarize what we've learned: We learned how to create our own custom tools. We learned what ReAct agents are. We learned how to use inbuilt tools. We learned how to create ReAct agents. We learned what all the different type annotations are, the different types of messages are, the different types of fundamental building blocks are, such as conditional edges, tools, etc. We built six agents in together. We showed how to visualize agents. We showed how to visualize graphs. We learned a ton of stuff. So massive, massive congratulations for completing this. But remember, practice makes perfect. So now go ahead and actually build these AI agents and graphs by yourself. Try to build any AI agent. Try to solve your own problem because remember, practicing is how you're going to actually solidify your fundamentals and understanding. If you have any further questions or want to state your opinion or anything, you can write them in the comments in under this video. With that being said, that brings us to the end of this tutorial, and I'll see you in another tutorial. Goodbye.\n",
  "dumpedAt": "2025-07-21T18:43:26.195Z"
}