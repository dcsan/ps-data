{
  "episodeId": "bfqiGAn6Ws0",
  "channelSlug": "@datacamp",
  "title": "Big Data with PySpark Crash Course | Machine Learning, Feature Engineering and More",
  "publishedAt": "2025-06-04T17:24:37.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Hi data scamps and data champs. Here",
      "offset": 0.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we'll explore using the Python API for",
      "offset": 2.399,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Apache Spark in Big Data with Pispark.",
      "offset": 4.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "You'll learn how to leverage parallel",
      "offset": 7.759,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "computing to process and analyze big",
      "offset": 9.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "data efficiently. We'll cover data",
      "offset": 11.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "cleaning, feature engineering, and",
      "offset": 13.519,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "building machine learning models at",
      "offset": 15.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "scale. You'll get hands-on experience",
      "offset": 17.039,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "creating recommendation engines using",
      "offset": 18.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "real world data sets like movie lens and",
      "offset": 21.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the million songs data set. To explore",
      "offset": 23.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "further, check out the skill track",
      "offset": 25.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "linked in the description below. Please",
      "offset": 26.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "like and subscribe for more.",
      "offset": 28.8,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "Welcome. In this course, we'll explore",
      "offset": 31.96,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "Pispark, a powerful tool for processing",
      "offset": 35.96,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and analyzing big",
      "offset": 39.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "data designed for data engineers, data",
      "offset": 41.719,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "scientists, and machine learning",
      "offset": 45.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "enthusiasts. This course will teach you",
      "offset": 46.96,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "to work with largecale data sets in",
      "offset": 50.559,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "distributed",
      "offset": 53.92,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "environments, transforming raw data into",
      "offset": 55.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "valuable",
      "offset": 58.879,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "insights. I'm Benjamin, your instructor",
      "offset": 60.76,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "for this course.",
      "offset": 64.72,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "I've been a data engineer for nearly a",
      "offset": 67.04,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "decade working with big data solutions",
      "offset": 69.439,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "in Pispark for ETL pipelines, data",
      "offset": 72.479,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "cleaning, and machine",
      "offset": 75.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "learning. PiSpark is one of the most",
      "offset": 77.96,
      "duration": 8.12
    },
    {
      "lang": "en",
      "text": "versatile tools for data professionals.",
      "offset": 81,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "Apache Spark is an open-source",
      "offset": 86.08,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "distributed computing system designed",
      "offset": 89.56,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "for fast processing of largecale",
      "offset": 92.799,
      "duration": 8.401
    },
    {
      "lang": "en",
      "text": "data. PiSpark is the Python interface",
      "offset": 96.759,
      "duration": 8.521
    },
    {
      "lang": "en",
      "text": "for Apache Spark. It handles large data",
      "offset": 101.2,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "sets efficiently with parallel",
      "offset": 105.28,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "computation in Python",
      "offset": 107.88,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "workflows. Ideal for batch processing.",
      "offset": 110.439,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "Real time streaming, machine learning,",
      "offset": 114.159,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "data analytics, and SQL",
      "offset": 117.52,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "querying. PiSpark supports industries",
      "offset": 120.759,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "like finance, healthcare, and ecommerce",
      "offset": 123.759,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "with speed and",
      "offset": 126.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "scalability. PiSpark is ideal for",
      "offset": 129.479,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "handling large data sets that can't be",
      "offset": 132.08,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "processed on a single machine. It excels",
      "offset": 134.879,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "in big data analytics through",
      "offset": 139.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "distributed data processing using",
      "offset": 141.84,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "Spark's in-memory computation for faster",
      "offset": 144.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "processing. Machine learning on large",
      "offset": 148.92,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "data sets leverages Spark's MLIB for",
      "offset": 151.12,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "scalable model training and evaluation.",
      "offset": 155.36,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "ETL and ELT pipelines transforms large",
      "offset": 159.44,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "volumes of raw data from",
      "offset": 163.84,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "sources into structured formats. PISpark",
      "offset": 167.4,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "is flexible working with diverse data",
      "offset": 171.76,
      "duration": 7.479
    },
    {
      "lang": "en",
      "text": "sources like CSV, Parquet and many",
      "offset": 174.8,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "more. A key component of working with",
      "offset": 179.239,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "Pispark is clusters.",
      "offset": 182.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "A Spark cluster is a group of computers",
      "offset": 185.44,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "or nodes that collaboratively process",
      "offset": 188.64,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "large data sets using Apache Spark with",
      "offset": 192.159,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "a master node coordinating multiple",
      "offset": 195.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "worker",
      "offset": 199.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "nodes. This architecture enables",
      "offset": 200.28,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "distributed processing.",
      "offset": 203.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "The master node manages resources and",
      "offset": 206.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "tasks while worker nodes execute",
      "offset": 209.28,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "assigned compute",
      "offset": 212.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "tasks. A Spark session is the entry",
      "offset": 215.4,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "point into PIS Spark, enabling",
      "offset": 218.56,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "interaction with Apache Spark's core",
      "offset": 221.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "capabilities. It allows us to execute",
      "offset": 225,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "queries, process data, and manage",
      "offset": 227.44,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "resources in the Spark",
      "offset": 230.12,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "cluster. To create a Spark session, run",
      "offset": 233.64,
      "duration": 8.76
    },
    {
      "lang": "en",
      "text": "from PISparks SQL import spark session.",
      "offset": 237.68,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "We'll create a session named my Spark",
      "offset": 242.4,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "app using Spark session builder stored",
      "offset": 244.879,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "as a variable",
      "offset": 248.959,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Spark. The builder method sets up the",
      "offset": 250.84,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "session while get or create initiates a",
      "offset": 254.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "new session or retrieves an existing",
      "offset": 257.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "one.",
      "offset": 260.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "The app name method helps manage",
      "offset": 262.079,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "multiple PISpark",
      "offset": 264.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "applications. With our Spark session",
      "offset": 267.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "ready, we can load data and apply",
      "offset": 270.16,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "transformations or",
      "offset": 273.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "actions. It's best practice to use Spark",
      "offset": 276.04,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "session.builder.get or create which",
      "offset": 279.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "returns an existing session or creates a",
      "offset": 282.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "new one if necessary.",
      "offset": 284.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "PISpark dataf frames are distributed",
      "offset": 287.36,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "tablelike structures optimized for",
      "offset": 290.56,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "largecale data processing. Their syntax",
      "offset": 294.12,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "is similar to pandas with the main",
      "offset": 298.4,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "difference being how data is managed at",
      "offset": 302.199,
      "duration": 8.961
    },
    {
      "lang": "en",
      "text": "a low level. To create a PIS spark data",
      "offset": 306.32,
      "duration": 9.92
    },
    {
      "lang": "en",
      "text": "frame, use the Spark read CSV function",
      "offset": 311.16,
      "duration": 10.36
    },
    {
      "lang": "en",
      "text": "in the Spark session with a CSV file.",
      "offset": 316.24,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "For this example, we'll use a",
      "offset": 321.52,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "generalized data variable representing",
      "offset": 324.16,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "any data source and columns to define",
      "offset": 327.919,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "the schema. To see our data frame, we",
      "offset": 331.68,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "can use the show method. We'll explore",
      "offset": 335.12,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "these concepts further throughout this",
      "offset": 338.72,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "course. Now, let's go see these concepts",
      "offset": 343.08,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "in",
      "offset": 347.52,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "action. Welcome back. We've already",
      "offset": 351.56,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "started with dataf frames in Pispark in",
      "offset": 354.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "the previous video, but now let's dive",
      "offset": 357.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "deeper.",
      "offset": 360.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "While we may be familiar with data",
      "offset": 362.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "frames from pandas, the key difference",
      "offset": 364.56,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "in pi spark is how the data is",
      "offset": 367.12,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "distributed. Pandas operates on a single",
      "offset": 371,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "compute instance while pi spark",
      "offset": 373.84,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "distributes data across multiple",
      "offset": 376.319,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "instances affecting processing speed and",
      "offset": 379.24,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "data scalability.",
      "offset": 382.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Dataf frames are essential in Pispark",
      "offset": 385.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "for efficiently managing largecale data",
      "offset": 387.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "across",
      "offset": 390.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "clusters. While they resemble Pandanda's",
      "offset": 392.36,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "dataf frames, they are designed for much",
      "offset": 395.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "larger data sets. will frequently",
      "offset": 397.68,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "interact with data using PISpark data",
      "offset": 400.639,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "frames which support various",
      "offset": 403.039,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "manipulation tasks such as filtering,",
      "offset": 405.759,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "grouping, and aggregating on distributed",
      "offset": 408.319,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "data making them vital for big data",
      "offset": 410.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "analytics. Additionally, data frames",
      "offset": 413.479,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "support SQL like operations on tables.",
      "offset": 416.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "As we go through this course, you'll",
      "offset": 420.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "probably notice similarities between",
      "offset": 423.28,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Pandanda's dataf frame syntax and",
      "offset": 425.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Pispark dataf frame",
      "offset": 428.479,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "syntax. But bear in mind, data frames in",
      "offset": 430.52,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "Pispark operate slightly differently. So",
      "offset": 434.16,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "be sure to pay",
      "offset": 437.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "attention. Let's start by creating a",
      "offset": 439,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "dataf frame in",
      "offset": 441.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "PISpark. A common method for loading",
      "offset": 442.84,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "data is using spark readad.cssv CSV",
      "offset": 445.759,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "which easily reads CSV files into a",
      "offset": 449.599,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "PiSpark dataf frame allowing us to",
      "offset": 452.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "define headers and automatically infer",
      "offset": 455.12,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "schema",
      "offset": 458.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "types. This code loads a CSV treating",
      "offset": 459.639,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "the first row as headers and inferring",
      "offset": 463.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "data types for each column. We also",
      "offset": 465.919,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "include header equals true and infer",
      "offset": 468.639,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "schema equals true which are true for",
      "offset": 471.199,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "this particular table. But there are",
      "offset": 475.12,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "also many other arguments available. So",
      "offset": 478,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "check out your",
      "offset": 482.56,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "documentation. Using the show method, we",
      "offset": 484.52,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "can display the first five rows of the",
      "offset": 487.759,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "data frame. While we can also create",
      "offset": 490,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "dataf frames using the create dataf",
      "offset": 492.879,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "frame function. Read.csv CSV is",
      "offset": 495.28,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "generally faster, offering significant",
      "offset": 499.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "speed improvements at scale, especially",
      "offset": 502.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "as we gain real world experience with",
      "offset": 504.8,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "big data. To inspect the schema, use",
      "offset": 507.28,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "print schema to view the dataf frame",
      "offset": 510.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "structure. Once we've loaded data into a",
      "offset": 514.039,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "dataf frame, we can perform basic",
      "offset": 516.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "analytics like aggregations, which",
      "offset": 519.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "summarize data by counting rows, summing",
      "offset": 521.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "values, or calculating averages. These",
      "offset": 524.32,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "will return an integer or float.",
      "offset": 527.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Typically, for instance, to count the",
      "offset": 531,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "rows in a data frame, use the count",
      "offset": 533.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "method. For more advanced summaries, we",
      "offset": 536.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "can employ the group by and aggregate",
      "offset": 539.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "methods.",
      "offset": 541.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to group data by a column and calculate",
      "offset": 543.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the average of another. We can use",
      "offset": 545.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "various aggregation functions like",
      "offset": 548.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "standard deviation, sum, and more that",
      "offset": 550.48,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "you are probably already familiar",
      "offset": 554.32,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "with. This approach above groups data by",
      "offset": 556.92,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "gender and computes the average of",
      "offset": 560.959,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "salary USD, showing how to combine steps",
      "offset": 563.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "for data summarization and quick",
      "offset": 566.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "insights.",
      "offset": 569.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Some of the most important methods we'll",
      "offset": 571.36,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "use with dataf frames are group by,",
      "offset": 573.36,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "aggreg, filter, and",
      "offset": 576.68,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "select. Let's start with select which",
      "offset": 579.24,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "operates like it does in SQL where it",
      "offset": 581.839,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "will take specific named columns. Filter",
      "offset": 584.08,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "operates like SQL's where group by is",
      "offset": 587.92,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "also like SQL's similar same named",
      "offset": 591.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "keyword.",
      "offset": 594.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "A takes a function like sum and the",
      "offset": 596.8,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "specific columns we're interested in. In",
      "offset": 601.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "each case, we need to pass the data",
      "offset": 604.48,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "frame name and the columns as a",
      "offset": 607.279,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "list. For example, we can filter the",
      "offset": 611.24,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "data frame for rows where the value in",
      "offset": 614.56,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the age column is greater than 50 and",
      "offset": 617.04,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "select only those columns.",
      "offset": 619.839,
      "duration": 8.481
    },
    {
      "lang": "en",
      "text": "Let's practice dataf frames in",
      "offset": 623.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "pispark. Welcome back. In this video,",
      "offset": 629.959,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "we'll work with some basic dataf frame",
      "offset": 633.279,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "operations in Pispark. Reading data from",
      "offset": 636.519,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "various formats allows flexibility in",
      "offset": 640.079,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "handling diverse data sets. Each format",
      "offset": 643.279,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "offering unique benefits.",
      "offset": 647.36,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "CSV files or commaepparated value files",
      "offset": 650.079,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "are widely used because of their",
      "offset": 654.079,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "simplicity and compatibility across many",
      "offset": 656.079,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "platforms. They store data in a plain",
      "offset": 659.32,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "text format, making them easy to read",
      "offset": 662.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and write without needing specialized",
      "offset": 665.839,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "tools.",
      "offset": 668.48,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "However, CSV files lack schema",
      "offset": 669.959,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "enforcement, which means they don't",
      "offset": 673.959,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "define or enforce data types for each",
      "offset": 676.12,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "column, leading to potential",
      "offset": 680.6,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "inconsistencies. We can read the data",
      "offset": 683.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "using the read CSV function.",
      "offset": 685.6,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "JSON or JavaScript object notation files",
      "offset": 689.56,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "are ideal for representing nested data",
      "offset": 693.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "structures, making them a good choice",
      "offset": 696.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "for data that includes hierarchical",
      "offset": 699.2,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "relationships or arrays or high",
      "offset": 702.24,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "compatibility. However, JSON files can",
      "offset": 706.279,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "become storage intensive when scaled to",
      "offset": 709.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "large data sets. We can load them using",
      "offset": 712.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the read JSON",
      "offset": 715.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "function. Paret is a columnar storage",
      "offset": 717.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "format optimized for read heavy",
      "offset": 721.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "operations making it a powerful choice",
      "offset": 724.2,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "for large data sets that require",
      "offset": 727.2,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "frequent querying.",
      "offset": 729.76,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "Additionally, Parquet enforces schema",
      "offset": 732.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "definitions which helps maintain data",
      "offset": 735.639,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "consistency and supports complex data",
      "offset": 739.16,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "types like nested structures much like",
      "offset": 741.92,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "JSON. We can load paret files using read",
      "offset": 746.519,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "parquet.",
      "offset": 750.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Using these formats, Pispark enables",
      "offset": 752.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "data engineers and data scientists to",
      "offset": 755.04,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "tailor their storage to the needs of",
      "offset": 757.36,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "their specific",
      "offset": 760.639,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "applications. Spark can automatically",
      "offset": 762.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "infer schemas, but sometimes it",
      "offset": 765.279,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "misinterprets data types, particularly",
      "offset": 768.44,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "with complex or ambiguous data. Manually",
      "offset": 771.279,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "defining a",
      "offset": 775.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "schema can ensure accurate data",
      "offset": 777,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "handling.",
      "offset": 780.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "To manually configure a schema, we need",
      "offset": 782.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "to define the data type using the",
      "offset": 785.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "strruct field function calling the write",
      "offset": 787.519,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "data type",
      "offset": 790.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "method. PISpark dataf frames can support",
      "offset": 791.959,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "various data types similar to SQL and",
      "offset": 795.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "pandas. The primary ones are integer",
      "offset": 797.839,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "type for whole numbers, long type for",
      "offset": 801.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "large integers, float type and double",
      "offset": 804.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "type for decimal numbers and string type",
      "offset": 807.279,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "for strings. We import the specific",
      "offset": 810.48,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "classes from",
      "offset": 813.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "pispark.sql.types. To define the schema",
      "offset": 816.839,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "for our dataf frame, we use strruct type",
      "offset": 819.279,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "and strct field functions to define the",
      "offset": 822.24,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "structure and fields of a dataf frame.",
      "offset": 825.32,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "Filling in the columns and their",
      "offset": 828.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "types, selecting specific columns and",
      "offset": 832.36,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "filtering rows are fundamental",
      "offset": 835.44,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "operations in data",
      "offset": 838.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "analysis. With Pispark, you can perform",
      "offset": 840.6,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "these operations on large data sets with",
      "offset": 843.76,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "efficiency using the select, filter,",
      "offset": 846.639,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "sort, and wear methods.",
      "offset": 850.399,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "where and filter operates similarly to",
      "offset": 853.519,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "those in SQL where we pass a column or",
      "offset": 856.88,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "columns and a condition to",
      "offset": 860.32,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "match. Sorting and handling missing data",
      "offset": 863.32,
      "duration": 7.879
    },
    {
      "lang": "en",
      "text": "are common tasks. Dropping nulls can",
      "offset": 867.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "clean data but in some cases may may",
      "offset": 871.199,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "want to fill or input values instead",
      "offset": 874.24,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "which Spark also supports. We can use",
      "offset": 878.399,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "sort for simple flexible sorting and",
      "offset": 882,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "order by for complex multicolumn sorting",
      "offset": 885.519,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "methods to sort and order a dataf frame",
      "offset": 889.36,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "similar to the same command in",
      "offset": 892.639,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "SQL. We can use the na drop to drop all",
      "offset": 895,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "nulls in a dataf",
      "offset": 898.88,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "frame. Here's a cheat sheet to help you",
      "offset": 900.36,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "out. Now let's go see these data frames",
      "offset": 903.959,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "in practice.",
      "offset": 907.92,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "In this video, we'll cover essential",
      "offset": 913.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "techniques for handling missing data,",
      "offset": 916.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "managing dataf frame columns and rows,",
      "offset": 919.04,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "and using PIS Spark's built-in functions",
      "offset": 921.6,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "to clean and transform",
      "offset": 924.959,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "data. Handling null values in PiSpark is",
      "offset": 927.399,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "essential for accurate",
      "offset": 931.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "analysis. Nulls can skew results or",
      "offset": 934.36,
      "duration": 7.479
    },
    {
      "lang": "en",
      "text": "cause errors. PiSpark offers two primary",
      "offset": 937.839,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "methods to address this issue. The first",
      "offset": 941.839,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "approach is to drop rows with null",
      "offset": 945.519,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "values using NA drop either across the",
      "offset": 948.56,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "entire data frame or in specific",
      "offset": 953.12,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "columns. This simplifies the data set",
      "offset": 956.6,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "but may significantly reduce the size if",
      "offset": 960.079,
      "duration": 7.641
    },
    {
      "lang": "en",
      "text": "nulls are common. for column specific",
      "offset": 963.519,
      "duration": 8.76
    },
    {
      "lang": "en",
      "text": "filtering where and is not null can be",
      "offset": 967.72,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "used. The second approach is to replace",
      "offset": 972.279,
      "duration": 8.201
    },
    {
      "lang": "en",
      "text": "nulls with default values using na fill.",
      "offset": 975.6,
      "duration": 8.359
    },
    {
      "lang": "en",
      "text": "This method is ideal when nulls are",
      "offset": 980.48,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "sparse or when removing rows would",
      "offset": 983.959,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "result in data loss",
      "offset": 986.8,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "ensuring the data set remains complete",
      "offset": 989.32,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "and consistent.",
      "offset": 992.639,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "PiSpark simplifies creating new columns",
      "offset": 994.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "with the width column method, allowing",
      "offset": 998.16,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "users to define the column name and",
      "offset": 1000.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "computation. This is useful for derived",
      "offset": 1003.959,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "metrics or",
      "offset": 1007.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "transformations. Renaming columns with",
      "offset": 1009.32,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "column renamed improves clarity, making",
      "offset": 1012.24,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "data frames easier to work with with in",
      "offset": 1016.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "collaborative settings.",
      "offset": 1019.68,
      "duration": 5.999
    },
    {
      "lang": "en",
      "text": "Clear descriptive names reduce ambiguity",
      "offset": 1022.16,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 1025.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "ensure accurate",
      "offset": 1026.52,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "documentation. Dropping columns with",
      "offset": 1029.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "drop removes redundant or irrelevant",
      "offset": 1031.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "data, focusing the data frame on",
      "offset": 1034.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "essential",
      "offset": 1037.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "points. This helps manage large data",
      "offset": 1038.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "sets by reducing memory usage and",
      "offset": 1041.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "improving efficiency.",
      "offset": 1044,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "These operations enhance PiSpark dataf",
      "offset": 1046.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "frames flexibility and clarity,",
      "offset": 1049.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "tailoring them to specific analysis",
      "offset": 1052.32,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "needs while maintaining a clean",
      "offset": 1054.76,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "structure. Row operations are central to",
      "offset": 1059,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "data analysis. Filtering is for",
      "offset": 1062.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "narrowing down a data frame to the most",
      "offset": 1066.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "relevant data. By applying conditions to",
      "offset": 1068.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "filter rows, we can isolate subsets that",
      "offset": 1072.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "meet specific criteria such as data from",
      "offset": 1075.44,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "a particular time period, geographic",
      "offset": 1078.64,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "region, or category. We use the filter",
      "offset": 1081.16,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "method providing the column name and the",
      "offset": 1085.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "filter criteria.",
      "offset": 1088.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Grouping rows based on specific fields",
      "offset": 1091.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "or categories allows us to organize the",
      "offset": 1094.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "data by meaningful segments such as by",
      "offset": 1097.76,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "customer, product, or date. Grouping",
      "offset": 1100.88,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "enables us to analyze patterns and",
      "offset": 1104.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "trends within each category, offering",
      "offset": 1107.039,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "insights that would be difficult to see",
      "offset": 1110.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in ungrouped data.",
      "offset": 1113.2,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "We use the group by method and then the",
      "offset": 1115.52,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "aggregation we would be interested in",
      "offset": 1118.919,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "with the columns we want to focus",
      "offset": 1122.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "on. Here's what the code from the",
      "offset": 1125.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "previous slide may look",
      "offset": 1128.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "like. And here is a cheat sheet to help",
      "offset": 1130.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you going",
      "offset": 1133.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "forward. Let's go practice how data",
      "offset": 1135.96,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "frames work in Pispark.",
      "offset": 1139.2,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "In this video, we'll explore powerful",
      "offset": 1145.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "data manipulation techniques in PiSpark,",
      "offset": 1148.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "including joins, unions, and complex",
      "offset": 1151.039,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "data types. Joins in PiSpark combine",
      "offset": 1154.32,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "data from multiple data frames based on",
      "offset": 1158.24,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "shared columns. Similar to",
      "offset": 1161.28,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "SQL, this enriches data sets such as as",
      "offset": 1164.28,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "merging customer details with purchase",
      "offset": 1169.28,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "history to analyze buying patterns.",
      "offset": 1171.679,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "PISpark supports inner, left, right, and",
      "offset": 1175.679,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "full outer",
      "offset": 1179.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "joints performed using the join method",
      "offset": 1181.08,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "by specifying the second data frame,",
      "offset": 1184.72,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "join type, and columns",
      "offset": 1187.679,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "impacted. For columns with different",
      "offset": 1190.52,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "names, you must explicitly specify the",
      "offset": 1193.32,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "joining columns.",
      "offset": 1197.6,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "Similar to SQL",
      "offset": 1199.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "syntax, the union operation in Pispark",
      "offset": 1202.039,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "is a powerful tool that enable us to",
      "offset": 1205.76,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "combine or stack two data frames as long",
      "offset": 1208.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "as they share the same",
      "offset": 1212.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "structure, meaning they have the same",
      "offset": 1214.6,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "number and types of columns in the same",
      "offset": 1216.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "order.",
      "offset": 1220.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "This operation is particularly useful",
      "offset": 1222.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "when we're working with data sets that",
      "offset": 1225.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "have been split across different sources",
      "offset": 1227.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "or time periods and we want to",
      "offset": 1230.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "consolidate them into a single data",
      "offset": 1232.799,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "frame for easier analysis and",
      "offset": 1234.96,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "processing. If the data frames don't",
      "offset": 1238.84,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "have the same schema, a union will not",
      "offset": 1241.6,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "work and create an error.",
      "offset": 1245.36,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Using union, we can append one data",
      "offset": 1248.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "frame on top of another, effectively",
      "offset": 1251.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "combining rows from both dataf frames",
      "offset": 1254.159,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "into a single unified data set. For",
      "offset": 1256.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "example, if we're working with monthly",
      "offset": 1260.12,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "sales data stored as separate files for",
      "offset": 1262.559,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "each month, union allows us to combine",
      "offset": 1265.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "all monthly data frames into a single",
      "offset": 1268.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "data frame that represents the entire",
      "offset": 1271.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "year.",
      "offset": 1275.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "This consolidated view simplifies",
      "offset": 1276.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "further analyses as it eliminates the",
      "offset": 1279.2,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "need to handle multiple separate data",
      "offset": 1282.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "frames. Here's the syntax for performing",
      "offset": 1285,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a union in pi",
      "offset": 1287.84,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "spark. This operation stacks df2",
      "offset": 1289.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "underneath",
      "offset": 1293.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "df1. However, it's important to note",
      "offset": 1295.64,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "that the dataf frames must have",
      "offset": 1298.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "identical schemas for this operation to",
      "offset": 1300.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "work correctly.",
      "offset": 1303.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Otherwise, pispark will raise an error.",
      "offset": 1305.4,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "This schema alignment is crucial because",
      "offset": 1308.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "mismatched columns or data types would",
      "offset": 1311.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "prevent the rows from being combined",
      "offset": 1314.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "accurately and showing the same type of",
      "offset": 1317.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "data.",
      "offset": 1320.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Complex data types in Pispark like",
      "offset": 1322.24,
      "duration": 8.559
    },
    {
      "lang": "en",
      "text": "arrays, strrus, maps enhance flexibility",
      "offset": 1325.72,
      "duration": 9.24
    },
    {
      "lang": "en",
      "text": "by allowing nested data within each row.",
      "offset": 1330.799,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "These data types enable Pispark to",
      "offset": 1334.96,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "manage structured data within a single",
      "offset": 1337.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "column. Find a way to work with these",
      "offset": 1341.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "more complex data relationships and",
      "offset": 1344.64,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "hierarchy directly in the data frame.",
      "offset": 1347.64,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "Arrays store lists within a column.",
      "offset": 1352.159,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "Useful for attributes with multiple",
      "offset": 1355.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "values. For an array, define the values",
      "offset": 1358.039,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "being passed using the appropriate data",
      "offset": 1361.6,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "type. Here we are using lit for a",
      "offset": 1364.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "specific",
      "offset": 1367.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "value. Maps store dynamic key value",
      "offset": 1368.84,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "pairs within a column providing a",
      "offset": 1372.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "flexible way to store dynamic data",
      "offset": 1376.64,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "attributes. where each row might have",
      "offset": 1379.84,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "different keys. The map method needs to",
      "offset": 1382.799,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "have passed the value, the data type",
      "offset": 1387.44,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "method we've seen before, and a boolean",
      "offset": 1390.64,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "for requirement of the key value",
      "offset": 1394.64,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "pair. Strs group related fields within a",
      "offset": 1398.039,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "single",
      "offset": 1402.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "column. This is valuable for managing",
      "offset": 1403.799,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "hierarchical data within one column.",
      "offset": 1406.64,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Similar to map, we pass a value with",
      "offset": 1410.08,
      "duration": 8.599
    },
    {
      "lang": "en",
      "text": "strruct field defining the name and data",
      "offset": 1413.44,
      "duration": 8.84
    },
    {
      "lang": "en",
      "text": "type. Let's go see these SQL like",
      "offset": 1418.679,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "actions in",
      "offset": 1422.28,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "practice. Now we'll take a look at",
      "offset": 1427.64,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "userdefined functions or UDFs in",
      "offset": 1430.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Pispark.",
      "offset": 1434.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "By the end of this video, we'll have a",
      "offset": 1435.6,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "thorough understanding of what UDFs are",
      "offset": 1438.32,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "and how to create them. A UDF is a",
      "offset": 1441.679,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "custom function we create to work with",
      "offset": 1445.76,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "data using PiSpark data frames. We'll",
      "offset": 1448.88,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "discuss two kinds of UDFs. PiSpark UDFs",
      "offset": 1452.799,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "and Pandas UDFs.",
      "offset": 1456.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "The main difference is the size of the",
      "offset": 1459.52,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "data set they are designed to",
      "offset": 1462.559,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "handle. There are some differences in",
      "offset": 1465.08,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "execution but they operate very",
      "offset": 1468.2,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "similarly. They are both reusable and",
      "offset": 1471.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "repeatable because they are registered",
      "offset": 1475.08,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "directly to the spark session.",
      "offset": 1477.559,
      "duration": 7.561
    },
    {
      "lang": "en",
      "text": "We have the two types of UDFs. Pandas",
      "offset": 1481.279,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "UDFs for large data sets and PISpark",
      "offset": 1485.12,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "UDFs for smaller data",
      "offset": 1488.559,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "sets. This may seem",
      "offset": 1491.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "counterintuitive, but it is due to how",
      "offset": 1495.24,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "each type of UDF handles data on a",
      "offset": 1497.76,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "rowbyrow basis.",
      "offset": 1501.799,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "It's outside our scope here, but well",
      "offset": 1504.64,
      "duration": 8.279
    },
    {
      "lang": "en",
      "text": "worth exploring as we get more PIS spark",
      "offset": 1507.6,
      "duration": 9.439
    },
    {
      "lang": "en",
      "text": "experience. Now, how do we create a UDF?",
      "offset": 1512.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "Let's see how it's",
      "offset": 1517.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "done. First, we create a regular Python",
      "offset": 1518.52,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "function called to uppercase to convert",
      "offset": 1521.679,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "all strings in a column to uppercase.",
      "offset": 1525.679,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "Next, we register the function as a UDF",
      "offset": 1529.36,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "using PiSpark's UDF function and pass",
      "offset": 1533.2,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "the correct data type method. We're",
      "offset": 1537.6,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "doing string",
      "offset": 1541.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "operations. So, we use string type",
      "offset": 1542.6,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "without registering the UDF would not be",
      "offset": 1546,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "available to all the worker nodes of the",
      "offset": 1549.279,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "Spark session. Lastly, we apply it to",
      "offset": 1552.48,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "the dataf frame df and show the",
      "offset": 1556.4,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "results. While pi spark udfs are",
      "offset": 1560.2,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "incredibly useful, they can also",
      "offset": 1563.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "introduce performance overhead because",
      "offset": 1565.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "pispark has to do a series of",
      "offset": 1568.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "inefficient conversions causing",
      "offset": 1570.799,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "frustrating performance",
      "offset": 1573.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "problems. To mitigate this, pispark has",
      "offset": 1575.72,
      "duration": 7.959
    },
    {
      "lang": "en",
      "text": "introduced pandas udfs. Here's an",
      "offset": 1579.44,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "example of a pandas UDF which is defined",
      "offset": 1583.679,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "as a pandas",
      "offset": 1587.6,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "udf. Two things to notice here. One, we",
      "offset": 1590.679,
      "duration": 8.401
    },
    {
      "lang": "en",
      "text": "have to import the pandas udf function.",
      "offset": 1594.799,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "Second, we have to use the decorator at",
      "offset": 1599.08,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "to define the data type using at",
      "offset": 1602.559,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "pandanda's UDF",
      "offset": 1606.08,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "flopp which is a way to take a action at",
      "offset": 1608.2,
      "duration": 9
    },
    {
      "lang": "en",
      "text": "a function as an argument. We also don't",
      "offset": 1613.279,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "need to register it to the spark",
      "offset": 1617.2,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "session. So when should we use pispark",
      "offset": 1620.919,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "udfs and when should we opt for pandas",
      "offset": 1624.08,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "udfs? If we are working with small data",
      "offset": 1626.88,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "sets or simple",
      "offset": 1630.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "transformations, pispark udfs will",
      "offset": 1633.08,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "suffice. We use them on the columner",
      "offset": 1636.36,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "level and register directly with the",
      "offset": 1639.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "spark",
      "offset": 1642.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "session. Registering to a spark session",
      "offset": 1643.96,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "will make the UDF work with all nodes of",
      "offset": 1646.799,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "the spark",
      "offset": 1650.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "cluster. However, for large data sets,",
      "offset": 1651.48,
      "duration": 7.559
    },
    {
      "lang": "en",
      "text": "Pandas UDFs are preferred due to their",
      "offset": 1655.919,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "superior performance at scale and",
      "offset": 1659.039,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "incorporate the code outside the Spark",
      "offset": 1662.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "session.",
      "offset": 1665.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "There are many scenarios where we would",
      "offset": 1666.88,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "consider a pispark udf over a pandas udf",
      "offset": 1669.44,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "but they involve a costbenefit analysis",
      "offset": 1674.559,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of compute cost development environment",
      "offset": 1677.76,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "data size and data type that are outside",
      "offset": 1680.799,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "the scope of this",
      "offset": 1684.399,
      "duration": 8.361
    },
    {
      "lang": "en",
      "text": "course. Let's go practice UDFs.",
      "offset": 1686.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Let's explore a foundational component",
      "offset": 1694.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 1697.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "PiSpark, resilient distributed data",
      "offset": 1698.36,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "sets,",
      "offset": 1701.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "RDDDs. One of Pispark's greatest",
      "offset": 1703.799,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "strengths is its ability to handle",
      "offset": 1706.64,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "largecale data processing through",
      "offset": 1709.279,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "parallelization, which splits data and",
      "offset": 1713.08,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "computations across multiple nodes in a",
      "offset": 1715.96,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "cluster.",
      "offset": 1719.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Operations defined in Spark are",
      "offset": 1721.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "automatically distributed, enabling",
      "offset": 1723.679,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "efficient processing of large data",
      "offset": 1726.48,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "sets. Tasks are assigned to worker nodes",
      "offset": 1730.52,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "that process data in parallel with",
      "offset": 1734.24,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "results combined at the",
      "offset": 1737.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "end. This approach allows for efficient",
      "offset": 1740.2,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "processing at scale, accommodating",
      "offset": 1743.2,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "gigabytes or even terabytes of data.",
      "offset": 1745.84,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "RDDs are the core building blocks of",
      "offset": 1749.88,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "Spark, representing distributed",
      "offset": 1752.799,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "collections of data across a",
      "offset": 1755.12,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "cluster. While RDDDs enable fast access",
      "offset": 1757.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 1762.159,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "analysis, data frames offer greater user",
      "offset": 1763.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "friendliness due to their simpler",
      "offset": 1766.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "syntax, although they can be",
      "offset": 1768.6,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "slower. RDDs are immutable, meaning once",
      "offset": 1771.64,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "created, it cannot be changed. But a new",
      "offset": 1776.44,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "RDD can be created using operations like",
      "offset": 1780.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "map and",
      "offset": 1784.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "filter. They also support actions like",
      "offset": 1785.48,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "collect which retrieves the results of",
      "offset": 1788.24,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "RDD",
      "offset": 1791.039,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "operations. Let's create an RDD using a",
      "offset": 1793.48,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "CSV. We'll load the data frame from the",
      "offset": 1797.72,
      "duration": 8.199
    },
    {
      "lang": "en",
      "text": "CSV, then use the RDD method, convert it",
      "offset": 1801.159,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "to an RDD.",
      "offset": 1805.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "The data is distributed across the",
      "offset": 1808.32,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "cluster when the RDD is",
      "offset": 1810.88,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "created. Here we create an RDD from a",
      "offset": 1813.96,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "CSV file and use a collect action to",
      "offset": 1818,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "retrieve and display the",
      "offset": 1821.52,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "data. As you can see, collect shows a",
      "offset": 1823.88,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "summary of the processes that were done.",
      "offset": 1828.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "It also it is also rather verbose to do",
      "offset": 1831.36,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "a print",
      "offset": 1835.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "statement. As you can see, collect shows",
      "offset": 1836.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "a summary of the processes that were",
      "offset": 1840.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "done. It is also rather verbose to do a",
      "offset": 1842.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "print",
      "offset": 1845.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "statement. RDDs offer a lowlevel",
      "offset": 1847.32,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "interface providing maximum flexibility.",
      "offset": 1850.799,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "You can manipulate data at a granular",
      "offset": 1854.64,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "level, but this flexibility comes at the",
      "offset": 1857.039,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "cost of requiring more lines of code for",
      "offset": 1860.399,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "even moderately complex",
      "offset": 1864.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "operations. One strength of RDDs is",
      "offset": 1867,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "their ability to preserve data types",
      "offset": 1870.32,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "across operations.",
      "offset": 1872.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "However, they lack the schema",
      "offset": 1875.64,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "optimizations of data frames, which",
      "offset": 1878.48,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "means operations on structured data are",
      "offset": 1881.52,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "less efficient and harder to",
      "offset": 1884.399,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "express. While RDDDs can scale to handle",
      "offset": 1887.88,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "large data sets, they're not optimized",
      "offset": 1891.36,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "for analytics like data frames.",
      "offset": 1894.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Data frames are operated for ease of",
      "offset": 1898.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "use, providing a high-level abstraction",
      "offset": 1900.64,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "for working with data. They encapsulate",
      "offset": 1904.32,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "complex computations, making it easier",
      "offset": 1908.159,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "to achieve our objective with less code",
      "offset": 1910.799,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "and fewer",
      "offset": 1914.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "errors. One of the standout features of",
      "offset": 1916.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "dataf frames is their SQL like",
      "offset": 1919.279,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "functionality. With SQL syntax, even",
      "offset": 1921.72,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "complex transformations and analysis can",
      "offset": 1925.039,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "be performed in just a few lines of",
      "offset": 1928.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "code. Data frames come with built-in",
      "offset": 1932.279,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "schema awareness, meaning they contain",
      "offset": 1935.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "column names and data types just like a",
      "offset": 1938.64,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "structured table in",
      "offset": 1941.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "SQL. Here are a handful of useful",
      "offset": 1944.2,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "functions you'll be seeing in the",
      "offset": 1946.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "following exercises.",
      "offset": 1948.559,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "Map is useful to apply a function to an",
      "offset": 1950.88,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "RDD. This can include a lambda function",
      "offset": 1955.32,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "or any other function defined or",
      "offset": 1958.159,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "imported",
      "offset": 1960.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "elsewhere. Collect gathers data across",
      "offset": 1961.64,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "the cluster using the parallelization of",
      "offset": 1964.72,
      "duration": 8.559
    },
    {
      "lang": "en",
      "text": "pispark. Let's go look at RDDs and data",
      "offset": 1969,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "frames in practice.",
      "offset": 1973.279,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "Welcome back. We'll now learn how Spark",
      "offset": 1978.799,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "SQL enables us to use Python and SQL in",
      "offset": 1982,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "the same PIS spark",
      "offset": 1986.159,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "environment. Let's explore Spark SQL, a",
      "offset": 1988.36,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "powerful component of Apache Spark that",
      "offset": 1992.32,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "integrates seamlessly with its",
      "offset": 1995.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "ecosystem. It enables processing of",
      "offset": 1998.039,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "structured and semistructured data using",
      "offset": 2000.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "SQL syntax.",
      "offset": 2004,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "Why choose Spark SQL over other",
      "offset": 2006.24,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "tools? First, it leverages Spark's",
      "offset": 2009.64,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "distributed computing power, handling",
      "offset": 2012.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "massive data sets effortlessly by",
      "offset": 2015.76,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "distributing computations across a",
      "offset": 2018.24,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "cluster. Second, Spark SQL simplifies",
      "offset": 2021.64,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "querying with familiar SQL syntax,",
      "offset": 2025.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "making it accessible to analysts and",
      "offset": 2028.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "engineers.",
      "offset": 2031.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Third, its integration with PISpark",
      "offset": 2033.2,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "dataf frames enables blending SQL and",
      "offset": 2036.32,
      "duration": 8.12
    },
    {
      "lang": "en",
      "text": "programmatic operations for complex",
      "offset": 2040.399,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "workflows. In short, Spark SQL delivers",
      "offset": 2044.44,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "the flexibility, speed, and scalability",
      "offset": 2047.76,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "needed for working with large diverse",
      "offset": 2052,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "data sets.",
      "offset": 2055.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "To work with SQL in Pispark, start by",
      "offset": 2057.2,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "registering a dataf frame as a temporary",
      "offset": 2060.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "view. We can use the create dataf frame",
      "offset": 2063.119,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "method to make the data frame or load",
      "offset": 2066.32,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "the data from a flat file.",
      "offset": 2070.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "This allows interaction with the dataf",
      "offset": 2073.119,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "frame using SQL",
      "offset": 2075.839,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "syntax through the create or replace",
      "offset": 2078.599,
      "duration": 7.881
    },
    {
      "lang": "en",
      "text": "temp view method where you pass the view",
      "offset": 2082.48,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "name as a string. For example, we'll",
      "offset": 2086.48,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "create a view called people.",
      "offset": 2090.399,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Temporary views exist only for the",
      "offset": 2093.76,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "current session, making them ideal for",
      "offset": 2097.119,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "quick sessionbased",
      "offset": 2100.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "exploration. For instance, to find",
      "offset": 2103.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "people older than 30, use the",
      "offset": 2106,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "spark.sql function and pass your SQL",
      "offset": 2109.56,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "query as a string.",
      "offset": 2112.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "This query retrieves the name and age",
      "offset": 2115.599,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "columns from the people view filtering",
      "offset": 2118.96,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "to return only rows where age exceeds",
      "offset": 2121.839,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "30. The result is another data frame",
      "offset": 2125.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "which you can continue processing",
      "offset": 2128.88,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "pispark outside the",
      "offset": 2131.24,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "session. Let's explore how to load data",
      "offset": 2134.2,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "from CSV files into dataf frames.",
      "offset": 2138.16,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "register them as temporary views, run",
      "offset": 2141.68,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "SQL queries, and blend SQL with dataf",
      "offset": 2145.119,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "frame commands for advanced",
      "offset": 2148.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "manipulation. To query a dataf frame",
      "offset": 2151.96,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "with SQL, we first load the data into a",
      "offset": 2154.72,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "dataf frame using methods like spark",
      "offset": 2158.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "read csv.",
      "offset": 2162.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "After loading, we register the dataf",
      "offset": 2165.2,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "frame as a temporary view allowing",
      "offset": 2167.839,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "SQLbased",
      "offset": 2170.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "interaction while safeguarding the",
      "offset": 2172.28,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "underlying data and data source during",
      "offset": 2174.48,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "analysis. These views are session scoped",
      "offset": 2179.24,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "lasting only as long as a spark session",
      "offset": 2183.04,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "is active. For extended use, global",
      "offset": 2185.68,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "temporary views or permanent tables must",
      "offset": 2189.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "be created.",
      "offset": 2193.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "One of the most powerful aspects of",
      "offset": 2195.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Spark SQL is its seamless integration",
      "offset": 2197.839,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "with dataf frame",
      "offset": 2201.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "operations. After running a SQL query,",
      "offset": 2203.079,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "the results are returned as a dataf",
      "offset": 2206.72,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "frame allowing us to apply additional",
      "offset": 2208.96,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "transformations",
      "offset": 2211.8,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "programmatically. Here we use SQL to",
      "offset": 2213.8,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "filter high earning employees and then",
      "offset": 2216.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "added a bonus column using dataf frame",
      "offset": 2219.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "operations.",
      "offset": 2222.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Let's go seamlessly blend SQL and PIS",
      "offset": 2224.96,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "Spark in single",
      "offset": 2228.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "scripts. Welcome back. Let's take a look",
      "offset": 2234.44,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "at some complex PISPARC",
      "offset": 2237.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "aggregations. PiSpark SQL provides a",
      "offset": 2240.839,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "suite of built-in aggregation functions",
      "offset": 2244,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "for summarizing data.",
      "offset": 2246.16,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "Commonly used functions include sum,",
      "offset": 2248.96,
      "duration": 8.879
    },
    {
      "lang": "en",
      "text": "count, average, max, and min. These are",
      "offset": 2253.52,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "applied using either SQL queries with",
      "offset": 2257.839,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "spark.sql or the dataf frame",
      "offset": 2261,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "toolkit. Let's see an example of",
      "offset": 2264.04,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "calculating the total and average salary",
      "offset": 2266.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "for employees in different departments",
      "offset": 2269.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "using SQL syntax.",
      "offset": 2272.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "PiSpark allows us to mix SQL operations",
      "offset": 2275.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "with the dataf frame interface for",
      "offset": 2278.88,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "greater",
      "offset": 2281.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "flexibility. For instance, we can filter",
      "offset": 2283.079,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "and pre-process data using dataf frame",
      "offset": 2286,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "operations before applying SQL",
      "offset": 2289.88,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "aggregations. This combination leverages",
      "offset": 2293.96,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "the strengths of both approaches.",
      "offset": 2296.96,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "By combining these two tools, we get the",
      "offset": 2300.88,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "best of both worlds, the expressiveness",
      "offset": 2304.48,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "of SQL and the programmatic control of",
      "offset": 2307.76,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "the pandas data frame. Notice how in",
      "offset": 2311.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "each step we are registering a new",
      "offset": 2315.52,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "temporary view. This lets us easily roll",
      "offset": 2318,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "back changes and catch errors before it",
      "offset": 2322.079,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "hits our data",
      "offset": 2326.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "store. While performing aggregations,",
      "offset": 2327.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "data type mismatches can lead to errors",
      "offset": 2330.96,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "or unexpected results. For instance,",
      "offset": 2333.76,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "numerical data stored as strings might",
      "offset": 2337.599,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "not aggregate",
      "offset": 2341.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "correctly. PiSpark provides functions",
      "offset": 2342.76,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "and methods like cast to convert data",
      "offset": 2345.68,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "types before processing.",
      "offset": 2349.119,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "We should always validate and",
      "offset": 2352.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "standardize our data types before",
      "offset": 2354.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "running",
      "offset": 2357.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "aggregations. This protects our pipeline",
      "offset": 2358.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "from costly",
      "offset": 2361.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "errors. Aggregations are a cornerstone",
      "offset": 2364.28,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "of data and analytics, helping us",
      "offset": 2367.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "summarize and gain insights from large",
      "offset": 2370.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "data sets.",
      "offset": 2373.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "PiSpark provides powerful aggregation",
      "offset": 2375.599,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "functions through its SQL interface and",
      "offset": 2378.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "the dataf frame",
      "offset": 2381.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "API. Now we'll explore some of these",
      "offset": 2382.68,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "common pi spark SQL functions like sum",
      "offset": 2386.64,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 2390.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "average. While RDDDs are useful for",
      "offset": 2391.72,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "certain use cases involving scale and",
      "offset": 2395.2,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "data movement across",
      "offset": 2398,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "clusters, data frames are the preferred",
      "offset": 2400.359,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "choice for most modern PISPARC analytics",
      "offset": 2403.04,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "applications due to their simplicity of",
      "offset": 2406.68,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "syntax. RDDs, as a general rule, do not",
      "offset": 2410.76,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "have simple code syntax for aggregation",
      "offset": 2414.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "or analytics.",
      "offset": 2417.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "The code we're seeing now does the same",
      "offset": 2419.119,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "as the aggregations we saw earlier with",
      "offset": 2421.44,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "an RDD. The best way to do this is with",
      "offset": 2425.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "a lambda function, a small",
      "offset": 2428.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "transformation function with very",
      "offset": 2431.44,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "limited scope that is targeted to our",
      "offset": 2433.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "circumstance. And then we apply the new",
      "offset": 2436.839,
      "duration": 7.641
    },
    {
      "lang": "en",
      "text": "lambda function with the RDD map method.",
      "offset": 2439.599,
      "duration": 8.401
    },
    {
      "lang": "en",
      "text": "The reduce by key method applies the",
      "offset": 2444.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "function previously defined to the",
      "offset": 2448,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "entire data",
      "offset": 2450.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "frame. It's a lot. As we can see, the",
      "offset": 2452.2,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "verbosity of RDDs for analytics requires",
      "offset": 2456.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the writing of custom functions and",
      "offset": 2459.92,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "multiple lines to apply it.",
      "offset": 2462.56,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "compared to the single line application",
      "offset": 2466.4,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "that we saw with dataf",
      "offset": 2469.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "frames. Here are some best practices for",
      "offset": 2472.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "pi spark",
      "offset": 2475.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "aggregations. Filter early to reduce",
      "offset": 2477.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "data size before performing",
      "offset": 2480.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "aggregations. Ensure data is clean and",
      "offset": 2483.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "correctly",
      "offset": 2486.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "typed. Avoid using the entire data set",
      "offset": 2487.88,
      "duration": 7.959
    },
    {
      "lang": "en",
      "text": "by minimizing operations like group by.",
      "offset": 2491.52,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "Choose the right tool by preferring data",
      "offset": 2495.839,
      "duration": 7.721
    },
    {
      "lang": "en",
      "text": "frames for most tasks due to their",
      "offset": 2498.96,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "optimizations. Monitor performance by",
      "offset": 2503.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "using the",
      "offset": 2506.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "explain to inspect the execution plan",
      "offset": 2507.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and optimize",
      "offset": 2510.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "accordingly. Here are some key",
      "offset": 2513.4,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "takeaways. Let's go check out some PIS",
      "offset": 2516.52,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "spark aggregations.",
      "offset": 2519.839,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "Welcome back. Let's talk about what",
      "offset": 2525.359,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "happens when we use PiSpark at",
      "offset": 2528.079,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "scale. As our data grows, optimizing",
      "offset": 2531.64,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "PISpark jobs becomes essential for",
      "offset": 2535.04,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "managing performance, resource usage,",
      "offset": 2538.16,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "and execution",
      "offset": 2541.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "speed. In this video, we'll focus on",
      "offset": 2543.4,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "techniques to scale Pispark workflows",
      "offset": 2546.64,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "effectively.",
      "offset": 2549.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "First, we'll explore how to interpret",
      "offset": 2551.319,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "Spark execution plans to identify",
      "offset": 2554.079,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "performance",
      "offset": 2557.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "bottlenecks. Next, we'll discuss caching",
      "offset": 2558.28,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "and persisting data frames, which can",
      "offset": 2561.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "significantly speed up iterative",
      "offset": 2563.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "queries. Finally, we'll cover best",
      "offset": 2566.76,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "practices for optimizing PiSpark jobs,",
      "offset": 2569.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "ensuring we make the most of Spark's",
      "offset": 2573.359,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "distributed computing power.",
      "offset": 2575.76,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "Scaling Pispark is not just about faster",
      "offset": 2579.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "results. It's about building workflows",
      "offset": 2583.079,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "that are efficient, maintainable, and",
      "offset": 2585.599,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "capable of handling large data sets with",
      "offset": 2588.24,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "ease. Methods like broadcast will load a",
      "offset": 2592.839,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "smaller data set across the cluster",
      "offset": 2596.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "using all available compute. As we work",
      "offset": 2599.599,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "more with Pispark, the more we will need",
      "offset": 2603.04,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "to think in moderately abstract ways.",
      "offset": 2606,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "A key tool for optimizing pi spark jobs",
      "offset": 2610.56,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "is understanding spark execution",
      "offset": 2614,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "plans. Whenever we run an operation on a",
      "offset": 2617.96,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "dataf frame, spark constructs a logical",
      "offset": 2621.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "plan which is then optimizes into a",
      "offset": 2623.92,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "physical plan. This physical plan",
      "offset": 2627.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "outlines how spark will execute the task",
      "offset": 2630.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "on its distributed cluster.",
      "offset": 2633.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "We can inspect the process using the",
      "offset": 2637.44,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "explain method. It gives us a breakdown",
      "offset": 2639.92,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "of the query's execution plan showing",
      "offset": 2643.599,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "the operations performed at each",
      "offset": 2646.56,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "stage. This outputs a logical and",
      "offset": 2650.839,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "physical plan. By analyzing these plans,",
      "offset": 2653.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "we can spot inefficiencies like",
      "offset": 2657.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "redundant shuffles or unoptimized joins",
      "offset": 2659.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "and address them before running the job.",
      "offset": 2662.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "The explain method details the logical",
      "offset": 2666.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "and physical plans for",
      "offset": 2669.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "optimization, showing what the code ran",
      "offset": 2671.96,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "and what hardware it used. This outlines",
      "offset": 2674.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "how to improve",
      "offset": 2678.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it. When working with large data sets,",
      "offset": 2680.76,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "we often reuse intermediate results",
      "offset": 2684.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "across multiple operations.",
      "offset": 2687.68,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Recomputing these results can be costly,",
      "offset": 2690.8,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "especially when reading from",
      "offset": 2694.88,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "disk. To avoid this, we have two tools",
      "offset": 2697.88,
      "duration": 7.479
    },
    {
      "lang": "en",
      "text": "that keeps data readily available,",
      "offset": 2701.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "caching and persisting.",
      "offset": 2705.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "The cache method stores a dataf frame in",
      "offset": 2707.92,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "memory for fast use while the persist",
      "offset": 2710.96,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "method offers flexibility by letting us",
      "offset": 2714.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "choose storage",
      "offset": 2718.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "levels. Here the data set is read once",
      "offset": 2720.28,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "and subsequent operations reuse the",
      "offset": 2724,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "cached",
      "offset": 2727.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "version. The second option is using the",
      "offset": 2728.44,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "persist method which allows us greater",
      "offset": 2731.76,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "control of storage levels. We can use",
      "offset": 2734.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "disk when memory is",
      "offset": 2738.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "insufficient. This is especially useful",
      "offset": 2740.44,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "for longunning jobs on large",
      "offset": 2743.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "clusters. However, caching consumes",
      "offset": 2746.599,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "memory. So, it's important to uncach",
      "offset": 2749.76,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "data when it's no longer needed using",
      "offset": 2752.96,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "the unpersist",
      "offset": 2756.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "method. In this example, if the data",
      "offset": 2758.28,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "frame doesn't fit in memory, Spark",
      "offset": 2761.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "writes the overflow to disk. This",
      "offset": 2763.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "ensures our operations can still proceed",
      "offset": 2766.56,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "without crashing due to resource",
      "offset": 2769.52,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "constraints. This concept is difficult",
      "offset": 2773.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "for us to demonstrate in this",
      "offset": 2776.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "course, but you will encounter it as you",
      "offset": 2778.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "work with PiSpark in the real",
      "offset": 2781.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "world. Beyond caching and persistence,",
      "offset": 2784.599,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "here are a few best practices to",
      "offset": 2787.92,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "optimize PiSpark jobs.",
      "offset": 2790.4,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "Use small",
      "offset": 2794.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "subsections. Favor targeted functions",
      "offset": 2797.319,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "and methods like map over whole data set",
      "offset": 2799.76,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "tools like group by that require",
      "offset": 2802.48,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "shuffles. Broadcast joins for small data",
      "offset": 2806.2,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "sets. Use broadcast to load the data",
      "offset": 2809.839,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "onto all the nodes and avoid",
      "offset": 2812.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "shuffles. Lastly, avoid repeated",
      "offset": 2815.8,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "actions.",
      "offset": 2818.56,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "Operations like count or show trigger",
      "offset": 2820.079,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "jobs store intermediate results to",
      "offset": 2823.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "prevent",
      "offset": 2826.64,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "recomputation. Let's go look at these",
      "offset": 2828.92,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "ideas in",
      "offset": 2831.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "practice. Congratulations on reaching",
      "offset": 2837.079,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "the end of the course. You've come a",
      "offset": 2839.92,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "long way building up foundational skills",
      "offset": 2843.119,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "and exploring powerful tools for big",
      "offset": 2847.68,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "data processing with",
      "offset": 2851.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Pispark. Starting from an overview of",
      "offset": 2853.96,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "Apache Spark's architecture, you dove",
      "offset": 2857.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "into the fundamentals of distributed",
      "offset": 2860.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "data processing.",
      "offset": 2863.04,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "You learned about resilient data sets,",
      "offset": 2865.04,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "RDDDs, transformations, and",
      "offset": 2868.92,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "actions, key concepts that power",
      "offset": 2871.48,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "Pispark's ability to handle massive data",
      "offset": 2874.8,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "sets effectively.",
      "offset": 2879.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "You also navigated the world of dataf",
      "offset": 2882.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "frames, a critical pipark component for",
      "offset": 2884.44,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "handling structured data, and discovered",
      "offset": 2887.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "the flexibility of PISpark SQL for SQL",
      "offset": 2890.319,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 2893.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "operations. With this foundation, you",
      "offset": 2894.599,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "now understand how to filter, aggregate,",
      "offset": 2897.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and join data, making complex data",
      "offset": 2900.079,
      "duration": 8.721
    },
    {
      "lang": "en",
      "text": "wrangling tasks intuitive and scalable.",
      "offset": 2903.44,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "In addition to these core skills, you",
      "offset": 2908.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "ventured into advanced topics like",
      "offset": 2912.4,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "userdefined functions within",
      "offset": 2915.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Pispark. These tools allow you to extend",
      "offset": 2918.76,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "PiSpark's capabilities from applying",
      "offset": 2922.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "custom transformations to building and",
      "offset": 2925.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "running machine learning models on",
      "offset": 2928.96,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "distributed data sets.",
      "offset": 2931.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "However, there is still more to explore",
      "offset": 2934.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "in the PiSpark",
      "offset": 2937.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "ecosystem. This course didn't cover",
      "offset": 2939.559,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "topics such as advanced cluster",
      "offset": 2942.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "configuration, performance optimization,",
      "offset": 2945.319,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "big data applications, and streaming",
      "offset": 2948.16,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "data processing with PISpark streaming.",
      "offset": 2950.64,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "Additionally, deep dives into Spark's",
      "offset": 2954.839,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "machine learning pipelines and",
      "offset": 2958.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "integration with cloud-based tools were",
      "offset": 2960.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "beyond the scope of this course. These",
      "offset": 2963.68,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "are advanced topics you can and should",
      "offset": 2966.4,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "explore as you continue your Pispark",
      "offset": 2969.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "journey.",
      "offset": 2972.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Whether you're a data engineer, a data",
      "offset": 2974.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "scientist, or a machine learning",
      "offset": 2976.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "engineer, you now have the skills to",
      "offset": 2978.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "leverage Pispark for managing and",
      "offset": 2981.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "analyzing big",
      "offset": 2984.079,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "data. Thank you for joining me on this",
      "offset": 2985.8,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "journey. I'm Benjamin Schmidt and",
      "offset": 2989.319,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "congratulations again on mastering the",
      "offset": 2992.079,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "foundations of Pispark. Goodbye and good",
      "offset": 2995.16,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "luck.",
      "offset": 2999.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Welcome to the first video of big data",
      "offset": 3003.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "fundamentals via pispark course. My name",
      "offset": 3005.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "is Upendra Devishetti and I'm a science",
      "offset": 3008.16,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "analyst at",
      "offset": 3010.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "cywords. Let's get started. What exactly",
      "offset": 3011.64,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "is big data? There's no single",
      "offset": 3014.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "definition of big data because projects,",
      "offset": 3017.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "vendors, practitioners and business",
      "offset": 3019.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "professionals use it quite differently.",
      "offset": 3021.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "According to Wikipedia, big data is a",
      "offset": 3023.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "term used to refer to the study and",
      "offset": 3026.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "applications of data sets that are too",
      "offset": 3028.88,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "complex for traditional data processing",
      "offset": 3031.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "software. There are three Vs of big data",
      "offset": 3034.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that are used to describe its",
      "offset": 3036.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "characteristics. They are volume,",
      "offset": 3038.68,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "velocity and variety. Volume refers to",
      "offset": 3041.119,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the size of the data. Variety refers to",
      "offset": 3044.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the different sources and formats of",
      "offset": 3047.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "data. Velocity is the speed at which",
      "offset": 3049.119,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "data is generated and available for",
      "offset": 3051.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "processing. Now let's take a look at",
      "offset": 3054.599,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "some of the concepts and terminology of",
      "offset": 3056.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "big data. Clustered computing is the",
      "offset": 3058.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "pooling of resources of multiple",
      "offset": 3061.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "machines to complete jobs. Parallel",
      "offset": 3063.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "computing is a type of computation in",
      "offset": 3066.96,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "which many calculations are carried out",
      "offset": 3069.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "simultaneously. A distributed computing",
      "offset": 3071.64,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "involves nodes or network computers that",
      "offset": 3074,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "run jobs in parallel.",
      "offset": 3076.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Back processing refers to the breaking",
      "offset": 3079.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "data into small pieces and running each",
      "offset": 3081.28,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "piece on an individual",
      "offset": 3084,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "machine. Realtime processing demands",
      "offset": 3085.72,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "that information is processed and made",
      "offset": 3088.16,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "ready",
      "offset": 3090.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "immediately. There are two popular",
      "offset": 3091.559,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "frameworks for big data processing. The",
      "offset": 3093.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "first is the highly successful Hadoop",
      "offset": 3096.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "map reduce framework. Hadoop map reduce",
      "offset": 3098.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "framework is open-source and scalable",
      "offset": 3101.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "framework for batch data. The second is",
      "offset": 3104.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the most popular Apache Spark which is a",
      "offset": 3107.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "parallel framework for storing and",
      "offset": 3110,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "processing of big data across clustered",
      "offset": 3111.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "computers. It is also open-source and is",
      "offset": 3114.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "suited for both batch and real-time data",
      "offset": 3117.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "processing. In this course, you learn",
      "offset": 3120.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "about Apache",
      "offset": 3122.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Spark. Let's talk about the main",
      "offset": 3124.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "features of Apache Spark.",
      "offset": 3126.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Spark distributes data and computation",
      "offset": 3129.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "across multiple computers executing",
      "offset": 3131.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "complex multi-stage applications such as",
      "offset": 3134.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "machine learning. Spark runs most",
      "offset": 3136.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "computations in memory and thereby",
      "offset": 3139.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "provides better performance for",
      "offset": 3141.2,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "applications such as interactive data",
      "offset": 3143.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "mining. Spark helps to run an",
      "offset": 3145.72,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "application up to 100 times faster in",
      "offset": 3148.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "memory and 10 times faster when running",
      "offset": 3150.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "on disk.",
      "offset": 3152.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Spark is mainly written in Scala",
      "offset": 3154.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "language but also have support for Java,",
      "offset": 3156.16,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Python, R and",
      "offset": 3158.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "SQL. Apache Spark is a powerful",
      "offset": 3161.16,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "alternative to Hadoop map reduce with",
      "offset": 3163.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "rich features like machine learning,",
      "offset": 3166.319,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "realtime stream processing and graph",
      "offset": 3168.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "computations. At the center of the",
      "offset": 3170.68,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "ecosystem is the Spark core which",
      "offset": 3172.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "contains the basic functionality of",
      "offset": 3174.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Spark. The rest of Spark's libraries are",
      "offset": 3176.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "built on top of it.",
      "offset": 3179.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "First is SparkSQL which is a library for",
      "offset": 3181.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "processing structured and",
      "offset": 3184.559,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "semi-structured data in Python, Java and",
      "offset": 3185.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Scala. The second is MLI which is a",
      "offset": 3188.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "library of common machine learning",
      "offset": 3191.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "algorithms. The third component is",
      "offset": 3194.28,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "graphics which is a collection of",
      "offset": 3196.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "algorithms and tools for manipulating",
      "offset": 3198.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "graphs and performing parallel graph",
      "offset": 3201.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "computations.",
      "offset": 3203.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Finally, Spark streaming is a scalable",
      "offset": 3205.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "high throughput processing library for",
      "offset": 3207.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "realtime data. In this course, you'll",
      "offset": 3209.92,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "learn about SparkSQL and",
      "offset": 3212.559,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "MLA. Spark can be run on two modes. The",
      "offset": 3215.4,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "first is the local mode where you can",
      "offset": 3218.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "run Spark on a single machine such as",
      "offset": 3220.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "your laptop. The local mode is very",
      "offset": 3222.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "convenient for testing, debugging, and",
      "offset": 3225.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "demonstration purposes. The second is",
      "offset": 3228.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the cluster mode where Spark is run on a",
      "offset": 3231.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "cluster. The cluster mode is mainly used",
      "offset": 3233.599,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 3236.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "production. The development workflow is",
      "offset": 3237.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "that you start on a local mode and",
      "offset": 3239.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "transition to cluster mode. During the",
      "offset": 3241.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "transition from local to cluster mode,",
      "offset": 3244.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "no code changes necessary. In this",
      "offset": 3247.119,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "course, you'll be using local mode. In",
      "offset": 3249.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the next video, you learn about Pispark,",
      "offset": 3253.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which is Python.",
      "offset": 3255.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "In the last video, you were introduced",
      "offset": 3257.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "to Apache Spark, which is a fast and",
      "offset": 3259.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "general purpose framework for big data",
      "offset": 3261.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "processing. Apache Spark provides highle",
      "offset": 3263.8,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "APIs in Scala, Java, Python, and R. In",
      "offset": 3266.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "this video, you'll learn about Pispark,",
      "offset": 3270.24,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "which is Spark's version of",
      "offset": 3272.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Python. Apache Spark is originally",
      "offset": 3274.119,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "written in Scola programming language.",
      "offset": 3276.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "To support Python with Spark, Pispark",
      "offset": 3278.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "was developed. Unlike previous versions,",
      "offset": 3281.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the newest version of Pispark provides",
      "offset": 3284.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "computation power similar to Scala. APIs",
      "offset": 3286.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "in Pispark are similar to pandas and",
      "offset": 3290.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "scikitlearn Python packages. Thus, the",
      "offset": 3292.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "entry-level barrier to Pispark is very",
      "offset": 3295.04,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "low for",
      "offset": 3297.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "beginners. Spark comes with interactive",
      "offset": 3298.52,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "shells that enable ad hoc data analysis.",
      "offset": 3300.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Spark shell is an interactive",
      "offset": 3304.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "environment through which one can access",
      "offset": 3305.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Spark's functionality quickly and",
      "offset": 3308.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "conveniently.",
      "offset": 3310.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Spark shell is particularly helpful for",
      "offset": 3312.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "fast interactive prototyping before",
      "offset": 3315.28,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "running the jobs on",
      "offset": 3317.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "clusters. Unlike most other shells,",
      "offset": 3319.4,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "Spark shell allows you to interact with",
      "offset": 3322.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "data that is distributed on disk or in",
      "offset": 3324.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "memory across many machines and Spark",
      "offset": 3327.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "take care of automatically distributing",
      "offset": 3330.24,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 3332.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "processing. Spark provides the shell in",
      "offset": 3333.16,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "three programming languages. Spark shell",
      "offset": 3335.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "for Scala, Pispark for Python and Spark",
      "offset": 3338.16,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 3341.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "R. Pispark shell is a Python based",
      "offset": 3341.96,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "command line tool to develop Sparks",
      "offset": 3344.72,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "interactive applications in",
      "offset": 3347.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "Python. Pispark helps data scientists",
      "offset": 3349.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "interface with Spark data structures in",
      "offset": 3352.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Apache Spark and Python. Similar to",
      "offset": 3354.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Scholar Shell, Pispark shell has been",
      "offset": 3358.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "augmented to support connecting to a",
      "offset": 3360.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "cluster. In this course, you'll use",
      "offset": 3362.4,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "Pispark",
      "offset": 3365.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "shell. In order to interact with Spark",
      "offset": 3366.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "using Pispark shell, you need an entry",
      "offset": 3369.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "point. Spark context is an entry point",
      "offset": 3371.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "to interact with underlying Spark",
      "offset": 3374.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "functionality. Before understanding",
      "offset": 3377.559,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "Spark context, let's understand what an",
      "offset": 3379.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "entry point is. An entry point is where",
      "offset": 3381.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "control is transferred from the",
      "offset": 3384.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "operating system to the provided",
      "offset": 3386.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "program. In simpler terms, it's like a",
      "offset": 3387.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "key to your house. Without the key, you",
      "offset": 3390.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "cannot enter the house. Similarly,",
      "offset": 3392.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "without an entry point, you cannot run",
      "offset": 3394.16,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "any pispark",
      "offset": 3396.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "jobs. You can access the spark context",
      "offset": 3397.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "in the pispark shell as a variable named",
      "offset": 3400.559,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "sc. Now, let's take a look at some of",
      "offset": 3403.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the important attributes of spark",
      "offset": 3406.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "context. The first is the version. This",
      "offset": 3408.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "attribute shows the version of spark",
      "offset": 3411.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that you're currently running. In this",
      "offset": 3413.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "example, SCV version shows that the",
      "offset": 3415.359,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "version of Spark is",
      "offset": 3417.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "2.3.1. The second is the Python version.",
      "offset": 3420.2,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "This attribute shows the version of",
      "offset": 3423.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Python that Spark is currently using. In",
      "offset": 3425.119,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "this example, SC.Python work shows that",
      "offset": 3428.319,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "Spark is using Python version",
      "offset": 3431.599,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "3.6. The final attribute is the master.",
      "offset": 3434.839,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "Master is the URL of the cluster or",
      "offset": 3438.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "local string to run in local mode.",
      "offset": 3440.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "In this example, sc.m master returns",
      "offset": 3443.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "local asterisk, meaning the spark",
      "offset": 3446.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "context access a master on a local node",
      "offset": 3448.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "using all available threads on the",
      "offset": 3451.52,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "computer where it is",
      "offset": 3453.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "running. You can load your raw data into",
      "offset": 3455.16,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "pi spark using spark context by two",
      "offset": 3457.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "different methods. The first is the",
      "offset": 3460.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "spark context paralyze method and a",
      "offset": 3462.799,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "list. For example, here is how to create",
      "offset": 3464.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "paralyze collections holding the numbers",
      "offset": 3468.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "1 to 5. The second is the spark context",
      "offset": 3470.64,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "text file method on a file. For example,",
      "offset": 3473.76,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "here is a way to load a text file named",
      "offset": 3476.799,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "test.txt using spark context text file",
      "offset": 3479.319,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "method. Now that you understand pispark,",
      "offset": 3482.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "let's write your first spark code",
      "offset": 3486.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "in. Understanding pispark becomes a lot",
      "offset": 3488.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "easier if you understand functional",
      "offset": 3491.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "programming principles in python. In",
      "offset": 3493.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this video, let's review some of the",
      "offset": 3495.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Python functions such as lambda, map,",
      "offset": 3497.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and filter. Python supports the creation",
      "offset": 3500.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of anonymous functions, that is",
      "offset": 3503.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "functions that are not bound to a name",
      "offset": 3505.359,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "at runtime using a construct called the",
      "offset": 3507.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "lambda. Lambda functions are very",
      "offset": 3510.28,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "powerful, well integrated into Python",
      "offset": 3512.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and are often used in conjugation with",
      "offset": 3515.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "typical functional concepts like map and",
      "offset": 3517.2,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "filter",
      "offset": 3519.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "functions. like dev the lambda creates a",
      "offset": 3520.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "function to be called later in the",
      "offset": 3524,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "program. However, it returns the",
      "offset": 3525.799,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "function instead of assigning it to a",
      "offset": 3528.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "name. This is why lambdas are known as",
      "offset": 3530.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "anonymous",
      "offset": 3533.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "functions. In practice, they are used as",
      "offset": 3534.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "a way to inline a function definition or",
      "offset": 3537.52,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "to defer execution of a",
      "offset": 3539.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "code. Lambda functions can be used",
      "offset": 3541.88,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "whenever function objects are required.",
      "offset": 3544.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "They can have any number of arguments",
      "offset": 3547.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "but only one expression and the",
      "offset": 3549.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "expression is evaluated and returned.",
      "offset": 3551.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "The general syntax of lambda function is",
      "offset": 3554.559,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "shown here. Here is an example of lambda",
      "offset": 3556.72,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "function. In this example, lambda x",
      "offset": 3559.96,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "asterisk 2. X is the argument and x",
      "offset": 3564.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "asterisk 2 is the expression that gets",
      "offset": 3567.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "evaluated and returned. This function",
      "offset": 3569.119,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "has no name. It returns a function",
      "offset": 3572,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "object which is assigned to the",
      "offset": 3574.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "identifier double. Here applying the",
      "offset": 3576.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "lambda function to a number such as",
      "offset": 3579.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "three returns six which is the double of",
      "offset": 3581.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the original number. Let's take a look",
      "offset": 3584.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "at the differences between deaf and",
      "offset": 3586.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "lambda. Here is the Python code to",
      "offset": 3588.92,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "illustrate cube of a number showing the",
      "offset": 3591.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "difference between normal Python",
      "offset": 3593.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "function using dev and anonymous",
      "offset": 3595.2,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "function using lambda. As you can see,",
      "offset": 3597.119,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "both deaf and lambda do exactly the",
      "offset": 3600.559,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "same. The main difference is that the",
      "offset": 3603.079,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "lambda definition does not include a",
      "offset": 3606.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "return statement and it always contains",
      "offset": 3608.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "an expression which is returned. Also",
      "offset": 3610.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "note that we can put a lambda definition",
      "offset": 3613.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "anywhere a function is expected and we",
      "offset": 3616.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "don't have to assign it to a variable at",
      "offset": 3618.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "all unlike normal Python function using",
      "offset": 3620.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "dev.",
      "offset": 3623.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "We use lambda functions when we require",
      "offset": 3625.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "a nameless function for a short period",
      "offset": 3627.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "of time. Most of the times we use",
      "offset": 3628.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "lambdas with built-in functions like map",
      "offset": 3631.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and filter. The map function is called",
      "offset": 3634.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "with all the items in the list and a new",
      "offset": 3637.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "list is returned which contains items",
      "offset": 3639.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "returned by that function for each",
      "offset": 3642,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "item. The general syntax of map function",
      "offset": 3644.52,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "is shown here. It takes in a function",
      "offset": 3647.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and a list. Here is an example of map",
      "offset": 3650.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "function with lambda to add the number",
      "offset": 3653.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "two to all the items in a list. The",
      "offset": 3655.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "result indicates that the number two is",
      "offset": 3658.64,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "added to 1 2 3 4 resulting in 3 4 5 6.",
      "offset": 3661.04,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "The filter function in Python takes in a",
      "offset": 3666.48,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "function and list as",
      "offset": 3668.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "argument. The function is called with",
      "offset": 3670.839,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "all the items in the list and a new list",
      "offset": 3673.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "is returned which contains items for",
      "offset": 3675.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "which the function evaluates to true.",
      "offset": 3677.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Here is a general syntax of filter",
      "offset": 3680.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "function in Python. Similar to map, it",
      "offset": 3682.64,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "takes in a function and a list as",
      "offset": 3685.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "arguments. Here is an example use of",
      "offset": 3688.119,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "filter with lambda to filter out only",
      "offset": 3690.799,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "odd numbers from a list. As shown in the",
      "offset": 3693.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "example, filtering the items list",
      "offset": 3696.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "containing numbers 1 2 3 4 resulted in 1",
      "offset": 3698.48,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "and three which are the only odd numbers",
      "offset": 3702.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for the input list.",
      "offset": 3705.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Lambda functions are incredibly useful",
      "offset": 3707.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and before going deep into pi spark",
      "offset": 3710.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "let's practice some lambda functions in",
      "offset": 3712.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in the first chapter you have learned",
      "offset": 3715.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "about the different components of spark",
      "offset": 3717.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "namely spark core spark sql and spark ml",
      "offset": 3719.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "in this chapter we'll start with",
      "offset": 3723.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "oddities which are spark's core",
      "offset": 3724.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "abstraction for working with data let's",
      "offset": 3726.64,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "get",
      "offset": 3729.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "started stands for resilient distributed",
      "offset": 3730.68,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "data sets it is simply a collection of",
      "offset": 3733.359,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "data distributed across the cluster. Odd",
      "offset": 3736,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is the fundamental and backbone data",
      "offset": 3739.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "type in PISpark. When Spark starts",
      "offset": 3741.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "processing data, it divides the data",
      "offset": 3744.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "into partitions and distributes the data",
      "offset": 3746.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "across cluster nodes with each node",
      "offset": 3748.799,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "containing a slice of",
      "offset": 3751.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "data. Now let's take a look at the",
      "offset": 3753.16,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "different features of RDD. The name RDD",
      "offset": 3755.599,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "captures three important properties.",
      "offset": 3758.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Resilient, which means the ability to",
      "offset": 3760.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "withstand failures and recomputee",
      "offset": 3762.96,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "missing or damaged",
      "offset": 3764.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "partitions. Distributed, which means",
      "offset": 3766.599,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "spanning the jobs across multiple nodes",
      "offset": 3768.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "in the cluster for efficient",
      "offset": 3770.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "computation. Data sets, which is a",
      "offset": 3772.92,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "collection of partition data, example,",
      "offset": 3775.28,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "arrays, tables, tupils or other",
      "offset": 3777.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "objects. There are three different",
      "offset": 3779.799,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "methods for creating RDDs. You have",
      "offset": 3781.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "already seen two methods in the previous",
      "offset": 3784.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "chapter even though you are not aware",
      "offset": 3785.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that you are creating RDDs.",
      "offset": 3787.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "The simplest method to create audits is",
      "offset": 3790.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to take an existing collection, example",
      "offset": 3792.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "a list, an array or a set and pass it to",
      "offset": 3794.64,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "Spark context paralyze method. A more",
      "offset": 3798,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "common way to create audit is to load",
      "offset": 3801.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "data from external data sets such as",
      "offset": 3803.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "files stored in HDFS or objects in",
      "offset": 3806,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Amazon S3 buckets or from lines in a",
      "offset": 3808.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "text file stored locally and pass it to",
      "offset": 3810.72,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "Spark context text file method. Finally,",
      "offset": 3813.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "audits can also be created from existing",
      "offset": 3816.799,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "audits which we'll see in the next",
      "offset": 3818.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "video. In the first method, oddities are",
      "offset": 3821.559,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "created from a list or a set using the",
      "offset": 3824.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "spark context paralyze method. Let's try",
      "offset": 3826.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and understand how oddities are created",
      "offset": 3829.839,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "using this method with a couple of",
      "offset": 3831.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "examples. In the first example, an",
      "offset": 3834.44,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "oddity named num is created from a",
      "offset": 3836.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Python list containing numbers 1 2 3 and",
      "offset": 3839.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "four.",
      "offset": 3842.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "In the second example, an RD named hello",
      "offset": 3844.319,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "RD is created from the hello world",
      "offset": 3846.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "string. You can confirm the object",
      "offset": 3849.799,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "created is Rud using Python's type",
      "offset": 3852.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "method. Creating oddities from external",
      "offset": 3856.039,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "data sets is by far the most common",
      "offset": 3858.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "method in Pispark. In this method, Ruds",
      "offset": 3860.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "are created using the Spark context text",
      "offset": 3864.079,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "file method. In this simple example, an",
      "offset": 3866.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "RDD named file RDD is created from the",
      "offset": 3869.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "lines of readme.md file stored locally",
      "offset": 3872.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "on our computer. Similar to previous",
      "offset": 3875.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "method, you can confirm the RDD using",
      "offset": 3878.319,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "the type",
      "offset": 3880.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "method. Data partitioning is an",
      "offset": 3882.119,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "important concept in Spark and",
      "offset": 3884.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "understanding how Spark deals with",
      "offset": 3886.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "partitions allows one to control",
      "offset": 3888.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "parallelism.",
      "offset": 3890.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "A partition in Spark is the division of",
      "offset": 3892.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the lost data set with each part being",
      "offset": 3894.319,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "stored in multiple locations across the",
      "offset": 3896.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "cluster. By default, Spark partitions",
      "offset": 3899.4,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "the data at the time of creating Rud",
      "offset": 3902.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "based on several factors such as",
      "offset": 3904.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "available resources, sectional data",
      "offset": 3906.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "sets, etc. However, this behavior can be",
      "offset": 3908.559,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "controlled by passing a second argument",
      "offset": 3911.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "called min partitions which define the",
      "offset": 3913.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "minimum number of partitions to be",
      "offset": 3916.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "created for an RDD.",
      "offset": 3918.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "In the first example, we create an RD",
      "offset": 3921.039,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "named num RDD from the list of 10",
      "offset": 3923.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "integers using the spark context",
      "offset": 3926.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "paralyze method with six partitions. In",
      "offset": 3928.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the second example, we create another RD",
      "offset": 3931.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "named filed using spark context text",
      "offset": 3934.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "file method with six partitions. The",
      "offset": 3937.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "number of partitions in an RDD can",
      "offset": 3940.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "always be found by using the get num",
      "offset": 3942.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "partitions method. In the next video,",
      "offset": 3944.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you'll see the final method of creating",
      "offset": 3947.92,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "oddities. For now, let's create some",
      "offset": 3949.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "oddities. In the last video, you have",
      "offset": 3952.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "learned how to load your data into",
      "offset": 3955.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "oddities. In this video, you learn about",
      "offset": 3957.4,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "the various operations that support",
      "offset": 3960.079,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "audits in",
      "offset": 3962.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Pispark. Ruds in Pispark supports two",
      "offset": 3963.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "different types of operations,",
      "offset": 3966.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "transformations and actions.",
      "offset": 3968.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Transformations are operations and RDDs",
      "offset": 3971.039,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that return a new RDD and actions are",
      "offset": 3973.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "operations that perform some computation",
      "offset": 3975.839,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "on the RDD. The most important feature",
      "offset": 3978,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "which helps Ruds in fall tolerance and",
      "offset": 3981.039,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "optimizing resource use is the lazy",
      "offset": 3983.359,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "evaluation. So what is lazy",
      "offset": 3986.039,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "evaluation? Spark creates a graph from",
      "offset": 3988.76,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "all the operations you perform on an ODD",
      "offset": 3991.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and execution of the graph starts only",
      "offset": 3993.839,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "when an action is performed in OD as",
      "offset": 3996,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "shown in this figure. This is called",
      "offset": 3998.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "lazy evaluation in Spark.",
      "offset": 4000.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "The RD transformations we will look in",
      "offset": 4003.76,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "this video are map filter flat map and",
      "offset": 4006,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "union. The map transformation takes in a",
      "offset": 4010.68,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "function and applies it to each element",
      "offset": 4013.599,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "in the",
      "offset": 4015.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "RDD. Say you have an input RDD with",
      "offset": 4016.76,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "elements 1 2 3 4. The map transformation",
      "offset": 4019.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "takes in a function and applies it to",
      "offset": 4023.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "each element in the RDD with the result",
      "offset": 4026,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of the function being the new value of",
      "offset": 4028.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "each element in the resulting odd. In",
      "offset": 4030.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this example, the square function is",
      "offset": 4033.52,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "applied to each element of the",
      "offset": 4035.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "RD. Let's understand this with an",
      "offset": 4037.799,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "example. We first create an OD using",
      "offset": 4040.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "spark context parallelize method on a",
      "offset": 4043.119,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "list containing elements 1 2 3 4. Next,",
      "offset": 4045.68,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "we apply map transformation for squaring",
      "offset": 4050.16,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "each element of the",
      "offset": 4052.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "RDD. The filter transformation takes in",
      "offset": 4054.599,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "a function and returns an odd that only",
      "offset": 4057.359,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "has elements that pass the condition.",
      "offset": 4060,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Suppose we have an input odd with",
      "offset": 4063.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "numbers 1 2 3 4 and we want to select",
      "offset": 4065.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "numbers greater than 2. We can apply the",
      "offset": 4068.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "filter transformation.",
      "offset": 4071.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Here is an example of the filter",
      "offset": 4073.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "transformation wherein we use the same",
      "offset": 4074.96,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "RDD as before to apply filter",
      "offset": 4077.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "transformation to filter out the numbers",
      "offset": 4079.48,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "that are greater than two. Flat map is",
      "offset": 4081.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "similar to map transformation except it",
      "offset": 4085.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "returns multiple values for each element",
      "offset": 4087.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "in the source RDD. A simple usage of",
      "offset": 4089.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "flat map is splitting up an input string",
      "offset": 4092.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "into words. Here you have an input RDD",
      "offset": 4095.28,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "with two elements hello world and how",
      "offset": 4098.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "are you. Applying the split function of",
      "offset": 4101.199,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the flat map transformation results in",
      "offset": 4103.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "five elements in the resulting RDD.",
      "offset": 4105.759,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "Hello, world, how are you? As you can",
      "offset": 4108,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "see, even though the input RDD has two",
      "offset": 4111.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "elements, the output RDD now contains",
      "offset": 4114.48,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "five",
      "offset": 4116.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "elements. In this example, we create an",
      "offset": 4117.799,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "RDD from a list containing the words",
      "offset": 4120.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "hello world and how are you. Next we",
      "offset": 4122.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "apply flat map along with the split",
      "offset": 4125.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "function on the RD to split the input",
      "offset": 4127.6,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "string into individual",
      "offset": 4129.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "words. Union transformation returns the",
      "offset": 4131.799,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "union of one RDD with another RD. In",
      "offset": 4134.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "this figure we are filtering the input",
      "offset": 4138.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "RDD and creating two RDDs errors RDD and",
      "offset": 4140.159,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "warnings RDD. And next we are combining",
      "offset": 4143.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "both the RDDs using the unit",
      "offset": 4146.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "transformation.",
      "offset": 4148.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "To illustrate this using pispark code,",
      "offset": 4150.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "let's first create an input RD from a",
      "offset": 4152.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "local file using spark context text file",
      "offset": 4155.279,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "method. Next, we will use two filter",
      "offset": 4157.6,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "transformation to create two RDDs, error",
      "offset": 4160.4,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "odd and warnings RDD. And finally, using",
      "offset": 4163.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "union transformation, we will combine",
      "offset": 4167.04,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "them",
      "offset": 4169.04,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "both. So far, you have seen how Rud",
      "offset": 4169.799,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "transformations work. But after applying",
      "offset": 4173.48,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "transformations at some point you will",
      "offset": 4176.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "want to actually do something with your",
      "offset": 4178.719,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "data set. This is where actions come",
      "offset": 4181.12,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "into",
      "offset": 4183.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "picture. Actions are the operations that",
      "offset": 4184.359,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "are applied on auditis to return a value",
      "offset": 4186.96,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "after running a",
      "offset": 4189.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "computation. The four basic actions that",
      "offset": 4190.839,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "you will learn in this lesson are",
      "offset": 4194.159,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "collect, take, reduce and count.",
      "offset": 4196.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Collect action returns complete list of",
      "offset": 4200.64,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "the elements from the",
      "offset": 4202.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "odd whereas take of n prints an n number",
      "offset": 4204.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "of elements from the",
      "offset": 4208.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "odd. Continuing the map transformation",
      "offset": 4209.719,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "example executing collect returns all",
      "offset": 4212.32,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "elements that is 1 4 9 16 from the RD",
      "offset": 4214.96,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "map RDD that you created earlier. Simply",
      "offset": 4219.679,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "here is an example of take of two action",
      "offset": 4223.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that prints the first two elements that",
      "offset": 4225.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "is one and four from the",
      "offset": 4227.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "RDD_map",
      "offset": 4230.04,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "RDD. Sometimes you just want to print",
      "offset": 4231.64,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "the first element of the OD. First",
      "offset": 4234.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "action returns the first element in an",
      "offset": 4236.96,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "RDD. It is similar to take of one. Here",
      "offset": 4239.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "is an example of the first action which",
      "offset": 4242.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "prints the first element that is one",
      "offset": 4244.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "from the RDD_map RDD.",
      "offset": 4247.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Finally, the count action is used to",
      "offset": 4250.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "return the total number of rows or",
      "offset": 4253.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "elements in the RDD. Here is an example",
      "offset": 4254.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of the count action to count the number",
      "offset": 4257.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "of elements in the RDD flatmap RDD. The",
      "offset": 4259.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "result here indicates that there are",
      "offset": 4263.6,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "five elements in the RDD flatmap",
      "offset": 4265.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "RDD. It's time for you to practice RDD",
      "offset": 4268.36,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "operations in. In the last video, you",
      "offset": 4271.199,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "were introduced to some basic RDD",
      "offset": 4274.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "operations. And in this video you'll",
      "offset": 4276.44,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "learn how to work with oddities of key",
      "offset": 4278.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "value pairs which are a common data type",
      "offset": 4280.719,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "required for many operations in",
      "offset": 4283.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Spark. Most of the real world data sets",
      "offset": 4285.4,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "are generally key value pairs. An",
      "offset": 4288.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "example of this kind of data set has the",
      "offset": 4290.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "team name as key and the list of players",
      "offset": 4293.04,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "as",
      "offset": 4295.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "values. The typical pattern of this kind",
      "offset": 4296.679,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "of data set is each row is a key that",
      "offset": 4299.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "maps to one or more values.",
      "offset": 4302.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "In order to deal with this kind of data",
      "offset": 4305.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "set, Pispark provides a special data",
      "offset": 4307.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "structure called pair audits. In pair",
      "offset": 4309.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "audits, the key refers to the identifier",
      "offset": 4312.48,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "whereas the value refers to the",
      "offset": 4315.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "data. There are a number of ways to",
      "offset": 4317.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "create pair audits. The two most common",
      "offset": 4319.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "ways are creating from a",
      "offset": 4322.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "list of the key value to pull or from a",
      "offset": 4324.92,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "regular RDD. Irrespective of the method,",
      "offset": 4328,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the first step in creating pair odds is",
      "offset": 4330.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to get the data into key value form.",
      "offset": 4333.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "Here is an example of creating pair odd",
      "offset": 4336.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "from a list of the key value tuple that",
      "offset": 4338.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "contains the name as key and as the",
      "offset": 4341.44,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "value using spark context paralyze",
      "offset": 4344.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "method. And here is an example of",
      "offset": 4346.92,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "creating pair odd from regulars. In this",
      "offset": 4349.199,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "example, a regular RDD is created from a",
      "offset": 4352.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "list that contains strings using Spark",
      "offset": 4355.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "context paralyze method. Next, we create",
      "offset": 4357.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "a pay RDD using map function which",
      "offset": 4360.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "returns tuple with key value pairs with",
      "offset": 4364.32,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "key being the name and is being the",
      "offset": 4366.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "value. Pair a RDS are still odds and",
      "offset": 4369.239,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "thus all the transformations available",
      "offset": 4372.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "to",
      "offset": 4374.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "regularities. Since pay audits contains",
      "offset": 4375.64,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "tupils, we need to pass functions that",
      "offset": 4378.4,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "operate on key value",
      "offset": 4380.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "pairs. A few special operations are",
      "offset": 4382.679,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "available for this kind such as reduce",
      "offset": 4385.28,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "by key, group by key, sort by key, and",
      "offset": 4387.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "join. Let's take a look at each of these",
      "offset": 4390.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "four pair transformations in detail.",
      "offset": 4393.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Now, the reduce by key transformation is",
      "offset": 4396.36,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "the most popular payd transformation",
      "offset": 4399.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "which combines values with the same key",
      "offset": 4401.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "using a function. Reduce by key runs",
      "offset": 4404.239,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "several parallel operations, one for",
      "offset": 4407.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "each key in the data set. Because data",
      "offset": 4410.159,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "sets can have very large number of keys,",
      "offset": 4413.04,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "reduce by key is not implemented as an",
      "offset": 4416,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "action. Instead, it returns a new RDD",
      "offset": 4418.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "consisting of each key and the reduced",
      "offset": 4421.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "value for that key. Here is an example",
      "offset": 4424.159,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "of reduce by transformation that uses a",
      "offset": 4427.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "function to combine all the goals scored",
      "offset": 4429.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "by each of the players.",
      "offset": 4432.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "The result shows that the player as key",
      "offset": 4434.32,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "and the total number of goals scored as",
      "offset": 4436.719,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "value. Sorting of data is necessary for",
      "offset": 4439.56,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "many downstream applications. We can",
      "offset": 4442.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sort pay RDD as long there is an",
      "offset": 4445.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "ordering defined in the key. Sort by key",
      "offset": 4447.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "transformation returns an RDD sorted by",
      "offset": 4451.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "key in ascending or descending order.",
      "offset": 4453.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Continuing our reduced by key example.",
      "offset": 4456.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Here is an example that sorts the data",
      "offset": 4459.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "based on the number of goals scored by",
      "offset": 4461.6,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "each",
      "offset": 4463.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "player. A common use case of pay rd is",
      "offset": 4464.84,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "grouping the data by key. For example,",
      "offset": 4468.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "viewing all of the airports for a",
      "offset": 4471.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "particular country together. If the data",
      "offset": 4473.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is already keyed in the way that we",
      "offset": 4475.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "want. The group by key operation groups",
      "offset": 4477.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "all the values with the same key in the",
      "offset": 4480.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "pair RDD. Here is an example of group by",
      "offset": 4482.48,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "key transformation that groups all the",
      "offset": 4485.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "airports for a particular country from",
      "offset": 4488.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "an input list that contains the list of",
      "offset": 4489.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "tupils. Each tuple consists of country",
      "offset": 4492.239,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "code and the corresponding airport",
      "offset": 4495.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "code. Join transformation joins two pair",
      "offset": 4497.8,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "RDDs based on their key. Let's",
      "offset": 4500.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "demonstrate this with an example. First",
      "offset": 4503.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we create two RDDs. RDD1 contains the",
      "offset": 4506.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "list of tupils with each tupole",
      "offset": 4509.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "consisting of name and is and RDD2",
      "offset": 4511.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "contains the list of tupils with each",
      "offset": 4515.04,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "tupil consisting of name and income.",
      "offset": 4517.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Applying the join transformation on RDD1",
      "offset": 4520.719,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "and RDD2 merges two RDDs together by",
      "offset": 4523.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "grouping elements with the same key.",
      "offset": 4527.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Here is an example that shows the result",
      "offset": 4529.84,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "of joint transformation of RDD1 and",
      "offset": 4531.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "RDD2. Now that you have learned all",
      "offset": 4535.4,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "about pay audits, it's time for you.",
      "offset": 4537.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Previously, you learned about advanced",
      "offset": 4541.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "audit transformations for key value data",
      "offset": 4543.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "sets. Similar to advanced RD",
      "offset": 4545.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "transformations, there are advanced RDD",
      "offset": 4548.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "actions which you'll see in this video.",
      "offset": 4550.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Reduce action takes in a function which",
      "offset": 4553.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "operates on two elements of the same",
      "offset": 4555.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "type of RD and returns a new element of",
      "offset": 4557.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the same type.",
      "offset": 4559.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "The function should be commutative and",
      "offset": 4561.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "associative so that it can be computed",
      "offset": 4563.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "correctly in parallel. A simple example",
      "offset": 4566,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of such a function is plus which we can",
      "offset": 4568.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "use to sum our RD. Here is an example of",
      "offset": 4571.12,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "reduce action that calculates the sum of",
      "offset": 4574.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "all the elements in an RD. In this",
      "offset": 4576.719,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "example, input RD is created using spark",
      "offset": 4579.199,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "context paralyze method on a list",
      "offset": 4582.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "consisting of numbers 1 3 4 6. Executing",
      "offset": 4584.64,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "reduce action results in 14 which is the",
      "offset": 4588.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "sum of 1 3 4",
      "offset": 4591.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "6. In many cases it's not advisable to",
      "offset": 4593.4,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "run collect action on audit because of",
      "offset": 4596.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the huge size of the data. In these",
      "offset": 4598.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "cases it's common to write data out to a",
      "offset": 4601.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "distributed storage system such as HDFS",
      "offset": 4603.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "or Amazon",
      "offset": 4606.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "S3. Save as text file action can be used",
      "offset": 4607.96,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "to save RD as a text file inside a",
      "offset": 4611.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "particular directory. By default, save",
      "offset": 4614,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "as text file saves OD with each",
      "offset": 4617.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "partition as a separate file inside a",
      "offset": 4619.36,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "directory. Here is an example of save as",
      "offset": 4621.8,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "text file that saves an OD with each",
      "offset": 4624.719,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "partition as a separate file inside a",
      "offset": 4627.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "directory. However, you can change it to",
      "offset": 4630.12,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "return a new that is reduced into a",
      "offset": 4632.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "single partition using the co method.",
      "offset": 4635.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Here is an example of save as text file",
      "offset": 4638.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that saves RD as a single file inside a",
      "offset": 4641.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "directory.",
      "offset": 4644.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Similar to pay audit transformations,",
      "offset": 4646.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "there are also Rud actions available for",
      "offset": 4648.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "pay audits. However, pay audits also",
      "offset": 4650.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "attain some additional actions of Pi",
      "offset": 4654.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Pispark, especially those that leverage",
      "offset": 4656.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the advantages of data which is of key",
      "offset": 4659.28,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "value",
      "offset": 4661.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "nature. Let's take a look at two pair",
      "offset": 4662.84,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "actions. Count by key and collect as map",
      "offset": 4666,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "in this video.",
      "offset": 4668.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Count by key is only available on",
      "offset": 4670.719,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "oddities of type key",
      "offset": 4672.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "value. With the count by key operation,",
      "offset": 4674.679,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "we can count the number of elements for",
      "offset": 4677.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "each key. Here is an example of counting",
      "offset": 4679.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "the number of values for each key in the",
      "offset": 4682.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "data set. In this example, we first",
      "offset": 4685.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "create a pay rd named RD using spark",
      "offset": 4688.08,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "context paral method. Since count by key",
      "offset": 4691.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "generates a dictionary, next we iterate",
      "offset": 4695.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "over the dictionary to print the unique",
      "offset": 4697.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "key and the number of values associated",
      "offset": 4700.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "with each key as shown here. One thing",
      "offset": 4702.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to note is that count by key should only",
      "offset": 4705.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "be used in a data set whose size is",
      "offset": 4707.76,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "small enough to fit in a",
      "offset": 4709.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "memory. Collect as map returns the key",
      "offset": 4712.199,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "value pairs in the RD as a dictionary.",
      "offset": 4715.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Here is an example of collect as map on",
      "offset": 4718.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "a pair RDD. As before, we create a pair",
      "offset": 4720.239,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "RD using spark context paralyze method",
      "offset": 4723.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and next use collect as map action.",
      "offset": 4726.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Collect as map produces the key value",
      "offset": 4729.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "pairs in the RDD as a dictionary which",
      "offset": 4731.84,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "can be used for downstream",
      "offset": 4734.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "analysis. Similar to count by key, this",
      "offset": 4736.199,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "action should only be used if the",
      "offset": 4739.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "resulting data is expected to be small",
      "offset": 4741.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "as all the data is loaded into the",
      "offset": 4743.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "memory. Let's practice some of these",
      "offset": 4746.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "advanced actions on some test data in",
      "offset": 4748.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Python.",
      "offset": 4750.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "In the previous chapter, you looked at",
      "offset": 4752.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "RDS, which is Spark's co- abstraction",
      "offset": 4754.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for working with data. In this chapter,",
      "offset": 4756.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "we will explore Pispark SQL, which is",
      "offset": 4758.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Spark's highle API for working with",
      "offset": 4761.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "structured data. Isparks SQL is a Spark",
      "offset": 4763.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "library for structured data. Unlike the",
      "offset": 4766.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Pispark RDD API, isparks SQL provides",
      "offset": 4769.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "more information about the structure of",
      "offset": 4772.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the data and the computation being",
      "offset": 4774.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "performed. Isparks SQL provides a",
      "offset": 4776.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "programming abstraction called dataf",
      "offset": 4779.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "frames. A dataf frame is an immutable",
      "offset": 4781.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "distributed collection of data with",
      "offset": 4783.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "named columns. It is similar to a table",
      "offset": 4785.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in SQL. Data frames are designed to",
      "offset": 4788,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "process a large collection of structured",
      "offset": 4791.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "data such as relational database and",
      "offset": 4792.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "semiructured data such as JSON or",
      "offset": 4795.28,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "JavaScript object",
      "offset": 4797.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "notation. Dataf frame API currently",
      "offset": 4799.08,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "supports several languages such as",
      "offset": 4801.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Python, R, Scala, and Java. Dataf frames",
      "offset": 4803.28,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "allow pispark to query data using SQL",
      "offset": 4807.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "for example select asterisk from table",
      "offset": 4810.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "or using the expression method for",
      "offset": 4812.88,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "example df",
      "offset": 4814.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "select previously you have learned about",
      "offset": 4816.679,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "spark context which is the main entry",
      "offset": 4819.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "point for creating a similarly spark",
      "offset": 4821.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "session provides a single point of entry",
      "offset": 4825.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to interact with underlying spark",
      "offset": 4827.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "functionality and allows programming",
      "offset": 4829.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "spark with dataf frame API.",
      "offset": 4831.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "The spark session does for data frames",
      "offset": 4834.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "what the spark context does for RDS. A",
      "offset": 4837.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "spark session can be used to create data",
      "offset": 4841.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "frame, register data frame as tables,",
      "offset": 4843.52,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "execute SQL over tables, cache tables,",
      "offset": 4845.84,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "etc. Similar to spark context, spark",
      "offset": 4849.96,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "session is exposed to the pispark shell",
      "offset": 4853.04,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "as variable",
      "offset": 4855.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "spark. Data frames in pispark can be",
      "offset": 4856.92,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "created in two main ways. from an",
      "offset": 4859.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "existing RDD using sparse sessions",
      "offset": 4862.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "create dataf frame method and from",
      "offset": 4864.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "different data sources such as CSV, JSON",
      "offset": 4867.679,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "text using sparse sessions read",
      "offset": 4870.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "method. Before going into the details of",
      "offset": 4873.32,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "creating data frames, let's understand",
      "offset": 4875.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "what schema is. Schema is the structure",
      "offset": 4878.239,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "of data in data frames and helps Spark",
      "offset": 4881.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to optimize queries on the data more",
      "offset": 4884.719,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "efficiently.",
      "offset": 4886.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "A schema provides informationational",
      "offset": 4888.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "details such as the column name, the",
      "offset": 4890.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "type of data in that column, and whether",
      "offset": 4892.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "null or empty values are allowed in the",
      "offset": 4894.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "column. To create a data frame from an",
      "offset": 4896.52,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "OD, we will need to pass an OD and a",
      "offset": 4899.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "schema into Spark sessions create dataf",
      "offset": 4902.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "frame method. In this example, we will",
      "offset": 4904.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "first create an ODD named",
      "offset": 4908,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "iPhones_D from a list of iPhones using",
      "offset": 4910.52,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "Spark context paralyze method. Next, we",
      "offset": 4913.6,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "will create a data frame using Spark",
      "offset": 4916.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sessions create data frame method using",
      "offset": 4918.719,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "iPhone RD and the list of column names",
      "offset": 4921.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "such as model, year, height, width and",
      "offset": 4924.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "weight as",
      "offset": 4927.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "schema. The type of object created can",
      "offset": 4928.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "be confirmed using type method which",
      "offset": 4931.199,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "shows that it is a pispark data",
      "offset": 4933.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "frame. A thing to note here is when the",
      "offset": 4935.96,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "schema is a list of column names, the",
      "offset": 4939.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "type of each column will be inferred",
      "offset": 4941.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "from data as shown above. However, when",
      "offset": 4943.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the schema is none, it will try to infer",
      "offset": 4945.76,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "the schema from the data. To create a",
      "offset": 4948.56,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "data frame from CSV, JSON text txt",
      "offset": 4952.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "files, we will make use of the Spark",
      "offset": 4955.12,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "sessions spark read",
      "offset": 4957.52,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "property. Here is an example of create",
      "offset": 4960.04,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "DF CSV data frame from people CSV file",
      "offset": 4962.639,
      "duration": 8.641
    },
    {
      "lang": "en",
      "text": "using spark read. CSV method. Similarly,",
      "offset": 4967.36,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "here is an example for creating df_json",
      "offset": 4971.28,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "data frame from people.json file using",
      "offset": 4974.32,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "spark readad.gsv json",
      "offset": 4977.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "method. Finally, here is an example for",
      "offset": 4981,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "creating df_txt dataf frame from",
      "offset": 4984.08,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "people.txt file using spark read.txt",
      "offset": 4987.639,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "method. Irrespective of the file type,",
      "offset": 4991.96,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "this method requires the path to the",
      "offset": 4994.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "file and two optional parameters. The",
      "offset": 4996.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "first optional parameter header equals",
      "offset": 4999.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to true may be passed to make sure that",
      "offset": 5001.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the meth method treats the first row as",
      "offset": 5004.4,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "column",
      "offset": 5007.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "names. The second optional parameter",
      "offset": 5008.12,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "infer schema equals to true may be",
      "offset": 5010.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "passed to instruct the data from reader",
      "offset": 5013.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "to infer the schema from the data and by",
      "offset": 5015.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "doing so it will attempt to assign the",
      "offset": 5018.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "right data type to each column based on",
      "offset": 5020.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the content. Now let's practice creating",
      "offset": 5023.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "some data frames.",
      "offset": 5026.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Just like Ruds, dataf frames also",
      "offset": 5028.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "support both transformations and",
      "offset": 5030.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "actions. In this video, you learned some",
      "offset": 5032.639,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "dataf frame operations in",
      "offset": 5035.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Pispark. Similar to RDD operations, the",
      "offset": 5037.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "data frame operations in Pispark can be",
      "offset": 5040.239,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "divided into transformations and",
      "offset": 5042.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "actions. Pispark dataf frame provides",
      "offset": 5044.92,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "operations to filter, group or compute",
      "offset": 5047.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "aggregates and can be used with pispark",
      "offset": 5050.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "SQL.",
      "offset": 5052.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Let's explore some of the most common",
      "offset": 5054.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "data frame transformations such as",
      "offset": 5056.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "select, filter, group by, order by, drop",
      "offset": 5058.08,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "duplicates with column renamed, and some",
      "offset": 5061.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "common data frame actions such as print",
      "offset": 5064.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "schema, show, count, columns, and",
      "offset": 5066.48,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "describe. In this",
      "offset": 5069.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "video, let's start with select and show",
      "offset": 5071.239,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "operations. The select transformation is",
      "offset": 5074.28,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "used to extract one or more columns from",
      "offset": 5076.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "a data frame. We need to pass the column",
      "offset": 5079.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "name inside select operation. As an",
      "offset": 5081.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "example, let's select a column from a",
      "offset": 5084.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "test data frame. Select is a",
      "offset": 5086.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "transformation and so it creates a new",
      "offset": 5089.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "data frame. And in order to print the",
      "offset": 5091.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "rows from df id a dataf frame, we need",
      "offset": 5093.76,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "to execute an action. Show is an action",
      "offset": 5097.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that prints the first 20 rows by",
      "offset": 5100.719,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "default. Let's apply show of three on df",
      "offset": 5102.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "id age data frame and print the first",
      "offset": 5106.32,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "three rows as shown in this",
      "offset": 5109.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "example. Unlike select the filter",
      "offset": 5111.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "transformation selects only rows that",
      "offset": 5114.639,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "pass the condition specified. The",
      "offset": 5116.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "parameter you specified is the column",
      "offset": 5119.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "name and the value of what you want to",
      "offset": 5121.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "filter that column on. For example, if",
      "offset": 5124.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you want to filter out the rows with a",
      "offset": 5127.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is greater than 21, we pass the column",
      "offset": 5129.28,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "expression",
      "offset": 5132,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "new_df.as and the condition greater than",
      "offset": 5133.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "21. As shown in",
      "offset": 5136.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "here, we can use show up three action to",
      "offset": 5138.04,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "print out the first three rows from the",
      "offset": 5141.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "new data",
      "offset": 5143.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "frame. The group by transformation",
      "offset": 5144.04,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "groups the data frame using the",
      "offset": 5146.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "specified columns so we can run",
      "offset": 5148.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "aggregations on them. To better",
      "offset": 5150.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "understand, we will first group the a",
      "offset": 5153.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "column and create another data frame.",
      "offset": 5155.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Then we will use count action that",
      "offset": 5158.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "returns the total number of rows in the",
      "offset": 5160.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "data frame. And finally use show of",
      "offset": 5162.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "three operation to print the first three",
      "offset": 5164.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "rows in the data frame. The result is a",
      "offset": 5167.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "table that shows the first three groups",
      "offset": 5170.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and the corresponding number of members",
      "offset": 5173.199,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "in each",
      "offset": 5175.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "group. Order by transformation returns a",
      "offset": 5176.199,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "data frame sorted by the given columns.",
      "offset": 5179.199,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "Let's sort the",
      "offset": 5182.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "test_df a_g groupoup. count that we",
      "offset": 5183.8,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "obtained in the previous example based",
      "offset": 5187.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "on a column and print out the first",
      "offset": 5189.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "three rows of the data frame using the",
      "offset": 5192.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "show of three action. As you can see the",
      "offset": 5194.239,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "ace group has been sorted in ascending",
      "offset": 5197.6,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "order.",
      "offset": 5200.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Now the drop duplicate transformation",
      "offset": 5201.8,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "returns a new data frame with duplicate",
      "offset": 5204.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "rows removed. Here is an example where",
      "offset": 5207.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "drop duplicate transformation is used to",
      "offset": 5210.159,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "remove duplicate rows in user ID is and",
      "offset": 5212.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "gender columns and finally creating a",
      "offset": 5216.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "new data frame. You can execute count",
      "offset": 5218.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "action on this new data frame to print",
      "offset": 5221.84,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the number of non-duplicate",
      "offset": 5224.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "rows. The width column rename",
      "offset": 5226.04,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "transformation returns a new data frame",
      "offset": 5228.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "by renaming an existing column. It takes",
      "offset": 5230.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "two arguments the name of the old and",
      "offset": 5233.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "new columns. In this example, we rename",
      "offset": 5236.159,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the column name gender to sex and create",
      "offset": 5239.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "a",
      "offset": 5242.239,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "test_df sex. We can use show of three",
      "offset": 5243.96,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "action to print out the first three rows",
      "offset": 5247.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "from the new data frame. To check the",
      "offset": 5249.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "type of columns in the data frame, we",
      "offset": 5252.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "can use the print schema action. Here is",
      "offset": 5254.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "an example of print schema action on",
      "offset": 5257.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "test_df data frame that we used",
      "offset": 5260.76,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "previously. Print schema prints out the",
      "offset": 5263.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "schema in tree format as shown here and",
      "offset": 5266.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "helps to spot the issues with the schema",
      "offset": 5268.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "of the data. As an example, product ID",
      "offset": 5271.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is shown as string even though it is",
      "offset": 5274.159,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "supposed to be an",
      "offset": 5276.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "integer. The column operation returns",
      "offset": 5277.639,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the names of all the columns in the data",
      "offset": 5280.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "frame as an array of string. Let's print",
      "offset": 5282.239,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "the column names in test_d of data",
      "offset": 5285.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "frame. In this example, the test_d of",
      "offset": 5288.159,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "dataf frame has three columns. user ID,",
      "offset": 5291.6,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "gender and",
      "offset": 5294.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "age. Describe operation is used to",
      "offset": 5295.8,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "calculate the summary statistics of the",
      "offset": 5299.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "numerical columns in the data frame. If",
      "offset": 5301.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "you don't specify the name of columns,",
      "offset": 5304,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "it will calculate summary statistics for",
      "offset": 5306.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "all numerical columns present in the",
      "offset": 5308.639,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "data frame as shown in this",
      "offset": 5310.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "example. Now that you are familiar with",
      "offset": 5313,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "data frame operations, let's practice",
      "offset": 5315.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "using some of these operations on real.",
      "offset": 5317.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Previously you have seen how to interact",
      "offset": 5320.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "with Pisparks SQL using dataf frame API.",
      "offset": 5322.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "In this video you'll learn how to",
      "offset": 5326.159,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "interact with Pispark SQL using SQL",
      "offset": 5328.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "query. In addition to dataf frame API,",
      "offset": 5331.08,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "pispark SQL allows you to manipulate",
      "offset": 5334.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "dataf frames with SQL queries. What you",
      "offset": 5337.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "can do using dataf frame API can be done",
      "offset": 5340.159,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "using SQL queries and vice versa. So",
      "offset": 5343.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "what are the differences between dataf",
      "offset": 5346.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "frame API and SQL queries? The dataf",
      "offset": 5348.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "frame API provides a programmatic",
      "offset": 5351.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "interface basically a domain specific",
      "offset": 5353.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "language or DSL for interacting with",
      "offset": 5356.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "data. Dataf frame queries are much more",
      "offset": 5359,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "easier to construct",
      "offset": 5362.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "programmatically. Plain SQL queries can",
      "offset": 5364.679,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "be significantly more concise and easier",
      "offset": 5367.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to understand. They also portable and",
      "offset": 5369.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "can be used without any modifications",
      "offset": 5372.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "with every supported",
      "offset": 5375.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "language. Many of the dataf frame",
      "offset": 5376.76,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "operations that you have seen in the",
      "offset": 5378.88,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "previous chapter can be done using SQL",
      "offset": 5380.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "queries. The spark session provides a",
      "offset": 5383.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "method called SQL which can be used to",
      "offset": 5386.08,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "execute SQL query. The SQL method takes",
      "offset": 5388.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "a SQL statement as an argument and",
      "offset": 5392.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "returns a data frame representing the",
      "offset": 5394.8,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "result of the given query.",
      "offset": 5396.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Unfortunately, SQL queries cannot be run",
      "offset": 5399.56,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "directly against a data frame. To issue",
      "offset": 5402.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "SQL queries against an existing data",
      "offset": 5405.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "frame, we can leverage the create or",
      "offset": 5407.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "replace tempu function to build a",
      "offset": 5409.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "temporary table as shown in this",
      "offset": 5412.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "example. After creating the temporary",
      "offset": 5413.96,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "table, we can simply use the SQL method",
      "offset": 5416.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "which allows us to write SQL code to",
      "offset": 5419.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "manipulate the data within a data frame.",
      "offset": 5422.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "In this example, we simply extract two",
      "offset": 5424.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "columns, field one and field two from",
      "offset": 5426.88,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "the table using",
      "offset": 5428.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "select. Since the result is a data",
      "offset": 5430.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "frame, you can run data frame actions",
      "offset": 5433.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "such as collect, first, show, etc. An",
      "offset": 5435.6,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "example of collect action is shown here.",
      "offset": 5438.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "In the previous lesson, you have seen",
      "offset": 5441.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "how to use select operation to subset",
      "offset": 5443.76,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "the data from a data",
      "offset": 5446,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "frame. Here is an example of how you can",
      "offset": 5447.719,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "do the same with a SQL query. In this",
      "offset": 5450.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "example, we will first construct a query",
      "offset": 5453.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "for selecting the product ID column from",
      "offset": 5455.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the temporary table. Next, we will pass",
      "offset": 5458.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the query to the Spark sessions SQL",
      "offset": 5460.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "method to create a new data frame.",
      "offset": 5463.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Because the result of SQL query returns",
      "offset": 5466.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "a data frame, all the usual data frame",
      "offset": 5468.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "operations are available. Here we can",
      "offset": 5470.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "use show of five action to print the",
      "offset": 5473.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "first row, five rows of the data frame.",
      "offset": 5475.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "The SQL queries are not limited to",
      "offset": 5479.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "extracting data as seen in the previous",
      "offset": 5481.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "slide. We can also create SQL queries to",
      "offset": 5483.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "run",
      "offset": 5486.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "aggregations. In this example, we first",
      "offset": 5487.4,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "construct a query for selecting a and",
      "offset": 5490.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "purchase columns. Then aggregate the",
      "offset": 5492.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "total of all the purchases the maximum",
      "offset": 5494.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "per age group. We can then provide the",
      "offset": 5497.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "query to spark sessions SQL method and",
      "offset": 5500.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "use show five action to print out the",
      "offset": 5503.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "first five rows as shown in here.",
      "offset": 5505.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "In addition to extracting and",
      "offset": 5509.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "summarizing the data, SparkSQL queries",
      "offset": 5510.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "can also be constructed for filtering",
      "offset": 5513.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the rows from a data frame. Suppose you",
      "offset": 5515.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "want to filter out the rows of purchase",
      "offset": 5518.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "and gender columns where the gender is",
      "offset": 5521.28,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "female and the purchase is greater than",
      "offset": 5523.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "20,000. You can construct a query as",
      "offset": 5525.159,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "shown in this example. You can confirm",
      "offset": 5527.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "whether or not query worked by providing",
      "offset": 5530.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the query to the spark sessions SQL",
      "offset": 5533.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "method and using show five action to",
      "offset": 5535.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "print out the first five rows as shown",
      "offset": 5537.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "in this example. Let's practice some SQL",
      "offset": 5540,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "within",
      "offset": 5543.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Pispark. Visualization is an essential",
      "offset": 5544.92,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "part of data analysis. In this video, we",
      "offset": 5547.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "will explore some visualization methods",
      "offset": 5550.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that can help us make sense of our data",
      "offset": 5551.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "in Pispark data frames. Data",
      "offset": 5554.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "visualization is the way of representing",
      "offset": 5556.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "your data in the form of graphs or",
      "offset": 5559.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "charts. It is considered a crucial",
      "offset": 5561.6,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "component of exploratory data",
      "offset": 5564,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "analysis. Several open-source tools",
      "offset": 5566.199,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "exist to aid visualization in Python",
      "offset": 5568.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "such as Mattplot liabet",
      "offset": 5570.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "etc. However, none of these",
      "offset": 5573.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "visualization tools can be used directly",
      "offset": 5575.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "with pisparks data frames.",
      "offset": 5578.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Currently there are three different",
      "offset": 5580.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "methods available to create charts using",
      "offset": 5582.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "pispark data frames. Pispark d explore",
      "offset": 5584.56,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "library to pandas method and handy spark",
      "offset": 5588.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "library. Let's understand each of these",
      "offset": 5591.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "methods with",
      "offset": 5593.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "examples. Pispark d explorer is a",
      "offset": 5594.92,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "plotting library to get quick insights",
      "offset": 5597.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "on data in pispark data frames.",
      "offset": 5600.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "There are three functions available in",
      "offset": 5603.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "pispark dish explore to create mattplot",
      "offset": 5605.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "lib graphs while minimizing the amount",
      "offset": 5607.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "of computation needed hist dishplot and",
      "offset": 5609.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "pandas",
      "offset": 5612.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "histogram. Here is an example of",
      "offset": 5614.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "creating a histogram using pispark dish",
      "offset": 5616.56,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "explore package on the test_df data.",
      "offset": 5618.88,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "First the csv file is loaded into spark",
      "offset": 5622.719,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "data frame using the spark sessions read",
      "offset": 5626.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "csv method. Then we select the as column",
      "offset": 5628.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "from the test_df data frame using the",
      "offset": 5632,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "select operation. Finally, we use the",
      "offset": 5634.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "hist function of the pispark dist",
      "offset": 5637.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "explore package to plot a histogram of a",
      "offset": 5639.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "in the",
      "offset": 5642.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "test_df data set. The second method of",
      "offset": 5643.4,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "creating charts is by using two pandas",
      "offset": 5647.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "on pispark data frames which converts",
      "offset": 5649.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the pispark data frame into pandas data",
      "offset": 5652.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "frame. After conversion, it's easy to",
      "offset": 5655.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "create charts from pandas data frames",
      "offset": 5657.679,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "using mattplot lib or cp plotting",
      "offset": 5659.92,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "libraries. In this example, first the",
      "offset": 5663.719,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "CSV is loaded in spark data frame using",
      "offset": 5666.48,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "the read. CSV method. Next, using two",
      "offset": 5669.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "pandas method, we will convert the spark",
      "offset": 5673.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "data frame into pandas data frame.",
      "offset": 5675.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Finally, we will create a histogram of",
      "offset": 5678.48,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "the as column using mattplot lib's h",
      "offset": 5680.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "method. Before we look at the third",
      "offset": 5683.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "method, let's take a look at the",
      "offset": 5686.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "differences between pandas versus spark",
      "offset": 5688.4,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "data",
      "offset": 5691.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "frames. Pandas won't work in every case.",
      "offset": 5691.96,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "It is a single machine tool and",
      "offset": 5695.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "constrained by single machine limits.",
      "offset": 5697.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Those so the size is limited by your",
      "offset": 5699.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "server memory and you will process them",
      "offset": 5702.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "with the power of single server. In",
      "offset": 5705.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "contrast, operations on pispark data",
      "offset": 5707.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "frames run parallel on different nodes",
      "offset": 5710.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "in the cluster. In pandas data frames we",
      "offset": 5712.56,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "get the result as soon as we apply any",
      "offset": 5716,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "operation. Whereas operations in pispark",
      "offset": 5718.92,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "data frames are lazy in nature. You can",
      "offset": 5721.679,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "change a pandas data frame using",
      "offset": 5725.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "methods. We can't change a pispark data",
      "offset": 5728.12,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "frame due to its immutable property.",
      "offset": 5730.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Finally the pandas API supports more",
      "offset": 5733.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "operations than pispark data frames.",
      "offset": 5735.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "The final method of creating charts is",
      "offset": 5739.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "using the handy spark library which is",
      "offset": 5741.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "relatively a new package. Handy spark is",
      "offset": 5743.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "designed to improve pispark user",
      "offset": 5746.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "experience especially when it comes to",
      "offset": 5748.44,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "exploratory data analysis including",
      "offset": 5751.04,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "visualization",
      "offset": 5753.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "capabilities. It makes fetching data or",
      "offset": 5754.679,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "computing statistics for columns really",
      "offset": 5757.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "easy written pandas objects straight",
      "offset": 5759.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "away. It brings the long missing",
      "offset": 5761.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "capability of plotting data while",
      "offset": 5764.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "retaining the advantages of performing",
      "offset": 5766.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "the distributed computation. Here is an",
      "offset": 5768.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "example of handy spark method for",
      "offset": 5771.6,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "creating a",
      "offset": 5773.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "histogram. Just like before, we load the",
      "offset": 5774.92,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "CSV into a pi spark data frame using the",
      "offset": 5777.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "spark sessions read. CSV method. After",
      "offset": 5780.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "creating the data frame, we convert the",
      "offset": 5783.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "data frame to a handy Spark data frame",
      "offset": 5785.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "using the two handy method. Finally, we",
      "offset": 5788.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "create a histogram of the ace column",
      "offset": 5791.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "using the hist function of handy spark",
      "offset": 5793.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "library. We have learned three exciting",
      "offset": 5796.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "methods of visualizing pispark data",
      "offset": 5799.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "frames. And let's practice creating ch",
      "offset": 5801.6,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "some charts with them now on real",
      "offset": 5804.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "world. In the last chapter, you learned",
      "offset": 5807.32,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "about pispark sql which is one of the",
      "offset": 5810,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "highlevel API built on top of spark core",
      "offset": 5812.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "for structured data.",
      "offset": 5815.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "In this chapter, you learn about Pispark",
      "offset": 5817.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "MLIB, which is a built-in library for",
      "offset": 5819.6,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "scalable machine",
      "offset": 5822.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "learning. Before diving deep into",
      "offset": 5823.48,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "Pispark MLIB, let's quickly define what",
      "offset": 5825.76,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "machine learning is. According to",
      "offset": 5828.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Wikipedia, machine learning is a",
      "offset": 5831.32,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "scientific discipline that explores the",
      "offset": 5833.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "construction and study of algorithms",
      "offset": 5835.119,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "that can learn from",
      "offset": 5837.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "data. Pispark MLib is a machine learning",
      "offset": 5839.08,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "library. Its goal is to make practical",
      "offset": 5842.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "machine learning scalable and easy. At a",
      "offset": 5844.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "high level, Pispock ML provides tools",
      "offset": 5848.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "such as machine learning algorithms",
      "offset": 5850.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "which include collaborative filtering,",
      "offset": 5853.199,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "classification and regression and",
      "offset": 5855.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "clustering. Featurization which include",
      "offset": 5858.119,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "feature extraction, transformation,",
      "offset": 5860.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "dimensionality reduction and selection",
      "offset": 5862.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "pipelines which include constructing,",
      "offset": 5865.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "evaluating and tuning machine learning",
      "offset": 5867.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pipelines.",
      "offset": 5869.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "In this chapter, we will explore machine",
      "offset": 5871.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "learning algorithms, collaborative",
      "offset": 5873.36,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "filtering, classification, and",
      "offset": 5875.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "clustering. Many of you have heard about",
      "offset": 5878.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "scikitlearn, which is a very popular and",
      "offset": 5880.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "easy to use Python library for machine",
      "offset": 5882.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "learning. Then what is the need for",
      "offset": 5884.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "pispark ML?",
      "offset": 5887.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Scikitlearn algorithms work well for",
      "offset": 5889.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "small to medium-sized data sets that can",
      "offset": 5892.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "be processed on a single machine but not",
      "offset": 5894.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for large data sets that require the",
      "offset": 5896.88,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "power of parallel",
      "offset": 5899.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "processing. On the other hand, Pispark",
      "offset": 5900.679,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "MLB only contains algorithms in which",
      "offset": 5903.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "operations can be applied in parallel",
      "offset": 5906.48,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "across nodes in a",
      "offset": 5908.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "cluster. Unlike scikitlearn, MLib",
      "offset": 5910.199,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "supports several other highle languages",
      "offset": 5913.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "such as Scala, Java and R. In addition",
      "offset": 5916.239,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "to",
      "offset": 5919.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Python, MLA also provides a highle API",
      "offset": 5920.44,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "to build machine learning pipelines. A",
      "offset": 5923.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "machine learning pipeline is a complete",
      "offset": 5926.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "workflow combining multiple machine",
      "offset": 5927.92,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "learning algorithms",
      "offset": 5929.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "together. Pispark is good for iterative",
      "offset": 5931.159,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "algorithms and using iterative",
      "offset": 5934.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "algorithms. Many machine learning",
      "offset": 5936,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "algorithms have been implemented in",
      "offset": 5937.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Pispark MLB.",
      "offset": 5939.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Ispark currently supports various",
      "offset": 5941.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "methods for binary classification,",
      "offset": 5943.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "multiclass classification and regression",
      "offset": 5945.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "analysis. Some of the algorithms include",
      "offset": 5948.04,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "linear SVMs, logistic regression,",
      "offset": 5950.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "decision trees, random forest, gradient",
      "offset": 5953.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "posted trees, naive base, linear le",
      "offset": 5955.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "squares, lasso, rich regression,",
      "offset": 5959.199,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "isotonic",
      "offset": 5961.6,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "regression. Collaborative filtering is",
      "offset": 5962.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "commonly used for recommended systems",
      "offset": 5965.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "and Pispark MLIP uses the alternative",
      "offset": 5967.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "least squares or ALS algorithm for",
      "offset": 5969.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "collaborative filtering. Clustering",
      "offset": 5972.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "algorithm consists of K means Gaussian",
      "offset": 5975.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "mixture power iteration clustering",
      "offset": 5978.239,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "bisecting K means and streaming K means.",
      "offset": 5980.56,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "While Pispark MLIPs include several",
      "offset": 5984.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "machine learning algorithms, we will",
      "offset": 5986.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "specifically focus on three key areas",
      "offset": 5988.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "often referred to as the three C's of",
      "offset": 5990.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "machine learning. Collaborative",
      "offset": 5992.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "filtering, classification, and",
      "offset": 5994.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "clustering. Collaborative filtering",
      "offset": 5996.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "produces recommendations based on past",
      "offset": 5998.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "behavior, preferences, or similarities",
      "offset": 6001.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to known entities or users.",
      "offset": 6004,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Classification is the problem of",
      "offset": 6006.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "identifying to which of a set of",
      "offset": 6008.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "categories a new observation belongs.",
      "offset": 6010.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Clustering is grouping of data into",
      "offset": 6013.52,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "clusters based on similar",
      "offset": 6015.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "characteristics. We will go in more",
      "offset": 6018.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "details in the next few",
      "offset": 6020,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "lessons. Now that you learned the three",
      "offset": 6022.199,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "C's of machine learning, let's quickly",
      "offset": 6024.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "understand how we can import this",
      "offset": 6026.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Pispark MLI libraries in the Pispark",
      "offset": 6028.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "shell environment. Let's start with",
      "offset": 6031.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Pispark's collaborative filtering which",
      "offset": 6033.84,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "is available in",
      "offset": 6035.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "pispark.mlib.recommendation subm module.",
      "offset": 6037.719,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Here is how you import the ALS or",
      "offset": 6040,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "alternative lease square class in",
      "offset": 6042.159,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Pispark",
      "offset": 6044.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "shell for binary classification. Here is",
      "offset": 6045.639,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "an example of how you import logistic",
      "offset": 6048.719,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "regression with LBFGS class in the",
      "offset": 6050.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "pispark.mlip.classification subm module",
      "offset": 6055.639,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "inside the pispark shell. Similarly for",
      "offset": 6057.52,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "clustering here is an example of",
      "offset": 6060.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "importing the kins class in pispark",
      "offset": 6062.639,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "shell using the",
      "offset": 6066,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "pispark.mlipclustering subm",
      "offset": 6068.199,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "module. Let's practice how well you",
      "offset": 6069.96,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "understand the different machine",
      "offset": 6072.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "learning algorithms by importing them.",
      "offset": 6073.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "In the previous video you have been",
      "offset": 6076.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "introduced with the three C's of machine",
      "offset": 6078.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "learning. In this video, we'll start",
      "offset": 6080.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "with the first C which is collaborative",
      "offset": 6082.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "filtering and gain a basic understanding",
      "offset": 6084.96,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "of recommended systems in Spa. Let's get",
      "offset": 6086.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "started. Collaborative filtering is a",
      "offset": 6091,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "method of making automatic predictions",
      "offset": 6093.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "about the interests of a user by",
      "offset": 6095.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "collecting preferences or taste",
      "offset": 6097.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "information from many users.",
      "offset": 6098.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Collaborative filtering is one of the",
      "offset": 6101.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "most commonly used algorithms in",
      "offset": 6103.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "recommended systems. Collaborative",
      "offset": 6105.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "filtering has two approaches. the user",
      "offset": 6108.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "user approach and item item approach.",
      "offset": 6111.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "The user user approach finds users that",
      "offset": 6114.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "are similar to the target user and uses",
      "offset": 6116.96,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "the collaborative ratings to make",
      "offset": 6119.6,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "recommendations for the target user. The",
      "offset": 6121.719,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "item item approach finds and recommends",
      "offset": 6125.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "items that are similar or related to",
      "offset": 6127.199,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "items associated with the target",
      "offset": 6129.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "user. Now let's take a look at different",
      "offset": 6132.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "components that are needed to build a",
      "offset": 6134.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "recommendation system in Pispark.",
      "offset": 6136.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "The rating class in pispark.mmile",
      "offset": 6140,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "dorecommendation subm module is a",
      "offset": 6142.48,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "wrapper around tupil of user product and",
      "offset": 6144.639,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "rating. The rating class is useful for",
      "offset": 6147.88,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "parsing the odd and creating a tupole of",
      "offset": 6150.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "user product and",
      "offset": 6153.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "rating. Here is a simple example of how",
      "offset": 6154.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "you can create an instance of rating",
      "offset": 6157.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "class R with the values of user equals",
      "offset": 6159.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "to 1, product equals to 2 and rating",
      "offset": 6162,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "equals to",
      "offset": 6164.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "5.0. Once the rating class is created,",
      "offset": 6165.56,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "you can extract the user product and",
      "offset": 6168.719,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "rating value using the index of R",
      "offset": 6171.119,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "instance. In this example, R of zero, R",
      "offset": 6173.56,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "of one and R of2 shows the user ID,",
      "offset": 6177.119,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "product ID and ratings for the R",
      "offset": 6179.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "instance. Splitting the data into",
      "offset": 6182.6,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "training and testing sets is an integral",
      "offset": 6184.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "part of machine learning. The training",
      "offset": 6187.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "portion will be used to train the model",
      "offset": 6190.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "while the testing data is used to",
      "offset": 6192.159,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "evaluate the model's",
      "offset": 6194,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "performance. Typically, a large portion",
      "offset": 6196.199,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "of the data is assigned for training and",
      "offset": 6198.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "a small portion for",
      "offset": 6201.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "testing. Highspark's random split",
      "offset": 6203,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "function can be used to randomly split",
      "offset": 6205.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the data with the provided weights and",
      "offset": 6207.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "return multiple RDS. In this example, we",
      "offset": 6209.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "first create an RDD which consists of",
      "offset": 6213.36,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "numbers 1 to 10 and using random split",
      "offset": 6215.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "function, we create two RDDs with 60 to",
      "offset": 6219.119,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "40 ratio. The output of the random split",
      "offset": 6222.08,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "function shows training RDDs contain six",
      "offset": 6225.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "elements whereas the test RDD contains",
      "offset": 6229.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "four elements.",
      "offset": 6231.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "The alternating leash squares or ALS",
      "offset": 6233.36,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "algorithm available in",
      "offset": 6236.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "spark.mlip helps to find products that",
      "offset": 6238.28,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "the customers might like based on their",
      "offset": 6240.88,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "previous purchases or ratings. The also",
      "offset": 6243.199,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "method requests that we represent rating",
      "offset": 6247.199,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "objects as user ID, items ID, rating",
      "offset": 6249.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "tuples along with the training",
      "offset": 6252.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "parameters, rank and",
      "offset": 6254.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "iterations. Rank represents the number",
      "offset": 6257.08,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "of features. Iterations represent the",
      "offset": 6259.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "number of iterations to run the least",
      "offset": 6262.159,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "square",
      "offset": 6264.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "computation. Here is an example of",
      "offset": 6265.4,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "running the ALS model. First, we create",
      "offset": 6267.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "an RDD from a list of reading objects",
      "offset": 6270.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and print out the contents of the RDD",
      "offset": 6272.96,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "using collect action. Next, we use",
      "offset": 6275.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "ALS.train to train the training data as",
      "offset": 6278.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "shown in this",
      "offset": 6280.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "example. After training the model, the",
      "offset": 6282.199,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "next step is predicting the ratings for",
      "offset": 6284.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the user ID and product pairs.",
      "offset": 6286.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "The predict all method takes an ODD of",
      "offset": 6289.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "user ID and product ID pairs and returns",
      "offset": 6292.159,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "a prediction for each",
      "offset": 6294.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "pair. In order to get the example to",
      "offset": 6297.08,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "work, let's create an OD from a list of",
      "offset": 6299.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "tuples containing user ID and product ID",
      "offset": 6302.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "using spark context paralyze method.",
      "offset": 6305.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Next, we apply the predict all method on",
      "offset": 6308.32,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 6310.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "unrated_D. Running collect action and",
      "offset": 6312.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "prediction shows a list of predicted",
      "offset": 6315.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "ratings generated by ALS model for the",
      "offset": 6317.36,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "user ID 1 and product ids 1 and",
      "offset": 6319.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "two. For evaluating the model train",
      "offset": 6323.08,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "using ALS, we can use the mean squared",
      "offset": 6325.92,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "error or",
      "offset": 6329.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "MSE. The MSE measures the average of the",
      "offset": 6330.119,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "squares of the errors between what is",
      "offset": 6333.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "estimated and the existing data.",
      "offset": 6335.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Continuing on our previous example,",
      "offset": 6338,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we'll first organize our ratings and",
      "offset": 6340.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "predictions data to make user product",
      "offset": 6342.48,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 6345.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "rating. Next, we will join the ratings",
      "offset": 6346.6,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "RD with the prediction RDD and the",
      "offset": 6349.6,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "result looks as",
      "offset": 6352.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "follows. Finally, we apply a square",
      "offset": 6353.719,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "difference function to the map",
      "offset": 6356.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "transformation of the rates RDD and then",
      "offset": 6358.08,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "use the mean to get the MSE.",
      "offset": 6361.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Now it's your turn to try your hand at",
      "offset": 6365.199,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "collaborative",
      "offset": 6367.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "filtering. In the previous video you",
      "offset": 6369.08,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "learned about collaborative filtering",
      "offset": 6371.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "which is the first C of machine learning",
      "offset": 6373.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "algorithms in Pispark MLB. In this video",
      "offset": 6375.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you learn about the second C of machine",
      "offset": 6378.56,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "learning which is",
      "offset": 6380.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "classification. Classification is a",
      "offset": 6382.119,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "popular machine learning algorithm that",
      "offset": 6384.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "identifies which category an item",
      "offset": 6386.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "belongs to. For example, whether an",
      "offset": 6388.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "email is spam or non-spam based on",
      "offset": 6390.719,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "labeled examples of other",
      "offset": 6392.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "items. Classification takes a set of",
      "offset": 6395.32,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "data with known labels and predetermined",
      "offset": 6397.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "features and learns how to label new",
      "offset": 6400.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "records based on that information. That",
      "offset": 6402.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "is why classification comes under a",
      "offset": 6404.719,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "supervised learning",
      "offset": 6406.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "technique. Classifications can be",
      "offset": 6408.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "divided into two different types. Binary",
      "offset": 6410.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "classification and multiclass",
      "offset": 6413.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "classification. In binary",
      "offset": 6414.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "classification, we want to classify",
      "offset": 6416.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "entities into two distinct categories.",
      "offset": 6418.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "For example, determining whether a",
      "offset": 6421.04,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "cancer type is malignant or not. Pispark",
      "offset": 6423.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "ML supports various methods for binary",
      "offset": 6426.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "classification such as linear SVMs,",
      "offset": 6428.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "logistic regression, decision trees,",
      "offset": 6431.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "random forests, gradient boosted trees,",
      "offset": 6433.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "naive base. In multiclass",
      "offset": 6435.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "classification, we want to classify",
      "offset": 6438.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "entities into more than two categories.",
      "offset": 6439.84,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "For example, determining what category a",
      "offset": 6442.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "news article belongs",
      "offset": 6445.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "to. Highspark ML supports various",
      "offset": 6446.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "methods for multiclass classification",
      "offset": 6449.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "such as logistic regression, decision",
      "offset": 6451.6,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "trace, random forest, naive base. Let's",
      "offset": 6453.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "focus on logistic regression which is",
      "offset": 6457.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the most popular supervised machine",
      "offset": 6458.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "learning method. Logistic regression is",
      "offset": 6460.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a classification method to predict a",
      "offset": 6463.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "binary response given some independent",
      "offset": 6465.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "variable. It measures the relationship",
      "offset": 6468.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "between the label on the y-axis and",
      "offset": 6470.639,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "features on the x-axis using a logistic",
      "offset": 6473.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "function as shown in this figure. In",
      "offset": 6475.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "logistic regression, the output must be",
      "offset": 6478.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "zero or one. The convention is if the",
      "offset": 6480.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "probability is greater than 50% then the",
      "offset": 6483.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "logistic regression output is one",
      "offset": 6486,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "otherwise it is",
      "offset": 6488,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "zero. Highspark malle contains a few",
      "offset": 6489.48,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "specific data types such as vectors and",
      "offset": 6492.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "labeled point. Let's understand each of",
      "offset": 6495.199,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "these data",
      "offset": 6497.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "types. Vectors in pispark mlib comes in",
      "offset": 6499,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "two flavors dense and",
      "offset": 6502.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "sparse. Dense vectors store all their",
      "offset": 6505,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "entries in an array of flo floating",
      "offset": 6508.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "point numbers. For example, a vector of",
      "offset": 6510.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "100 will contain 100 double values. In",
      "offset": 6512.96,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "contrast, sparse vector stores only the",
      "offset": 6516.639,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "nonzero values and their indices.",
      "offset": 6519.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Here is an example of creating a dense",
      "offset": 6522.4,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "vector of 1 2 3 using vectors dance",
      "offset": 6524.48,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "method. And here is an example of",
      "offset": 6529.08,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "creating a sparse vector with the size",
      "offset": 6531.44,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "of the vector equals to 4 and nonzero",
      "offset": 6533.36,
      "duration": 9.279
    },
    {
      "lang": "en",
      "text": "entities 1 col 1.0 3 5.5 as a dictionary",
      "offset": 6537.08,
      "duration": 9
    },
    {
      "lang": "en",
      "text": "using vector sparse method. A label",
      "offset": 6542.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "point is a wrapper around the input",
      "offset": 6546.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "features and the predicted value. Label",
      "offset": 6548,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "point includes a label and a feature",
      "offset": 6550.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "vector. The label is a floating point",
      "offset": 6552.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "value and in the case of binary",
      "offset": 6555.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "classification it is either one positive",
      "offset": 6557.28,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "or zero negative. This example shows a",
      "offset": 6560.159,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "positive label point with label 1 and a",
      "offset": 6563.679,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "feature vector",
      "offset": 6566.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "1.0.0 3.0 and a negative label point",
      "offset": 6568.199,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "with label zero and feature vector 2.0",
      "offset": 6571.76,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "1.0 1.0. Pispark MLIB has an algorithm",
      "offset": 6575.04,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "called hashing TF that computes a term",
      "offset": 6579.28,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "frequency vector of a given size from a",
      "offset": 6582.32,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "document. Let's illustrate this with an",
      "offset": 6585.639,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "example. In this simple example, first",
      "offset": 6588.6,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "we will split the sentence hello world",
      "offset": 6591.36,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "into a list of words using the split",
      "offset": 6594.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "method and we will create vectors of",
      "offset": 6597.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "size 10,000.",
      "offset": 6599.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Finally, we compute the term frequency",
      "offset": 6601.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "vector by using TF's transform method on",
      "offset": 6603.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the words. As you can see, the sentence",
      "offset": 6606.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is turned into a sparse vector holding",
      "offset": 6609.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "feature number and the occurrences of",
      "offset": 6612,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "each word. Among several algorithms, the",
      "offset": 6614.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "popular algorithm available for logistic",
      "offset": 6617.36,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "regression is Pispark ML is LBFGS.",
      "offset": 6619.52,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "The minimum requirement for logistic",
      "offset": 6624.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "regression with LBFGS is an OD of",
      "offset": 6626.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "labeled point. To understand how",
      "offset": 6629.679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "logistic regression works, let's see a",
      "offset": 6632.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "simple example. We first create a list",
      "offset": 6634.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "of labeled points with labels zero and",
      "offset": 6636.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "one. And then using spark context",
      "offset": 6639.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "parallel method, we will create an RDD.",
      "offset": 6642.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Then we will use logistic regression",
      "offset": 6645.28,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "with",
      "offset": 6647.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "lbfgs. Method to train a logistic",
      "offset": 6648.36,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "regression model on the RDD. Once the",
      "offset": 6650.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model is trained from logistic",
      "offset": 6653.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "regression with LBFGS algorithm, the",
      "offset": 6655.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "predict method computes a score between",
      "offset": 6658,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "zero and one for each point as shown",
      "offset": 6660.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "here. Now it's your turn to practice",
      "offset": 6663,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "classification",
      "offset": 6665.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "using in the previous video you learned",
      "offset": 6667.32,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "about classification, a type of",
      "offset": 6670,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "supervised learning method. But what if",
      "offset": 6671.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "if you want to make sense of unlabelled",
      "offset": 6674.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "data? In this video, you learn about",
      "offset": 6676,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "clustering, which is a type of",
      "offset": 6678.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "unsupervised learning method to group",
      "offset": 6679.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "unlabelled data together. So, what",
      "offset": 6681.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "exactly is clustering? Clustering is the",
      "offset": 6684.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "unsupervised learning task that involves",
      "offset": 6687.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "grouping objects into clusters of high",
      "offset": 6689.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "similarity with no labels. Unlike the",
      "offset": 6691.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "supervised learning methods that you",
      "offset": 6694.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have seen before, such as collaborative",
      "offset": 6696.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "filtering and classification where data",
      "offset": 6698.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is labeled, clustering can be used to",
      "offset": 6700.719,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "make sense of unlabelled data. Icepark",
      "offset": 6703.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "ML library offers a handful of",
      "offset": 6706.56,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "clustering models such as C means",
      "offset": 6708.96,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "clustering, Gaussian mixture",
      "offset": 6711.08,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "clustering, power iteration clustering,",
      "offset": 6713.719,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "bisecting C means clustering and",
      "offset": 6716.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "streaming CN clustering. In this video,",
      "offset": 6718.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "we will focus on C means clustering",
      "offset": 6721.599,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "because of its simplicity and",
      "offset": 6723.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "popularity.",
      "offset": 6725.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "K means is an unsupervised method that",
      "offset": 6727.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "takes data points in an input data and",
      "offset": 6729.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "will identify which data points belong",
      "offset": 6732.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "to each one of the clusters. As shown in",
      "offset": 6734.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the left side of the figure, we can",
      "offset": 6738,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "provide n data points and a predefined",
      "offset": 6739.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "number of K clusters. The K means",
      "offset": 6742.56,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "algorithm through a series of",
      "offset": 6745.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "iterations, create clusters as shown on",
      "offset": 6747.08,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "the right side of the figure. The K",
      "offset": 6749.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "means clustering minimally requires that",
      "offset": 6752.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the data is a set of numerical features",
      "offset": 6754.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and that we specify the target number of",
      "offset": 6757.28,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "K clusters",
      "offset": 6759.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "ahead. The first step in implementing",
      "offset": 6761.239,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "the C means clustering algorithm using",
      "offset": 6764.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Pispark MLI is loading the numerical",
      "offset": 6767.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "data into an OD and then parsing the",
      "offset": 6769.679,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "data based on a TL limiter. Here is an",
      "offset": 6772.08,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "example of how you load a CSV file into",
      "offset": 6775.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "an RD using Spark context text file",
      "offset": 6778.159,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "method. Then passing the RDD based on a",
      "offset": 6780.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "comma delimiter and finally converting",
      "offset": 6783.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "the floats to integers. The contents of",
      "offset": 6786.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the first five lines of RDD can be",
      "offset": 6789.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "printed using takeoff",
      "offset": 6791.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "file. As you can see the data set",
      "offset": 6793.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "contains two columns. Each column",
      "offset": 6796.159,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "indicating a feature loaded into an RDD.",
      "offset": 6798.4,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "Like other algorithms, you invoke K",
      "offset": 6802.639,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "means by calling K means train method",
      "offset": 6805.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "which takes an OD the number of clusters",
      "offset": 6807.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "we expect and the maximum number of",
      "offset": 6810,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "iterations",
      "offset": 6812,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "allowed. Continuing our previous",
      "offset": 6813.4,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "example, first we can import the K means",
      "offset": 6815.52,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "class from",
      "offset": 6818.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "pispark.mlip.clustering subm module.",
      "offset": 6819.88,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "Next we call K means train method on RD",
      "offset": 6822.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and the two parameters K equals to two",
      "offset": 6825.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and the maximum iterations equals to 10.",
      "offset": 6828.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "K means train returns a K means model",
      "offset": 6831.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "that lets you access the cluster centers",
      "offset": 6834.48,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "using the model.cluster centers method.",
      "offset": 6837.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "An example of cluster centers for K",
      "offset": 6840.639,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "equals to 2 is shown here. The next step",
      "offset": 6842.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in K means clustering is to evaluate the",
      "offset": 6846.239,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "model by computing the error function.",
      "offset": 6848.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Unfortunately, icebark came algorithm",
      "offset": 6851,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "doesn't have a method already. So we",
      "offset": 6853.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "have to write a function but ourselves",
      "offset": 6855.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "as shown here. We will next apply the",
      "offset": 6858.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "error function on the OD and calculate",
      "offset": 6861.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "within set sum of squared error.",
      "offset": 6864.159,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Continuing our previous example, we",
      "offset": 6867.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "apply map transformation of error",
      "offset": 6869.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "function to a input odd to calculate",
      "offset": 6871.44,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "within set sum of squared error which is",
      "offset": 6874.48,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "77.96 in this example. An optional but",
      "offset": 6878.04,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "highly recommended step in K means",
      "offset": 6881.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "clustering is cluster visualization.",
      "offset": 6883.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Continuing from a previous example,",
      "offset": 6885.88,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "let's first create a scatter plot of the",
      "offset": 6888.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "two feature columns in the sample data.",
      "offset": 6890.639,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Next, overlay it with the cluster",
      "offset": 6893.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "centers from the K means model, which",
      "offset": 6895.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "are indicated by the colored axis in",
      "offset": 6897.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "this figure. The purple and yellow",
      "offset": 6900.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "colors here represent the labels created",
      "offset": 6903.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "from the model based on K, which is two",
      "offset": 6905.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in this example.",
      "offset": 6908,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "As you can see the overlaid scatter plot",
      "offset": 6909.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "shows a reasonable clustering with the",
      "offset": 6912.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "two centroids or cluster centers placed",
      "offset": 6914.239,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "in the center of each of the",
      "offset": 6916.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "cluster. Now let's quickly take a look",
      "offset": 6918.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "at the code to generate the previous",
      "offset": 6921.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "plot. As seen previously plotting",
      "offset": 6923.56,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "libraries doesn't work that directly on",
      "offset": 6926.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "a data frames as shown here. We first",
      "offset": 6928.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "convert Rud to Spark data frame and then",
      "offset": 6931.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "to pandas data frame. We also convert",
      "offset": 6935.119,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the cluster centers from K means model",
      "offset": 6938.08,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "into a pandas data frame. Finally, we",
      "offset": 6940.159,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "use plt function in map plot clip",
      "offset": 6943.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "library to create a overlay scatter plot",
      "offset": 6946.239,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "as shown in this previous slide. Let's",
      "offset": 6948.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "use a real world data and generate some",
      "offset": 6952.239,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "nice clusters using pispark mlip k",
      "offset": 6954.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "means. Congratulations on successfully",
      "offset": 6957.4,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "completing fundamentals of big data via",
      "offset": 6960.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "pispark course. Our goal through this",
      "offset": 6962.719,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "course was to equip you with a basic",
      "offset": 6965.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "understanding of big data and show how",
      "offset": 6967.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Apachi Spa can be used to perform",
      "offset": 6970.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "powerful data analysis at scale. Let's",
      "offset": 6972.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "quickly review what you have learned so",
      "offset": 6976.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "far in this course and recommend you a",
      "offset": 6978.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "few courses that you can take next.",
      "offset": 6979.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Analyzing big data is equivalent to",
      "offset": 6983.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "conducting both descriptive and",
      "offset": 6985.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "inferential analysis using distributed",
      "offset": 6987.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "computing techniques such as Spar with",
      "offset": 6989.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the hope that the volume, velocity,",
      "offset": 6992.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "variety of big data that makes",
      "offset": 6994.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "distributed computing necessary will",
      "offset": 6996.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "lead into deeper or more targeted",
      "offset": 6999.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "insights.",
      "offset": 7001.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Chapter one started with the",
      "offset": 7003.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "fundamentals of big data and introduced",
      "offset": 7004.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Apache Spark as an open-source",
      "offset": 7006.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "distributed big data processing engine",
      "offset": 7008.88,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "as well as its different components",
      "offset": 7011.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "namely Spark Core, Sparks SQL, Spark ML",
      "offset": 7013.639,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "Graphics, and Spark streaming.",
      "offset": 7017.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Because Python is one of the most",
      "offset": 7020.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "popular languages for data science, we",
      "offset": 7022.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "looked specifically at how you might use",
      "offset": 7025.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "PISPA which is Spark's Python API to",
      "offset": 7028.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "execute Spark jobs and PIS spark shell",
      "offset": 7030.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to develop Sparks and track to",
      "offset": 7033.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "applications in Python. Finally, you",
      "offset": 7035.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "learned about the two different modes of",
      "offset": 7038.639,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "running Spark, namely local mode and",
      "offset": 7040.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "cluster mode. Chapter 2 introduced",
      "offset": 7043.679,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Pispark RDD which is the main API in",
      "offset": 7046.639,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Spark core for processing unstructured",
      "offset": 7049.599,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "data. We learned about the different",
      "offset": 7051.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "features of Ruds, different methods of",
      "offset": 7054.239,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "creating RDDs and finally Rud operations",
      "offset": 7056.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "namely transformations and",
      "offset": 7060.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "actions. Chapter 3 explored Pispark SQL",
      "offset": 7062.52,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "which is Spark's highle API for working",
      "offset": 7066,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "with structured data. Icepark SQL",
      "offset": 7068.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "creates data frames which provides more",
      "offset": 7071.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "information about the structure of data",
      "offset": 7073.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and the computation being performed. We",
      "offset": 7075.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "looked at the different methods of",
      "offset": 7078.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "creating data frames, data frame",
      "offset": 7080.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "operations namely transformations and",
      "offset": 7082.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "actions and finally different methods of",
      "offset": 7084.08,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "visualizing big data using data frames.",
      "offset": 7086.639,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "Chapter 4 delve deep into Pispark ML",
      "offset": 7090.4,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "Spark's built-in library for machine",
      "offset": 7094,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "learning and discussed how PISpark MLA",
      "offset": 7096.199,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "makes practical machine learning",
      "offset": 7099.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "scalable and easy. This chapter also",
      "offset": 7100.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "introduced the three C's of ML",
      "offset": 7103.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "collaborative filtering classification",
      "offset": 7106.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and clustering. The ecosystem of Apache",
      "offset": 7108.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Spark is vast and ever expanding. But",
      "offset": 7111.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "throughout the course, we have discussed",
      "offset": 7114.08,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "the essential underlying concepts. Where",
      "offset": 7116.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you choose to go from here, whether that",
      "offset": 7119.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "be experimenting and applying some of",
      "offset": 7121.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "these tools and patterns on your own or",
      "offset": 7123.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "investigating Spark components such as",
      "offset": 7126.239,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Spark SQL or Spark ML more deeply is up",
      "offset": 7128.239,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "to you. But we hope that the concepts,",
      "offset": 7132.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "tools, and techniques that we are",
      "offset": 7135.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "introduced in this course have provided",
      "offset": 7137.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "a well-informed starting point and",
      "offset": 7139.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "continue to serve as a basis for you to",
      "offset": 7142.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "refer back to throughout your",
      "offset": 7144.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "distributed data analysis journey.",
      "offset": 7146.639,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "With this general understanding of Pisp,",
      "offset": 7149.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we would encourage you to look at other",
      "offset": 7152.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "data camp Pisp courses focused on",
      "offset": 7154.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "feature engineering and recommended",
      "offset": 7157.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "engines to for Welcome to data cleaning",
      "offset": 7159.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "in Apache Spark with Python. My name is",
      "offset": 7162.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Mike Mezer. I'm a data engineering",
      "offset": 7165.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "consultant and I will be your instructor",
      "offset": 7166.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "for this course. We will cover what data",
      "offset": 7168.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "cleaning is, why it's important, and how",
      "offset": 7171.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "to implement it with Spark and Python.",
      "offset": 7173.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Let's get",
      "offset": 7175.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "started. In this course, we'll define",
      "offset": 7176.28,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "data cleaning as preparing raw data for",
      "offset": 7178.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "use in processing pipelines. We'll",
      "offset": 7180.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "discuss what a pipeline is later on, but",
      "offset": 7182.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "for now, it's sufficient to say the data",
      "offset": 7184.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "cleaning is a necessary part of any",
      "offset": 7186.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "production data system. If your data",
      "offset": 7188.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "isn't clean, it's not trustworthy and",
      "offset": 7190.96,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "could cause problems later",
      "offset": 7192.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "on. There are many tasks that can fall",
      "offset": 7194.679,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "under the data cleaning umbrella. A few",
      "offset": 7197.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of these include reformatting or",
      "offset": 7199.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "replacing text, performing calculations",
      "offset": 7201.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "based on the data, and removing garbage",
      "offset": 7203.599,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "or incomplete",
      "offset": 7205.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "data. Most data cleaning systems have",
      "offset": 7207.239,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "two big problems. Optimizing performance",
      "offset": 7209.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and organizing the flow of data. A",
      "offset": 7212.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "typical programming language such as",
      "offset": 7215.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Pearl, C++, or even standard SQL may be",
      "offset": 7217.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "able to clean data when you have small",
      "offset": 7220.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "quantities of data. But consider what",
      "offset": 7221.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "happens when you have millions or even",
      "offset": 7224.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "billions of pieces of data.",
      "offset": 7225.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "those languages wouldn't be able to",
      "offset": 7228.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "process that amount of information in a",
      "offset": 7229.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "timely manner. Spark lets you scale your",
      "offset": 7231.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "data processing capacity as your",
      "offset": 7234.239,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "requirements",
      "offset": 7235.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "evolve. Beyond the performance issues,",
      "offset": 7237.4,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "dealing with large quantities of data",
      "offset": 7239.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "requires a process or pipeline of steps.",
      "offset": 7241.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Spark allows management of many complex",
      "offset": 7244.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "tasks within a single",
      "offset": 7246.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "framework. Here's an example of cleaning",
      "offset": 7248.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "a small data set. We're given a table of",
      "offset": 7250.719,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "names, age, and years, and a city. Our",
      "offset": 7253.239,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "requirements are for a data frame with",
      "offset": 7256.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "first and last name in separate columns,",
      "offset": 7258.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the age and months, and which state the",
      "offset": 7260.639,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "city is in. We also want to remove any",
      "offset": 7263.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "rows where the data is out of the",
      "offset": 7266.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "ordinary. Using Spark transformations,",
      "offset": 7268.44,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "we can create a data frame with these",
      "offset": 7270.96,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "properties and continue processing",
      "offset": 7272.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "afterwards. A primary function of data",
      "offset": 7275.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "cleaning is to verify all data is in the",
      "offset": 7277.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "expected format. Spark provides a",
      "offset": 7279.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "built-in ability to validate data sets",
      "offset": 7282.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "with schemas. You may have used schemas",
      "offset": 7284.159,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "before with databases or XML. Spark is",
      "offset": 7286.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "similar. A schema defines and validates",
      "offset": 7289.08,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "the number and types of columns for a",
      "offset": 7291.76,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "given data",
      "offset": 7293.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "frame. A schema can contain many",
      "offset": 7295.239,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "different types of data, integers,",
      "offset": 7297.679,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "floats, dates, strings, and even arrays",
      "offset": 7299.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "or mapping",
      "offset": 7303.119,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "structures. A defined schema allows",
      "offset": 7304.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Spark to filter out data that doesn't",
      "offset": 7306.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "conform during read, ensuring expected",
      "offset": 7308.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "correctness.",
      "offset": 7310.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "In addition, schemas also have",
      "offset": 7313.199,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "performance benefits. Normally, a data",
      "offset": 7314.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "import will try to infer a schema on",
      "offset": 7317.599,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "read. This requires reading the data",
      "offset": 7319.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "twice. Defining a schema limits this to",
      "offset": 7321.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a single read",
      "offset": 7324.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "operation. Here's an example schema to",
      "offset": 7326.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "import the data from our previous",
      "offset": 7328.719,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "example. First, we'll import the",
      "offset": 7330.599,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "pispark.sql.types library. Next, we'll",
      "offset": 7333.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "define the actual strruct type list of",
      "offset": 7336.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "strct fields containing an entry for",
      "offset": 7337.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "each field in the data. Each strct field",
      "offset": 7339.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "consists of a field name, data type, and",
      "offset": 7342.639,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "whether the data can be null. Once our",
      "offset": 7344.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "schema is defined, we can add it into",
      "offset": 7347.679,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "our",
      "offset": 7349.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "spark.format.load call and process it",
      "offset": 7351.239,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "against our data. The load method takes",
      "offset": 7353.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "two arguments, the file name and a",
      "offset": 7356.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "schema. This is where we apply our",
      "offset": 7358.239,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "schema to the data being",
      "offset": 7360.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "loaded. We've gone over a lot of",
      "offset": 7362.199,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "information regarding data cleaning and",
      "offset": 7364.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the importance of dataf frame schemas.",
      "offset": 7365.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Let's put that information to use and",
      "offset": 7368.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "practice.",
      "offset": 7370.159,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "Welcome back. We've had a quick",
      "offset": 7374,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "discussion about data cleaning, data",
      "offset": 7375.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "types, and schemas. Let's move on to",
      "offset": 7377.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "some further Spark concepts.",
      "offset": 7380.32,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "Immutability and lazy",
      "offset": 7382,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "processing. Normally in Python and most",
      "offset": 7384.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "other languages, variables are fully",
      "offset": 7386.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "mutable. The values can be changed at",
      "offset": 7388.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "any given time, assuming the scope of",
      "offset": 7391.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the variable is valid.",
      "offset": 7392.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "While very flexible, this does present",
      "offset": 7395.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "problems anytime there are multiple",
      "offset": 7397.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "concurrent components trying to modify",
      "offset": 7398.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "the same data. Most languages work",
      "offset": 7400.639,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "around these issues using constructs",
      "offset": 7403.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "like mutxes and semaphors etc. This can",
      "offset": 7405.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "add complexity especially with",
      "offset": 7409.04,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "non-trivial",
      "offset": 7410.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "programs. Unlike typical Python",
      "offset": 7411.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "variables, Spark data frames are",
      "offset": 7414.08,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "immutable. While not strictly required,",
      "offset": 7416.199,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "immutability is often a component of",
      "offset": 7419.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "functional programming.",
      "offset": 7421.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "We won't go into everything that implies",
      "offset": 7423.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "here, but understand that Spark is",
      "offset": 7425.28,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "designed to use immutable",
      "offset": 7427.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "objects. Practically, this means Spark",
      "offset": 7429.08,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "data frames are defined once and are not",
      "offset": 7431.52,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "modifiable after",
      "offset": 7433.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "initialization. If the variable name is",
      "offset": 7435.639,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "reused, the original data is removed,",
      "offset": 7437.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "assuming it's not in use elsewhere, and",
      "offset": 7440.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "the variable name is reassigned to the",
      "offset": 7442.4,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "new",
      "offset": 7444.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "data. While this seems inefficient, it",
      "offset": 7445.159,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "actually allows Spark to share data",
      "offset": 7447.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "between all cluster components. it can",
      "offset": 7449.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do so without worry about concurrent",
      "offset": 7452.08,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "data",
      "offset": 7453.679,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "objects. This is a quick example of the",
      "offset": 7455.239,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "immutability of dataf frames in Spark.",
      "offset": 7457.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "It's okay if you don't understand the",
      "offset": 7460.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "actual code. This example is more about",
      "offset": 7461.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the concepts of what",
      "offset": 7464,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "happens. First, we create a dataf frame",
      "offset": 7465.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "from a CSV file called voter",
      "offset": 7468.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "data.csv. This creates a new dataf frame",
      "offset": 7471.639,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "definition and assigns it to the",
      "offset": 7474,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "variable name",
      "offset": 7475.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "voter_df. Once created, we want to do",
      "offset": 7477.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "two further",
      "offset": 7480.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "operations. The first is to create a",
      "offset": 7482.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "full year column by using a two-digit",
      "offset": 7484.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "year present in the data set and adding",
      "offset": 7486.4,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "2,00 to each",
      "offset": 7488.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "entry. This does not actually change the",
      "offset": 7490.199,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "data frame at all. It copies the",
      "offset": 7492.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "original definition, adds the",
      "offset": 7495.04,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "transformation, and assigns it to the",
      "offset": 7496.96,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "voter_df variable",
      "offset": 7498.84,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "name. Our second operation is similar.",
      "offset": 7501.159,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "Now, we want to drop the original year",
      "offset": 7504.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "column from the data frame. Again, this",
      "offset": 7506.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "copies the definition, adds a",
      "offset": 7509.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "transformation, and reassigns the",
      "offset": 7511.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "variable name to this new object. The",
      "offset": 7512.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "original objects are destroyed. Please",
      "offset": 7515.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "note that the original year column is",
      "offset": 7518.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "now permanently gone from this instance,",
      "offset": 7520,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "though not from the underlying data. For",
      "offset": 7522.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "example, you could simply reload it into",
      "offset": 7524.96,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "a new data frame if",
      "offset": 7526.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "desired. You may be wondering how Spark",
      "offset": 7528.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "does this so quickly, especially on",
      "offset": 7531.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "large data sets. Spark can do this",
      "offset": 7532.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "because of something called lazy",
      "offset": 7535.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "processing.",
      "offset": 7536.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Lazy processing in Spark is the idea",
      "offset": 7538.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "that very little actually happens until",
      "offset": 7540.32,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "an action is",
      "offset": 7542.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "performed. In our previous example, we",
      "offset": 7543.8,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "read a CSV file, added a new column, and",
      "offset": 7546.639,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "deleted",
      "offset": 7549.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "another. The trick is that no data was",
      "offset": 7550.679,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "actually read, added, or modified. We",
      "offset": 7553.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "only updated the instructions, aka",
      "offset": 7556.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "transformations, for what we wanted",
      "offset": 7558.28,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "Spark to do.",
      "offset": 7560.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "This functionality allows Spark to",
      "offset": 7562.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "perform the most efficient set of",
      "offset": 7564.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "operations to get the desired result.",
      "offset": 7565.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "The code example is the same as the",
      "offset": 7568.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "previous slide, but with the added count",
      "offset": 7570.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "method call. This classifies as an",
      "offset": 7572,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "action in Spark, and we'll process all",
      "offset": 7574.719,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "the transformation",
      "offset": 7576.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "operations. These concepts can be a",
      "offset": 7578.36,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "little tricky to grasp without some",
      "offset": 7580.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "examples. Let's practice these ideas in",
      "offset": 7581.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the coming",
      "offset": 7584.08,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "exercises. Welcome back. As we've seen,",
      "offset": 7587.32,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "Spark can read in text and CSV files.",
      "offset": 7590.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "While this gives us access to many data",
      "offset": 7593.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "sources, it's not always the most",
      "offset": 7595.36,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "convenient format to work with. Let's",
      "offset": 7597.119,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "take a look at a few problems with CSV",
      "offset": 7599.599,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "files. Some common issues with CSV files",
      "offset": 7602.36,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "include the schema is not defined. There",
      "offset": 7605.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "are no data types included nor column",
      "offset": 7608.8,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "names beyond a header",
      "offset": 7610.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "row. Using content containing a comma or",
      "offset": 7612.679,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "another delimiter requires escaping.",
      "offset": 7615.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Using the escape character within the",
      "offset": 7619.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "content requires even further escaping.",
      "offset": 7621.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "The available encoding formats are",
      "offset": 7624.079,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "limited depending on the language",
      "offset": 7625.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "used. In addition to the issues with CSV",
      "offset": 7628.04,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "files in general, Spark has some",
      "offset": 7630.88,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "specific problems processing CSV",
      "offset": 7632.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "data. CSV files are quite slow to import",
      "offset": 7635.719,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "and parse. The files cannot be shared",
      "offset": 7639.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "between workers during the import",
      "offset": 7642,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "process.",
      "offset": 7643.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "If no schema is defined, all data must",
      "offset": 7645.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "be read before a schema can be",
      "offset": 7647.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "inferred. Spark has a feature known as",
      "offset": 7649.8,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "predicate push down. Basically, this is",
      "offset": 7652.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the idea of ordering tasks to do the",
      "offset": 7654.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "least amount of work. Filtering data",
      "offset": 7656.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "prior to processing is one of the",
      "offset": 7659.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "primary optimizations of predicate",
      "offset": 7661.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "pushdown. This drastically reduces the",
      "offset": 7663.56,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "amount of information that must be",
      "offset": 7665.92,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "processed in large data",
      "offset": 7667.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sets. Unfortunately, you cannot filter",
      "offset": 7669.239,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "the CSV data via predicate push down.",
      "offset": 7671.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Finally, Spark processes are often",
      "offset": 7675.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "multi-step and may utilize an",
      "offset": 7677.76,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "intermediate file",
      "offset": 7679.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "representation. These representations",
      "offset": 7681.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "allow data to be used later without",
      "offset": 7684.079,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "regenerating the data from",
      "offset": 7686,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "source. Using CSV would instead require",
      "offset": 7687.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "a significant amount of extra work",
      "offset": 7690.88,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "defining schemas, encoding formats,",
      "offset": 7692.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "etc. Parquet is a compressed columner",
      "offset": 7695.639,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "data format developed for use in any",
      "offset": 7698.639,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Hadoop based system. This includes",
      "offset": 7700.4,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "Spark, Hadoop, Apache, Impala and so",
      "offset": 7703.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "forth. The parket format is structured",
      "offset": 7706.28,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "with data accessible in chunks allowing",
      "offset": 7708.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "efficient readr operations without",
      "offset": 7710.719,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "processing the entire",
      "offset": 7713.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "file. This structured format supports",
      "offset": 7715.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Spark's predicate push down",
      "offset": 7717.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "functionality providing significant",
      "offset": 7719.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "performance",
      "offset": 7721.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "improvement. Finally, parquet files",
      "offset": 7722.92,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "automatically include schema information",
      "offset": 7725.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and handle data encoding.",
      "offset": 7727.119,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "This is perfect for intermediary or on",
      "offset": 7729.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "disk representation of processed data.",
      "offset": 7731.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Note that parquet files are a binary",
      "offset": 7734.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "file format and can only be used with",
      "offset": 7736.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the proper tools. This is in contrast to",
      "offset": 7738.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "CSV files which can be edited with any",
      "offset": 7740.96,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "text",
      "offset": 7743.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "editor. Interacting with paret files is",
      "offset": 7744.119,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "very straightforward. To read a parquet",
      "offset": 7746.719,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "file into a dataf frame, you have two",
      "offset": 7749.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "options. The first is using the spark",
      "offset": 7751.159,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "readad.format method we've seen",
      "offset": 7753.84,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "previously.",
      "offset": 7755.599,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the data frame df equals spark",
      "offset": 7757.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "read.format",
      "offset": 7760.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "parquet.load",
      "offset": 7762.28,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "filename.parket. The second option is",
      "offset": 7764.679,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "the shortcut version. The dataf frame df",
      "offset": 7766.639,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "equals spark",
      "offset": 7769.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "read.parquet file",
      "offset": 7771.32,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "name.parquet. Typically the shortcut",
      "offset": 7773.96,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "version is the easiest to use but you",
      "offset": 7776.079,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "can use them",
      "offset": 7778.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "interchangeably. Writing parquet files",
      "offset": 7779.639,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "is similar using either df.right.format",
      "offset": 7781.76,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "format",
      "offset": 7785.36,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "parquet.save file",
      "offset": 7787.159,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "name.parquet or",
      "offset": 7789.079,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "df.right.parquet",
      "offset": 7792.599,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "filename.parquet. The long form versions",
      "offset": 7795.079,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "of each permit extra option flags such",
      "offset": 7797.199,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "as when overwriting an existing parquet",
      "offset": 7799.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "file. Paret files have various uses",
      "offset": 7801.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "within",
      "offset": 7804.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "Spark. We've discussed using them as an",
      "offset": 7805.719,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "intermediate data format, but they are",
      "offset": 7808.4,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "also perfect for performing SQL",
      "offset": 7810.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "operations. To perform a SQL query",
      "offset": 7813.079,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "against a parquet file, we first need to",
      "offset": 7815.52,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "create a data frame via the spark",
      "offset": 7817.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "read.parquet method. Once we have the",
      "offset": 7819.719,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "data frame, we can use the create or",
      "offset": 7822.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "replace temp view method to add an alias",
      "offset": 7824.56,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "of the parquet data as a SQL",
      "offset": 7826.8,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "table. Finally, we run our query using",
      "offset": 7829.48,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "normal SQL syntax and spark.sql method.",
      "offset": 7832.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "In this case, we're looking for all",
      "offset": 7835.92,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "flights with a duration under 100",
      "offset": 7837.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "minutes. Because we're using parquet as",
      "offset": 7839.8,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the backing store, we get all the",
      "offset": 7842.159,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "performance benefits we've discussed",
      "offset": 7843.84,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "previously, primarily defined schemas",
      "offset": 7845.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and the available use of predicate",
      "offset": 7848.079,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "pushdown. You've seen a bit about what a",
      "offset": 7849.96,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "paret file is and why we'd want to use",
      "offset": 7852.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "them. Now, let's practice working with",
      "offset": 7854.079,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "paret",
      "offset": 7856.239,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "files. Welcome back. In the first",
      "offset": 7859.639,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "chapter, we've spent some time",
      "offset": 7862.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "discussing the basics of Spark data and",
      "offset": 7863.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "file handling. Let's now take a look at",
      "offset": 7865.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "how to use Spark column operations to",
      "offset": 7868,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "clean data. Before we discuss",
      "offset": 7869.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "manipulating data frames in depth, let's",
      "offset": 7872.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "talk about some of their features. Dataf",
      "offset": 7874.639,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "frames are made up of rows and columns",
      "offset": 7877.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and are generally analogous to a",
      "offset": 7879.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "database table. Dataf frames are",
      "offset": 7881.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "immutable. Any change to the structure",
      "offset": 7884.28,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "or content of the data creates a new",
      "offset": 7886.56,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "data",
      "offset": 7888.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "frame. Dataf frames are modified through",
      "offset": 7889.719,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "the use of transformations.",
      "offset": 7892.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "An example is the filter command to only",
      "offset": 7894.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "return rows where the name starts with",
      "offset": 7897.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the letter M. Another operation is",
      "offset": 7898.719,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "select. In this case, returning only the",
      "offset": 7901.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "name and position",
      "offset": 7904.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "fields. There are many different",
      "offset": 7905.88,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "transformations for use on a data frame.",
      "offset": 7907.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "They vary depending on what you'd like",
      "offset": 7910.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to do. Some common transformations",
      "offset": 7911.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "include the filter clause, which",
      "offset": 7914.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "includes only rows that satisfy the",
      "offset": 7917.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "requirement defined in the argument.",
      "offset": 7919.199,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "This is analogous to the wear clause in",
      "offset": 7921.76,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "SQL. Spark includes a wear alias you can",
      "offset": 7924.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "use in place of filter if",
      "offset": 7928.079,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "desired. This call returns only rows",
      "offset": 7930.199,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "where the vote occurred after January",
      "offset": 7932.719,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "1st,",
      "offset": 7935.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "2019. Another common option is the",
      "offset": 7937.239,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "select method which returns the columns",
      "offset": 7939.679,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "requested from the data",
      "offset": 7941.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "frame. The width column method creates a",
      "offset": 7943.239,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "new column in the data frame.",
      "offset": 7946.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "The first argument is the name of the",
      "offset": 7948.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "column and the second is the command to",
      "offset": 7950.4,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "create",
      "offset": 7952.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it. In this case, we create a column",
      "offset": 7953.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "called year with just the year",
      "offset": 7956.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "information. We can also use the drop",
      "offset": 7958.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "method to remove a column from a data",
      "offset": 7960.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "frame. Among the most common operations",
      "offset": 7963.96,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "used when cleaning a data frame,",
      "offset": 7966.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "filtering lets us use only the data",
      "offset": 7968.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "matching our desired result. We can use",
      "offset": 7970.159,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "filter for many tasks such as removing",
      "offset": 7973.119,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "null",
      "offset": 7976.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "values, removing odd entries, anything",
      "offset": 7977.239,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "that doesn't fit our desired format. We",
      "offset": 7979.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "can also split a data frame containing",
      "offset": 7982.639,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "combined data such as a SIS log",
      "offset": 7984.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "file. As mentioned previously, use the",
      "offset": 7987.56,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "filter method to return only rows that",
      "offset": 7990.4,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "meet the specified",
      "offset": 7992.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "criteria. The contains function takes a",
      "offset": 7993.88,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "string argument that the column must",
      "offset": 7996.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "have to return true.",
      "offset": 7998.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "You can negate these results using the",
      "offset": 8001.04,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "till day",
      "offset": 8002.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "character. Some of the most common",
      "offset": 8004.199,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "operations used in data cleaning are",
      "offset": 8006.159,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "modifying and converting",
      "offset": 8008.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "strings. You will typically apply these",
      "offset": 8010.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to each column as a",
      "offset": 8012.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "transformation. Many of these functions",
      "offset": 8014.119,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "are in the",
      "offset": 8016.079,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "spark.sql.functions",
      "offset": 8017.159,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "library. For brevity, we'll import it as",
      "offset": 8019,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the alias",
      "offset": 8021.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "f. We use the width column function to",
      "offset": 8023,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "create a new column called upper using",
      "offset": 8025.84,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "pispark.sql.functions.",
      "offset": 8027.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "functions.upupper on the name column.",
      "offset": 8029.639,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "The upper column will contain uppercase",
      "offset": 8032.56,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "versions of all",
      "offset": 8034.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "names. We can create intermediary",
      "offset": 8035.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "columns that are only for",
      "offset": 8038.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "processing. This is useful to clarify",
      "offset": 8040.199,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "complex transformations requiring",
      "offset": 8042.639,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "multiple",
      "offset": 8044.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "steps. In this instance, we call the",
      "offset": 8045.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "split function with the name of the",
      "offset": 8048,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "column and the space character to split",
      "offset": 8049.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "on. This returns a list of words in a",
      "offset": 8051.639,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "column called splits. A very common",
      "offset": 8054.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "operation is converting string data to a",
      "offset": 8057.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "different type such as converting a",
      "offset": 8059.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "string column to an integer. We use the",
      "offset": 8061.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "cast function to perform the conversion",
      "offset": 8063.84,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "to an integer",
      "offset": 8065.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "type. While performing data cleaning",
      "offset": 8067,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "with Spark, you may need to interact",
      "offset": 8069.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with array type columns. These are",
      "offset": 8071.04,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "analogous to lists in normal Python",
      "offset": 8073.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "environments. One function we will use",
      "offset": 8076.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "is size, which returns the number of",
      "offset": 8078.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "items present in the specified array",
      "offset": 8081.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "type argument.",
      "offset": 8083.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Another commonly used function for array",
      "offset": 8084.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "types is get item. It takes an index",
      "offset": 8086.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "argument and returns the item present at",
      "offset": 8090,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "that index in the list",
      "offset": 8092.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "column. Spark has many more",
      "offset": 8094.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "transformations and utility functions",
      "offset": 8096,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "available. When using Spark in",
      "offset": 8098.28,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "production, make sure to reference the",
      "offset": 8100.4,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "documentation for available",
      "offset": 8102.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "options. We've discussed some of the",
      "offset": 8104.599,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "common operations used on Spark dataf",
      "offset": 8106.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "frame columns. Let's practice some of",
      "offset": 8108.96,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "these now.",
      "offset": 8110.96,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "We've looked at some of the power",
      "offset": 8115.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "available when using Sparks functions to",
      "offset": 8116.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "filter and modify our data frames. Let's",
      "offset": 8118.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "spend some time with some more advanced",
      "offset": 8121.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "options. The dataf frame transformations",
      "offset": 8123.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we've covered thus far are blanket",
      "offset": 8125.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "transformations, meaning they're applied",
      "offset": 8127.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "regardless of the",
      "offset": 8129.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "data. Often you want to conditionally",
      "offset": 8130.84,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "change some aspect of the contents.",
      "offset": 8133.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Spark provides some built-in conditional",
      "offset": 8136.159,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "clauses which act similar to an if then",
      "offset": 8138,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "else statement in a traditional",
      "offset": 8140.079,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "programming",
      "offset": 8141.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "environment. While it is possible to",
      "offset": 8142.599,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "perform a traditional if then else style",
      "offset": 8144.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "statement in Spark, it can lead to",
      "offset": 8147.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "serious performance degradation as each",
      "offset": 8149.119,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "row of a dataf frame would be evaluated",
      "offset": 8151.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "independently. Using the optimized",
      "offset": 8154.04,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "built-in conditionals alleviates this.",
      "offset": 8156.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "There are two components to the",
      "offset": 8159.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "conditional clauses when and the",
      "offset": 8160.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "optional otherwise. Let's look at them",
      "offset": 8162.88,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "in more",
      "offset": 8165.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "depth. The when clause is a method",
      "offset": 8166.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "available from the",
      "offset": 8168.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "pispark.sql.functions library that is",
      "offset": 8170.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "looking for two components. The if",
      "offset": 8172.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "condition and what to do if it evaluates",
      "offset": 8174.48,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "to true. This is best seen from an",
      "offset": 8176.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "example. Consider a data frame with the",
      "offset": 8180.199,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "name and age columns. We can actually",
      "offset": 8182.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "add an extra argument to our select",
      "offset": 8185.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "method when using the when clause. We",
      "offset": 8187.199,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "select the df.name and df.age age as",
      "offset": 8190,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "usual. For the third argument, we'll",
      "offset": 8193,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "define a when conditional. If the age",
      "offset": 8195.439,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "column is 18 or up, we'll add the string",
      "offset": 8198.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "adult. If the clause doesn't match,",
      "offset": 8201.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "nothing is",
      "offset": 8203.679,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "returned. Note that our return data",
      "offset": 8205.319,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "frame contains an unnamed column we",
      "offset": 8207.599,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "didn't define using with column. The",
      "offset": 8209.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "select function can create columns",
      "offset": 8212.399,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "dynamically based on the arguments",
      "offset": 8214.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "provided. Let's look at some more",
      "offset": 8216.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "examples.",
      "offset": 8218.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "You can chain multiple when statements",
      "offset": 8220.88,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "together similar to an if else if",
      "offset": 8222.719,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "structure. In this case, we define two",
      "offset": 8225.479,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "when clauses and return adult or minor",
      "offset": 8228.24,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "based on the age",
      "offset": 8230.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "column. You can chain as many when",
      "offset": 8232.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "clauses together as",
      "offset": 8234.399,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "required. In addition to when is the",
      "offset": 8236.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "otherwise",
      "offset": 8238.719,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "clause. Otherwise is analogous to the",
      "offset": 8239.8,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "else statement.",
      "offset": 8242.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "It takes a single argument which is what",
      "offset": 8244.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to return in case the when clause or",
      "offset": 8246.24,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "clauses do not evaluate as",
      "offset": 8248.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "true. In this example, we return adult",
      "offset": 8250.84,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "when the age column is 18 or higher.",
      "offset": 8253.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Otherwise, we return minor. The",
      "offset": 8256.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "resulting data frame is the same, but",
      "offset": 8259.12,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "the method is",
      "offset": 8260.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "different. While you can have multiple",
      "offset": 8262.439,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "win statements chained together, you can",
      "offset": 8264.8,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "only have a single otherwise per win",
      "offset": 8266.639,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "chain.",
      "offset": 8268.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Let's try a couple of examples of using",
      "offset": 8270.399,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "when and otherwise to modify some data",
      "offset": 8272.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "frames. We've looked at the built-in",
      "offset": 8276.92,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "functions in Spark and I've had great",
      "offset": 8279.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "results using these, but let's consider",
      "offset": 8280.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "what you would do if you needed to apply",
      "offset": 8283.84,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "some custom logic to your data cleaning",
      "offset": 8285.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "processes. A userdefined function or UDF",
      "offset": 8288.439,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "is a Python method that the user writes",
      "offset": 8291.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "to perform a specific bit of logic. Once",
      "offset": 8293.84,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "written, the method is called via the",
      "offset": 8297.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "pispark.sql.functions.f",
      "offset": 8300.359,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "method. The result is stored as a",
      "offset": 8301.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "variable and can be called as a normal",
      "offset": 8304.16,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "Spark function. Let's look at a couple",
      "offset": 8305.92,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "examples. Here's a fairly trivial",
      "offset": 8309.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "example to illustrate how a UDF is",
      "offset": 8311.519,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "defined. First, we define a Python",
      "offset": 8314.04,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "function. We'll call our function",
      "offset": 8316.679,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "reverse string with an argument called",
      "offset": 8318.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Meister. We'll use some Python shorthand",
      "offset": 8321.399,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "to reverse the string and return it.",
      "offset": 8323.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Don't worry about understanding how the",
      "offset": 8326.8,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "return statement works only that it will",
      "offset": 8328.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "reverse the lettering of whatever is fed",
      "offset": 8330.479,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "into it. For example, help becomes",
      "offset": 8332.24,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "pl. The next step is to wrap the",
      "offset": 8336.599,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "function and store it in a variable for",
      "offset": 8339.04,
      "duration": 2.599
    },
    {
      "lang": "en",
      "text": "later",
      "offset": 8340.719,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "use. We'll use the",
      "offset": 8341.639,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "pispark.sql.functions.f method. It takes",
      "offset": 8344.679,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "two arguments. The name of the method",
      "offset": 8347.359,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you just defined and the spark data type",
      "offset": 8349.359,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it will return.",
      "offset": 8351.439,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "This can be any of the options in",
      "offset": 8353.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "pispark.sql.types and can even be a more",
      "offset": 8356.28,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "complex type including a fully defined",
      "offset": 8358.479,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "schema",
      "offset": 8360.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "object. Most often you'll return either",
      "offset": 8361.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "a simple object type or perhaps an array",
      "offset": 8364.479,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "type. We'll call UDF with our new method",
      "offset": 8367.08,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "name and use the string type, then store",
      "offset": 8370.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "this as UDF reverse",
      "offset": 8372.479,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "string. Finally, we use our new UDF to",
      "offset": 8375.16,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "add a column to the user df data frame",
      "offset": 8378.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "within the width column method.",
      "offset": 8380.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Note that we pass the column we're",
      "offset": 8383.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "interested in as the argument to UDF",
      "offset": 8384.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "reverse string. The UDF function is",
      "offset": 8387.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "called for each row of the data frame.",
      "offset": 8390.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Under the hood, the UDF function takes",
      "offset": 8393.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the value stored for the specified",
      "offset": 8395.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "column per row and passes it to the",
      "offset": 8396.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Python method. The result is fed back to",
      "offset": 8399.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the resulting data frame. Another quick",
      "offset": 8401.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "example is using a function that does",
      "offset": 8404.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "not require an argument.",
      "offset": 8406.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "We're defining our sorting cap function",
      "offset": 8409.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to return one of the letters G, H, R, or",
      "offset": 8411.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "S at random. We still create our UDF",
      "offset": 8414.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "wrap function and define the return type",
      "offset": 8417.92,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "as string type. The primary difference",
      "offset": 8419.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is calling the function this time",
      "offset": 8422.479,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "without passing in an argument as it is",
      "offset": 8424.08,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "not",
      "offset": 8426.08,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "required. As always, the best way to",
      "offset": 8427.16,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "learn is practice. Let's create some",
      "offset": 8429.439,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "userdefined functions.",
      "offset": 8431.28,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "Welcome back to our discussion about",
      "offset": 8436,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "modifying and cleaning data frames.",
      "offset": 8437.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "We've discussed various transformations",
      "offset": 8440.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and methods to modify our data, but we",
      "offset": 8442,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "haven't covered much about how Spark",
      "offset": 8444.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "actually processes the data. Let's look",
      "offset": 8445.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "at that now. Spark breaks data frames",
      "offset": 8448.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "into partitions or chunks of data. These",
      "offset": 8450.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "partitions can be automatically defined,",
      "offset": 8454.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "enlarged, shrunk, and can differ greatly",
      "offset": 8456.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "based on the type of Spark cluster being",
      "offset": 8458.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "used.",
      "offset": 8460.479,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "The size of the partition does vary, but",
      "offset": 8461.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "generally try to keep your partition",
      "offset": 8464.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sizes equal. We'll discuss more about",
      "offset": 8465.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "optimizing partitioning and cluster",
      "offset": 8468.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "details later on. For now, let's assume",
      "offset": 8469.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "that each partition is handled",
      "offset": 8472.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "independently. This is part of what",
      "offset": 8475.08,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "provides the performance levels and",
      "offset": 8476.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "horizontal scaling ability in Spark.",
      "offset": 8478.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "If a Spark node doesn't need to compete",
      "offset": 8482,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for resources nor consult with other",
      "offset": 8484.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Spark nodes for answers, it can reliably",
      "offset": 8486.24,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "schedule the processing for the best",
      "offset": 8488.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "performance. In Spark, any",
      "offset": 8491.64,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "transformation operation is lazy. It's",
      "offset": 8493.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "more like a recipe than a command. It",
      "offset": 8496.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "defines what should be done with a dataf",
      "offset": 8499.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "frame rather than actually doing it.",
      "offset": 8500.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Most operations at Spark are actually",
      "offset": 8503.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "transformations, including with column,",
      "offset": 8505.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "select, filter, and so forth.",
      "offset": 8507.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "The set of transformations you define",
      "offset": 8510.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "are only executed when you run a Spark",
      "offset": 8512.56,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "action. This includes count, write, etc.",
      "offset": 8514.84,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "Anything that requires the",
      "offset": 8518.479,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "transformations to be run to properly",
      "offset": 8519.68,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "obtain an",
      "offset": 8521.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "answer. Spark can reorder",
      "offset": 8522.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "transformations for the best",
      "offset": 8524.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "performance. Usually, this isn't",
      "offset": 8526.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "noticeable, but can occasionally cause",
      "offset": 8528.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "unexpected behavior, such as IDs not",
      "offset": 8530.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "being added until after other",
      "offset": 8532.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "transformations have completed. This",
      "offset": 8534.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "doesn't actually cause a problem, but",
      "offset": 8537.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the data can look unusual if you don't",
      "offset": 8538.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "know what to",
      "offset": 8540.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "expect. Relational databases tend to",
      "offset": 8541.64,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "have a field used to identify the row,",
      "offset": 8544.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "whether it is for an actual relationship",
      "offset": 8546.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "reference or just for data",
      "offset": 8548.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "identification. These IDs are typically",
      "offset": 8551,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "an integer that increases in value, is",
      "offset": 8553.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "sequential, and most importantly",
      "offset": 8555.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "unique. The problem with these IDs is",
      "offset": 8558.28,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "they're not very parallel in nature.",
      "offset": 8560.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Given that the values are given out",
      "offset": 8563.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "sequentially if there are multiple",
      "offset": 8564.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "workers they must all refer to a common",
      "offset": 8566.64,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "source for the next",
      "offset": 8568.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "entry. This is okay in a single server",
      "offset": 8570.28,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "environment but in a distributed",
      "offset": 8572.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "platform such as Spark it creates some",
      "offset": 8574.399,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "undo",
      "offset": 8576.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "bottlenecks. Let's take a look at how to",
      "offset": 8577.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "generate IDs in",
      "offset": 8579.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Spark. Spark has a built-in function",
      "offset": 8580.68,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "called monotonically increasing ID",
      "offset": 8583.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "designed to provide an integer ID that",
      "offset": 8585.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "increases in value and is unique. These",
      "offset": 8587.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "IDs are not necessarily sequential.",
      "offset": 8590.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "There can be gaps, often quite large,",
      "offset": 8593.52,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "between",
      "offset": 8595.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "values. Unlike a normal relational ID,",
      "offset": 8596.92,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "Sparks is completely parallel. Each",
      "offset": 8600.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "partition is allocated up to 8 billion",
      "offset": 8602.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "IDs that can be assigned. Notice that",
      "offset": 8605.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the ID fields in the sample table are",
      "offset": 8607.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "integers, increasing in value, but are",
      "offset": 8609.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "not sequential.",
      "offset": 8612.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "It's a little out of scope, but the IDs",
      "offset": 8614.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "are a 64-bit number effectively split",
      "offset": 8616.56,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "into groups based on the Spark",
      "offset": 8618.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "partition. Each group contains 8.4",
      "offset": 8620.68,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "billion IDs, and there are 2.1 billion",
      "offset": 8623.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "possible groups, none of which",
      "offset": 8626.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "overlap. There's a lot of nuance to how",
      "offset": 8628.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "partitions and the monotonically",
      "offset": 8630.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "increasing IDs work. Remembering that",
      "offset": 8632.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Spark is lazy, it often helps in",
      "offset": 8635.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "troubleshooting what can happen.",
      "offset": 8637.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "operations are often out of order,",
      "offset": 8639.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "especially if joins are involved. It's",
      "offset": 8641.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "best to test your",
      "offset": 8643.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "transformations. We've discussed a lot",
      "offset": 8645.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "of detail in this lesson. Let's now take",
      "offset": 8647.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a look at a few exercises that will help",
      "offset": 8650,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "solidify how everything",
      "offset": 8651.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "works. Now that we've discussed some",
      "offset": 8656.04,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "data cleaning tasks using Spark, let's",
      "offset": 8658.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "look at how to improve the performance",
      "offset": 8660.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "of running those tasks using caching.",
      "offset": 8661.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Caching in Spark refers to storing the",
      "offset": 8664.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "results of a dataf frame in memory or on",
      "offset": 8666.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "disk of the processing nodes in a",
      "offset": 8668.96,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "cluster. Caching improves the speed for",
      "offset": 8671,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "subsequent transformations or actions as",
      "offset": 8673.439,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the data likely no longer needs to be",
      "offset": 8675.92,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "retrieved from the original data",
      "offset": 8677.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "source. Using caching reduces the",
      "offset": 8679.8,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "resource utilization of the cluster.",
      "offset": 8682.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "There's less need to access the storage,",
      "offset": 8684.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "networking, and CPU of the Spark nodes",
      "offset": 8686.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "as the data is likely already present.",
      "offset": 8688.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "There are a few disadvantages of caching",
      "offset": 8691.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you should be aware of. Very large data",
      "offset": 8693.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "sets may not fit in the memory reserved",
      "offset": 8695.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "for cache data frames. Depending on the",
      "offset": 8697.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "later transformations requested, the",
      "offset": 8700.56,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "cache may not do anything to help",
      "offset": 8702.399,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "performance. If a data set does not stay",
      "offset": 8704.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "cached in memory, it may be persisted to",
      "offset": 8707.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "disk. Depending on the disk",
      "offset": 8709.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "configuration of a sport cluster, this",
      "offset": 8711.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "may not be a large performance",
      "offset": 8714,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "improvement. If you're reading from a",
      "offset": 8715.88,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "local network resource and have slow",
      "offset": 8717.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "local disc IO, it may be better to avoid",
      "offset": 8719.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "caching the",
      "offset": 8722.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "objects. Finally, the lifetime of a",
      "offset": 8723.64,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "cached object is not guaranteed. Spark",
      "offset": 8726.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "handles regenerating data frames for you",
      "offset": 8728.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "automatically, but this can cause delays",
      "offset": 8730.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "in",
      "offset": 8732.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "processing. Caching is incredibly",
      "offset": 8733.64,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "useful, but only if you plan to use the",
      "offset": 8735.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "dataf frame again. If you only need it",
      "offset": 8737.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "for a single task, it's not worth",
      "offset": 8740.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "caching.",
      "offset": 8741.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "The best way to gauge performance with",
      "offset": 8743.68,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "caching is to test various",
      "offset": 8745.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "configurations. Try caching your data",
      "offset": 8747.8,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "frames at various points in the",
      "offset": 8750,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "processing cycle and check if it",
      "offset": 8751.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "improves your processing time. Try to",
      "offset": 8753.12,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "cache in memory or fast NVMe or SSD",
      "offset": 8755.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "storage. While still slower than main",
      "offset": 8759.479,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "memory, modern SSDbased storage is",
      "offset": 8761.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "drastically faster than spinning disc.",
      "offset": 8763.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Local spinning hard drives can still be",
      "offset": 8766.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "useful if you are processing large dataf",
      "offset": 8768.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "frames that require a lot of steps to",
      "offset": 8770.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "degenerate or must be accessed over the",
      "offset": 8772.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "internet. Testing this is crucial. If",
      "offset": 8774.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "normal caching doesn't seem to work, try",
      "offset": 8777.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "creating intermediate paret",
      "offset": 8779.92,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "representations like we did in chapter",
      "offset": 8781.359,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "1. These can provide a checkpoint in",
      "offset": 8783.319,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "case a job fails mid task and can still",
      "offset": 8785.84,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "be used with caching to further improve",
      "offset": 8788.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "performance. Finally, you can manually",
      "offset": 8791.16,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "stop caching a dataf frame when you're",
      "offset": 8793.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "finished with it. This frees up cache",
      "offset": 8795.12,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "resources for other dataf",
      "offset": 8797.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "frames. Implementing caching in Spark is",
      "offset": 8798.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "simple. The primary way is to call the",
      "offset": 8801.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "function cache on a dataf frame object",
      "offset": 8804,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "prior to a given action. It requires no",
      "offset": 8806.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "arguments. One example is creating a",
      "offset": 8809.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "dataf frame from some original CSV",
      "offset": 8811.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "data. Prior to running a count on the",
      "offset": 8814.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "data, we call cache to tell Spark to",
      "offset": 8816.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "store it in cache. Another option is to",
      "offset": 8818.8,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "call cache separately. Here we create an",
      "offset": 8821.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "ID in one transformation. Then we call",
      "offset": 8824.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "cache on the dataf frame. When we call",
      "offset": 8826.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the show action, the voter df data frame",
      "offset": 8829.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "will be",
      "offset": 8831.6,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "cached. If you're following closely,",
      "offset": 8833,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this means that cache is a spark",
      "offset": 8835.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "transformation. Nothing is actually",
      "offset": 8837.24,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "cached until an action is called. A",
      "offset": 8839.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "couple other options are available with",
      "offset": 8842,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "caching and spark.",
      "offset": 8843.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "To check if a dataf frame is cached, use",
      "offset": 8845.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the is cached boolean property which",
      "offset": 8848.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "returns true as in this case or false.",
      "offset": 8850.319,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "To uncach a dataf frame, we call",
      "offset": 8853.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "unpersist with no arguments. This",
      "offset": 8855.6,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "removes the object from the",
      "offset": 8858.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "cache. We've discussed caching in depth.",
      "offset": 8859.8,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "Let's practice how to use",
      "offset": 8862.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it. We've discussed the benefits of",
      "offset": 8866.359,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "caching when working with Spark data",
      "offset": 8868.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "frames. Let's look at how to improve the",
      "offset": 8870.439,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "speed when getting data into a data",
      "offset": 8872.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "frame.",
      "offset": 8874.399,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "Spark clusters consist of two types of",
      "offset": 8875.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "processes. One driver process and as",
      "offset": 8877.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "many worker processes as",
      "offset": 8880.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "required. The driver handles task",
      "offset": 8882.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "assignments and consolidation of the",
      "offset": 8884.96,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "data results from the workers. The",
      "offset": 8886.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "workers typically handle the actual",
      "offset": 8889.439,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "transformation action steps of his work",
      "offset": 8891.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "job. Once assigned tasks, they operate",
      "offset": 8893.24,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "fairly independently and report results",
      "offset": 8896.319,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "back to the",
      "offset": 8898.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "driver. It is possible to have a single",
      "offset": 8899.8,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "node sport cluster. This is what we're",
      "offset": 8902.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "using for this course, but you'll rarely",
      "offset": 8904.399,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "see this in a production",
      "offset": 8906.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "environment. There are different ways to",
      "offset": 8908.439,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "run Spark clusters. The method used",
      "offset": 8910.319,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "depends on your specific",
      "offset": 8912.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "environment. When importing data to",
      "offset": 8914.6,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "Spark data frames, it's important to",
      "offset": 8916.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "understand how the cluster implements",
      "offset": 8918.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the job. The process varies depending on",
      "offset": 8920.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the type of task, but it's safe to",
      "offset": 8922.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "assume that the more import objects",
      "offset": 8924.64,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "available, the better the cluster can",
      "offset": 8926.479,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "divvy up the job. This may not matter on",
      "offset": 8928.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "a single node cluster, but with larger",
      "offset": 8931.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "clusters, each worker can take part in",
      "offset": 8933.04,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "the import",
      "offset": 8934.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "process. In clearer terms, one large",
      "offset": 8935.88,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "file will perform considerably worse",
      "offset": 8938.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "than many smaller ones. Depending on the",
      "offset": 8940.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "configuration of your cluster, you may",
      "offset": 8943.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "not be able to process larger files, but",
      "offset": 8945.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "could easily handle the same amount of",
      "offset": 8947.84,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "data split between smaller",
      "offset": 8949.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "files. Note, you can define a single",
      "offset": 8952.04,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "import statement even if there are",
      "offset": 8954.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "multiple files. You can use any form of",
      "offset": 8956.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "standard wild card symbol when defining",
      "offset": 8958.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the import file name. While less",
      "offset": 8960.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "important, if objects are about the same",
      "offset": 8963.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "size, the cluster will perform better",
      "offset": 8964.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "than having a mix of very large and very",
      "offset": 8966.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "small",
      "offset": 8968.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "objects. If you remember from chapter 1,",
      "offset": 8969.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we discussed the importance of Spark",
      "offset": 8972.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "schemas. Well-defined schemas in Spark",
      "offset": 8974.6,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "drastically improve import performance.",
      "offset": 8977.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Without a schema defined, import tasks",
      "offset": 8980,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "require reading the data multiple times",
      "offset": 8982.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to infer structure. This is very slow",
      "offset": 8983.92,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "when you have a lot of data. Spark may",
      "offset": 8986.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "not define the objects in the data the",
      "offset": 8989.439,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "same as you would. Spark schemas also",
      "offset": 8991.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "provide validation on import. This can",
      "offset": 8994.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "save steps with data cleaning jobs and",
      "offset": 8997.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "improve the overall processing time.",
      "offset": 8998.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "There are various effective ways to",
      "offset": 9001.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "split an object files mostly into more",
      "offset": 9003.28,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "smaller",
      "offset": 9005.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "objects. The first is to use built-in OS",
      "offset": 9007.24,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "utilities such as split, cut, or a. An",
      "offset": 9010.24,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "example using split uses the -l argument",
      "offset": 9014,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "with the number of lines to have per",
      "offset": 9016.56,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "file 10,000 in this",
      "offset": 9018.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "case. The - d argument tells split to",
      "offset": 9020.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "use numeric",
      "offset": 9023.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "suffixes. The last two arguments are the",
      "offset": 9024.359,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "name of the file to be split and the",
      "offset": 9026.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "prefix to be used. Assuming large file",
      "offset": 9028.479,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "has 10 million records, we would have",
      "offset": 9031.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "files named",
      "offset": 9033.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "chunk-00000000 through chunk 9999.",
      "offset": 9035.64,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "Another method is to use Python or any",
      "offset": 9040.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "other language to split the objects up",
      "offset": 9042.399,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "as we see",
      "offset": 9044.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "fit. Sometimes you may not have the",
      "offset": 9045.24,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "tools available to split a large file.",
      "offset": 9047.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "If you're going to be working with a",
      "offset": 9050,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "dataf frame often, a simple method is to",
      "offset": 9051.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "read in the single file then write it",
      "offset": 9053.12,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "back out as",
      "offset": 9054.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "paret. We've done this in previous",
      "offset": 9056.04,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "examples and it works well for later",
      "offset": 9058.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "analysis even if the initial import is",
      "offset": 9060.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "slow.",
      "offset": 9062.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "It's important to note that if you're",
      "offset": 9064,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "hitting limitations due to cluster",
      "offset": 9065.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "sizing, try to do as little processing",
      "offset": 9067.6,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "as possible before writing to",
      "offset": 9069.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "parquet. Let's practice some of the",
      "offset": 9072.2,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "import tricks we've discussed.",
      "offset": 9074.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Now, we've just finished working with",
      "offset": 9077.64,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "improving import performance in Spark.",
      "offset": 9080.08,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "Let's take a look at cluster",
      "offset": 9082.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "configurations. Spark has many available",
      "offset": 9084.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "configuration settings controlling all",
      "offset": 9087.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "aspects of the installation.",
      "offset": 9088.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "These configurations can be modified to",
      "offset": 9091.28,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "best match the specific needs for the",
      "offset": 9093.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "cluster. The configurations are",
      "offset": 9095.399,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "available in the configuration files via",
      "offset": 9097.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the Spark web interface and via the",
      "offset": 9099.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "runtime code. Our test cluster is only",
      "offset": 9102,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "accessible via command shell. So we'll",
      "offset": 9105.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "use the last",
      "offset": 9107.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "option. To read a configuration setting,",
      "offset": 9108.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "call",
      "offset": 9110.88,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "spark.conf.get with the name and the",
      "offset": 9111.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "setting as the",
      "offset": 9113.439,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "argument. To write a configuration",
      "offset": 9114.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "setting, call spark.conf. set with the",
      "offset": 9116.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "name of the setting and the actual value",
      "offset": 9119.52,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "as the function",
      "offset": 9121.28,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "arguments. Spark deployments can vary",
      "offset": 9122.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "depending on the exact needs of the",
      "offset": 9125.439,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "users. One component of a deployment is",
      "offset": 9127.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the cluster management",
      "offset": 9130.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "mechanism. Spark clusters can be single",
      "offset": 9132.2,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "node clusters deploying all components",
      "offset": 9135.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "on a single system physical VM or",
      "offset": 9137.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "container.",
      "offset": 9141.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "standalone clusters with dedicated",
      "offset": 9142.64,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "machines as the driver and",
      "offset": 9144.72,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "workers or managed clusters meaning that",
      "offset": 9146.84,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the cluster components are handled by a",
      "offset": 9149.439,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "third party cluster manager such as",
      "offset": 9151.28,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "yarn, msos or",
      "offset": 9152.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "kubernetes. In this course, we're using",
      "offset": 9154.92,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "a single node cluster. Your production",
      "offset": 9156.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "environment can vary wildly, but we'll",
      "offset": 9159.359,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "discuss standalone clusters as the",
      "offset": 9161.52,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "concepts flow across all management",
      "offset": 9163.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "types. If you recall, there is one",
      "offset": 9165.319,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "driver per sport cluster. The driver is",
      "offset": 9167.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "responsible for several things including",
      "offset": 9170.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the following. Handling task assignment",
      "offset": 9172.16,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "to the various nodes or processes in the",
      "offset": 9174.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "cluster. The driver monitors the state",
      "offset": 9177.399,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "of all processes and tasks and handles",
      "offset": 9179.92,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "any task",
      "offset": 9182.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "retries. The driver is also responsible",
      "offset": 9183.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "for consolidating results from the other",
      "offset": 9185.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "processes in the",
      "offset": 9187.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "cluster. The driver handles any access",
      "offset": 9189.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "to shared data and verifies each worker",
      "offset": 9192.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "process has the necessary resources,",
      "offset": 9194.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "code, data, etc.",
      "offset": 9197.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Given the importance of the driver, it",
      "offset": 9199.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "is often worth increasing the",
      "offset": 9201.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "specifications of the node compared to",
      "offset": 9202.64,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "the other",
      "offset": 9204.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "systems. Doubling the memory compared to",
      "offset": 9205.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the other nodes is recommended. This is",
      "offset": 9208.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "useful for task monitoring and data",
      "offset": 9210.8,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "consolidation",
      "offset": 9212.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "tasks. As with all Spark systems, fast",
      "offset": 9214.12,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "local storage is useful for running",
      "offset": 9216.96,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Spark in an ideal",
      "offset": 9218.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "setup. The Spark worker handles running",
      "offset": 9220.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "tasks assigned by the driver and",
      "offset": 9222.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "communicates those results back to the",
      "offset": 9224.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "driver. Ideally, the worker has a copy",
      "offset": 9226.319,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of all code, data, and access to the",
      "offset": 9229.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "necessary resources required to complete",
      "offset": 9231.439,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "a given task. If any of these are",
      "offset": 9233.359,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "unavailable, the worker must pause to",
      "offset": 9236,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "obtain the",
      "offset": 9237.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "resources. When sizing a cluster, there",
      "offset": 9238.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "are a few",
      "offset": 9241.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "recommendations. Depending on the type",
      "offset": 9242.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "of task, more worker nodes is often",
      "offset": 9244.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "better than larger nodes. This can be",
      "offset": 9246.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "especially obvious during import and",
      "offset": 9249.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "export operations, as there are more",
      "offset": 9250.8,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "machines available to do the work. As",
      "offset": 9252.399,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "with everything in Spark, test various",
      "offset": 9255.439,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "configurations to find the correct",
      "offset": 9257.28,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "balance for your workload. Assuming a",
      "offset": 9258.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "cloud environment, 16 worker nodes may",
      "offset": 9261.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "complete a job in an hour and cost $50",
      "offset": 9263.84,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "in",
      "offset": 9265.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "resources. An eight worker configuration",
      "offset": 9267.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "might take 1 and a4 hours, but cost only",
      "offset": 9269.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "half as much. Finally, workers can make",
      "offset": 9272.16,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "use of fast local storage, SSD, or NVMe",
      "offset": 9275.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "for caching intermediate files, etc.",
      "offset": 9278.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Now that we've discussed cluster sizing",
      "offset": 9281.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and configuration, let's practice",
      "offset": 9283.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "working with these",
      "offset": 9285.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "options. We've discussed Spark clusters",
      "offset": 9289.16,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "and improving import performance. Let's",
      "offset": 9291.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "look at how to improve the performance",
      "offset": 9294.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of Spark tasks in general. To understand",
      "offset": 9295.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "performance implications of Spark, you",
      "offset": 9298.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "must be able to see what it's doing",
      "offset": 9300.479,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "under the hood. The easiest way to do",
      "offset": 9301.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this is to use the explain function on a",
      "offset": 9304.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "dataf frame. This example is taken from",
      "offset": 9306.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "an earlier exercise. simply requesting a",
      "offset": 9309.359,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "single column and running distinct",
      "offset": 9311.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "against it. The result is the estimated",
      "offset": 9313.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "plan that will be run to generate the",
      "offset": 9316.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "results from the data frame. Don't worry",
      "offset": 9318.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "about the specifics of the plan yet.",
      "offset": 9320.56,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "Just remember how to view it if",
      "offset": 9322.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "needed. Spark distributes data amongst",
      "offset": 9324.359,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the various nodes in the cluster. A side",
      "offset": 9326.88,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "effect of this is what is known as",
      "offset": 9329.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "shuffling. Shuffling is moving of data",
      "offset": 9331.319,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "fragments to various workers as required",
      "offset": 9333.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to complete certain tasks.",
      "offset": 9335.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Shuffling is useful and hides overall",
      "offset": 9338.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "complexity from the user. The user",
      "offset": 9340.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "doesn't have to know which nodes have",
      "offset": 9342.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "what data. That being said, it can be",
      "offset": 9344.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "slow to complete the necessary",
      "offset": 9347.28,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "transfers, especially if a few nodes",
      "offset": 9348.64,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "require all the",
      "offset": 9350.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "data. Shuffling lowers the overall",
      "offset": 9352.04,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "throughput of the cluster as the workers",
      "offset": 9354.479,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "must spend time waiting for the data to",
      "offset": 9356.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "transfer. This limits the amount of",
      "offset": 9358.04,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "available workers for the remaining",
      "offset": 9360.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tasks in the system.",
      "offset": 9361.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Shuffling is often a necessary",
      "offset": 9364.24,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "component, but it's helpful to try to",
      "offset": 9365.92,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "minimize it as much as",
      "offset": 9367.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "possible. It can be tricky to remove",
      "offset": 9369.319,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "shuffling operations entirely, but there",
      "offset": 9371.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "are a few things that can limit it. The",
      "offset": 9373.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "dataf frame repartition function takes a",
      "offset": 9376.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "single argument, the number of",
      "offset": 9378.319,
      "duration": 2.441
    },
    {
      "lang": "en",
      "text": "partitions",
      "offset": 9379.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "requested. We've used this in an earlier",
      "offset": 9380.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "chapter to illustrate the effect of",
      "offset": 9383.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "partitions with monotonically increasing",
      "offset": 9384.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "ID function.",
      "offset": 9386.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Repartitioning requires a full shuffle",
      "offset": 9388.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of data between nodes and processes and",
      "offset": 9390.56,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "it's quite",
      "offset": 9392.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "costly. If you need to reduce the number",
      "offset": 9393.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of partitions, use the coales function",
      "offset": 9396.399,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "instead. It takes a number of partitions",
      "offset": 9398.92,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "smaller than the current one and",
      "offset": 9401.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "consolidates the data without requiring",
      "offset": 9402.8,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "a full data",
      "offset": 9404.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "shuffle. Note, calling coales with a",
      "offset": 9405.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "larger number of partitions does not",
      "offset": 9408.56,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "actually do",
      "offset": 9410.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "anything. The join function is a great",
      "offset": 9411.24,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "use of spark and provides a lot of",
      "offset": 9413.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "power. Calling join indiscriminately can",
      "offset": 9415.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "often cause shuffle operations leading",
      "offset": 9418.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to increased cluster load and slower",
      "offset": 9420.399,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "processing",
      "offset": 9422.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "times. To avoid some of the shuffle",
      "offset": 9423.319,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "operations when joining Spark data",
      "offset": 9425.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "frames, you can use the broadcast",
      "offset": 9427.28,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "function. I'll talk about this more in a",
      "offset": 9428.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "moment. Finally, an important note about",
      "offset": 9431.479,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "data cleaning operations is remembering",
      "offset": 9433.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "to optimize for what matters. The speed",
      "offset": 9435.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of your initial code may be perfectly",
      "offset": 9438.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "acceptable and time may be better spent",
      "offset": 9440.08,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "elsewhere.",
      "offset": 9441.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Broadcasting in Spark is a method to",
      "offset": 9443.359,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "provide a copy of an object to each",
      "offset": 9445.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "worker. When each worker has its own",
      "offset": 9447.56,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "copy of the data, there is less need for",
      "offset": 9449.84,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "communication between",
      "offset": 9451.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "nodes. This limits data shuffles and",
      "offset": 9453.479,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "it's more likely a node will fulfill",
      "offset": 9456.08,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "tasks",
      "offset": 9458,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "independently. Using broadcasting can",
      "offset": 9459.24,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "drastically speed up join operations,",
      "offset": 9461.439,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "especially if one of the dataf frames",
      "offset": 9463.52,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "being joined is much smaller than the",
      "offset": 9464.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "other. To implement broadcasting, you",
      "offset": 9466.92,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "must import the broadcast function from",
      "offset": 9469.52,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "pispark.sql.function.",
      "offset": 9471.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "functions. Once imported, simply call",
      "offset": 9473.16,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "the broadcast function with the name of",
      "offset": 9475.439,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "the data frame you wish to",
      "offset": 9476.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "broadcast. Note, broadcasting can slow",
      "offset": 9479.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "operations when using very small data",
      "offset": 9481.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "frames or if you broadcast the larger",
      "offset": 9483.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "data frame in a join. Spark will often",
      "offset": 9485.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "optimize this for you, but as usual, run",
      "offset": 9488.56,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "tests in your environment for best",
      "offset": 9490.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "performance. We've looked how to limit",
      "offset": 9492.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "shuffling and implement broadcasting for",
      "offset": 9494.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "data frames. Let's practice utilizing",
      "offset": 9496.56,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "these tools now.",
      "offset": 9498.8,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "We've spent most of this course working",
      "offset": 9503.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "with individual transformations and",
      "offset": 9504.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "actions to clean data in Spark. But data",
      "offset": 9506.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is rarely so simple that a couple",
      "offset": 9509.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "transformations or actions can prepare",
      "offset": 9511.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "for real analysis. Let's look now at",
      "offset": 9512.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "data",
      "offset": 9515.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "pipelines. Data pipelines are simply the",
      "offset": 9517,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "set of steps needed to move from an",
      "offset": 9519.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "input data source or sources and convert",
      "offset": 9521.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it to the desired output.",
      "offset": 9523.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "A data pipeline can consist of any",
      "offset": 9525.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "number of steps or components and can",
      "offset": 9527.76,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "span many",
      "offset": 9529.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "systems. For our purposes, we'll be",
      "offset": 9531.319,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "setting up a data pipeline within Spark,",
      "offset": 9533.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but realize that a full production data",
      "offset": 9535.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "pipeline will likely communicate with",
      "offset": 9537.68,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "many",
      "offset": 9539.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "systems. Much like Spark in general, a",
      "offset": 9540.439,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "data pipeline typically consists of",
      "offset": 9542.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "inputs, transformations, and the outputs",
      "offset": 9544.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "of those",
      "offset": 9546.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "steps. In addition, there is often",
      "offset": 9547.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "validation and analysis steps before",
      "offset": 9550.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "delivery of the data to the next user.",
      "offset": 9552.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "An input can be any of the data types",
      "offset": 9555.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we've looked at so far including CSV,",
      "offset": 9557.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "JSON, text, etc. It can be from the",
      "offset": 9559.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "local file system or from web services,",
      "offset": 9562.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "APIs, databases, and so on. The basic",
      "offset": 9564.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "idea is to read the data into a data",
      "offset": 9568.08,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "frame as we've done",
      "offset": 9569.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "previously. Once we have the data in a",
      "offset": 9571.319,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "data frame, we need to transform it in",
      "offset": 9573.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "some fashion.",
      "offset": 9575.28,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "You've done the individual steps several",
      "offset": 9576.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "times throughout this course. Adding",
      "offset": 9578.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "columns, filtering rows, performing",
      "offset": 9580.56,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "calculations as",
      "offset": 9582.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "needed. In our previous examples, we've",
      "offset": 9584.6,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "only done one or two of these steps at a",
      "offset": 9587.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "time, but a pipeline can consist of as",
      "offset": 9589.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "many of these steps as needed so we can",
      "offset": 9591.52,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "format the data into our desired",
      "offset": 9593.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "output. After we've defined our",
      "offset": 9596.359,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "transformations, we need to output the",
      "offset": 9598.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "data into a usable form. You've already",
      "offset": 9600.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "written files out to CSV or parquet",
      "offset": 9602.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "format, but it could include multiple",
      "offset": 9605.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "copies with various formats or instead",
      "offset": 9607.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "write the output to a database, a web",
      "offset": 9609.6,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "service,",
      "offset": 9611.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "etc. The last two steps vary greatly",
      "offset": 9612.84,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "depending on your needs. We'll discuss",
      "offset": 9615.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "validation in a later lesson, but the",
      "offset": 9617.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "idea is to run some form of testing on",
      "offset": 9619.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "the data to verify it is as",
      "offset": 9621.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "expected. Analysis is often the final",
      "offset": 9624.6,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "step before handing the data off to the",
      "offset": 9627.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "next user. This can include things such",
      "offset": 9628.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "as row counts, specific calculations, or",
      "offset": 9631.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "pretty much anything that makes it",
      "offset": 9634,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "easier for the user to consume the data",
      "offset": 9635.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "set. It's important to note that in",
      "offset": 9638.04,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "Spark, a data pipeline is not a formally",
      "offset": 9640.319,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "defined object, but rather a",
      "offset": 9642.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "concept. This is different than if",
      "offset": 9644.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you've used the pipeline object in",
      "offset": 9646.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Spark.ml. If you haven't, don't worry.",
      "offset": 9648.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "It's not needed for this",
      "offset": 9651.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "course. For our purposes, a Spark",
      "offset": 9652.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "pipeline is all the normal code required",
      "offset": 9655.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "to complete a task.",
      "offset": 9656.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "In this example, we're doing the various",
      "offset": 9659.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "tasks required to define a schema, read",
      "offset": 9661.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "a data file, add an ID, then write out",
      "offset": 9663.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "two separate data types. The task could",
      "offset": 9666.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "be much more complex, but the concept is",
      "offset": 9669.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "usually the",
      "offset": 9671.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "same. When you look at the components, a",
      "offset": 9672.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "data pipeline in Spark is fairly simple,",
      "offset": 9674.88,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "but can be very",
      "offset": 9676.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "powerful. Let's start working on a more",
      "offset": 9678.52,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "elaborate data pipeline.",
      "offset": 9680.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Now, we've worked with many aspects of",
      "offset": 9685,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Spark when it comes to data cleaning",
      "offset": 9687.439,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "operations. Let's look at how to use",
      "offset": 9689.319,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "some of the methods we've learned to",
      "offset": 9691.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "parse unconventional data. When reading",
      "offset": 9692.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "data into Spark, you're rarely given a",
      "offset": 9695.359,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "fully uniform file. Often, there is",
      "offset": 9697.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "content that needs to be removed or",
      "offset": 9699.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "reformatted.",
      "offset": 9701.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Some common issues include incorrect",
      "offset": 9703.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "data consisting of empty rows, commented",
      "offset": 9705.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "lines, headers, or even rows that don't",
      "offset": 9708.319,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "match the intended",
      "offset": 9710.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "schema. Real world data often includes",
      "offset": 9711.88,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "nested structures, including columns",
      "offset": 9714.479,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "that use different",
      "offset": 9716.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "delimiters. This could include the",
      "offset": 9717.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "primary columns separated via comma, but",
      "offset": 9719.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "including some components separated via",
      "offset": 9722.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "semicolon. Real data often won't fit",
      "offset": 9725,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "into a tabular format, sometimes",
      "offset": 9727.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "consisting of a differing number of",
      "offset": 9729.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "columns per row.",
      "offset": 9730.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "There are various ways to parse data in",
      "offset": 9733.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "all these situations. The way you choose",
      "offset": 9734.8,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "will depend on your specific",
      "offset": 9737.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "needs. We are focusing on CSV data for",
      "offset": 9739.16,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "this course, but the general scenarios",
      "offset": 9742,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "described apply to other formats as",
      "offset": 9743.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "well. For this chapter, we're going to",
      "offset": 9746.12,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "use the Stanford imageet annotations,",
      "offset": 9748.399,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which focus on finding and identifying",
      "offset": 9750.479,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "dogs in various imageet images.",
      "offset": 9752.479,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "The annotations provide a list of all",
      "offset": 9755.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "identified dogs in an image, including",
      "offset": 9757.6,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "when multiple dogs are in the same",
      "offset": 9759.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "image. Other metadata is included,",
      "offset": 9762.28,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "including the folder within the imageet",
      "offset": 9764.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "data set, the image dimensions, and the",
      "offset": 9766.64,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "bounding box or boxes of the dogs in the",
      "offset": 9769.04,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "image. In the example rows, we have the",
      "offset": 9772.68,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "folder names, the imageet image",
      "offset": 9775.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "reference, width, and height. Then there",
      "offset": 9777.359,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is the image data for the type of dog or",
      "offset": 9780.319,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "dogs in each image. Each breed column",
      "offset": 9782.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "consists of the breed name and the",
      "offset": 9785.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "bounding box in the image. The first row",
      "offset": 9787.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "contains one new found, but notice that",
      "offset": 9789.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the second row actually has two bull",
      "offset": 9791.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "mastiffs identified and an additional",
      "offset": 9793.68,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "column",
      "offset": 9795.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "defined. Spark CSV parser can handle",
      "offset": 9797.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "many common data issues via optional",
      "offset": 9799.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "parameters. Blake lines are",
      "offset": 9802.68,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "automatically removed unless",
      "offset": 9804.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "specifically instructed otherwise. When",
      "offset": 9805.92,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "using the CSV",
      "offset": 9807.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "parsing, comments can be removed with an",
      "offset": 9809.56,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "optional named argument, comment, and",
      "offset": 9812.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "specifying the character that any",
      "offset": 9814.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "comment line would be defined by. Note",
      "offset": 9816.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "that this handles lines that begin with",
      "offset": 9819.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a specific comment. Parsing more complex",
      "offset": 9820.64,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "comment usage requires more involved",
      "offset": 9823.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "procedures. Header rows can be parsed",
      "offset": 9825.88,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "via an optional parameter named header",
      "offset": 9828.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and set to true or false. If no schema",
      "offset": 9830.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "is defined, column names will be",
      "offset": 9834.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "initially set as defined by the header.",
      "offset": 9835.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "If a schema is defined, the row is not",
      "offset": 9838.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "used as data, but the header names are",
      "offset": 9840.479,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "otherwise",
      "offset": 9842.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "ignored. When importing CSV data into",
      "offset": 9843.24,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "Spork, it will automatically create",
      "offset": 9845.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "dataf frame columns if it can. It will",
      "offset": 9847.439,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "split a row of text from the CSV on a",
      "offset": 9849.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "defined separator argument named SE. If",
      "offset": 9852,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "SE is not defined, it will default to",
      "offset": 9855.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "using a comma.",
      "offset": 9857.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "The CSV parser will still succeed in",
      "offset": 9859.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "parsing data if the separator character",
      "offset": 9861.6,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "is not within the",
      "offset": 9863.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "string. It will store the entire row in",
      "offset": 9864.92,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "a column named underscore C0 by default.",
      "offset": 9867.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Using this trick allows parsing of",
      "offset": 9870.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "nested or complex data. We'll look at",
      "offset": 9872.319,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "this more later",
      "offset": 9874.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "on. Let's practice working with this",
      "offset": 9875.88,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "data and extending our data pipeline",
      "offset": 9878.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "further. Welcome back. Validation is one",
      "offset": 9882.76,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "step of a data pipeline we haven't",
      "offset": 9886.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "covered yet, but it is very important in",
      "offset": 9887.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "verifying the quality of the data we're",
      "offset": 9889.52,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "delivering. Let's look at how to",
      "offset": 9891.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "implement validation steps in a data",
      "offset": 9893.439,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "cleaning pipeline. In this context,",
      "offset": 9895.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "validation is verifying that a data set",
      "offset": 9898.08,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "complies with an expected",
      "offset": 9900.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "format. This can include verifying the",
      "offset": 9901.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "number of rows and columns is as",
      "offset": 9904.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "expected. For example, is the row count",
      "offset": 9906.359,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "within 2% of the previous month's row",
      "offset": 9908.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "count?",
      "offset": 9910.72,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Another common test is do the data types",
      "offset": 9912.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "match? If not specifically validated",
      "offset": 9914.76,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "with a schema, does the content meet the",
      "offset": 9917.2,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "requirements only 9 characters or less,",
      "offset": 9919.2,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "etc. Finally, you can validate against",
      "offset": 9922.84,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "more complex rules. This includes",
      "offset": 9925.6,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "verifying that the values of a set of",
      "offset": 9927.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "sensor ratings are within physically",
      "offset": 9929.359,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "possible quantities.",
      "offset": 9931.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "One technique used to validate data in",
      "offset": 9933.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Spark is using joins to verify the",
      "offset": 9935.6,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "content of a data frame matches a known",
      "offset": 9937.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "set. Validating via a join will compare",
      "offset": 9940.279,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "data against a set of known values. This",
      "offset": 9943.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "could be a list of known ids, companies,",
      "offset": 9946,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "addresses,",
      "offset": 9948.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "etc. Joins make it easy to determine if",
      "offset": 9950.04,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "data is present in a set. This can be",
      "offset": 9952.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "only rows that are in one data frame,",
      "offset": 9955.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "present in both, or present in neither.",
      "offset": 9957.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Joins are also comparatively fast,",
      "offset": 9960.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "especially versus validating individual",
      "offset": 9962.64,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "rows against a long list of",
      "offset": 9964.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "entries. The simplest example of this is",
      "offset": 9966.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "using an inner join of two data frames",
      "offset": 9969.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to validate the data. A new data frame",
      "offset": 9971.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "parsed to DF is loaded from a given",
      "offset": 9974.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "paret file. The second data frame is",
      "offset": 9976.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "loaded containing a list of known",
      "offset": 9979.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "company names. A new data frame is",
      "offset": 9980.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "created by joining parsed df and company",
      "offset": 9984.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "df on the company name.",
      "offset": 9986.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "As this is an inner join, only rows from",
      "offset": 9989.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "parsed DF with company names that are",
      "offset": 9991.359,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "present in company DF would be included",
      "offset": 9993.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "in the new data frame verified",
      "offset": 9995.439,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "DF. This has the effect of automatically",
      "offset": 9997.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "filtering out rows that don't meet any",
      "offset": 10000.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "of the specified criteria. This is done",
      "offset": 10002,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "without any kind of spark filter or",
      "offset": 10004.72,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "comparison",
      "offset": 10006.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "code. Complex rule validation is the",
      "offset": 10007.56,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "idea of using Spark components to",
      "offset": 10010.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "validate logic. This may be as simple as",
      "offset": 10011.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "using the various spark calculations to",
      "offset": 10014.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "verify the number of columns in an",
      "offset": 10016.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "irregular data set. You've done",
      "offset": 10017.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "something like this already in the",
      "offset": 10020.16,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "previous",
      "offset": 10021.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "lessons. The validation can also be",
      "offset": 10022.84,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "applied against an external source, web",
      "offset": 10025.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "service, local files, API calls. These",
      "offset": 10027.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "rules are often implemented as a UDF to",
      "offset": 10030.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "encapsulate the logic to one place and",
      "offset": 10032.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "easily run against the content of a data",
      "offset": 10034.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "frame.",
      "offset": 10036.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Let's try validating our data against",
      "offset": 10038.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "our specific requirements for this data",
      "offset": 10040.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "set. Enjoy the exercises and we'll get",
      "offset": 10042.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to the last lesson of this",
      "offset": 10044.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "course. We've worked with a lot of sport",
      "offset": 10048.439,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "components while exploring data",
      "offset": 10050.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "cleaning. Let's finish up this data",
      "offset": 10052.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "cleaning pipeline with some final",
      "offset": 10054.24,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "analysis",
      "offset": 10055.52,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "calculations. Analysis calculations are",
      "offset": 10057,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the process of using the columns of data",
      "offset": 10059.439,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "in a data frame to compute some useful",
      "offset": 10061.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "value using Sparks functionality.",
      "offset": 10063.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "We've used UDFs in previous chapters and",
      "offset": 10066,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "this version illustrates calculating an",
      "offset": 10068.56,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "average sale price from a given list of",
      "offset": 10070.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "sales. A Python function takes a sales",
      "offset": 10072.84,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "list argument. For every sale in the",
      "offset": 10075.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "sales list, the function adds the sale",
      "offset": 10078.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "entry from value two and three in the",
      "offset": 10080.399,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "sale",
      "offset": 10082.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "double. Once complete, it calculates the",
      "offset": 10083.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "actual average per row and returns",
      "offset": 10086.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "it. The remaining code is what we've",
      "offset": 10088.52,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "done previously when defining a UDF and",
      "offset": 10091.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "using it within a dataf frame.",
      "offset": 10093.359,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Spark UDFs are very powerful and",
      "offset": 10096,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "flexible and are sometimes the only way",
      "offset": 10098.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to handle certain types of data.",
      "offset": 10099.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Unfortunately, UDFs do come at a",
      "offset": 10101.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "performance penalty compared to the",
      "offset": 10103.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "built-in Spark functions, especially for",
      "offset": 10105.04,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "certain",
      "offset": 10106.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "operations. The solution is to perform",
      "offset": 10108.279,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "calculations in line if possible. Spark",
      "offset": 10110.72,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "columns can be defined using inline math",
      "offset": 10113.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "operations which can then be optimized",
      "offset": 10115.96,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "for the best performance.",
      "offset": 10118.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "In this case, we read in a data file and",
      "offset": 10120.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then have two examples of adding",
      "offset": 10122.64,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "calculated",
      "offset": 10124.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "columns. The first is a simple average",
      "offset": 10125.72,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "computed by using two columns in the",
      "offset": 10128.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "data frame. The second option computes a",
      "offset": 10129.92,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "square footage by multiplying the value",
      "offset": 10132.8,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "in the two columns together to create a",
      "offset": 10134.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "third. The final line shows the option",
      "offset": 10136.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "of mixing a UDF with an inline",
      "offset": 10139.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "calculation. There's often a better way",
      "offset": 10140.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to do this, but it does illustrate Spark",
      "offset": 10143.279,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "does not care about the source of the",
      "offset": 10145.2,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "info as long as it conforms to the",
      "offset": 10146.479,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "expected input",
      "offset": 10148.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "format. Let's finish up this course by",
      "offset": 10149.72,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "performing some analysis on our data",
      "offset": 10152.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "frame and add meaningful information to",
      "offset": 10154.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 10156.24,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "data.",
      "offset": 10159.16,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "Congratulations. You've successfully",
      "offset": 10160.92,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "completed this course by performing data",
      "offset": 10162.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "cleaning operations on several data sets",
      "offset": 10164.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "using Python and Apache Spark. While",
      "offset": 10166.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "we've touched on many topics, there is a",
      "offset": 10170.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "great deal to learn about Spark and how",
      "offset": 10172.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "best to perform data cleaning. To",
      "offset": 10174.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "continue your journey with using Apache",
      "offset": 10176.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Spark, there are a few areas I'd advise",
      "offset": 10178.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you to focus on. Reading the Spark",
      "offset": 10180.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "documentation is a great way to add to",
      "offset": 10183.12,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "your knowledge and fill in gaps of",
      "offset": 10184.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "understanding. Spark is constantly",
      "offset": 10187,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "changing and often adds new features",
      "offset": 10189.04,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "without a lot of fanfare. Seasoned Spark",
      "offset": 10190.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "developers often find new techniques",
      "offset": 10193.439,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that remove a lot of complexity from",
      "offset": 10195.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "existing code.",
      "offset": 10196.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Spark works on many platforms regardless",
      "offset": 10198.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of size, but it really shines when using",
      "offset": 10200.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "it on multi-node clusters with a lot of",
      "offset": 10203.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "RAM. You'll be surprised how quickly",
      "offset": 10205.08,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "Spark processes data when given the",
      "offset": 10207.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "resources to function as designed. I've",
      "offset": 10209.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "personally processed multi-billion row",
      "offset": 10211.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "data sets in a few hours on a relatively",
      "offset": 10213.6,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "modest",
      "offset": 10215.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "cluster. Finally, I'd suggest working",
      "offset": 10216.6,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "with as many different data sets as you",
      "offset": 10219.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "can find. Different types of data",
      "offset": 10220.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "require different techniques in Spark",
      "offset": 10223.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and each has challenges when trying to",
      "offset": 10224.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "process data in an efficient way. The",
      "offset": 10226.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "data sets available within the various",
      "offset": 10229.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "courses here on Data Camp are a great",
      "offset": 10230.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "place to start. Thanks and good luck on",
      "offset": 10232.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "your journey using Apache",
      "offset": 10235.279,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Spark. Hi, I'm John Hogue and welcome to",
      "offset": 10239.64,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "feature engineering with PIS Spark.",
      "offset": 10242.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Easily one of the most important aspects",
      "offset": 10244.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of applied machine learning is feature",
      "offset": 10246.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "engineering. It is the process of using",
      "offset": 10248.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "domain knowledge to create new features",
      "offset": 10251.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "to help our models perform better. In",
      "offset": 10253.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "this course, we will look at a real data",
      "offset": 10255.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "set and work our way to building a",
      "offset": 10257.439,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "regression model in",
      "offset": 10259.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Pispark. Before we dive in, it's",
      "offset": 10261.16,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "important to note that while the",
      "offset": 10263.6,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "techniques you'll learn in this course",
      "offset": 10264.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "are invaluable, the data science cannot",
      "offset": 10266.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "be applied as a cookie cutter. You'll",
      "offset": 10268.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "need to research your data and become",
      "offset": 10271.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "your own expert. There's much to be said",
      "offset": 10272.8,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "of the dangers of not understanding your",
      "offset": 10275.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "data, especially where our outputs are",
      "offset": 10277.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "increasingly being used to make",
      "offset": 10279.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "decisions and inform policies. Before",
      "offset": 10281.439,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you dive into modeling, spend time to",
      "offset": 10284.319,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "define what your goals are and how the",
      "offset": 10286.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "output might be used. Take time to",
      "offset": 10288.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "research your data and its limitations.",
      "offset": 10291.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Often times, you may be tasked with",
      "offset": 10294,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "explaining what is and isn't possible.",
      "offset": 10295.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Lastly, remember that data science is",
      "offset": 10298.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "about being curious, asking questions,",
      "offset": 10300.16,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and applying new ways to solve",
      "offset": 10302.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "problems. Every project and data set is",
      "offset": 10304.6,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "different. Data science is an iterative",
      "offset": 10307.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "process that requires comfort with",
      "offset": 10309.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "uncertainty, as at any point, you may",
      "offset": 10311.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "have to go backward or even start over.",
      "offset": 10313.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "A good project may inspire further",
      "offset": 10316.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "questions that set the goals for your",
      "offset": 10318.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "next project.",
      "offset": 10319.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "As we progress through this process,",
      "offset": 10321.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this course will have extra emphasis on",
      "offset": 10323.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "a lot of the art sides of data science.",
      "offset": 10325.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Exploring data, cleaning it, and",
      "offset": 10328.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "engineering it for use in a model.",
      "offset": 10330.72,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Before we get started, as a cutting edge",
      "offset": 10333.2,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "technology, Spark changes fast and",
      "offset": 10336.2,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "frequently. Make sure you're looking at",
      "offset": 10338.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the right version. You can always go to",
      "offset": 10340.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "the latest URL by using the slash latest",
      "offset": 10342.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "or put the version number major, minor,",
      "offset": 10345.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and patch to get a specific version.",
      "offset": 10348.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Programmatically, you can check your",
      "offset": 10350.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "version of Spark with these commands.",
      "offset": 10352.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "That way, you can ensure you're looking",
      "offset": 10354.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "at the right documentation and not using",
      "offset": 10356.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "deprecated methods. For this course, we",
      "offset": 10358.319,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "will be using a parquet file. Like most",
      "offset": 10361.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "data in Hadoop, the platform that Spark",
      "offset": 10363.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "runs on, it is write once, read many",
      "offset": 10365.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "times format. Paret is columner, meaning",
      "offset": 10368.479,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "that is organized by columns, an",
      "offset": 10372,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "important feature for huge data sets as",
      "offset": 10374,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it is blazingly fast to read in only the",
      "offset": 10375.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "data you need. CSVs on the other hand",
      "offset": 10378.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "have to read and parse the whole data",
      "offset": 10381.359,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "set to read a single field. Another",
      "offset": 10383.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "difference is paret fields are defined",
      "offset": 10385.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and typed saving users from defining",
      "offset": 10388,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "data types like dates, booleans, or",
      "offset": 10390.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "strings. For this reason, parquet is",
      "offset": 10393.56,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "relatively slow to write. Since it's not",
      "offset": 10396,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "delimited by characters, it's less",
      "offset": 10398.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "likely to be read in wrong if those",
      "offset": 10400.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "characters exist in the data. These are",
      "offset": 10402.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "just a few of the advantages that are",
      "offset": 10404.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "causing the industry to adopt paret",
      "offset": 10406.479,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "quickly.",
      "offset": 10408.16,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "We have many format readers to choose",
      "offset": 10409.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "from for converting various file types",
      "offset": 10411.439,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "to a pispark dataf frame. Here we will",
      "offset": 10413.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "use spark read parquet and put the",
      "offset": 10416,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "results into variable df representing a",
      "offset": 10418.479,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "dataf frame. In this video we covered",
      "offset": 10420.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "off on some important considerations",
      "offset": 10424,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "when starting any data science project.",
      "offset": 10425.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "We also learned about parquet and how to",
      "offset": 10428.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "load it to a spark dataf frame. In the",
      "offset": 10430.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "exercises you'll verify the versioning",
      "offset": 10432.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of pi spark and python. And finally,",
      "offset": 10434.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "you'll load the data",
      "offset": 10437.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "yourself. What's the point of doing an",
      "offset": 10440.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "analysis if you aren't solving the right",
      "offset": 10443.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "problem? In this video, we'll define our",
      "offset": 10444.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "problem in the context of our data. We",
      "offset": 10447.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "are going to build a model to predict",
      "offset": 10450.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "how much a house sells for. This",
      "offset": 10452.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "question can be interpreted multiple",
      "offset": 10454.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "ways, which is why it's important to",
      "offset": 10456,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "take the time to formally define it.",
      "offset": 10457.6,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "Let's assume we are real estate tycoons",
      "offset": 10460.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "looking for the next best investment",
      "offset": 10463.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "opportunity.",
      "offset": 10465.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "For a given house on the market with a",
      "offset": 10466.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "listed price and a series of attributes",
      "offset": 10468.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "describing the home, what is it likely",
      "offset": 10471.359,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to actually sell for, aka the sales",
      "offset": 10473.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "close",
      "offset": 10476.479,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "price? The data set we have is a sample",
      "offset": 10477.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of homes that sold over the course of",
      "offset": 10480.319,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "2017. Using this sample, we are to",
      "offset": 10482.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "provide a quick proof of concept of",
      "offset": 10485.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "whether it's worth investing in more",
      "offset": 10487.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "data for the 5.5 million homes that sold",
      "offset": 10489.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "in the US in 2017.",
      "offset": 10492.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "To do this, we need to understand some",
      "offset": 10495.359,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "of the limitations of the data we",
      "offset": 10497.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "have. First, we only have a small",
      "offset": 10499.479,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "geographical area, so to apply our model",
      "offset": 10502.16,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "to new areas poses serious",
      "offset": 10504.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "risk. We know that we only have",
      "offset": 10507,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "residential data, so we shouldn't expect",
      "offset": 10509.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to predict how much a business location",
      "offset": 10511.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "is worth. Lastly, we only have one",
      "offset": 10513.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "year's worth of data, which will make it",
      "offset": 10516.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "hard to draw strong conclusions about",
      "offset": 10518.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the seasonality in the data set.",
      "offset": 10520.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "The original data set has hundreds of",
      "offset": 10523.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "attributes available, but in order to",
      "offset": 10525.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "start simple, we've already worked with",
      "offset": 10527.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "our client to identify around 50",
      "offset": 10528.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "attributes they think are likely to",
      "offset": 10530.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "influence the price of a home. These",
      "offset": 10532.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "attributes generally fall into these",
      "offset": 10535.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "groups. For dates, we have date listed,",
      "offset": 10536.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and the year the home was built. For",
      "offset": 10540.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "location data, we have the city that the",
      "offset": 10543.04,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "home is in, its school district, and its",
      "offset": 10545.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "actual postal address.",
      "offset": 10548.479,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "We also have many different metrics to",
      "offset": 10550.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "gauge the size of a home like number of",
      "offset": 10552.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "bed and bathrooms as well as the area of",
      "offset": 10554.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "living space. For prices, we have the",
      "offset": 10556.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "listing price and we wouldn't be able to",
      "offset": 10560.08,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "predict anything without the sales",
      "offset": 10562.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "price. We also have a lot of data",
      "offset": 10564.359,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "available on the amenities that a house",
      "offset": 10566.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "has like a pool or garage as well as the",
      "offset": 10568.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "construction materials that were used to",
      "offset": 10571.68,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "build the",
      "offset": 10573.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "house. Big data means a lot can go wrong",
      "offset": 10574.2,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "when loading data. Make sure you have",
      "offset": 10577.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the right number of records and columns.",
      "offset": 10579.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "We can use df count to get the row",
      "offset": 10581.68,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "count, df columns to get the list of",
      "offset": 10583.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "columns, and we can take the length of",
      "offset": 10586.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "df columns to get the number of",
      "offset": 10588.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "columns. When we use parket, it set the",
      "offset": 10591.72,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "data types for all of our fields, which",
      "offset": 10595.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is a huge advantage over CSV. It's still",
      "offset": 10597.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "worth checking, especially if you",
      "offset": 10600,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "weren't the one defining it. Here we can",
      "offset": 10601.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "use dtypes on our data frame to create a",
      "offset": 10604.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "list of tupils containing a column name",
      "offset": 10607.04,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "and its corresponding data",
      "offset": 10609.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "type. In this video, we learned about",
      "offset": 10611.24,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "the data set we will be using and the",
      "offset": 10613.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "problem we will be trying to solve.",
      "offset": 10615.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Additionally, we learned how to check to",
      "offset": 10617.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "see if our data loaded properly by",
      "offset": 10620,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "checking rows, columns, and data types.",
      "offset": 10621.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Now, it's your turn to apply what you've",
      "offset": 10624.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "learned in the exercises to verify our",
      "offset": 10626.399,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "data got loaded correctly.",
      "offset": 10628.64,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "Data comes in all shapes and sizes. In",
      "offset": 10633.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the field, you will be tasked with using",
      "offset": 10636.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "less than perfect data. This means",
      "offset": 10638,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "you'll need to understand its strengths,",
      "offset": 10640.479,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "weaknesses, and limitations to leverage",
      "offset": 10642.319,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 10644.399,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "effectively. To get started with",
      "offset": 10645.16,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "understanding your data, take a peek at",
      "offset": 10647.12,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "each column to see what they",
      "offset": 10649.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "contain. The describe function provides",
      "offset": 10651.16,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "some bare bone basics of count, mean,",
      "offset": 10653.68,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "standard deviation, min, and max. You",
      "offset": 10657.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "can run on the whole data frame, a",
      "offset": 10661.04,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "single column or list of",
      "offset": 10663.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "columns. Remember to add show at the end",
      "offset": 10665.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "if you wish to immediately display the",
      "offset": 10668.319,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "results. To further help us understand",
      "offset": 10670.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "our data, PISpark has many built-in",
      "offset": 10673.279,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "descriptive functions available.",
      "offset": 10675.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "mean function is considered an aggregate",
      "offset": 10678.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "function and as such it needs to be",
      "offset": 10681.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "passed to the a method along with the",
      "offset": 10683.279,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "column to run it on as a dictionary to",
      "offset": 10685.439,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "force it to return results immediately.",
      "offset": 10688.479,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Use",
      "offset": 10690.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "collect. Coariance is a function that",
      "offset": 10691.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "lets us see how two variables vary",
      "offset": 10693.92,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "together. This function is applied to a",
      "offset": 10696.04,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "data frame and takes two numeric columns",
      "offset": 10698.479,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "and returns a",
      "offset": 10701.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "value. An excellent way to explore your",
      "offset": 10702.52,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "data is through statistical plotting.",
      "offset": 10705.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Seabour is a Python data visualization",
      "offset": 10708.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "library designed specifically for this.",
      "offset": 10710.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "We will look at a few plotting examples",
      "offset": 10713.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but there are many many more for you to",
      "offset": 10715.68,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "follow up",
      "offset": 10717.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on. We can plot data using non-Spark",
      "offset": 10718.68,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "libraries like Seabor but they require",
      "offset": 10721.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "converting your PIS spark dataf frame to",
      "offset": 10724,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "a pandas dataf frame. Be aware that",
      "offset": 10726,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "converting large data sets can cause",
      "offset": 10728.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "pandas to crash. This is because pi",
      "offset": 10730.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "spark is made for massive data sets",
      "offset": 10733.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "whereas pandas is not. The sample",
      "offset": 10735.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "function can help us get a smaller data",
      "offset": 10738,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "set to",
      "offset": 10739.92,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "plot. Here we will keep sampling with",
      "offset": 10740.76,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "replacement off. Take 50% of the data",
      "offset": 10743.439,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "and set a random seed for",
      "offset": 10746.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "reproducibility. Using count shows us",
      "offset": 10749.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the number of records has",
      "offset": 10751.359,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "changed. We will leverage Seabour's disc",
      "offset": 10753.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "plot which will show us the distribution",
      "offset": 10756.08,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "of our dependent variable sales close",
      "offset": 10757.76,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "price. Please note there are many",
      "offset": 10760.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "optional parameters which aren't covered",
      "offset": 10762.479,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "here. Here we will import seabor then",
      "offset": 10764.319,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "filter the spark data frame down to the",
      "offset": 10768.08,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "sales close price column and then sample",
      "offset": 10770.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it. Then we convert it to a pandas dataf",
      "offset": 10773.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "frame so we can use it with",
      "offset": 10775.92,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "seabor. Lastly call this plot function",
      "offset": 10777.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "with",
      "offset": 10782.319,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "pandas_df to",
      "offset": 10783.08,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "plot. After plotting we can see that",
      "offset": 10785.16,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "most of the data is pushed to the left.",
      "offset": 10788,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Something that may need to be remedied",
      "offset": 10790.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "depending on the model type we choose.",
      "offset": 10792.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "We will cover one option log scaling in",
      "offset": 10794.8,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "adjusting data later in this",
      "offset": 10797.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "course. Another great plot to use is LM",
      "offset": 10799.64,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "plot. LM is short for linear model and",
      "offset": 10802.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "allows us to quickly see if there's a",
      "offset": 10806,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "linear relationship between two",
      "offset": 10807.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "variables. For this example, we will",
      "offset": 10810.439,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "look at how sales close price changes",
      "offset": 10812.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "depending on square footage above",
      "offset": 10814.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "ground.",
      "offset": 10817.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "To do this, we will import Seabor,",
      "offset": 10818.56,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "filter our data set to the two",
      "offset": 10821.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "columns, sample it, and convert it to a",
      "offset": 10823.319,
      "duration": 7.721
    },
    {
      "lang": "en",
      "text": "pandas data frame. Lastly, use the SNS",
      "offset": 10826.88,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "LM plot function with our X and Y",
      "offset": 10831.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "columns and the data frame. Here we can",
      "offset": 10833.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "see that there's what looks to be a",
      "offset": 10836.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "strong relationship between the size of",
      "offset": 10838.88,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "a home and the price it sells for.",
      "offset": 10840.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Therefore, we might make the assumption",
      "offset": 10843.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that square footage above ground is a",
      "offset": 10845.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "good variable to consider in the",
      "offset": 10847.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "predicting of house prices. In this",
      "offset": 10848.88,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "video, we explored our data with",
      "offset": 10851.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "numerical summaries and",
      "offset": 10853.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "visualizations. Now, it's your turn to",
      "offset": 10855.56,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "try them",
      "offset": 10857.6,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "out. More data is better, right? Not if",
      "offset": 10860.76,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "it's bad data. The saying garbage in,",
      "offset": 10864,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "garbage out, is doubly true in data",
      "offset": 10866.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "science. Data has a lot of places where",
      "offset": 10868.52,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "it can get messed up. Data may be",
      "offset": 10870.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "recorded incorrectly or contain extreme",
      "offset": 10873.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "events. Inconsistent formatting, such as",
      "offset": 10875.64,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "mixing numeric and text data, can make a",
      "offset": 10878.319,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "field hard to use. Duplications can add",
      "offset": 10880.56,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "redundant",
      "offset": 10883.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "observations. Missing data can cause",
      "offset": 10884.84,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "your analysis to have blind spots.",
      "offset": 10887.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Lastly, sometimes the data is just not",
      "offset": 10889.84,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "relevant to the",
      "offset": 10892,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "analysis. Failure to account for bad",
      "offset": 10893.399,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "data can set your analysis up to fail.",
      "offset": 10895.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Please be careful.",
      "offset": 10897.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Inspecting our data, we see there are",
      "offset": 10899.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "some columns that are not worth",
      "offset": 10901.6,
      "duration": 2.44
    },
    {
      "lang": "en",
      "text": "including in our",
      "offset": 10902.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "analysis. No is just the record number.",
      "offset": 10904.04,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "Unit number is the apartment or house",
      "offset": 10907.52,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "number and class is completely",
      "offset": 10909.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "constant. Dropping the columns requires",
      "offset": 10912.439,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "entering a single column name or passing",
      "offset": 10915.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "a list of columns to drop. Here we pass",
      "offset": 10917.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "a list called calls to drop to the",
      "offset": 10920.08,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "function and then drop them. Please note",
      "offset": 10922.399,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the star which tells the function to",
      "offset": 10925.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "unpack the list and feed them to the",
      "offset": 10927.359,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "function one by one. A common task in",
      "offset": 10929.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "cleaning your data will be filtering it.",
      "offset": 10931.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Here we will filter records that contain",
      "offset": 10933.84,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "specific text",
      "offset": 10935.6,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "values. Where applies the filter to the",
      "offset": 10937,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "dataf frame records like creates a true",
      "offset": 10939.359,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "false condition for the records. The",
      "offset": 10942.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "till day provides a way to take the",
      "offset": 10945.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "opposite or a not. The PISpark code",
      "offset": 10946.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "reads, &quot;Filter the data frame where the",
      "offset": 10950.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "potential short sale field is not like",
      "offset": 10952.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the string not",
      "offset": 10955.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "disclosed.&quot; For initial pass of the",
      "offset": 10957,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "model, it might be helpful to remove",
      "offset": 10959.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "large outliers. One definition of an",
      "offset": 10960.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "outlier for near-normally distributed",
      "offset": 10963.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "data is something that occurs more than",
      "offset": 10965.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "three standard deviations from the mean.",
      "offset": 10967.84,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "Only 0.3% of the data should be",
      "offset": 10970.399,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "filtered. Remember that outliers occur",
      "offset": 10973.08,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "on both sides, so filter on both sides",
      "offset": 10975.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "of the mean. Here we will filter extreme",
      "offset": 10977.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "values from the list price column. To",
      "offset": 10980.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "start we will use the aggregate function",
      "offset": 10983.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "standard deviation and mean. Then use",
      "offset": 10985.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "collect to force the calculation to run",
      "offset": 10988.64,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "and use the 000 index to access the",
      "offset": 10990.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "values. Lastly, we created a",
      "offset": 10994.279,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "multiconditional filter which is just",
      "offset": 10996.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "two boolean statements and together.",
      "offset": 10999.359,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "It reads filter where the list price is",
      "offset": 11002.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "less than the high bound and more than",
      "offset": 11005.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the low bound. Dealing with missing data",
      "offset": 11007.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "is something we will cover later as",
      "offset": 11009.92,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "dropping data is usually a naive",
      "offset": 11011.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "approach. Nevertheless, it's important",
      "offset": 11013.72,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "to sometimes take shortcuts to quickly",
      "offset": 11015.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "prove out the basis for further work.",
      "offset": 11017.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Drop NA does what you'd expect it to do.",
      "offset": 11020.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "In its basic form, it will remove a",
      "offset": 11023.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "record where there's any null value in",
      "offset": 11025.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "any column. You can get more specific as",
      "offset": 11027.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we'll see on the next slide.",
      "offset": 11029.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "In the first example, we'll drop any",
      "offset": 11032.24,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "record that contains a null",
      "offset": 11034.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "value. The second example, we'll look at",
      "offset": 11036.279,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "only two columns and if both are null,",
      "offset": 11038.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "then we will remove the",
      "offset": 11041.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "record. Lastly, we can apply a threshold",
      "offset": 11043.08,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "across all the columns and say if more",
      "offset": 11046.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "than two columns contain null values,",
      "offset": 11049.2,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "remove the entire",
      "offset": 11051.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "record. Duplicates occur when two or",
      "offset": 11052.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "more records contain the exact same",
      "offset": 11055.12,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "information. Often this can happen after",
      "offset": 11056.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you drop columns or join data sets. Drop",
      "offset": 11059.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "duplicates will drop the first duplicate",
      "offset": 11062.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it finds. Since Spark is distributed,",
      "offset": 11064.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "which one it finds may or may not be in",
      "offset": 11067.84,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "the order of how your file was",
      "offset": 11070.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "loaded. If you want to be more picky",
      "offset": 11071.72,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "about where you're looking for",
      "offset": 11073.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "duplicates, you can specify a list of",
      "offset": 11075.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "column names to look for them there",
      "offset": 11077.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "specifically.",
      "offset": 11079.2,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "In this video, we learn why we might",
      "offset": 11080.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have bad data and several ways to remove",
      "offset": 11082.479,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "it so it doesn't adversely impact our",
      "offset": 11084.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "analysis. Let's see you take a shot at",
      "offset": 11086.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "removing some data in the",
      "offset": 11088.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "exercises. Jeff Hooper of Bell Labs once",
      "offset": 11093.319,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "said, &quot;Data does not give up its secrets",
      "offset": 11096.24,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "easily. It must be tortured to",
      "offset": 11098.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "confess.&quot; This lesson will arm you with",
      "offset": 11100.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the tools to get your data to",
      "offset": 11102.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "behave. Real data is ugly and rarely",
      "offset": 11104.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "comes ready to be analyzed. Many",
      "offset": 11107.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "algorithms and statistical methods have",
      "offset": 11110.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "assumptions that a variable conforms to.",
      "offset": 11111.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "If our data doesn't fit these criteria,",
      "offset": 11114.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "all hope isn't lost yet. We can try",
      "offset": 11116.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "mathematical operations to adjust the",
      "offset": 11119.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "data to become the beautiful butterflies",
      "offset": 11120.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "our methods require. One common",
      "offset": 11122.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "transformation is scaling. For many",
      "offset": 11125.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "algorithms like KN&amp;N or regression, you",
      "offset": 11128,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "need to ensure all your variables are on",
      "offset": 11130.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the same scale. One variable can't be",
      "offset": 11132.56,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "from 1,000 to 5,000 and another between",
      "offset": 11135.12,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "01 and 002. These algorithms will try to",
      "offset": 11139.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "reduce the errors in the first variable",
      "offset": 11142.8,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "much more than the second. We can avoid",
      "offset": 11144.8,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "this by scaling each feature between 0",
      "offset": 11147.439,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and one. This is called minmax scaling",
      "offset": 11149.439,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "and doesn't change the shape of the",
      "offset": 11152.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "distribution only its range.",
      "offset": 11154.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "To minmax scale, take the variable to be",
      "offset": 11157.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "scaled, subtract the minimum value, and",
      "offset": 11159.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "divide by the difference between the max",
      "offset": 11162.399,
      "duration": 2.681
    },
    {
      "lang": "en",
      "text": "and the",
      "offset": 11164.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "min. To scale our data, we need to first",
      "offset": 11165.08,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "find the min and max values of the",
      "offset": 11167.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "column we want to scale. Here we are",
      "offset": 11169.68,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "using aggregate functions min and max.",
      "offset": 11172.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "We will use collect to force the",
      "offset": 11175.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "calculation to run and use the 00 index",
      "offset": 11176.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "to access the",
      "offset": 11179.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "values. To create a new column, we will",
      "offset": 11181.88,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "use with column that creates a new",
      "offset": 11184.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "column based off of some sort of",
      "offset": 11186.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "transformation to an existing one. In",
      "offset": 11188.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "this case, days on market. Lastly, we",
      "offset": 11191.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "can see that our values are now between",
      "offset": 11194.319,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "0 and",
      "offset": 11196,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "one. Another common restriction is that",
      "offset": 11197.24,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "data must closely follow the standard",
      "offset": 11199.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "normal distribution.",
      "offset": 11201.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Standardization or Z transforming is the",
      "offset": 11203.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "process of shifting and scaling your",
      "offset": 11206.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "data to better resemble a standard",
      "offset": 11207.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "normal distribution which has a mean of",
      "offset": 11209.68,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "zero and a standard deviation of one. In",
      "offset": 11212,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the image, you can see how the original",
      "offset": 11215.439,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "data in blue shifts to green where it",
      "offset": 11217.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "has a mean of zero and then the final",
      "offset": 11220.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "step scales to the standard normal",
      "offset": 11222.319,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "distribution in red. To z transform our",
      "offset": 11224.479,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "data, we calculate the aggregate",
      "offset": 11227.279,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "functions mean and standard deviation of",
      "offset": 11229.279,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "the column we are",
      "offset": 11231.439,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "transforming. Since we want to use the",
      "offset": 11232.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "values in the next step, we will use",
      "offset": 11234.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "collect to immediately calculate them",
      "offset": 11236.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and use the index values of 0 0 to",
      "offset": 11238.72,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "access the return",
      "offset": 11241.6,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "values. We can then apply the",
      "offset": 11243.399,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "standardization formula to our column",
      "offset": 11245.439,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "and put the results into a new column ZR",
      "offset": 11247.76,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "days by using with column.",
      "offset": 11250.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Lastly, we can verify the transform data",
      "offset": 11254.479,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "does have approximate mean of zero and",
      "offset": 11256.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "standard deviation of",
      "offset": 11259.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "one. Our data for sales close price is",
      "offset": 11261,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "pushed to the left. This is called",
      "offset": 11264,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "positive skew. One potential way to",
      "offset": 11266,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "treat skewed data is to apply a log",
      "offset": 11268.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "transformation on the data. This has the",
      "offset": 11270.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "impact of making our data look more like",
      "offset": 11272.64,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "a normal",
      "offset": 11274.479,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "distribution. To apply a log",
      "offset": 11275.8,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "transformation, you will need to import",
      "offset": 11277.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the log function from PISpark SQL",
      "offset": 11279.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "functions.",
      "offset": 11281.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "We can then create a new column log",
      "offset": 11283.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "sales close price based on the",
      "offset": 11285.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "application of the log function on sales",
      "offset": 11287.6,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "close",
      "offset": 11290.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "price. In this video, you learn why and",
      "offset": 11291.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "how to apply transformations to your",
      "offset": 11294.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "data. Now, it's time for you to adjust",
      "offset": 11295.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "some",
      "offset": 11298.319,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "data. Missing data is frustrating. In",
      "offset": 11301.399,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "this lesson, we will touch on a few ways",
      "offset": 11304.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "to handle it. How does data go missing",
      "offset": 11306.16,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "in the digital age? Sensors can fail.",
      "offset": 11308.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Surveys can miss people or new ways to",
      "offset": 11312.319,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "measure things can cause gaps in data",
      "offset": 11314.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "sets. Data storage rules can force data",
      "offset": 11316.68,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "that doesn't fit the specified type to",
      "offset": 11319.359,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "be null. For example, dates in different",
      "offset": 11321.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "formats, abbreviations, or a currency",
      "offset": 11324.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "with a comma instead of a period.",
      "offset": 11327.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Joining data sets can enrich your model,",
      "offset": 11329.52,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "but can induce missing values if they",
      "offset": 11331.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "are not captured at the same",
      "offset": 11333.439,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "granularity.",
      "offset": 11334.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "If you combine daily data with monthly",
      "offset": 11336.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "data, it will create gaps for all the",
      "offset": 11338.08,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "days where the monthly data was not",
      "offset": 11339.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "captured. Lastly, data can be missing",
      "offset": 11341.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "intentionally. Attributes used in",
      "offset": 11345,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "combination might be enough to",
      "offset": 11346.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "compromise privacy. This can be seen in",
      "offset": 11348.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "government data sets like the census",
      "offset": 11350.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "where they will omit data if there is a",
      "offset": 11352.399,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "concern. Understanding why your data is",
      "offset": 11355,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "missing is important. Missing completely",
      "offset": 11357.439,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "at random occurs when the data is",
      "offset": 11360.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "missing with no pattern. your data is",
      "offset": 11362.16,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "likely still representative of the whole",
      "offset": 11364.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "population. Missing at random occurs",
      "offset": 11367.479,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "when the probability of missing data on",
      "offset": 11369.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the y variable is unrelated to the value",
      "offset": 11371.84,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "of y. For example, suppose males are",
      "offset": 11374.56,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "less likely to answer a depression",
      "offset": 11378.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "survey. This has no relationship with",
      "offset": 11380.279,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "their level of depression after",
      "offset": 11382.88,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "accounting for",
      "offset": 11384.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "maleness. Missing not at random is when",
      "offset": 11386.359,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "the value that is missing is related to",
      "offset": 11389.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the reason why it's missing.",
      "offset": 11391.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Supposing that people with severe health",
      "offset": 11393.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "problems do not answer a question asking",
      "offset": 11395.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "them to rank their health would indicate",
      "offset": 11398,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "missing not at",
      "offset": 11400.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "random. Earlier we showed how to use the",
      "offset": 11401.96,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "function drop NA, but we didn't talk",
      "offset": 11404.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "about when to use it. If your data only",
      "offset": 11406.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "has a few missing values and they are",
      "offset": 11408.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "missing completely at random, it may be",
      "offset": 11410.88,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "fine to remove the",
      "offset": 11413.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "rows. But how can we check to see how",
      "offset": 11414.6,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "many missing values we have in our data",
      "offset": 11417.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "set?",
      "offset": 11418.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "We can use the isnull function. It",
      "offset": 11420.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "returns true if the condition is true.",
      "offset": 11422.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Here we use it to filter our data to",
      "offset": 11426,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "records where they are null and then",
      "offset": 11427.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "count them. We can also use Seabor to",
      "offset": 11429.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "help us visualize missing values by",
      "offset": 11432.88,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "leveraging the heat map",
      "offset": 11434.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "function. Using the same steps as",
      "offset": 11436.68,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "before, we sample our data, convert it,",
      "offset": 11439,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "and then use Seabor to plot the heat",
      "offset": 11442.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "map.",
      "offset": 11444.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Note we use the Panda's dataf frame is",
      "offset": 11446.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "null to convert the data frame to a true",
      "offset": 11448.479,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "false for its null",
      "offset": 11450.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "values. Here we can see the missing",
      "offset": 11452.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "values as white spaces on the",
      "offset": 11454.88,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "chart. Another way to handle missing",
      "offset": 11457.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "values is to replace them. The",
      "offset": 11459.439,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "replacement value might be based on",
      "offset": 11461.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "business rules such as missing sales",
      "offset": 11463.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "means there were no sales and replace",
      "offset": 11465.279,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "them with",
      "offset": 11467.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "zero. If the missing data is missing",
      "offset": 11468.04,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "completely at random, it may make sense",
      "offset": 11470.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "to impute them using the mean or the",
      "offset": 11472.479,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "median.",
      "offset": 11474.56,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "Another option could be to use",
      "offset": 11475.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "interpolation, creating another model to",
      "offset": 11477.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "predict their",
      "offset": 11479.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "values. Replacing values shouldn't be",
      "offset": 11481.479,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "done without some serious",
      "offset": 11484.08,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "considerations. Make sure you research",
      "offset": 11485.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 11487.279,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "appropriateness. To replace missing",
      "offset": 11488.6,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "values, we will use pi sparks fill na",
      "offset": 11490.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "which takes the value to use for",
      "offset": 11493.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "replacement as well as a list of column",
      "offset": 11495.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "names. Here we will replace values with",
      "offset": 11497.88,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "zero. We can also replace values with",
      "offset": 11500.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the mean by calculating it using an",
      "offset": 11503.359,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "aggregate function as we have done",
      "offset": 11505.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "before. Then the mean only needs to be",
      "offset": 11507.479,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "placed in the fill na function. In this",
      "offset": 11509.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "video, you learned about the types of",
      "offset": 11513.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "missing data, how to assess missing",
      "offset": 11514.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "values, and some methods to treat them.",
      "offset": 11516.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Take some time to do the exercises and",
      "offset": 11519.68,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "try out what you've learned.",
      "offset": 11521.76,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "The world of big data means having",
      "offset": 11526.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "access to much more information to",
      "offset": 11528,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "include in our analysis. In this video,",
      "offset": 11529.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we will cover how to connect additional",
      "offset": 11532.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "data to our data set. External data is a",
      "offset": 11534.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "wonderful way to boost model",
      "offset": 11537.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "performance, but there are pros and cons",
      "offset": 11539.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to choosing to include it. Adding",
      "offset": 11541.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "external data may add excellent",
      "offset": 11543.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "predictors for a model, but adding too",
      "offset": 11546,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "many features may impact the performance",
      "offset": 11548.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of the model. There is serious risk of",
      "offset": 11550.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "spirious correlations between variables",
      "offset": 11553.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "in the world of big data. External data",
      "offset": 11555.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "can be a great way to replace missing or",
      "offset": 11558.24,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "aggregated values with a better",
      "offset": 11560.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "feature. Adding data comes at the risk",
      "offset": 11562.6,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "of inducing data leakage. If we wish to",
      "offset": 11564.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "include local crime information, we will",
      "offset": 11567.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "have to ensure that buyers would also",
      "offset": 11570.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have access to that information at the",
      "offset": 11571.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "time of purchasing a home.",
      "offset": 11574,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to include data available later is",
      "offset": 11576.8,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "leaking information from the",
      "offset": 11578.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "future. Another consideration is how",
      "offset": 11581.24,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "cheap and easy it is to obtain. Today we",
      "offset": 11583.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "have many more data sets easily",
      "offset": 11586.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "available to us, but it may come at the",
      "offset": 11588.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "cost of needing to become the subject",
      "offset": 11590.479,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "matter experts to know the meaning and",
      "offset": 11592.16,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "credibility of our",
      "offset": 11594.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "sources. To understand the different",
      "offset": 11596.04,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "kinds of joins, we'll need to orient",
      "offset": 11598,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "ourselves.",
      "offset": 11600.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "We'll call the original data set we",
      "offset": 11602.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "started with the left and the one we",
      "offset": 11604,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "wish to incorporate our",
      "offset": 11606.319,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "right. There are many ways to join data",
      "offset": 11608.279,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "together. Most commonly the inner or",
      "offset": 11611.319,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "left joins depending on your goals. For",
      "offset": 11614.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "us, we want to make sure we always keep",
      "offset": 11616.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the full left data set and add data",
      "offset": 11619.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "where available from the right. This",
      "offset": 11622.16,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "means we will be using a left",
      "offset": 11624.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "join. Joins can be done one of two ways",
      "offset": 11626.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "in Pispark.",
      "offset": 11629.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "First is the dataf frame join method.",
      "offset": 11631.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "The data frame that calls the join is",
      "offset": 11634.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the left dataf frame. Other is the right",
      "offset": 11636.16,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "dataf frame in this case the new data",
      "offset": 11639.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "set. on is the pair of column conditions",
      "offset": 11641.8,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "we will match on. How is the type of",
      "offset": 11645.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "join to perform. Suppose we want to see",
      "offset": 11648.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the impact of home sold on bank",
      "offset": 11651.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "holidays.",
      "offset": 11653.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "We can join the data frames together by",
      "offset": 11654.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "creating a join condition where df",
      "offset": 11656.56,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "offmarket date equals hdf",
      "offset": 11659.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "dt. We can then put this condition into",
      "offset": 11663.16,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "our join function and use left to make",
      "offset": 11665.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "sure we keep all the original records in",
      "offset": 11668.479,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "df. Lastly count holiday sales by using",
      "offset": 11671,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the till day to take the not of",
      "offset": 11674.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "isnull. Not surprisingly no houses are",
      "offset": 11677.64,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "sold on bank holidays. Perhaps later we",
      "offset": 11680.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "can check to see if a holiday week",
      "offset": 11683.439,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "impacts",
      "offset": 11685.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "sales. The second way we can join dataf",
      "offset": 11686.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "frames together is to use Spark SQL",
      "offset": 11688.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "which allows us to apply SQL statements",
      "offset": 11691.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "directly to dataf frames. This may be",
      "offset": 11693.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "your preferred method if you are",
      "offset": 11696.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "familiar with SQL or attempting to do",
      "offset": 11697.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "complicated joins or",
      "offset": 11699.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "filters. To do this, we need to register",
      "offset": 11701.64,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "the dataf frame as a temp table and give",
      "offset": 11704.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "it a name. Once that's done, we can use",
      "offset": 11706.479,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "spark SQL to execute a query and return",
      "offset": 11710,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "it back in the form of a dataf frame. In",
      "offset": 11712.8,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "this one, we are using select star to",
      "offset": 11716.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "get all the columns available. Using",
      "offset": 11718.479,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "from df to create our starting table,",
      "offset": 11721.04,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "left join hdf as the table we'd like to",
      "offset": 11724.64,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "join with and using on df offmarket date",
      "offset": 11728.08,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "equals hdf dt to create the join",
      "offset": 11732,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "condition. In this video, we learned",
      "offset": 11735.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "that combining data sets can be",
      "offset": 11738.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "powerful, but requires some caution. We",
      "offset": 11740,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "also learned how to join data in two",
      "offset": 11743.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "different ways. In the exercises, we",
      "offset": 11744.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "will build on these and learn some",
      "offset": 11747.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "things to watch out",
      "offset": 11748.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "for. In this video, we will learn a lot",
      "offset": 11752.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "about the nuts and bolts of feature",
      "offset": 11754.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "engineering. Just because it's called",
      "offset": 11757,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "machine learning doesn't mean it can",
      "offset": 11759.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "figure out everything on its own. So, we",
      "offset": 11760.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "will use some tricks to help it out by",
      "offset": 11763.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "creating new features that will better",
      "offset": 11765.279,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "capture patterns in the data. This video",
      "offset": 11766.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "will cover feature generation and show",
      "offset": 11769.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you how using the new features can",
      "offset": 11771.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "improve a model. Why generate new",
      "offset": 11772.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "features if the information is already",
      "offset": 11775.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "available in the data set? Combining",
      "offset": 11777.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "features together can capture subtle",
      "offset": 11780.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "dependent effects between them that",
      "offset": 11781.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "impact the outcome variable. These can",
      "offset": 11783.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "be represented by multiplying, summing,",
      "offset": 11786.399,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "differencing, or dividing two or more",
      "offset": 11789.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "variables. To see the impact of",
      "offset": 11792.359,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "generating these features, let's suppose",
      "offset": 11794.399,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "you have two attributes, length and",
      "offset": 11796.239,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "width, and then the price of a",
      "offset": 11798.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "singlestory home. If these are your only",
      "offset": 11800.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "two features, how can you best create a",
      "offset": 11803.12,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "model to predict",
      "offset": 11805.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "price? They certainly don't look to be",
      "offset": 11806.359,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "very strong features as is. Taking the",
      "offset": 11808.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "previous example a step further, we can",
      "offset": 11811.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "think about how a person might buy a",
      "offset": 11813.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "home. If we use some intuition that",
      "offset": 11815.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "people often consider the area of a",
      "offset": 11817.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "home, we can create a new feature, total",
      "offset": 11819.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "square footage, by multiplying the width",
      "offset": 11821.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and length. The results are much better",
      "offset": 11823.76,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "with an R squ of 81. Applying your",
      "offset": 11826.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "reasoning and understanding to the",
      "offset": 11830.319,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "problem can help you build powerful",
      "offset": 11831.68,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "predictors. Our data set doesn't include",
      "offset": 11833.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "width and length because no one would",
      "offset": 11836.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "ever actually look for a house that way.",
      "offset": 11838,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "However, we don't have a total square",
      "offset": 11841.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "footage calculated, but we can create it",
      "offset": 11843.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "using width column and by adding square",
      "offset": 11845.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "footage below ground and square footage",
      "offset": 11847.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "above ground together. We can build",
      "offset": 11849.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "another feature price per square footage",
      "offset": 11852.64,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "using our previous feature square",
      "offset": 11855.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "footage. This is now the combination of",
      "offset": 11858.2,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "three independent variables. There isn't",
      "offset": 11860.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a limit to how deep you can go, but the",
      "offset": 11863.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "interpretability of what it means starts",
      "offset": 11865.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to become difficult after three. We can",
      "offset": 11867.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "create days on market as a difference",
      "offset": 11870.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "between list date and offmarket date. We",
      "offset": 11872.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "will cover how to get list date and",
      "offset": 11875.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "offmarket date into the datetime format",
      "offset": 11877.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "in the next section, but for now know",
      "offset": 11879.68,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "that new features can be generated many",
      "offset": 11882.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "ways. There's a major push in the data",
      "offset": 11885,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "science community to automate some of",
      "offset": 11887.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the generation of features. If this is",
      "offset": 11889.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of interest to you, I'd recommend you",
      "offset": 11891.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "check out the Python libraries feature",
      "offset": 11893.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tools and ts.",
      "offset": 11895.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I will caution you that simply",
      "offset": 11897.68,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "multiplying each feature pair-wise will",
      "offset": 11899.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "square your number of features. This can",
      "offset": 11901.439,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "cause an explosion of features that can",
      "offset": 11904.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "be unwieldy to model or could",
      "offset": 11905.68,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "potentially overfit your model by pure",
      "offset": 11907.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "coincidence. Many of the features may",
      "offset": 11910.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "convey similar information and won't be",
      "offset": 11912.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "needed. Lastly, there's no limit to how",
      "offset": 11914.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "many features you can combine, but the",
      "offset": 11917.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "interpretability certainly takes a steep",
      "offset": 11919.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "dive after three. Beyond this is the",
      "offset": 11921.68,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "realm of deep feature generation, a",
      "offset": 11924.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "topic for another course. In this video,",
      "offset": 11926.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "you learned that you can generate new",
      "offset": 11929.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "powerful features to represent complex",
      "offset": 11930.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "relationships between them. Lastly, you",
      "offset": 11932.96,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "saw that feature combinations are",
      "offset": 11935.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "everywhere and that many are already in",
      "offset": 11937.439,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "our data set. It's your turn to take",
      "offset": 11939.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "what you've learned and build and",
      "offset": 11942.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "evaluate new features generated from",
      "offset": 11943.6,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "what's available.",
      "offset": 11945.52,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "In this video, we will talk about using",
      "offset": 11949.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "time in our models since it isn't as",
      "offset": 11951.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "easy as throwing it into our models as a",
      "offset": 11953.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "continuous variable. Things repeat. Each",
      "offset": 11955.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "day has a noon, each week has a Monday,",
      "offset": 11958.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and each year has a January. We want to",
      "offset": 11961.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "help our model by building features that",
      "offset": 11964.16,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "help it associate cyclical events with",
      "offset": 11966,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "changes in our outcome variable, such as",
      "offset": 11968.239,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "summer having a higher volume of homes",
      "offset": 11970.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "sold than in winter. Building the right",
      "offset": 11972.399,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "time features is important. The high",
      "offset": 11975.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "variation in daily number of homes sold",
      "offset": 11977.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "makes this pattern hard for us to see",
      "offset": 11979.68,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "and the model to",
      "offset": 11981.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "understand. If we change the aggregation",
      "offset": 11983,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "to look at grouping by month, we can see",
      "offset": 11985.439,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the pattern much more clearly. Choosing",
      "offset": 11987.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the right level to build out time",
      "offset": 11990.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "related features is important as too",
      "offset": 11991.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "granular and they are too noisy for our",
      "offset": 11993.84,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "model, too broad and our model misses",
      "offset": 11996.08,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "trends. To work with dates, we need them",
      "offset": 11999.64,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "to be of the spark date type. We can do",
      "offset": 12002.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the conversion with two date function",
      "offset": 12005.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that takes a single column. If you wish",
      "offset": 12007.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to keep the time components, use two",
      "offset": 12009.84,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "timestamp",
      "offset": 12012.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "instead. With our data typed correctly,",
      "offset": 12013.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we can use built-ins to get various time",
      "offset": 12016.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "components. One popular way to handle",
      "offset": 12018.359,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "dates is to convert them into ordinal",
      "offset": 12020.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "features like year or month using the",
      "offset": 12022.479,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "functions year and month respectively.",
      "offset": 12025.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "We can also extract more complicated",
      "offset": 12027.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "things like day number in the month with",
      "offset": 12029.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "day of month or the week number in the",
      "offset": 12032.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "year with week of year to further build",
      "offset": 12034.8,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "out our",
      "offset": 12037.12,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "features. Many more functions can be",
      "offset": 12038.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "found in the pi spark SQL functions docs",
      "offset": 12040.239,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "online. One simple timebased metric is",
      "offset": 12043.8,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "the number of days a property remains",
      "offset": 12046.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "unsold from the date it was listed. Days",
      "offset": 12048.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "on market is an important feature to",
      "offset": 12051.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "buyers. They may perceive that a house",
      "offset": 12053.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that has been on the market for a while",
      "offset": 12055.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "has something wrong with it or that the",
      "offset": 12057.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "seller may be more willing to give them",
      "offset": 12058.96,
      "duration": 2.519
    },
    {
      "lang": "en",
      "text": "a",
      "offset": 12060.479,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "discount. We can create this metric by",
      "offset": 12061.479,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "applying the date diff function to",
      "offset": 12063.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "offmarket date and list date",
      "offset": 12065.84,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "columns. Lagging time features is a",
      "offset": 12068.76,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "common approach to add propagation time",
      "offset": 12071.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "for a variable's effect to impact the",
      "offset": 12073.359,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "outcome",
      "offset": 12075.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "variable. This is similar to how a drop",
      "offset": 12076.12,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "creates waves that take time to hit the",
      "offset": 12078.479,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "edge of our glass.",
      "offset": 12080.479,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "To capture this, we will shift values",
      "offset": 12082.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "forwards or backwards until the timings",
      "offset": 12084.319,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "line up. To create a lagged feature,",
      "offset": 12086.72,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "we'll need a few new",
      "offset": 12089.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "functions. First, the window function.",
      "offset": 12091.319,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "Window allows you to return a value for",
      "offset": 12094.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "each record based on some calculation",
      "offset": 12096.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "against a group of records such as rank",
      "offset": 12099.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "or moving average. The second function",
      "offset": 12101.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is lag, a window function that returns a",
      "offset": 12104.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "value that is offset by rows before the",
      "offset": 12107.359,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "current row.",
      "offset": 12109.68,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "It takes a dataf frame column as an",
      "offset": 12111.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "input. Count is how many periods you",
      "offset": 12113.239,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "wish to lag. Let's see it in",
      "offset": 12115.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "action. For this example, we will look",
      "offset": 12118.2,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "at lagging weekly mortgage rates as it",
      "offset": 12120.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "often takes time for people to adjust",
      "offset": 12122.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the price of their homes. To begin, we",
      "offset": 12124.16,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "will need to import our new",
      "offset": 12127.359,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "functions. Then we will create a window",
      "offset": 12129.239,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "which will group things by our ordered",
      "offset": 12132.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "date column, making our window weekly.",
      "offset": 12133.68,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "Once that is done, we can create a new",
      "offset": 12137.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "column using the lag function, telling",
      "offset": 12140.399,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "it to lag mortgage rate 30 US by one",
      "offset": 12142.88,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "period. The over function takes the",
      "offset": 12147,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "window W so that the lag knows how to",
      "offset": 12150.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "compare to the current record.",
      "offset": 12152.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Now it's your turn to create some of",
      "offset": 12155.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "your own time related features with",
      "offset": 12157.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "built-in datetime functions as well as",
      "offset": 12158.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "more complex ones using the window",
      "offset": 12160.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "function. Data sets frequently have rich",
      "offset": 12165.56,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "features trapped in messy combination",
      "offset": 12168.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "fields, lists, or even free form text.",
      "offset": 12169.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "In this video, we'll go over how to",
      "offset": 12173.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "wrangle columns into useful information",
      "offset": 12175.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for machine learning. We can see that",
      "offset": 12177.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "within this roof column, there are many",
      "offset": 12179.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "useful features. For instance, an old",
      "offset": 12181.279,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "roof is very expensive to replace, and",
      "offset": 12184,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "knowing that might impact the price of",
      "offset": 12186.319,
      "duration": 2.601
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 12188.16,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "house. The age in this data set is",
      "offset": 12188.92,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "either over 8 years or less. This will",
      "offset": 12191.439,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "be better as a boolean variable 0 or 1,",
      "offset": 12194.479,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "something that we can calculate",
      "offset": 12197.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "on. To create a boolean column, we will",
      "offset": 12199.64,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "use the when function to create an if",
      "offset": 12202.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "then. The when function evaluates the",
      "offset": 12205.399,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "boolean condition and then does",
      "offset": 12208.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "something.",
      "offset": 12209.68,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "In this case, our boolean conditions are",
      "offset": 12211.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "find over 8 and find under eight, which",
      "offset": 12213.439,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "use the like function to return true",
      "offset": 12216.56,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "false depending on if the string is",
      "offset": 12218.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "found. You might notice that we use the",
      "offset": 12221.239,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "percent sign before and after the string",
      "offset": 12223.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "we are looking for. These are wild cards",
      "offset": 12225.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that allow any number of characters",
      "offset": 12228.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "before or after the string. Now that we",
      "offset": 12230,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "have these conditions created, we can",
      "offset": 12232.8,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "put them into the when",
      "offset": 12234.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "function. When find over 8 is true,",
      "offset": 12236.04,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "assign one. When find under eight is",
      "offset": 12239.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "true, assign zero. If neither is true,",
      "offset": 12241.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the otherwise function allows us to",
      "offset": 12245.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "assign none. So they are null. We can",
      "offset": 12246.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "see that the roof age has now been",
      "offset": 12249.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "created into a new boolean variable.",
      "offset": 12251.359,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Let's look at the roof column again.",
      "offset": 12254.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "You'll notice that if there is a value",
      "offset": 12256.239,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in roof, it seems to be a list starting",
      "offset": 12257.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "with the type of materials it was made",
      "offset": 12260.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "out of. If we know the pattern, we can",
      "offset": 12262.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "split this into its own column called",
      "offset": 12265.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "roof material. To split a column, we",
      "offset": 12267.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "need to introduce a new function from",
      "offset": 12270.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "the PISpark SQL functions module, split,",
      "offset": 12271.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "which takes a column to split and a",
      "offset": 12275.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "character to split on. In our example,",
      "offset": 12277.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we will split on DF roof and use the",
      "offset": 12280.319,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "comma as the delimter between the",
      "offset": 12282.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "values. Once we have that created, we",
      "offset": 12285.239,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "can use our familiar width column to",
      "offset": 12287.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "create a new column from the first value",
      "offset": 12289.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "with split get item zero. Get item zero",
      "offset": 12292,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "takes the zero index position of the",
      "offset": 12295.92,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "split column and returns the value. Here",
      "offset": 12298.479,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "we can verify our code performed as",
      "offset": 12301.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "expected. Splitting the roof column and",
      "offset": 12303.56,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "putting the first value into a new",
      "offset": 12305.92,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "column roof",
      "offset": 12307.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "material. What if the order of the",
      "offset": 12309.479,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "listed values in a column is not",
      "offset": 12311.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "guaranteed and we want to extract out",
      "offset": 12313.319,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "all the values to their own columns? To",
      "offset": 12315.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "do this is a two-step process. The first",
      "offset": 12318.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "step is called exploding. changing a",
      "offset": 12321.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "compound field so that each value has a",
      "offset": 12324.399,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "separate record with everything else",
      "offset": 12326.64,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "repeated. The second step is to pivot",
      "offset": 12329.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "these repeated fields into",
      "offset": 12332.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "columns. You'll notice how we have",
      "offset": 12334.52,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "columns for each possible value in the",
      "offset": 12336.56,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "compound field we started",
      "offset": 12338.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "with. To do this in Pispark, we need to",
      "offset": 12340.439,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "import several functions. Split,",
      "offset": 12343.359,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "explode, lit, coales, and",
      "offset": 12346.12,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "first. Then we need to split our roof",
      "offset": 12349.8,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "column into an array column. Now we can",
      "offset": 12352.239,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "explode our roof list to create a new",
      "offset": 12355.279,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "record for each",
      "offset": 12357.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "value. Next, we'll create a constant",
      "offset": 12359.239,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "column to help our pivot. The pivot",
      "offset": 12361.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "function will group by our record ID so",
      "offset": 12364.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that only one row is returned for",
      "offset": 12367.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "pivoting X roof list.",
      "offset": 12369.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Since pivot is an aggregate function, we",
      "offset": 12372.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "will use a constant value column with",
      "offset": 12375.6,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "coales to ignore nulls and first to take",
      "offset": 12378.8,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "the first",
      "offset": 12382.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "value. In this video, we learned how to",
      "offset": 12383.319,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "salvage some messy fields into machine",
      "offset": 12385.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "learnable features. Now, it's your turn",
      "offset": 12388,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "to wrangle some features on your own.",
      "offset": 12389.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "This video will cover the basics of",
      "offset": 12395.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "binarizing, bucketing, and encoding with",
      "offset": 12396.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "Pispark with Spark ML transformers.",
      "offset": 12398.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "These methods are great ways to get the",
      "offset": 12402.319,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "most out of your",
      "offset": 12404.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "features. Binarization of data is a",
      "offset": 12405.399,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "helpful way to collapse some of the",
      "offset": 12407.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "nuance in your model to just a yes no.",
      "offset": 12409.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Homeowners often use yes no filters to",
      "offset": 12412.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "narrow their search for homes. For",
      "offset": 12414.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "example, they may only consider homes",
      "offset": 12416.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that have a fireplace but not care about",
      "offset": 12418.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "how many fireplaces as long as it's more",
      "offset": 12420.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "than one. Binarization takes values",
      "offset": 12423.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "below or equal to a threshold and",
      "offset": 12426.08,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "replaces them by zero values above it by",
      "offset": 12428.16,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "one. For this example, we will leverage",
      "offset": 12431.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "the Spark ML feature transformer",
      "offset": 12434.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "binarizer. Introduction of PISpark",
      "offset": 12437.399,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "showcase transformers in detail, so",
      "offset": 12439.68,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "we'll spend time just using",
      "offset": 12441.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "them. After importing binarizer, we need",
      "offset": 12443.479,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "to make sure the column we want to apply",
      "offset": 12446.64,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "it on is of type",
      "offset": 12448.239,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "double. We need to create a",
      "offset": 12450.2,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "transformation called bin with the",
      "offset": 12452.319,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "binarizer",
      "offset": 12454.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "class. Setting the threshold to zero so",
      "offset": 12456.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "that anything over zero will be",
      "offset": 12458.96,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "converted to one. Then set our input",
      "offset": 12460.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "call to fireplaces and our output to",
      "offset": 12463.439,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "fireplace t. To apply the",
      "offset": 12466.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "transformation, we apply transform with",
      "offset": 12468.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "our data frame. We can see the",
      "offset": 12471.04,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "transformation worked as expected",
      "offset": 12473.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "below. If you're a homeowner, you might",
      "offset": 12476.359,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "want to know that a house has 1, 2, 3,",
      "offset": 12479.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "or more bathrooms. But once you hit a",
      "offset": 12481.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "certain point, you don't really care",
      "offset": 12484.08,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "whether the house has seven or eight",
      "offset": 12485.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "bathrooms. Bucketing, also known as",
      "offset": 12487.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "binning, is a way to create ordinal",
      "offset": 12490.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "variables. Like the binarizer, we'll",
      "offset": 12492.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "import",
      "offset": 12495.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "bucketizer. Then we need to define our",
      "offset": 12496.04,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "splits for buckets of values. We want 0",
      "offset": 12498.319,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "to 1 mapped to 1. Greater than 1 to 2",
      "offset": 12501.76,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "mapped to two. Greater than 2 to 3",
      "offset": 12505.84,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "mapped to three. And lastly, anything",
      "offset": 12509.68,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "more than four map to four using the",
      "offset": 12512.64,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "infinity value float inf for the upper",
      "offset": 12515.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "bound. Then we can create the",
      "offset": 12518.92,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "transformer buck with our splits, the",
      "offset": 12520.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "input column and the output column.",
      "offset": 12523.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "We can then apply the transformer to our",
      "offset": 12526.239,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "data frame with",
      "offset": 12528.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "transform. As you can see, the",
      "offset": 12530.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "transformation created buckets for our",
      "offset": 12532.88,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "values",
      "offset": 12534.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "correctly. Some algorithms cannot handle",
      "offset": 12535.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "categorical data like the text field",
      "offset": 12538.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "city and it must be converted to a",
      "offset": 12540.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "numeric format like the ones to the",
      "offset": 12543.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "right to be evaluated correctly.",
      "offset": 12544.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "One method to handle this is called one",
      "offset": 12547.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "hot encoding where you pivot each",
      "offset": 12549.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "categorical variable to a true false",
      "offset": 12552.08,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "column of its own. Keep in mind for",
      "offset": 12555.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "columns with lots of different values,",
      "offset": 12558.239,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "this can create potentially hundreds or",
      "offset": 12560.16,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "thousands of new",
      "offset": 12562.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "columns. To apply one hot encoder",
      "offset": 12563.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "transformer, we need to do it in two",
      "offset": 12566,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "steps. First, we need the string indexer",
      "offset": 12568.04,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "transformer. The string indexer takes a",
      "offset": 12571.08,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "string in and maps each word to a",
      "offset": 12573.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "number.",
      "offset": 12575.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Then we can use the fit and transform",
      "offset": 12576.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "methods to perform the mapping and",
      "offset": 12578.72,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "transform the strings to numbers. Now we",
      "offset": 12580.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "can apply the one hot encoder",
      "offset": 12583.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "transformer on our index city values and",
      "offset": 12584.96,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "output all the encoded indexes to a",
      "offset": 12587.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "single column of type vector which is",
      "offset": 12590.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "more efficient than storing them all as",
      "offset": 12593.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "individual columns. Another thing to",
      "offset": 12594.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "note is that the last category is not",
      "offset": 12597.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "included by default because it is",
      "offset": 12599.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "linearly dependent on the other columns",
      "offset": 12601.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and is not needed. In this video, we",
      "offset": 12603.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "learned how to group values together as",
      "offset": 12606.479,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "well as how to convert categorical",
      "offset": 12608.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "values to numeric. You will apply these",
      "offset": 12610.239,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "transformers in the following examples.",
      "offset": 12613.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Good",
      "offset": 12615.2,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "luck. PiSpark has many different machine",
      "offset": 12618.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "learning algorithms available to choose",
      "offset": 12620.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "from. While this makes our ability to",
      "offset": 12622.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "predict, classify or cluster on enormous",
      "offset": 12624.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "data sets easier, the onus is on us to",
      "offset": 12627.359,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "choose the correct",
      "offset": 12630,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "one. This flowchart can help us navigate",
      "offset": 12631.399,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "what's available in Pispark's machine",
      "offset": 12634,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "learning library for dataf frames, ML.",
      "offset": 12635.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Recall that we are going to predict the",
      "offset": 12639.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "price of a home. This price is a",
      "offset": 12640.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "quantity, in this case of dollars, and",
      "offset": 12642.88,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "it is",
      "offset": 12644.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "continuous. That takes us to the",
      "offset": 12646.279,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "regression archetype, which predicts",
      "offset": 12648.239,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "continuous values.",
      "offset": 12650.239,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Lastly, we can see that algorithms for",
      "offset": 12652.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "solving our problem can be found within",
      "offset": 12654.479,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "the ML regression",
      "offset": 12656.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "module. ML regression provides us with",
      "offset": 12657.88,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "many different algorithms we could use.",
      "offset": 12660.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "These first methods differ mostly in how",
      "offset": 12663.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "they regularize, which is how they",
      "offset": 12665.12,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "prevent themselves from finding overly",
      "offset": 12667.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "complex solutions that are likely to",
      "offset": 12669.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "overfit the data. While these methods",
      "offset": 12671.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "can be powerful if used correctly, they",
      "offset": 12673.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "require a lot of upfront work to ensure",
      "offset": 12675.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "their assumptions are met. ML regression",
      "offset": 12677.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "also contains treebased methods which",
      "offset": 12680.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "have the ability to easily handle things",
      "offset": 12682.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like missing and categorical variables",
      "offset": 12684.64,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "right out of the",
      "offset": 12687.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "box. Decision trees are easy to",
      "offset": 12688.439,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "interpret but a lot of work needs to go",
      "offset": 12690.72,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "in to prevent",
      "offset": 12692.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "overfitting. So now we are down to two",
      "offset": 12693.72,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "algorithms random forest and GBT",
      "offset": 12695.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "regression which differ mostly in how",
      "offset": 12698.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "they handle error reduction.",
      "offset": 12700.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "We will choose to evaluate both random",
      "offset": 12702.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "forests as well as gradient boosted",
      "offset": 12704.72,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "trees or GBT",
      "offset": 12706.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "regression. Both random forest and",
      "offset": 12709.319,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "gradient boosted tree models are example",
      "offset": 12711.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of ensemble models. They combine many",
      "offset": 12713.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "smaller models together to create a more",
      "offset": 12716.479,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "powerful model. In the diagram, you can",
      "offset": 12718.399,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "see that we have many decision trees,",
      "offset": 12721.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "each only trained on a sample of the",
      "offset": 12723.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "data to prevent overfitting.",
      "offset": 12725.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "When it comes time to predict, a new",
      "offset": 12727.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "value runs through the decision trees",
      "offset": 12730.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and they merge their answers together to",
      "offset": 12731.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "create a prediction. If you've had some",
      "offset": 12733.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "exposure to machine learning, you may",
      "offset": 12736.319,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "have seen the crucial step of splitting",
      "offset": 12738,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "your data into test and training sets,",
      "offset": 12739.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which needs to be done before applying",
      "offset": 12741.92,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "feature",
      "offset": 12743.68,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "transformations. Commonly, data is split",
      "offset": 12744.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "randomly. Ours contains a time",
      "offset": 12747.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "component. So, splitting randomly would",
      "offset": 12749.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "leak information about what happens in",
      "offset": 12751.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "the future.",
      "offset": 12753.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "To prevent this, you can split your data",
      "offset": 12754.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "sequentially and train your model on the",
      "offset": 12756.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "first sequences and then test it with",
      "offset": 12758.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "the last. The size of your sets depend",
      "offset": 12760.64,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "on how far out you need to",
      "offset": 12763.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "forecast. Doing incremental testing is",
      "offset": 12765.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "called step forward",
      "offset": 12767.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "optimization. Here we will create just",
      "offset": 12769.399,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "one of the sequential test train splits.",
      "offset": 12771.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "With some added logic, you could build",
      "offset": 12774,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "out walk forward optimization seen",
      "offset": 12775.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "previously. First, we'll dynamically set",
      "offset": 12778.6,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "our time variables. It's important as",
      "offset": 12781.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "when your data set refreshes, you don't",
      "offset": 12783.439,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "have to remember to change",
      "offset": 12785.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "them. To start, we'll calculate the min",
      "offset": 12787.08,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "and max of offmarket dates. Then we can",
      "offset": 12789.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "put them into our date diff function to",
      "offset": 12793.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "get the number of days our data spans.",
      "offset": 12794.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "To create an 80/20 split, we can",
      "offset": 12797.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "multiply it by8 and add it to our min",
      "offset": 12799.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "date with date add to get the date",
      "offset": 12802.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "value. We can create our train and test",
      "offset": 12805.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "sets by using a wear function on DF",
      "offset": 12808.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "offmarket date to filter them. An extra",
      "offset": 12810.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "wear is needed on list state to ensure",
      "offset": 12814.08,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "it contains items listed as of the split",
      "offset": 12815.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "date. In this video, we saw how to",
      "offset": 12819.16,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "navigate PISpark ML and a few",
      "offset": 12821.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "considerations in the algorithm",
      "offset": 12824.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "selection process. Lastly, you learned",
      "offset": 12825.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "how to create tests and training sets",
      "offset": 12828.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "for time series. Let's see you",
      "offset": 12830.319,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "try. Each machine learning algorithm has",
      "offset": 12835.16,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "its own assumptions you need to take",
      "offset": 12837.76,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "into account for it to work",
      "offset": 12839.52,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "appropriately. In this video, we will",
      "offset": 12841.239,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "cover what the assumptions are for",
      "offset": 12843.439,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "random forest regression, what features",
      "offset": 12844.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we have in our final data set, and",
      "offset": 12847.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "lastly, how to get them ready for",
      "offset": 12849.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "building a model. The lack of",
      "offset": 12850.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "assumptions needed for random forest",
      "offset": 12853.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "regression make it and its related",
      "offset": 12854.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "methods some of the most popular choices",
      "offset": 12856.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "for predicting continuous values.",
      "offset": 12858.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "For example, random forests are able to",
      "offset": 12861.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "work with non-normally distributed data",
      "offset": 12863.92,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "or data that is",
      "offset": 12866,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "unscaled. Missing and categorical data",
      "offset": 12867.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "can be handled very easily with value",
      "offset": 12870,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "replacements. Adding in external data",
      "offset": 12873.08,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "sets is one of my personal favorite",
      "offset": 12875.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "parts of modeling. It's where I find",
      "offset": 12877.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that you can often make huge",
      "offset": 12879.6,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "improvements in your model relatively",
      "offset": 12880.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "easily. Here are a few that I added.",
      "offset": 12882.84,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "30-year mortgage rate to see how much",
      "offset": 12886.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "people are willing to pay depending on",
      "offset": 12888.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "their rate. City data to see how unique",
      "offset": 12890.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "a house is in the area or if it's",
      "offset": 12893.6,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "exceptionally cheap or",
      "offset": 12895.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "expensive. Transportation metrics can",
      "offset": 12897.479,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "help us understand how much people are",
      "offset": 12899.84,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "willing to pay for a convenient",
      "offset": 12901.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "location. Lastly, I included bank",
      "offset": 12903.239,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "holidays to see if that impacted how or",
      "offset": 12905.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "when houses were sold. By all means,",
      "offset": 12907.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "this is not an exhaustive list of data",
      "offset": 12910.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sets to include, but just some I chose.",
      "offset": 12912.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Even though we were able to avoid a lot",
      "offset": 12915.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of the ownorous pre-processing steps by",
      "offset": 12917.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "using random force regression, there's",
      "offset": 12919.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "still plenty of work to do with",
      "offset": 12921.12,
      "duration": 2.359
    },
    {
      "lang": "en",
      "text": "engineering",
      "offset": 12922.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "features. Time components like month or",
      "offset": 12923.479,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "week that a holiday falls on are needed",
      "offset": 12926,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "to help attribute seasonal",
      "offset": 12927.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "effects. Valuable, but often the hardest",
      "offset": 12930.359,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "to create features are rates, ratios,",
      "offset": 12932.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and other generated features that need",
      "offset": 12935.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "either business or personal context to",
      "offset": 12936.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "create.",
      "offset": 12938.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Lastly, choosing whether or not to",
      "offset": 12940.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "expand compound fields is ultimately a",
      "offset": 12942.479,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "judgment call and may be something to",
      "offset": 12944.8,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "consider in the second iteration of your",
      "offset": 12946.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "modeling. Since Pispark dataf frames",
      "offset": 12948.439,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "don't have a shape attribute, we'll have",
      "offset": 12951.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to print our own to inspect the final",
      "offset": 12953.12,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "set of",
      "offset": 12954.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "information. PiSpark ML algorithms",
      "offset": 12956.439,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "require all of the features to be",
      "offset": 12959.359,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "provided in a single column of type",
      "offset": 12960.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "vector. We will need to convert our",
      "offset": 12963,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "columns for random forest regression to",
      "offset": 12965.12,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "work.",
      "offset": 12966.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "To do this, we need to import vector",
      "offset": 12968.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "assembler transformer to use it later.",
      "offset": 12970.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Sadly, while random forest regression",
      "offset": 12973.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "can handle missing values, vectors",
      "offset": 12975.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "cannot. Due to the nature of how",
      "offset": 12977.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "treebased machine learning partitions",
      "offset": 12980.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "data, we can just assign missings a",
      "offset": 12982.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "value that is outside the range of the",
      "offset": 12984.319,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "variable and replace the nulls with",
      "offset": 12986.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "it. But first, we need to know which",
      "offset": 12989,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "columns to convert.",
      "offset": 12991.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "We can take a list of column names and",
      "offset": 12993.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "remove our dependent variable so the",
      "offset": 12995.52,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "vector contains only",
      "offset": 12997.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "features. To create a vector assembler,",
      "offset": 12999.399,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "we need to supply it with our list of",
      "offset": 13002.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "columns and a name for output. Applying",
      "offset": 13003.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the transformation is done via the",
      "offset": 13007.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "transform method. Lastly, we need to",
      "offset": 13008.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "create a new data frame with just the",
      "offset": 13011.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "columns that matter sales close price",
      "offset": 13013.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 13015.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "features. Finally, we are ready for",
      "offset": 13017.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "machine learning. Our features have been",
      "offset": 13019.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "created and prepared for the algorithm",
      "offset": 13021.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that we are running. Now it's your turn",
      "offset": 13023.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to convert the columns to vectors and",
      "offset": 13025.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "get ready for applying random forest",
      "offset": 13027.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "regression. We've covered all the",
      "offset": 13031.319,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "necessary steps to prepare ourselves for",
      "offset": 13033.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "modeling. In this video, we will cover",
      "offset": 13035.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "training, predicting, and evaluating a",
      "offset": 13037.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "random force regression model.",
      "offset": 13039.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "The Pispark random force regressor",
      "offset": 13041.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "method has a ton of optional parameters",
      "offset": 13043.6,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "and a few hyperparameters used for",
      "offset": 13045.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tuning. To have a minimally viable",
      "offset": 13047.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "model, you will need to set only a",
      "offset": 13050.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "handful. First up is features call which",
      "offset": 13052.52,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "tells the model which column is the",
      "offset": 13056,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "vector we created with the vector",
      "offset": 13057.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "assembler that now represents all of our",
      "offset": 13059.12,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "feature",
      "offset": 13061.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "data. Since we named the column",
      "offset": 13062.12,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "features, we will use that to set",
      "offset": 13064.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "features call.",
      "offset": 13066,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Next up is label call which sets the",
      "offset": 13067.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "dependent variable for the model. Ours",
      "offset": 13070.239,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "is named sales close",
      "offset": 13072.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "price. Then we need to name our output",
      "offset": 13074.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "column by setting prediction call. I",
      "offset": 13077.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "find it helpful to be explicit rather",
      "offset": 13079.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "than leaving it the default value. So",
      "offset": 13082.08,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "I've named it prediction",
      "offset": 13083.76,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "price. Last of the basic parameters is",
      "offset": 13085.56,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "seed which by setting this to a value we",
      "offset": 13088.68,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "can ensure that subsequent runs return",
      "offset": 13091.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the same model. Without it, the random",
      "offset": 13093.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "forest would be slightly different. I",
      "offset": 13095.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "have set mine to 42 for good luck, but",
      "offset": 13098,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the specific number isn't important.",
      "offset": 13100.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Enough talking. Let's build our model.",
      "offset": 13103.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "To start, we need to import the random",
      "offset": 13106,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "forest regressor from Pispark's ML",
      "offset": 13108,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "module. Once that's done, we can",
      "offset": 13110.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "initialize random forest regressor with",
      "offset": 13113.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the appropriate columns to use for",
      "offset": 13115.04,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "training and predicting. Again, setting",
      "offset": 13116.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the seed is crucial for repeatability.",
      "offset": 13119.439,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "Lastly, we create a variable to hold our",
      "offset": 13122.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "train model uninspiredly called model",
      "offset": 13124.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and train the random force regressor RF",
      "offset": 13128.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "by calling fit with our training data",
      "offset": 13130.64,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "frame train",
      "offset": 13132.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "DF. Congratulations, you've created a",
      "offset": 13133.72,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "model. Wait, you want to predict new",
      "offset": 13136.64,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "values with it? Predicting house prices",
      "offset": 13139.439,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "with the model we just trained is",
      "offset": 13142.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "straightforward. To do so, we can call",
      "offset": 13144.279,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "transform with the data withheld from",
      "offset": 13146.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "training the test set testd.",
      "offset": 13148.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "If you had new listings of homes and",
      "offset": 13151.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "wanted to predict their prices, you'd",
      "offset": 13153.359,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "merely have to pre-process it in the",
      "offset": 13155.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "same manner as testd before using the",
      "offset": 13157.12,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "model to predict",
      "offset": 13159.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "prices. Given that testdf has the actual",
      "offset": 13160.76,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "home sale prices, we can inspect them",
      "offset": 13163.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "side by side by using the select to grab",
      "offset": 13165.84,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "only the columns we care about and",
      "offset": 13168.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "displaying them with show. Predicting",
      "offset": 13170.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "values is great, but if we don't know",
      "offset": 13173.04,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "how good we are at it, then what's the",
      "offset": 13174.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "point?",
      "offset": 13176.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "To evaluate the model, we need to import",
      "offset": 13178,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "regression evaluator which allows us to",
      "offset": 13180.56,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "calculate various metrics to gauge model",
      "offset": 13182.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "performance. To initialize it, we need",
      "offset": 13185.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "to provide the actual values in this",
      "offset": 13188,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "case sales close price and the predicted",
      "offset": 13190,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "values which we named prediction price",
      "offset": 13192.72,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "when we created the",
      "offset": 13194.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "model. Once we have the instance of",
      "offset": 13196.359,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "evaluator created, we can call it with",
      "offset": 13198.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "our predictions data frame and a",
      "offset": 13200.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "dictionary of the metric type we wish to",
      "offset": 13202.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "evaluate it with. Which metric you",
      "offset": 13204.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "choose to optimize is an important",
      "offset": 13207.12,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "decision to",
      "offset": 13209.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "make. We can see that our model's RMSSE",
      "offset": 13210.279,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "returns a value in the thousands while",
      "offset": 13213.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "our R 2 is less than one. R 2 is easy to",
      "offset": 13215.2,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "interpret regardless of what you are",
      "offset": 13218.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "predicting. If it's zero, you are no",
      "offset": 13220.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "better than random chance. If it's one,",
      "offset": 13222.479,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you are predicting perfectly. On the",
      "offset": 13224.96,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "other hand, RMSSE provides an absolute",
      "offset": 13227.2,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "number of unexplained variance in our",
      "offset": 13229.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "model. It's even in the same units as",
      "offset": 13231.239,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "our prediction, US dollars.",
      "offset": 13233.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So even though our R squared is really",
      "offset": 13236.239,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "high, RMSSE indicates that we have",
      "offset": 13237.92,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "$22,000 of unexplained variance on",
      "offset": 13240.52,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "average. This video showed you little",
      "offset": 13244.12,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "code is needed to train, predict, and",
      "offset": 13246.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "evaluate a model with PISPARC. Now it's",
      "offset": 13248.399,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "your turn to build some models on your",
      "offset": 13250.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "own. In this video, we will go over how",
      "offset": 13254.359,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "to interpret the model and then how to",
      "offset": 13256.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "save and load it for later use.",
      "offset": 13258.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Now that we've evaluated our model, we",
      "offset": 13261.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "will want to understand what features",
      "offset": 13263.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "are important in predicting a home",
      "offset": 13265.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "selling price. To do this, we will need",
      "offset": 13266.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "to import the pandas library to",
      "offset": 13269.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "manipulate this tiny array easier. To",
      "offset": 13271.359,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "use spark on this would be using a",
      "offset": 13274.16,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "sledgehammer for a delicate",
      "offset": 13275.84,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "task. We will create a dataf frame f to",
      "offset": 13277.8,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "hold our feature importances. These",
      "offset": 13281.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "feature importances can be accessed by",
      "offset": 13283.68,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "calling feature importances on the model",
      "offset": 13285.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and then converting them to an array",
      "offset": 13288.239,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "with two",
      "offset": 13290.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "array. Since this is just an array of",
      "offset": 13291.399,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "numbers, we will need to name the new",
      "offset": 13293.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "column in the dataf frame",
      "offset": 13295.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "importance. Now we just have a single",
      "offset": 13297.72,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "column data frame. We will want to",
      "offset": 13299.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "create another column using the list of",
      "offset": 13302.239,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "feature names we fed into the vector",
      "offset": 13304,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "assembler earlier. We can convert this",
      "offset": 13305.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "list into a series by wrapping it with",
      "offset": 13308.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "PD series.",
      "offset": 13310.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Next, since we have over a 100 features,",
      "offset": 13312.479,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "we only want to look at the most",
      "offset": 13315.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "important ones. So, we will use Panda's",
      "offset": 13316.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sort values to sort the column",
      "offset": 13319.2,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "importances in descending",
      "offset": 13321.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "order. Now, it's as simple as displaying",
      "offset": 13323.72,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "out the results to the screen. Here, we",
      "offset": 13326.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "can see the biggest predictor of how",
      "offset": 13328.399,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "much your house will sell for is how",
      "offset": 13329.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "much you listed it for. Intuitively,",
      "offset": 13331.439,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this makes a lot of sense. Realators are",
      "offset": 13334,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "skilled in setting the value of the",
      "offset": 13336.319,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "house and it has the effect of anchoring",
      "offset": 13337.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the price, meaning it will likely only",
      "offset": 13339.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "marginally increase or decrease from",
      "offset": 13341.92,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "that",
      "offset": 13343.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "value. Last but not least, it's",
      "offset": 13344.6,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "important to know how to save and load",
      "offset": 13346.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the model. Luckily, this is very simple.",
      "offset": 13348.399,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Now, to save it, just call save on your",
      "offset": 13350.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "model and give it a model name. Note",
      "offset": 13353.439,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "that the model isn't a single file, but",
      "offset": 13356.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "a directory containing many files",
      "offset": 13358,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "defining your model.",
      "offset": 13359.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "To load your data, you need to import",
      "offset": 13362,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "random forest regression model from",
      "offset": 13364.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "PISPARC ML regression and provided the",
      "offset": 13366.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "location and the name of your model. In",
      "offset": 13369.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this video, we learned how to interpret",
      "offset": 13371.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "results and save and load a model for",
      "offset": 13373.2,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "later use. Let's see you give it a",
      "offset": 13375.52,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "try. Thank you for making it to the end",
      "offset": 13380.12,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "of this course. Let's take a minute to",
      "offset": 13382.319,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "review what we've learned. In this",
      "offset": 13384.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "course, you learned many ways to inspect",
      "offset": 13386.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "your data visually and statistically,",
      "offset": 13388.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "data manipulation techniques such as",
      "offset": 13390.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "dropping rows and columns, scaling and",
      "offset": 13392.88,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "adjusting data, and handling missing",
      "offset": 13395.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "values. You also learned how to enrich",
      "offset": 13397.96,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "your data sets by joining external data",
      "offset": 13400.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "sets, generating features, extracting",
      "offset": 13402.479,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "variables from messy fields, and various",
      "offset": 13405.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "encoding techniques.",
      "offset": 13407.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Lastly, you learned a bit about how all",
      "offset": 13409.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this ties into modeling by training and",
      "offset": 13411.68,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "evaluating a model and interpreting the",
      "offset": 13413.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "results. That sure was a lot of stuff we",
      "offset": 13416.359,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "covered in a short amount of time. I",
      "offset": 13418.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "hope you will be able to take what",
      "offset": 13420.96,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "you've learned here and apply it to real",
      "offset": 13422,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "world problems and make the world a",
      "offset": 13423.6,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "better place with data science. John",
      "offset": 13425.359,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Hogout. Hi, welcome to the course on",
      "offset": 13430.439,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "machine learning with Apache Spark in",
      "offset": 13433.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "which you will learn how to build",
      "offset": 13436.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "machine learning models on large data",
      "offset": 13438.319,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "sets using distributed computing",
      "offset": 13441.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "techniques. Let's start with some",
      "offset": 13444.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "fundamental",
      "offset": 13446.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "concepts. Suppose you wanted to teach a",
      "offset": 13448.04,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "computer how to make waffles. You could",
      "offset": 13450.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "find a good recipe and then give the",
      "offset": 13453.76,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "computer explicit instructions about",
      "offset": 13455.84,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "ingredients and proportions.",
      "offset": 13458.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Alternatively, you could present the",
      "offset": 13461.56,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "computer with selection of different",
      "offset": 13463.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "waffle recipes and let it figure out the",
      "offset": 13466.399,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "ingredients and proportions for the best",
      "offset": 13469.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "recipe. The second approach is how",
      "offset": 13473.08,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "machine learning works. The computer",
      "offset": 13475.76,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "literally learns from",
      "offset": 13478.56,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "examples. Machine learning problems are",
      "offset": 13482.04,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "generally less esoteric than finding the",
      "offset": 13484.479,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "perfect waffle recipe. The most common",
      "offset": 13487.52,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "problems apply either regression or",
      "offset": 13490.479,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "classification. A regression model",
      "offset": 13494.68,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "learns to predict a number. For example,",
      "offset": 13497.359,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "when making waffles, how much flour",
      "offset": 13501.199,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "should be used for a particular amount",
      "offset": 13503.76,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 13506.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "sugar. A classification model, on the",
      "offset": 13507.479,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "other hand, predicts a discrete or",
      "offset": 13510.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "categorical value.",
      "offset": 13513.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "For example, is a recipe calling for a",
      "offset": 13516,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "particular amount of sugar and salt more",
      "offset": 13519.04,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "likely to be for waffles or",
      "offset": 13521.92,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "cupcakes? The performance of a machine",
      "offset": 13525.8,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "learning model depends on data. In",
      "offset": 13528.319,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "general, more data is a good thing. If",
      "offset": 13531.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "an algorithm is able to train on a",
      "offset": 13535.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "larger set of data, then its ability to",
      "offset": 13537.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "generalize to new data will inevitably",
      "offset": 13540.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "improve.",
      "offset": 13543.6,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "However, there are some practical",
      "offset": 13545.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "constraints. If the data can fit",
      "offset": 13548.359,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "entirely into RAM, then the algorithm",
      "offset": 13550.88,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "can operate",
      "offset": 13553.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "efficiently. What happens when those",
      "offset": 13555.479,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "data no longer fit into",
      "offset": 13557.6,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "memory? The computer will start to use",
      "offset": 13560.199,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "virtual memory and data will be paged",
      "offset": 13563.359,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "back and forth between RAM and disk.",
      "offset": 13566.8,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Relative to RAM access, retrieving data",
      "offset": 13570.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "from disk is",
      "offset": 13573.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "slow. As the size of the data grows,",
      "offset": 13575.8,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "paging becomes more intense and the",
      "offset": 13578.96,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "computer begins to spend more and more",
      "offset": 13581.359,
      "duration": 8.601
    },
    {
      "lang": "en",
      "text": "time waiting for data. Performance",
      "offset": 13584.399,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "plummets. How then do we deal with truly",
      "offset": 13589.96,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "large data sets? One option is to",
      "offset": 13593.359,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "distribute the problem across multiple",
      "offset": 13596.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "computers in a",
      "offset": 13599.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "cluster. Rather than trying to handle a",
      "offset": 13601.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "large data set on a single machine, it's",
      "offset": 13604.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "divided up into partitions which are",
      "offset": 13607.76,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "processed",
      "offset": 13610.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "separately. Ideally, each data partition",
      "offset": 13612.359,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "can fit into RAM on a single computer in",
      "offset": 13615.68,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 13618.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "cluster. This is the approach used by",
      "offset": 13619.56,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "Spark.",
      "offset": 13622.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Spark is a generalpurpose framework for",
      "offset": 13624.16,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "cluster",
      "offset": 13626.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "computing. It is popular for two main",
      "offset": 13628.439,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "reasons. It's generally much faster than",
      "offset": 13631.56,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "other big data technologies like Hadoop",
      "offset": 13634.56,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "because it does most processing in",
      "offset": 13637.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "memory and it has a developer friendly",
      "offset": 13640.76,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "interface which hides much of the",
      "offset": 13643.92,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "complexity of distributed computing.",
      "offset": 13646.319,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "Let's review the components of a Spark",
      "offset": 13650.399,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "cluster. The cluster itself consists of",
      "offset": 13653.479,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "one or more nodes. Each node is a",
      "offset": 13656.319,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "computer with CPU, RAM, and physical",
      "offset": 13660.239,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "storage. A cluster manager allocates",
      "offset": 13664.199,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "resources and coordinates activity",
      "offset": 13667.279,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "across the",
      "offset": 13670.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "cluster. Every application running on",
      "offset": 13671.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "the Spark cluster has a driver",
      "offset": 13675.04,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "program. Using the Spark API, the driver",
      "offset": 13677.96,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "communicates with the cluster manager",
      "offset": 13681.68,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "which in turn distributes work to the",
      "offset": 13684,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "nodes. On each node, Spark launches an",
      "offset": 13687.56,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "executive process which persists for the",
      "offset": 13691.279,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "duration of the",
      "offset": 13694.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "application. Work is divided up into",
      "offset": 13696.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "tasks which are simply units of",
      "offset": 13698.8,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "computation. The executives run tasks in",
      "offset": 13701.64,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "multiple threads across the cores in a",
      "offset": 13705.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "node.",
      "offset": 13707.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "When working with Spark, you normally",
      "offset": 13709.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "don't need to worry too much about the",
      "offset": 13711.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "details of the cluster. Spark sets up",
      "offset": 13713.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "all of that infrastructure for you and",
      "offset": 13717.04,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "handles all interactions within the",
      "offset": 13719.359,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "cluster. However, it's still useful to",
      "offset": 13722.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "know how it works under the",
      "offset": 13725.199,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "hood. You now have a basic understanding",
      "offset": 13727.72,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "of the principles of machine learning",
      "offset": 13730.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and distributed computing with Spark.",
      "offset": 13732.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Next, we'll learn how to connect to a",
      "offset": 13735.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Spark",
      "offset": 13738.319,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "cluster. The previous lesson was",
      "offset": 13741.96,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "highlevel overviews of machine learning",
      "offset": 13744.72,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 13747.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Spark. In this lesson, you'll review the",
      "offset": 13748.439,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "process of connecting to Spark. The",
      "offset": 13751.279,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "connection with Spark is established by",
      "offset": 13754,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "the driver which can be written in",
      "offset": 13756.239,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "either Java, Scala, Python or R.",
      "offset": 13758.479,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "Each of these languages has advantages",
      "offset": 13763.439,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 13766,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "disadvantages. Java is relatively",
      "offset": 13767.08,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "verbose, requiring a lot of code to",
      "offset": 13769.6,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "accomplish even simple tasks. By",
      "offset": 13772.08,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "contrast, Scala, Python, and R are",
      "offset": 13775.239,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "highle languages which can accomplish",
      "offset": 13778.479,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "much with only a small amount of code.",
      "offset": 13780.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "They also offer a ripple or read",
      "offset": 13784.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "evaluate print loop which is crucial for",
      "offset": 13787.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "interactive development.",
      "offset": 13790.88,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "you'll be using",
      "offset": 13793.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Python. Python doesn't talk natively to",
      "offset": 13794.68,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "Spark. So, we'll kick off by importing",
      "offset": 13797.84,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "the PIS Spark module, which makes Spark",
      "offset": 13800.399,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "functionality available in the Python",
      "offset": 13803.479,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "interpreter. Spark is under vigorous",
      "offset": 13807.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "development. Because the interface is",
      "offset": 13809.88,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "evolving, it's important to know what",
      "offset": 13812.04,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "version you're working with. We'll be",
      "offset": 13814.72,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "using version 2.4.1,",
      "offset": 13817.279,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "4.1 which was released in March",
      "offset": 13819.56,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "2019. In addition to the main pi spark",
      "offset": 13824.199,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "module, there are a few subm modules",
      "offset": 13827.52,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "which implement different aspects of the",
      "offset": 13830.64,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "spark",
      "offset": 13833.199,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "interface. There are two versions of",
      "offset": 13834.279,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "spark machine learning. MLIB which uses",
      "offset": 13836.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "an unstructured representation of data",
      "offset": 13840.239,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "and RDDs and has been deprecated and ML",
      "offset": 13842.72,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "which is based on a structured tabular",
      "offset": 13846.72,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "representation of data and data frames.",
      "offset": 13849.279,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "We'll be using the",
      "offset": 13852.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "letter. With the PIS spark module",
      "offset": 13854.12,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "loaded, you're able to connect to Spark.",
      "offset": 13856.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "The next thing you need to do is tell",
      "offset": 13860.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Spark where the cluster is located. Here",
      "offset": 13862.56,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "there are two options.",
      "offset": 13866.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "You can either connect to a remote",
      "offset": 13869.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "cluster in which case you need to",
      "offset": 13872,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "specify a spark URL which gives the",
      "offset": 13874.16,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "network location of the cluster's master",
      "offset": 13877.439,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "node. The URL is composed of an IP",
      "offset": 13880.279,
      "duration": 7.881
    },
    {
      "lang": "en",
      "text": "address or DNS name and a port number.",
      "offset": 13884.16,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "The default port for Spark is",
      "offset": 13888.16,
      "duration": 8.119
    },
    {
      "lang": "en",
      "text": "7077, but this must still be explicitly",
      "offset": 13891.479,
      "duration": 7.401
    },
    {
      "lang": "en",
      "text": "specified. When you're figuring out how",
      "offset": 13896.279,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "Spark works, the infrastructure of a",
      "offset": 13898.88,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "distributed cluster can get in the way.",
      "offset": 13901.52,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "That's why it's useful to create a local",
      "offset": 13905.439,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "cluster where everything happens on a",
      "offset": 13908.08,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "single",
      "offset": 13910.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "computer. This is the setup that you're",
      "offset": 13911.56,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "going to use throughout this course. For",
      "offset": 13914.239,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "a local cluster, you need only specify",
      "offset": 13917.279,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "local and optionally the number of cores",
      "offset": 13920.439,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "to use. By default, a local cluster will",
      "offset": 13923.76,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "run on a single core. Alternatively, you",
      "offset": 13927.439,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "can give a specific number of cores or",
      "offset": 13930.64,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "simply use the wild card to choose all",
      "offset": 13933.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "available",
      "offset": 13937.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "cores. You connect to Spark by creating",
      "offset": 13938.92,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "a Spark session object.",
      "offset": 13941.84,
      "duration": 7.559
    },
    {
      "lang": "en",
      "text": "The Spark session class is found in the",
      "offset": 13944.88,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "pispark.SQL subm module. You specify the",
      "offset": 13949.399,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "location of the cluster using the master",
      "offset": 13953.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "method. Optionally, you can assign a",
      "offset": 13956.279,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "name to the application using the app",
      "offset": 13959.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "name method. Finally, you call the get",
      "offset": 13961.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "or create method which will either",
      "offset": 13965.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "create a new session object or return an",
      "offset": 13968.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "existing object.",
      "offset": 13971.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Once the session has been created,",
      "offset": 13974.16,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "you're able to interact with",
      "offset": 13976.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Spark. Finally, although it's possible",
      "offset": 13978.92,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "for multiple Spark sessions to coexist,",
      "offset": 13982.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "it's good practice to stop the Spark",
      "offset": 13985.279,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "session when you're",
      "offset": 13987.68,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "done. Great. Let's connect",
      "offset": 13989.56,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "Spark. In this lesson, you'll look at",
      "offset": 13995.319,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "how to read data into Spark.",
      "offset": 13998,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "Spark represents tabular data using the",
      "offset": 14001.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "dataf frame class. The data are captured",
      "offset": 14004.479,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "as rows or records, each of which is",
      "offset": 14007.6,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "broken down into one or more columns or",
      "offset": 14010.96,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "fields. Every column has a name and a",
      "offset": 14014.76,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "specific data type. Some selected",
      "offset": 14018.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "methods and attributes of the dataf",
      "offset": 14021.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "frame class are listed here. The count",
      "offset": 14024.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "method gives the number of rows. The",
      "offset": 14027.52,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "show method will display a subset of",
      "offset": 14030.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "rows. The print schema method and the",
      "offset": 14033.64,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "dtypes attribute give different views on",
      "offset": 14036.56,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "column types. This is really scratching",
      "offset": 14040.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "the surface of what's possible with a",
      "offset": 14043.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "dataf frame. You can find out more by",
      "offset": 14045.279,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "consulting the extensive",
      "offset": 14048.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "documentation. CSV is a common format",
      "offset": 14051.479,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "for storing tabular data. For",
      "offset": 14054.319,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "illustration, we'll be using a CSV file",
      "offset": 14057.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "with characteristics for a selection of",
      "offset": 14060.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "motor",
      "offset": 14062.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "vehicles. Each line in a CSV file is a",
      "offset": 14063.88,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "new record. And within each record,",
      "offset": 14067.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "fields are separated by a delimiter",
      "offset": 14070.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "character, which is normally a comma.",
      "offset": 14072.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "The first line is an optional header",
      "offset": 14075.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "record, which gives column names. Our",
      "offset": 14077.68,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "session object has a read attribute",
      "offset": 14081.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "which in turn has a CSV method which",
      "offset": 14084.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "reads data from a CSV file and returns a",
      "offset": 14088.08,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "data frame. The CSV method has one",
      "offset": 14091.76,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "mandatory argument, the path to the CSV",
      "offset": 14095.84,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "file. There are a number of optional",
      "offset": 14099.56,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "arguments. We'll take a quick look at",
      "offset": 14102.279,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "some of the most important ones.",
      "offset": 14104.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "The header argument specifies whether or",
      "offset": 14107.359,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "not there is a header",
      "offset": 14110.16,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "record. The SE argument gives the field",
      "offset": 14112.12,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "separator which is a comma by default.",
      "offset": 14115.439,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "There are two arguments which pertain to",
      "offset": 14119.04,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "column data types schema and infer",
      "offset": 14121.84,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "schema. Finally, the null value argument",
      "offset": 14125.88,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "gives the placeholder used to indicate",
      "offset": 14129.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "missing data.",
      "offset": 14131.92,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "Let's take a look at the data we've just",
      "offset": 14134.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "loaded. Using the show method, we can",
      "offset": 14136.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "take a look at a slice of the data",
      "offset": 14139.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "frame. The CSV method has split the data",
      "offset": 14141.479,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "into rows and columns and picked up the",
      "offset": 14144.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "column names from the header record.",
      "offset": 14147.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Looks great, doesn't it? Unfortunately,",
      "offset": 14150.96,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "there's a small",
      "offset": 14154.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "snag. Before we unravel that snag, it's",
      "offset": 14155.479,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "important to note that the first value",
      "offset": 14158.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "in the cylinder column is not a number.",
      "offset": 14161.279,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "It's the string NA, which indicates",
      "offset": 14164.56,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "missing",
      "offset": 14167.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "data. If you check the column data",
      "offset": 14169.399,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "types, then you'll find that they are",
      "offset": 14172.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "all strings. That doesn't make sense",
      "offset": 14174.08,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "since the last six columns are clearly",
      "offset": 14177.04,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "numbers. However, this is the expected",
      "offset": 14180.52,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "behavior. The CSV method treats all",
      "offset": 14183.76,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "columns as strings by",
      "offset": 14187.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "default. You need to do a little more",
      "offset": 14189.56,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "work to get the correct column types.",
      "offset": 14191.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "There are two ways that you can do this.",
      "offset": 14195.439,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "Infer the column types from the data or",
      "offset": 14198.16,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "manually specify the",
      "offset": 14201.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "types. It's possible to reasonably",
      "offset": 14203.72,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "deduce the column types by setting the",
      "offset": 14206.479,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "infer schema argument to true.",
      "offset": 14209.359,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "There is a price to pay though. Spark",
      "offset": 14212.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "needs to make an extra pass over the",
      "offset": 14215.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "data to figure out the column types",
      "offset": 14218.239,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "before reading the data. If the data",
      "offset": 14221.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "file is big, then this will increase the",
      "offset": 14224.319,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "load time",
      "offset": 14226.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "notably. Using this approach, all of the",
      "offset": 14228.92,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "column types are correctly identified",
      "offset": 14231.92,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "except for cylinder. Why? The first",
      "offset": 14235.12,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "value in this column is in A. So Spark",
      "offset": 14238.56,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "thinks that the column contains",
      "offset": 14242.399,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "strings. Missing data in CSV files are",
      "offset": 14245.16,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "normally represented by a placeholder",
      "offset": 14249.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "like the NA string. We can use the null",
      "offset": 14251.76,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "value argument to specify the",
      "offset": 14255.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "placeholder. It's always a good idea to",
      "offset": 14258.04,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "explicitly define the missing data",
      "offset": 14260.96,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "placeholder.",
      "offset": 14263.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "The null value argument is",
      "offset": 14265.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "case-sensitive. So it's important to",
      "offset": 14267.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "provide it in exactly the same form as",
      "offset": 14269.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "it appears in the data",
      "offset": 14272.8,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "file. If inferring column type is not",
      "offset": 14274.92,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "successful, then you have the option of",
      "offset": 14279.239,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "specifying the type of each column in an",
      "offset": 14281.6,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "explicit",
      "offset": 14284.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "schema. This also makes it possible to",
      "offset": 14285.64,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "choose alternative column names.",
      "offset": 14288.72,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "This is what the final car's data look",
      "offset": 14292.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "like. Note that the missing value at the",
      "offset": 14295.08,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "top of the cylinders column is indicated",
      "offset": 14298.08,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "by the special null",
      "offset": 14300.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "constant. You're ready to use what",
      "offset": 14303.479,
      "duration": 7.401
    },
    {
      "lang": "en",
      "text": "you've learned to load data from CSV",
      "offset": 14305.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "files. In this lesson, you're going to",
      "offset": 14311.8,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "learn how to prepare data for building a",
      "offset": 14314.239,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "machine learning model. You'll be",
      "offset": 14316.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "working with the cars data again. This",
      "offset": 14319.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "is what the data look like at present.",
      "offset": 14322.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "There are columns for the maker and",
      "offset": 14325.04,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "model. The origin, either USA or non",
      "offset": 14326.96,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "USA, the type, number of cylinders,",
      "offset": 14330.52,
      "duration": 8.28
    },
    {
      "lang": "en",
      "text": "engine size, weight, length, RPM, and",
      "offset": 14334.319,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "fuel consumption.",
      "offset": 14338.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "The models that you'll be building will",
      "offset": 14341.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "depend on the physical characteristics",
      "offset": 14342.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "of the cars rather than the model names",
      "offset": 14345.12,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "or",
      "offset": 14348.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "manufacturers. So you'll remove the",
      "offset": 14348.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "corresponding columns from the",
      "offset": 14351.12,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "data. There are two approaches to doing",
      "offset": 14353.239,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "this. Either you can drop the columns",
      "offset": 14355.96,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "that you don't want or you can select",
      "offset": 14359.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "the fields which you do want to retain.",
      "offset": 14362.16,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Either way, the resulting data does not",
      "offset": 14365.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "include those",
      "offset": 14368.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "columns earlier. You saw that there is a",
      "offset": 14370.76,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "missing value in the cylinders column.",
      "offset": 14373.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Let's check to see how many other",
      "offset": 14377.199,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "missing values there are. You'll use the",
      "offset": 14379.84,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "filter method and provide a logical",
      "offset": 14383.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "predicate using SQL syntax which",
      "offset": 14385.439,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "identifies null values. Then the count",
      "offset": 14388.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "method tells you how many records there",
      "offset": 14392.319,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "are remaining, just one. In this case,",
      "offset": 14394.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "it makes sense to simply remove the",
      "offset": 14399.359,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "record with the missing",
      "offset": 14401.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "value. There are a couple of ways that",
      "offset": 14403.64,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "you could do this. You could use the",
      "offset": 14405.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "filter method again with a different",
      "offset": 14408.239,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "predicate. Or you could take a more",
      "offset": 14410.76,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "aggressive approach and use the drop NA",
      "offset": 14413.439,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "method to drop all records with missing",
      "offset": 14416.319,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "values in any",
      "offset": 14419.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "column. However, this should be done",
      "offset": 14421.319,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "with care because it could result in the",
      "offset": 14424.319,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "loss of a lot of otherwise useful",
      "offset": 14426.96,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "data. You've now stripped down the data",
      "offset": 14430.439,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "to what's needed to build the model. At",
      "offset": 14433.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "present, the weight and length columns",
      "offset": 14436.8,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "are in units of pounds and inches,",
      "offset": 14439.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "respectively. You'll use the width",
      "offset": 14442.279,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "column method to create a new mass",
      "offset": 14444.56,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "column in units of kilograms. The round",
      "offset": 14447.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "function is used to limit the precision",
      "offset": 14451.12,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "of the",
      "offset": 14453.84,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "result. You can also use the width",
      "offset": 14454.92,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "column method to replace the existing",
      "offset": 14457.439,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "length column with values in meters.",
      "offset": 14460.239,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "You now have mass and length in metric",
      "offset": 14464.239,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "units. The type column consists of",
      "offset": 14468.439,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "strings which represent six categories",
      "offset": 14471.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of vehicle type. You will need to",
      "offset": 14473.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "transform those strings into numbers.",
      "offset": 14476.479,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "You do this using an instance of the",
      "offset": 14479.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "string indexer class. In the",
      "offset": 14482.239,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "constructor, you provide the name of the",
      "offset": 14485.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "string input column and a name for the",
      "offset": 14487.279,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "new output column to be created.",
      "offset": 14490.239,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "The indexer is first fit to the data",
      "offset": 14493.84,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "creating a string indexer model. During",
      "offset": 14497.04,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the fitting process, the distinct string",
      "offset": 14500.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "values are identified and an index is",
      "offset": 14503.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "assigned to each value. The model is",
      "offset": 14506.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "then used to transform the data,",
      "offset": 14509.84,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "creating a new column with the index",
      "offset": 14512.239,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "values. By default, the index values are",
      "offset": 14516.04,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "assigned according to the descending",
      "offset": 14519.12,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "relative frequency of each of the string",
      "offset": 14521.68,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "values. Midsize is the most common, so",
      "offset": 14524.84,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "it gets an index of zero. Small is the",
      "offset": 14528.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "next most common, so its index is one,",
      "offset": 14531.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and so on.",
      "offset": 14534.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "It's possible to choose different",
      "offset": 14536.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "strategies for assigning index values by",
      "offset": 14538.72,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "specifying the string order type",
      "offset": 14542.16,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "argument. Rather than using frequency of",
      "offset": 14545.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "occurrence, strings can be ordered",
      "offset": 14548.12,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "alphabetically. It's also possible to",
      "offset": 14551.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "choose between ascending and descending",
      "offset": 14553.279,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "order. You'll be building a classifier",
      "offset": 14556.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "to predict whether or not a car was",
      "offset": 14559.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "manufactured in the USA. So the origin",
      "offset": 14561.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "column also needs to be converted from",
      "offset": 14565.279,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "strings into",
      "offset": 14567.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "numbers. The final step in preparing the",
      "offset": 14569.8,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "car's data is to consolidate the various",
      "offset": 14572.72,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "input columns into a single column. This",
      "offset": 14575.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "is necessary because the machine",
      "offset": 14579.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "learning algorithms in Spark operate on",
      "offset": 14581.199,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "a single vector of predictors. Although",
      "offset": 14584.56,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "each element in that vector may consist",
      "offset": 14588.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "of multiple values.",
      "offset": 14591.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "To illustrate the process, you'll start",
      "offset": 14593.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "with just a pair of features, cylinders,",
      "offset": 14596.16,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 14599.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "size. First, you create an instance of",
      "offset": 14600.439,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "the vector assembler class, providing it",
      "offset": 14603.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with the names of the columns that you",
      "offset": 14606.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "want to consolidate and the name of the",
      "offset": 14608,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "new output column. The assembler is then",
      "offset": 14610.479,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "used to transform the data.",
      "offset": 14614.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Taking a look at the relevant columns,",
      "offset": 14617.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "you see that the new features column",
      "offset": 14619.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "consists of values from the cylinders",
      "offset": 14622.239,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "and size columns consolidated into a",
      "offset": 14624.479,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "vector. Ultimately, you're going to",
      "offset": 14628.52,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "assemble all of the predictors into a",
      "offset": 14631.04,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "single column. Let's try out what we've",
      "offset": 14633.6,
      "duration": 8.52
    },
    {
      "lang": "en",
      "text": "learned on the SMS and flights data.",
      "offset": 14636.479,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "Your first machine learning model will",
      "offset": 14643.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "be a decision tree. This is probably the",
      "offset": 14645.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "most intuitive model, so it seems like a",
      "offset": 14648.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "good place to",
      "offset": 14651.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "start. A decision tree is constructed",
      "offset": 14653,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "using an algorithm called recursive",
      "offset": 14655.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "partitioning. Consider a hypothetical",
      "offset": 14658.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "example in which you build a decision",
      "offset": 14661.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "tree to divide data into two classes,",
      "offset": 14663.76,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "green and blue. You start by putting all",
      "offset": 14667.279,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "of the records into the root node.",
      "offset": 14671.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Suppose that there are more green",
      "offset": 14673.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "records than blue, in which case this",
      "offset": 14675.6,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "node will be labeled",
      "offset": 14678.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "green. Now from amongst the predictors",
      "offset": 14680.199,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "in the data, you need to choose the one",
      "offset": 14683.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "that will result in the most informative",
      "offset": 14685.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "split of the data into two groups.",
      "offset": 14688.16,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Ideally, you want the groups to be as",
      "offset": 14691.279,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "homogeneous or pure as possible. One",
      "offset": 14693.64,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "should be mostly green and the other",
      "offset": 14697.76,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "should be mostly",
      "offset": 14700.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "blue. Once you've identified the most",
      "offset": 14702.199,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "informative predictor, you split the",
      "offset": 14704.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "data into two sets labeled green or blue",
      "offset": 14707.279,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "according to the dominant class. And",
      "offset": 14710.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "this is where the recursion kicks in.",
      "offset": 14714.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "You then apply exactly the same",
      "offset": 14716.64,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "procedure on each of the child nodes,",
      "offset": 14718.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "selecting the most informative predictor",
      "offset": 14722.399,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "and splitting",
      "offset": 14725.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "again. So for example, the green node on",
      "offset": 14727.239,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "the left could be split again into two",
      "offset": 14731.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "groups and the resulting green node",
      "offset": 14734.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "could once again be",
      "offset": 14737.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "split. The depth of each branch of the",
      "offset": 14739.16,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "tree need not be the same. There are a",
      "offset": 14741.92,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "variety of stopping criteria which can",
      "offset": 14745.52,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "cause splitting to stop along a",
      "offset": 14748.479,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "branch. For example, if the number of",
      "offset": 14751.72,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "records in a node falls below a",
      "offset": 14754.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "threshold or the purity of a node is",
      "offset": 14757.479,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "above a threshold, then you might stop",
      "offset": 14760.319,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "splitting. Once you have built the",
      "offset": 14764.279,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "decision tree, you can use it to make",
      "offset": 14766.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "predictions for new data by following",
      "offset": 14768.399,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the splits from the root node along to",
      "offset": 14771.279,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the tip of a branch. The label for the",
      "offset": 14774.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "final node would then be the prediction",
      "offset": 14777.359,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "for the new",
      "offset": 14779.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "data. Let's make this more concrete by",
      "offset": 14781.16,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "looking at the car's data.",
      "offset": 14784,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "You've transformed the country of origin",
      "offset": 14787.04,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "column into a numeric index called label",
      "offset": 14789.279,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "with zero corresponding to cars",
      "offset": 14793.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "manufactured in the USA and one for",
      "offset": 14795.439,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "everything else. The remaining columns",
      "offset": 14798.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "have all been consolidated into a column",
      "offset": 14801.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "called",
      "offset": 14805.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "features. You want to build the decision",
      "offset": 14806.52,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "tree which will use features to predict",
      "offset": 14809.12,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "label. An important aspect of building a",
      "offset": 14813.16,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "machine learning model is being able to",
      "offset": 14816.239,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "assess how well it works. In order to do",
      "offset": 14818.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "this, we use the random split method to",
      "offset": 14822.319,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "randomly split our data into two sets, a",
      "offset": 14825.52,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "training set and a testing set. The",
      "offset": 14829.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "proportions may vary, but generally",
      "offset": 14832.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you're looking at something like an",
      "offset": 14834.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "80/20 split, which means that the",
      "offset": 14836.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "training set ends up having around four",
      "offset": 14839.359,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "times as many records as the testing",
      "offset": 14841.68,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "set. Finally, the moment has come.",
      "offset": 14844.92,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "You're going to build a decision tree.",
      "offset": 14848.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "You start by creating a decision tree",
      "offset": 14850.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "classifier object. The next step is to",
      "offset": 14854.08,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "fit the model to the training data by",
      "offset": 14857.84,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "calling the fit",
      "offset": 14860.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "method. Now that you've trained the",
      "offset": 14862.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "model, you can assess how effective it",
      "offset": 14865.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "is by making predictions on the test set",
      "offset": 14867.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and comparing the predictions to the",
      "offset": 14871.04,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "known",
      "offset": 14873.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "values. The transform method adds new",
      "offset": 14874.12,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "columns to the data frame. The",
      "offset": 14877.439,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "prediction column gives the class",
      "offset": 14880.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "assigned by the model. You can compare",
      "offset": 14882.319,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "this directly to the known labels in the",
      "offset": 14885.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "testing data. Although the model gets",
      "offset": 14887.6,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the first example wrong, it's correct",
      "offset": 14890.72,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "for the following four",
      "offset": 14893.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "examples. There's also a probability",
      "offset": 14895.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "column which gives the probabilities",
      "offset": 14897.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "assigned to each of the outcome classes.",
      "offset": 14899.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "For the first example, the model",
      "offset": 14903.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "predicts that the outcome is zero with",
      "offset": 14905.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "probability 96%.",
      "offset": 14908.199,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "A good way to understand the performance",
      "offset": 14911.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "of a model is to create a confusion",
      "offset": 14913.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "matrix which gives a breakdown of a",
      "offset": 14916.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "model predictions versus the known",
      "offset": 14918.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "labels. The confusion matrix consists of",
      "offset": 14921.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "four counts which are labeled as",
      "offset": 14924.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "follows. Positive indicates a prediction",
      "offset": 14926.76,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "of one while negative indicates a",
      "offset": 14930,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "prediction of zero and true corresponds",
      "offset": 14933.04,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "to a correct prediction while false",
      "offset": 14937.359,
      "duration": 7.241
    },
    {
      "lang": "en",
      "text": "designates an incorrect",
      "offset": 14940.8,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "prediction. In this case, the true",
      "offset": 14944.6,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "positives and true negatives dominate,",
      "offset": 14947.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "but the model still makes a number of",
      "offset": 14950.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "incorrect predictions.",
      "offset": 14952.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "These counts can be used to calculate",
      "offset": 14955.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the accuracy which is the proportion of",
      "offset": 14957.279,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "correct predictions. For our model, the",
      "offset": 14960.16,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "accuracy is",
      "offset": 14963.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "74%. So now that you know how to build",
      "offset": 14965.56,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "the decision tree model with Spark, you",
      "offset": 14968.8,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "can try that out on the flight",
      "offset": 14971.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "data. You've learned to build the",
      "offset": 14976.199,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "decision tree, but it's good to have",
      "offset": 14978.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "options. Logistic regression is another",
      "offset": 14980.96,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "commonly used classification model. It",
      "offset": 14984.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "uses a logistic function to model a",
      "offset": 14987.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "binary target where the target states",
      "offset": 14990,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "are usually denoted by one and zero or",
      "offset": 14992.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "true and false. The maths of the model",
      "offset": 14996,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "are outside the scope of this course.",
      "offset": 14998.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "But this is what the logistic function",
      "offset": 15001.439,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "looks like. For a logistic regression",
      "offset": 15003.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "model, the x-axis is a linear",
      "offset": 15006.399,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "combination of predictive variables and",
      "offset": 15008.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the yaxis is the output of the model.",
      "offset": 15011.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Since the value of the logistic function",
      "offset": 15015.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is a number between 0 and one, it's",
      "offset": 15017.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "often thought of as a",
      "offset": 15019.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "probability. In order to translate the",
      "offset": 15021.8,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "number into one or other of the target",
      "offset": 15024.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "states, it's compared to a threshold,",
      "offset": 15026.319,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "which is normally set at 1/2.",
      "offset": 15029.04,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "If the number is above the threshold,",
      "offset": 15032.8,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "then the predicted state is one.",
      "offset": 15035.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Conversely, if it's below the threshold,",
      "offset": 15038.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "then the predicted state is",
      "offset": 15041.199,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "zero. The model derives coefficients for",
      "offset": 15043.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "each of the numerical",
      "offset": 15047.04,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "predictors. Those coefficients might",
      "offset": 15048.68,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "shift the curve to the right or to the",
      "offset": 15051.439,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "left.",
      "offset": 15054.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "They might make the transition between",
      "offset": 15056.08,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "states more gradual or more",
      "offset": 15058.16,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "rapid. These characteristics are all",
      "offset": 15062.52,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "extracted from the training data and",
      "offset": 15066.319,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "will vary from one set of data to",
      "offset": 15068.96,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "another. Let's make this more concrete",
      "offset": 15072.439,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "by returning to the cars data. You'll",
      "offset": 15075.439,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "focus on the numerical predictors for",
      "offset": 15078.56,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the moment and return to the categorical",
      "offset": 15080.8,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "predictors later",
      "offset": 15083.439,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "on. As before, you prepare the data by",
      "offset": 15085.239,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "consolidating the predictors into a",
      "offset": 15089.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "single column and then randomly",
      "offset": 15091.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "splitting the data into training and",
      "offset": 15094.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "testing sets.",
      "offset": 15097.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "To build a logistic regression model,",
      "offset": 15099.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you first need to import the associated",
      "offset": 15101.92,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "class and then create a classifier",
      "offset": 15104.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "object. This is then fit to the training",
      "offset": 15107.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "data using the fit",
      "offset": 15110.319,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "method. With the trained model, you're",
      "offset": 15112.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "able to make predictions on the testing",
      "offset": 15115.439,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "data. As you saw with the decision tree,",
      "offset": 15117.64,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "the transform method adds the prediction",
      "offset": 15120.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and probability columns. The probability",
      "offset": 15123.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "column gives the predicted probability",
      "offset": 15126.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "of each class while the prediction",
      "offset": 15128.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "column reflects the predicted label",
      "offset": 15131.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "which is derived from the probabilities",
      "offset": 15134.399,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "by applying the threshold mentioned",
      "offset": 15136.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "earlier. You can assess the quality of",
      "offset": 15139.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the predictions by forming a confusion",
      "offset": 15142.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "matrix. The quantities in the cells of",
      "offset": 15144.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "the matrix can then be used to form some",
      "offset": 15147.279,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "informative ratios.",
      "offset": 15149.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Recall that a positive prediction",
      "offset": 15152.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "indicates that a car is manufactured",
      "offset": 15154.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "outside of the US and that the",
      "offset": 15156.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "predictions are considered to be true or",
      "offset": 15159.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "false depending on whether they are",
      "offset": 15161.359,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "correct or",
      "offset": 15163.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "not. Precision is the proportion of",
      "offset": 15164.359,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "positive predictions which are correct.",
      "offset": 15167.439,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "For your model, 2/3 of predictions for",
      "offset": 15170.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "cars manufactured outside of the US are",
      "offset": 15173.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "correct.",
      "offset": 15176.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Recall is the proportion of positive",
      "offset": 15178.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "targets which are correctly predicted.",
      "offset": 15181.12,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Your model also identifies 80% of cars",
      "offset": 15184.159,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "which are actually manufactured outside",
      "offset": 15187.84,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "of the",
      "offset": 15190.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "US. Bear in mind that these metrics are",
      "offset": 15191.479,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "based on a relatively small testing",
      "offset": 15194.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "set. Another way of looking at these",
      "offset": 15197.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "ratios is to weight them across the",
      "offset": 15200.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "positive and negative predictions. You",
      "offset": 15202.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "can do this by creating an evaluator",
      "offset": 15206.08,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "object and then calling the evaluate",
      "offset": 15208.239,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "method. This method accepts an argument",
      "offset": 15210.92,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "which specifies the required metric.",
      "offset": 15214.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "It's possible to request the weighted",
      "offset": 15217.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "precision and recall as well as the",
      "offset": 15220,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "overall",
      "offset": 15222.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "accuracy. It's also possible to get the",
      "offset": 15223.8,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "F1 metric, the harmonic mean of",
      "offset": 15226.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "precision and recall which is generally",
      "offset": 15228.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "more robust than the accuracy.",
      "offset": 15231.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "All of these metrics have assumed a",
      "offset": 15234.8,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "threshold of",
      "offset": 15236.96,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "1/2. What happens if you vary that",
      "offset": 15238.199,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "threshold? A threshold is used to decide",
      "offset": 15241.319,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "whether the number returned by the",
      "offset": 15244.159,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "logistic regression model translates",
      "offset": 15246.08,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "into either the positive or the negative",
      "offset": 15248.479,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "class. By default, that threshold is set",
      "offset": 15251.479,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "at a half. However, this is not the only",
      "offset": 15254.399,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "choice. Choosing a larger or smaller",
      "offset": 15257.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "value for the threshold will affect the",
      "offset": 15260.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "performance of the model.",
      "offset": 15262.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "The ROC curve plots the true positive",
      "offset": 15264.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "rate versus the false positive rate as",
      "offset": 15267.68,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "the threshold increases from zero top",
      "offset": 15270.72,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "right to one bottom left. The AU",
      "offset": 15273.6,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "summarizes the ROC curve in a single",
      "offset": 15278.319,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "number. It's literally the area under",
      "offset": 15281.199,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "the ROC",
      "offset": 15284.479,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "curve. AU indicates how well a model",
      "offset": 15285.88,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "performs across all values of the",
      "offset": 15289.439,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "threshold. An ideal model that performs",
      "offset": 15291.6,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "perfectly regardless of the threshold",
      "offset": 15295.12,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "would have AU of one. In an exercise,",
      "offset": 15297.439,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "we'll see how to use another evaluator",
      "offset": 15301.6,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "to calculate the",
      "offset": 15304.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "AU. You now know how to build a logistic",
      "offset": 15305.96,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "regression model and assess the",
      "offset": 15308.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "performance of that model using various",
      "offset": 15311.04,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "metrics. Let's give this a try.",
      "offset": 15312.96,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "It's said that 80% of machine learning",
      "offset": 15318.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "is data preparation. As we'll see in",
      "offset": 15320.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "this lesson, this is particularly true",
      "offset": 15323.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "for text data. Before you can use",
      "offset": 15326.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "machine learning algorithms, you need to",
      "offset": 15329.279,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "take unstructured text data and create",
      "offset": 15331.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "structure ultimately transforming the",
      "offset": 15335.159,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "data into a table. We start with a",
      "offset": 15337.68,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "collection of documents. These documents",
      "offset": 15341.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "might be anything from a short snippet",
      "offset": 15344.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "of text like an SMS or email to a",
      "offset": 15346.64,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "lengthy report or book. Each document",
      "offset": 15349.76,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "will become a record in the",
      "offset": 15353.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "table. The text in each document will be",
      "offset": 15355.8,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "mapped to columns in the table. First,",
      "offset": 15359.04,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "the text is split into words or tokens.",
      "offset": 15362.399,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "You then remove short or common words",
      "offset": 15365.439,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "that do not convey too much information.",
      "offset": 15368.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "The table will then indicate the number",
      "offset": 15371.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of times that each of the remaining",
      "offset": 15373.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "words occurred in the text. This table",
      "offset": 15375.359,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "is also known as a term document",
      "offset": 15378.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "matrix. There are some nuances to the",
      "offset": 15382.199,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "process, but that's the central",
      "offset": 15384.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "idea. Suppose that your documents are",
      "offset": 15387.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the names of children's",
      "offset": 15390.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "books. The raw data might look like",
      "offset": 15392.439,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "this.",
      "offset": 15395.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Your job will be to transform these data",
      "offset": 15396.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "into a table with one row per document",
      "offset": 15398.8,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "and a column for each of the",
      "offset": 15402.08,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "words. You're interested in words, not",
      "offset": 15404.12,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "punctuation. You'll use regular",
      "offset": 15407.56,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "expressions or reax, a mini language for",
      "offset": 15410.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "pattern matching to remove the",
      "offset": 15413.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "punctuation symbols.",
      "offset": 15415.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "Regular expressions is another big topic",
      "offset": 15418.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and outside the scope of this course,",
      "offset": 15420.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "but basically you're giving a list of",
      "offset": 15423.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "symbols or text pattern to match. The",
      "offset": 15425.279,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "hyphen is escaped by the backslashes",
      "offset": 15428.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "because it has another meaning in the",
      "offset": 15431.52,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "context of regular",
      "offset": 15433.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "expressions. By escaping it, you tell",
      "offset": 15434.92,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "Spark to interpret the hyphen",
      "offset": 15437.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "literally. You need to specify a column",
      "offset": 15441.239,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "name books.ext text, a pattern to be",
      "offset": 15443.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "matched, stored in the variable reax,",
      "offset": 15446.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and the replacement text, which is",
      "offset": 15449.76,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "simply a",
      "offset": 15451.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "space. You now have some double spaces,",
      "offset": 15452.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "but you can use reax to clean those up,",
      "offset": 15456,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "too. Next, you split the text into words",
      "offset": 15459.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "or",
      "offset": 15462.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tokens. You create a tokenizer object,",
      "offset": 15463.399,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "giving it the name of the input column",
      "offset": 15466.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "containing the text and the output",
      "offset": 15468.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "column, which will contain the tokens.",
      "offset": 15470.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "The tokenizer is then applied to the",
      "offset": 15473.68,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "text using the transform method. In the",
      "offset": 15476,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "results, you see a new column in which",
      "offset": 15479.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "each document has been transformed into",
      "offset": 15481.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "a list of words. As a side effect, the",
      "offset": 15484,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "words have all been reduced to",
      "offset": 15487.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "lowercase. Some words occur frequently",
      "offset": 15490.92,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "in all of the documents. These common or",
      "offset": 15493.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "stop words convey very little",
      "offset": 15496.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "information. So you will also remove",
      "offset": 15499,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "them using an instance of the stop words",
      "offset": 15501.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "remover class. This contains a list of",
      "offset": 15504.159,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "stop words which can be customized if",
      "offset": 15507.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "necessary. Since you didn't give the",
      "offset": 15510.68,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "input and output column names earlier,",
      "offset": 15512.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you specify them now and then apply the",
      "offset": 15515.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "transform method. You could also have",
      "offset": 15518,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "given these names when you created the",
      "offset": 15520.72,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "remover. Your documents might contain a",
      "offset": 15524.12,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "large variety of words. So in principle",
      "offset": 15527.439,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "our table could end up with an enormous",
      "offset": 15530.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "number of columns many of which would be",
      "offset": 15532.64,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "only sparsely",
      "offset": 15535.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "populated. It would also be handy to",
      "offset": 15537.319,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "convert the words into",
      "offset": 15539.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "numbers. Enter the hashing trick which",
      "offset": 15542.199,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "in simple terms converts words into",
      "offset": 15545.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "numbers. You create an instance of the",
      "offset": 15548.84,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "hashing TF class providing the names of",
      "offset": 15551.279,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "the input and output columns.",
      "offset": 15554.239,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "You also give the number of features",
      "offset": 15557.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "which is effectively the largest number",
      "offset": 15559.359,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "that will be produced by the hashing",
      "offset": 15561.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "trick. This needs to be sufficiently big",
      "offset": 15563.72,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "to capture the diversity in the words.",
      "offset": 15566.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "The output in the hash column is",
      "offset": 15569.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "presented in sparse format which we will",
      "offset": 15571.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "talk about more later on. For the moment",
      "offset": 15574.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "though it's enough to note that there",
      "offset": 15578.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "are two lists. The first list contains",
      "offset": 15579.84,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "the hashed values and the second list",
      "offset": 15582.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "indicates how many times each of those",
      "offset": 15585.279,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "values",
      "offset": 15587.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "occurs. For example, in the first",
      "offset": 15588.52,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "document, the word long has a hash of 8",
      "offset": 15591.12,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "and occurs",
      "offset": 15594.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "twice. Similarly, the word five has a",
      "offset": 15596.279,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "hash of six and occurs once in each of",
      "offset": 15599.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the last two documents.",
      "offset": 15603.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "The final step is to account for some",
      "offset": 15606.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "words occurring frequently across many",
      "offset": 15608.08,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "documents. If a word appears in many",
      "offset": 15610.76,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "documents, then it's probably going to",
      "offset": 15613.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "be less useful for building a",
      "offset": 15615.439,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "classifier. We want to weight the number",
      "offset": 15617.319,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "of counts for a word in a particular",
      "offset": 15619.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "document against how frequently that",
      "offset": 15621.68,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "word occurs across all",
      "offset": 15624.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "documents. To do this, you reduce the",
      "offset": 15626.439,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "effective count for more common words,",
      "offset": 15629.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "giving what is known as the inverse",
      "offset": 15631.439,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "document frequency.",
      "offset": 15633.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "The word five for example occurs in",
      "offset": 15635.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "multiple documents so its effective",
      "offset": 15637.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "frequency is reduced. Conversely the",
      "offset": 15640.239,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "word long only occurs in one document so",
      "offset": 15643.12,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "its effective frequency is",
      "offset": 15646.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "increased. Inverse document frequency is",
      "offset": 15649.399,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "generated by the IDF",
      "offset": 15652.239,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "class. The inverse document frequencies",
      "offset": 15655.72,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "are precisely what we need for building",
      "offset": 15658.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "a machine learning model. Let's do that",
      "offset": 15660.319,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "with the SMS",
      "offset": 15663.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "data. In the last chapter, you saw how",
      "offset": 15667.159,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "to use categorical variables in a model",
      "offset": 15670.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "by simply converting them to indexed",
      "offset": 15673.439,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "numerical",
      "offset": 15676.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "values. In general, this is not",
      "offset": 15677.319,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "sufficient for a regression model. Let's",
      "offset": 15679.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "see why. In the car's data, the type",
      "offset": 15682.319,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "column is categorical with six levels.",
      "offset": 15686,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "midsize, small, compact, sporty, large,",
      "offset": 15688.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "and van. Here you can see the number of",
      "offset": 15692.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "times that each of those levels occurs",
      "offset": 15695.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "in the data. You use the string indexer",
      "offset": 15697.68,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "to assign a numerical index to each",
      "offset": 15701.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "level. However, there's a problem with",
      "offset": 15703.96,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "the index. The numbers don't have any",
      "offset": 15706.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "objective meaning. The index for sporty",
      "offset": 15709.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "is three. Does it make sense to do",
      "offset": 15712.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "arithmetic on that index? No.",
      "offset": 15715.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "For example, it wouldn't be meaningful",
      "offset": 15718.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to add the index for sporty to the index",
      "offset": 15720.08,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 15723.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "compact. Nor would it be valid to",
      "offset": 15724.359,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "compare those indexes and say that",
      "offset": 15726.8,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "sporty is larger or smaller than",
      "offset": 15728.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "compact. However, a regression model",
      "offset": 15732.04,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "works by doing precisely this arithmetic",
      "offset": 15734.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "on predictive variables.",
      "offset": 15738.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "You need to convert the indexed values",
      "offset": 15741.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "into a format in which you can perform",
      "offset": 15743.84,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "meaningful mathematical",
      "offset": 15746.479,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "operations. The first step is to create",
      "offset": 15749.72,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "a column for each of the levels.",
      "offset": 15752.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Effectively, you then place a check in",
      "offset": 15755.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the column corresponding to the value in",
      "offset": 15757.6,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "each row. So, for example, a record with",
      "offset": 15759.84,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "a type of sporty would have a check in",
      "offset": 15763.439,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "the sporty column. These new columns are",
      "offset": 15766.319,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "known as dummy",
      "offset": 15769.76,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "variables. However, rather than having",
      "offset": 15771.399,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "checks in the dummy variable columns, it",
      "offset": 15774.479,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "makes more sense to use binary values",
      "offset": 15777.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "where a one indicates the presence of",
      "offset": 15779.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the corresponding level. It might occur",
      "offset": 15782.08,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "to you that the volume of data has",
      "offset": 15785.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "exploded. You've gone from a single",
      "offset": 15787.72,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "column of categorical variables to six",
      "offset": 15790.08,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "binary encoded dummy variables. If there",
      "offset": 15793.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "were more levels, then you'd have even",
      "offset": 15797.439,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "more columns. This could get out of",
      "offset": 15799.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "hand. However, the majority of the cells",
      "offset": 15802.68,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "in the new columns contain zeros. The",
      "offset": 15806,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "nonzero values, which actually encode",
      "offset": 15809.12,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "the information, are relatively",
      "offset": 15812.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "infrequent. This effect becomes even",
      "offset": 15815,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "more pronounced if there are more",
      "offset": 15817.68,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "levels. You can exploit this by",
      "offset": 15819.72,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "converting the data into a sparse",
      "offset": 15822.159,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "format.",
      "offset": 15824.64,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "Rather than recording the individual",
      "offset": 15826.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "values, the sparse representation simply",
      "offset": 15828.6,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "records the column numbers and the value",
      "offset": 15831.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "for the nonzero",
      "offset": 15835.04,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "values. You can take this one step",
      "offset": 15836.92,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "further. Since the categorical levels",
      "offset": 15839.399,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "are mutually exclusive, you can drop one",
      "offset": 15842.08,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "of the columns. If type is not midsize,",
      "offset": 15845.04,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "small, compact, sporty or large, then it",
      "offset": 15848.64,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "must be van.",
      "offset": 15852.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "The process of creating dummy variables",
      "offset": 15854.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "is called one hot encoding because only",
      "offset": 15856.64,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "one of the columns created is ever",
      "offset": 15859.68,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "active or",
      "offset": 15862.479,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "hot. Let's see how this is done in",
      "offset": 15864.279,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "Spark. As you might expect, there's a",
      "offset": 15867.56,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "class for doing one hot encoding. Import",
      "offset": 15870.399,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "the one hot encoder estimator class from",
      "offset": 15873.52,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "the feature subm module. When",
      "offset": 15876.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "instantiating the class, you need to",
      "offset": 15879.439,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "specify the names of the input and",
      "offset": 15881.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "output columns. For car type, the input",
      "offset": 15883.199,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "column is the index we defined earlier.",
      "offset": 15886.56,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "Choose type dummy as the output column",
      "offset": 15889.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "name. Note that these arguments are",
      "offset": 15892.6,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "given as lists, so it's possible to",
      "offset": 15895.12,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "specify multiple columns if",
      "offset": 15897.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "necessary. Next, fit the encoder to the",
      "offset": 15900.6,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "data. Check how many category levels",
      "offset": 15903.68,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "have been",
      "offset": 15906.479,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "identified. Six. As",
      "offset": 15907.8,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "expected, now that the encoder is set",
      "offset": 15910.84,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "up, it can be applied to the data by",
      "offset": 15913.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "calling the transform method. Let's take",
      "offset": 15915.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "a look at the results. There's now a",
      "offset": 15918.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "type dummy column which captures the",
      "offset": 15921.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "dummy variables. As mentioned earlier,",
      "offset": 15924,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the final level is treated differently.",
      "offset": 15927.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "No column is assigned to type van",
      "offset": 15929.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "because if a vehicle isn't one of the",
      "offset": 15932.479,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "other types then it must be a van. To",
      "offset": 15934.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "have a separate dummy variable for van",
      "offset": 15937.84,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "would be",
      "offset": 15940.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "redundant. The sparse format used to",
      "offset": 15941.479,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "represent dummy variables looks a little",
      "offset": 15944.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "complicated. Let's take a moment to dig",
      "offset": 15947.239,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "into dense versus sparse",
      "offset": 15949.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "formats. Suppose that you want to store",
      "offset": 15952.84,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "a vector which consists mostly of zer.",
      "offset": 15955.199,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "You could store it as a dense vector in",
      "offset": 15959.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "which each of the elements of the vector",
      "offset": 15961.68,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "is stored explicitly. This is wasteful",
      "offset": 15964.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "though because most of those elements",
      "offset": 15967.199,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "are",
      "offset": 15969.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "zeros. A sparse representation is a much",
      "offset": 15970.04,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "better alternative. To create a sparse",
      "offset": 15973.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "vector, you need to specify the size of",
      "offset": 15976.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the vector in this case 8. The positions",
      "offset": 15979.279,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "which are non zero in this case",
      "offset": 15982.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "positions 0 and five. Noting that we",
      "offset": 15984.84,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "start counting at zero and the values",
      "offset": 15988.159,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "for each of those positions one and",
      "offset": 15991.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "seven. Sparse representation is",
      "offset": 15994.439,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "essential for effective one hot encoding",
      "offset": 15997.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "on large data sets. Let's try out one",
      "offset": 15999.52,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "hot encoding on the flights",
      "offset": 16002.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "data. In the previous lesson, you",
      "offset": 16007.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "learned how to one hot encode",
      "offset": 16010.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "categorical features which is essential",
      "offset": 16012.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for building regression models.",
      "offset": 16014.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "In this lesson, you'll find out how to",
      "offset": 16017.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "build a regression model to predict",
      "offset": 16019.279,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "numerical",
      "offset": 16021.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "values. Returning to the car's data,",
      "offset": 16022.439,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "suppose you wanted to predict fuel",
      "offset": 16025.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "consumption using vehicle mass. A",
      "offset": 16027.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "scatter plot is a good way to visualize",
      "offset": 16030.159,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "the relationship between those two",
      "offset": 16032.08,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "variables. Only a subset of the data are",
      "offset": 16034.76,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "included in this plot, but it's clear",
      "offset": 16037.439,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "that consumption increases with mass.",
      "offset": 16039.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "However, the relationship is not",
      "offset": 16042.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "perfectly linear. There's scatter for",
      "offset": 16045.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "individual points. A model should",
      "offset": 16047.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "describe the average relationship of",
      "offset": 16050.479,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "consumption to mass without necessarily",
      "offset": 16052.56,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "passing through individual",
      "offset": 16055.199,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "points. This line, for example, might",
      "offset": 16057.479,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "describe the underlying trend in the",
      "offset": 16060.479,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "data. But there are other lines which",
      "offset": 16063.08,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "could equally well describe that trend.",
      "offset": 16065.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "How do you choose the line which best",
      "offset": 16068.72,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "describes the",
      "offset": 16070.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "relationship? First, we need to define",
      "offset": 16072.359,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "the concept of residuals. The residual",
      "offset": 16074.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is the difference between the observed",
      "offset": 16078,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "value and the corresponding model value.",
      "offset": 16079.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "The residuals are indicated in the plot",
      "offset": 16082.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "as the vertical lines between the data",
      "offset": 16085.12,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "points and the model",
      "offset": 16087.359,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "line. The best model would somehow make",
      "offset": 16089.64,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "these residuals as small as",
      "offset": 16092.479,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "possible. Out of all possible models,",
      "offset": 16095.399,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "the best model is found by minimizing a",
      "offset": 16098,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "loss function, which is an equation that",
      "offset": 16100.8,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "describes how well the model fits the",
      "offset": 16103.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "data. This is the equation for the mean",
      "offset": 16105.479,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "squared error loss function. Let's",
      "offset": 16108.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "quickly break it down. You've got the",
      "offset": 16110.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "observed values y sub i and the model",
      "offset": 16113.12,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "values y subi. The difference between",
      "offset": 16116.159,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "these is the residual.",
      "offset": 16119.439,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "The residuals are squared and then",
      "offset": 16121.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "summed together before finally dividing",
      "offset": 16124.159,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "through by the number of data points to",
      "offset": 16127.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "give the mean or",
      "offset": 16129.359,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "average. By minimizing the loss",
      "offset": 16131,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "function, you're effectively minimizing",
      "offset": 16133.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "the average residual or the average",
      "offset": 16135.84,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "distance between the observed and",
      "offset": 16138.8,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "modeled",
      "offset": 16141.199,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "values. If this looks a little",
      "offset": 16142.359,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "complicated, don't worry. Spark will do",
      "offset": 16144.479,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "all of the maths for you.",
      "offset": 16147.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Let's build a regression model to",
      "offset": 16150.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "predict fuel consumption using three",
      "offset": 16152.72,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "predictors. Mass, number of cylinders,",
      "offset": 16155.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "and vehicle type where the last is a",
      "offset": 16158.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "categorical which we've already one",
      "offset": 16161.04,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "household",
      "offset": 16162.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "encoded. As before, the first step",
      "offset": 16163.64,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "towards building a model is to take our",
      "offset": 16166.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "predictors and assemble them into a",
      "offset": 16168.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "single column called",
      "offset": 16171.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "features. The data are then randomly",
      "offset": 16172.84,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "split into training and testing sets.",
      "offset": 16175.439,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "The model is created using the linear",
      "offset": 16178.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "regression class which is imported from",
      "offset": 16181.279,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "the regression",
      "offset": 16183.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "module. By default, this class expects",
      "offset": 16185.64,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "to find the target data in a column",
      "offset": 16188.8,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "called",
      "offset": 16191.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "label. Since you are aiming to predict",
      "offset": 16192.279,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "the consumption column, you need to",
      "offset": 16194.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "explicitly specify the name of the label",
      "offset": 16196.88,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "column when creating a regression",
      "offset": 16199.439,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "object. Next, train the model on the",
      "offset": 16202.199,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "training data using the fit method.",
      "offset": 16204.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "The trained model can then be used to",
      "offset": 16207.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "make predictions on the testing data",
      "offset": 16210.319,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "using the transform",
      "offset": 16212.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "method. Comparing the predicted values",
      "offset": 16215.159,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "to the known values from the testing",
      "offset": 16217.6,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "data, you'll see that there is",
      "offset": 16219.359,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "reasonable",
      "offset": 16221.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "agreement. It's hard to tell from a",
      "offset": 16222.68,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "table though. A plot gives a clearer",
      "offset": 16224.96,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "picture. The dashed diagonal line",
      "offset": 16228.359,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "represents perfect prediction.",
      "offset": 16231.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Most of the points lie close to this",
      "offset": 16234.239,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "line which is",
      "offset": 16236.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "good. It's useful to have a single",
      "offset": 16237.479,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "number which summarizes the performance",
      "offset": 16239.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "of a model for classifiers. There are a",
      "offset": 16241.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "variety of such metrics. The root mean",
      "offset": 16245.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "squared error is often used for",
      "offset": 16247.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "regression models. It's the square root",
      "offset": 16249.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "of the mean squared error which you've",
      "offset": 16252.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "already encountered and it corresponds",
      "offset": 16255.279,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to the standard deviation of the",
      "offset": 16257.6,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "residuals.",
      "offset": 16259.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "The metrics for a classifier like",
      "offset": 16261.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "accuracy, precision, and recall are",
      "offset": 16263.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "measured on an absolute scale where it's",
      "offset": 16266.479,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "possible to immediately identify values",
      "offset": 16268.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that are good or bad. Values of root",
      "offset": 16271.199,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "mean squared error are relative to the",
      "offset": 16274.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "scale of a value that you're aiming to",
      "offset": 16276.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "predict. So interpretation is a little",
      "offset": 16278.64,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "more",
      "offset": 16281.199,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "challenging. A smaller root mean squared",
      "offset": 16282.12,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "error, however, always indicates better",
      "offset": 16284.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "predictions.",
      "offset": 16287.84,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "Let's examine the",
      "offset": 16289.439,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "model. The intercept is the value",
      "offset": 16291,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "predicted by the model when all",
      "offset": 16293.279,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "predictors are",
      "offset": 16295.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "zero. On the plot, this is the point",
      "offset": 16296.359,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "where the model line intersects the",
      "offset": 16299.04,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "vertical dash",
      "offset": 16300.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "line. You can find this value for the",
      "offset": 16302.68,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "model using the intercept attribute.",
      "offset": 16305.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "This is the predicted fuel consumption",
      "offset": 16307.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "when both mass and number of cylinders",
      "offset": 16309.68,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "are zero and the vehicle type is",
      "offset": 16312.319,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "van. Of course, this is an entirely",
      "offset": 16315.399,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "hypothetical scenario. No vehicle could",
      "offset": 16318.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "have zero",
      "offset": 16321.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "mass. There's a slope associated with",
      "offset": 16322.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "each of the predictors, too, which",
      "offset": 16325.84,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "represents how rapidly the model changes",
      "offset": 16328,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "when that predictor changes. The",
      "offset": 16330.479,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "coefficients attribute gives you access",
      "offset": 16333.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to those values. There's a coefficient",
      "offset": 16335.84,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "for each of the predictors. The",
      "offset": 16338.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "coefficients for mass and number of",
      "offset": 16341.359,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "cylinders are positive, indicating that",
      "offset": 16343.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "heavier cars with more cylinders consume",
      "offset": 16345.6,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "more",
      "offset": 16348.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "fuel. These coefficients also represent",
      "offset": 16349.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "the rate of change for the corresponding",
      "offset": 16352.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "predictor. For example, the coefficient",
      "offset": 16355.319,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "for mass indicates the change in fuel",
      "offset": 16358.08,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "consumption when mass increases by one",
      "offset": 16360.8,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "unit. Remember that there's no dummy",
      "offset": 16364.76,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "variable for van. The coefficients for",
      "offset": 16367.199,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "the type dummy variables are relative to",
      "offset": 16370.239,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "vans. These coefficients should also be",
      "offset": 16373,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "interpreted with care. If you are going",
      "offset": 16376.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "to compare the values for different",
      "offset": 16378.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "vehicle types, then this needs to be",
      "offset": 16380.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "done for fixed mass and number of",
      "offset": 16382.72,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "cylinders.",
      "offset": 16385.119,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Since all of the type dummy coefficients",
      "offset": 16386.879,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "are negative, the model indicates that",
      "offset": 16389.199,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "for a specific mass and number of",
      "offset": 16391.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "cylinders, all other vehicle types",
      "offset": 16393.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "consume less fuel than a",
      "offset": 16396.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "van. Large vehicles have the most",
      "offset": 16398.92,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "negative coefficient. So, it's possible",
      "offset": 16401.84,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "to say that for a specific mass and",
      "offset": 16404.639,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "number of cylinders, large vehicles are",
      "offset": 16406.959,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "the most fuel",
      "offset": 16409.439,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "efficient. You've covered a lot of",
      "offset": 16411,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "ground in this lesson. Let's apply what",
      "offset": 16412.879,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "you've learned to the flights",
      "offset": 16415.76,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "data. The largest improvements in",
      "offset": 16420.68,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "machine learning model performance are",
      "offset": 16423.199,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "often achieved by carefully manipulating",
      "offset": 16425.359,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "features. In this lesson, you'll be",
      "offset": 16428.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "learning about a few approaches to doing",
      "offset": 16431.119,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this. Let's start with bucketing. It's",
      "offset": 16433.24,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "often convenient to convert a continuous",
      "offset": 16436.639,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "variable like age or height into",
      "offset": 16439.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "discrete values.",
      "offset": 16442.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "This can be done by assigning values to",
      "offset": 16444.24,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "buckets or bins with well-defined",
      "offset": 16446.639,
      "duration": 5.922
    },
    {
      "lang": "en",
      "text": "boundaries. The buckets might have",
      "offset": 16450.039,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "uniform or variable width. Let's make",
      "offset": 16452.561,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "this more concrete by thinking about",
      "offset": 16455.52,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "observations of people's",
      "offset": 16457.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "heights. If you plot the heights on a",
      "offset": 16459.799,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "histogram, then it seems reasonable to",
      "offset": 16462.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "divide the heights up into ranges. To",
      "offset": 16465.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "each of these ranges, you assign a",
      "offset": 16468.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "label. Then you create a new column in",
      "offset": 16470.4,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "the data with the appropriate labels.",
      "offset": 16473.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "The resulting categorical variable is",
      "offset": 16477.279,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "often a more powerful predictor than the",
      "offset": 16479.6,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "original continuous",
      "offset": 16482.4,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "variable. Let's apply this to the car's",
      "offset": 16484.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "data. Looking at the distribution of",
      "offset": 16487.24,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "values for RPM, you see that the",
      "offset": 16489.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "majority lie in the range between 4500",
      "offset": 16492.24,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 16495.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "6,000. There are a few either below or",
      "offset": 16496.279,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "above this range.",
      "offset": 16499.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "This suggests that it would make sense",
      "offset": 16501.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to bucket these values according to",
      "offset": 16503.76,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "those",
      "offset": 16505.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "boundaries. You create a bucketizer",
      "offset": 16507.639,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "object specifying the bin boundaries as",
      "offset": 16510.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the splits argument and also providing",
      "offset": 16512.879,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "the names of the input and output",
      "offset": 16515.359,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "columns. You then apply this object to",
      "offset": 16518.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the data by calling the transform",
      "offset": 16521.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "method. The result has a new column with",
      "offset": 16524.6,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "the discrete bucket values.",
      "offset": 16527.279,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "The three buckets have been assigned",
      "offset": 16530.639,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "indexed values zero, one and two",
      "offset": 16532.879,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "corresponding to the low, medium, and",
      "offset": 16536.4,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "high ranges for",
      "offset": 16538.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "RPM. As you saw earlier, before you can",
      "offset": 16540.279,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "use these index values in a regression",
      "offset": 16543.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "model, they first need to be one hot",
      "offset": 16546,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "encoded.",
      "offset": 16548.639,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "The low and medium RPM ranges are mapped",
      "offset": 16550.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "to a distinct dummy variable while the",
      "offset": 16553.359,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "high range is the reference level and",
      "offset": 16556.48,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "does not get a separate dummy",
      "offset": 16558.879,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "variable. Let's look at the intercept",
      "offset": 16561.16,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "and coefficients for a model which",
      "offset": 16563.359,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "predicts fuel consumption based on",
      "offset": 16565.199,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "bucketed RPM data. The intercept tells",
      "offset": 16568.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "us what the fuel consumption is for the",
      "offset": 16572,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "reference level which is the high RPM",
      "offset": 16574.48,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "bucket. To get the consumption for the",
      "offset": 16577.279,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "low RPM bucket, you add the first",
      "offset": 16580.561,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "coefficient to the",
      "offset": 16583.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "intercept. Similarly, to find the",
      "offset": 16585.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "consumption for the medium RPM bucket,",
      "offset": 16587.92,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "you add the second coefficient to the",
      "offset": 16590.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "intercept. There are many other",
      "offset": 16594.359,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "approaches to engineering new features.",
      "offset": 16596.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "It's common to apply arithmetic",
      "offset": 16598.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "operations to one or more columns to",
      "offset": 16600.799,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "create new features.",
      "offset": 16603.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Returning to the heights data, suppose",
      "offset": 16605.439,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that we also had data for mass. Then it",
      "offset": 16607.92,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "might be perfectly reasonable to",
      "offset": 16611.439,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "engineer a new column for BMI.",
      "offset": 16613.039,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Potentially BMI might be a more powerful",
      "offset": 16616.199,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "predictor than either height or mass in",
      "offset": 16619.199,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "isolation. Let's apply this idea to the",
      "offset": 16623.32,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "car's data. You have columns for mass",
      "offset": 16625.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and length. Perhaps some combination of",
      "offset": 16628.799,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "the two might be even more meaningful.",
      "offset": 16632,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "You can create different forms of",
      "offset": 16635.52,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "density by dividing the mass through by",
      "offset": 16637.439,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "the first three powers of length. Since",
      "offset": 16640.561,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "you only have the length of the vehicle",
      "offset": 16643.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but not their width or height, the",
      "offset": 16645.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "length has been used as a proxy for",
      "offset": 16648.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "these missing",
      "offset": 16650.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "dimensions. In so doing, you create",
      "offset": 16652.92,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "three new predictors. The first density",
      "offset": 16655.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "represents how mass changes with vehicle",
      "offset": 16658.719,
      "duration": 5.842
    },
    {
      "lang": "en",
      "text": "length. The second and third densities",
      "offset": 16661.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "approximate how mass varies with the",
      "offset": 16664.561,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "area and volume of the",
      "offset": 16667.039,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "vehicle. Which of these will be",
      "offset": 16669.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "meaningful for our",
      "offset": 16671.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "model? Right now, you don't know. You're",
      "offset": 16672.92,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "just trying things out. Powerful new",
      "offset": 16675.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "features are often discovered through",
      "offset": 16678.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "trial and error.",
      "offset": 16680.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "In the next lesson, you'll learn about a",
      "offset": 16682.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "technique for selecting only the",
      "offset": 16684.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "relevant predictors in a regression",
      "offset": 16686.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "model. Right now though, let's apply",
      "offset": 16688.52,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "what you've learned to the flights",
      "offset": 16691.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "data. The regression models that you've",
      "offset": 16696.039,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "built up until now have blindly included",
      "offset": 16698.639,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "all of the provided features. Next,",
      "offset": 16701.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you're going to learn about a more",
      "offset": 16704.799,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "sophisticated model which effectively",
      "offset": 16706.24,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "selects only the most useful",
      "offset": 16708.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "features. A linear regression model",
      "offset": 16711.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "attempts to derive a coefficient for",
      "offset": 16714,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "each feature in the data. The",
      "offset": 16716.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "coefficients quantify the effects of the",
      "offset": 16718.561,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "corresponding features. More features",
      "offset": 16721.359,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "imply more",
      "offset": 16724.48,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "coefficients. This works well when your",
      "offset": 16726.279,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "data set has a few columns and many",
      "offset": 16728.879,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "rows. You need to derive a few",
      "offset": 16731.52,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "coefficients and you have plenty of",
      "offset": 16734.08,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "data. The converse situation, many",
      "offset": 16736.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "columns and few rows is much more",
      "offset": 16740.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "challenging. Now you need to calculate",
      "offset": 16743.24,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "values for numerous coefficients, but",
      "offset": 16746,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "you don't have much data to do it. Even",
      "offset": 16748.561,
      "duration": 6.318
    },
    {
      "lang": "en",
      "text": "if you do manage to derive values for",
      "offset": 16752.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "all of those coefficients, your model",
      "offset": 16754.879,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "will end up being very complicated and",
      "offset": 16757.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "difficult to interpret.",
      "offset": 16760.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Ideally, you want to create a",
      "offset": 16762.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "parimonious model, one that has just the",
      "offset": 16764.16,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "minimum required number of predictors.",
      "offset": 16767.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "It will be as simple as possible yet",
      "offset": 16770.561,
      "duration": 5.638
    },
    {
      "lang": "en",
      "text": "still able to make robust",
      "offset": 16773.199,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "predictions. The obvious solution is to",
      "offset": 16776.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "simply select the best subset of",
      "offset": 16778.638,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "columns. But how to choose that subset?",
      "offset": 16781.24,
      "duration": 5.558
    },
    {
      "lang": "en",
      "text": "There are a variety of approaches to",
      "offset": 16784.798,
      "duration": 5.282
    },
    {
      "lang": "en",
      "text": "this feature selection problem. In this",
      "offset": 16786.798,
      "duration": 5.762
    },
    {
      "lang": "en",
      "text": "lesson, we'll be exploring one such",
      "offset": 16790.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "approach to feature selection known as",
      "offset": 16792.56,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "penalized",
      "offset": 16795.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "regression. The basic idea is that the",
      "offset": 16796.6,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "model is penalized or punished for",
      "offset": 16799.68,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "having too many",
      "offset": 16802.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "coefficients. Recall that the",
      "offset": 16805.16,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "conventional regression algorithm",
      "offset": 16807.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "chooses coefficients to minimize the",
      "offset": 16809.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "loss function, which is the average of",
      "offset": 16811.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the squared residuals.",
      "offset": 16813.28,
      "duration": 5.358
    },
    {
      "lang": "en",
      "text": "A good model will result in low mean",
      "offset": 16815.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "squared error because its predictions",
      "offset": 16818.638,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "will be close to the observed",
      "offset": 16821.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "values. With penalized regression, an",
      "offset": 16823.638,
      "duration": 5.402
    },
    {
      "lang": "en",
      "text": "additional regularization or shrinkage",
      "offset": 16826.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "term is added to the loss function",
      "offset": 16829.04,
      "duration": 5.838
    },
    {
      "lang": "en",
      "text": "rather than depending on the data. This",
      "offset": 16832.24,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "term is a function of the model",
      "offset": 16834.878,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "coefficients. There are two standard",
      "offset": 16837.48,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "forms for the regularization term. Lasso",
      "offset": 16839.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "regression uses a term which is",
      "offset": 16843.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "proportional to the absolute value of",
      "offset": 16845.36,
      "duration": 5.358
    },
    {
      "lang": "en",
      "text": "the coefficients while ridge regression",
      "offset": 16847.36,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "uses the square of the",
      "offset": 16850.718,
      "duration": 5.762
    },
    {
      "lang": "en",
      "text": "coefficients. In both cases, this extra",
      "offset": 16853.08,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "term in the loss function penalizes",
      "offset": 16856.48,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "models with too many",
      "offset": 16859.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "coefficients. There's a subtle",
      "offset": 16862.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "distinction between lesso and ridge",
      "offset": 16864.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "regression. Both will shrink the",
      "offset": 16866.52,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "coefficients of unimportant predictors.",
      "offset": 16869.04,
      "duration": 5.678
    },
    {
      "lang": "en",
      "text": "However, whereas ridge will result in",
      "offset": 16871.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "those coefficients being close to zero,",
      "offset": 16874.718,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "lasso will actually force them to zero",
      "offset": 16877.84,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "precisely. It's also possible to have a",
      "offset": 16881.878,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "mix of lasso and",
      "offset": 16884.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "ridge. The strength of the",
      "offset": 16886.44,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "regularization is determined by a",
      "offset": 16888.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "parameter which is generally denoted by",
      "offset": 16890.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the Greek symbol lambda. When lambda is",
      "offset": 16892.48,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "zero, there is no",
      "offset": 16895.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "regularization. And when lambda is",
      "offset": 16898.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "large, regularization completely",
      "offset": 16900.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "dominates. Ideally, you want to choose a",
      "offset": 16903.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "value for lambda between these two",
      "offset": 16905.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "extremes. Let's make this more concrete",
      "offset": 16908.84,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "by returning to the car's data. We've",
      "offset": 16911.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "assembled the mass, cylinders, and type",
      "offset": 16914.16,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "columns along with the freshly",
      "offset": 16916.56,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "engineered density",
      "offset": 16918.878,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "columns. We've effectively got 10",
      "offset": 16920.68,
      "duration": 6.118
    },
    {
      "lang": "en",
      "text": "predictors available for the model. As",
      "offset": 16923.6,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "usual, we'll split these data into",
      "offset": 16926.798,
      "duration": 4.042
    },
    {
      "lang": "en",
      "text": "training and testing",
      "offset": 16928.878,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "sets. Let's start by fitting a standard",
      "offset": 16930.84,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "linear regression model to the training",
      "offset": 16933.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "data. You can then make predictions on",
      "offset": 16935.638,
      "duration": 4.762
    },
    {
      "lang": "en",
      "text": "the testing data and calculate the root",
      "offset": 16938.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "mean squared error. When you look at the",
      "offset": 16940.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model coefficients, you find that all",
      "offset": 16943.28,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "predictors have been assigned nonzero",
      "offset": 16945.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "values. This means that every predictor",
      "offset": 16948.44,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "is contributing to the model. This is",
      "offset": 16951.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "certainly possible, but it's unlikely",
      "offset": 16954,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that all of the features are equally",
      "offset": 16956.24,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "important for predicting",
      "offset": 16958.08,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "consumption. Now, let's fit a ridge",
      "offset": 16960.2,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "regression model to the same data. You",
      "offset": 16962.958,
      "duration": 5.042
    },
    {
      "lang": "en",
      "text": "get a ridge regression model by giving a",
      "offset": 16965.76,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "value of zero for the elastic net param",
      "offset": 16968,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "parameter. An arbitrary value of 0.1 has",
      "offset": 16972.2,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "been chosen for the regularization",
      "offset": 16975.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "strength. Later you'll learn a way to",
      "offset": 16977.6,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "choose good values for this parameter",
      "offset": 16980.56,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "based on the",
      "offset": 16982.878,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "data. When you calculate the root mean",
      "offset": 16984.28,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "squared error on the testing data you",
      "offset": 16986.798,
      "duration": 5.682
    },
    {
      "lang": "en",
      "text": "find that it has increased slightly but",
      "offset": 16989.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "not enough to cause",
      "offset": 16992.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "concern. Looking at the coefficients you",
      "offset": 16994.12,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "see that they are all smaller than the",
      "offset": 16997.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "coefficients for the standard linear",
      "offset": 16999.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "regression model. They have been shrunk.",
      "offset": 17001.12,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "Finally, let's build a lasso regression",
      "offset": 17005.6,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "model by setting elastic net param to",
      "offset": 17008.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "one. Again, you find that the testing",
      "offset": 17011.4,
      "duration": 5.478
    },
    {
      "lang": "en",
      "text": "root mean error has increased, but not",
      "offset": 17014.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "by a significant",
      "offset": 17016.878,
      "duration": 4.322
    },
    {
      "lang": "en",
      "text": "degree. Turning to the coefficients",
      "offset": 17018.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "though, you see that something important",
      "offset": 17021.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "has happened. All but two of the",
      "offset": 17023.52,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "coefficients are now",
      "offset": 17026.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "zero. There are effectively only two",
      "offset": 17028.6,
      "duration": 6.038
    },
    {
      "lang": "en",
      "text": "predictors left in the model. the dummy",
      "offset": 17031.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "variable for a small type car and the",
      "offset": 17034.638,
      "duration": 3.882
    },
    {
      "lang": "en",
      "text": "linear",
      "offset": 17037.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "density. Lesso regression has identified",
      "offset": 17038.52,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "the most important predictors and set",
      "offset": 17041.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "the coefficients for the rest to",
      "offset": 17044,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "zero. This tells us that we can get a",
      "offset": 17046.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "good model by simply knowing whether or",
      "offset": 17049.44,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "not a car is small and its linear",
      "offset": 17051.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "density. A simpler model with no",
      "offset": 17054.92,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "significant loss in performance.",
      "offset": 17057.28,
      "duration": 5.358
    },
    {
      "lang": "en",
      "text": "Let's try out regularization on our",
      "offset": 17060.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "flight duration",
      "offset": 17062.638,
      "duration": 3.042
    },
    {
      "lang": "en",
      "text": "model. Welcome back. So far, you've",
      "offset": 17066.6,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "learned how to build classifier and",
      "offset": 17070.08,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "regression models using",
      "offset": 17072.16,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "Spark. In this chapter, you'll learn how",
      "offset": 17074.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to make those models",
      "offset": 17076.878,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "better. You'll start by taking a look at",
      "offset": 17079,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "pipelines, which will seriously",
      "offset": 17081.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "streamline your workflow. They will also",
      "offset": 17083.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "help to ensure that training and testing",
      "offset": 17086.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "data are treated consistently and that",
      "offset": 17089.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "no leakage of information between these",
      "offset": 17091.84,
      "duration": 4.118
    },
    {
      "lang": "en",
      "text": "two sets takes",
      "offset": 17094.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "place. What do I mean by leakage? Most",
      "offset": 17095.958,
      "duration": 5.802
    },
    {
      "lang": "en",
      "text": "of the actions you've been using involve",
      "offset": 17099.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "both a fit and a transform method. Those",
      "offset": 17101.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "methods have been applied in a fairly",
      "offset": 17105.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "relaxed way. But to get really robust",
      "offset": 17107.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "results, you need to be careful only to",
      "offset": 17111.28,
      "duration": 6.598
    },
    {
      "lang": "en",
      "text": "apply the fit method to training data.",
      "offset": 17114,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "Why? Because if a fit method is applied",
      "offset": 17117.878,
      "duration": 5.882
    },
    {
      "lang": "en",
      "text": "to any of the testing data, then the",
      "offset": 17121.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "model will effectively have seen those",
      "offset": 17123.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "data during the training",
      "offset": 17126.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "phase. So the results of testing will no",
      "offset": 17128.12,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "longer be objective. The transform",
      "offset": 17131.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "method on the other hand can be applied",
      "offset": 17133.84,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "to both training and testing data since",
      "offset": 17135.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it does not result in any changes in the",
      "offset": 17138.718,
      "duration": 3.562
    },
    {
      "lang": "en",
      "text": "underlying",
      "offset": 17141.04,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "model. A figure should make this",
      "offset": 17142.28,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "clearer. Leakage occurs whenever a fit",
      "offset": 17144.84,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "method is applied to testing data.",
      "offset": 17147.68,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "Suppose that you fit a model using both",
      "offset": 17150.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the training and testing data. The model",
      "offset": 17152.638,
      "duration": 5.842
    },
    {
      "lang": "en",
      "text": "will then already have seen the testing",
      "offset": 17156,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "data. So using those data to test the",
      "offset": 17158.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "model would not be fair. Of course, the",
      "offset": 17161.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model will perform well on data which",
      "offset": 17163.84,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "has been used for",
      "offset": 17166.16,
      "duration": 4.558
    },
    {
      "lang": "en",
      "text": "training. This sounds obvious, but care",
      "offset": 17168.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "must be taken not to fall into this",
      "offset": 17170.718,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "trap. Remember that there are normally",
      "offset": 17172.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "multiple stages in building a model and",
      "offset": 17175.44,
      "duration": 5.198
    },
    {
      "lang": "en",
      "text": "if the fit method in any of those stages",
      "offset": 17178,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "is applied to the testing data, then the",
      "offset": 17180.638,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "model is compromised.",
      "offset": 17183.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "However, if you are careful to only",
      "offset": 17185.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "apply fit to the training data, then",
      "offset": 17187.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "your model will be in good shape. When",
      "offset": 17189.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it comes to testing, it will not have",
      "offset": 17192.16,
      "duration": 4.478
    },
    {
      "lang": "en",
      "text": "seen any of the testing data and the",
      "offset": 17194.24,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "test results will be completely",
      "offset": 17196.638,
      "duration": 5.842
    },
    {
      "lang": "en",
      "text": "objective. Luckily, a pipeline will make",
      "offset": 17199.56,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "it easier to avoid leakage because it",
      "offset": 17202.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "simplifies the training and testing",
      "offset": 17205.2,
      "duration": 3.438
    },
    {
      "lang": "en",
      "text": "process.",
      "offset": 17207.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "A pipeline is a mechanism to combine a",
      "offset": 17208.638,
      "duration": 5.602
    },
    {
      "lang": "en",
      "text": "series of steps. Rather than applying",
      "offset": 17211.12,
      "duration": 5.758
    },
    {
      "lang": "en",
      "text": "each of the steps individually, they are",
      "offset": 17214.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "all grouped together and applied as a",
      "offset": 17216.878,
      "duration": 3.802
    },
    {
      "lang": "en",
      "text": "single",
      "offset": 17219.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "unit. Let's return to our car's",
      "offset": 17220.68,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "regression model. Recall that there were",
      "offset": 17223.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "a number of steps involved. Using a",
      "offset": 17226.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "string indexer to convert the type",
      "offset": 17229.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "column to indexed values. applying a",
      "offset": 17231.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "oneh hot encoder to convert those",
      "offset": 17234,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "indexed values into dummy variables.",
      "offset": 17236,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Then assembling a set of predictors into",
      "offset": 17239.36,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "a single features column and finally",
      "offset": 17241.84,
      "duration": 5.798
    },
    {
      "lang": "en",
      "text": "building a regression",
      "offset": 17245.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "model. Let's map out the process of",
      "offset": 17247.638,
      "duration": 5.482
    },
    {
      "lang": "en",
      "text": "applying those steps. First, you fit the",
      "offset": 17250.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "indexer to the training data. Then you",
      "offset": 17253.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "call the transform method on the",
      "offset": 17255.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "training data to add the indexed column.",
      "offset": 17257.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Then you call the transform method on",
      "offset": 17260.56,
      "duration": 4.398
    },
    {
      "lang": "en",
      "text": "the testing data to add the indexed",
      "offset": 17262.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "column there too. Note that the testing",
      "offset": 17264.958,
      "duration": 6.802
    },
    {
      "lang": "en",
      "text": "data was not used to fit the indexer.",
      "offset": 17268.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Next, you do the same things for the one",
      "offset": 17271.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "hot encoder, fitting the training data",
      "offset": 17274.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and then using the fitted encoder to",
      "offset": 17277.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "update the training and testing data",
      "offset": 17279.04,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "sets. The assembler is next. In this",
      "offset": 17282.12,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "case, there is no fit method. So you",
      "offset": 17285.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "simply apply the transform method to the",
      "offset": 17287.84,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "training and testing",
      "offset": 17290.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "data. Finally, the data are ready. You",
      "offset": 17292.52,
      "duration": 5.358
    },
    {
      "lang": "en",
      "text": "fit the regression model to the training",
      "offset": 17295.6,
      "duration": 5.198
    },
    {
      "lang": "en",
      "text": "data and then use the model to make",
      "offset": 17297.878,
      "duration": 5.602
    },
    {
      "lang": "en",
      "text": "predictions on the testing",
      "offset": 17300.798,
      "duration": 5.522
    },
    {
      "lang": "en",
      "text": "data. Throughout the process, you've",
      "offset": 17303.48,
      "duration": 5.158
    },
    {
      "lang": "en",
      "text": "been careful to keep the testing data",
      "offset": 17306.32,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "out of the training",
      "offset": 17308.638,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "process. But this is hard work and it's",
      "offset": 17310.6,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "easy enough to slip up. A pipeline makes",
      "offset": 17313.44,
      "duration": 5.438
    },
    {
      "lang": "en",
      "text": "training and testing a complicated model",
      "offset": 17316.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "a lot easier. The pipeline class lives",
      "offset": 17318.878,
      "duration": 6.642
    },
    {
      "lang": "en",
      "text": "in the ML subm module. You create a",
      "offset": 17322.4,
      "duration": 5.558
    },
    {
      "lang": "en",
      "text": "pipeline by specifying a sequence of",
      "offset": 17325.52,
      "duration": 5.438
    },
    {
      "lang": "en",
      "text": "stages where each stage corresponds to a",
      "offset": 17327.958,
      "duration": 5.362
    },
    {
      "lang": "en",
      "text": "step in the model building",
      "offset": 17330.958,
      "duration": 5.602
    },
    {
      "lang": "en",
      "text": "process. The stages are executed in",
      "offset": 17333.32,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "order. Now rather than calling the fit",
      "offset": 17336.56,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and transform methods for each stage,",
      "offset": 17340,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you simply call the fit method for the",
      "offset": 17342.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "pipeline on the training",
      "offset": 17344.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "data. Each of the stages in the pipeline",
      "offset": 17346.92,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "is then automatically applied to the",
      "offset": 17350,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "training data in turn. This will",
      "offset": 17352.32,
      "duration": 4.318
    },
    {
      "lang": "en",
      "text": "systematically apply the fit and",
      "offset": 17354.798,
      "duration": 4.322
    },
    {
      "lang": "en",
      "text": "transform methods for each stage in the",
      "offset": 17356.638,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "pipeline.",
      "offset": 17359.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "The train pipeline can then be used to",
      "offset": 17360.798,
      "duration": 5.202
    },
    {
      "lang": "en",
      "text": "make predictions on the testing data by",
      "offset": 17363.04,
      "duration": 5.678
    },
    {
      "lang": "en",
      "text": "calling its transform method. The",
      "offset": 17366,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "pipeline transform method will only call",
      "offset": 17368.718,
      "duration": 4.402
    },
    {
      "lang": "en",
      "text": "the transform method for each of the",
      "offset": 17371.28,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "stages in the pipeline. Isn't that",
      "offset": 17373.12,
      "duration": 5.758
    },
    {
      "lang": "en",
      "text": "simple? You can access the stages in the",
      "offset": 17376.2,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "pipeline by using the stages attribute",
      "offset": 17378.878,
      "duration": 6.162
    },
    {
      "lang": "en",
      "text": "which is a list. You pick out individual",
      "offset": 17381.84,
      "duration": 6.878
    },
    {
      "lang": "en",
      "text": "stages by indexing into the list.",
      "offset": 17385.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "For example, to access the regression",
      "offset": 17388.718,
      "duration": 5.362
    },
    {
      "lang": "en",
      "text": "component of the pipeline, you'd use an",
      "offset": 17391.52,
      "duration": 5.438
    },
    {
      "lang": "en",
      "text": "index of three. Having access to that",
      "offset": 17394.08,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "component makes it possible to get the",
      "offset": 17396.958,
      "duration": 4.082
    },
    {
      "lang": "en",
      "text": "intercept and coefficients for the",
      "offset": 17398.958,
      "duration": 4.362
    },
    {
      "lang": "en",
      "text": "trained linear regression",
      "offset": 17401.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "model. Pipelines make your code easier",
      "offset": 17403.32,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "to read and maintain. Let's try them out",
      "offset": 17406.24,
      "duration": 7.638
    },
    {
      "lang": "en",
      "text": "with our flights model.",
      "offset": 17410,
      "duration": 3.878
    },
    {
      "lang": "en",
      "text": "Up until now, you've been testing models",
      "offset": 17414.798,
      "duration": 5.282
    },
    {
      "lang": "en",
      "text": "using a rather simple technique.",
      "offset": 17417.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Randomly splitting the data into",
      "offset": 17420.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "training and testing sets, training the",
      "offset": 17421.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "model on the training data, and then",
      "offset": 17424.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "evaluating its performance on the",
      "offset": 17426.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "testing set. There's one major drawback",
      "offset": 17428.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "to this approach. You only get one",
      "offset": 17431.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "estimate of model performance. You would",
      "offset": 17434.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "have a more robust idea of how well a",
      "offset": 17437.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "model works if you were able to test it",
      "offset": 17439.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "multiple times.",
      "offset": 17442.16,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "This is precisely the idea behind cross",
      "offset": 17444.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "validation. You start out with the full",
      "offset": 17447.56,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "set of data. You still split these data",
      "offset": 17450.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "into a training set and a testing set.",
      "offset": 17453.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Remember that before splitting, it's",
      "offset": 17457.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "important to first randomize the data.",
      "offset": 17459.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "So the distributions in the training and",
      "offset": 17462.48,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "testing data are similar. You then split",
      "offset": 17465.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the training data into a number of",
      "offset": 17469.28,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "partitions or",
      "offset": 17471.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "folds. The number of folds normally",
      "offset": 17473.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "factors into the name of the technique.",
      "offset": 17475.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "For example, if you split into five",
      "offset": 17478.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "folds, then you talk about five-fold",
      "offset": 17480.56,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "cross",
      "offset": 17483.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "validation. Once the training data have",
      "offset": 17485.16,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "been split into folds, you can start",
      "offset": 17487.52,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "cross",
      "offset": 17490.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "validating. First, keep aside the data",
      "offset": 17491.32,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "in the first fold. Train a model on the",
      "offset": 17494.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "remaining four folds. Then evaluate that",
      "offset": 17497.04,
      "duration": 5.678
    },
    {
      "lang": "en",
      "text": "model on the data from the first fold.",
      "offset": 17499.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "This will give you the first value for",
      "offset": 17502.718,
      "duration": 5.842
    },
    {
      "lang": "en",
      "text": "the evaluation metric. Next, you move on",
      "offset": 17505.04,
      "duration": 5.598
    },
    {
      "lang": "en",
      "text": "to the second fold where the same",
      "offset": 17508.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "process is repeated. Data in the second",
      "offset": 17510.638,
      "duration": 5.522
    },
    {
      "lang": "en",
      "text": "fold are set aside for testing while the",
      "offset": 17513.52,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "remaining four folds are used to train a",
      "offset": 17516.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "model. That model is tested on the",
      "offset": 17518.638,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "second fold data yielding the second",
      "offset": 17522.16,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "value for the evaluation",
      "offset": 17524.878,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "metric. You repeat the process for the",
      "offset": 17526.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "remaining",
      "offset": 17529.84,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "folds. Each of the folds is used in turn",
      "offset": 17530.92,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "as testing data and you end up with as",
      "offset": 17534.878,
      "duration": 5.522
    },
    {
      "lang": "en",
      "text": "many values for the evaluation metric as",
      "offset": 17537.6,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "there are",
      "offset": 17540.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "folds. At this point, you're in a",
      "offset": 17541.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "position to calculate the average of the",
      "offset": 17544.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "evaluation metric over all the folds,",
      "offset": 17546.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "which is a much more robust measure of",
      "offset": 17549.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "model performance than a single",
      "offset": 17552.24,
      "duration": 5.398
    },
    {
      "lang": "en",
      "text": "value. Let's see how this works in",
      "offset": 17555,
      "duration": 5.718
    },
    {
      "lang": "en",
      "text": "practice. Remember the car's data? Of",
      "offset": 17557.638,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "course, you do. You're going to build a",
      "offset": 17560.718,
      "duration": 4.322
    },
    {
      "lang": "en",
      "text": "cross validated regression model to",
      "offset": 17562.878,
      "duration": 3.162
    },
    {
      "lang": "en",
      "text": "predict",
      "offset": 17565.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "consumption. Here are the first two",
      "offset": 17566.04,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "ingredients which you need to perform",
      "offset": 17568.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "cross validation.",
      "offset": 17570.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "an estimator, which builds the model and",
      "offset": 17572.16,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "is often a pipeline, and an evaluator,",
      "offset": 17574.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "which quantifies how well a model works",
      "offset": 17578.32,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "on testing",
      "offset": 17581.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "data. We've seen both of these a few",
      "offset": 17582.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "times",
      "offset": 17586,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "already. Now, the final ingredients,",
      "offset": 17587.32,
      "duration": 5.638
    },
    {
      "lang": "en",
      "text": "you'll need two new classes, cross",
      "offset": 17590.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "validator and param grid builder, both",
      "offset": 17592.958,
      "duration": 6.402
    },
    {
      "lang": "en",
      "text": "from the tuning subm module.",
      "offset": 17596.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "You'll create a parameter grid which",
      "offset": 17599.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you'll leave empty for the moment but",
      "offset": 17601.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we'll return to in detail during the",
      "offset": 17604,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "next",
      "offset": 17605.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "lesson. Finally, you have everything",
      "offset": 17607.32,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "required to create a cross validator",
      "offset": 17610.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "object, an estimator which is the linear",
      "offset": 17612.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "regression model, an empty grid of",
      "offset": 17615.04,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "parameters for the estimator and an",
      "offset": 17617.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "evaluator which will calculate the root",
      "offset": 17620.2,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "mean squared error.",
      "offset": 17622.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "You can optionally specify the number of",
      "offset": 17625.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "folds which defaults to three and a",
      "offset": 17627.36,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "random number seed for",
      "offset": 17630.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "repeatability. The cross validator has a",
      "offset": 17633.56,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "fit method which will apply the cross",
      "offset": 17636.24,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "validation procedure to the training",
      "offset": 17638.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "data. You can then look at the average",
      "offset": 17640.68,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "root mean squared error calculated",
      "offset": 17643.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "across all of the folds.",
      "offset": 17645.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "This is a more robust measure of model",
      "offset": 17648.24,
      "duration": 4.398
    },
    {
      "lang": "en",
      "text": "performance because it is based on",
      "offset": 17650.24,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "multiple trained test",
      "offset": 17652.638,
      "duration": 5.842
    },
    {
      "lang": "en",
      "text": "splits. Note that the average metric is",
      "offset": 17655.4,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "returned as a list. You'll see why in",
      "offset": 17658.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "the next",
      "offset": 17661.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "lesson. The trained cross validator",
      "offset": 17662.36,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "object acts just like any other model.",
      "offset": 17665.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "It has a transform method which can be",
      "offset": 17668,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "used to make predictions on new data. If",
      "offset": 17670.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "we evaluate the predictions on the",
      "offset": 17674,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "original testing data, then we find a",
      "offset": 17676.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "smaller value for the root mean squared",
      "offset": 17679.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "error than we obtained using cross",
      "offset": 17681.68,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "validation. This means that a simple",
      "offset": 17684.2,
      "duration": 4.678
    },
    {
      "lang": "en",
      "text": "train test split would have given an",
      "offset": 17686.638,
      "duration": 5.642
    },
    {
      "lang": "en",
      "text": "overly optimistic view on model",
      "offset": 17688.878,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "performance. Let's give cross validation",
      "offset": 17692.28,
      "duration": 7.518
    },
    {
      "lang": "en",
      "text": "a try on our flights model.",
      "offset": 17694.718,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "So far, you've been using the default",
      "offset": 17700.718,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "parameters for almost everything. You've",
      "offset": 17702.718,
      "duration": 5.522
    },
    {
      "lang": "en",
      "text": "built some decent models, but they could",
      "offset": 17705.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "probably be improved by choosing better",
      "offset": 17708.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "model",
      "offset": 17711.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "parameters. There is no universal best",
      "offset": 17712.44,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "set of parameters for a particular",
      "offset": 17715.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "model. The optimal choice of parameters",
      "offset": 17717.2,
      "duration": 6.598
    },
    {
      "lang": "en",
      "text": "will depend on the data and the modeling",
      "offset": 17719.84,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "goal. The idea is relatively simple. You",
      "offset": 17723.798,
      "duration": 6.042
    },
    {
      "lang": "en",
      "text": "build a selection of models, one for",
      "offset": 17727.36,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "each set of model parameters. Then you",
      "offset": 17729.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "evaluate those models and choose the",
      "offset": 17732.638,
      "duration": 5.282
    },
    {
      "lang": "en",
      "text": "best one. You'll be looking at the fuel",
      "offset": 17735.04,
      "duration": 5.918
    },
    {
      "lang": "en",
      "text": "consumption regression model. Again,",
      "offset": 17737.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you'll start by doing something simple,",
      "offset": 17740.958,
      "duration": 5.202
    },
    {
      "lang": "en",
      "text": "comparing a linear regression model with",
      "offset": 17743.6,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "an intercept to one that passes through",
      "offset": 17746.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the origin. By default, a linear",
      "offset": 17748.638,
      "duration": 5.442
    },
    {
      "lang": "en",
      "text": "regression model will always fit an",
      "offset": 17751.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "intercept, but you're going to be",
      "offset": 17754.08,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "explicit and specify the fit intercept",
      "offset": 17756,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "parameter as true. You fit the model to",
      "offset": 17758.958,
      "duration": 5.362
    },
    {
      "lang": "en",
      "text": "the training data and then calculate the",
      "offset": 17762.32,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "root mean squared error for the testing",
      "offset": 17764.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "data. Next, you repeat the process but",
      "offset": 17766.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "specify false for the fit intercept",
      "offset": 17770.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "parameter. Now, you are creating a model",
      "offset": 17773.4,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "which passes through the origin.",
      "offset": 17775.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "When you evaluate this model, you find",
      "offset": 17779.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "that the root mean squared error is",
      "offset": 17781.12,
      "duration": 5.838
    },
    {
      "lang": "en",
      "text": "higher. So comparing these two models,",
      "offset": 17783.56,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "you'd naturally choose the first one",
      "offset": 17786.958,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "because it has a lower root mean squared",
      "offset": 17789.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "error. However, there's a problem with",
      "offset": 17791.718,
      "duration": 5.482
    },
    {
      "lang": "en",
      "text": "this approach. Just getting a single",
      "offset": 17794.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "estimate of root mean squared error is",
      "offset": 17797.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "not very robust. It would be better to",
      "offset": 17799.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "make this comparison using cross",
      "offset": 17803.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "validation.",
      "offset": 17805.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "You also have to manually build the",
      "offset": 17807.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "models for the two different parameter",
      "offset": 17809.36,
      "duration": 6.518
    },
    {
      "lang": "en",
      "text": "values. It would be great if that were",
      "offset": 17811.56,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "automated. You can systematically",
      "offset": 17815.878,
      "duration": 4.442
    },
    {
      "lang": "en",
      "text": "evaluate a model across a grid of",
      "offset": 17818.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "parameter values using a technique known",
      "offset": 17820.32,
      "duration": 6.558
    },
    {
      "lang": "en",
      "text": "as grid search. To do this, you need to",
      "offset": 17823.2,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "set up a parameter grid. You actually",
      "offset": 17826.878,
      "duration": 5.442
    },
    {
      "lang": "en",
      "text": "saw this in the previous lesson where",
      "offset": 17830.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "you simply created an empty grid.",
      "offset": 17832.32,
      "duration": 5.638
    },
    {
      "lang": "en",
      "text": "Now you're going to add points to the",
      "offset": 17835.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "grid. First you create a grid builder",
      "offset": 17837.958,
      "duration": 7.082
    },
    {
      "lang": "en",
      "text": "and then you add one or more grids. At",
      "offset": 17841.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "present there's just one grid which",
      "offset": 17845.04,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "takes two values for the fit intercept",
      "offset": 17847.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "parameter. Call the build method to",
      "offset": 17850.92,
      "duration": 5.798
    },
    {
      "lang": "en",
      "text": "construct the grid. A separate model",
      "offset": 17853.68,
      "duration": 5.958
    },
    {
      "lang": "en",
      "text": "will be built for each point in the",
      "offset": 17856.718,
      "duration": 5.522
    },
    {
      "lang": "en",
      "text": "grid. You can check how many models this",
      "offset": 17859.638,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "corresponds to. And of course this is",
      "offset": 17862.24,
      "duration": 3.478
    },
    {
      "lang": "en",
      "text": "just",
      "offset": 17864.638,
      "duration": 4.642
    },
    {
      "lang": "en",
      "text": "two. Now you create a cross validator",
      "offset": 17865.718,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "object and fit it to the training data.",
      "offset": 17869.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "This builds a bunch of models, one model",
      "offset": 17872.878,
      "duration": 6.282
    },
    {
      "lang": "en",
      "text": "for each fold and point in the parameter",
      "offset": 17875.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "grid. Since there are two points in the",
      "offset": 17879.16,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "grid and 10 folds, this translates into",
      "offset": 17881.68,
      "duration": 7.038
    },
    {
      "lang": "en",
      "text": "20 models. The cross validator is going",
      "offset": 17885.36,
      "duration": 5.518
    },
    {
      "lang": "en",
      "text": "to loop through each of the points in",
      "offset": 17888.718,
      "duration": 5.042
    },
    {
      "lang": "en",
      "text": "the parameter grid. and for each point",
      "offset": 17890.878,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it will create a cross validated model",
      "offset": 17893.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "using the corresponding parameter",
      "offset": 17896.638,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "values. When you take a look at the",
      "offset": 17899,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "average metrics attribute, you can see",
      "offset": 17901.2,
      "duration": 5.518
    },
    {
      "lang": "en",
      "text": "why the metric is given as a list. You",
      "offset": 17904,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "get one average value for each point in",
      "offset": 17906.718,
      "duration": 3.322
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 17909.36,
      "duration": 3.438
    },
    {
      "lang": "en",
      "text": "grid. The values confirm what you",
      "offset": 17910.04,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "observe before. The model that includes",
      "offset": 17912.798,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "an intercept is superior to the model",
      "offset": 17915.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "without an intercept. Our goal was to",
      "offset": 17917.68,
      "duration": 7.038
    },
    {
      "lang": "en",
      "text": "get the best model for the data. You",
      "offset": 17921.76,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "retrieve this using the appropriately",
      "offset": 17924.718,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "named best model",
      "offset": 17926.718,
      "duration": 5.122
    },
    {
      "lang": "en",
      "text": "attribute. But it's not actually",
      "offset": 17929.878,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "necessary to work with this directly",
      "offset": 17931.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "because the cross validator object will",
      "offset": 17934.638,
      "duration": 4.922
    },
    {
      "lang": "en",
      "text": "behave like the best",
      "offset": 17937.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "model. So you can use it directly to",
      "offset": 17939.56,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "make predictions on the testing data. Of",
      "offset": 17943.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "course, you want to know what the best",
      "offset": 17947.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "parameter value is, and you can retrieve",
      "offset": 17949.28,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "this using the explain param method. As",
      "offset": 17952.08,
      "duration": 6.878
    },
    {
      "lang": "en",
      "text": "expected, the best value for the fitt",
      "offset": 17955.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "parameter is true. You can see this",
      "offset": 17958.958,
      "duration": 5.882
    },
    {
      "lang": "en",
      "text": "after the word current in the",
      "offset": 17961.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "output. It's possible to add more",
      "offset": 17964.84,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "parameters to the grid. Here, in",
      "offset": 17967.52,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "addition to whether or not to include an",
      "offset": 17970.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "intercept, you're also considering a",
      "offset": 17972.798,
      "duration": 3.682
    },
    {
      "lang": "en",
      "text": "selection of values for the",
      "offset": 17975.12,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "regularization",
      "offset": 17976.48,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "parameter and the elastic net",
      "offset": 17978.04,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "parameter. Of course, the more",
      "offset": 17981.32,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "parameters and values you add to the",
      "offset": 17983.6,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "grid, the more models you have to",
      "offset": 17985.6,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "evaluate. Because each of these models",
      "offset": 17988.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "will be evaluated using cross",
      "offset": 17990.878,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "validation. This might take a little",
      "offset": 17992.76,
      "duration": 6.038
    },
    {
      "lang": "en",
      "text": "while, but it will be time well spent",
      "offset": 17995.878,
      "duration": 5.562
    },
    {
      "lang": "en",
      "text": "because the model that you get back will",
      "offset": 17998.798,
      "duration": 5.442
    },
    {
      "lang": "en",
      "text": "in principle be much better than what",
      "offset": 18001.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you would have obtained by just using",
      "offset": 18004.24,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the default",
      "offset": 18006.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "parameters. Let's apply grid search on",
      "offset": 18008.28,
      "duration": 7.678
    },
    {
      "lang": "en",
      "text": "the flights and SMS models.",
      "offset": 18011.28,
      "duration": 4.678
    },
    {
      "lang": "en",
      "text": "You now know how to choose a good set of",
      "offset": 18017.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "parameters for any model using cross",
      "offset": 18019.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "validation and grid search. In the final",
      "offset": 18022.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "lesson, you're going to learn about how",
      "offset": 18025.76,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "models can be combined to form a",
      "offset": 18028,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "collection or",
      "offset": 18030.638,
      "duration": 2.722
    },
    {
      "lang": "en",
      "text": "ensl. Simply put, an ensom model is just",
      "offset": 18036.52,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "a collection of models. An ensom model",
      "offset": 18040.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "combines the results from multiple",
      "offset": 18043.6,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "models to produce better predictions",
      "offset": 18045.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "than any one of those models acting",
      "offset": 18048.718,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "alone. The concept is based on the idea",
      "offset": 18051.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "of the wisdom of the crowd which implies",
      "offset": 18054.958,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that the aggregated opinion of a group",
      "offset": 18057.76,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "is better than the opinions of the",
      "offset": 18060.798,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "individuals in that group even if the",
      "offset": 18062.718,
      "duration": 4.442
    },
    {
      "lang": "en",
      "text": "individuals are",
      "offset": 18065.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "experts. As the quote suggests for this",
      "offset": 18067.16,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "idea to be true there must be diversity",
      "offset": 18070.32,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and independence in the crowd. This",
      "offset": 18073.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "applies to models too. A successful",
      "offset": 18077.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "ensem requires diverse models. It does",
      "offset": 18079.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "not help if all of the models in the",
      "offset": 18083.28,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "ensom are similar or exactly the same.",
      "offset": 18085.2,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "Ideally, each of the models in the ensom",
      "offset": 18089.24,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "should be",
      "offset": 18092.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "different. A random forest, as the name",
      "offset": 18093.958,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "implies, is a collection of trees. To",
      "offset": 18097.04,
      "duration": 5.598
    },
    {
      "lang": "en",
      "text": "ensure that each of those trees is",
      "offset": 18100.638,
      "duration": 4.402
    },
    {
      "lang": "en",
      "text": "different, the decision tree algorithm",
      "offset": 18102.638,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "is modified slightly. Each tree is",
      "offset": 18105.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "trained on a different random subset of",
      "offset": 18108.718,
      "duration": 5.842
    },
    {
      "lang": "en",
      "text": "the data and within each tree a random",
      "offset": 18111.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "subset of features is used for splitting",
      "offset": 18114.56,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "at each node. The result is a collection",
      "offset": 18117.12,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "of trees where no two trees are the",
      "offset": 18120.48,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "same within the random forest model. All",
      "offset": 18124.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "of the trees operate in",
      "offset": 18127.52,
      "duration": 5.438
    },
    {
      "lang": "en",
      "text": "parallel. Let's go back to the car",
      "offset": 18130.6,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "classifier yet again.",
      "offset": 18132.958,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "You create a random forest model using",
      "offset": 18135.28,
      "duration": 5.518
    },
    {
      "lang": "en",
      "text": "the random forest classifier class from",
      "offset": 18137.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "the classification subm",
      "offset": 18140.798,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "module. You can select the number of",
      "offset": 18143,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "trees in the forest using the num trees",
      "offset": 18145.36,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "parameter. By default, this is 20, but",
      "offset": 18148.84,
      "duration": 6.038
    },
    {
      "lang": "en",
      "text": "we'll drop that down to five so the",
      "offset": 18152.16,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "results are easier to",
      "offset": 18154.878,
      "duration": 5.202
    },
    {
      "lang": "en",
      "text": "interpret. As is the case with any other",
      "offset": 18157.32,
      "duration": 5.558
    },
    {
      "lang": "en",
      "text": "model, the random forest is fit to the",
      "offset": 18160.08,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "training data.",
      "offset": 18162.878,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "Once the model is trained, it's possible",
      "offset": 18164.958,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "to access the individual trees in the",
      "offset": 18167.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "forest using the trees",
      "offset": 18169.84,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "attribute. You would not normally do",
      "offset": 18172.44,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "this, but it's useful for illustrative",
      "offset": 18174.718,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "purposes. There are precisely five trees",
      "offset": 18177.718,
      "duration": 6.362
    },
    {
      "lang": "en",
      "text": "in the forest as specified. The trees",
      "offset": 18180.638,
      "duration": 5.682
    },
    {
      "lang": "en",
      "text": "are all different as can be seen from",
      "offset": 18184.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the varying number of nodes in each",
      "offset": 18186.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tree.",
      "offset": 18188.56,
      "duration": 4.238
    },
    {
      "lang": "en",
      "text": "You can then make predictions using each",
      "offset": 18190.08,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "tree",
      "offset": 18192.798,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "individually. Here are the predictions",
      "offset": 18194.84,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "of individual trees on a subset of the",
      "offset": 18197.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "testing data. Each row represents",
      "offset": 18199.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "predictions from each of the five trees",
      "offset": 18202.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "for a specific record. In some cases,",
      "offset": 18205.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "all the trees agree, but there is often",
      "offset": 18209.36,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "some disscent amongst the models. This",
      "offset": 18212.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is precisely where the random forest",
      "offset": 18215.52,
      "duration": 6.118
    },
    {
      "lang": "en",
      "text": "works best, where the prediction is not",
      "offset": 18217.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "clearcut. The random forest model",
      "offset": 18221.638,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "creates a consensus prediction by",
      "offset": 18224.08,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "aggregating the predictions across all",
      "offset": 18226.638,
      "duration": 3.722
    },
    {
      "lang": "en",
      "text": "of the individual",
      "offset": 18228.878,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "trees. You don't need to worry about",
      "offset": 18230.36,
      "duration": 4.278
    },
    {
      "lang": "en",
      "text": "these details though because the",
      "offset": 18232.638,
      "duration": 4.402
    },
    {
      "lang": "en",
      "text": "transform method will automatically",
      "offset": 18234.638,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "generate a consensus prediction column.",
      "offset": 18237.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "It also creates a probability column",
      "offset": 18240.718,
      "duration": 5.282
    },
    {
      "lang": "en",
      "text": "which assigns aggregate probabilities to",
      "offset": 18243.12,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "each of the",
      "offset": 18246,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "outcomes. It's possible to get an idea",
      "offset": 18247.4,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "of the relative importance of the",
      "offset": 18249.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "features in the model by looking at the",
      "offset": 18252.16,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "feature importances attribute. An",
      "offset": 18254.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "importance is assigned to each feature",
      "offset": 18256.958,
      "duration": 5.042
    },
    {
      "lang": "en",
      "text": "where a larger importance indicates a",
      "offset": 18259.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "feature which makes a larger",
      "offset": 18262,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "contribution to the model. Looking",
      "offset": 18263.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "carefully at the importances, we see",
      "offset": 18266.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that feature 4, RPM, is the most",
      "offset": 18268.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "important, while feature zero, the",
      "offset": 18271.28,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "number of cylinders, is the least",
      "offset": 18273.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "important. The second ensemble you'll be",
      "offset": 18276.6,
      "duration": 6.038
    },
    {
      "lang": "en",
      "text": "looking at is gradient boosted trees.",
      "offset": 18279.76,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "Again, the aim is to build a collection",
      "offset": 18282.638,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "of diverse models, but the approach is",
      "offset": 18284.638,
      "duration": 5.442
    },
    {
      "lang": "en",
      "text": "slightly different. Rather than building",
      "offset": 18287.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "a set of trees that operate in parallel,",
      "offset": 18290.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "now we build trees which work in series.",
      "offset": 18293.04,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "The boosting algorithm works",
      "offset": 18296.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "iteratively. First build a decision tree",
      "offset": 18298.84,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "and add to the ensem. Then use the ensem",
      "offset": 18301.76,
      "duration": 6.038
    },
    {
      "lang": "en",
      "text": "to make predictions on the training",
      "offset": 18305.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "data. Compare the predicted labels to",
      "offset": 18307.798,
      "duration": 6.682
    },
    {
      "lang": "en",
      "text": "the known labels. Now identify training",
      "offset": 18311.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "instances where the predictions were",
      "offset": 18314.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "incorrect. return to the start and train",
      "offset": 18316.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "another tree which focuses on improving",
      "offset": 18319.28,
      "duration": 4.438
    },
    {
      "lang": "en",
      "text": "the incorrect",
      "offset": 18321.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "predictions. As trees are added to the",
      "offset": 18323.718,
      "duration": 5.962
    },
    {
      "lang": "en",
      "text": "onsaw, its predictions improve because",
      "offset": 18326,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "each new tree focuses on correcting the",
      "offset": 18329.68,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "shortcomings of the preceding",
      "offset": 18332.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "trees. The class for the gradient",
      "offset": 18335.16,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "boosted tree classifier is also found in",
      "offset": 18338.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the classification subm module. After",
      "offset": 18341.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "creating an instance of the class, you",
      "offset": 18344.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "fit it to the training data. You can",
      "offset": 18346.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "make an objective comparison between a",
      "offset": 18349.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "plain decision tree and the two ONSM",
      "offset": 18351.28,
      "duration": 5.438
    },
    {
      "lang": "en",
      "text": "methods by looking at the values of AU",
      "offset": 18354,
      "duration": 5.638
    },
    {
      "lang": "en",
      "text": "obtained by each of them on the testing",
      "offset": 18356.718,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "data. Both of the ONSAM methods score",
      "offset": 18359.638,
      "duration": 5.562
    },
    {
      "lang": "en",
      "text": "better than the decision tree. This is",
      "offset": 18362.638,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "not too surprising since they are",
      "offset": 18365.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "significantly more powerful models. It's",
      "offset": 18366.958,
      "duration": 5.602
    },
    {
      "lang": "en",
      "text": "also worth noting that these results are",
      "offset": 18370.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "based on the default parameters for",
      "offset": 18372.56,
      "duration": 4.318
    },
    {
      "lang": "en",
      "text": "these models. It should be possible to",
      "offset": 18374.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "get even better performance by tuning",
      "offset": 18376.878,
      "duration": 5.242
    },
    {
      "lang": "en",
      "text": "those parameters using cross",
      "offset": 18379.76,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "validation. In the final set of",
      "offset": 18382.12,
      "duration": 4.598
    },
    {
      "lang": "en",
      "text": "exercises, you'll try out ensemble",
      "offset": 18384.2,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "methods on the flights",
      "offset": 18386.718,
      "duration": 3.682
    },
    {
      "lang": "en",
      "text": "data. Congratulations on completing this",
      "offset": 18391.4,
      "duration": 5.318
    },
    {
      "lang": "en",
      "text": "course on machine learning with Apache",
      "offset": 18394.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Spark. You have covered a lot of ground",
      "offset": 18396.718,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "reviewing some machine learning",
      "offset": 18400.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "fundamentals and seeing how they can be",
      "offset": 18401.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "applied to large data sets using Spark",
      "offset": 18403.84,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "for distributed",
      "offset": 18407.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "computing. You learned how to load data",
      "offset": 18408.92,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "into Spark and then perform a variety of",
      "offset": 18411.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "operations on those data. Specifically,",
      "offset": 18414.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you learned basic column manipulation on",
      "offset": 18417.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "data frames, how to deal with text data,",
      "offset": 18420.56,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "bucketing continuous data, and one hot",
      "offset": 18423.84,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "encoding categorical",
      "offset": 18426.48,
      "duration": 5.318
    },
    {
      "lang": "en",
      "text": "data. You then delved into two types of",
      "offset": 18428.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "classifiers, decision trees and logistic",
      "offset": 18431.798,
      "duration": 5.962
    },
    {
      "lang": "en",
      "text": "regression. In the process, building a",
      "offset": 18435.32,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "robust spam classifier.",
      "offset": 18437.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "You also learned about partitioning your",
      "offset": 18441.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "data and how to use testing data and a",
      "offset": 18443.2,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "selection of metrics to evaluate a",
      "offset": 18446,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "model. Next, you learned about",
      "offset": 18448.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "regression, starting with a simple",
      "offset": 18451.4,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "linear regression model and progressing",
      "offset": 18453.6,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "to penalized regression, which allowed",
      "offset": 18456,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you to build a model using only the most",
      "offset": 18458.718,
      "duration": 3.962
    },
    {
      "lang": "en",
      "text": "relevant",
      "offset": 18461.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "predictors. You learned about pipelines",
      "offset": 18462.68,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "and how they can make your Spark code",
      "offset": 18465.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "cleaner and easier to maintain. This led",
      "offset": 18467.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "naturally into using cross validation",
      "offset": 18471.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and grid search to derive more robust",
      "offset": 18473.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "model metrics and use them to select",
      "offset": 18476.4,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "good model",
      "offset": 18479.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "parameters. Finally, you encountered two",
      "offset": 18481,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "forms of",
      "offset": 18484,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "models. Of course, there are many topics",
      "offset": 18486.2,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "that were not covered in this course. If",
      "offset": 18488.958,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "you want to dig deeper, then consult the",
      "offset": 18491.28,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "excellent and extensive online",
      "offset": 18493.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "documentation. Importantly, you can find",
      "offset": 18496.36,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "instructions for setting up and securing",
      "offset": 18499.28,
      "duration": 4.438
    },
    {
      "lang": "en",
      "text": "a Spark",
      "offset": 18502,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "cluster. Now, go and use what you've",
      "offset": 18503.718,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "learned to solve challenging and",
      "offset": 18506.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "interesting big data problems in the",
      "offset": 18508.798,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "real",
      "offset": 18511.44,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "world. Hi, welcome to this course on",
      "offset": 18514.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "building recommendation engines using",
      "offset": 18518,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "alternating lease squares or ALS in",
      "offset": 18519.92,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "Pispark.",
      "offset": 18523.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "You're probably already familiar with",
      "offset": 18524.718,
      "duration": 2.962
    },
    {
      "lang": "en",
      "text": "the output of these types of",
      "offset": 18526.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "recommendation engines where a website",
      "offset": 18527.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "tells you something along the lines of,",
      "offset": 18529.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "&quot;If you like that, then you'll probably",
      "offset": 18532.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like this.&quot; You've likely seen these",
      "offset": 18533.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "types of recommendations on your",
      "offset": 18536.4,
      "duration": 3.478
    },
    {
      "lang": "en",
      "text": "favorite retail or media streaming",
      "offset": 18537.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "websites. These recommendations are",
      "offset": 18539.878,
      "duration": 3.722
    },
    {
      "lang": "en",
      "text": "generated through different types of",
      "offset": 18542.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "data that you, as a user, provide either",
      "offset": 18543.6,
      "duration": 5.198
    },
    {
      "lang": "en",
      "text": "directly or indirectly.",
      "offset": 18546.4,
      "duration": 4.398
    },
    {
      "lang": "en",
      "text": "When you purchase something online or",
      "offset": 18548.798,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "watch a movie or even read an article,",
      "offset": 18550.798,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "you're often given a chance to rate that",
      "offset": 18553.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "item on a scale of 1 to five stars, a",
      "offset": 18555.36,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "thumbs up or thumbs down, or some other",
      "offset": 18558.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "type of rating scale. Based on your",
      "offset": 18560.638,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "feedback from these types of rating",
      "offset": 18563.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "systems, companies can learn a lot about",
      "offset": 18564.958,
      "duration": 4.162
    },
    {
      "lang": "en",
      "text": "your preferences and offer you",
      "offset": 18567.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "recommendations based on preferences of",
      "offset": 18569.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "users that are similar to you.",
      "offset": 18571.36,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "For example, if your movie streaming",
      "offset": 18574.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "service sees that you like The Dark",
      "offset": 18576.638,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Knight and Iron Man and did not like",
      "offset": 18578.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Tangled and it also sees other users",
      "offset": 18580.638,
      "duration": 4.642
    },
    {
      "lang": "en",
      "text": "that also like The Dark Knight and Iron",
      "offset": 18583.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Man and also did not like Tangled, the",
      "offset": 18585.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "ALS algorithm would see that you and",
      "offset": 18588,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "these other users have similar tastes.",
      "offset": 18590.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "It would then look at the movies that",
      "offset": 18593.04,
      "duration": 3.758
    },
    {
      "lang": "en",
      "text": "you have not yet seen and see which ones",
      "offset": 18594.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are the highest rated among those",
      "offset": 18596.798,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "similar users and offer them as",
      "offset": 18598.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "recommendations to you. This is why",
      "offset": 18600.878,
      "duration": 5.042
    },
    {
      "lang": "en",
      "text": "websites will often say things like",
      "offset": 18603.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "because you like that movie, we think",
      "offset": 18605.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you'll like this movie or users like you",
      "offset": 18607.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "also watched this movie. These types of",
      "offset": 18610.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "rating systems are extremely powerful.",
      "offset": 18613.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "In fact, an article published by",
      "offset": 18616.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Mckenzian Company in October of 2013",
      "offset": 18618,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "stated that 35% of what customers buy on",
      "offset": 18621.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Amazon and 75% of what they watch on",
      "offset": 18624.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Netflix come from product",
      "offset": 18627.2,
      "duration": 3.518
    },
    {
      "lang": "en",
      "text": "recommendations based on algorithms such",
      "offset": 18628.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "as the one you are going to be learning",
      "offset": 18630.718,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in this course. That's a powerful use of",
      "offset": 18632.32,
      "duration": 4.638
    },
    {
      "lang": "en",
      "text": "data. And with this course, you will",
      "offset": 18634.958,
      "duration": 4.642
    },
    {
      "lang": "en",
      "text": "learn how to do this. In addition to",
      "offset": 18636.958,
      "duration": 4.322
    },
    {
      "lang": "en",
      "text": "this, there are alternate uses of",
      "offset": 18639.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "recommendation algorithms that can be",
      "offset": 18641.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "extremely useful for purposes as broad",
      "offset": 18643.28,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "as feature space reduction, image",
      "offset": 18645.92,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "compression, mathematical user and",
      "offset": 18648.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "product grouping, latent feature",
      "offset": 18650.798,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "discovery, and you're going to learn",
      "offset": 18653.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "some of these in this course. This",
      "offset": 18654.718,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "tutorial is intended for those that have",
      "offset": 18657.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "experience with Spark and Python and",
      "offset": 18659.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "understand the fundamentals of machine",
      "offset": 18662.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "learning. If needed, some good",
      "offset": 18663.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "introductory resources are data camp's",
      "offset": 18666.08,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "introduction to pispark course, their",
      "offset": 18668.32,
      "duration": 4.318
    },
    {
      "lang": "en",
      "text": "intermediate Python for data science",
      "offset": 18670.798,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "course, and their supervised machine",
      "offset": 18672.638,
      "duration": 4.282
    },
    {
      "lang": "en",
      "text": "learning with Python's scikitlearn",
      "offset": 18674.878,
      "duration": 5.602
    },
    {
      "lang": "en",
      "text": "course. Let's jump",
      "offset": 18676.92,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "in. In the world of recommendation",
      "offset": 18680.84,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "engines, there are two basic types.",
      "offset": 18683.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Collaborative filtering engines and",
      "offset": 18686.08,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "contentbased filtering engines. Both aim",
      "offset": 18687.92,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "to offer meaningful recommendations, but",
      "offset": 18690.878,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "they do so in slightly different ways.",
      "offset": 18692.878,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Contentbased filtering, as the name",
      "offset": 18695.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "suggests, tries to understand the",
      "offset": 18697.638,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "content or features of the items and",
      "offset": 18699.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "makes recommendations based on your",
      "offset": 18702.718,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "preferences for those specific features.",
      "offset": 18704.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "For example, a movie streaming service",
      "offset": 18707.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "might go to great lengths to add",
      "offset": 18709.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "descriptive tags to their movies, such",
      "offset": 18711.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "as the genre, whether it's animated or",
      "offset": 18713.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "not, the language spoken in the movie,",
      "offset": 18716.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the decade it was filmed, and which",
      "offset": 18719.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "actors were in it. So when a user like",
      "offset": 18721.76,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "you gives five stars to a really",
      "offset": 18724.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "dramatic Portuguese movie with specific",
      "offset": 18726.638,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "actors from a specific decade, they can",
      "offset": 18729.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "infer that you like movies like this and",
      "offset": 18731.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "will also like other dramatic movies in",
      "offset": 18733.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Portuguese with those same actors and",
      "offset": 18736.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "recommend those movies to you.",
      "offset": 18738.56,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Collaborative filtering is a little bit",
      "offset": 18741.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "different. As explained in the previous",
      "offset": 18742.84,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "video, collaborative filtering is based",
      "offset": 18745.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "on user similarity.",
      "offset": 18747.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "However, unlike contentbased filtering,",
      "offset": 18749.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "manually created tags are not necessary.",
      "offset": 18752.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "The features and groupings are created",
      "offset": 18755.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "mathematically from patterns in the",
      "offset": 18757.92,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "ratings provided by users. When you",
      "offset": 18759.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "provide ratings for a product or item,",
      "offset": 18762.638,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "whether it be a thumbs up or thumbs",
      "offset": 18765.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "down, or even if you just watch a video",
      "offset": 18766.798,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "without even giving it a rating, you are",
      "offset": 18769.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "providing meaningful insight about your",
      "offset": 18771.6,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "preferences. From this behavior, the ALS",
      "offset": 18773.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "algorithm can mathematically group you",
      "offset": 18776.638,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with similar users, predict your",
      "offset": 18778.48,
      "duration": 4.398
    },
    {
      "lang": "en",
      "text": "behavior, and help you have a more",
      "offset": 18780.878,
      "duration": 3.642
    },
    {
      "lang": "en",
      "text": "effective customer",
      "offset": 18782.878,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "experience. While ALS can have",
      "offset": 18784.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "contentbased applications, this course",
      "offset": 18786.878,
      "duration": 4.322
    },
    {
      "lang": "en",
      "text": "will focus on its application to",
      "offset": 18789.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "collaborative filtering, but many of the",
      "offset": 18791.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "principles of collaborative filtering",
      "offset": 18793.68,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "can be implied to contentbased",
      "offset": 18795.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "applications. Now, let's talk about",
      "offset": 18798.36,
      "duration": 4.518
    },
    {
      "lang": "en",
      "text": "ratings. In the realm of recommendation",
      "offset": 18800.24,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "engines, there are two main types of",
      "offset": 18802.878,
      "duration": 5.322
    },
    {
      "lang": "en",
      "text": "ratings. Explicit ratings and implicit",
      "offset": 18804.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "ratings. Explicit ratings are pretty",
      "offset": 18808.2,
      "duration": 4.678
    },
    {
      "lang": "en",
      "text": "straightforward. Examples of these are",
      "offset": 18810.76,
      "duration": 4.198
    },
    {
      "lang": "en",
      "text": "when you input a number of stars or",
      "offset": 18812.878,
      "duration": 3.882
    },
    {
      "lang": "en",
      "text": "something like a thumbs up or thumbs",
      "offset": 18814.958,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "down. These are explicit ratings because",
      "offset": 18816.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "users explicitly state how much they",
      "offset": 18819.44,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "like or dislike",
      "offset": 18821.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "something. Implicit ratings are a little",
      "offset": 18823.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "bit different. They are based on the",
      "offset": 18825.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "passive tracking of your behavior like",
      "offset": 18827.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the number of movies you've seen in",
      "offset": 18829.68,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "different",
      "offset": 18831.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "genres. Fundamentally, implicit ratings",
      "offset": 18832.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "are generated from the frequency of your",
      "offset": 18835.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "actions. For example, if you watch 30",
      "offset": 18837.56,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "movies and of those 30 movies, 22 are",
      "offset": 18840.32,
      "duration": 6.558
    },
    {
      "lang": "en",
      "text": "action movies and only one is a comedy,",
      "offset": 18843.68,
      "duration": 5.198
    },
    {
      "lang": "en",
      "text": "the low number of comedy views will be",
      "offset": 18846.878,
      "duration": 4.242
    },
    {
      "lang": "en",
      "text": "converted into low confidence that you",
      "offset": 18848.878,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "like comedies. and the high number of",
      "offset": 18851.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "action movies will be converted into a",
      "offset": 18853.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "high confidence that you like action",
      "offset": 18855.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "movies. These confidence scores are then",
      "offset": 18857.48,
      "duration": 5.238
    },
    {
      "lang": "en",
      "text": "used as ratings. The logic behind this",
      "offset": 18859.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "is in essence the more you carry out a",
      "offset": 18862.718,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "behavior the higher the likelihood that",
      "offset": 18865.44,
      "duration": 4.198
    },
    {
      "lang": "en",
      "text": "you like it and thus a higher",
      "offset": 18867.6,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "rating. Additionally, in some cases you",
      "offset": 18869.638,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "may not have access to user behavior",
      "offset": 18872.718,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "counts like this. A simpler form of",
      "offset": 18874.718,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "ratings that still works with the ALS",
      "offset": 18877.28,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "algorithm is the use of simple binary",
      "offset": 18879.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "ratings. Rather than having a count of",
      "offset": 18882.04,
      "duration": 4.678
    },
    {
      "lang": "en",
      "text": "user actions, binary ratings just show",
      "offset": 18884.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "whether a user has done something like",
      "offset": 18886.718,
      "duration": 5.042
    },
    {
      "lang": "en",
      "text": "watched a comedy represented by a one or",
      "offset": 18888.56,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "not watched a comedy represented by a",
      "offset": 18891.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "zero. These types of ratings aren't",
      "offset": 18894.2,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "nearly as rich, but they still can",
      "offset": 18896.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "provide meaningful insight and still",
      "offset": 18898.56,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "work perfectly fine with the ALS",
      "offset": 18900.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "algorithm. Now, let's look at some",
      "offset": 18902.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "actual data.",
      "offset": 18904.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "So far, we've only considered",
      "offset": 18908.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "recommendations as a use case for the",
      "offset": 18909.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "ALS algorithm, but there are other",
      "offset": 18911.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "applications that are also useful. These",
      "offset": 18913.92,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "include latent feature discovery, item",
      "offset": 18916.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "grouping, dimensionality reduction, and",
      "offset": 18918.878,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "image compression. In this course, we'll",
      "offset": 18921.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "only talk about some of these. First,",
      "offset": 18923.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "let's talk about latent features. As",
      "offset": 18925.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "mentioned earlier, people will go to",
      "offset": 18928.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "great lengths to effectively categorize",
      "offset": 18929.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "items. But some products span various",
      "offset": 18931.68,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "categories making them difficult to",
      "offset": 18934.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "organize. Movies are often like this.",
      "offset": 18936.12,
      "duration": 4.598
    },
    {
      "lang": "en",
      "text": "For example, horror movies can be",
      "offset": 18939.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "comedies. Dramas can be satires.",
      "offset": 18940.718,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "Documentaries can be romances or even",
      "offset": 18943.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "mysteries. Because of this, they can",
      "offset": 18945.798,
      "duration": 4.522
    },
    {
      "lang": "en",
      "text": "sometimes be difficult to market. If we",
      "offset": 18947.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "had a better understanding of how",
      "offset": 18950.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "consumers categorize movies based on",
      "offset": 18951.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "their experience watching them, we could",
      "offset": 18953.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "add more power to marketing strategies.",
      "offset": 18955.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "ALS can help with",
      "offset": 18958.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this. When we have a matrix that",
      "offset": 18960.6,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "contains users and movie ratings, ALS",
      "offset": 18963.36,
      "duration": 4.358
    },
    {
      "lang": "en",
      "text": "will factor that matrix into two",
      "offset": 18966.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "matrices. One containing user",
      "offset": 18967.718,
      "duration": 3.802
    },
    {
      "lang": "en",
      "text": "information and the other containing",
      "offset": 18969.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "product information or in this case",
      "offset": 18971.52,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "movie information. Each matrix takes the",
      "offset": 18974,
      "duration": 4.638
    },
    {
      "lang": "en",
      "text": "respective labeled axis from the",
      "offset": 18976.798,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "original matrix and is given another",
      "offset": 18978.638,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "axis that is unlabeled. The unlabeled",
      "offset": 18980.718,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "axes contain what's called latent",
      "offset": 18983.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "features. The number of latent features",
      "offset": 18985.718,
      "duration": 4.282
    },
    {
      "lang": "en",
      "text": "is referred to as the rank of these",
      "offset": 18988.24,
      "duration": 4.638
    },
    {
      "lang": "en",
      "text": "matrices. In this case, the rank chosen",
      "offset": 18990,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "is three. You, as a data scientist, get",
      "offset": 18992.878,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "to choose how many of these ALS will",
      "offset": 18995.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "create. These latent features represent",
      "offset": 18997.798,
      "duration": 4.762
    },
    {
      "lang": "en",
      "text": "groups that are created from patterns in",
      "offset": 19000.4,
      "duration": 4.318
    },
    {
      "lang": "en",
      "text": "the original ratings matrix. And the",
      "offset": 19002.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "values in these columns represent how",
      "offset": 19004.718,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "much each item falls into these groups.",
      "offset": 19006.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "For example, in the original ratings",
      "offset": 19009.2,
      "duration": 3.598
    },
    {
      "lang": "en",
      "text": "matrix, there might be a lot of people",
      "offset": 19011.36,
      "duration": 3.358
    },
    {
      "lang": "en",
      "text": "who like horror movies and don't like",
      "offset": 19012.798,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "dramas. They would rate horror movies",
      "offset": 19014.718,
      "duration": 5.442
    },
    {
      "lang": "en",
      "text": "high and dramas low. Likewise, other",
      "offset": 19016.958,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "people might like dramas and not like",
      "offset": 19020.16,
      "duration": 4.478
    },
    {
      "lang": "en",
      "text": "horror movies and would rate dramas high",
      "offset": 19021.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and horror films low. ALS can see this",
      "offset": 19024.638,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "and determine that these are different",
      "offset": 19028.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "types of movies. And if we were to look",
      "offset": 19029.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "at the movie factor matrix, we would",
      "offset": 19031.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "likely see that in one of the latent",
      "offset": 19034.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "feature rows, the dramas would score",
      "offset": 19035.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "high and the horror movies would score",
      "offset": 19037.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "low, while in another latent feature",
      "offset": 19039.68,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "column, we might see the opposite.",
      "offset": 19042,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "Knowing a little about these movies, we",
      "offset": 19044.798,
      "duration": 3.442
    },
    {
      "lang": "en",
      "text": "could determine that those latent",
      "offset": 19046.878,
      "duration": 4.162
    },
    {
      "lang": "en",
      "text": "features reflect those two genres. This",
      "offset": 19048.24,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "allows us to mathematically see how",
      "offset": 19051.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "users experience these movies and to",
      "offset": 19052.958,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "what degree users feel each movie falls",
      "offset": 19055.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "into each respective category. This",
      "offset": 19057.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "concept goes a bit deeper though. For",
      "offset": 19060.08,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "example, if we look at a movie matrix,",
      "offset": 19062.48,
      "duration": 4.238
    },
    {
      "lang": "en",
      "text": "we might see in one latent feature",
      "offset": 19064.798,
      "duration": 4.322
    },
    {
      "lang": "en",
      "text": "column that several movies have scored",
      "offset": 19066.718,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "very high, but they don't seem to have",
      "offset": 19069.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "anything in common. If they're all",
      "offset": 19071.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "popular movies, we might want to",
      "offset": 19073.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "research what's going on here to see if",
      "offset": 19075.44,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "there's a business",
      "offset": 19077.28,
      "duration": 3.518
    },
    {
      "lang": "en",
      "text": "opportunity. Digging deeper, we find",
      "offset": 19078.68,
      "duration": 4.198
    },
    {
      "lang": "en",
      "text": "that these movies are all adaptations of",
      "offset": 19080.798,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Shakespeare plays and that there seems",
      "offset": 19082.878,
      "duration": 3.682
    },
    {
      "lang": "en",
      "text": "to be a strong customer group that likes",
      "offset": 19084.878,
      "duration": 4.162
    },
    {
      "lang": "en",
      "text": "these types of movies. Now that we know",
      "offset": 19086.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this, we can use this information to",
      "offset": 19089.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "inform how we choose what movies to make",
      "offset": 19091.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and hopefully give our customers more of",
      "offset": 19093.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what they want. It's worth reiterating",
      "offset": 19095.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that in the original data set, there was",
      "offset": 19097.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "no column anywhere called Shakespeare",
      "offset": 19099.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "adaptations. It's also worth noting that",
      "offset": 19101.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "many or all of these customers may not",
      "offset": 19104,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "even know that this is something that",
      "offset": 19105.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "draws them to these movies. This is the",
      "offset": 19107.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "type of powerful information that ALS",
      "offset": 19109.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "can help us uncover. Now, let's try to",
      "offset": 19111.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "actually use",
      "offset": 19113.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "this. As you've probably realized,",
      "offset": 19116.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "matrix operations are fundamental to the",
      "offset": 19119.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "ALS algorithm. We're going to review",
      "offset": 19121.6,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "matrix multiplication and matrix",
      "offset": 19124.08,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "factorization. Let's start with matrix",
      "offset": 19126.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "multiplication. Here we have two square",
      "offset": 19128.6,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "matrices. In order to multiply them",
      "offset": 19131.48,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "together, we make specific pairs of the",
      "offset": 19133.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "values from the two matrices and add the",
      "offset": 19136.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "products of those pairs. We start at the",
      "offset": 19139.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "top leftand corner of each matrix and",
      "offset": 19141.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "create pairs moving to the right on the",
      "offset": 19144.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "first matrix and moving down on the",
      "offset": 19146.08,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "second matrix one at a time. Each pair",
      "offset": 19148.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is multiplied and the products from all",
      "offset": 19150.878,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "pairs are added together. The final sum",
      "offset": 19153.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "will make up one number of the resulting",
      "offset": 19155.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "matrix. That's a lot to digest. So,",
      "offset": 19157.52,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "let's walk through an",
      "offset": 19160.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "example. Starting at the top left number",
      "offset": 19161.4,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "of each matrix, we have a pair of",
      "offset": 19164.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "numbers 1 and 9. We will multiply those",
      "offset": 19166.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "numbers together. Then, moving to the",
      "offset": 19169.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "right on the first matrix and down on",
      "offset": 19171.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the second matrix, we have 2 and six.",
      "offset": 19173.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Then, moving right again on the first",
      "offset": 19177.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "matrix and down again on the second",
      "offset": 19179.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "matrix, we have three and three. We have",
      "offset": 19181.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "completed the first set of pairs. So,",
      "offset": 19184.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "let's add their products together. 1 * 9",
      "offset": 19186.08,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "+ 2 * 6 + 3 * 3 is 9 + 12 + 9, which",
      "offset": 19189.6,
      "duration": 9.118
    },
    {
      "lang": "en",
      "text": "gives us 30. 30 is the first number in",
      "offset": 19195.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "our final",
      "offset": 19198.718,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "matrix. From here, we stay on the first",
      "offset": 19200.52,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "row of the first matrix, but move on to",
      "offset": 19203.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the second column of the second matrix.",
      "offset": 19205.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "These pairs give us 1 and 8, 2 and 5,",
      "offset": 19208.24,
      "duration": 10.24
    },
    {
      "lang": "en",
      "text": "and 3 and 2. 1 * 8 + 2 * 5 + 3 * 2 is",
      "offset": 19212.16,
      "duration": 10.28
    },
    {
      "lang": "en",
      "text": "equal to 8 + 10 + 6 which is",
      "offset": 19218.48,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "24. Moving to the next set of pairs, we",
      "offset": 19222.44,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "multiply 1 and 7, 2 and 4, and 3 and 1.",
      "offset": 19225.6,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "Their products are 7, 8, and 3, which",
      "offset": 19230.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "makes 18. Once we've multiplied the",
      "offset": 19233.52,
      "duration": 5.358
    },
    {
      "lang": "en",
      "text": "first row of the first matrix by all",
      "offset": 19236.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "columns of the second matrix, we then go",
      "offset": 19238.878,
      "duration": 4.402
    },
    {
      "lang": "en",
      "text": "through the same process for the second",
      "offset": 19241.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "row of the first matrix with all columns",
      "offset": 19243.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "of the second matrix and so on until all",
      "offset": 19245.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the rows of the first matrix have been",
      "offset": 19249.28,
      "duration": 4.678
    },
    {
      "lang": "en",
      "text": "multiplied by all columns of the second",
      "offset": 19251.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "matrix. In this example, we multiplied",
      "offset": 19253.958,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "two square matrices of the same",
      "offset": 19257.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "dimensions. In reality, you can multiply",
      "offset": 19258.878,
      "duration": 5.442
    },
    {
      "lang": "en",
      "text": "any two matrices as long as the number",
      "offset": 19261.68,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "of columns of the first matrix matches",
      "offset": 19264.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the number of rows of the second matrix.",
      "offset": 19266.798,
      "duration": 5.602
    },
    {
      "lang": "en",
      "text": "If they don't, then some values in one",
      "offset": 19269.92,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "of the matrices won't be paired and",
      "offset": 19272.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "multiplication can't be completed. Let's",
      "offset": 19274.798,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "look at some examples and practice",
      "offset": 19277.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "matrix multiplication.",
      "offset": 19278.958,
      "duration": 3.402
    },
    {
      "lang": "en",
      "text": "Matrix factorization or matrix",
      "offset": 19283.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "decomposition is essentially the",
      "offset": 19285.958,
      "duration": 3.362
    },
    {
      "lang": "en",
      "text": "opposite of matrix",
      "offset": 19287.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "multiplication. Rather than multiplying",
      "offset": 19289.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "two matrices together to get one new",
      "offset": 19291.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "matrix, matrix factorization splits a",
      "offset": 19293.4,
      "duration": 5.398
    },
    {
      "lang": "en",
      "text": "matrix into two or more matrices which",
      "offset": 19296.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "when multiplied back together produce an",
      "offset": 19298.798,
      "duration": 5.522
    },
    {
      "lang": "en",
      "text": "approximation of the original matrix.",
      "offset": 19301.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "There are several different mathematical",
      "offset": 19304.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "approaches for this each of which has a",
      "offset": 19305.84,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "different application. We aren't going",
      "offset": 19308.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to go into any of that here. We are",
      "offset": 19310.638,
      "duration": 3.682
    },
    {
      "lang": "en",
      "text": "simply going to review the factorization",
      "offset": 19312.48,
      "duration": 3.318
    },
    {
      "lang": "en",
      "text": "that ALS",
      "offset": 19314.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "performs. Used in the context of",
      "offset": 19315.798,
      "duration": 4.602
    },
    {
      "lang": "en",
      "text": "collaborative filtering, ALS uses a",
      "offset": 19317.92,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "factorization called non-gative matrix",
      "offset": 19320.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "factorization. Because matrix",
      "offset": 19323.56,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "factorization generally returns only",
      "offset": 19325.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "approximations of the original matrix.",
      "offset": 19327.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "In some cases, they can return negative",
      "offset": 19329.92,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "values in the factor matrices even when",
      "offset": 19331.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "attempting to predict positive values.",
      "offset": 19334.638,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "When predicting what rating a user will",
      "offset": 19337.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "give to an item, negative values don't",
      "offset": 19339.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "really make sense. Neither do they make",
      "offset": 19341.84,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "sense in the context of latent features.",
      "offset": 19344.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "For this reason, the version of ALS that",
      "offset": 19346.958,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "we will be using will require that the",
      "offset": 19349.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "factorization return only positive",
      "offset": 19351.68,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "values. Let's look at some sample",
      "offset": 19354.04,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "factorizations. Here's a sample matrix",
      "offset": 19358.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "of possible item ratings. There are five",
      "offset": 19360.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "rows and five columns. And here's one",
      "offset": 19363.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "factorization of that matrix called the",
      "offset": 19365.92,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "LU",
      "offset": 19367.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "factorization. Notice that the factor",
      "offset": 19369.08,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "matrices are the same dimensions or rank",
      "offset": 19371.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "as the original matrix. Also notice that",
      "offset": 19373.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "some of the values in this factorization",
      "offset": 19376.32,
      "duration": 4.398
    },
    {
      "lang": "en",
      "text": "are negative. Using this type of",
      "offset": 19378.16,
      "duration": 4.478
    },
    {
      "lang": "en",
      "text": "factorization could result in negative",
      "offset": 19380.718,
      "duration": 3.602
    },
    {
      "lang": "en",
      "text": "predictions that wouldn't make sense in",
      "offset": 19382.638,
      "duration": 2.682
    },
    {
      "lang": "en",
      "text": "our",
      "offset": 19384.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "context. Here's another factorization.",
      "offset": 19385.32,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "In this case, all the values are",
      "offset": 19388.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "positive, meaning that the resulting",
      "offset": 19390.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "product of these factor matrices is",
      "offset": 19392.24,
      "duration": 4.478
    },
    {
      "lang": "en",
      "text": "guaranteed to be positive. This is",
      "offset": 19394.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "closer to what we need for our",
      "offset": 19396.718,
      "duration": 3.842
    },
    {
      "lang": "en",
      "text": "purposes. Notice here that the",
      "offset": 19398.68,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "dimensions of the factor matrices are",
      "offset": 19400.56,
      "duration": 4.318
    },
    {
      "lang": "en",
      "text": "such that the first factor matrix has",
      "offset": 19402.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the same number of rows as the original",
      "offset": 19404.878,
      "duration": 3.802
    },
    {
      "lang": "en",
      "text": "matrix, but a different number of",
      "offset": 19406.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "columns. Also, the second factor matrix",
      "offset": 19408.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "has the same number of columns as the",
      "offset": 19411.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "original matrix, but a different number",
      "offset": 19413.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of rows. The dimensions of the factor",
      "offset": 19415.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "matrices that don't match the original",
      "offset": 19418.24,
      "duration": 4.398
    },
    {
      "lang": "en",
      "text": "matrix are called the rank or the number",
      "offset": 19420,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "of latent features. In this case, we",
      "offset": 19422.638,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have chosen the rank of the factor",
      "offset": 19425.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "matrices to be three. What that means is",
      "offset": 19426.878,
      "duration": 5.042
    },
    {
      "lang": "en",
      "text": "that the number of latent features of",
      "offset": 19430.24,
      "duration": 4.398
    },
    {
      "lang": "en",
      "text": "the factor matrices is three. Remember",
      "offset": 19431.92,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "that as a data scientist when doing",
      "offset": 19434.638,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "these types of factorizations, you get",
      "offset": 19436.878,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to choose the rank or the number of",
      "offset": 19439.2,
      "duration": 3.518
    },
    {
      "lang": "en",
      "text": "latent features the factor matrices will",
      "offset": 19440.718,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have.",
      "offset": 19442.718,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "Now look at this matrix. Not all the",
      "offset": 19444.958,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "cells have numbers in them. Despite",
      "offset": 19447.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this, we can still factor the values in",
      "offset": 19449.84,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "the matrix. Also notice that because",
      "offset": 19451.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "there is at least one value in every row",
      "offset": 19454.638,
      "duration": 5.922
    },
    {
      "lang": "en",
      "text": "and at least one value in every column",
      "offset": 19457.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that each of the factor matrices are",
      "offset": 19460.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "totally full. Because of this, factoring",
      "offset": 19462.32,
      "duration": 6.318
    },
    {
      "lang": "en",
      "text": "a sparse matrix into two factor matrices",
      "offset": 19465.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "gives us the means to not only",
      "offset": 19468.638,
      "duration": 3.362
    },
    {
      "lang": "en",
      "text": "approximate the original values that",
      "offset": 19470.32,
      "duration": 4.318
    },
    {
      "lang": "en",
      "text": "existed in the matrix to begin with, but",
      "offset": 19472,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "also provide predictions for the cells",
      "offset": 19474.638,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "that were originally blank. And because",
      "offset": 19476.878,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "the factorization is based on the values",
      "offset": 19479.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that existed previously, the blank cells",
      "offset": 19481.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "are filled in based on those already",
      "offset": 19484.24,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "existing patterns. So when we do this",
      "offset": 19486,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "with user ratings, the blanks are filled",
      "offset": 19488.958,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in with values that reflect the",
      "offset": 19491.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "individual user behavior and the",
      "offset": 19492.798,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "behavior of users similar to them. This",
      "offset": 19495.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is why this method is called",
      "offset": 19497.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "collaborative filtering. Let's look at",
      "offset": 19499.04,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "some real life examples.",
      "offset": 19501.28,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Now that we've covered matrix",
      "offset": 19506,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "multiplication and matrix factorization,",
      "offset": 19507.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we're ready to begin exploring how ALS",
      "offset": 19510.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "uses non-gative matrix factorization to",
      "offset": 19512.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "predict how users will rate movies they",
      "offset": 19515.6,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "haven't yet seen. Let's start at the",
      "offset": 19517.52,
      "duration": 5.198
    },
    {
      "lang": "en",
      "text": "beginning. Here is a portion of a matrix",
      "offset": 19520.28,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "of users and their movie ratings. In",
      "offset": 19522.718,
      "duration": 7.562
    },
    {
      "lang": "en",
      "text": "total, there are 671 users and 9,066",
      "offset": 19525.84,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "movies. Of the 6.1 million possible",
      "offset": 19530.28,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "ratings we could have in this matrix",
      "offset": 19533.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "with this many users and movies, we only",
      "offset": 19535.2,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "have about",
      "offset": 19538,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "100,000. That means that 98% of the",
      "offset": 19539.16,
      "duration": 5.638
    },
    {
      "lang": "en",
      "text": "matrix is totally blank. This makes",
      "offset": 19541.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "sense because 9,066 movies are far too",
      "offset": 19544.798,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "many movies for any normal person to",
      "offset": 19548.16,
      "duration": 3.638
    },
    {
      "lang": "en",
      "text": "watch in their",
      "offset": 19550.638,
      "duration": 3.842
    },
    {
      "lang": "en",
      "text": "lifetime. One of the benefits of ALS is",
      "offset": 19551.798,
      "duration": 4.602
    },
    {
      "lang": "en",
      "text": "that it works well with sparse matrices",
      "offset": 19554.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like this.",
      "offset": 19556.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Now, the first thing ALS does with a",
      "offset": 19558.16,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "matrix like this is factor it into two",
      "offset": 19560.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "different matrices as you see here.",
      "offset": 19562.958,
      "duration": 4.642
    },
    {
      "lang": "en",
      "text": "Remember that factorizations like this",
      "offset": 19565.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "produce two matrices which when",
      "offset": 19567.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "multiplied back together produce an",
      "offset": 19569.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "approximation of the original matrix.",
      "offset": 19572.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "In order to get the closest",
      "offset": 19575.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "approximation of the original matrix R,",
      "offset": 19576.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "ALS first fills in the factor matrices",
      "offset": 19579.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "with random numbers and then makes",
      "offset": 19582.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "slight adjustments to the matrices one",
      "offset": 19584.48,
      "duration": 4.318
    },
    {
      "lang": "en",
      "text": "at a time until it has the best",
      "offset": 19586.56,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "approximation",
      "offset": 19588.798,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "possible. In other words, ALS holds the",
      "offset": 19590.12,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "matrix R and the matrix U constant and",
      "offset": 19593.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "makes adjustments to the matrix P. It",
      "offset": 19596.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "then multiplies the two factor matrices",
      "offset": 19599.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "together to see how far the predictions",
      "offset": 19601.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are from the original matrix using the",
      "offset": 19603.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "root mean squared error or RMSSE as an",
      "offset": 19605.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "error metric. The RMSSE basically tells",
      "offset": 19609.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you on average how far off your",
      "offset": 19612.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "predictions are from the actual values.",
      "offset": 19614.16,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "We'll talk more about this later in the",
      "offset": 19617.04,
      "duration": 4.758
    },
    {
      "lang": "en",
      "text": "course, but note that in calculating the",
      "offset": 19619,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "RMSSE, only the values that existed in",
      "offset": 19621.798,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the original matrix are considered. the",
      "offset": 19624.48,
      "duration": 5.158
    },
    {
      "lang": "en",
      "text": "missing values are not",
      "offset": 19626.878,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "considered. ALS then holds P and R",
      "offset": 19629.638,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "constant and adjusts values in the",
      "offset": 19632.638,
      "duration": 5.682
    },
    {
      "lang": "en",
      "text": "matrix U. The RMSSE is calculated again",
      "offset": 19634.638,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and ALS again switches and calculates",
      "offset": 19638.32,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the RMSSE",
      "offset": 19640.878,
      "duration": 4.242
    },
    {
      "lang": "en",
      "text": "again. ALS will continue to iterate",
      "offset": 19642.44,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "until instructed to stop at which point",
      "offset": 19645.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "ALS has the best possible approximation",
      "offset": 19648.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of the original matrix R. The beauty of",
      "offset": 19651.12,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "all this is that when the RMSSE is fully",
      "offset": 19654.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "minimized, ALS simply multiplies the",
      "offset": 19656.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "matrices back together and the blank",
      "offset": 19659.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "cells are filled in with predictions.",
      "offset": 19661.92,
      "duration": 5.638
    },
    {
      "lang": "en",
      "text": "In other words, when we take a sparse",
      "offset": 19665.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "matrix and factor it into two matrices,",
      "offset": 19667.558,
      "duration": 6.042
    },
    {
      "lang": "en",
      "text": "every rating in the original matrix must",
      "offset": 19671.36,
      "duration": 5.198
    },
    {
      "lang": "en",
      "text": "have a respective row and column full of",
      "offset": 19673.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "values in the respective factor matrices",
      "offset": 19676.558,
      "duration": 5.282
    },
    {
      "lang": "en",
      "text": "that can be multiplied back together to",
      "offset": 19679.6,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "approximate that original value. And",
      "offset": 19681.84,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "since there is at least one rating in",
      "offset": 19684.718,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "every row and at least one rating in",
      "offset": 19686.878,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "every column of the original matrix,",
      "offset": 19689.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "when ALS creates the two respective",
      "offset": 19691.84,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "factor matrices, there are values in",
      "offset": 19694,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "every cell of the two factor matrices,",
      "offset": 19696.878,
      "duration": 4.402
    },
    {
      "lang": "en",
      "text": "which allows us to then create",
      "offset": 19699.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "predictions for the previously blank",
      "offset": 19701.28,
      "duration": 3.518
    },
    {
      "lang": "en",
      "text": "spaces.",
      "offset": 19703.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "So when ALS iterates to make sure that",
      "offset": 19704.798,
      "duration": 4.322
    },
    {
      "lang": "en",
      "text": "its resulting product is as close to",
      "offset": 19707.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "those original cells as possible, the",
      "offset": 19709.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "result is that the previously blank",
      "offset": 19711.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "cells are now filled in with values that",
      "offset": 19713.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "are based on how each user has behaved",
      "offset": 19716.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "in the past relative to the behavior of",
      "offset": 19719.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "similar users. Let's jump into some",
      "offset": 19721.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "examples and see how this is done in",
      "offset": 19724.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "real",
      "offset": 19725.92,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "life. Let's talk about data preparation.",
      "offset": 19729.16,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "Data preparation will consist of two",
      "offset": 19732.638,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "things. Correct dataf frame format and",
      "offset": 19734.48,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "correct schema. First, dataf frame",
      "offset": 19737.12,
      "duration": 5.518
    },
    {
      "lang": "en",
      "text": "format. Most dataf frames you've seen",
      "offset": 19740.2,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "probably look like this with user ids in",
      "offset": 19742.638,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "one column, all the features in the",
      "offset": 19745.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "remaining columns and the values of",
      "offset": 19747.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "those features making up the contents of",
      "offset": 19749.76,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "those columns. However, many pipark",
      "offset": 19751.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "algorithms ALS included require your",
      "offset": 19754.718,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "data to be in row-based format like",
      "offset": 19757.52,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "this. The data is the same. The first",
      "offset": 19759.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "column contains user ids. But rather",
      "offset": 19762.638,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "than a different feature in each column,",
      "offset": 19765.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "column 2 contains feature names and",
      "offset": 19767.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "column 3 contains the value of that",
      "offset": 19769.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "feature for that user. So a user's data",
      "offset": 19772,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "can be spread across several rows and",
      "offset": 19775.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "rows contain no null values. Depending",
      "offset": 19777.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "on your data, you may need to convert it",
      "offset": 19780.48,
      "duration": 4.478
    },
    {
      "lang": "en",
      "text": "to this format. Now let's talk about",
      "offset": 19782.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "creating the right schema. As you see,",
      "offset": 19784.958,
      "duration": 5.122
    },
    {
      "lang": "en",
      "text": "our user ID column and our generically",
      "offset": 19787.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "named column of movie titles are",
      "offset": 19790.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "strings. PiSpark's implementation of ALS",
      "offset": 19792.2,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "can only consume user ids and movie IDs",
      "offset": 19795.2,
      "duration": 5.758
    },
    {
      "lang": "en",
      "text": "as integers. So again, you might need to",
      "offset": 19798,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "convert your data to",
      "offset": 19800.958,
      "duration": 4.402
    },
    {
      "lang": "en",
      "text": "integers. Let's walk through an example",
      "offset": 19803.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "of how to do all of this. Here's a",
      "offset": 19805.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "conventional data frame. To convert it",
      "offset": 19808,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to a long or dense matrix, we will need",
      "offset": 19810.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to use a userdefined function called",
      "offset": 19813.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "wide to long. We won't go into the",
      "offset": 19815.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "detail of how it works here, but it",
      "offset": 19818,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "turns the conventional data frame into a",
      "offset": 19819.6,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "row-based dataf frame like",
      "offset": 19821.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this. If you like access to this",
      "offset": 19823.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "function directly, a link will be",
      "offset": 19825.76,
      "duration": 3.798
    },
    {
      "lang": "en",
      "text": "provided at the end of the",
      "offset": 19827.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "course. So, we have the right dataf",
      "offset": 19829.558,
      "duration": 3.882
    },
    {
      "lang": "en",
      "text": "frame format. Let's get the right",
      "offset": 19831.6,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "schema. In order to have integer, user",
      "offset": 19833.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and movie IDs, we need to assign unique",
      "offset": 19836.878,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "integers to the user ids and to the",
      "offset": 19839.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "movie IDs. To do this, we will follow",
      "offset": 19841.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "three steps. Extract unique user IDs and",
      "offset": 19844.4,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "movie IDs. Assign each one an integer ID",
      "offset": 19847.92,
      "duration": 6.878
    },
    {
      "lang": "en",
      "text": "and rejoin these unique integer IDs back",
      "offset": 19852.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to the ratings data. Let's start with",
      "offset": 19854.798,
      "duration": 5.442
    },
    {
      "lang": "en",
      "text": "user IDs. Let's first run this query to",
      "offset": 19857.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "get all the distinct user ids into one",
      "offset": 19860.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "dataf frame and call it users. Then",
      "offset": 19862.4,
      "duration": 4.398
    },
    {
      "lang": "en",
      "text": "we'll import a method called",
      "offset": 19865.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "monotonically increasing ID which will",
      "offset": 19866.798,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "assign a unique integer to each row of",
      "offset": 19869.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "our users dataf frame. We need to be",
      "offset": 19871.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "careful when using this because it will",
      "offset": 19874.24,
      "duration": 3.638
    },
    {
      "lang": "en",
      "text": "treat each partition of data",
      "offset": 19876,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "independently meaning the same integer",
      "offset": 19877.878,
      "duration": 4.602
    },
    {
      "lang": "en",
      "text": "could be used in different partitions.",
      "offset": 19880.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "In order to get around this, we'll",
      "offset": 19882.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "convert our data into one partition",
      "offset": 19884,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "using the coales method.",
      "offset": 19885.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Also note that while the integers will",
      "offset": 19888.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "be increasing by a value of one over",
      "offset": 19890.24,
      "duration": 4.318
    },
    {
      "lang": "en",
      "text": "each row, they may not necessarily start",
      "offset": 19892.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "at one. That's not super important here.",
      "offset": 19894.558,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "What's really important is that they are",
      "offset": 19897.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "unique. So now we can create a new",
      "offset": 19899.558,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "column in our users data frame called",
      "offset": 19902.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "user int ID. Set it to monotonically",
      "offset": 19904.798,
      "duration": 5.682
    },
    {
      "lang": "en",
      "text": "increasing ID. And we will have our new",
      "offset": 19907.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "user integer IDs.",
      "offset": 19910.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Note that the monotonically increasing",
      "offset": 19913.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "ID method can be a bit tricky as the",
      "offset": 19915.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "values it provides can change as you do",
      "offset": 19918.08,
      "duration": 4.638
    },
    {
      "lang": "en",
      "text": "different things to your data set. For",
      "offset": 19920.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this reason, we've called the persist",
      "offset": 19922.718,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "method to tell Spark to keep these",
      "offset": 19924.48,
      "duration": 4.238
    },
    {
      "lang": "en",
      "text": "values the same across all subsequent",
      "offset": 19926.558,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "dataf frame",
      "offset": 19928.718,
      "duration": 3.362
    },
    {
      "lang": "en",
      "text": "operations. We'll do the same thing with",
      "offset": 19929.878,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "the movie ids. And now we have two dataf",
      "offset": 19932.08,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "frames, one with our user IDs and one",
      "offset": 19934.798,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "with our movie ids. So let's join them",
      "offset": 19936.798,
      "duration": 5.122
    },
    {
      "lang": "en",
      "text": "together along with our original data",
      "offset": 19939.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "frame on our user ID and variable",
      "offset": 19941.92,
      "duration": 6.638
    },
    {
      "lang": "en",
      "text": "columns using the join method specifying",
      "offset": 19945.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "a left join. We can be even more",
      "offset": 19948.558,
      "duration": 4.642
    },
    {
      "lang": "en",
      "text": "thorough by creating a new dataf frame",
      "offset": 19951.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "with only the columns ALS needs and",
      "offset": 19953.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "renaming our columns using the doalias",
      "offset": 19955.92,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "method which renames the column on which",
      "offset": 19958.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it is called like this. Now let's",
      "offset": 19960.798,
      "duration": 6.602
    },
    {
      "lang": "en",
      "text": "prepare some data.",
      "offset": 19963.84,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "As with many other algorithms, ALS has",
      "offset": 19968.558,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "arguments that we give it and",
      "offset": 19971.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "hyperparameters which must be tuned in",
      "offset": 19973.04,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "order to generate the best",
      "offset": 19975.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "predictions. Here's what a built out ALS",
      "offset": 19977.32,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "model looks like. Let's review each",
      "offset": 19979.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "argument and",
      "offset": 19982.32,
      "duration": 4.558
    },
    {
      "lang": "en",
      "text": "hyperparameter. The user call, item",
      "offset": 19984.2,
      "duration": 4.438
    },
    {
      "lang": "en",
      "text": "call, and rating call are",
      "offset": 19986.878,
      "duration": 4.322
    },
    {
      "lang": "en",
      "text": "straightforward. They simply tell Spark",
      "offset": 19988.638,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "which columns in your dataf frame",
      "offset": 19991.2,
      "duration": 5.518
    },
    {
      "lang": "en",
      "text": "contain the respective user ids, item",
      "offset": 19993.6,
      "duration": 4.198
    },
    {
      "lang": "en",
      "text": "ids and",
      "offset": 19996.718,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "ratings. The first ALS hyperparameter is",
      "offset": 19997.798,
      "duration": 6.522
    },
    {
      "lang": "en",
      "text": "the rank. As you already know, ALS will",
      "offset": 20000.798,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "take a matrix of ratings and will factor",
      "offset": 20004.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that matrix into two different matrices.",
      "offset": 20006.718,
      "duration": 5.042
    },
    {
      "lang": "en",
      "text": "One representing the users and the other",
      "offset": 20009.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "representing the products or items or in",
      "offset": 20011.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "our case movies. In the process of doing",
      "offset": 20014.4,
      "duration": 6.238
    },
    {
      "lang": "en",
      "text": "this, latent features are uncovered. ALS",
      "offset": 20017.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "allows you to choose the number of",
      "offset": 20020.638,
      "duration": 3.442
    },
    {
      "lang": "en",
      "text": "latent features that are created, which",
      "offset": 20022,
      "duration": 3.718
    },
    {
      "lang": "en",
      "text": "is referred to as the rank",
      "offset": 20024.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "hyperparameter, often represented by the",
      "offset": 20025.718,
      "duration": 3.602
    },
    {
      "lang": "en",
      "text": "letter",
      "offset": 20027.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "K. Your objective with the data will",
      "offset": 20029.32,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "determine the rank. If you're trying to",
      "offset": 20032.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "find meaningful groupings or categories",
      "offset": 20034.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of movies to see how similar or",
      "offset": 20036.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "different movies are, you may want to",
      "offset": 20038.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "experiment with different numbers of",
      "offset": 20040.48,
      "duration": 4.398
    },
    {
      "lang": "en",
      "text": "latent features. If you have too few or",
      "offset": 20041.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "too many latent features, the groupings",
      "offset": 20044.878,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "might be difficult to understand. So you",
      "offset": 20047.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "want to look at different numbers of",
      "offset": 20049.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "latent features and manually identify",
      "offset": 20051.04,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "what makes the most",
      "offset": 20053.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "sense. For purposes of recommendations,",
      "offset": 20054.6,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "however, the best number of latent",
      "offset": 20057.36,
      "duration": 3.438
    },
    {
      "lang": "en",
      "text": "features will be found through cross",
      "offset": 20059.36,
      "duration": 3.278
    },
    {
      "lang": "en",
      "text": "validation.",
      "offset": 20060.798,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "The number of iterations or max itter",
      "offset": 20062.638,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "simply tells ALS how many times to",
      "offset": 20065.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "iterate back and forth between the",
      "offset": 20067.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "factor matrices adjusting the values to",
      "offset": 20069.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "reduce the",
      "offset": 20072.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "RMSSE. Obviously, the higher number of",
      "offset": 20073.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "iterations, the longer it will take to",
      "offset": 20075.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "complete, and the fewer number of",
      "offset": 20077.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "iterations, the higher the risk of not",
      "offset": 20079.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "fully reducing the error. So, you'll",
      "offset": 20082,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have to determine what works best for",
      "offset": 20084.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you.",
      "offset": 20085.76,
      "duration": 3.198
    },
    {
      "lang": "en",
      "text": "Many other machine learning algorithms",
      "offset": 20087.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "have a regularization parameter often",
      "offset": 20088.958,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "called a lambda. A lambda is simply a",
      "offset": 20091.84,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "number that is added to an error metric",
      "offset": 20094.798,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to keep the algorithm from converging",
      "offset": 20096.958,
      "duration": 4.162
    },
    {
      "lang": "en",
      "text": "too quickly and overfitting to the",
      "offset": 20098.958,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "training data. The lambda for ALS and",
      "offset": 20101.12,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "PISPARC is referred to as the reggg",
      "offset": 20103.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "param. We'll talk about alpha later in",
      "offset": 20107.16,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "the course, but suffice it to say the",
      "offset": 20109.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "alpha is only used when using implicit",
      "offset": 20112,
      "duration": 5.798
    },
    {
      "lang": "en",
      "text": "ratings and not used when using explicit",
      "offset": 20114.16,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "ratings. Let's talk about the ALS",
      "offset": 20117.798,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "arguments. As mentioned previously,",
      "offset": 20120.36,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "there are several different",
      "offset": 20122.718,
      "duration": 2.642
    },
    {
      "lang": "en",
      "text": "factorizations that can be used to",
      "offset": 20123.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "factor a matrix. The one that we are",
      "offset": 20125.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "interested in is the non- negative",
      "offset": 20127.84,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "matrix",
      "offset": 20129.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "factorization. So, we set the non-gative",
      "offset": 20130.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "argument to true.",
      "offset": 20133.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "You might be familiar with the term cold",
      "offset": 20135.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "start strategy already in the context of",
      "offset": 20137.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "ALS when splitting data into test and",
      "offset": 20140,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "train sets. It's possible for a user to",
      "offset": 20142.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "have all of their ratings inadvertently",
      "offset": 20145.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "put into the test set, leaving nothing",
      "offset": 20147.28,
      "duration": 4.678
    },
    {
      "lang": "en",
      "text": "in the train set to be used for making a",
      "offset": 20149.68,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "prediction. In this case, ALS can't make",
      "offset": 20151.958,
      "duration": 5.162
    },
    {
      "lang": "en",
      "text": "meaningful predictions for that user or",
      "offset": 20154.638,
      "duration": 5.122
    },
    {
      "lang": "en",
      "text": "calculate an error metric. To avoid",
      "offset": 20157.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this, we set the cold start strategy to",
      "offset": 20159.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "drop, which tells Spark that when these",
      "offset": 20161.92,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "situations arise to not use them to",
      "offset": 20164.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "calculate the RMSSE and to only use",
      "offset": 20166.958,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "users that have ratings in both the test",
      "offset": 20169.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and training sets. We also need to tell",
      "offset": 20171.44,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "Spark whether our ratings are implicit",
      "offset": 20174.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "or explicit. We do this by setting the",
      "offset": 20176.558,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "implicit prefs argument to true or",
      "offset": 20179.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "false.",
      "offset": 20181.52,
      "duration": 3.278
    },
    {
      "lang": "en",
      "text": "Once we have a built-out model like you",
      "offset": 20183.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "see here, we can fit it to training data",
      "offset": 20184.798,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "and then generate test predictions to",
      "offset": 20187.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "see how well it performs. We can do this",
      "offset": 20189.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "by calling the fit and transform methods",
      "offset": 20191.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "as you see here. You'll do this yourself",
      "offset": 20193.92,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "in subsequent exercises. Now, it's your",
      "offset": 20196.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "turn to build some",
      "offset": 20198.958,
      "duration": 2.722
    },
    {
      "lang": "en",
      "text": "models. Up until now, we've only been",
      "offset": 20202.28,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "using sample data sets. Now, we're going",
      "offset": 20204.638,
      "duration": 5.122
    },
    {
      "lang": "en",
      "text": "to begin using actual data using the",
      "offset": 20207.2,
      "duration": 5.598
    },
    {
      "lang": "en",
      "text": "movie lens data set. This data set is",
      "offset": 20209.76,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "made available by the good people at",
      "offset": 20212.798,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "grouplands.org and contains roughly 20",
      "offset": 20215.16,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "million ratings for over 138,000 users",
      "offset": 20217.68,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "and more than 27,000 movies. In order to",
      "offset": 20221.36,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "provide you with a better learning",
      "offset": 20225.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "experience, we will achieve shorter run",
      "offset": 20226.84,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "times by using a subset of the original",
      "offset": 20229.28,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "data set, including 100,000",
      "offset": 20231.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "ratings. In addition to the ratings",
      "offset": 20234.84,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "data, grouplands.org or also provides",
      "offset": 20237.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "additional data files that include",
      "offset": 20239.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "information on movie genres and other",
      "offset": 20241.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "types of tags that movie watchers have",
      "offset": 20243.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "provided for them. We'll take what",
      "offset": 20245.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you've learned from the previous",
      "offset": 20247.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "chapters and explore the data. Prepare",
      "offset": 20249.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the data. Build out a cross validated",
      "offset": 20251.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "ALS model, generate predictions, and",
      "offset": 20253.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "assess the model's",
      "offset": 20256.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "performance. First, we'll view the data",
      "offset": 20258.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "using the dot show and dotc columns",
      "offset": 20260.32,
      "duration": 4.238
    },
    {
      "lang": "en",
      "text": "methods, as well as some other methods",
      "offset": 20262.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to understand the nature of the data",
      "offset": 20264.558,
      "duration": 3.842
    },
    {
      "lang": "en",
      "text": "set.",
      "offset": 20266.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Then we'll calculate its sparsity using",
      "offset": 20268.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this sparity formula. And then we'll",
      "offset": 20270.4,
      "duration": 4.558
    },
    {
      "lang": "en",
      "text": "assess whether further preparation is",
      "offset": 20273.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "needed in order to adequately prepare it",
      "offset": 20274.958,
      "duration": 3.162
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 20277.28,
      "duration": 3.358
    },
    {
      "lang": "en",
      "text": "ALS. If you're not familiar with the",
      "offset": 20278.12,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "term sparity, it simply provides a",
      "offset": 20280.638,
      "duration": 5.442
    },
    {
      "lang": "en",
      "text": "measure of how empty a matrix is or what",
      "offset": 20283.28,
      "duration": 5.598
    },
    {
      "lang": "en",
      "text": "percentage of the matrix is empty. In",
      "offset": 20286.08,
      "duration": 4.478
    },
    {
      "lang": "en",
      "text": "essence, this formula is simply the",
      "offset": 20288.878,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "number of ratings that a matrix contains",
      "offset": 20290.558,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "divided by the number of ratings it",
      "offset": 20293.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "could contain given the number of users",
      "offset": 20295.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and movies in the matrix. The code to",
      "offset": 20297.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "calculate sparity is pretty",
      "offset": 20300.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "straightforward. We'll simply get the",
      "offset": 20302.36,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "numerator by counting the number of",
      "offset": 20304.32,
      "duration": 4.318
    },
    {
      "lang": "en",
      "text": "ratings in the ratings data frame. Then",
      "offset": 20306,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we'll get the number of distinct users",
      "offset": 20308.638,
      "duration": 3.762
    },
    {
      "lang": "en",
      "text": "and the number of distinct items or",
      "offset": 20310.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "movies.",
      "offset": 20312.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "We'll then multiply the number of users",
      "offset": 20313.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and the number of movies together to get",
      "offset": 20315.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the denominator and simply divide the",
      "offset": 20317.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "numerator by the denominator and",
      "offset": 20320.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "subtract the result from one. Because",
      "offset": 20322.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "division in Python will return an",
      "offset": 20325.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "integer, we multiply the numerator by",
      "offset": 20327.52,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "1.0 to ensure a decimal or float is",
      "offset": 20329.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "returned. Let's go over some other",
      "offset": 20333.48,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "techniques that may or may not be new to",
      "offset": 20335.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you.",
      "offset": 20337.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "As you may already know, the distinct",
      "offset": 20339.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "method simply returns all the unique",
      "offset": 20341.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "values in a column. For example, if you",
      "offset": 20343.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "want to know how many unique users there",
      "offset": 20346.48,
      "duration": 4.398
    },
    {
      "lang": "en",
      "text": "are in a table, you could simply select",
      "offset": 20348.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the user column from the data frame,",
      "offset": 20350.878,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "then run the distinct and count methods",
      "offset": 20353.68,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "like you see here. The group by method",
      "offset": 20355.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "organizes data by the unique values of a",
      "offset": 20358.558,
      "duration": 5.202
    },
    {
      "lang": "en",
      "text": "specific column to return subtotals for",
      "offset": 20361.04,
      "duration": 5.758
    },
    {
      "lang": "en",
      "text": "those unique values. For example, if you",
      "offset": 20363.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "wanted to look at total number of",
      "offset": 20366.798,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "ratings each user has provided, you",
      "offset": 20368.24,
      "duration": 4.638
    },
    {
      "lang": "en",
      "text": "would first need to group by user ID as",
      "offset": 20370.798,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "you see here, then call the count method",
      "offset": 20372.878,
      "duration": 5.362
    },
    {
      "lang": "en",
      "text": "as you see here. With this, you could",
      "offset": 20375.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "then get the min or max or average of",
      "offset": 20378.24,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "that same",
      "offset": 20382.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "column. The filter method allows you to",
      "offset": 20383.4,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "filter out any data that doesn't meet",
      "offset": 20386,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "your specified criteria. For example, if",
      "offset": 20387.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you wanted to only consider users that",
      "offset": 20391.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "have rated at least 20 movies, you would",
      "offset": 20393.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "simply apply the same group by and count",
      "offset": 20395.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "methods and then add a filter method",
      "offset": 20397.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "specifying that the count column should",
      "offset": 20399.84,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "only include values greater than 20.",
      "offset": 20401.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Let's apply what you've learned to a",
      "offset": 20404.558,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "real data",
      "offset": 20406,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "set. Remember from the last chapter, you",
      "offset": 20407.638,
      "duration": 4.442
    },
    {
      "lang": "en",
      "text": "built out a model on the ratings data",
      "offset": 20410.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "set and the code looked like this. Now,",
      "offset": 20412.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the RMSSE that you got was lower than",
      "offset": 20415.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the 1.45 45 shown here. But what if you",
      "offset": 20417.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "went through this whole process and got",
      "offset": 20420.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "an error metric that you weren't",
      "offset": 20421.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "satisfied with like this RMSSE of",
      "offset": 20423.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "1.45? You might want to try other",
      "offset": 20426.12,
      "duration": 4.438
    },
    {
      "lang": "en",
      "text": "combinations of hyperparameter values to",
      "offset": 20428.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "try and reduce that. Spark makes it easy",
      "offset": 20430.558,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "to do this by using two additional tools",
      "offset": 20433.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "called the param grid builder and the",
      "offset": 20435.52,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "cross validator. These tools will allow",
      "offset": 20437.6,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "you to try many different hyperparameter",
      "offset": 20440.638,
      "duration": 4.362
    },
    {
      "lang": "en",
      "text": "values and have Spark identify the best",
      "offset": 20442.558,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "combination. Let's talk about how to use",
      "offset": 20445,
      "duration": 5.638
    },
    {
      "lang": "en",
      "text": "them. The param grid builder tells Spark",
      "offset": 20447.798,
      "duration": 4.522
    },
    {
      "lang": "en",
      "text": "all the hyperparameter values you want",
      "offset": 20450.638,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "it to try. To do this, we first import",
      "offset": 20452.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the param grid builder package,",
      "offset": 20455.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "instantiate a param grid builder, and",
      "offset": 20457.36,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "give it a name. We'll call it param",
      "offset": 20459.92,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "grid. We then add each hyperparameter",
      "offset": 20462.2,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "name by calling the add grid method on",
      "offset": 20464.798,
      "duration": 5.362
    },
    {
      "lang": "en",
      "text": "our ls algorithm and hyperparameter name",
      "offset": 20467.04,
      "duration": 5.838
    },
    {
      "lang": "en",
      "text": "as you see here. Notice the empty list",
      "offset": 20470.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to the right of the hyperparameter",
      "offset": 20472.878,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "names. This is where we input the values",
      "offset": 20474.24,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "we want Spark to try for each",
      "offset": 20476.558,
      "duration": 4.282
    },
    {
      "lang": "en",
      "text": "hyperparameter like",
      "offset": 20478.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "this. Once we've added all of this, we",
      "offset": 20480.84,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "call the build method to complete the",
      "offset": 20483.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "build on our param grid. Now, let's look",
      "offset": 20485.44,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "at the cross",
      "offset": 20488,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "validator. The cross validator",
      "offset": 20489.08,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "essentially fits a model to several",
      "offset": 20491.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "different portions of our training data",
      "offset": 20493.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "set called folds and then generates",
      "offset": 20495.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "predictions for each respective hold out",
      "offset": 20497.6,
      "duration": 4.358
    },
    {
      "lang": "en",
      "text": "portion of the data set to see how it",
      "offset": 20499.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "performs. To properly use the cross",
      "offset": 20501.958,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "validator, we first import the cross",
      "offset": 20504.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "validator package, instantiate a cross",
      "offset": 20506.718,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "validator, and give it a name. We'll",
      "offset": 20509.44,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "call it CV",
      "offset": 20511.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "here. We then tell it to use our ALS",
      "offset": 20513,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "model as an estimator by setting the",
      "offset": 20515.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "estimator argument equal to the name of",
      "offset": 20518.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "our model, which is ALS.",
      "offset": 20520.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "We'll then set the estimator param maps",
      "offset": 20523.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to our param grid that we built so that",
      "offset": 20526,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Spark knows what values to try as it",
      "offset": 20528,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "works to identify the best combination",
      "offset": 20530.08,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 20531.76,
      "duration": 3.118
    },
    {
      "lang": "en",
      "text": "hyperparameters. Then we provide the",
      "offset": 20533,
      "duration": 3.958
    },
    {
      "lang": "en",
      "text": "name of our evaluator so it knows how to",
      "offset": 20534.878,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "measure each model's performance. We do",
      "offset": 20536.958,
      "duration": 4.242
    },
    {
      "lang": "en",
      "text": "this by simply setting the evaluator",
      "offset": 20539.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "argument to the name of our evaluator",
      "offset": 20541.2,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "which is",
      "offset": 20543.36,
      "duration": 3.518
    },
    {
      "lang": "en",
      "text": "evaluator. We finish by setting the",
      "offset": 20544.76,
      "duration": 4.118
    },
    {
      "lang": "en",
      "text": "numfolds argument to the number of times",
      "offset": 20546.878,
      "duration": 4.162
    },
    {
      "lang": "en",
      "text": "we want Spark to test each model on the",
      "offset": 20548.878,
      "duration": 5.602
    },
    {
      "lang": "en",
      "text": "training data. in this case five times.",
      "offset": 20551.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Let's go over how to integrate these",
      "offset": 20554.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "into a full code buildout. We'll first",
      "offset": 20556.08,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "split our data into training and test",
      "offset": 20559.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "sets using the random split method. And",
      "offset": 20560.798,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "we'll build a generic ALS model without",
      "offset": 20563.52,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "any",
      "offset": 20565.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "hyperparameters. Only the model",
      "offset": 20566.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "arguments as you see here. The cross",
      "offset": 20568.48,
      "duration": 5.078
    },
    {
      "lang": "en",
      "text": "validator will take care of the",
      "offset": 20571.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "hyperparameters. We'll build our param",
      "offset": 20573.558,
      "duration": 3.642
    },
    {
      "lang": "en",
      "text": "grid builder so Spark knows what",
      "offset": 20575.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "hyperparameter values to test.",
      "offset": 20577.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "We'll create an evaluator so Spark knows",
      "offset": 20579.76,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "how to evaluate each",
      "offset": 20581.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model. Then the cross validator will",
      "offset": 20583.32,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "tell Spark the algorithm, the",
      "offset": 20585.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "hyperparameters and values, and the",
      "offset": 20587.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "evaluator to use to find the best model",
      "offset": 20589.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and the number of training set folds we",
      "offset": 20592.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "want each model to be tested on. We then",
      "offset": 20594.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "fit our cross validator on the training",
      "offset": 20597.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "data to have Spark try all the",
      "offset": 20599.2,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "combinations of hyperparameters we",
      "offset": 20601.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "specified by calling the CV.fit method",
      "offset": 20603.08,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "on the training data. Once it's finished",
      "offset": 20606.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "running, we extract the best performing",
      "offset": 20609.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "model by calling the best model method",
      "offset": 20611.6,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "on our model. We'll call this our best",
      "offset": 20613.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "model. And with it, we can generate",
      "offset": 20616.638,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "predictions on the test set. Print the",
      "offset": 20619.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "error metric and the respective",
      "offset": 20621.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "hyperparameter values using the code you",
      "offset": 20623.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "see here. And now we have our cross",
      "offset": 20625.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "validated model. Let's build a real",
      "offset": 20627.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "model on a real data",
      "offset": 20630.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "set. Congratulations.",
      "offset": 20634.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "You just built your first cross",
      "offset": 20636.558,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "validated ALS model. Now let's determine",
      "offset": 20638,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "whether the model suits your needs or",
      "offset": 20641.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "not. The primary way to do this is to",
      "offset": 20642.84,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "examine the error metric. In this case,",
      "offset": 20645.44,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 20647.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "RMSSE. The RMSSE or root mean squared",
      "offset": 20648.36,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "error tells us on average how far a",
      "offset": 20651.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "given prediction is from its",
      "offset": 20654.32,
      "duration": 4.478
    },
    {
      "lang": "en",
      "text": "corresponding actual value. It's pretty",
      "offset": 20655.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "straightforward. If we have predictions",
      "offset": 20658.798,
      "duration": 5.122
    },
    {
      "lang": "en",
      "text": "and actual values, the RMSSE subtracts",
      "offset": 20661.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the actual value from the prediction,",
      "offset": 20663.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then squares those differences to make",
      "offset": 20666.48,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "them",
      "offset": 20668.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "positive. It then sums those",
      "offset": 20669,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "differences, takes the average by",
      "offset": 20671.24,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "dividing by the number of observations,",
      "offset": 20673.36,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "in this case, n= 4. And it then takes",
      "offset": 20675.28,
      "duration": 5.438
    },
    {
      "lang": "en",
      "text": "the square root to undo the squaring of",
      "offset": 20678.638,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "the values that we did previously.",
      "offset": 20680.718,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "So if we have an RMSSE of 61, then on",
      "offset": 20683.44,
      "duration": 5.358
    },
    {
      "lang": "en",
      "text": "average our predictions are either 61",
      "offset": 20686.558,
      "duration": 5.122
    },
    {
      "lang": "en",
      "text": "above or below the original rating.",
      "offset": 20688.798,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "Another way to evaluate our model is to",
      "offset": 20691.68,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "look at its",
      "offset": 20693.52,
      "duration": 3.278
    },
    {
      "lang": "en",
      "text": "recommendations. Remember, however, that",
      "offset": 20694.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "ALS is often used to identify patterns",
      "offset": 20696.798,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "and uncover latent features that are",
      "offset": 20699.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "unobservable by humans. Meaning that ALS",
      "offset": 20701.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "can sometimes see things that may not",
      "offset": 20704.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "initially make sense to us as humans.",
      "offset": 20706.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Bear this in mind as you move",
      "offset": 20709.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "forward. To generate recommendations, we",
      "offset": 20711,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "will use the native Spark function",
      "offset": 20713.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "recommend for all users, which generates",
      "offset": 20715.2,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the top recommendations for all",
      "offset": 20717.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "users. ALS recommendation output has two",
      "offset": 20720.28,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "challenges that need to be addressed.",
      "offset": 20723.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "The first is that it is in a format like",
      "offset": 20725.92,
      "duration": 4.638
    },
    {
      "lang": "en",
      "text": "this, which is perfectly usable in",
      "offset": 20728.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "PISpark, but isn't very human readable.",
      "offset": 20730.558,
      "duration": 5.362
    },
    {
      "lang": "en",
      "text": "To resolve this, we save the dataf frame",
      "offset": 20734,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "as a temporary table and use this SQL",
      "offset": 20735.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "query to make it",
      "offset": 20738.718,
      "duration": 3.442
    },
    {
      "lang": "en",
      "text": "readable. The explode command",
      "offset": 20740.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "essentially takes an array like our",
      "offset": 20742.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "recommendation column and separates each",
      "offset": 20743.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "item within it like",
      "offset": 20746.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this. Notice that only one movie ID and",
      "offset": 20748.12,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "its respective recommendation value for",
      "offset": 20751.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "each user is contained on each line",
      "offset": 20753.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "where previously all recommendations for",
      "offset": 20756.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "a given user were contained on one line.",
      "offset": 20758.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Also notice that ALS conveniently",
      "offset": 20761.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "includes the movie ID and rating column",
      "offset": 20763.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "names with each value on each line. This",
      "offset": 20766.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "makes it easy to separate them into",
      "offset": 20769.52,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "different",
      "offset": 20771.28,
      "duration": 3.598
    },
    {
      "lang": "en",
      "text": "columns. Adding the lateral view to the",
      "offset": 20772.44,
      "duration": 4.518
    },
    {
      "lang": "en",
      "text": "explode function allows us to treat the",
      "offset": 20774.878,
      "duration": 5.362
    },
    {
      "lang": "en",
      "text": "exploded column as a table and extract",
      "offset": 20776.958,
      "duration": 5.162
    },
    {
      "lang": "en",
      "text": "the individual values as separate",
      "offset": 20780.24,
      "duration": 4.558
    },
    {
      "lang": "en",
      "text": "columns. We first name the lateral view.",
      "offset": 20782.12,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "In this case, we call it exploded table",
      "offset": 20784.798,
      "duration": 4.322
    },
    {
      "lang": "en",
      "text": "and then give it a formal table name",
      "offset": 20787.36,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "which we will call movie ids and",
      "offset": 20789.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "ratings. This allows us to select the",
      "offset": 20791.24,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "user ID and then get the movie ID and",
      "offset": 20793.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "ratings by referencing the movie IDs and",
      "offset": 20797.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "ratings table in the beginning of our",
      "offset": 20799.68,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "query. The output is now",
      "offset": 20801.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "readable. And if we join it to the",
      "offset": 20804.28,
      "duration": 4.598
    },
    {
      "lang": "en",
      "text": "original movie information, we can see",
      "offset": 20806.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "what's going on even better. The other",
      "offset": 20808.878,
      "duration": 4.482
    },
    {
      "lang": "en",
      "text": "challenge with these recommendations is",
      "offset": 20811.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that they include predictions for movies",
      "offset": 20813.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that have already been seen. Remember",
      "offset": 20815.2,
      "duration": 5.518
    },
    {
      "lang": "en",
      "text": "how ALS creates two factor matrices that",
      "offset": 20818.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "are multiplied together to produce an",
      "offset": 20820.718,
      "duration": 3.562
    },
    {
      "lang": "en",
      "text": "approximation of the original ratings",
      "offset": 20822.48,
      "duration": 4.238
    },
    {
      "lang": "en",
      "text": "matrix? That's essentially what the ALS",
      "offset": 20824.28,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "output is, including all movies for all",
      "offset": 20826.718,
      "duration": 5.522
    },
    {
      "lang": "en",
      "text": "users, whether they've seen them or not.",
      "offset": 20829.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "A simple way to address this is to",
      "offset": 20832.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "filter out the movies that have already",
      "offset": 20834,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "been seen. Since we already have our",
      "offset": 20835.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "clean recommendations and the original",
      "offset": 20838.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "movie ratings, we can simply join these",
      "offset": 20841.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "two data frames together on user ID and",
      "offset": 20843.52,
      "duration": 5.358
    },
    {
      "lang": "en",
      "text": "movie ID using a left join. The movies",
      "offset": 20845.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that have already been seen are those",
      "offset": 20848.878,
      "duration": 3.122
    },
    {
      "lang": "en",
      "text": "that have a rating from the original",
      "offset": 20850.48,
      "duration": 4.158
    },
    {
      "lang": "en",
      "text": "movie ratings data frame. So if we",
      "offset": 20852,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "simply add a filter so that the rating",
      "offset": 20854.638,
      "duration": 4.402
    },
    {
      "lang": "en",
      "text": "column of the movie ratings data frame",
      "offset": 20857.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is null, we'll only have predictions for",
      "offset": 20859.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "movies that the individual users have",
      "offset": 20861.6,
      "duration": 7.038
    },
    {
      "lang": "en",
      "text": "not seen. Now let's evaluate your model.",
      "offset": 20863.76,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "By now, you should be pretty comfortable",
      "offset": 20870,
      "duration": 4.558
    },
    {
      "lang": "en",
      "text": "with ALS. So far, we've only used",
      "offset": 20871.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "explicit ratings. In most real life",
      "offset": 20874.558,
      "duration": 4.642
    },
    {
      "lang": "en",
      "text": "situations, however, explicit ratings",
      "offset": 20877.2,
      "duration": 3.518
    },
    {
      "lang": "en",
      "text": "aren't available, and you'll have to get",
      "offset": 20879.2,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "creative in building these types of",
      "offset": 20880.718,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "models. One way to get around this is to",
      "offset": 20882.36,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "use implicit ratings. Remember that",
      "offset": 20884.798,
      "duration": 4.402
    },
    {
      "lang": "en",
      "text": "while explicit ratings are explicitly",
      "offset": 20887.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "provided by users in various forms,",
      "offset": 20889.2,
      "duration": 5.518
    },
    {
      "lang": "en",
      "text": "implicit ratings are data used to infer",
      "offset": 20891.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "ratings. For example, if a news website",
      "offset": 20894.718,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "sees that in the last month you clicked",
      "offset": 20897.6,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "on 21 geopolitical articles and only one",
      "offset": 20899.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "local news article, ALS can convert",
      "offset": 20902.638,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "these numbers into scores indicating how",
      "offset": 20905.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "confident it is that you like them. This",
      "offset": 20907.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "approach assumes that the more you do",
      "offset": 20910.4,
      "duration": 4.558
    },
    {
      "lang": "en",
      "text": "something, the more you prefer it. ALS",
      "offset": 20912,
      "duration": 4.638
    },
    {
      "lang": "en",
      "text": "can use these confidence ratings to",
      "offset": 20914.958,
      "duration": 3.202
    },
    {
      "lang": "en",
      "text": "generate recommendations and you're",
      "offset": 20916.638,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "going to learn how to do this. First,",
      "offset": 20918.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "let's talk about the data set you will",
      "offset": 20920.638,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "be using. The data set this time comes",
      "offset": 20922.08,
      "duration": 4.478
    },
    {
      "lang": "en",
      "text": "from the million songs data set",
      "offset": 20924.958,
      "duration": 3.882
    },
    {
      "lang": "en",
      "text": "available from Lab Rosa at Columbia",
      "offset": 20926.558,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "University. You're going to be using one",
      "offset": 20928.84,
      "duration": 3.878
    },
    {
      "lang": "en",
      "text": "file of this data set called the Echo",
      "offset": 20930.878,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "Nest Taste Profile data set. It contains",
      "offset": 20932.718,
      "duration": 5.282
    },
    {
      "lang": "en",
      "text": "information on over 1 million users,",
      "offset": 20935.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "including the number of times they've",
      "offset": 20938,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "played nearly 400,000",
      "offset": 20939.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "songs. This is more data than we can use",
      "offset": 20941.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "for this course, so we will only be",
      "offset": 20943.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "using a portion of it. We'll first",
      "offset": 20945.44,
      "duration": 4.278
    },
    {
      "lang": "en",
      "text": "examine the data, get summary",
      "offset": 20947.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "statistics, and then build and evaluate",
      "offset": 20949.718,
      "duration": 3.642
    },
    {
      "lang": "en",
      "text": "our model.",
      "offset": 20951.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "One thing to note here is that because",
      "offset": 20953.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the use of implicit ratings causes ALS",
      "offset": 20955.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to calculate a level of confidence that",
      "offset": 20957.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a user likes a song based on the number",
      "offset": 20959.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of times they played it, a matrix will",
      "offset": 20962,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "need to include zeros for the songs that",
      "offset": 20964.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "each user has not yet listen to. In case",
      "offset": 20966.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "your data doesn't already include the",
      "offset": 20969.36,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "zeros, we'll walk through how to do",
      "offset": 20971.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this. Let's say we have a ratings data",
      "offset": 20973.48,
      "duration": 5.158
    },
    {
      "lang": "en",
      "text": "frame like this. You can use the dot",
      "offset": 20975.84,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "distinct method to extract the unique",
      "offset": 20978.638,
      "duration": 5.442
    },
    {
      "lang": "en",
      "text": "user ids and song ids like this. You can",
      "offset": 20980.718,
      "duration": 5.602
    },
    {
      "lang": "en",
      "text": "then perform a cross join which joins",
      "offset": 20984.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "each user to each song like this. Notice",
      "offset": 20986.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that the three users and three songs we",
      "offset": 20989.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "originally had now create nine unique",
      "offset": 20992.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "pairs. Using a left join, you can take",
      "offset": 20994.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "that cross join table and join it with",
      "offset": 20997.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the original ratings to get the num",
      "offset": 20999.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "plays column. Notice it joins on both",
      "offset": 21001.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "user ID and song ID. And because we want",
      "offset": 21004.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "zeros in place of the null values so",
      "offset": 21007.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that every user has a value for every",
      "offset": 21009.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "song, we simply call the fillna method",
      "offset": 21011.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "telling spark to fill the null values",
      "offset": 21015.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "with zero. And you have your final",
      "offset": 21017.04,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "product to feed to",
      "offset": 21019.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "ALS. Here are those steps in a nice",
      "offset": 21020.68,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "clean function. Let's do this with our",
      "offset": 21023.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "million songs data",
      "offset": 21025.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "set. Now that we have an implicit",
      "offset": 21028.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "ratings data set, let's discuss these",
      "offset": 21031.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "types of models. The first thing you",
      "offset": 21033.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "should know is that implicit ratings",
      "offset": 21036.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "models have an additional hyperparameter",
      "offset": 21037.76,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "called alpha. Alpha is an integer value",
      "offset": 21039.68,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "that tells Spark how much each",
      "offset": 21042.798,
      "duration": 4.242
    },
    {
      "lang": "en",
      "text": "additional song play should add to the",
      "offset": 21044.558,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "model's confidence that a user actually",
      "offset": 21047.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "likes a",
      "offset": 21049.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "song. Like the other hyperparameters,",
      "offset": 21050.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "this will need to be tuned through cross",
      "offset": 21053.92,
      "duration": 4.638
    },
    {
      "lang": "en",
      "text": "validation. The challenge of these",
      "offset": 21056.52,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "models is the evaluation.",
      "offset": 21058.558,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "With explicit ratings, we used the",
      "offset": 21061.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "RMSSE. It made sense in that situation",
      "offset": 21063.638,
      "duration": 4.362
    },
    {
      "lang": "en",
      "text": "because we could match predictions back",
      "offset": 21066.32,
      "duration": 4.558
    },
    {
      "lang": "en",
      "text": "to a true measure of user preference. In",
      "offset": 21068,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the case of implicit ratings, however,",
      "offset": 21070.878,
      "duration": 4.202
    },
    {
      "lang": "en",
      "text": "we don't have a true measure of user",
      "offset": 21073.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "preference. We only have the number of",
      "offset": 21075.08,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "times a user listened to a song and a",
      "offset": 21077.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "measure of how confident our model is",
      "offset": 21079.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that they like that song. These aren't",
      "offset": 21081.68,
      "duration": 5.278
    },
    {
      "lang": "en",
      "text": "the same thing, and calculating an RMSSE",
      "offset": 21084.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "between them doesn't make sense.",
      "offset": 21086.958,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "However, using a test set, we can see if",
      "offset": 21089.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "our model is giving high predictions to",
      "offset": 21091.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the songs that users have actually",
      "offset": 21093.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "listened to. The logic here is that if",
      "offset": 21095.44,
      "duration": 5.358
    },
    {
      "lang": "en",
      "text": "our model is returning a high prediction",
      "offset": 21098.24,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "for a song that the respective user has",
      "offset": 21100.798,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "actually listened to, then the",
      "offset": 21102.958,
      "duration": 4.242
    },
    {
      "lang": "en",
      "text": "predictions make sense, especially if",
      "offset": 21104.718,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "they've listened to it more than once.",
      "offset": 21107.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "We can measure this using this rank",
      "offset": 21109.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "order error metric or ROEM.",
      "offset": 21111.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "In essence, this metric checks to see if",
      "offset": 21115.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "songs with higher numbers of plays have",
      "offset": 21117.84,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "higher",
      "offset": 21120.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "predictions. For example, here's a set",
      "offset": 21121.4,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "of bad predictions. The percent rank",
      "offset": 21124.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "column has ranked the predictions for",
      "offset": 21127.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "each individual user such that the",
      "offset": 21129.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "lowest prediction is in the highest",
      "offset": 21131.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "percentile and the highest prediction is",
      "offset": 21133.44,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "in the lowest",
      "offset": 21135.68,
      "duration": 3.278
    },
    {
      "lang": "en",
      "text": "percentile. Notice that these bad",
      "offset": 21136.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "predictions include low predictions and",
      "offset": 21138.958,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "high predictions for songs with more",
      "offset": 21141.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "than one play. indicating that the",
      "offset": 21143.52,
      "duration": 4.438
    },
    {
      "lang": "en",
      "text": "predictions may not be any better than",
      "offset": 21145.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "random. If we multiply the number of",
      "offset": 21147.958,
      "duration": 5.322
    },
    {
      "lang": "en",
      "text": "plays by the percent rank, we get this",
      "offset": 21150.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "NP rank",
      "offset": 21153.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "column. When we sum that column, we get",
      "offset": 21154.84,
      "duration": 5.718
    },
    {
      "lang": "en",
      "text": "our ROEM numerator and the sum of the",
      "offset": 21157.52,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "num plays column gives us our ROE",
      "offset": 21160.558,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "denominator. Using these, we can",
      "offset": 21163.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "calculate our ROEM to be",
      "offset": 21165.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "0.556. Values close to.5 indicate that",
      "offset": 21168.68,
      "duration": 5.878
    },
    {
      "lang": "en",
      "text": "they aren't much better than random. If",
      "offset": 21171.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we were to look at good predictions",
      "offset": 21174.558,
      "duration": 3.282
    },
    {
      "lang": "en",
      "text": "where the model gave high predictions to",
      "offset": 21175.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "songs that had more than one play, they",
      "offset": 21177.84,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "might look like",
      "offset": 21180,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this. Notice that songs that have been",
      "offset": 21181.08,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "played have high ratings indicating that",
      "offset": 21183.52,
      "duration": 5.198
    },
    {
      "lang": "en",
      "text": "the predictions are better than random,",
      "offset": 21185.92,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "which subsequently gives us an ROM of",
      "offset": 21188.718,
      "duration": 6.642
    },
    {
      "lang": "en",
      "text": "0.111. This is much closer to zero,",
      "offset": 21192.68,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "which is where we want to be.",
      "offset": 21195.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Unfortunately, Spark hasn't implemented",
      "offset": 21198.16,
      "duration": 4.638
    },
    {
      "lang": "en",
      "text": "an evaluator metric like this, so you'll",
      "offset": 21200.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "need to build it manually. An ROEM",
      "offset": 21202.798,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "function will be provided to you in",
      "offset": 21205.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "subsequent exercises. And for your",
      "offset": 21207.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "reference, the code to build it is",
      "offset": 21209.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "provided at the end of this course.",
      "offset": 21211.28,
      "duration": 5.358
    },
    {
      "lang": "en",
      "text": "Using this function and a for loop, you",
      "offset": 21213.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "can build several models as you see",
      "offset": 21216.638,
      "duration": 4.282
    },
    {
      "lang": "en",
      "text": "here, each with different hyperparameter",
      "offset": 21218.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "values. You'll want to create a model",
      "offset": 21220.92,
      "duration": 3.958
    },
    {
      "lang": "en",
      "text": "for each combination of hyperparameter",
      "offset": 21223.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "values that you'll want to try.",
      "offset": 21224.878,
      "duration": 4.162
    },
    {
      "lang": "en",
      "text": "You can then fit each one to the",
      "offset": 21227.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "training data, extract each model's test",
      "offset": 21229.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "predictions, and then calculate the ROE",
      "offset": 21231.44,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "for each one. This is a simplified way",
      "offset": 21233.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to do this. Full cross validation is",
      "offset": 21236.558,
      "duration": 5.202
    },
    {
      "lang": "en",
      "text": "imperative to building good models. It",
      "offset": 21239.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is beyond the scope of this course to",
      "offset": 21241.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "teach how to code a function that",
      "offset": 21243.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "manually cross validates and evaluates",
      "offset": 21245.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "models like this. But doing so should be",
      "offset": 21247.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "done and code to do so is provided at",
      "offset": 21249.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the end of this course. Now, let's put",
      "offset": 21251.92,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "this into practice.",
      "offset": 21254,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "So far, we've covered situations when",
      "offset": 21258.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you have explicit ratings and when you",
      "offset": 21260.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "have implicit ratings from user behavior",
      "offset": 21262.48,
      "duration": 4.078
    },
    {
      "lang": "en",
      "text": "counts. Now, we're going to cover the",
      "offset": 21264.44,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "situation when you might not even have",
      "offset": 21266.558,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "behavior counts. In some situations, you",
      "offset": 21268,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "may only have binary data that tells you",
      "offset": 21271.12,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "whether a user has or has not taken an",
      "offset": 21273.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "action with no indication of how many",
      "offset": 21275.32,
      "duration": 5.238
    },
    {
      "lang": "en",
      "text": "times they've done so. To go back to the",
      "offset": 21277.76,
      "duration": 4.878
    },
    {
      "lang": "en",
      "text": "movie example, if you know whether",
      "offset": 21280.558,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "customers have watched certain movies,",
      "offset": 21282.638,
      "duration": 3.522
    },
    {
      "lang": "en",
      "text": "but don't have information on how many",
      "offset": 21284.558,
      "duration": 3.442
    },
    {
      "lang": "en",
      "text": "times or how much they actually liked",
      "offset": 21286.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "them, you could simply feed binary data",
      "offset": 21288,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "to ALS that indicates which customers",
      "offset": 21290.4,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "have watched each movie and which ones",
      "offset": 21292.718,
      "duration": 5.122
    },
    {
      "lang": "en",
      "text": "haven't. ALS can still pull signal from",
      "offset": 21295.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "this type of data and make meaningful",
      "offset": 21297.84,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "predictions. When taking this approach,",
      "offset": 21300.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the data will look like this. Notice",
      "offset": 21302.958,
      "duration": 4.642
    },
    {
      "lang": "en",
      "text": "that all ratings are either a one or a",
      "offset": 21305.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "zero. We must treat binary ratings like",
      "offset": 21307.6,
      "duration": 5.198
    },
    {
      "lang": "en",
      "text": "these as implicit ratings. If we treated",
      "offset": 21310.24,
      "duration": 4.718
    },
    {
      "lang": "en",
      "text": "them like explicit ratings and didn't",
      "offset": 21312.798,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "include the zeros, the best performing",
      "offset": 21314.958,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "model would simply predict one for",
      "offset": 21316.958,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "everything and deliver a deceivingly",
      "offset": 21318.718,
      "duration": 4.522
    },
    {
      "lang": "en",
      "text": "ideal RMSSE of",
      "offset": 21320.878,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "zero. Also, as with our previous million",
      "offset": 21323.24,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "songs model, we can't use the RMSSE as a",
      "offset": 21326.558,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "model evaluation metric. Ultimately,",
      "offset": 21329.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "when our machine learning process holds",
      "offset": 21332.558,
      "duration": 4.562
    },
    {
      "lang": "en",
      "text": "out random observations in the test set,",
      "offset": 21334.4,
      "duration": 4.318
    },
    {
      "lang": "en",
      "text": "we want our model to generate high",
      "offset": 21337.12,
      "duration": 3.438
    },
    {
      "lang": "en",
      "text": "predictions for those movies that users",
      "offset": 21338.718,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have actually watched. For this reason,",
      "offset": 21340.558,
      "duration": 4.762
    },
    {
      "lang": "en",
      "text": "we'll use our ROEM metric.",
      "offset": 21342.958,
      "duration": 4.802
    },
    {
      "lang": "en",
      "text": "Again, we'll apply the same concepts",
      "offset": 21345.32,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "we've covered previously on this binary",
      "offset": 21347.76,
      "duration": 4.958
    },
    {
      "lang": "en",
      "text": "data set. The convenience of using the",
      "offset": 21350.16,
      "duration": 4.558
    },
    {
      "lang": "en",
      "text": "movie lens data set is that we can see",
      "offset": 21352.718,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "how our binary model performs against",
      "offset": 21354.718,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the original true preference ratings of",
      "offset": 21356.798,
      "duration": 5.122
    },
    {
      "lang": "en",
      "text": "the original movie lens data set.",
      "offset": 21358.798,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "One word about binary models. While it's",
      "offset": 21361.92,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "perfectly feasible to feed binary data",
      "offset": 21364.798,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "like this into ALS and get meaningful",
      "offset": 21366.958,
      "duration": 5.042
    },
    {
      "lang": "en",
      "text": "recommendations, the data does have sort",
      "offset": 21369.718,
      "duration": 3.962
    },
    {
      "lang": "en",
      "text": "of a class imbalance where the vast",
      "offset": 21372,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "majority of ratings are zeros with a",
      "offset": 21373.68,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "small percentage of",
      "offset": 21376,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "ones. Since implicit ratings models use",
      "offset": 21377.48,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "customized error metrics like ROEM and",
      "offset": 21380.798,
      "duration": 5.282
    },
    {
      "lang": "en",
      "text": "not RMSSE, the class imbalance doesn't",
      "offset": 21383.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "really pose a problem like it might in",
      "offset": 21386.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "classification problems.",
      "offset": 21388,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "ALS can still generate meaningful",
      "offset": 21390.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "recommendations from this type of data,",
      "offset": 21392.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "but there are strategies that can be",
      "offset": 21394.32,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "taken with the data to try and improve",
      "offset": 21395.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "recommendations. For example, rather",
      "offset": 21398.92,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "than treat unseen movies purely as",
      "offset": 21401.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "zeros, you can weight them higher if",
      "offset": 21403.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "more people have seen them. This assumes",
      "offset": 21405.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that if many people have seen a movie,",
      "offset": 21408,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it must be a pretty good movie and",
      "offset": 21410,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "therefore should be treated with a",
      "offset": 21412.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "little more weight and vice versa. This",
      "offset": 21413.52,
      "duration": 5.438
    },
    {
      "lang": "en",
      "text": "is called item waiting. Likewise, you",
      "offset": 21416.16,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "could weight movies by individual user",
      "offset": 21418.958,
      "duration": 4.642
    },
    {
      "lang": "en",
      "text": "behavior. For example, if a user has",
      "offset": 21421.08,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "seen lots of movies, you could weight",
      "offset": 21423.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "their unseen movies lower, assuming that",
      "offset": 21425.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "if a user has seen lots of movies, they",
      "offset": 21428.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "know what they like and have",
      "offset": 21431.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "deliberately chosen not to view the",
      "offset": 21432.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "movies they haven't seen and therefore",
      "offset": 21434,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "those movies deserve a lower waiting.",
      "offset": 21436.24,
      "duration": 4.478
    },
    {
      "lang": "en",
      "text": "While these methods are applicable,",
      "offset": 21438.798,
      "duration": 3.442
    },
    {
      "lang": "en",
      "text": "their methods haven't been implemented",
      "offset": 21440.718,
      "duration": 3.682
    },
    {
      "lang": "en",
      "text": "into the PiSpark framework and therefore",
      "offset": 21442.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "require a lot of manual work which is",
      "offset": 21444.4,
      "duration": 4.238
    },
    {
      "lang": "en",
      "text": "beyond the scope of this course.",
      "offset": 21446.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "However, if you'd like to learn more",
      "offset": 21448.638,
      "duration": 3.282
    },
    {
      "lang": "en",
      "text": "about these types of approaches, you can",
      "offset": 21450.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "read the paper referenced at the end of",
      "offset": 21451.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this course. Let's build a binary",
      "offset": 21453.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "ratings",
      "offset": 21456,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "model. Congratulations. You've now",
      "offset": 21459.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "completed this course on building",
      "offset": 21462.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "collaborative filtering recommendation",
      "offset": 21463.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "engines in Pispark.",
      "offset": 21465.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "We've covered a number of things from",
      "offset": 21467.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "why these are important to matrix",
      "offset": 21469.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "multiplication and factorization and",
      "offset": 21471.76,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "latent features. But most importantly,",
      "offset": 21474.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you've learned how to build and",
      "offset": 21476.798,
      "duration": 3.042
    },
    {
      "lang": "en",
      "text": "interpret recommendation engines with",
      "offset": 21478.08,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "three different types of data. Explicit",
      "offset": 21479.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "ratings, implicit ratings using user",
      "offset": 21482.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "behavior counts, and implicit ratings",
      "offset": 21485.44,
      "duration": 5.438
    },
    {
      "lang": "en",
      "text": "using binary user behavior. With this",
      "offset": 21487.76,
      "duration": 4.798
    },
    {
      "lang": "en",
      "text": "information, you'll be well prepared to",
      "offset": 21490.878,
      "duration": 3.042
    },
    {
      "lang": "en",
      "text": "build a collaborative filtering",
      "offset": 21492.558,
      "duration": 3.602
    },
    {
      "lang": "en",
      "text": "recommendation engine with the relevant",
      "offset": 21493.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "data available to you as a data",
      "offset": 21496.16,
      "duration": 4.398
    },
    {
      "lang": "en",
      "text": "scientist. Some things to bear in mind",
      "offset": 21498.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "about these types of models. If users",
      "offset": 21500.558,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "don't have a lot of ratings and ALS",
      "offset": 21503.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "can't infer much about them, it's likely",
      "offset": 21505.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that ALS will make broad general",
      "offset": 21507.76,
      "duration": 3.878
    },
    {
      "lang": "en",
      "text": "recommendations that aren't really",
      "offset": 21509.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "personalized. You might have seen this",
      "offset": 21511.638,
      "duration": 3.642
    },
    {
      "lang": "en",
      "text": "if you spent extra time exploring some",
      "offset": 21513.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of the recommendation output. Like all",
      "offset": 21515.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "models, the more data there is, the",
      "offset": 21517.84,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "better the model",
      "offset": 21519.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "performs. While we've gone over",
      "offset": 21521.16,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "different ways of evaluating",
      "offset": 21523.12,
      "duration": 3.678
    },
    {
      "lang": "en",
      "text": "recommendation engines, the only way to",
      "offset": 21524.4,
      "duration": 4.318
    },
    {
      "lang": "en",
      "text": "really know if your model performs well",
      "offset": 21526.798,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is to test it on users and see if they",
      "offset": 21528.718,
      "duration": 3.482
    },
    {
      "lang": "en",
      "text": "actually take your",
      "offset": 21530.798,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "recommendations. It's entirely possible",
      "offset": 21532.2,
      "duration": 4.758
    },
    {
      "lang": "en",
      "text": "that a simple binary implicit ratings",
      "offset": 21534.718,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "model provides much better",
      "offset": 21536.958,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "recommendations for users than an",
      "offset": 21538.878,
      "duration": 4.162
    },
    {
      "lang": "en",
      "text": "explicit model, but the only way to know",
      "offset": 21540.718,
      "duration": 4.962
    },
    {
      "lang": "en",
      "text": "is to test it. Bear this in mind as you",
      "offset": 21543.04,
      "duration": 5.918
    },
    {
      "lang": "en",
      "text": "move forward. Here are some resources to",
      "offset": 21545.68,
      "duration": 5.038
    },
    {
      "lang": "en",
      "text": "help you as you continue to learn about",
      "offset": 21548.958,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "these models and begin to build them on",
      "offset": 21550.718,
      "duration": 4.722
    },
    {
      "lang": "en",
      "text": "your own. The first is a paper published",
      "offset": 21552.638,
      "duration": 5.122
    },
    {
      "lang": "en",
      "text": "by McKenzie and company discussing the",
      "offset": 21555.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "power of recommendation engines like",
      "offset": 21557.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "ALSbased models. The second is the code",
      "offset": 21559.6,
      "duration": 5.358
    },
    {
      "lang": "en",
      "text": "to build the wide to long function",
      "offset": 21563.2,
      "duration": 3.598
    },
    {
      "lang": "en",
      "text": "discussed in the section about preparing",
      "offset": 21564.958,
      "duration": 3.162
    },
    {
      "lang": "en",
      "text": "data for",
      "offset": 21566.798,
      "duration": 4.402
    },
    {
      "lang": "en",
      "text": "ALS. The third is the white paper that",
      "offset": 21568.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "provides the academic background for",
      "offset": 21571.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "building ALS models using implicit",
      "offset": 21573.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "ratings. I highly recommend reading this",
      "offset": 21575.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "paper as it provides a lot of context",
      "offset": 21577.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and insight into how these models work",
      "offset": 21579.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and alternative ways to evaluate them.",
      "offset": 21581.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "The fourth is a GitHub link for code",
      "offset": 21585.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that manages the cross validation and",
      "offset": 21587.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "model evaluation for implicit ratings",
      "offset": 21589.44,
      "duration": 5.118
    },
    {
      "lang": "en",
      "text": "models using ALS and PISpark. The last",
      "offset": 21591.36,
      "duration": 5.438
    },
    {
      "lang": "en",
      "text": "resource listed here is a paper that",
      "offset": 21594.558,
      "duration": 4.642
    },
    {
      "lang": "en",
      "text": "discusses the math and intuition behind",
      "offset": 21596.798,
      "duration": 4.882
    },
    {
      "lang": "en",
      "text": "the user-based waiting and item based",
      "offset": 21599.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "waiting methodologies for addressing the",
      "offset": 21601.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "class imbalance present in binary",
      "offset": 21603.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "ratings models. Congratulations on",
      "offset": 21605.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "completing this course and best of luck",
      "offset": 21608.08,
      "duration": 5.558
    },
    {
      "lang": "en",
      "text": "as you move forward.",
      "offset": 21610.08,
      "duration": 3.558
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 21613.76,
      "duration": 21.249
    }
  ],
  "cleanText": "Hi data scamps and data champs.\nHere we'll explore using the Python API for Apache Spark in Big Data with PySpark.\nYou'll learn how to leverage parallel computing to process and analyze big data efficiently.\nWe'll cover data cleaning, Feature Engineering, and building Machine Learning models at scale.\nYou'll get hands-on experience creating Recommendation Engines using real world data sets like MovieLens and the Million Songs data set.\nTo explore further, check out the skill track linked in the description below.\nPlease like and subscribe for more.\n\nWelcome.\nIn this course, we'll explore PySpark, a powerful tool for processing and analyzing Big Data designed for data engineers, data scientists, and Machine Learning enthusiasts.\nThis course will teach you to work with large-scale data sets in distributed environments, transforming raw data into valuable insights.\nI'm Benjamin, your instructor for this course.\nI've been a data engineer for nearly a decade working with Big Data solutions in PySpark for ETL pipelines, Data Cleaning, and Machine Learning.\nPySpark is one of the most versatile tools for data professionals.\nApache Spark is an open-source distributed computing system designed for fast processing of large-scale data.\nPySpark is the Python interface for Apache Spark.\nIt handles large data sets efficiently with parallel computation in Python workflows.\nIdeal for batch processing, real-time streaming, Machine Learning, data analytics, and SQL querying.\nPySpark supports industries like finance, healthcare, and e-commerce with speed and scalability.\nPySpark is ideal for handling large data sets that can't be processed on a single machine.\nIt excels in Big Data analytics through distributed data processing using Spark's in-memory computation for faster processing.\nMachine Learning on large data sets leverages Spark's MLlib for scalable model training and evaluation.\nETL and ELT pipelines transforms large volumes of raw data from sources into structured formats.\nPySpark is flexible working with diverse data sources like CSV, Parquet and many more.\nA key component of working with PySpark is clusters.\nA Spark cluster is a group of computers or nodes that collaboratively process large data sets using Apache Spark with a master node coordinating multiple worker nodes.\nThis architecture enables distributed processing.\nThe master node manages resources and tasks while worker nodes execute assigned compute tasks.\nA Spark session is the entry point into PySpark, enabling interaction with Apache Spark's core capabilities.\nIt allows us to execute queries, process data, and manage resources in the Spark cluster.\nTo create a Spark session, run from PySpark SQL import SparkSession.\nWe'll create a session named my Spark app using SparkSession builder stored as a variable Spark.\nThe builder method sets up the session while get or create initiates a new session or retrieves an existing one.\nThe app name method helps manage multiple PySpark applications.\nWith our Spark session ready, we can load data and apply transformations or actions.\nIt's best practice to use SparkSession.builder.get or create which returns an existing session or creates a new one if necessary.\nPySpark DataFrames are distributed table-like structures optimized for large-scale data processing.\nTheir syntax is similar to Pandas with the main difference being how data is managed at a low level.\nTo create a PySpark DataFrame, use the Spark read CSV function in the Spark session with a CSV file.\nFor this example, we'll use a generalized data variable representing any data source and columns to define the schema.\nTo see our DataFrame, we can use the show method.\nWe'll explore these concepts further throughout this course.\nNow, let's go see these concepts in action.\nWelcome back.\nWe've already started with DataFrames in PySpark in the previous video, but now let's dive deeper.\nWhile we may be familiar with DataFrames from Pandas, the key difference in PySpark is how the data is distributed.\nPandas operates on a single compute instance while PySpark distributes data across multiple instances affecting processing speed and data scalability.\nDataFrames are essential in PySpark for efficiently managing large-scale data across clusters.\nWhile they resemble Pandas' DataFrames, they are designed for much larger data sets.\nWe'll frequently interact with data using PySpark DataFrames which support various manipulation tasks such as filtering, grouping, and aggregating on distributed data making them vital for Big Data analytics.\nAdditionally, DataFrames support SQL-like operations on tables.\nAs we go through this course, you'll probably notice similarities between Pandas' DataFrame syntax and PySpark DataFrame syntax.\nBut bear in mind, DataFrames in PySpark operate slightly differently.\nSo be sure to pay attention.\nLet's start by creating a DataFrame in PySpark.\nA common method for loading data is using spark read.csv CSV which easily reads CSV files into a PySpark DataFrame allowing us to define headers and automatically infer schema types.\nThis code loads a CSV treating the first row as headers and inferring data types for each column.\nWe also include header equals true and infer schema equals true which are true for this particular table.\nBut there are also many other arguments available.\nSo check out your documentation.\nUsing the show method, we can display the first five rows of the DataFrame.\nWhile we can also create DataFrames using the create DataFrame function.\nRead.csv CSV is generally faster, offering significant speed improvements at scale, especially as we gain real world experience with Big Data.\nTo inspect the schema, use print schema to view the DataFrame structure.\nOnce we've loaded data into a DataFrame, we can perform basic analytics like aggregations, which summarize data by counting rows, summing values, or calculating averages.\nThese will return an integer or float.\nTypically, for instance, to count the rows in a DataFrame, use the count method.\nFor more advanced summaries, we can employ the group by and aggregate methods.\nTo group data by a column and calculate the average of another.\nWe can use various aggregation functions like standard deviation, sum, and more that you are probably already familiar with.\nThis approach above groups data by gender and computes the average of salary USD, showing how to combine steps for data summarization and quick insights.\nSome of the most important methods we'll use with DataFrames are group by, aggreg, filter, and select.\nLet's start with select which operates like it does in SQL where it will take specific named columns.\nFilter operates like SQL's where group by is also like SQL's similar same named keyword.\nA takes a function like sum and the specific columns we're interested in.\nIn each case, we need to pass the DataFrame name and the columns as a list.\nFor example, we can filter the DataFrame for rows where the value in the age column is greater than 50 and select only those columns.\nLet's practice DataFrames in PySpark.\nWelcome back.\nIn this video, we'll work with some basic DataFrame operations in PySpark.\nReading data from various formats allows flexibility in handling diverse data sets.\nEach format offering unique benefits.\nCSV files or comma-separated value files are widely used because of their simplicity and compatibility across many platforms.\nThey store data in a plain text format, making them easy to read and write without needing specialized tools.\nHowever, CSV files lack schema enforcement, which means they don't define or enforce data types for each column, leading to potential inconsistencies.\nWe can read the data using the read CSV function.\nJSON or JavaScript object notation files are ideal for representing nested data structures, making them a good choice for data that includes hierarchical relationships or arrays or high compatibility.\nHowever, JSON files can become storage intensive when scaled to large data sets.\nWe can load them using the read JSON function.\nParquet is a columnar storage format optimized for read heavy operations making it a powerful choice for large data sets that require frequent querying.\nAdditionally, Parquet enforces schema definitions which helps maintain data consistency and supports complex data types like nested structures much like JSON.\nWe can load parquet files using read parquet.\nUsing these formats, PySpark enables data engineers and data scientists to tailor their storage to the needs of their specific applications.\nSpark can automatically infer schemas, but sometimes it misinterprets data types, particularly with complex or ambiguous data.\nManually defining a schema can ensure accurate data handling.\nTo manually configure a schema, we need to define the data type using the struct field function calling the write data type method.\nPySpark DataFrames can support various data types similar to SQL and Pandas.\nThe primary ones are integer type for whole numbers, long type for large integers, float type and double type for decimal numbers and string type for strings.\nWe import the specific classes from PySpark.sql.types.\nTo define the schema for our DataFrame, we use struct type and struct field functions to define the structure and fields of a DataFrame.\nFilling in the columns and their types, selecting specific columns and filtering rows are fundamental operations in data analysis.\nWith PySpark, you can perform these operations on large data sets with efficiency using the select, filter, sort, and wear methods.\nwhere and filter operates similarly to those in SQL where we pass a column or columns and a condition to match.\nSorting and handling missing data are common tasks.\nDropping nulls can clean data but in some cases may may want to fill or input values instead which Spark also supports.\nWe can use sort for simple flexible sorting and order by for complex multicolumn sorting methods to sort and order a DataFrame similar to the same command in SQL.\nWe can use the na drop to drop all nulls in a DataFrame.\nHere's a cheat sheet to help you out.\nNow let's go see these DataFrames in practice.\nIn this video, we'll cover essential techniques for handling missing data, managing DataFrame columns and rows, and using PySpark's built-in functions to clean and transform data.\nHandling null values in PySpark is essential for accurate analysis.\nNulls can skew results or cause errors.\nPySpark offers two primary methods to address this issue.\nThe first approach is to drop rows with null values using NA drop either across the entire DataFrame or in specific columns.\nThis simplifies the data set but may significantly reduce the size if nulls are common.\nFor column specific filtering where and is not null can be used.\nThe second approach is to replace nulls with default values using na fill.\nThis method is ideal when nulls are sparse or when removing rows would result in data loss ensuring the data set remains complete and consistent.\nPySpark simplifies creating new columns with the width column method, allowing users to define the column name and computation.\nThis is useful for derived metrics or transformations.\nRenaming columns with column renamed improves clarity, making data frames easier to work with in collaborative settings.\nClear descriptive names reduce ambiguity and ensure accurate documentation.\nDropping columns with drop removes redundant or irrelevant data, focusing the DataFrame on essential points.\nThis helps manage large data sets by reducing memory usage and improving efficiency.\nThese operations enhance PySpark DataFrame's flexibility and clarity, tailoring them to specific analysis needs while maintaining a clean structure.\nRow operations are central to data analysis.\nFiltering is for narrowing down a DataFrame to the most relevant data.\nBy applying conditions to filter rows, we can isolate subsets that meet specific criteria such as data from a particular time period, geographic region, or category.\nWe use the filter method providing the column name and the filter criteria.\nGrouping rows based on specific fields or categories allows us to organize the data by meaningful segments such as by customer, product, or date.\nGrouping enables us to analyze patterns and trends within each category, offering insights that would be difficult to see in ungrouped data.\nWe use the group by method and then the aggregation we would be interested in with the columns we want to focus on.\nHere's what the code from the previous slide may look like.\nAnd here is a cheat sheet to help you going forward.\nLet's go practice how DataFrames work in PySpark.\nIn this video, we'll explore powerful data manipulation techniques in PySpark, including joins, unions, and complex data types.\nJoins in PySpark combine data from multiple DataFrames based on shared columns.\nSimilar to SQL, this enriches data sets such as merging customer details with purchase history to analyze buying patterns.\nPySpark supports inner, left, right, and full outer joins performed using the join method by specifying the second DataFrame, join type, and columns impacted.\nFor columns with different names, you must explicitly specify the joining columns.\nSimilar to SQL syntax, the union operation in PySpark is a powerful tool that enable us to combine or stack two DataFrames as long as they share the same structure, meaning they have the same number and types of columns in the same order.\nThis operation is particularly useful when we're working with data sets that have been split across different sources or time periods and we want to consolidate them into a single DataFrame for easier analysis and processing.\nIf the DataFrames don't have the same schema, a union will not work and create an error.\nUsing union, we can append one DataFrame on top of another, effectively combining rows from both DataFrames into a single unified data set.\nFor example, if we're working with monthly sales data stored as separate files for each month, union allows us to combine all monthly DataFrames into a single DataFrame that represents the entire year.\nThis consolidated view simplifies further analyses as it eliminates the need to handle multiple separate DataFrames.\nHere's the syntax for performing a union in PySpark.\nThis operation stacks df2 underneath df1.\nHowever, it's important to note that the DataFrames must have identical schemas for this operation to work correctly.\nOtherwise, PySpark will raise an error.\nThis schema alignment is crucial because mismatched columns or data types would prevent the rows from being combined accurately and showing the same type of data.\nComplex data types in PySpark like arrays, structs, maps enhance flexibility by allowing nested data within each row.\nThese data types enable PySpark to manage structured data within a single column.\nFind a way to work with these more complex data relationships and hierarchy directly in the DataFrame.\nArrays store lists within a column.\nUseful for attributes with multiple values.\nFor an array, define the values being passed using the appropriate data type.\nHere we are using lit for a specific value.\nMaps store dynamic key value pairs within a column providing a flexible way to store dynamic data attributes.\nWhere each row might have different keys.\nThe map method needs to have passed the value, the data type method we've seen before, and a boolean for requirement of the key value pair.\nStructs group related fields within a single column.\nThis is valuable for managing hierarchical data within one column.\n\n\nSimilar to map, we pass a value with a struct field defining the name and data type. Let's go see these SQL-like actions in practice. Now we'll take a look at User-Defined Functions, or UDFs, in PySpark.\n\nBy the end of this video, we'll have a thorough understanding of what UDFs are and how to create them. A UDF is a custom function we create to work with data using PySpark DataFrames. We'll discuss two kinds of UDFs: PySpark UDFs and Pandas UDFs.\n\nThe main difference is the size of the data set they are designed to handle. There are some differences in execution, but they operate very similarly. They are both reusable and repeatable because they are registered directly to the Spark session.\n\nWe have the two types of UDFs: Pandas UDFs for large data sets and PySpark UDFs for smaller data sets. This may seem counterintuitive, but it is due to how each type of UDF handles data on a row-by-row basis.\n\nIt's outside our scope here, but well worth exploring as we get more PySpark experience. Now, how do we create a UDF? Let's see how it's done. First, we create a regular Python function called to uppercase to convert all strings in a column to uppercase. Next, we register the function as a UDF using PySpark's UDF function and pass the correct data type method. We're doing string operations, so we use string type. Without registering, the UDF would not be available to all the worker nodes of the Spark session. Lastly, we apply it to the DataFrame, `df`, and show the results.\n\nWhile PySpark UDFs are incredibly useful, they can also introduce performance overhead because PySpark has to do a series of inefficient conversions, causing frustrating performance problems. To mitigate this, PySpark has introduced Pandas UDFs. Here's an example of a Pandas UDF, which is defined as a Pandas UDF. Two things to notice here: one, we have to import the pandas UDF function; second, we have to use the decorator to define the data type using `@pandas.UDF`.\n\nWe also don't need to register it to the Spark session. So when should we use PySpark UDFs and when should we opt for Pandas UDFs? If we are working with small data sets or simple transformations, PySpark UDFs will suffice. We use them on the column level and register directly with the Spark session. Registering to a Spark session will make the UDF work with all nodes of the Spark cluster. However, for large data sets, Pandas UDFs are preferred due to their superior performance at scale and incorporate the code outside the Spark session.\n\nThere are many scenarios where we would consider a PySpark UDF over a Pandas UDF, but they involve a cost-benefit analysis of compute cost, development environment, data size, and data type that are outside the scope of this course. Let's go practice UDFs.\n\nLet's explore a foundational component of PySpark: resilient distributed data sets, RDDs. One of PySpark's greatest strengths is its ability to handle large-scale data processing through parallelization, which splits data and computations across multiple nodes in a cluster. Operations defined in Spark are automatically distributed, enabling efficient processing of large data sets. Tasks are assigned to worker nodes that process data in parallel with results combined at the end. This approach allows for efficient processing at scale, accommodating gigabytes or even terabytes of data.\n\nRDDs are the core building blocks of Spark, representing distributed collections of data across a cluster. While RDDs enable fast access and analysis, DataFrames offer greater user-friendliness due to their simpler syntax, although they can be slower. RDDs are immutable, meaning once created, they cannot be changed, but a new RDD can be created using operations like map and filter. They also support actions like collect, which retrieves the results of RDD operations.\n\nLet's create an RDD using a CSV. We'll load the DataFrame from the CSV, then use the RDD method, convert it to an RDD. The data is distributed across the cluster when the RDD is created. Here we create an RDD from a CSV file and use a collect action to retrieve and display the data. As you can see, collect shows a summary of the processes that were done. It is also rather verbose to do a print statement.\n\nRDDs offer a low-level interface providing maximum flexibility. You can manipulate data at a granular level, but this flexibility comes at the cost of requiring more lines of code for even moderately complex operations. One strength of RDDs is their ability to preserve data types across operations. However, they lack the schema optimizations of DataFrames, which means operations on structured data are less efficient and harder to express. While RDDs can scale to handle large data sets, they're not optimized for analytics like DataFrames.\n\nDataFrames are operated for ease of use, providing a high-level abstraction for working with data. They encapsulate complex computations, making it easier to achieve our objective with less code and fewer errors. One of the standout features of DataFrames is their SQL-like functionality. With SQL syntax, even complex transformations and analysis can be performed in just a few lines of code. DataFrames come with built-in schema awareness, meaning they contain column names and data types, just like a structured table in SQL. Here are a handful of useful functions you'll be seeing in the following exercises.\n\nMap is useful to apply a function to an RDD. This can include a lambda function or any other function defined or imported elsewhere. Collect gathers data across the cluster using the parallelization of PySpark. Let's go look at RDDs and DataFrames in practice.\n\nWelcome back. We'll now learn how Spark SQL enables us to use Python and SQL in the same PySpark environment. Let's explore Spark SQL, a powerful component of Apache Spark that integrates seamlessly with its ecosystem. It enables processing of structured and semi-structured data using SQL syntax.\n\nWhy choose Spark SQL over other tools? First, it leverages Spark's distributed computing power, handling massive data sets effortlessly by distributing computations across a cluster. Second, Spark SQL simplifies querying with familiar SQL syntax, making it accessible to analysts and engineers. Third, its integration with PySpark DataFrames enables blending SQL and programmatic operations for complex workflows. In short, Spark SQL delivers the flexibility, speed, and scalability needed for working with large, diverse data sets.\n\nTo work with SQL in PySpark, start by registering a DataFrame as a temporary view. We can use the `createDataFrame` method to make the DataFrame or load the data from a flat file. This allows interaction with the DataFrame using SQL syntax through the `createOrReplaceTempView` method, where you pass the view name as a string. For example, we'll create a view called people.\n\nTemporary views exist only for the current session, making them ideal for quick session-based exploration. For instance, to find people older than 30, use the `spark.sql` function and pass your SQL query as a string. This query retrieves the name and age columns from the people view, filtering to return only rows where age exceeds 30. The result is another DataFrame, which you can continue processing in PySpark outside the session.\n\nLet's explore how to load data from CSV files into DataFrames, register them as temporary views, run SQL queries, and blend SQL with DataFrame commands for advanced manipulation. To query a DataFrame with SQL, we first load the data into a DataFrame using methods like `spark.read.csv`. After loading, we register the DataFrame as a temporary view, allowing SQL-based interaction while safeguarding the underlying data and data source during analysis. These views are session-scoped, lasting only as long as a Spark session is active. For extended use, global temporary views or permanent tables must be created.\n\nOne of the most powerful aspects of Spark SQL is its seamless integration with DataFrame operations. After running a SQL query, the results are returned as a DataFrame, allowing us to apply additional transformations programmatically. Here we use SQL to filter high-earning employees and then added a bonus column using DataFrame operations.\n\nLet's go seamlessly blend SQL and PySpark in single scripts. Welcome back. Let's take a look at some complex PySpark aggregations. PySpark SQL provides a suite of built-in aggregation functions for summarizing data. Commonly used functions include sum, count, average, max, and min. These are applied using either SQL queries with `spark.sql` or the DataFrame toolkit. Let's see an example of calculating the total and average salary for employees in different departments using SQL syntax.\n\nPySpark allows us to mix SQL operations with the DataFrame interface for greater flexibility. For instance, we can filter and pre-process data using DataFrame operations before applying SQL aggregations. This combination leverages the strengths of both approaches. By combining these two tools, we get the best of both worlds: the expressiveness of SQL and the programmatic control of the Pandas DataFrame. Notice how in each step we are registering a new temporary view. This lets us easily roll back changes and catch errors before it hits our data store.\n\nWhile performing aggregations, data type mismatches can lead to errors or unexpected results. For instance, numerical data stored as strings might not aggregate correctly. PySpark provides functions and methods like `cast` to convert data types before processing. We should always validate and standardize our data types before running aggregations. This protects our pipeline from costly errors. Aggregations are a cornerstone of data and analytics, helping us summarize and gain insights from large data sets. PySpark provides powerful aggregation functions through its SQL interface and the DataFrame API. Now we'll explore some of these common PySpark SQL functions like sum and average.\n\nWhile RDDs are useful for certain use cases involving scale and data movement across clusters, DataFrames are the preferred choice for most modern PySpark analytics applications due to their simplicity of syntax. RDDs, as a general rule, do not have simple code syntax for aggregation or analytics. The code we're seeing now does the same as the aggregations we saw earlier with an RDD. The best way to do this is with a lambda function, a small transformation function with very limited scope that is targeted to our circumstance. And then we apply the new lambda function with the RDD map method. The `reduceByKey` method applies the function previously defined to the entire DataFrame. It's a lot. As we can see, the verbosity of RDDs for analytics requires the writing of custom functions and multiple lines to apply it, compared to the single-line application that we saw with DataFrames.\n\nHere are some best practices for PySpark aggregations: Filter early to reduce data size before performing aggregations. Ensure data is clean and correctly typed. Avoid using the entire data set by minimizing operations like group by. Choose the right tool by preferring DataFrames for most tasks due to their optimizations. Monitor performance by using the `explain` to inspect the execution plan and optimize accordingly. Here are some key takeaways. Let's go check out some PySpark aggregations.\n\nWelcome back. Let's talk about what happens when we use PySpark at scale. As our data grows, optimizing PySpark jobs becomes essential for managing performance, resource usage, and execution speed. In this video, we'll focus on techniques to scale PySpark workflows effectively.\n\nFirst, we'll explore how to interpret Spark execution plans to identify performance bottlenecks. Next, we'll discuss caching and persisting DataFrames, which can significantly speed up iterative queries. Finally, we'll cover best practices for optimizing PySpark jobs, ensuring we make the most of Spark's distributed computing power.\n\nScaling PySpark is not just about faster results. It's about building workflows that are efficient, maintainable, and capable of handling large data sets with ease. Methods like broadcast will load a smaller data set across the cluster using all available compute. As we work more with PySpark, the more we will need to think in moderately abstract ways.\n\nA key tool for optimizing PySpark jobs is understanding Spark execution plans. Whenever we run an operation on a DataFrame, Spark constructs a logical plan, which is then optimized into a physical plan. This physical plan outlines how Spark will execute the task on its distributed cluster. We can inspect the process using the `explain` method. It gives us a breakdown of the query's execution plan, showing the operations performed at each stage. This outputs a logical and physical plan. By analyzing these plans, we can spot inefficiencies like redundant shuffles or unoptimized joins and address them before running the job. The `explain` method details the logical and physical plans for optimization, showing what the code ran and what hardware it used. This outlines how to improve it.\n\nWhen working with large data sets, we often reuse intermediate results across multiple operations. Recomputing these results can be costly, especially when reading from disk. To avoid this, we have two tools that keep data readily available: caching and persisting. The `cache` method stores a DataFrame in memory for fast use, while the `persist` method offers flexibility by letting us choose storage levels. Here the data set is read once, and subsequent operations reuse the cached version. The second option is using the `persist` method, which allows us greater control of storage levels. We can use disk when memory is insufficient. This is especially useful for long-running jobs on large clusters. However, caching consumes memory, so it's important to uncache data when it's no longer needed using the `unpersist` method. In this example, if the DataFrame doesn't fit in memory, Spark writes the overflow to disk. This ensures our operations can still proceed without crashing due to resource constraints. This concept is difficult for us to demonstrate in this course, but you will encounter it as you work with PySpark in the real world.\n\nBeyond caching and persistence, here are a few best practices to optimize PySpark jobs: Use small subsections. Favor targeted functions and methods like map over whole data set tools like group by that require shuffles. Broadcast joins for small data sets. Use broadcast to load the data onto all the nodes and avoid shuffles. Lastly, avoid repeated actions. Operations like count or show trigger jobs; store intermediate results to prevent recomputation. Let's go look at these ideas in practice. Congratulations on reaching the end of the\n\n\nCourse. You've come a long way building up foundational skills and exploring powerful tools for big data processing with PySpark. Starting from an overview of Apache Spark's architecture, you dove into the fundamentals of distributed data processing. You learned about resilient data sets, RDDs, transformations, and actions, key concepts that power PySpark's ability to handle massive data sets effectively. You also navigated the world of DataFrames, a critical PySpark component for handling structured data, and discovered the flexibility of PySpark SQL for SQL-like operations. With this foundation, you now understand how to filter, aggregate, and join data, making complex data wrangling tasks intuitive and scalable. In addition to these core skills, you ventured into advanced topics like User-Defined Functions within PySpark. These tools allow you to extend PySpark's capabilities from applying custom transformations to building and running Machine Learning models on distributed data sets. However, there is still more to explore in the PySpark ecosystem. This course didn't cover topics such as advanced cluster configuration, performance optimization, Big Data applications, and streaming data processing with PySpark streaming. Additionally, deep dives into Spark's Machine Learning pipelines and integration with cloud-based tools were beyond the scope of this course. These are advanced topics you can and should explore as you continue your PySpark journey. Whether you're a Data Engineer, a Data Scientist, or a Machine Learning Engineer, you now have the skills to leverage PySpark for managing and analyzing Big Data. Thank you for joining me on this journey. I'm Benjamin Schmidt and congratulations again on mastering the foundations of PySpark. Goodbye and good luck.\n\nWelcome to the first video of Big Data fundamentals via PySpark course. My name is Upendra Devishetti and I'm a science analyst at Cywords. Let's get started. What exactly is Big Data? There's no single definition of Big Data because projects, vendors, practitioners, and business professionals use it quite differently. According to Wikipedia, Big Data is a term used to refer to the study and applications of data sets that are too complex for traditional data processing software. There are three Vs of Big Data that are used to describe its characteristics. They are volume, velocity, and variety. Volume refers to the size of the data. Variety refers to the different sources and formats of data. Velocity is the speed at which data is generated and available for processing. Now let's take a look at some of the concepts and terminology of Big Data. Clustered computing is the pooling of resources of multiple machines to complete jobs. Parallel computing is a type of computation in which many calculations are carried out simultaneously. A distributed computing involves nodes or network computers that run jobs in parallel. Batch processing refers to the breaking data into small pieces and running each piece on an individual machine. Realtime processing demands that information is processed and made ready immediately. There are two popular frameworks for Big Data processing. The first is the highly successful Hadoop map reduce framework. Hadoop map reduce framework is open-source and scalable framework for batch data. The second is the most popular Apache Spark which is a parallel framework for storing and processing of Big Data across clustered computers. It is also open-source and is suited for both batch and real-time data processing. In this course, you learn about Apache Spark. Let's talk about the main features of Apache Spark. Spark distributes data and computation across multiple computers executing complex multi-stage applications such as Machine Learning. Spark runs most computations in memory and thereby provides better performance for applications such as interactive data mining. Spark helps to run an application up to 100 times faster in memory and 10 times faster when running on disk. Spark is mainly written in Scala language but also have support for Java, Python, R, and SQL. Apache Spark is a powerful alternative to Hadoop map reduce with rich features like Machine Learning, realtime stream processing and graph computations. At the center of the ecosystem is the Spark core which contains the basic functionality of Spark. The rest of Spark's libraries are built on top of it. First is SparkSQL which is a library for processing structured and semi-structured data in Python, Java and Scala. The second is MLlib which is a library of common Machine Learning algorithms. The third component is Graphics which is a collection of algorithms and tools for manipulating graphs and performing parallel graph computations. Finally, Spark streaming is a scalable high throughput processing library for realtime data. In this course, you'll learn about SparkSQL and MLlib. Spark can be run on two modes. The first is the local mode where you can run Spark on a single machine such as your laptop. The local mode is very convenient for testing, debugging, and demonstration purposes. The second is the cluster mode where Spark is run on a cluster. The cluster mode is mainly used for production. The development workflow is that you start on a local mode and transition to cluster mode. During the transition from local to cluster mode, no code changes necessary. In this course, you'll be using local mode. In the next video, you learn about PySpark, which is Python.\n\nIn the last video, you were introduced to Apache Spark, which is a fast and general purpose framework for Big Data processing. Apache Spark provides high-level APIs in Scala, Java, Python, and R. In this video, you'll learn about PySpark, which is Spark's version of Python. Apache Spark is originally written in Scala programming language. To support Python with Spark, PySpark was developed. Unlike previous versions, the newest version of PySpark provides computation power similar to Scala. APIs in PySpark are similar to Pandas and scikit-learn Python packages. Thus, the entry-level barrier to PySpark is very low for beginners. Spark comes with interactive shells that enable ad hoc data analysis. Spark shell is an interactive environment through which one can access Spark's functionality quickly and conveniently. Spark shell is particularly helpful for fast interactive prototyping before running the jobs on clusters. Unlike most other shells, Spark shell allows you to interact with data that is distributed on disk or in memory across many machines and Spark take care of automatically distributing this processing. Spark provides the shell in three programming languages. Spark shell for Scala, PySpark for Python and Spark for R. PySpark shell is a Python based command line tool to develop Sparks interactive applications in Python. PySpark helps data scientists interface with Spark data structures in Apache Spark and Python. Similar to Scholar Shell, PySpark shell has been augmented to support connecting to a cluster. In this course, you'll use PySpark shell. In order to interact with Spark using PySpark shell, you need an entry point. Spark context is an entry point to interact with underlying Spark functionality. Before understanding Spark context, let's understand what an entry point is. An entry point is where control is transferred from the operating system to the provided program. In simpler terms, it's like a key to your house. Without the key, you cannot enter the house. Similarly, without an entry point, you cannot run any PySpark jobs. You can access the spark context in the PySpark shell as a variable named sc. Now, let's take a look at some of the important attributes of spark context. The first is the version. This attribute shows the version of Spark that you're currently running. In this example, SCV version shows that the version of Spark is 2.3.1. The second is the Python version. This attribute shows the version of Python that Spark is currently using. In this example, SC.Python work shows that Spark is using Python version 3.6. The final attribute is the master. Master is the URL of the cluster or local string to run in local mode. In this example, sc.m master returns local asterisk, meaning the spark context access a master on a local node using all available threads on the computer where it is running. You can load your raw data into PySpark using spark context by two different methods. The first is the spark context paralyze method and a list. For example, here is how to create paralyze collections holding the numbers 1 to 5. The second is the spark context text file method on a file. For example, here is a way to load a text file named test.txt using spark context text file method. Now that you understand PySpark, let's write your first spark code in. Understanding PySpark becomes a lot easier if you understand functional programming principles in Python. In this video, let's review some of the Python functions such as lambda, map, and filter. Python supports the creation of anonymous functions, that is functions that are not bound to a name at runtime using a construct called the lambda. Lambda functions are very powerful, well integrated into Python and are often used in conjugation with typical functional concepts like map and filter functions. Like dev the lambda creates a function to be called later in the program. However, it returns the function instead of assigning it to a name. This is why lambdas are known as anonymous functions. In practice, they are used as a way to inline a function definition or to defer execution of a code. Lambda functions can be used whenever function objects are required. They can have any number of arguments but only one expression and the expression is evaluated and returned. The general syntax of lambda function is shown here. Here is an example of lambda function. In this example, lambda x asterisk 2. X is the argument and x asterisk 2 is the expression that gets evaluated and returned. This function has no name. It returns a function object which is assigned to the identifier double. Here applying the lambda function to a number such as three returns six which is the double of the original number. Let's take a look at the differences between deaf and lambda. Here is the Python code to illustrate cube of a number showing the difference between normal Python function using dev and anonymous function using lambda. As you can see, both deaf and lambda do exactly the same. The main difference is that the lambda definition does not include a return statement and it always contains an expression which is returned. Also note that we can put a lambda definition anywhere a function is expected and we don't have to assign it to a variable at all unlike normal Python function using dev. We use lambda functions when we require a nameless function for a short period of time. Most of the times we use lambdas with built-in functions like map and filter. The map function is called with all the items in the list and a new list is returned which contains items returned by that function for each item. The general syntax of map function is shown here. It takes in a function and a list. Here is an example of map function with lambda to add the number two to all the items in a list. The result indicates that the number two is added to 1 2 3 4 resulting in 3 4 5 6. The filter function in Python takes in a function and list as argument. The function is called with all the items in the list and a new list is returned which contains items for which the function evaluates to true. Here is a general syntax of filter function in Python. Similar to map, it takes in a function and a list as arguments. Here is an example use of filter with lambda to filter out only odd numbers from a list. As shown in the example, filtering the items list containing numbers 1 2 3 4 resulted in 1 and three which are the only odd numbers for the input list. Lambda functions are incredibly useful and before going deep into PySpark let's practice some lambda functions in. In the first chapter you have learned about the different components of Spark namely Spark core Spark SQL and Spark ML. In this chapter we'll start with RDDs which are Spark's core abstraction for working with data let's get started. RDD stands for resilient distributed data sets. It is simply a collection of data distributed across the cluster. RDD is the fundamental and backbone data type in PySpark. When Spark starts processing data, it divides the data into partitions and distributes the data across cluster nodes with each node containing a slice of data. Now let's take a look at the different features of RDD. The name RDD captures three important properties. Resilient, which means the ability to withstand failures and recompute missing or damaged partitions. Distributed, which means spanning the jobs across multiple nodes in the cluster for efficient computation. Data sets, which is a collection of partition data, example, arrays, tables, tuples or other objects. There are three different methods for creating RDDs. You have already seen two methods in the previous chapter even though you are not aware that you are creating RDDs. The simplest method to create RDDs is to take an existing collection, example a list, an array or a set and pass it to Spark context paralyze method. A more common way to create RDD is to load data from external data sets such as files stored in HDFS or objects in Amazon S3 buckets or from lines in a text file stored locally and pass it to Spark context text file method. Finally, RDDs can also be created from existing RDDs which we'll see in the next video. In the first method, RDDs are created from a list or a set using the spark context paralyze method. Let's try and understand how RDDs are created using this method with a couple of examples. In the first example, an RDD named num is created from a Python list containing numbers 1 2 3 and four. In the second example, an RDD named hello RD is created from the hello world string. You can confirm the object created is RDD using Python's type method. Creating RDDs from external data sets is by far the most common method in PySpark. In this method, RDDs are created using the Spark context text file method. In this simple example, an RDD named file RDD is created from the lines of readme.md file stored locally on our computer. Similar to previous method, you can confirm the RDD using the type method. Data partitioning is an important concept in Spark and understanding how Spark deals with partitions allows one to control parallelism. A partition in Spark is the division of the lost data set with each part being stored in multiple locations across the cluster. By default, Spark partitions the data at the time of creating RDD based on several factors such as available resources, sectional data sets, etc. However, this behavior can be controlled by passing a second argument called min partitions which define the minimum number of partitions to be created for an RDD. In the first example, we create an RDD named num RDD from the list of 10 integers using the spark context paralyze method with six partitions. In the second example, we create another RDD named filed using spark context text file method with six partitions. The number of partitions in an RDD can always be found by using the get num partitions method. In the next video, you'll see the final method of creating RDDs. For now, let's create some RDDs. In the last video, you have learned how to load your data into RDD\n\n\nEntities. In this video, you learn about the various operations that support audits in PySpark. RDDs in PySpark support two different types of operations: transformations and actions. Transformations are operations on RDDs that return a new RDD, and actions are operations that perform some computation on the RDD. The most important feature, which helps RDDs in fault tolerance and optimizing resource use, is lazy evaluation. So what is lazy evaluation? Spark creates a graph from all the operations you perform on an RDD, and execution of the graph starts only when an action is performed in RDD, as shown in this figure. This is called lazy evaluation in Spark.\n\nThe RDD transformations we will look at in this video are map, filter, flatMap, and union. The map transformation takes in a function and applies it to each element in the RDD. Say you have an input RDD with elements 1, 2, 3, 4. The map transformation takes in a function and applies it to each element in the RDD, with the result of the function being the new value of each element in the resulting RDD. In this example, the square function is applied to each element of the RDD. Let's understand this with an example. We first create an RDD using spark context parallelize method on a list containing elements 1, 2, 3, 4. Next, we apply map transformation for squaring each element of the RDD. The filter transformation takes in a function and returns an RDD that only has elements that pass the condition. Suppose we have an input RDD with numbers 1, 2, 3, 4, and we want to select numbers greater than 2. We can apply the filter transformation.\n\nHere is an example of the filter transformation wherein we use the same RDD as before to apply filter transformation to filter out the numbers that are greater than two. FlatMap is similar to map transformation, except it returns multiple values for each element in the source RDD. A simple usage of flatMap is splitting up an input string into words. Here you have an input RDD with two elements: hello world and how are you. Applying the split function of the flatMap transformation results in five elements in the resulting RDD: Hello, world, how, are, you? As you can see, even though the input RDD has two elements, the output RDD now contains five elements. In this example, we create an RDD from a list containing the words hello world and how are you. Next, we apply flatMap along with the split function on the RDD to split the input string into individual words. Union transformation returns the union of one RDD with another RDD. In this figure, we are filtering the input RDD and creating two RDDs: errors RDD and warnings RDD. And next, we are combining both the RDDs using the union transformation.\n\nTo illustrate this using PySpark code, let's first create an input RDD from a local file using spark context text file method. Next, we will use two filter transformations to create two RDDs: error RDD and warnings RDD. And finally, using union transformation, we will combine them both. So far, you have seen how RDD transformations work. But after applying transformations, at some point you will want to actually do something with your data set. This is where actions come into picture. Actions are the operations that are applied on RDDs to return a value after running a computation. The four basic actions that you will learn in this lesson are collect, take, reduce, and count.\n\nCollect action returns complete list of the elements from the RDD, whereas take of n prints an n number of elements from the RDD. Continuing the map transformation example, executing collect returns all elements, that is 1, 4, 9, 16, from the RDD map RDD that you created earlier. Simply, here is an example of take of two action that prints the first two elements, that is one and four, from the RDD_map RDD. Sometimes you just want to print the first element of the RDD. First action returns the first element in an RDD. It is similar to take of one. Here is an example of the first action which prints the first element, that is one, from the RDD_map RDD.\n\nFinally, the count action is used to return the total number of rows or elements in the RDD. Here is an example of the count action to count the number of elements in the RDD flatmap RDD. The result here indicates that there are five elements in the RDD flatmap RDD. It's time for you to practice RDD operations. In the last video, you were introduced to some basic RDD operations, and in this video you'll learn how to work with RDDs of key-value pairs, which are a common data type required for many operations in Spark. Most of the real-world data sets are generally key-value pairs. An example of this kind of data set has the team name as key and the list of players as values. The typical pattern of this kind of data set is each row is a key that maps to one or more values.\n\nIn order to deal with this kind of data set, PySpark provides a special data structure called pair RDDs. In pair RDDs, the key refers to the identifier, whereas the value refers to the data. There are a number of ways to create pair RDDs. The two most common ways are creating from a list of the key-value tuple or from a regular RDD. Irrespective of the method, the first step in creating pair RDDs is to get the data into key-value form. Here is an example of creating pair RDD from a list of the key-value tuple that contains the name as key and as the value using spark context paralyze method. And here is an example of creating pair RDD from regulars. In this example, a regular RDD is created from a list that contains strings using Spark context paralyze method. Next, we create a pay RDD using map function which returns tuple with key-value pairs with key being the name and is being the value. Pair RDDs are still RDDs and thus all the transformations available to regularities. Since pay RDDs contains tuples, we need to pass functions that operate on key-value pairs. A few special operations are available for this kind, such as reduce by key, group by key, sort by key, and join. Let's take a look at each of these four pair transformations in detail.\n\nNow, the reduce by key transformation is the most popular pay RDD transformation, which combines values with the same key using a function. Reduce by key runs several parallel operations, one for each key in the data set. Because data sets can have a very large number of keys, reduce by key is not implemented as an action. Instead, it returns a new RDD consisting of each key and the reduced value for that key. Here is an example of reduce by transformation that uses a function to combine all the goals scored by each of the players.\n\nThe result shows that the player as key and the total number of goals scored as value. Sorting of data is necessary for many downstream applications. We can sort pay RDD as long there is an ordering defined in the key. Sort by key transformation returns an RDD sorted by key in ascending or descending order. Continuing our reduced by key example. Here is an example that sorts the data based on the number of goals scored by each player. A common use case of pay RDD is grouping the data by key. For example, viewing all of the airports for a particular country together. If the data is already keyed in the way that we want, the group by key operation groups all the values with the same key in the pair RDD. Here is an example of group by key transformation that groups all the airports for a particular country from an input list that contains the list of tuples. Each tuple consists of country code and the corresponding airport code. Join transformation joins two pair RDDs based on their key. Let's demonstrate this with an example. First, we create two RDDs. RDD1 contains the list of tuples with each tuple consisting of name and is, and RDD2 contains the list of tuples with each tuple consisting of name and income. Applying the join transformation on RDD1 and RDD2 merges two RDDs together by grouping elements with the same key.\n\nHere is an example that shows the result of joint transformation of RDD1 and RDD2. Now that you have learned all about pay RDDs, it's time for you. Previously, you learned about advanced RDD transformations for key-value data sets. Similar to advanced RDD transformations, there are advanced RDD actions which you'll see in this video. Reduce action takes in a function which operates on two elements of the same type of RDD and returns a new element of the same type.\n\nThe function should be commutative and associative so that it can be computed correctly in parallel. A simple example of such a function is plus, which we can use to sum our RDD. Here is an example of reduce action that calculates the sum of all the elements in an RDD. In this example, input RDD is created using spark context paralyze method on a list consisting of numbers 1, 3, 4, 6. Executing reduce action results in 14, which is the sum of 1, 3, 4, 6. In many cases, it's not advisable to run collect action on RDD because of the huge size of the data. In these cases, it's common to write data out to a distributed storage system such as HDFS or Amazon S3. Save as text file action can be used to save RDD as a text file inside a particular directory. By default, save as text file saves RDD with each partition as a separate file inside a directory. Here is an example of save as text file that saves an RDD with each partition as a separate file inside a directory. However, you can change it to return a new that is reduced into a single partition using the co method. Here is an example of save as text file that saves RDD as a single file inside a directory.\n\nSimilar to pay RDD transformations, there are also RDD actions available for pay RDDs. However, pay RDDs also attain some additional actions of PySpark, especially those that leverage the advantages of data which is of key-value nature. Let's take a look at two pair actions: count by key and collect as map in this video.\n\nCount by key is only available on RDDs of type key value. With the count by key operation, we can count the number of elements for each key. Here is an example of counting the number of values for each key in the data set. In this example, we first create a pay RDD named RDD using spark context paral method. Since count by key generates a dictionary, next we iterate over the dictionary to print the unique key and the number of values associated with each key as shown here. One thing to note is that count by key should only be used in a data set whose size is small enough to fit in a memory. Collect as map returns the key-value pairs in the RDD as a dictionary. Here is an example of collect as map on a pair RDD. As before, we create a pair RDD using spark context paralyze method and next use collect as map action. Collect as map produces the key-value pairs in the RDD as a dictionary which can be used for downstream analysis. Similar to count by key, this action should only be used if the resulting data is expected to be small as all the data is loaded into the memory. Let's practice some of these advanced actions on some test data in Python.\n\nIn the previous chapter, you looked at RDDs, which is Spark's core abstraction for working with data. In this chapter, we will explore PySpark SQL, which is Spark's high-level API for working with structured data. PySpark SQL is a Spark library for structured data. Unlike the PySpark RDD API, PySpark SQL provides more information about the structure of the data and the computation being performed. PySpark SQL provides a programming abstraction called data frames. A data frame is an immutable distributed collection of data with named columns. It is similar to a table in SQL. Data frames are designed to process a large collection of structured data such as relational database and semi-structured data such as JSON or JavaScript object notation. Data frame API currently supports several languages such as Python, R, Scala, and Java. Data frames allow PySpark to query data using SQL, for example, select asterisk from table, or using the expression method, for example, df select. Previously, you have learned about spark context, which is the main entry point for creating a similarly spark session provides a single point of entry to interact with underlying spark functionality and allows programming spark with data frame API.\n\nThe spark session does for data frames what the spark context does for RDDs. A spark session can be used to create data frame, register data frame as tables, execute SQL over tables, cache tables, etc. Similar to spark context, spark session is exposed to the PySpark shell as variable spark. Data frames in PySpark can be created in two main ways: from an existing RDD using sparse sessions create data frame method and from different data sources such as CSV, JSON text using sparse sessions read method. Before going into the details of creating data frames, let's understand what schema is. Schema is the structure of data in data frames and helps Spark to optimize queries on the data more efficiently.\n\nA schema provides informationational details such as the column name, the type of data in that column, and whether null or empty values are allowed in the column. To create a data frame from an RDD, we will need to pass an RDD and a schema into Spark sessions create data frame method. In this example, we will first create an RDD named iPhones_D from a list of iPhones using Spark context paralyze method. Next, we will create a data frame using Spark sessions create data frame method using iPhone RDD and the list of column names such as model, year, height, width, and weight as schema. The type of object created can be confirmed using type method which shows that it is a PySpark data frame. A thing to note here is when the schema is a list of column names, the type of each column will be inferred from data as shown above. However, when the schema is none, it will try to infer the schema from the data. To create a data frame from CSV, JSON text txt files, we will make use of the Spark sessions spark read property. Here is an example of create DF CSV data frame from people CSV file using spark read. CSV method. Similarly, here is an example for creating df_json data frame from people.json file using spark read.gsv json method. Finally, here is an example for creating df_txt data frame from people.txt file using spark read.txt method. Irrespective of the file type, this method requires the path to the file and two optional parameters. The first optional parameter header equals to true may be passed to make sure that the meth method treats the first row as column names. The second optional parameter infer schema equals to true may be passed to instruct the data from reader to infer the schema from the data and by doing so it will attempt to assign the right data type to each column based on the content. Now let's practice creating some data frames.\n\nJust like RDDs, data frames also support both transformations and actions. In this video, you learned some data frame operations in PySpark. Similar\n\n\nTo RDD operations, the data frame operations in PySpark can be divided into transformations and actions. PySpark DataFrame provides operations to filter, group, or compute aggregates and can be used with PySpark SQL.\n\nLet's explore some of the most common DataFrame transformations such as select, filter, group by, order by, drop duplicates with column renamed, and some common DataFrame actions such as print schema, show, count, columns, and describe. In this video, let's start with select and show operations. The select transformation is used to extract one or more columns from a DataFrame. We need to pass the column name inside select operation. As an example, let's select a column from a test DataFrame. Select is a transformation and so it creates a new DataFrame. And in order to print the rows from df id a DataFrame, we need to execute an action. Show is an action that prints the first 20 rows by default. Let's apply show of three on df id age DataFrame and print the first three rows as shown in this example. Unlike select, the filter transformation selects only rows that pass the condition specified. The parameter you specified is the column name and the value of what you want to filter that column on. For example, if you want to filter out the rows with a is greater than 21, we pass the column expression new_df.as and the condition greater than 21. As shown in here, we can use show up three action to print out the first three rows from the new data frame. The group by transformation groups the DataFrame using the specified columns so we can run aggregations on them. To better understand, we will first group the a column and create another DataFrame. Then we will use count action that returns the total number of rows in the DataFrame. And finally use show of three operation to print the first three rows in the DataFrame. The result is a table that shows the first three groups and the corresponding number of members in each group. Order by transformation returns a DataFrame sorted by the given columns. Let's sort the test_df a_g groupoup. count that we obtained in the previous example based on a column and print out the first three rows of the DataFrame using the show of three action. As you can see the ace group has been sorted in ascending order.\n\nNow the drop duplicate transformation returns a new DataFrame with duplicate rows removed. Here is an example where drop duplicate transformation is used to remove duplicate rows in user ID is and gender columns and finally creating a new DataFrame. You can execute count action on this new DataFrame to print the number of non-duplicate rows. The width column rename transformation returns a new DataFrame by renaming an existing column. It takes two arguments the name of the old and new columns. In this example, we rename the column name gender to sex and create a test_df sex. We can use show of three action to print out the first three rows from the new DataFrame. To check the type of columns in the DataFrame, we can use the print schema action. Here is an example of print schema action on test_df DataFrame that we used previously. Print schema prints out the schema in tree format as shown here and helps to spot the issues with the schema of the data. As an example, product ID is shown as string even though it is supposed to be an integer. The column operation returns the names of all the columns in the DataFrame as an array of string. Let's print the column names in test_d of data frame. In this example, the test_d of dataf frame has three columns: user ID, gender and age. Describe operation is used to calculate the summary statistics of the numerical columns in the DataFrame. If you don't specify the name of columns, it will calculate summary statistics for all numerical columns present in the data frame as shown in this example. Now that you are familiar with DataFrame operations, let's practice using some of these operations on real.\n\nPreviously you have seen how to interact with PySpark SQL using DataFrame API. In this video you'll learn how to interact with PySpark SQL using SQL query. In addition to DataFrame API, PySpark SQL allows you to manipulate DataFrames with SQL queries. What you can do using DataFrame API can be done using SQL queries and vice versa. So what are the differences between DataFrame API and SQL queries? The DataFrame API provides a programmatic interface basically a domain specific language or DSL for interacting with data. DataFrame queries are much more easier to construct programmatically. Plain SQL queries can be significantly more concise and easier to understand. They also portable and can be used without any modifications with every supported language. Many of the DataFrame operations that you have seen in the previous chapter can be done using SQL queries. The SparkSession provides a method called SQL which can be used to execute SQL query. The SQL method takes a SQL statement as an argument and returns a DataFrame representing the result of the given query.\n\nUnfortunately, SQL queries cannot be run directly against a DataFrame. To issue SQL queries against an existing data frame, we can leverage the create or replace tempu function to build a temporary table as shown in this example. After creating the temporary table, we can simply use the SQL method which allows us to write SQL code to manipulate the data within a DataFrame. In this example, we simply extract two columns, field one and field two from the table using select. Since the result is a DataFrame, you can run DataFrame actions such as collect, first, show, etc. An example of collect action is shown here. In the previous lesson, you have seen how to use select operation to subset the data from a DataFrame. Here is an example of how you can do the same with a SQL query. In this example, we will first construct a query for selecting the product ID column from the temporary table. Next, we will pass the query to the Spark sessions SQL method to create a new DataFrame. Because the result of SQL query returns a DataFrame, all the usual DataFrame operations are available. Here we can use show of five action to print the first row, five rows of the DataFrame. The SQL queries are not limited to extracting data as seen in the previous slide. We can also create SQL queries to run aggregations. In this example, we first construct a query for selecting a and purchase columns. Then aggregate the total of all the purchases the maximum per age group. We can then provide the query to spark sessions SQL method and use show five action to print out the first five rows as shown in here.\n\nIn addition to extracting and summarizing the data, SparkSQL queries can also be constructed for filtering the rows from a DataFrame. Suppose you want to filter out the rows of purchase and gender columns where the gender is female and the purchase is greater than 20,000. You can construct a query as shown in this example. You can confirm whether or not query worked by providing the query to the spark sessions SQL method and using show five action to print out the first five rows as shown in this example. Let's practice some SQL within PySpark. Visualization is an essential part of data analysis. In this video, we will explore some visualization methods that can help us make sense of our data in PySpark DataFrames. Data visualization is the way of representing your data in the form of graphs or charts. It is considered a crucial component of exploratory data analysis. Several open-source tools exist to aid visualization in Python such as Mattplot liabet etc. However, none of these visualization tools can be used directly with PySparks DataFrames.\n\nCurrently there are three different methods available to create charts using PySpark DataFrames: PySpark d explore library to pandas method and handy spark library. Let's understand each of these methods with examples. PySpark d explorer is a plotting library to get quick insights on data in PySpark DataFrames. There are three functions available in PySpark dish explore to create mattplot lib graphs while minimizing the amount of computation needed: hist dishplot and pandas histogram. Here is an example of creating a histogram using PySpark dish explore package on the test_df data. First the CSV file is loaded into spark data frame using the spark sessions read CSV method. Then we select the as column from the test_df data frame using the select operation. Finally, we use the hist function of the PySpark dist explore package to plot a histogram of a in the test_df data set. The second method of creating charts is by using two pandas on PySpark DataFrames which converts the PySpark data frame into pandas data frame. After conversion, it's easy to create charts from pandas data frames using mattplot lib or cp plotting libraries. In this example, first the CSV is loaded in spark data frame using the read. CSV method. Next, using two pandas method, we will convert the spark data frame into pandas data frame. Finally, we will create a histogram of the as column using mattplot lib's h method. Before we look at the third method, let's take a look at the differences between pandas versus spark data frames. Pandas won't work in every case. It is a single machine tool and constrained by single machine limits. Those so the size is limited by your server memory and you will process them with the power of single server. In contrast, operations on PySpark DataFrames run parallel on different nodes in the cluster. In pandas data frames we get the result as soon as we apply any operation. Whereas operations in PySpark DataFrames are lazy in nature. You can change a pandas data frame using methods. We can't change a PySpark data frame due to its immutable property. Finally the pandas API supports more operations than PySpark DataFrames.\n\nThe final method of creating charts is using the handy spark library which is relatively a new package. Handy spark is designed to improve PySpark user experience especially when it comes to exploratory data analysis including visualization capabilities. It makes fetching data or computing statistics for columns really easy written pandas objects straight away. It brings the long missing capability of plotting data while retaining the advantages of performing the distributed computation. Here is an example of handy spark method for creating a histogram. Just like before, we load the CSV into a PySpark data frame using the spark sessions read. CSV method. After creating the data frame, we convert the data frame to a handy Spark data frame using the two handy method. Finally, we create a histogram of the ace column using the hist function of handy spark library. We have learned three exciting methods of visualizing PySpark data frames. And let's practice creating ch some charts with them now on real world.\n\nIn the last chapter, you learned about PySpark SQL which is one of the highlevel API built on top of spark core for structured data. In this chapter, you learn about PySpark MLIB, which is a built-in library for scalable machine learning. Before diving deep into PySpark MLIB, let's quickly define what machine learning is. According to Wikipedia, machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data. PySpark MLib is a machine learning library. Its goal is to make practical machine learning scalable and easy. At a high level, Pispock ML provides tools such as machine learning algorithms which include collaborative filtering, classification and regression and clustering. Featurization which include feature extraction, transformation, dimensionality reduction and selection pipelines which include constructing, evaluating and tuning machine learning pipelines.\n\nIn this chapter, we will explore machine learning algorithms, collaborative filtering, classification, and clustering. Many of you have heard about scikitlearn, which is a very popular and easy to use Python library for machine learning. Then what is the need for PySpark ML? Scikitlearn algorithms work well for small to medium-sized data sets that can be processed on a single machine but not for large data sets that require the power of parallel processing. On the other hand, PySpark MLB only contains algorithms in which operations can be applied in parallel across nodes in a cluster. Unlike scikitlearn, MLib supports several other highle languages such as Scala, Java and R. In addition to Python, MLA also provides a highle API to build machine learning pipelines. A machine learning pipeline is a complete workflow combining multiple machine learning algorithms together. PySpark is good for iterative algorithms and using iterative algorithms. Many machine learning algorithms have been implemented in PySpark MLB.\n\nIspark currently supports various methods for binary classification, multiclass classification and regression analysis. Some of the algorithms include linear SVMs, logistic regression, decision trees, random forest, gradient posted trees, naive base, linear le squares, lasso, rich regression, isotonic regression. Collaborative filtering is commonly used for recommended systems and PySpark MLIP uses the alternative least squares or ALS algorithm for collaborative filtering. Clustering algorithm consists of K means Gaussian mixture power iteration clustering bisecting K means and streaming K means. While PySpark MLIPs include several machine learning algorithms, we will specifically focus on three key areas often referred to as the three C's of machine learning: collaborative filtering, classification, and clustering. Collaborative filtering produces recommendations based on past behavior, preferences, or similarities to known entities or users. Classification is the problem of identifying to which of a set of categories a new observation belongs. Clustering is grouping of data into clusters based on similar characteristics. We will go in more details in the next few lessons. Now that you learned the three C's of machine learning, let's quickly understand how we can import this PySpark MLI libraries in the PySpark shell environment. Let's start with PySpark's collaborative filtering which is available in pispark.mlib.recommendation subm module. Here is how you import the ALS or alternative lease square class in PySpark shell for binary classification. Here is an example of how you import logistic regression with LBFGS class in the pispark.mlip.classification subm module inside the pispark shell. Similarly for clustering here is an example of importing the kins class in pispark shell using the pispark.mlipclustering subm module. Let's practice how well you understand the different machine learning algorithms by importing them.\n\nIn the previous video you have been introduced with the three C's of machine learning. In this video, we'll start with the first C which is collaborative filtering and gain a basic understanding of recommended systems in Spa. Let's get started. Collaborative filtering is a method of making automatic predictions about the interests of a user by collecting preferences or taste information from many users. Collaborative filtering is one of the most commonly used algorithms in recommended systems. Collaborative filtering has two approaches: the user user approach and item item approach.\n\n\nThe user approach finds users that are similar to the target user and uses the collaborative ratings to make recommendations for the target user. The item item approach finds and recommends items that are similar or related to items associated with the target user. Now let's take a look at different components that are needed to build a recommendation system in PySpark.\nThe rating class in PySpark MLlib recommendation subm module is a wrapper around tupil of user product and rating. The rating class is useful for parsing the odd and creating a tupole of user product and rating. Here is a simple example of how you can create an instance of rating class R with the values of user equals to 1, product equals to 2 and rating equals to 5.0. Once the rating class is created, you can extract the user product and rating value using the index of R instance. In this example, R of zero, R of one and R of 2 shows the user ID, product ID and ratings for the R instance. Splitting the data into training and testing sets is an integral part of machine learning. The training portion will be used to train the model while the testing data is used to evaluate the model's performance. Typically, a large portion of the data is assigned for training and a small portion for testing. PySpark's random split function can be used to randomly split the data with the provided weights and return multiple RDDs. In this example, we first create an RDD which consists of numbers 1 to 10 and using random split function, we create two RDDs with 60 to 40 ratio. The output of the random split function shows training RDDs contain six elements whereas the test RDD contains four elements.\nThe alternating least squares or ALS algorithm available in Spark MLlib helps to find products that the customers might like based on their previous purchases or ratings. The ALS method requests that we represent rating objects as user ID, items ID, rating tuples along with the training parameters, rank and iterations. Rank represents the number of features. Iterations represent the number of iterations to run the least square computation. Here is an example of running the ALS model. First, we create an RDD from a list of reading objects and print out the contents of the RDD using collect action. Next, we use ALS.train to train the training data as shown in this example. After training the model, the next step is predicting the ratings for the user ID and product pairs.\nThe predict all method takes an RDD of user ID and product ID pairs and returns a prediction for each pair. In order to get the example to work, let's create an RDD from a list of tuples containing user ID and product ID using spark context paralyze method. Next, we apply the predict all method on the unrated_D. Running collect action and prediction shows a list of predicted ratings generated by ALS model for the user ID 1 and product ids 1 and two. For evaluating the model train using ALS, we can use the mean squared error or MSE. The MSE measures the average of the squares of the errors between what is estimated and the existing data.\nContinuing on our previous example, we'll first organize our ratings and predictions data to make user product the rating. Next, we will join the ratings RDD with the prediction RDD and the result looks as follows. Finally, we apply a square difference function to the map transformation of the rates RDD and then use the mean to get the MSE.\nNow it's your turn to try your hand at collaborative filtering. In the previous video you learned about collaborative filtering which is the first C of machine learning algorithms in PySpark MLlib. In this video you learn about the second C of machine learning which is classification. Classification is a popular machine learning algorithm that identifies which category an item belongs to. For example, whether an email is spam or non-spam based on labeled examples of other items. Classification takes a set of data with known labels and predetermined features and learns how to label new records based on that information. That is why classification comes under a supervised learning technique. Classifications can be divided into two different types. Binary classification and multiclass classification. In binary classification, we want to classify entities into two distinct categories. For example, determining whether a cancer type is malignant or not. PySpark ML supports various methods for binary classification such as linear SVMs, logistic regression, decision trees, random forests, gradient boosted trees, naive base. In multiclass classification, we want to classify entities into more than two categories. For example, determining what category a news article belongs to. PySpark ML supports various methods for multiclass classification such as logistic regression, decision trace, random forest, naive base. Let's focus on logistic regression which is the most popular supervised machine learning method. Logistic regression is a classification method to predict a binary response given some independent variable. It measures the relationship between the label on the y-axis and features on the x-axis using a logistic function as shown in this figure. In logistic regression, the output must be zero or one. The convention is if the probability is greater than 50% then the logistic regression output is one otherwise it is zero. PySpark MLlib contains a few specific data types such as vectors and labeled point. Let's understand each of these data types. Vectors in PySpark MLlib comes in two flavors dense and sparse. Dense vectors store all their entries in an array of flo floating point numbers. For example, a vector of 100 will contain 100 double values. In contrast, sparse vector stores only the nonzero values and their indices.\nHere is an example of creating a dense vector of 1 2 3 using vectors dance method. And here is an example of creating a sparse vector with the size of the vector equals to 4 and nonzero entities 1 col 1.0 3 5.5 as a dictionary using vector sparse method. A label point is a wrapper around the input features and the predicted value. Label point includes a label and a feature vector. The label is a floating point value and in the case of binary classification it is either one positive or zero negative. This example shows a positive label point with label 1 and a feature vector 1.0.0 3.0 and a negative label point with label zero and feature vector 2.0 1.0 1.0. PySpark MLlib has an algorithm called hashing TF that computes a term frequency vector of a given size from a document. Let's illustrate this with an example. In this simple example, first we will split the sentence hello world into a list of words using the split method and we will create vectors of size 10,000.\nFinally, we compute the term frequency vector by using TF's transform method on the words. As you can see, the sentence is turned into a sparse vector holding feature number and the occurrences of each word. Among several algorithms, the popular algorithm available for logistic regression is PySpark ML is LBFGS.\nThe minimum requirement for logistic regression with LBFGS is an RDD of labeled point. To understand how logistic regression works, let's see a simple example. We first create a list of labeled points with labels zero and one. And then using spark context parallel method, we will create an RDD. Then we will use logistic regression with lbfgs. Method to train a logistic regression model on the RDD. Once the model is trained from logistic regression with LBFGS algorithm, the predict method computes a score between zero and one for each point as shown here. Now it's your turn to practice classification.\nIn the previous video you learned about classification, a type of supervised learning method. But what if if you want to make sense of unlabelled data? In this video, you learn about clustering, which is a type of unsupervised learning method to group unlabelled data together. So, what exactly is clustering? Clustering is the unsupervised learning task that involves grouping objects into clusters of high similarity with no labels. Unlike the supervised learning methods that you have seen before, such as collaborative filtering and classification where data is labeled, clustering can be used to make sense of unlabelled data. PySpark ML library offers a handful of clustering models such as K means clustering, Gaussian mixture clustering, power iteration clustering, bisecting K means clustering and streaming K means clustering. In this video, we will focus on K means clustering because of its simplicity and popularity.\nK means is an unsupervised method that takes data points in an input data and will identify which data points belong to each one of the clusters. As shown in the left side of the figure, we can provide n data points and a predefined number of K clusters. The K means algorithm through a series of iterations, create clusters as shown on the right side of the figure. The K means clustering minimally requires that the data is a set of numerical features and that we specify the target number of K clusters ahead. The first step in implementing the K means clustering algorithm using PySpark MLlib is loading the numerical data into an RDD and then parsing the data based on a TL limiter. Here is an example of how you load a CSV file into an RDD using Spark context text file method. Then passing the RDD based on a comma delimiter and finally converting the floats to integers. The contents of the first five lines of RDD can be printed using takeoff file. As you can see the data set contains two columns. Each column indicating a feature loaded into an RDD. Like other algorithms, you invoke K means by calling K means train method which takes an RDD the number of clusters we expect and the maximum number of iterations allowed. Continuing our previous example, first we can import the K means class from PySpark.mlib.clustering subm module. Next we call K means train method on RDD and the two parameters K equals to two and the maximum iterations equals to 10. K means train returns a K means model that lets you access the cluster centers using the model.cluster centers method. An example of cluster centers for K equals to 2 is shown here. The next step in K means clustering is to evaluate the model by computing the error function. Unfortunately, PySpark came algorithm doesn't have a method already. So we have to write a function but ourselves as shown here. We will next apply the error function on the RDD and calculate within set sum of squared error.\nContinuing our previous example, we apply map transformation of error function to a input odd to calculate within set sum of squared error which is 77.96 in this example. An optional but highly recommended step in K means clustering is cluster visualization. Continuing from a previous example, let's first create a scatter plot of the two feature columns in the sample data. Next, overlay it with the cluster centers from the K means model, which are indicated by the colored axis in this figure. The purple and yellow colors here represent the labels created from the model based on K, which is two in this example.\nAs you can see the overlaid scatter plot shows a reasonable clustering with the two centroids or cluster centers placed in the center of each of the cluster. Now let's quickly take a look at the code to generate the previous plot. As seen previously plotting libraries doesn't work that directly on a data frames as shown here. We first convert RDD to Spark data frame and then to pandas data frame. We also convert the cluster centers from K means model into a pandas data frame. Finally, we use plt function in map plot clip library to create a overlay scatter plot as shown in this previous slide. Let's use a real world data and generate some nice clusters using PySpark MLlib K means. Congratulations on successfully completing fundamentals of big data via PySpark course. Our goal through this course was to equip you with a basic understanding of big data and show how Apache Spark can be used to perform powerful data analysis at scale. Let's quickly review what you have learned so far in this course and recommend you a few courses that you can take next.\nAnalyzing big data is equivalent to conducting both descriptive and inferential analysis using distributed computing techniques such as Spark with the hope that the volume, velocity, variety of big data that makes distributed computing necessary will lead into deeper or more targeted insights.\nChapter one started with the fundamentals of big data and introduced Apache Spark as an open-source distributed big data processing engine as well as its different components namely Spark Core, Sparks SQL, Spark ML Graphics, and Spark streaming.\nBecause Python is one of the most popular languages for data science, we looked specifically at how you might use PySpark which is Spark's Python API to execute Spark jobs and PySpark shell to develop Sparks and track to applications in Python. Finally, you learned about the two different modes of running Spark, namely local mode and cluster mode. Chapter 2 introduced PySpark RDD which is the main API in Spark core for processing unstructured data. We learned about the different features of RDDs, different methods of creating RDDs and finally RDD operations namely transformations and actions. Chapter 3 explored PySpark SQL which is Spark's highle API for working with structured data. PySpark SQL creates data frames which provides more information about the structure of data and the computation being performed. We looked at the different methods of creating data frames, data frame operations namely transformations and actions and finally different methods of visualizing big data using data frames. Chapter 4 delve deep into PySpark ML Spark's built-in library for machine learning and discussed how PySpark ML makes practical machine learning scalable and easy. This chapter also introduced the three C's of ML collaborative filtering classification and clustering. The ecosystem of Apache Spark is vast and ever expanding. But throughout the course, we have discussed the essential underlying concepts. Where you choose to go from here, whether that be experimenting and applying some of these tools and patterns on your own or investigating Spark components such as Spark SQL or Spark ML more deeply is up to you. But we hope that the concepts, tools, and techniques that we are introduced in this course have provided a well-informed starting point and continue to serve as a basis for you to refer back to throughout your distributed data analysis journey.\nWith this general understanding of PySpark, we would encourage you to look at other DataCamp PySpark courses focused on feature engineering and recommended engines to for Welcome to data cleaning in Apache Spark with Python. My name is Mike Mezer. I'm a data engineering consultant and I will be your instructor for this course. We will cover what data cleaning is, why it's important, and how to implement it with Spark and Python. Let's get started. In this course, we'll define data cleaning as preparing raw data for use in processing pipelines. We'll discuss what a pipeline is later on, but for now, it's sufficient to say the data cleaning is a necessary part of any production data system. If your data isn't clean, it's not trustworthy and could cause problems later on.\n\n\nThere are many tasks that can fall under the data cleaning umbrella. A few of these include reformatting or replacing text, performing calculations based on the data, and removing garbage or incomplete data. Most data cleaning systems have two big problems: optimizing performance and organizing the flow of data. A typical programming language such as Pearl, C++, or even standard SQL may be able to clean data when you have small quantities of data. But consider what happens when you have millions or even billions of pieces of data. Those languages wouldn't be able to process that amount of information in a timely manner. Spark lets you scale your data processing capacity as your requirements evolve. Beyond the performance issues, dealing with large quantities of data requires a process or pipeline of steps. Spark allows management of many complex tasks within a single framework. Here's an example of cleaning a small data set. We're given a table of names, age, and years, and a city. Our requirements are for a data frame with first and last name in separate columns, the age and months, and which state the city is in. We also want to remove any rows where the data is out of the ordinary. Using Spark transformations, we can create a data frame with these properties and continue processing afterwards. A primary function of data cleaning is to verify all data is in the expected format. Spark provides a built-in ability to validate data sets with schemas. You may have used schemas before with databases or XML. Spark is similar. A schema defines and validates the number and types of columns for a given data frame. A schema can contain many different types of data, integers, floats, dates, strings, and even arrays or mapping structures. A defined schema allows Spark to filter out data that doesn't conform during read, ensuring expected correctness. In addition, schemas also have performance benefits. Normally, a data import will try to infer a schema on read. This requires reading the data twice. Defining a schema limits this to a single read operation. Here's an example schema to import the data from our previous example. First, we'll import the pispark.sql.types library. Next, we'll define the actual strruct type list of strct fields containing an entry for each field in the data. Each strct field consists of a field name, data type, and whether the data can be null. Once our schema is defined, we can add it into our spark.format.load call and process it against our data. The load method takes two arguments, the file name and a schema. This is where we apply our schema to the data being loaded. We've gone over a lot of information regarding data cleaning and the importance of data frame schemas. Let's put that information to use and practice.\n\nWelcome back. We've had a quick discussion about data cleaning, data types, and schemas. Let's move on to some further Spark concepts: immutability and lazy processing. Normally in Python and most other languages, variables are fully mutable. The values can be changed at any given time, assuming the scope of the variable is valid. While very flexible, this does present problems anytime there are multiple concurrent components trying to modify the same data. Most languages work around these issues using constructs like mutxes and semaphors etc. This can add complexity especially with non-trivial programs. Unlike typical Python variables, Spark data frames are immutable. While not strictly required, immutability is often a component of functional programming. We won't go into everything that implies here, but understand that Spark is designed to use immutable objects. Practically, this means Spark data frames are defined once and are not modifiable after initialization. If the variable name is reused, the original data is removed, assuming it's not in use elsewhere, and the variable name is reassigned to the new data. While this seems inefficient, it actually allows Spark to share data between all cluster components. It can do so without worry about concurrent data objects. This is a quick example of the immutability of data frames in Spark. It's okay if you don't understand the actual code. This example is more about the concepts of what happens. First, we create a data frame from a CSV file called voter data.csv. This creates a new data frame definition and assigns it to the variable name voter_df. Once created, we want to do two further operations. The first is to create a full year column by using a two-digit year present in the data set and adding 2,00 to each entry. This does not actually change the data frame at all. It copies the original definition, adds the transformation, and assigns it to the voter_df variable name. Our second operation is similar. Now, we want to drop the original year column from the data frame. Again, this copies the definition, adds a transformation, and reassigns the variable name to this new object. The original objects are destroyed. Please note that the original year column is now permanently gone from this instance, though not from the underlying data. For example, you could simply reload it into a new data frame if desired. You may be wondering how Spark does this so quickly, especially on large data sets. Spark can do this because of something called lazy processing. Lazy processing in Spark is the idea that very little actually happens until an action is performed. In our previous example, we read a CSV file, added a new column, and deleted another. The trick is that no data was actually read, added, or modified. We only updated the instructions, aka transformations, for what we wanted Spark to do. This functionality allows Spark to perform the most efficient set of operations to get the desired result. The code example is the same as the previous slide, but with the added count method call. This classifies as an action in Spark, and we'll process all the transformation operations. These concepts can be a little tricky to grasp without some examples. Let's practice these ideas in the coming exercises. Welcome back. As we've seen, Spark can read in text and CSV files. While this gives us access to many data sources, it's not always the most convenient format to work with. Let's take a look at a few problems with CSV files. Some common issues with CSV files include the schema is not defined. There are no data types included nor column names beyond a header row. Using content containing a comma or another delimiter requires escaping. Using the escape character within the content requires even further escaping. The available encoding formats are limited depending on the language used. In addition to the issues with CSV files in general, Spark has some specific problems processing CSV data. CSV files are quite slow to import and parse. The files cannot be shared between workers during the import process. If no schema is defined, all data must be read before a schema can be inferred. Spark has a feature known as predicate push down. Basically, this is the idea of ordering tasks to do the least amount of work. Filtering data prior to processing is one of the primary optimizations of predicate pushdown. This drastically reduces the amount of information that must be processed in large data sets. Unfortunately, you cannot filter the CSV data via predicate push down. Finally, Spark processes are often multi-step and may utilize an intermediate file representation. These representations allow data to be used later without regenerating the data from source. Using CSV would instead require a significant amount of extra work defining schemas, encoding formats, etc. Parquet is a compressed columner data format developed for use in any Hadoop based system. This includes Spark, Hadoop, Apache, Impala and so forth. The parket format is structured with data accessible in chunks allowing efficient readr operations without processing the entire file. This structured format supports Spark's predicate push down functionality providing significant performance improvement. Finally, parquet files automatically include schema information and handle data encoding. This is perfect for intermediary or on disk representation of processed data. Note that parquet files are a binary file format and can only be used with the proper tools. This is in contrast to CSV files which can be edited with any text editor. Interacting with paret files is very straightforward. To read a parquet file into a data frame, you have two options. The first is using the spark readad.format method we've seen previously. the data frame df equals spark read.format parquet.load filename.parket. The second option is the shortcut version. The data frame df equals spark read.parquet file name.parquet. Typically the shortcut version is the easiest to use but you can use them interchangeably. Writing parquet files is similar using either df.right.format format parquet.save file name.parquet or df.right.parquet filename.parquet. The long form versions of each permit extra option flags such as when overwriting an existing parquet file. Paret files have various uses within Spark. We've discussed using them as an intermediate data format, but they are also perfect for performing SQL operations. To perform a SQL query against a parquet file, we first need to create a data frame via the spark read.parquet method. Once we have the data frame, we can use the create or replace temp view method to add an alias of the parquet data as a SQL table. Finally, we run our query using normal SQL syntax and spark.sql method. In this case, we're looking for all flights with a duration under 100 minutes. Because we're using parquet as the backing store, we get all the performance benefits we've discussed previously, primarily defined schemas and the available use of predicate pushdown. You've seen a bit about what a paret file is and why we'd want to use them. Now, let's practice working with paret files. Welcome back. In the first chapter, we've spent some time discussing the basics of Spark data and file handling. Let's now take a look at how to use Spark column operations to clean data. Before we discuss manipulating data frames in depth, let's talk about some of their features. Data frames are made up of rows and columns and are generally analogous to a database table. Data frames are immutable. Any change to the structure or content of the data creates a new data frame. Data frames are modified through the use of transformations. An example is the filter command to only return rows where the name starts with the letter M. Another operation is select. In this case, returning only the name and position fields. There are many different transformations for use on a data frame. They vary depending on what you'd like to do. Some common transformations include the filter clause, which includes only rows that satisfy the requirement defined in the argument. This is analogous to the wear clause in SQL. Spark includes a wear alias you can use in place of filter if desired. This call returns only rows where the vote occurred after January 1st, 2019. Another common option is the select method which returns the columns requested from the data frame. The width column method creates a new column in the data frame. The first argument is the name of the column and the second is the command to create it. In this case, we create a column called year with just the year information. We can also use the drop method to remove a column from a data frame. Among the most common operations used when cleaning a data frame, filtering lets us use only the data matching our desired result. We can use filter for many tasks such as removing null values, removing odd entries, anything that doesn't fit our desired format. We can also split a data frame containing combined data such as a SIS log file. As mentioned previously, use the filter method to return only rows that meet the specified criteria. The contains function takes a string argument that the column must have to return true. You can negate these results using the till day character. Some of the most common operations used in data cleaning are modifying and converting strings. You will typically apply these to each column as a transformation. Many of these functions are in the spark.sql.functions library. For brevity, we'll import it as the alias f. We use the width column function to create a new column called upper using pispark.sql.functions. functions.upupper on the name column. The upper column will contain uppercase versions of all names. We can create intermediary columns that are only for processing. This is useful to clarify complex transformations requiring multiple steps. In this instance, we call the split function with the name of the column and the space character to split on. This returns a list of words in a column called splits. A very common operation is converting string data to a different type such as converting a string column to an integer. We use the cast function to perform the conversion to an integer type. While performing data cleaning with Spark, you may need to interact with array type columns. These are analogous to lists in normal Python environments. One function we will use is size, which returns the number of items present in the specified array type argument. Another commonly used function for array types is get item. It takes an index argument and returns the item present at that index in the list column. Spark has many more transformations and utility functions available. When using Spark in production, make sure to reference the documentation for available options. We've discussed some of the common operations used on Spark data frame columns. Let's practice some of these now.\nWe've looked at some of the power available when using Sparks functions to filter and modify our data frames. Let's spend some time with some more advanced options. The data frame transformations we've covered thus far are blanket transformations, meaning they're applied regardless of the data. Often you want to conditionally change some aspect of the contents. Spark provides some built-in conditional clauses which act similar to an if then else statement in a traditional programming environment. While it is possible to perform a traditional if then else style statement in Spark, it can lead to serious performance degradation as each row of a data frame would be evaluated independently. Using the optimized built-in conditionals alleviates this. There are two components to the conditional clauses when and the optional otherwise. Let's look at them in more depth. The when clause is a method available from the pispark.sql.functions library that is looking for two components. The if condition and what to do if it evaluates to true. This is best seen from an example. Consider a data frame with the name and age columns. We can actually add an extra argument to our select method when using the when clause. We select the df.name and df.age age as usual. For the third argument, we'll define a when conditional. If the age column is 18 or up, we'll add the string adult. If the clause doesn't match, nothing is returned. Note that our return data frame contains an unnamed column we didn't define using with column. The select function can create columns dynamically based on the arguments provided. Let's look at some more examples. You can chain multiple when statements together similar\n\n\nTo an if-else-if structure.\nIn this case, we define two when clauses and return adult or minor based on the age column.\nYou can chain as many when clauses together as required.\nIn addition to when is the otherwise clause.\nOtherwise is analogous to the else statement.\nIt takes a single argument, which is what to return in case the when clause or clauses do not evaluate as true.\nIn this example, we return adult when the age column is 18 or higher.\nOtherwise, we return minor.\nThe resulting data frame is the same, but the method is different.\nWhile you can have multiple when statements chained together, you can only have a single otherwise per when chain.\n\nLet's try a couple of examples of using when and otherwise to modify some data frames.\nWe've looked at the built-in functions in Spark, and I've had great results using these, but let's consider what you would do if you needed to apply some custom logic to your data cleaning processes.\nA user-defined function, or UDF, is a Python method that the user writes to perform a specific bit of logic.\nOnce written, the method is called via the pispark.sql.functions.f method.\nThe result is stored as a variable and can be called as a normal Spark function.\nLet's look at a couple examples.\nHere's a fairly trivial example to illustrate how a UDF is defined.\nFirst, we define a Python function.\nWe'll call our function reverse string with an argument called Meister.\nWe'll use some Python shorthand to reverse the string and return it.\nDon't worry about understanding how the return statement works, only that it will reverse the lettering of whatever is fed into it.\nFor example, help becomes pl.\nThe next step is to wrap the function and store it in a variable for later use.\nWe'll use the pispark.sql.functions.f method.\nIt takes two arguments: the name of the method you just defined and the Spark data type it will return.\nThis can be any of the options in pispark.sql.types and can even be a more complex type, including a fully defined schema object.\nMost often, you'll return either a simple object type or perhaps an array type.\nWe'll call UDF with our new method name and use the string type, then store this as UDF reverse string.\nFinally, we use our new UDF to add a column to the user df data frame within the width column method.\nNote that we pass the column we're interested in as the argument to UDF reverse string.\nThe UDF function is called for each row of the data frame.\nUnder the hood, the UDF function takes the value stored for the specified column per row and passes it to the Python method.\nThe result is fed back to the resulting data frame.\nAnother quick example is using a function that does not require an argument.\nWe're defining our sorting cap function to return one of the letters G, H, R, or S at random.\nWe still create our UDF wrap function and define the return type as string type.\nThe primary difference is calling the function this time without passing in an argument, as it is not required.\nAs always, the best way to learn is practice.\nLet's create some user-defined functions.\n\nWelcome back to our discussion about modifying and cleaning data frames.\nWe've discussed various transformations and methods to modify our data, but we haven't covered much about how Spark actually processes the data.\nLet's look at that now.\nSpark breaks data frames into partitions or chunks of data.\nThese partitions can be automatically defined, enlarged, shrunk, and can differ greatly based on the type of Spark cluster being used.\nThe size of the partition does vary, but generally try to keep your partition sizes equal.\nWe'll discuss more about optimizing partitioning and cluster details later on.\nFor now, let's assume that each partition is handled independently.\nThis is part of what provides the performance levels and horizontal scaling ability in Spark.\nIf a Spark node doesn't need to compete for resources nor consult with other Spark nodes for answers, it can reliably schedule the processing for the best performance.\nIn Spark, any transformation operation is lazy.\nIt's more like a recipe than a command.\nIt defines what should be done with a data frame rather than actually doing it.\nMost operations at Spark are actually transformations, including with column, select, filter, and so forth.\nThe set of transformations you define are only executed when you run a Spark action.\nThis includes count, write, etc.\nAnything that requires the transformations to be run to properly obtain an answer.\nSpark can reorder transformations for the best performance.\nUsually, this isn't noticeable, but can occasionally cause unexpected behavior, such as IDs not being added until after other transformations have completed.\nThis doesn't actually cause a problem, but the data can look unusual if you don't know what to expect.\nRelational databases tend to have a field used to identify the row, whether it is for an actual relationship reference or just for data identification.\nThese IDs are typically an integer that increases in value, is sequential, and most importantly, unique.\nThe problem with these IDs is they're not very parallel in nature.\nGiven that the values are given out sequentially, if there are multiple workers, they must all refer to a common source for the next entry.\nThis is okay in a single server environment, but in a distributed platform such as Spark, it creates some bottlenecks.\nLet's take a look at how to generate IDs in Spark.\nSpark has a built-in function called monotonically increasing ID, designed to provide an integer ID that increases in value and is unique.\nThese IDs are not necessarily sequential.\nThere can be gaps, often quite large, between values.\nUnlike a normal relational ID, Sparks is completely parallel.\nEach partition is allocated up to 8 billion IDs that can be assigned.\nNotice that the ID fields in the sample table are integers, increasing in value, but are not sequential.\nIt's a little out of scope, but the IDs are a 64-bit number effectively split into groups based on the Spark partition.\nEach group contains 8.4 billion IDs, and there are 2.1 billion possible groups, none of which overlap.\nThere's a lot of nuance to how partitions and the monotonically increasing IDs work.\nRemembering that Spark is lazy, it often helps in troubleshooting what can happen.\nOperations are often out of order, especially if joins are involved.\nIt's best to test your transformations.\nWe've discussed a lot of detail in this lesson.\nLet's now take a look at a few exercises that will help solidify how everything works.\nNow that we've discussed some data cleaning tasks using Spark, let's look at how to improve the performance of running those tasks using caching.\nCaching in Spark refers to storing the results of a data frame in memory or on disk of the processing nodes in a cluster.\nCaching improves the speed for subsequent transformations or actions as the data likely no longer needs to be retrieved from the original data source.\nUsing caching reduces the resource utilization of the cluster.\nThere's less need to access the storage, networking, and CPU of the Spark nodes as the data is likely already present.\nThere are a few disadvantages of caching you should be aware of.\nVery large data sets may not fit in the memory reserved for cache data frames.\nDepending on the later transformations requested, the cache may not do anything to help performance.\nIf a data set does not stay cached in memory, it may be persisted to disk.\nDepending on the disk configuration of a sport cluster, this may not be a large performance improvement.\nIf you're reading from a local network resource and have slow local disc IO, it may be better to avoid caching the objects.\nFinally, the lifetime of a cached object is not guaranteed.\nSpark handles regenerating data frames for you automatically, but this can cause delays in processing.\nCaching is incredibly useful, but only if you plan to use the data frame again.\nIf you only need it for a single task, it's not worth caching.\nThe best way to gauge performance with caching is to test various configurations.\nTry caching your data frames at various points in the processing cycle and check if it improves your processing time.\nTry to cache in memory or fast NVMe or SSD storage.\nWhile still slower than main memory, modern SSD-based storage is drastically faster than spinning disc.\nLocal spinning hard drives can still be useful if you are processing large data frames that require a lot of steps to degenerate or must be accessed over the internet.\nTesting this is crucial.\nIf normal caching doesn't seem to work, try creating intermediate paret representations like we did in chapter 1.\nThese can provide a checkpoint in case a job fails mid-task and can still be used with caching to further improve performance.\nFinally, you can manually stop caching a data frame when you're finished with it.\nThis frees up cache resources for other data frames.\nImplementing caching in Spark is simple.\nThe primary way is to call the function cache on a data frame object prior to a given action.\nIt requires no arguments.\nOne example is creating a data frame from some original CSV data.\nPrior to running a count on the data, we call cache to tell Spark to store it in cache.\nAnother option is to call cache separately.\nHere we create an ID in one transformation.\nThen we call cache on the data frame.\nWhen we call the show action, the voter df data frame will be cached.\nIf you're following closely, this means that cache is a Spark transformation.\nNothing is actually cached until an action is called.\nA couple other options are available with caching and Spark.\nTo check if a data frame is cached, use the is cached boolean property, which returns true as in this case or false.\nTo uncache a data frame, we call unpersist with no arguments.\nThis removes the object from the cache.\nWe've discussed caching in depth.\nLet's practice how to use it.\nWe've discussed the benefits of caching when working with Spark data frames.\nLet's look at how to improve the speed when getting data into a data frame.\nSpark clusters consist of two types of processes: one driver process and as many worker processes as required.\nThe driver handles task assignments and consolidation of the data results from the workers.\nThe workers typically handle the actual transformation action steps of his work job.\nOnce assigned tasks, they operate fairly independently and report results back to the driver.\nIt is possible to have a single node sport cluster.\nThis is what we're using for this course, but you'll rarely see this in a production environment.\nThere are different ways to run Spark clusters.\nThe method used depends on your specific environment.\nWhen importing data to Spark data frames, it's important to understand how the cluster implements the job.\nThe process varies depending on the type of task, but it's safe to assume that the more import objects available, the better the cluster can divvy up the job.\nThis may not matter on a single node cluster, but with larger clusters, each worker can take part in the import process.\nIn clearer terms, one large file will perform considerably worse than many smaller ones.\nDepending on the configuration of your cluster, you may not be able to process larger files, but could easily handle the same amount of data split between smaller files.\nNote, you can define a single import statement even if there are multiple files.\nYou can use any form of standard wild card symbol when defining the import file name.\nWhile less important, if objects are about the same size, the cluster will perform better than having a mix of very large and very small objects.\nIf you remember from chapter 1, we discussed the importance of Spark schemas.\nWell-defined schemas in Spark drastically improve import performance.\nWithout a schema defined, import tasks require reading the data multiple times to infer structure.\nThis is very slow when you have a lot of data.\nSpark may not define the objects in the data the same as you would.\nSpark schemas also provide validation on import.\nThis can save steps with data cleaning jobs and improve the overall processing time.\nThere are various effective ways to split an object files mostly into more smaller objects.\nThe first is to use built-in OS utilities such as split, cut, or a.\nAn example using split uses the -l argument with the number of lines to have per file, 10,000 in this case.\nThe -d argument tells split to use numeric suffixes.\nThe last two arguments are the name of the file to be split and the prefix to be used.\nAssuming large file has 10 million records, we would have files named chunk-00000000 through chunk 9999.\nAnother method is to use Python or any other language to split the objects up as we see fit.\nSometimes you may not have the tools available to split a large file.\nIf you're going to be working with a data frame often, a simple method is to read in the single file then write it back out as paret.\nWe've done this in previous examples, and it works well for later analysis, even if the initial import is slow.\nIt's important to note that if you're hitting limitations due to cluster sizing, try to do as little processing as possible before writing to parquet.\nLet's practice some of the import tricks we've discussed.\nNow, we've just finished working with improving import performance in Spark.\nLet's take a look at cluster configurations.\nSpark has many available configuration settings controlling all aspects of the installation.\nThese configurations can be modified to best match the specific needs for the cluster.\nThe configurations are available in the configuration files via the Spark web interface and via the runtime code.\nOur test cluster is only accessible via command shell, so we'll use the last option.\nTo read a configuration setting, call spark.conf.get with the name and the setting as the argument.\nTo write a configuration setting, call spark.conf.set with the name of the setting and the actual value as the function arguments.\nSpark deployments can vary depending on the exact needs of the users.\nOne component of a deployment is the cluster management mechanism.\nSpark clusters can be single node clusters deploying all components on a single system, physical VM, or container, standalone clusters with dedicated machines as the driver and workers, or managed clusters, meaning that the cluster components are handled by a third-party cluster manager such as yarn, msos, or kubernetes.\nIn this course, we're using a single node cluster.\nYour production environment can vary wildly, but we'll discuss standalone clusters as the concepts flow across all management types.\nIf you recall, there is one driver per sport cluster.\nThe driver is responsible for several things, including the following: handling task assignment to the various nodes or processes in the cluster.\nThe driver monitors the state of all processes and tasks and handles any task retries.\nThe driver is also responsible for consolidating results from the other processes in the cluster.\nThe driver handles any access to shared data and verifies each worker process has the necessary resources, code, data, etc.\nGiven the importance of the driver, it is often worth increasing the specifications of the node compared to the other systems.\nDoubling the memory compared to the other nodes is recommended.\nThis is useful for task monitoring and data consolidation tasks.\nAs with all Spark systems, fast local storage is useful.\n\n\nFor running Spark in an ideal setup. The Spark worker handles running tasks assigned by the driver and communicates those results back to the driver. Ideally, the worker has a copy of all code, data, and access to the necessary resources required to complete a given task. If any of these are unavailable, the worker must pause to obtain the resources. When sizing a cluster, there are a few recommendations. Depending on the type of task, more worker nodes is often better than larger nodes. This can be especially obvious during import and export operations, as there are more machines available to do the work. As with everything in Spark, test various configurations to find the correct balance for your workload. Assuming a cloud environment, 16 worker nodes may complete a job in an hour and cost $50 in resources. An eight worker configuration might take 1 and a4 hours, but cost only half as much. Finally, workers can make use of fast local storage, SSD, or NVMe for caching intermediate files, etc.\n\nNow that we've discussed cluster sizing and configuration, let's practice working with these options. We've discussed Spark clusters and improving import performance. Let's look at how to improve the performance of Spark tasks in general. To understand performance implications of Spark, you must be able to see what it's doing under the hood. The easiest way to do this is to use the explain function on a DataFrame. This example is taken from an earlier exercise, simply requesting a single column and running distinct against it. The result is the estimated plan that will be run to generate the results from the DataFrame. Don't worry about the specifics of the plan yet. Just remember how to view it if needed.\n\nSpark distributes data amongst the various nodes in the cluster. A side effect of this is what is known as shuffling. Shuffling is moving of data fragments to various workers as required to complete certain tasks. Shuffling is useful and hides overall complexity from the user. The user doesn't have to know which nodes have what data. That being said, it can be slow to complete the necessary transfers, especially if a few nodes require all the data. Shuffling lowers the overall throughput of the cluster as the workers must spend time waiting for the data to transfer. This limits the amount of available workers for the remaining tasks in the system. Shuffling is often a necessary component, but it's helpful to try to minimize it as much as possible. It can be tricky to remove shuffling operations entirely, but there are a few things that can limit it. The DataFrame repartition function takes a single argument, the number of partitions requested. We've used this in an earlier chapter to illustrate the effect of partitions with monotonically increasing ID function. Repartitioning requires a full shuffle of data between nodes and processes and it's quite costly. If you need to reduce the number of partitions, use the coalesce function instead. It takes a number of partitions smaller than the current one and consolidates the data without requiring a full data shuffle. Note, calling coalesce with a larger number of partitions does not actually do anything.\n\nThe join function is a great use of Spark and provides a lot of power. Calling join indiscriminately can often cause shuffle operations leading to increased cluster load and slower processing times. To avoid some of the shuffle operations when joining Spark DataFrames, you can use the broadcast function. I'll talk about this more in a moment. Finally, an important note about data cleaning operations is remembering to optimize for what matters. The speed of your initial code may be perfectly acceptable and time may be better spent elsewhere.\n\nBroadcasting in Spark is a method to provide a copy of an object to each worker. When each worker has its own copy of the data, there is less need for communication between nodes. This limits data shuffles and it's more likely a node will fulfill tasks independently. Using broadcasting can drastically speed up join operations, especially if one of the DataFrames being joined is much smaller than the other. To implement broadcasting, you must import the broadcast function from pyspark.sql.functions. Once imported, simply call the broadcast function with the name of the DataFrame you wish to broadcast. Note, broadcasting can slow operations when using very small DataFrames or if you broadcast the larger DataFrame in a join. Spark will often optimize this for you, but as usual, run tests in your environment for best performance. We've looked how to limit shuffling and implement broadcasting for DataFrames. Let's practice utilizing these tools now.\n\nWe've spent most of this course working with individual transformations and actions to clean data in Spark. But data is rarely so simple that a couple transformations or actions can prepare for real analysis. Let's look now at data pipelines. Data pipelines are simply the set of steps needed to move from an input data source or sources and convert it to the desired output. A data pipeline can consist of any number of steps or components and can span many systems. For our purposes, we'll be setting up a data pipeline within Spark, but realize that a full production data pipeline will likely communicate with many systems. Much like Spark in general, a data pipeline typically consists of inputs, transformations, and the outputs of those steps. In addition, there is often validation and analysis steps before delivery of the data to the next user.\n\nAn input can be any of the data types we've looked at so far including CSV, JSON, text, etc. It can be from the local file system or from web services, APIs, databases, and so on. The basic idea is to read the data into a DataFrame as we've done previously. Once we have the data in a DataFrame, we need to transform it in some fashion. You've done the individual steps several times throughout this course. Adding columns, filtering rows, performing calculations as needed. In our previous examples, we've only done one or two of these steps at a time, but a pipeline can consist of as many of these steps as needed so we can format the data into our desired output. After we've defined our transformations, we need to output the data into a usable form. You've already written files out to CSV or Parquet format, but it could include multiple copies with various formats or instead write the output to a database, a web service, etc. The last two steps vary greatly depending on your needs. We'll discuss validation in a later lesson, but the idea is to run some form of testing on the data to verify it is as expected. Analysis is often the final step before handing the data off to the next user. This can include things such as row counts, specific calculations, or pretty much anything that makes it easier for the user to consume the data set. It's important to note that in Spark, a data pipeline is not a formally defined object, but rather a concept. This is different than if you've used the pipeline object in Spark.ml. If you haven't, don't worry. It's not needed for this course. For our purposes, a Spark pipeline is all the normal code required to complete a task. In this example, we're doing the various tasks required to define a schema, read a data file, add an ID, then write out two separate data types. The task could be much more complex, but the concept is usually the same. When you look at the components, a data pipeline in Spark is fairly simple, but can be very powerful. Let's start working on a more elaborate data pipeline.\n\nNow, we've worked with many aspects of Spark when it comes to data cleaning operations. Let's look at how to use some of the methods we've learned to parse unconventional data. When reading data into Spark, you're rarely given a fully uniform file. Often, there is content that needs to be removed or reformatted. Some common issues include incorrect data consisting of empty rows, commented lines, headers, or even rows that don't match the intended schema. Real world data often includes nested structures, including columns that use different delimiters. This could include the primary columns separated via comma, but including some components separated via semicolon. Real data often won't fit into a tabular format, sometimes consisting of a differing number of columns per row. There are various ways to parse data in all these situations. The way you choose will depend on your specific needs. We are focusing on CSV data for this course, but the general scenarios described apply to other formats as well. For this chapter, we're going to use the Stanford imageet annotations, which focus on finding and identifying dogs in various imageet images. The annotations provide a list of all identified dogs in an image, including when multiple dogs are in the same image. Other metadata is included, including the folder within the imageet data set, the image dimensions, and the bounding box or boxes of the dogs in the image. In the example rows, we have the folder names, the imageet image reference, width, and height. Then there is the image data for the type of dog or dogs in each image. Each breed column consists of the breed name and the bounding box in the image. The first row contains one new found, but notice that the second row actually has two bull mastiffs identified and an additional column defined.\n\nSpark CSV parser can handle many common data issues via optional parameters. Blake lines are automatically removed unless specifically instructed otherwise. When using the CSV parsing, comments can be removed with an optional named argument, comment, and specifying the character that any comment line would be defined by. Note that this handles lines that begin with a specific comment. Parsing more complex comment usage requires more involved procedures. Header rows can be parsed via an optional parameter named header and set to true or false. If no schema is defined, column names will be initially set as defined by the header. If a schema is defined, the row is not used as data, but the header names are otherwise ignored. When importing CSV data into Spork, it will automatically create DataFrame columns if it can. It will split a row of text from the CSV on a defined separator argument named SE. If SE is not defined, it will default to using a comma. The CSV parser will still succeed in parsing data if the separator character is not within the string. It will store the entire row in a column named underscore C0 by default. Using this trick allows parsing of nested or complex data. We'll look at this more later on. Let's practice working with this data and extending our data pipeline further.\n\nWelcome back. Validation is one step of a data pipeline we haven't covered yet, but it is very important in verifying the quality of the data we're delivering. Let's look at how to implement validation steps in a data cleaning pipeline. In this context, validation is verifying that a data set complies with an expected format. This can include verifying the number of rows and columns is as expected. For example, is the row count within 2% of the previous month's row count? Another common test is do the data types match? If not specifically validated with a schema, does the content meet the requirements only 9 characters or less, etc. Finally, you can validate against more complex rules. This includes verifying that the values of a set of sensor ratings are within physically possible quantities. One technique used to validate data in Spark is using joins to verify the content of a DataFrame matches a known set. Validating via a join will compare data against a set of known values. This could be a list of known ids, companies, addresses, etc. Joins make it easy to determine if data is present in a set. This can be only rows that are in one DataFrame, present in both, or present in neither. Joins are also comparatively fast, especially versus validating individual rows against a long list of entries. The simplest example of this is using an inner join of two data frames to validate the data. A new data frame parsed to DF is loaded from a given paret file. The second data frame is loaded containing a list of known company names. A new data frame is created by joining parsed df and company df on the company name. As this is an inner join, only rows from parsed DF with company names that are present in company DF would be included in the new data frame verified DF. This has the effect of automatically filtering out rows that don't meet any of the specified criteria. This is done without any kind of spark filter or comparison code. Complex rule validation is the idea of using Spark components to validate logic. This may be as simple as using the various spark calculations to verify the number of columns in an irregular data set. You've done something like this already in the previous lessons. The validation can also be applied against an external source, web service, local files, API calls. These rules are often implemented as a UDF to encapsulate the logic to one place and easily run against the content of a DataFrame. Let's try validating our data against our specific requirements for this data set.\n\nEnjoy the exercises and we'll get to the last lesson of this course. We've worked with a lot of sport components while exploring data cleaning. Let's finish up this data cleaning pipeline with some final analysis calculations. Analysis calculations are the process of using the columns of data in a DataFrame to compute some useful value using Sparks functionality. We've used UDFs in previous chapters and this version illustrates calculating an average sale price from a given list of sales. A Python function takes a sales list argument. For every sale in the sales list, the function adds the sale entry from value two and three in the sale double. Once complete, it calculates the actual average per row and returns it. The remaining code is what we've done previously when defining a UDF and using it within a DataFrame. Spark UDFs are very powerful and flexible and are sometimes the only way to handle certain types of data. Unfortunately, UDFs do come at a performance penalty compared to the built-in Spark functions, especially for certain operations. The solution is to perform calculations in line if possible. Spark columns can be defined using inline math operations which can then be optimized for the best performance. In this case, we read in a data file and then have two examples of adding calculated columns. The first is a simple average computed by using two columns in the DataFrame. The second option computes a square footage by multiplying the value in the two columns together to create a third. The final line shows the option of mixing a UDF with an inline calculation. There's often a better way to do this, but it does illustrate Spark does not care about the source of the info as long as it conforms to the expected input format. Let's finish up this course by performing some analysis on our data frame and add meaningful information to the data.\n\nCongratulations. You've successfully completed this course by performing data cleaning operations on several data sets using Python and Apache Spark. While we've touched on many topics, there is a great deal to learn about Spark and how best to perform data cleaning. To continue your journey with using Apache\n\n\nSpark, there are a few areas I'd advise you to focus on. Reading the Spark documentation is a great way to add to your knowledge and fill in gaps of understanding. Spark is constantly changing and often adds new features without a lot of fanfare. Seasoned Spark developers often find new techniques that remove a lot of complexity from existing code.\n\nSpark works on many platforms regardless of size, but it really shines when using it on multi-node clusters with a lot of RAM. You'll be surprised how quickly Spark processes data when given the resources to function as designed. I've personally processed multi-billion row data sets in a few hours on a relatively modest cluster. Finally, I'd suggest working with as many different data sets as you can find. Different types of data require different techniques in Spark and each has challenges when trying to process data in an efficient way. The data sets available within the various courses here on DataCamp are a great place to start. Thanks and good luck on your journey using Apache Spark.\n\nHi, I'm John Hogue and welcome to feature engineering with PySpark. Easily one of the most important aspects of applied machine learning is feature engineering. It is the process of using domain knowledge to create new features to help our models perform better. In this course, we will look at a real data set and work our way to building a regression model in PySpark. Before we dive in, it's important to note that while the techniques you'll learn in this course are invaluable, data science cannot be applied as a cookie cutter. You'll need to research your data and become your own expert. There's much to be said of the dangers of not understanding your data, especially where our outputs are increasingly being used to make decisions and inform policies. Before you dive into modeling, spend time to define what your goals are and how the output might be used. Take time to research your data and its limitations. Often times, you may be tasked with explaining what is and isn't possible. Lastly, remember that data science is about being curious, asking questions, and applying new ways to solve problems. Every project and data set is different. Data science is an iterative process that requires comfort with uncertainty, as at any point, you may have to go backward or even start over. A good project may inspire further questions that set the goals for your next project.\n\nAs we progress through this process, this course will have extra emphasis on a lot of the art sides of data science. Exploring data, cleaning it, and engineering it for use in a model.\n\nBefore we get started, as a cutting-edge technology, Spark changes fast and frequently. Make sure you're looking at the right version. You can always go to the latest URL by using the slash latest or put the version number major, minor, and patch to get a specific version. Programmatically, you can check your version of Spark with these commands. That way, you can ensure you're looking at the right documentation and not using deprecated methods. For this course, we will be using a Parquet file. Like most data in Hadoop, the platform that Spark runs on, it is a write once, read many times format. Parquet is columnar, meaning that is organized by columns, an important feature for huge data sets as it is blazingly fast to read in only the data you need. CSVs on the other hand have to read and parse the whole data set to read a single field. Another difference is Parquet fields are defined and typed saving users from defining data types like dates, booleans, or strings. For this reason, Parquet is relatively slow to write. Since it's not delimited by characters, it's less likely to be read in wrong if those characters exist in the data. These are just a few of the advantages that are causing the industry to adopt Parquet quickly.\n\nWe have many format readers to choose from for converting various file types to a PySpark DataFrame. Here we will use spark read parquet and put the results into variable df representing a DataFrame. In this video we covered off on some important considerations when starting any data science project. We also learned about Parquet and how to load it to a Spark DataFrame. In the exercises you'll verify the versioning of PySpark and Python. And finally, you'll load the data yourself.\n\nWhat's the point of doing an analysis if you aren't solving the right problem? In this video, we'll define our problem in the context of our data. We are going to build a model to predict how much a house sells for. This question can be interpreted multiple ways, which is why it's important to take the time to formally define it. Let's assume we are real estate tycoons looking for the next best investment opportunity.\n\nFor a given house on the market with a listed price and a series of attributes describing the home, what is it likely to actually sell for, aka the sales close price? The data set we have is a sample of homes that sold over the course of 2017. Using this sample, we are to provide a quick proof of concept of whether it's worth investing in more data for the 5.5 million homes that sold in the US in 2017.\n\nTo do this, we need to understand some of the limitations of the data we have. First, we only have a small geographical area, so to apply our model to new areas poses serious risk. We know that we only have residential data, so we shouldn't expect to predict how much a business location is worth. Lastly, we only have one year's worth of data, which will make it hard to draw strong conclusions about the seasonality in the data set.\n\nThe original data set has hundreds of attributes available, but in order to start simple, we've already worked with our client to identify around 50 attributes they think are likely to influence the price of a home. These attributes generally fall into these groups. For dates, we have date listed, and the year the home was built. For location data, we have the city that the home is in, its school district, and its actual postal address.\n\nWe also have many different metrics to gauge the size of a home like number of bed and bathrooms as well as the area of living space. For prices, we have the listing price and we wouldn't be able to predict anything without the sales price. We also have a lot of data available on the amenities that a house has like a pool or garage as well as the construction materials that were used to build the house.\n\nBig data means a lot can go wrong when loading data. Make sure you have the right number of records and columns. We can use df count to get the row count, df columns to get the list of columns, and we can take the length of df columns to get the number of columns. When we use Parquet, it set the data types for all of our fields, which is a huge advantage over CSV. It's still worth checking, especially if you weren't the one defining it. Here we can use dtypes on our data frame to create a list of tuples containing a column name and its corresponding data type. In this video, we learned about the data set we will be using and the problem we will be trying to solve. Additionally, we learned how to check to see if our data loaded properly by checking rows, columns, and data types. Now, it's your turn to apply what you've learned in the exercises to verify our data got loaded correctly.\n\nData comes in all shapes and sizes. In the field, you will be tasked with using less than perfect data. This means you'll need to understand its strengths, weaknesses, and limitations to leverage it effectively. To get started with understanding your data, take a peek at each column to see what they contain. The describe function provides some bare bone basics of count, mean, standard deviation, min, and max. You can run on the whole data frame, a single column or list of columns. Remember to add show at the end if you wish to immediately display the results. To further help us understand our data, PySpark has many built-in descriptive functions available.\n\nThe mean function is considered an aggregate function and as such it needs to be passed to the a method along with the column to run it on as a dictionary to force it to return results immediately. Use collect. Coariance is a function that lets us see how two variables vary together. This function is applied to a data frame and takes two numeric columns and returns a value. An excellent way to explore your data is through statistical plotting. Seaborn is a Python data visualization library designed specifically for this. We will look at a few plotting examples but there are many many more for you to follow up on. We can plot data using non-Spark libraries like Seaborn but they require converting your PySpark data frame to a pandas data frame. Be aware that converting large data sets can cause pandas to crash. This is because PySpark is made for massive data sets whereas pandas is not. The sample function can help us get a smaller data set to plot. Here we will keep sampling with replacement off. Take 50% of the data and set a random seed for reproducibility. Using count shows us the number of records has changed. We will leverage Seaborn's disc plot which will show us the distribution of our dependent variable sales close price. Please note there are many optional parameters which aren't covered here. Here we will import Seaborn then filter the Spark data frame down to the sales close price column and then sample it. Then we convert it to a pandas data frame so we can use it with Seaborn. Lastly call this plot function with pandas_df to plot. After plotting we can see that most of the data is pushed to the left. Something that may need to be remedied depending on the model type we choose. We will cover one option log scaling in adjusting data later in this course. Another great plot to use is LM plot. LM is short for linear model and allows us to quickly see if there's a linear relationship between two variables. For this example, we will look at how sales close price changes depending on square footage above ground.\n\nTo do this, we will import Seaborn, filter our data set to the two columns, sample it, and convert it to a pandas data frame. Lastly, use the SNS LM plot function with our X and Y columns and the data frame. Here we can see that there's what looks to be a strong relationship between the size of a home and the price it sells for. Therefore, we might make the assumption that square footage above ground is a good variable to consider in the predicting of house prices. In this video, we explored our data with numerical summaries and visualizations. Now, it's your turn to try them out.\n\nMore data is better, right? Not if it's bad data. The saying garbage in, garbage out, is doubly true in data science. Data has a lot of places where it can get messed up. Data may be recorded incorrectly or contain extreme events. Inconsistent formatting, such as mixing numeric and text data, can make a field hard to use. Duplications can add redundant observations. Missing data can cause your analysis to have blind spots. Lastly, sometimes the data is just not relevant to the analysis. Failure to account for bad data can set your analysis up to fail. Please be careful.\n\nInspecting our data, we see there are some columns that are not worth including in our analysis. No is just the record number. Unit number is the apartment or house number and class is completely constant. Dropping the columns requires entering a single column name or passing a list of columns to drop. Here we pass a list called calls to drop to the function and then drop them. Please note the star which tells the function to unpack the list and feed them to the function one by one. A common task in cleaning your data will be filtering it. Here we will filter records that contain specific text values. Where applies the filter to the data frame records like creates a true false condition for the records. The till day provides a way to take the opposite or a not. The PySpark code reads, \"Filter the data frame where the potential short sale field is not like the string not disclosed.\" For initial pass of the model, it might be helpful to remove large outliers. One definition of an outlier for near-normally distributed data is something that occurs more than three standard deviations from the mean. Only 0.3% of the data should be filtered. Remember that outliers occur on both sides, so filter on both sides of the mean. Here we will filter extreme values from the list price column. To start we will use the aggregate function standard deviation and mean. Then use collect to force the calculation to run and use the 000 index to access the values. Lastly, we created a multiconditional filter which is just two boolean statements and together. It reads filter where the list price is less than the high bound and more than the low bound. Dealing with missing data is something we will cover later as dropping data is usually a naive approach. Nevertheless, it's important to sometimes take shortcuts to quickly prove out the basis for further work. Drop NA does what you'd expect it to do. In its basic form, it will remove a record where there's any null value in any column. You can get more specific as we'll see on the next slide.\n\nIn the first example, we'll drop any record that contains a null value. The second example, we'll look at only two columns and if both are null, then we will remove the record. Lastly, we can apply a threshold across all the columns and say if more than two columns contain null values, remove the entire record. Duplicates occur when two or more records contain the exact same information. Often this can happen after you drop columns or join data sets. Drop duplicates will drop the first duplicate it finds. Since Spark is distributed, which one it finds may or may not be in the order of how your file was loaded. If you want to be more picky about where you're looking for duplicates, you can specify a list of column names to look for them there specifically.\n\nIn this video, we learn why we might have bad data and several ways to remove it so it doesn't adversely impact our analysis. Let's see you take a shot at removing some data in the exercises. Jeff Hooper of Bell Labs once said, \"Data does not give up its secrets easily. It must be tortured to confess.\" This lesson will arm you with the tools to get your data to behave. Real data is ugly and rarely comes ready to be analyzed. Many algorithms and statistical methods have assumptions that a variable conforms to. If our data doesn't fit these criteria, all hope isn't lost yet. We can try mathematical operations to adjust the data to become the beautiful butterflies our methods require. One common transformation is scaling. For many algorithms like KN&N or regression, you need to ensure all your variables are on the same scale. One variable can't be from 1,000 to 5,000 and another between 01 and 002. These algorithms will try to red\n\n\nReduce the errors in the first variable much more than the second.\nWe can avoid this by scaling each feature between zero and one.\nThis is called minmax scaling and doesn't change the shape of the distribution, only its range.\nTo minmax scale, take the variable to be scaled, subtract the minimum value, and divide by the difference between the max and the min.\nTo scale our data, we need to first find the min and max values of the column we want to scale.\nHere we are using aggregate functions min and max.\nWe will use collect to force the calculation to run and use the 0 0 index to access the values.\nTo create a new column, we will use with column that creates a new column based off of some sort of transformation to an existing one.\nIn this case, days on market.\nLastly, we can see that our values are now between zero and one.\nAnother common restriction is that data must closely follow the standard normal distribution.\nStandardization or Z transforming is the process of shifting and scaling your data to better resemble a standard normal distribution, which has a mean of zero and a standard deviation of one.\nIn the image, you can see how the original data in blue shifts to green where it has a mean of zero and then the final step scales to the standard normal distribution in red.\nTo z transform our data, we calculate the aggregate functions mean and standard deviation of the column we are transforming.\nSince we want to use the values in the next step, we will use collect to immediately calculate them and use the index values of 0 0 to access the return values.\nWe can then apply the standardization formula to our column and put the results into a new column ZR days by using with column.\nLastly, we can verify the transform data does have approximate mean of zero and standard deviation of one.\nOur data for sales close price is pushed to the left.\nThis is called positive skew.\nOne potential way to treat skewed data is to apply a log transformation on the data.\nThis has the impact of making our data look more like a normal distribution.\nTo apply a log transformation, you will need to import the log function from PySpark SQL functions.\nWe can then create a new column log sales close price based on the application of the log function on sales close price.\nIn this video, you learn why and how to apply transformations to your data.\nNow, it's time for you to adjust some data.\nMissing data is frustrating.\nIn this lesson, we will touch on a few ways to handle it.\nHow does data go missing in the digital age?\nSensors can fail.\nSurveys can miss people, or new ways to measure things can cause gaps in data sets.\nData storage rules can force data that doesn't fit the specified type to be null.\nFor example, dates in different formats, abbreviations, or a currency with a comma instead of a period.\nJoining data sets can enrich your model, but can induce missing values if they are not captured at the same granularity.\nIf you combine daily data with monthly data, it will create gaps for all the days where the monthly data was not captured.\nLastly, data can be missing intentionally.\nAttributes used in combination might be enough to compromise privacy.\nThis can be seen in government data sets like the census where they will omit data if there is a concern.\nUnderstanding why your data is missing is important.\nMissing completely at random occurs when the data is missing with no pattern.\nYour data is likely still representative of the whole population.\nMissing at random occurs when the probability of missing data on the y variable is unrelated to the value of y.\nFor example, suppose males are less likely to answer a depression survey.\nThis has no relationship with their level of depression after accounting for maleness.\nMissing not at random is when the value that is missing is related to the reason why it's missing.\nSupposing that people with severe health problems do not answer a question asking them to rank their health would indicate missing not at random.\nEarlier we showed how to use the function drop NA, but we didn't talk about when to use it.\nIf your data only has a few missing values and they are missing completely at random, it may be fine to remove the rows.\nBut how can we check to see how many missing values we have in our data set?\nWe can use the isnull function.\nIt returns true if the condition is true.\nHere we use it to filter our data to records where they are null and then count them.\nWe can also use Seabor to help us visualize missing values by leveraging the heat map function.\nUsing the same steps as before, we sample our data, convert it, and then use Seabor to plot the heat map.\nNote we use the Pandas data frame is null to convert the data frame to a true false for its null values.\nHere we can see the missing values as white spaces on the chart.\nAnother way to handle missing values is to replace them.\nThe replacement value might be based on business rules such as missing sales means there were no sales and replace them with zero.\nIf the missing data is missing completely at random, it may make sense to impute them using the mean or the median.\nAnother option could be to use interpolation, creating another model to predict their values.\nReplacing values shouldn't be done without some serious considerations.\nMake sure you research the appropriateness.\nTo replace missing values, we will use PySpark's fill NA, which takes the value to use for replacement as well as a list of column names.\nHere we will replace values with zero.\nWe can also replace values with the mean by calculating it using an aggregate function as we have done before.\nThen the mean only needs to be placed in the fill NA function.\nIn this video, you learned about the types of missing data, how to assess missing values, and some methods to treat them.\nTake some time to do the exercises and try out what you've learned.\nThe world of Big Data means having access to much more information to include in our analysis.\nIn this video, we will cover how to connect additional data to our data set.\nExternal data is a wonderful way to boost model performance, but there are pros and cons to choosing to include it.\nAdding external data may add excellent predictors for a model, but adding too many features may impact the performance of the model.\nThere is serious risk of spurious correlations between variables in the world of Big Data.\nExternal data can be a great way to replace missing or aggregated values with a better feature.\nAdding data comes at the risk of inducing data leakage.\nIf we wish to include local crime information, we will have to ensure that buyers would also have access to that information at the time of purchasing a home.\nTo include data available later is leaking information from the future.\nAnother consideration is how cheap and easy it is to obtain.\nToday we have many more data sets easily available to us, but it may come at the cost of needing to become the subject matter experts to know the meaning and credibility of our sources.\nTo understand the different kinds of joins, we'll need to orient ourselves.\nWe'll call the original data set we started with the left and the one we wish to incorporate our right.\nThere are many ways to join data together.\nMost commonly the inner or left joins depending on your goals.\nFor us, we want to make sure we always keep the full left data set and add data where available from the right.\nThis means we will be using a left join.\nJoins can be done one of two ways in PySpark.\nFirst is the data frame join method.\nThe data frame that calls the join is the left data frame.\nOther is the right data frame, in this case the new data set.\nOn is the pair of column conditions we will match on.\nHow is the type of join to perform.\nSuppose we want to see the impact of home sold on bank holidays.\nWe can join the data frames together by creating a join condition where df offmarket date equals hdf dt.\nWe can then put this condition into our join function and use left to make sure we keep all the original records in df.\nLastly count holiday sales by using the till day to take the not of isnull.\nNot surprisingly no houses are sold on bank holidays.\nPerhaps later we can check to see if a holiday week impacts sales.\nThe second way we can join data frames together is to use Spark SQL, which allows us to apply SQL statements directly to data frames.\nThis may be your preferred method if you are familiar with SQL or attempting to do complicated joins or filters.\nTo do this, we need to register the data frame as a temp table and give it a name.\nOnce that's done, we can use Spark SQL to execute a query and return it back in the form of a data frame.\nIn this one, we are using select star to get all the columns available.\nUsing from df to create our starting table, left join hdf as the table we'd like to join with and using on df offmarket date equals hdf dt to create the join condition.\nIn this video, we learned that combining data sets can be powerful, but requires some caution.\nWe also learned how to join data in two different ways.\nIn the exercises, we will build on these and learn some things to watch out for.\nIn this video, we will learn a lot about the nuts and bolts of Feature Engineering.\nJust because it's called Machine Learning doesn't mean it can figure out everything on its own.\nSo, we will use some tricks to help it out by creating new features that will better capture patterns in the data.\nThis video will cover feature generation and show you how using the new features can improve a model.\nWhy generate new features if the information is already available in the data set?\nCombining features together can capture subtle dependent effects between them that impact the outcome variable.\nThese can be represented by multiplying, summing, differencing, or dividing two or more variables.\nTo see the impact of generating these features, let's suppose you have two attributes, length and width, and then the price of a singlestory home.\nIf these are your only two features, how can you best create a model to predict price?\nThey certainly don't look to be very strong features as is.\nTaking the previous example a step further, we can think about how a person might buy a home.\nIf we use some intuition that people often consider the area of a home, we can create a new feature, total square footage, by multiplying the width and length.\nThe results are much better with an R squ of 81.\nApplying your reasoning and understanding to the problem can help you build powerful predictors.\nOur data set doesn't include width and length because no one would ever actually look for a house that way.\nHowever, we don't have a total square footage calculated, but we can create it using width column and by adding square footage below ground and square footage above ground together.\nWe can build another feature price per square footage using our previous feature square footage.\nThis is now the combination of three independent variables.\nThere isn't a limit to how deep you can go, but the interpretability of what it means starts to become difficult after three.\nWe can create days on market as a difference between list date and offmarket date.\nWe will cover how to get list date and offmarket date into the datetime format in the next section, but for now know that new features can be generated many ways.\nThere's a major push in the data science community to automate some of the generation of features.\nIf this is of interest to you, I'd recommend you check out the Python libraries feature tools and ts.\nI will caution you that simply multiplying each feature pair-wise will square your number of features.\nThis can cause an explosion of features that can be unwieldy to model or could potentially overfit your model by pure coincidence.\nMany of the features may convey similar information and won't be needed.\nLastly, there's no limit to how many features you can combine, but the interpretability certainly takes a steep dive after three.\nBeyond this is the realm of deep feature generation, a topic for another course.\nIn this video, you learned that you can generate new powerful features to represent complex relationships between them.\nLastly, you saw that feature combinations are everywhere and that many are already in our data set.\nIt's your turn to take what you've learned and build and evaluate new features generated from what's available.\nIn this video, we will talk about using time in our models since it isn't as easy as throwing it into our models as a continuous variable.\nThings repeat.\nEach day has a noon, each week has a Monday, and each year has a January.\nWe want to help our model by building features that help it associate cyclical events with changes in our outcome variable, such as summer having a higher volume of homes sold than in winter.\nBuilding the right time features is important.\nThe high variation in daily number of homes sold makes this pattern hard for us to see and the model to understand.\nIf we change the aggregation to look at grouping by month, we can see the pattern much more clearly.\nChoosing the right level to build out time related features is important as too granular and they are too noisy for our model, too broad and our model misses trends.\nTo work with dates, we need them to be of the Spark date type.\nWe can do the conversion with two date function that takes a single column.\nIf you wish to keep the time components, use two timestamp instead.\nWith our data typed correctly, we can use built-ins to get various time components.\nOne popular way to handle dates is to convert them into ordinal features like year or month using the functions year and month respectively.\nWe can also extract more complicated things like day number in the month with day of month or the week number in the year with week of year to further build out our features.\nMany more functions can be found in the PySpark SQL functions docs online.\nOne simple time-based metric is the number of days a property remains unsold from the date it was listed.\nDays on market is an important feature to buyers.\nThey may perceive that a house that has been on the market for a while has something wrong with it or that the seller may be more willing to give them a discount.\nWe can create this metric by applying the date diff function to offmarket date and list date columns.\nLagging time features is a common approach to add propagation time for a variable's effect to impact the outcome variable.\nThis is similar to how a drop creates waves that take time to hit the edge of our glass.\nTo capture this, we will shift values forwards or backwards until the timings line up.\nTo create a lagged feature, we'll need a few new functions.\nFirst, the window function.\nWindow allows you to return a value for each record based on some calculation against a group of records such as rank or moving average.\nThe second function is lag, a window function that returns a value that is offset by rows before the current row.\nIt takes a data frame column as an input.\nCount is how many periods you wish to lag.\nLet's see it in action.\nFor this example, we will look at lagging weekly mortgage rates as it often takes time for people to adjust the price of their homes.\nTo begin, we will need to import our new functions.\nThen we will create a window which will group things by our ordered date column,\n\n\nMaking our window weekly.\nOnce that is done, we can create a new column using the lag function, telling it to lag mortgage rate 30 US by one period.\nThe over function takes the window W so that the lag knows how to compare to the current record.\nNow it's your turn to create some of your own time related features with built-in datetime functions as well as more complex ones using the window function.\nData sets frequently have rich features trapped in messy combination fields, lists, or even free form text.\nIn this video, we'll go over how to wrangle columns into useful information for machine learning.\nWe can see that within this roof column, there are many useful features.\nFor instance, an old roof is very expensive to replace, and knowing that might impact the price of the house.\nThe age in this data set is either over 8 years or less.\nThis will be better as a boolean variable 0 or 1, something that we can calculate on.\nTo create a boolean column, we will use the when function to create an if then.\nThe when function evaluates the boolean condition and then does something.\nIn this case, our boolean conditions are find over 8 and find under eight, which use the like function to return true false depending on if the string is found.\nYou might notice that we use the percent sign before and after the string we are looking for.\nThese are wild cards that allow any number of characters before or after the string.\nNow that we have these conditions created, we can put them into the when function.\nWhen find over 8 is true, assign one.\nWhen find under eight is true, assign zero.\nIf neither is true, the otherwise function allows us to assign none.\nSo they are null.\nWe can see that the roof age has now been created into a new boolean variable.\nLet's look at the roof column again.\nYou'll notice that if there is a value in roof, it seems to be a list starting with the type of materials it was made out of.\nIf we know the pattern, we can split this into its own column called roof material.\nTo split a column, we need to introduce a new function from the PySpark SQL functions module, split, which takes a column to split and a character to split on.\nIn our example, we will split on DF roof and use the comma as the delimiter between the values.\nOnce we have that created, we can use our familiar width column to create a new column from the first value with split get item zero.\nGet item zero takes the zero index position of the split column and returns the value.\nHere we can verify our code performed as expected.\nSplitting the roof column and putting the first value into a new column roof material.\nWhat if the order of the listed values in a column is not guaranteed and we want to extract out all the values to their own columns?\nTo do this is a two-step process.\nThe first step is called exploding, changing a compound field so that each value has a separate record with everything else repeated.\nThe second step is to pivot these repeated fields into columns.\nYou'll notice how we have columns for each possible value in the compound field we started with.\nTo do this in PySpark, we need to import several functions: split, explode, lit, coalesce, and first.\nThen we need to split our roof column into an array column.\nNow we can explode our roof list to create a new record for each value.\nNext, we'll create a constant column to help our pivot.\nThe pivot function will group by our record ID so that only one row is returned for pivoting X roof list.\nSince pivot is an aggregate function, we will use a constant value column with coalesce to ignore nulls and first to take the first value.\nIn this video, we learned how to salvage some messy fields into machine learnable features.\nNow, it's your turn to wrangle some features on your own.\nThis video will cover the basics of binarizing, bucketing, and encoding with PySpark with Spark ML transformers.\nThese methods are great ways to get the most out of your features.\nBinarization of data is a helpful way to collapse some of the nuance in your model to just a yes no.\nHomeowners often use yes no filters to narrow their search for homes.\nFor example, they may only consider homes that have a fireplace but not care about how many fireplaces as long as it's more than one.\nBinarization takes values below or equal to a threshold and replaces them by zero values above it by one.\nFor this example, we will leverage the Spark ML feature transformer binarizer.\nIntroduction of PySpark showcase transformers in detail, so we'll spend time just using them.\nAfter importing binarizer, we need to make sure the column we want to apply it on is of type double.\nWe need to create a transformation called bin with the binarizer class.\nSetting the threshold to zero so that anything over zero will be converted to one.\nThen set our input call to fireplaces and our output to fireplace t.\nTo apply the transformation, we apply transform with our data frame.\nWe can see the transformation worked as expected below.\nIf you're a homeowner, you might want to know that a house has 1, 2, 3, or more bathrooms.\nBut once you hit a certain point, you don't really care whether the house has seven or eight bathrooms.\nBucketing, also known as binning, is a way to create ordinal variables.\nLike the binarizer, we'll import bucketizer.\nThen we need to define our splits for buckets of values.\nWe want 0 to 1 mapped to 1.\nGreater than 1 to 2 mapped to two.\nGreater than 2 to 3 mapped to three.\nAnd lastly, anything more than four map to four using the infinity value float inf for the upper bound.\nThen we can create the transformer buck with our splits, the input column and the output column.\nWe can then apply the transformer to our data frame with transform.\nAs you can see, the transformation created buckets for our values correctly.\nSome algorithms cannot handle categorical data like the text field city and it must be converted to a numeric format like the ones to the right to be evaluated correctly.\nOne method to handle this is called one hot encoding where you pivot each categorical variable to a true false column of its own.\nKeep in mind for columns with lots of different values, this can create potentially hundreds or thousands of new columns.\nTo apply one hot encoder transformer, we need to do it in two steps.\nFirst, we need the string indexer transformer.\nThe string indexer takes a string in and maps each word to a number.\nThen we can use the fit and transform methods to perform the mapping and transform the strings to numbers.\nNow we can apply the one hot encoder transformer on our index city values and output all the encoded indexes to a single column of type vector which is more efficient than storing them all as individual columns.\nAnother thing to note is that the last category is not included by default because it is linearly dependent on the other columns and is not needed.\nIn this video, we learned how to group values together as well as how to convert categorical values to numeric.\nYou will apply these transformers in the following examples.\nGood luck.\nPySpark has many different machine learning algorithms available to choose from.\nWhile this makes our ability to predict, classify or cluster on enormous data sets easier, the onus is on us to choose the correct one.\nThis flowchart can help us navigate what's available in PySpark's machine learning library for data frames, ML.\nRecall that we are going to predict the price of a home.\nThis price is a quantity, in this case of dollars, and it is continuous.\nThat takes us to the regression archetype, which predicts continuous values.\nLastly, we can see that algorithms for solving our problem can be found within the ML regression module.\nML regression provides us with many different algorithms we could use.\nThese first methods differ mostly in how they regularize, which is how they prevent themselves from finding overly complex solutions that are likely to overfit the data.\nWhile these methods can be powerful if used correctly, they require a lot of upfront work to ensure their assumptions are met.\nML regression also contains treebased methods which have the ability to easily handle things like missing and categorical variables right out of the box.\nDecision trees are easy to interpret but a lot of work needs to go in to prevent overfitting.\nSo now we are down to two algorithms random forest and GBT regression which differ mostly in how they handle error reduction.\nWe will choose to evaluate both random forests as well as gradient boosted trees or GBT regression.\nBoth random forest and gradient boosted tree models are example of ensemble models.\nThey combine many smaller models together to create a more powerful model.\nIn the diagram, you can see that we have many decision trees, each only trained on a sample of the data to prevent overfitting.\nWhen it comes time to predict, a new value runs through the decision trees and they merge their answers together to create a prediction.\nIf you've had some exposure to machine learning, you may have seen the crucial step of splitting your data into test and training sets, which needs to be done before applying feature transformations.\nCommonly, data is split randomly.\nOurs contains a time component.\nSo, splitting randomly would leak information about what happens in the future.\nTo prevent this, you can split your data sequentially and train your model on the first sequences and then test it with the last.\nThe size of your sets depend on how far out you need to forecast.\nDoing incremental testing is called step forward optimization.\nHere we will create just one of the sequential test train splits.\nWith some added logic, you could build out walk forward optimization seen previously.\nFirst, we'll dynamically set our time variables.\nIt's important as when your data set refreshes, you don't have to remember to change them.\nTo start, we'll calculate the min and max of offmarket dates.\nThen we can put them into our date diff function to get the number of days our data spans.\nTo create an 80/20 split, we can multiply it by 0.8 and add it to our min date with date add to get the date value.\nWe can create our train and test sets by using a wear function on DF offmarket date to filter them.\nAn extra wear is needed on list state to ensure it contains items listed as of the split date.\nIn this video, we saw how to navigate PySpark ML and a few considerations in the algorithm selection process.\nLastly, you learned how to create tests and training sets for time series.\nLet's see you try.\nEach machine learning algorithm has its own assumptions you need to take into account for it to work appropriately.\nIn this video, we will cover what the assumptions are for random forest regression, what features we have in our final data set, and lastly, how to get them ready for building a model.\nThe lack of assumptions needed for random forest regression make it and its related methods some of the most popular choices for predicting continuous values.\nFor example, random forests are able to work with non-normally distributed data or data that is unscaled.\nMissing and categorical data can be handled very easily with value replacements.\nAdding in external data sets is one of my personal favorite parts of modeling.\nIt's where I find that you can often make huge improvements in your model relatively easily.\nHere are a few that I added: 30-year mortgage rate to see how much people are willing to pay depending on their rate, city data to see how unique a house is in the area or if it's exceptionally cheap or expensive.\nTransportation metrics can help us understand how much people are willing to pay for a convenient location.\nLastly, I included bank holidays to see if that impacted how or when houses were sold.\nBy all means, this is not an exhaustive list of data sets to include, but just some I chose.\nEven though we were able to avoid a lot of the ownorous pre-processing steps by using random force regression, there's still plenty of work to do with engineering features.\nTime components like month or week that a holiday falls on are needed to help attribute seasonal effects.\nValuable, but often the hardest to create features are rates, ratios, and other generated features that need either business or personal context to create.\nLastly, choosing whether or not to expand compound fields is ultimately a judgment call and may be something to consider in the second iteration of your modeling.\nSince PySpark data frames don't have a shape attribute, we'll have to print our own to inspect the final set of information.\nPySpark ML algorithms require all of the features to be provided in a single column of type vector.\nWe will need to convert our columns for random forest regression to work.\nTo do this, we need to import vector assembler transformer to use it later.\nSadly, while random forest regression can handle missing values, vectors cannot.\nDue to the nature of how treebased machine learning partitions data, we can just assign missings a value that is outside the range of the variable and replace the nulls with it.\nBut first, we need to know which columns to convert.\nWe can take a list of column names and remove our dependent variable so the vector contains only features.\nTo create a vector assembler, we need to supply it with our list of columns and a name for output.\nApplying the transformation is done via the transform method.\nLastly, we need to create a new data frame with just the columns that matter sales close price and features.\nFinally, we are ready for machine learning.\nOur features have been created and prepared for the algorithm that we are running.\nNow it's your turn to convert the columns to vectors and get ready for applying random forest regression.\nWe've covered all the necessary steps to prepare ourselves for modeling.\nIn this video, we will cover training, predicting, and evaluating a random force regression model.\nThe PySpark random force regressor method has a ton of optional parameters and a few hyperparameters used for tuning.\nTo have a minimally viable model, you will need to set only a handful.\nFirst up is features call which tells the model which column is the vector we created with the vector assembler that now represents all of our feature data.\nSince we named the column features, we will use that to set features call.\nNext up is label call which sets the dependent variable for the model.\nOurs is named sales close price.\nThen we need to name our output column by setting prediction call.\nI find it helpful to be explicit rather than leaving it the default value.\nSo I've named it prediction price.\nLast of the basic parameters is seed which by setting this to a value we can ensure that subsequent runs return the same model.\nWithout it, the random forest would be slightly different.\nI have set mine to 42 for good luck, but the specific number isn't important.\nEnough talking.\nLet's build our model.\nTo start, we need to import the random forest regressor from PySpark's ML module.\nOnce that's done, we can initialize random forest regressor with the appropriate columns to use for training and predicting.\nAgain, setting the seed is crucial for repeatability.\nLastly, we create a variable to hold our train model uninspiredly called model and train the random force regressor RF\n\n\nBy calling fit with our training data frame train DF. Congratulations, you've created a model. Wait, you want to predict new values with it? Predicting house prices with the model we just trained is straightforward. To do so, we can call transform with the data withheld from training, the test set testd.\nIf you had new listings of homes and wanted to predict their prices, you'd merely have to pre-process it in the same manner as testd before using the model to predict prices. Given that testdf has the actual home sale prices, we can inspect them side by side by using the select to grab only the columns we care about and displaying them with show. Predicting values is great, but if we don't know how good we are at it, then what's the point?\nTo evaluate the model, we need to import regression evaluator, which allows us to calculate various metrics to gauge model performance. To initialize it, we need to provide the actual values, in this case, sales close price, and the predicted values, which we named prediction price when we created the model. Once we have the instance of evaluator created, we can call it with our predictions data frame and a dictionary of the metric type we wish to evaluate it with. Which metric you choose to optimize is an important decision to make. We can see that our model's RMSSE returns a value in the thousands while our R squared is less than one. R squared is easy to interpret regardless of what you are predicting. If it's zero, you are no better than random chance. If it's one, you are predicting perfectly. On the other hand, RMSSE provides an absolute number of unexplained variance in our model. It's even in the same units as our prediction, US dollars.\nSo even though our R squared is really high, RMSSE indicates that we have $22,000 of unexplained variance on average. This video showed you little code is needed to train, predict, and evaluate a model with PySpark. Now it's your turn to build some models on your own. In this video, we will go over how to interpret the model and then how to save and load it for later use.\nNow that we've evaluated our model, we will want to understand what features are important in predicting a home selling price. To do this, we will need to import the Pandas library to manipulate this tiny array easier. To use Spark on this would be using a sledgehammer for a delicate task. We will create a data frame f to hold our feature importances. These feature importances can be accessed by calling feature importances on the model and then converting them to an array with two array. Since this is just an array of numbers, we will need to name the new column in the data frame importance. Now we just have a single column data frame. We will want to create another column using the list of feature names we fed into the vector assembler earlier. We can convert this list into a series by wrapping it with PD series.\nNext, since we have over a 100 features, we only want to look at the most important ones. So, we will use Pandas sort values to sort the column importances in descending order. Now, it's as simple as displaying out the results to the screen. Here, we can see the biggest predictor of how much your house will sell for is how much you listed it for. Intuitively, this makes a lot of sense. Realators are skilled in setting the value of the house and it has the effect of anchoring the price, meaning it will likely only marginally increase or decrease from that value. Last but not least, it's important to know how to save and load the model. Luckily, this is very simple. Now, to save it, just call save on your model and give it a model name. Note that the model isn't a single file, but a directory containing many files defining your model.\nTo load your data, you need to import random forest regression model from PySpark ML regression and provided the location and the name of your model. In this video, we learned how to interpret results and save and load a model for later use. Let's see you give it a try. Thank you for making it to the end of this course. Let's take a minute to review what we've learned. In this course, you learned many ways to inspect your data visually and statistically, data manipulation techniques such as dropping rows and columns, scaling and adjusting data, and handling missing values. You also learned how to enrich your data sets by joining external data sets, generating features, extracting variables from messy fields, and various encoding techniques.\nLastly, you learned a bit about how all this ties into modeling by training and evaluating a model and interpreting the results. That sure was a lot of stuff we covered in a short amount of time. I hope you will be able to take what you've learned here and apply it to real-world problems and make the world a better place with data science. John Hogout. Hi, welcome to the course on machine learning with Apache Spark in which you will learn how to build machine learning models on large data sets using distributed computing techniques. Let's start with some fundamental concepts. Suppose you wanted to teach a computer how to make waffles. You could find a good recipe and then give the computer explicit instructions about ingredients and proportions.\nAlternatively, you could present the computer with selection of different waffle recipes and let it figure out the ingredients and proportions for the best recipe. The second approach is how machine learning works. The computer literally learns from examples. Machine learning problems are generally less esoteric than finding the perfect waffle recipe. The most common problems apply either regression or classification. A regression model learns to predict a number. For example, when making waffles, how much flour should be used for a particular amount of sugar. A classification model, on the other hand, predicts a discrete or categorical value.\nFor example, is a recipe calling for a particular amount of sugar and salt more likely to be for waffles or cupcakes? The performance of a machine learning model depends on data. In general, more data is a good thing. If an algorithm is able to train on a larger set of data, then its ability to generalize to new data will inevitably improve.\nHowever, there are some practical constraints. If the data can fit entirely into RAM, then the algorithm can operate efficiently. What happens when those data no longer fit into memory? The computer will start to use virtual memory and data will be paged back and forth between RAM and disk. Relative to RAM access, retrieving data from disk is slow. As the size of the data grows, paging becomes more intense and the computer begins to spend more and more time waiting for data. Performance plummets. How then do we deal with truly large data sets? One option is to distribute the problem across multiple computers in a cluster. Rather than trying to handle a large data set on a single machine, it's divided up into partitions which are processed separately. Ideally, each data partition can fit into RAM on a single computer in the cluster. This is the approach used by Spark.\nSpark is a general-purpose framework for cluster computing. It is popular for two main reasons. It's generally much faster than other Big Data technologies like Hadoop because it does most processing in memory and it has a developer-friendly interface which hides much of the complexity of distributed computing.\nLet's review the components of a Spark cluster. The cluster itself consists of one or more nodes. Each node is a computer with CPU, RAM, and physical storage. A cluster manager allocates resources and coordinates activity across the cluster. Every application running on the Spark cluster has a driver program. Using the Spark API, the driver communicates with the cluster manager which in turn distributes work to the nodes. On each node, Spark launches an executive process which persists for the duration of the application. Work is divided up into tasks which are simply units of computation. The executives run tasks in multiple threads across the cores in a node.\nWhen working with Spark, you normally don't need to worry too much about the details of the cluster. Spark sets up all of that infrastructure for you and handles all interactions within the cluster. However, it's still useful to know how it works under the hood. You now have a basic understanding of the principles of machine learning and distributed computing with Spark. Next, we'll learn how to connect to a Spark cluster. The previous lesson was high-level overviews of machine learning and Spark. In this lesson, you'll review the process of connecting to Spark. The connection with Spark is established by the driver which can be written in either Java, Scala, Python, or R.\nEach of these languages has advantages and disadvantages. Java is relatively verbose, requiring a lot of code to accomplish even simple tasks. By contrast, Scala, Python, and R are high-level languages which can accomplish much with only a small amount of code. They also offer a ripple or read evaluate print loop which is crucial for interactive development.\nYou'll be using Python. Python doesn't talk natively to Spark. So, we'll kick off by importing the PySpark module, which makes Spark functionality available in the Python interpreter. Spark is under vigorous development. Because the interface is evolving, it's important to know what version you're working with. We'll be using version 2.4.1, which was released in March 2019. In addition to the main PySpark module, there are a few submodules which implement different aspects of the Spark interface. There are two versions of Spark machine learning. MLlib, which uses an unstructured representation of data and RDDs and has been deprecated, and ML, which is based on a structured tabular representation of data and data frames.\nWe'll be using the latter. With the PySpark module loaded, you're able to connect to Spark. The next thing you need to do is tell Spark where the cluster is located. Here there are two options.\nYou can either connect to a remote cluster, in which case you need to specify a Spark URL which gives the network location of the cluster's master node. The URL is composed of an IP address or DNS name and a port number. The default port for Spark is 7077, but this must still be explicitly specified. When you're figuring out how Spark works, the infrastructure of a distributed cluster can get in the way. That's why it's useful to create a local cluster where everything happens on a single computer. This is the setup that you're going to use throughout this course. For a local cluster, you need only specify local and optionally the number of cores to use. By default, a local cluster will run on a single core. Alternatively, you can give a specific number of cores or simply use the wild card to choose all available cores. You connect to Spark by creating a Spark session object.\nThe Spark session class is found in the PySpark SQL submodule. You specify the location of the cluster using the master method. Optionally, you can assign a name to the application using the app name method. Finally, you call the get or create method which will either create a new session object or return an existing object.\nOnce the session has been created, you're able to interact with Spark. Finally, although it's possible for multiple Spark sessions to coexist, it's good practice to stop the Spark session when you're done. Great. Let's connect Spark. In this lesson, you'll look at how to read data into Spark.\nSpark represents tabular data using the DataFrame class. The data are captured as rows or records, each of which is broken down into one or more columns or fields. Every column has a name and a specific data type. Some selected methods and attributes of the DataFrame class are listed here. The count method gives the number of rows. The show method will display a subset of rows. The print schema method and the dtypes attribute give different views on column types. This is really scratching the surface of what's possible with a DataFrame. You can find out more by consulting the extensive documentation. CSV is a common format for storing tabular data. For illustration, we'll be using a CSV file with characteristics for a selection of motor vehicles. Each line in a CSV file is a new record. And within each record, fields are separated by a delimiter character, which is normally a comma.\nThe first line is an optional header record, which gives column names. Our session object has a read attribute which in turn has a CSV method which reads data from a CSV file and returns a data frame. The CSV method has one mandatory argument, the path to the CSV file. There are a number of optional arguments. We'll take a quick look at some of the most important ones.\nThe header argument specifies whether or not there is a header record. The SE argument gives the field separator which is a comma by default. There are two arguments which pertain to column data types: schema and infer schema. Finally, the null value argument gives the placeholder used to indicate missing data.\nLet's take a look at the data we've just loaded. Using the show method, we can take a look at a slice of the data frame. The CSV method has split the data into rows and columns and picked up the column names from the header record.\nLooks great, doesn't it? Unfortunately, there's a small snag. Before we unravel that snag, it's important to note that the first value in the cylinder column is not a number. It's the string NA, which indicates missing data. If you check the column data types, then you'll find that they are all strings. That doesn't make sense since the last six columns are clearly numbers. However, this is the expected behavior. The CSV method treats all columns as strings by default. You need to do a little more work to get the correct column types.\nThere are two ways that you can do this. Infer the column types from the data or manually specify the types. It's possible to reasonably deduce the column types by setting the infer schema argument to true.\nThere is a price to pay though. Spark needs to make an extra pass over the data to figure out the column types before reading the data. If the data file is big, then this will increase the load time notably. Using this approach, all of the column types are correctly identified except for cylinder. Why? The first value in this column is in A. So Spark thinks that the column contains strings. Missing data in CSV files are normally represented by a placeholder like the NA string. We can use the null value argument to specify the placeholder. It's always a good idea to explicitly define the missing data placeholder.\nThe null value argument is case-sensitive. So it's important to provide it in exactly the same form as it appears in the data file. If inferring column type is not successful, then you have the option of specifying the type of each column in an explicit schema. This is what the final car's data look like. Note that the missing value at the top of the cylinders column is indicated by the special\n\n\nConstant.\nYou're ready to use what you've learned to load data from CSV files.\nIn this lesson, you're going to learn how to prepare data for building a machine learning model.\nYou'll be working with the cars data again.\nThis is what the data look like at present.\nThere are columns for the maker and model, the origin, either USA or non USA, the type, number of cylinders, engine size, weight, length, RPM, and fuel consumption.\nThe models that you'll be building will depend on the physical characteristics of the cars rather than the model names or manufacturers.\nSo you'll remove the corresponding columns from the data.\nThere are two approaches to doing this.\nEither you can drop the columns that you don't want or you can select the fields which you do want to retain.\nEither way, the resulting data does not include those columns earlier.\nYou saw that there is a missing value in the cylinders column.\nLet's check to see how many other missing values there are.\nYou'll use the filter method and provide a logical predicate using SQL syntax which identifies null values.\nThen the count method tells you how many records there are remaining, just one.\nIn this case, it makes sense to simply remove the record with the missing value.\nThere are a couple of ways that you could do this.\nYou could use the filter method again with a different predicate.\nOr you could take a more aggressive approach and use the drop NA method to drop all records with missing values in any column.\nHowever, this should be done with care because it could result in the loss of a lot of otherwise useful data.\nYou've now stripped down the data to what's needed to build the model.\nAt present, the weight and length columns are in units of pounds and inches, respectively.\nYou'll use the width column method to create a new mass column in units of kilograms.\nThe round function is used to limit the precision of the result.\nYou can also use the width column method to replace the existing length column with values in meters.\nYou now have mass and length in metric units.\nThe type column consists of strings which represent six categories of vehicle type.\nYou will need to transform those strings into numbers.\nYou do this using an instance of the string indexer class.\nIn the constructor, you provide the name of the string input column and a name for the new output column to be created.\nThe indexer is first fit to the data creating a string indexer model.\nDuring the fitting process, the distinct string values are identified and an index is assigned to each value.\nThe model is then used to transform the data, creating a new column with the index values.\nBy default, the index values are assigned according to the descending relative frequency of each of the string values.\nMidsize is the most common, so it gets an index of zero.\nSmall is the next most common, so its index is one, and so on.\nIt's possible to choose different strategies for assigning index values by specifying the string order type argument.\nRather than using frequency of occurrence, strings can be ordered alphabetically.\nIt's also possible to choose between ascending and descending order.\nYou'll be building a classifier to predict whether or not a car was manufactured in the USA.\nSo the origin column also needs to be converted from strings into numbers.\nThe final step in preparing the car's data is to consolidate the various input columns into a single column.\nThis is necessary because the machine learning algorithms in Spark operate on a single vector of predictors, although each element in that vector may consist of multiple values.\nTo illustrate the process, you'll start with just a pair of features, cylinders, and size.\nFirst, you create an instance of the vector assembler class, providing it with the names of the columns that you want to consolidate and the name of the new output column.\nThe assembler is then used to transform the data.\nTaking a look at the relevant columns, you see that the new features column consists of values from the cylinders and size columns consolidated into a vector.\nUltimately, you're going to assemble all of the predictors into a single column.\nLet's try out what we've learned on the SMS and flights data.\nYour first machine learning model will be a decision tree.\nThis is probably the most intuitive model, so it seems like a good place to start.\nA decision tree is constructed using an algorithm called recursive partitioning.\nConsider a hypothetical example in which you build a decision tree to divide data into two classes, green and blue.\nYou start by putting all of the records into the root node.\nSuppose that there are more green records than blue, in which case this node will be labeled green.\nNow from amongst the predictors in the data, you need to choose the one that will result in the most informative split of the data into two groups.\nIdeally, you want the groups to be as homogeneous or pure as possible.\nOne should be mostly green and the other should be mostly blue.\nOnce you've identified the most informative predictor, you split the data into two sets labeled green or blue according to the dominant class.\nAnd this is where the recursion kicks in.\nYou then apply exactly the same procedure on each of the child nodes, selecting the most informative predictor and splitting again.\nSo for example, the green node on the left could be split again into two groups and the resulting green node could once again be split.\nThe depth of each branch of the tree need not be the same.\nThere are a variety of stopping criteria which can cause splitting to stop along a branch.\nFor example, if the number of records in a node falls below a threshold or the purity of a node is above a threshold, then you might stop splitting.\nOnce you have built the decision tree, you can use it to make predictions for new data by following the splits from the root node along to the tip of a branch.\nThe label for the final node would then be the prediction for the new data.\nLet's make this more concrete by looking at the car's data.\nYou've transformed the country of origin column into a numeric index called label with zero corresponding to cars manufactured in the USA and one for everything else.\nThe remaining columns have all been consolidated into a column called features.\nYou want to build the decision tree which will use features to predict label.\nAn important aspect of building a machine learning model is being able to assess how well it works.\nIn order to do this, we use the random split method to randomly split our data into two sets, a training set and a testing set.\nThe proportions may vary, but generally you're looking at something like an 80/20 split, which means that the training set ends up having around four times as many records as the testing set.\nFinally, the moment has come.\nYou're going to build a decision tree.\nYou start by creating a decision tree classifier object.\nThe next step is to fit the model to the training data by calling the fit method.\nNow that you've trained the model, you can assess how effective it is by making predictions on the test set and comparing the predictions to the known values.\nThe transform method adds new columns to the data frame.\nThe prediction column gives the class assigned by the model.\nYou can compare this directly to the known labels in the testing data.\nAlthough the model gets the first example wrong, it's correct for the following four examples.\nThere's also a probability column which gives the probabilities assigned to each of the outcome classes.\nFor the first example, the model predicts that the outcome is zero with probability 96%.\nA good way to understand the performance of a model is to create a confusion matrix which gives a breakdown of a model predictions versus the known labels.\nThe confusion matrix consists of four counts which are labeled as follows.\nPositive indicates a prediction of one while negative indicates a prediction of zero and true corresponds to a correct prediction while false designates an incorrect prediction.\nIn this case, the true positives and true negatives dominate, but the model still makes a number of incorrect predictions.\nThese counts can be used to calculate the accuracy which is the proportion of correct predictions.\nFor our model, the accuracy is 74%.\nSo now that you know how to build the decision tree model with Spark, you can try that out on the flight data.\nYou've learned to build the decision tree, but it's good to have options.\nLogistic regression is another commonly used classification model.\nIt uses a logistic function to model a binary target where the target states are usually denoted by one and zero or true and false.\nThe maths of the model are outside the scope of this course.\nBut this is what the logistic function looks like.\nFor a logistic regression model, the x-axis is a linear combination of predictive variables and the yaxis is the output of the model.\nSince the value of the logistic function is a number between 0 and one, it's often thought of as a probability.\nIn order to translate the number into one or other of the target states, it's compared to a threshold, which is normally set at 1/2.\nIf the number is above the threshold, then the predicted state is one.\nConversely, if it's below the threshold, then the predicted state is zero.\nThe model derives coefficients for each of the numerical predictors.\nThose coefficients might shift the curve to the right or to the left.\nThey might make the transition between states more gradual or more rapid.\nThese characteristics are all extracted from the training data and will vary from one set of data to another.\nLet's make this more concrete by returning to the cars data.\nYou'll focus on the numerical predictors for the moment and return to the categorical predictors later on.\nAs before, you prepare the data by consolidating the predictors into a single column and then randomly splitting the data into training and testing sets.\nTo build a logistic regression model, you first need to import the associated class and then create a classifier object.\nThis is then fit to the training data using the fit method.\nWith the trained model, you're able to make predictions on the testing data.\nAs you saw with the decision tree, the transform method adds the prediction and probability columns.\nThe probability column gives the predicted probability of each class while the prediction column reflects the predicted label which is derived from the probabilities by applying the threshold mentioned earlier.\nYou can assess the quality of the predictions by forming a confusion matrix.\nThe quantities in the cells of the matrix can then be used to form some informative ratios.\nRecall that a positive prediction indicates that a car is manufactured outside of the US and that the predictions are considered to be true or false depending on whether they are correct or not.\nPrecision is the proportion of positive predictions which are correct.\nFor your model, 2/3 of predictions for cars manufactured outside of the US are correct.\nRecall is the proportion of positive targets which are correctly predicted.\nYour model also identifies 80% of cars which are actually manufactured outside of the US.\nBear in mind that these metrics are based on a relatively small testing set.\nAnother way of looking at these ratios is to weight them across the positive and negative predictions.\nYou can do this by creating an evaluator object and then calling the evaluate method.\nThis method accepts an argument which specifies the required metric.\nIt's possible to request the weighted precision and recall as well as the overall accuracy.\nIt's also possible to get the F1 metric, the harmonic mean of precision and recall which is generally more robust than the accuracy.\nAll of these metrics have assumed a threshold of 1/2.\nWhat happens if you vary that threshold?\nA threshold is used to decide whether the number returned by the logistic regression model translates into either the positive or the negative class.\nBy default, that threshold is set at a half.\nHowever, this is not the only choice.\nChoosing a larger or smaller value for the threshold will affect the performance of the model.\nThe ROC curve plots the true positive rate versus the false positive rate as the threshold increases from zero top right to one bottom left.\nThe AU summarizes the ROC curve in a single number.\nIt's literally the area under the ROC curve.\nAU indicates how well a model performs across all values of the threshold.\nAn ideal model that performs perfectly regardless of the threshold would have AU of one.\nIn an exercise, we'll see how to use another evaluator to calculate the AU.\nYou now know how to build a logistic regression model and assess the performance of that model using various metrics.\nLet's give this a try.\nIt's said that 80% of machine learning is data preparation.\nAs we'll see in this lesson, this is particularly true for text data.\nBefore you can use machine learning algorithms, you need to take unstructured text data and create structure ultimately transforming the data into a table.\nWe start with a collection of documents.\nThese documents might be anything from a short snippet of text like an SMS or email to a lengthy report or book.\nEach document will become a record in the table.\nThe text in each document will be mapped to columns in the table.\nFirst, the text is split into words or tokens.\nYou then remove short or common words that do not convey too much information.\nThe table will then indicate the number of times that each of the remaining words occurred in the text.\nThis table is also known as a term document matrix.\nThere are some nuances to the process, but that's the central idea.\nSuppose that your documents are the names of children's books.\nThe raw data might look like this.\nYour job will be to transform these data into a table with one row per document and a column for each of the words.\nYou're interested in words, not punctuation.\nYou'll use regular expressions or reax, a mini language for pattern matching to remove the punctuation symbols.\nRegular expressions is another big topic and outside the scope of this course, but basically you're giving a list of symbols or text pattern to match.\nThe hyphen is escaped by the backslashes because it has another meaning in the context of regular expressions.\nBy escaping it, you tell Spark to interpret the hyphen literally.\nYou need to specify a column name books.ext text, a pattern to be matched, stored in the variable reax, and the replacement text, which is simply a space.\nYou now have some double spaces, but you can use reax to clean those up, too.\nNext, you split the text into words or tokens.\nYou create a tokenizer object, giving it the name of the input column containing the text and the output column, which will contain the tokens.\nThe tokenizer is then applied to the text using the transform method.\nIn the results, you see a new column in which each document has been transformed into a list of words.\nAs a side effect, the words have all been reduced to lowercase.\nSome words occur frequently in all of the documents.\nThese common or stop words convey very little information.\nSo you will also remove them using an instance of the stop words remover class.\nThis contains a list of stop words which can be customized if necessary.\nSince you didn't give the input and output column names earlier, you specify them now and then apply the transform method.\nYou could also have given these names when you created the remover.\nYour documents might contain a large variety of words.\nSo in principle our table could end up with an enormous number of columns many of\n\n\nWhich would be only sparsely populated. It would also be handy to convert the words into numbers. Enter the hashing trick, which in simple terms converts words into numbers. You create an instance of the hashing TF class, providing the names of the input and output columns. You also give the number of features, which is effectively the largest number that will be produced by the hashing trick. This needs to be sufficiently big to capture the diversity in the words. The output in the hash column is presented in sparse format, which we will talk about more later on. For the moment, though, it's enough to note that there are two lists. The first list contains the hashed values, and the second list indicates how many times each of those values occurs. For example, in the first document, the word long has a hash of 8 and occurs twice. Similarly, the word five has a hash of six and occurs once in each of the last two documents.\n\nThe final step is to account for some words occurring frequently across many documents. If a word appears in many documents, then it's probably going to be less useful for building a classifier. We want to weight the number of counts for a word in a particular document against how frequently that word occurs across all documents. To do this, you reduce the effective count for more common words, giving what is known as the inverse document frequency. The word five, for example, occurs in multiple documents, so its effective frequency is reduced. Conversely, the word long only occurs in one document, so its effective frequency is increased. Inverse document frequency is generated by the IDF class. The inverse document frequencies are precisely what we need for building a machine learning model. Let's do that with the SMS data.\n\nIn the last chapter, you saw how to use categorical variables in a model by simply converting them to indexed numerical values. In general, this is not sufficient for a regression model. Let's see why. In the car's data, the type column is categorical with six levels: midsize, small, compact, sporty, large, and van. Here you can see the number of times that each of those levels occurs in the data. You use the string indexer to assign a numerical index to each level. However, there's a problem with the index. The numbers don't have any objective meaning. The index for sporty is three. Does it make sense to do arithmetic on that index? No. For example, it wouldn't be meaningful to add the index for sporty to the index for compact. Nor would it be valid to compare those indexes and say that sporty is larger or smaller than compact. However, a regression model works by doing precisely this arithmetic on predictive variables. You need to convert the indexed values into a format in which you can perform meaningful mathematical operations. The first step is to create a column for each of the levels. Effectively, you then place a check in the column corresponding to the value in each row. So, for example, a record with a type of sporty would have a check in the sporty column. These new columns are known as dummy variables. However, rather than having checks in the dummy variable columns, it makes more sense to use binary values where a one indicates the presence of the corresponding level. It might occur to you that the volume of data has exploded. You've gone from a single column of categorical variables to six binary encoded dummy variables. If there were more levels, then you'd have even more columns. This could get out of hand. However, the majority of the cells in the new columns contain zeros. The nonzero values, which actually encode the information, are relatively infrequent. This effect becomes even more pronounced if there are more levels. You can exploit this by converting the data into a sparse format. Rather than recording the individual values, the sparse representation simply records the column numbers and the value for the nonzero values. You can take this one step further. Since the categorical levels are mutually exclusive, you can drop one of the columns. If type is not midsize, small, compact, sporty, or large, then it must be van.\n\nThe process of creating dummy variables is called one-hot encoding because only one of the columns created is ever active or hot. Let's see how this is done in Spark. As you might expect, there's a class for doing one-hot encoding. Import the one-hot encoder estimator class from the feature subm module. When instantiating the class, you need to specify the names of the input and output columns. For car type, the input column is the index we defined earlier. Choose type dummy as the output column name. Note that these arguments are given as lists, so it's possible to specify multiple columns if necessary. Next, fit the encoder to the data. Check how many category levels have been identified. Six. As expected, now that the encoder is set up, it can be applied to the data by calling the transform method. Let's take a look at the results. There's now a type dummy column which captures the dummy variables. As mentioned earlier, the final level is treated differently. No column is assigned to type van because if a vehicle isn't one of the other types, then it must be a van. To have a separate dummy variable for van would be redundant. The sparse format used to represent dummy variables looks a little complicated. Let's take a moment to dig into dense versus sparse formats. Suppose that you want to store a vector which consists mostly of zeros. You could store it as a dense vector in which each of the elements of the vector is stored explicitly. This is wasteful though because most of those elements are zeros. A sparse representation is a much better alternative. To create a sparse vector, you need to specify the size of the vector, in this case, 8, the positions which are non-zero, in this case, positions 0 and five, noting that we start counting at zero, and the values for each of those positions, one and seven. Sparse representation is essential for effective one-hot encoding on large data sets. Let's try out one-hot encoding on the flights data.\n\nIn the previous lesson, you learned how to one-hot encode categorical features, which is essential for building regression models. In this lesson, you'll find out how to build a regression model to predict numerical values. Returning to the car's data, suppose you wanted to predict fuel consumption using vehicle mass. A scatter plot is a good way to visualize the relationship between those two variables. Only a subset of the data are included in this plot, but it's clear that consumption increases with mass. However, the relationship is not perfectly linear. There's scatter for individual points. A model should describe the average relationship of consumption to mass without necessarily passing through individual points. This line, for example, might describe the underlying trend in the data. But there are other lines which could equally well describe that trend. How do you choose the line which best describes the relationship? First, we need to define the concept of residuals. The residual is the difference between the observed value and the corresponding model value. The residuals are indicated in the plot as the vertical lines between the data points and the model line. The best model would somehow make these residuals as small as possible. Out of all possible models, the best model is found by minimizing a loss function, which is an equation that describes how well the model fits the data. This is the equation for the mean squared error loss function. Let's quickly break it down. You've got the observed values y sub i and the model values y subi. The difference between these is the residual. The residuals are squared and then summed together before finally dividing through by the number of data points to give the mean or average. By minimizing the loss function, you're effectively minimizing the average residual or the average distance between the observed and modeled values. If this looks a little complicated, don't worry. Spark will do all of the maths for you.\n\nLet's build a regression model to predict fuel consumption using three predictors: mass, number of cylinders, and vehicle type, where the last is a categorical which we've already one-hot encoded. As before, the first step towards building a model is to take our predictors and assemble them into a single column called features. The data are then randomly split into training and testing sets. The model is created using the linear regression class, which is imported from the regression module. By default, this class expects to find the target data in a column called label. Since you are aiming to predict the consumption column, you need to explicitly specify the name of the label column when creating a regression object. Next, train the model on the training data using the fit method. The trained model can then be used to make predictions on the testing data using the transform method. Comparing the predicted values to the known values from the testing data, you'll see that there is reasonable agreement. It's hard to tell from a table though. A plot gives a clearer picture. The dashed diagonal line represents perfect prediction. Most of the points lie close to this line, which is good. It's useful to have a single number which summarizes the performance of a model for classifiers. There are a variety of such metrics. The root mean squared error is often used for regression models. It's the square root of the mean squared error, which you've already encountered, and it corresponds to the standard deviation of the residuals. The metrics for a classifier like accuracy, precision, and recall are measured on an absolute scale where it's possible to immediately identify values that are good or bad. Values of root mean squared error are relative to the scale of a value that you're aiming to predict. So interpretation is a little more challenging. A smaller root mean squared error, however, always indicates better predictions.\n\nLet's examine the model. The intercept is the value predicted by the model when all predictors are zero. On the plot, this is the point where the model line intersects the vertical dash line. You can find this value for the model using the intercept attribute. This is the predicted fuel consumption when both mass and number of cylinders are zero and the vehicle type is van. Of course, this is an entirely hypothetical scenario. No vehicle could have zero mass. There's a slope associated with each of the predictors, too, which represents how rapidly the model changes when that predictor changes. The coefficients attribute gives you access to those values. There's a coefficient for each of the predictors. The coefficients for mass and number of cylinders are positive, indicating that heavier cars with more cylinders consume more fuel. These coefficients also represent the rate of change for the corresponding predictor. For example, the coefficient for mass indicates the change in fuel consumption when mass increases by one unit. Remember that there's no dummy variable for van. The coefficients for the type dummy variables are relative to vans. These coefficients should also be interpreted with care. If you are going to compare the values for different vehicle types, then this needs to be done for fixed mass and number of cylinders. Since all of the type dummy coefficients are negative, the model indicates that for a specific mass and number of cylinders, all other vehicle types consume less fuel than a van. Large vehicles have the most negative coefficient. So, it's possible to say that for a specific mass and number of cylinders, large vehicles are the most fuel efficient.\n\nYou've covered a lot of ground in this lesson. Let's apply what you've learned to the flights data. The largest improvements in machine learning model performance are often achieved by carefully manipulating features. In this lesson, you'll be learning about a few approaches to doing this. Let's start with bucketing. It's often convenient to convert a continuous variable like age or height into discrete values. This can be done by assigning values to buckets or bins with well-defined boundaries. The buckets might have uniform or variable width. Let's make this more concrete by thinking about observations of people's heights. If you plot the heights on a histogram, then it seems reasonable to divide the heights up into ranges. To each of these ranges, you assign a label. Then you create a new column in the data with the appropriate labels. The resulting categorical variable is often a more powerful predictor than the original continuous variable. Let's apply this to the car's data. Looking at the distribution of values for RPM, you see that the majority lie in the range between 4500 and 6,000. There are a few either below or above this range. This suggests that it would make sense to bucket these values according to those boundaries. You create a bucketizer object specifying the bin boundaries as the splits argument and also providing the names of the input and output columns. You then apply this object to the data by calling the transform method. The result has a new column with the discrete bucket values. The three buckets have been assigned indexed values zero, one, and two, corresponding to the low, medium, and high ranges for RPM. As you saw earlier, before you can use these index values in a regression model, they first need to be one-hot encoded. The low and medium RPM ranges are mapped to a distinct dummy variable, while the high range is the reference level and does not get a separate dummy variable.\n\nLet's look at the intercept and coefficients for a model which predicts fuel consumption based on bucketed RPM data. The intercept tells us what the fuel consumption is for the reference level, which is the high RPM bucket. To get the consumption for the low RPM bucket, you add the first coefficient to the intercept. Similarly, to find the consumption for the medium RPM bucket, you add the second coefficient to the intercept. There are many other approaches to engineering new features. It's common to apply arithmetic operations to one or more columns to create new features. Returning to the heights data, suppose that we also had data for mass. Then it might be perfectly reasonable to engineer a new column for BMI. Potentially BMI might be a more powerful predictor than either height or mass in isolation. Let's apply this idea to the car's data. You have columns for mass and length. Perhaps some combination of the two might be even more meaningful. You can create different forms of density by dividing the mass through by the first three powers of length. Since you only have the length of the vehicle but not their width or height, the length has been used as a proxy for these missing dimensions. In so doing, you create three new predictors. The first density represents how mass changes with vehicle length. The second and third densities approximate how mass varies with the area and volume of the vehicle. Which of these will be meaningful for our model? Right now, you don't know. You're just trying things out. Powerful new features are often discovered through trial and error.\n\nIn the next lesson, you'll learn about a technique for selecting only the relevant predictors in a regression model. Right now though, let's apply what you've learned to the flights data. The regression models that you've built up until now have blindly included all of the provided features. Next, you're going to learn about a more sophisticated model which\n\n\neffectively selects only the most useful features. A linear regression model attempts to derive a coefficient for each feature in the data. The coefficients quantify the effects of the corresponding features. More features imply more coefficients. This works well when your data set has a few columns and many rows. You need to derive a few coefficients and you have plenty of data. The converse situation, many columns and few rows, is much more challenging. Now you need to calculate values for numerous coefficients, but you don't have much data to do it. Even if you do manage to derive values for all of those coefficients, your model will end up being very complicated and difficult to interpret.\n\nIdeally, you want to create a parimonious model, one that has just the minimum required number of predictors. It will be as simple as possible yet still able to make robust predictions. The obvious solution is to simply select the best subset of columns. But how to choose that subset? There are a variety of approaches to this feature selection problem. In this lesson, we'll be exploring one such approach to feature selection known as penalized regression. The basic idea is that the model is penalized or punished for having too many coefficients. Recall that the conventional regression algorithm chooses coefficients to minimize the loss function, which is the average of the squared residuals.\n\nA good model will result in low mean squared error because its predictions will be close to the observed values. With penalized regression, an additional regularization or shrinkage term is added to the loss function rather than depending on the data. This term is a function of the model coefficients. There are two standard forms for the regularization term. Lasso regression uses a term which is proportional to the absolute value of the coefficients while ridge regression uses the square of the coefficients. In both cases, this extra term in the loss function penalizes models with too many coefficients. There's a subtle distinction between lesso and ridge regression. Both will shrink the coefficients of unimportant predictors. However, whereas ridge will result in those coefficients being close to zero, lasso will actually force them to zero precisely. It's also possible to have a mix of lasso and ridge. The strength of the regularization is determined by a parameter which is generally denoted by the Greek symbol lambda. When lambda is zero, there is no regularization. And when lambda is large, regularization completely dominates. Ideally, you want to choose a value for lambda between these two extremes. Let's make this more concrete by returning to the car's data. We've assembled the mass, cylinders, and type columns along with the freshly engineered density columns. We've effectively got 10 predictors available for the model. As usual, we'll split these data into training and testing sets. Let's start by fitting a standard linear regression model to the training data. You can then make predictions on the testing data and calculate the root mean squared error. When you look at the model coefficients, you find that all predictors have been assigned nonzero values. This means that every predictor is contributing to the model. This is certainly possible, but it's unlikely that all of the features are equally important for predicting consumption. Now, let's fit a ridge regression model to the same data. You get a ridge regression model by giving a value of zero for the elastic net param parameter. An arbitrary value of 0.1 has been chosen for the regularization strength. Later you'll learn a way to choose good values for this parameter based on the data. When you calculate the root mean squared error on the testing data you find that it has increased slightly but not enough to cause concern. Looking at the coefficients you see that they are all smaller than the coefficients for the standard linear regression model. They have been shrunk. Finally, let's build a lasso regression model by setting elastic net param to one. Again, you find that the testing root mean error has increased, but not by a significant degree. Turning to the coefficients though, you see that something important has happened. All but two of the coefficients are now zero. There are effectively only two predictors left in the model: the dummy variable for a small type car and the linear density. Lesso regression has identified the most important predictors and set the coefficients for the rest to zero. This tells us that we can get a good model by simply knowing whether or not a car is small and its linear density. A simpler model with no significant loss in performance.\n\nLet's try out regularization on our flight duration model. Welcome back. So far, you've learned how to build classifier and regression models using Spark. In this chapter, you'll learn how to make those models better. You'll start by taking a look at pipelines, which will seriously streamline your workflow. They will also help to ensure that training and testing data are treated consistently and that no leakage of information between these two sets takes place. What do I mean by leakage? Most of the actions you've been using involve both a fit and a transform method. Those methods have been applied in a fairly relaxed way. But to get really robust results, you need to be careful only to apply the fit method to training data. Why? Because if a fit method is applied to any of the testing data, then the model will effectively have seen those data during the training phase. So the results of testing will no longer be objective. The transform method on the other hand can be applied to both training and testing data since it does not result in any changes in the underlying model. A figure should make this clearer. Leakage occurs whenever a fit method is applied to testing data. Suppose that you fit a model using both the training and testing data. The model will then already have seen the testing data. So using those data to test the model would not be fair. Of course, the model will perform well on data which has been used for training. This sounds obvious, but care must be taken not to fall into this trap. Remember that there are normally multiple stages in building a model and if the fit method in any of those stages is applied to the testing data, then the model is compromised. However, if you are careful to only apply fit to the training data, then your model will be in good shape. When it comes to testing, it will not have seen any of the testing data and the test results will be completely objective. Luckily, a pipeline will make it easier to avoid leakage because it simplifies the training and testing process.\n\nA pipeline is a mechanism to combine a series of steps. Rather than applying each of the steps individually, they are all grouped together and applied as a single unit. Let's return to our car's regression model. Recall that there were a number of steps involved: using a string indexer to convert the type column to indexed values, applying a one-hot encoder to convert those indexed values into dummy variables, then assembling a set of predictors into a single features column, and finally building a regression model. Let's map out the process of applying those steps. First, you fit the indexer to the training data. Then you call the transform method on the training data to add the indexed column. Then you call the transform method on the testing data to add the indexed column there too. Note that the testing data was not used to fit the indexer. Next, you do the same things for the one-hot encoder, fitting the training data and then using the fitted encoder to update the training and testing data sets. The assembler is next. In this case, there is no fit method. So you simply apply the transform method to the training and testing data. Finally, the data are ready. You fit the regression model to the training data and then use the model to make predictions on the testing data. Throughout the process, you've been careful to keep the testing data out of the training process. But this is hard work and it's easy enough to slip up. A pipeline makes training and testing a complicated model a lot easier. The pipeline class lives in the ML subm module. You create a pipeline by specifying a sequence of stages where each stage corresponds to a step in the model building process. The stages are executed in order. Now rather than calling the fit and transform methods for each stage, you simply call the fit method for the pipeline on the training data. Each of the stages in the pipeline is then automatically applied to the training data in turn. This will systematically apply the fit and transform methods for each stage in the pipeline. The train pipeline can then be used to make predictions on the testing data by calling its transform method. The pipeline transform method will only call the transform method for each of the stages in the pipeline. Isn't that simple? You can access the stages in the pipeline by using the stages attribute which is a list. You pick out individual stages by indexing into the list. For example, to access the regression component of the pipeline, you'd use an index of three. Having access to that component makes it possible to get the intercept and coefficients for the trained linear regression model. Pipelines make your code easier to read and maintain. Let's try them out with our flights model.\n\nUp until now, you've been testing models using a rather simple technique: randomly splitting the data into training and testing sets, training the model on the training data, and then evaluating its performance on the testing set. There's one major drawback to this approach. You only get one estimate of model performance. You would have a more robust idea of how well a model works if you were able to test it multiple times. This is precisely the idea behind cross validation. You start out with the full set of data. You still split these data into a training set and a testing set. Remember that before splitting, it's important to first randomize the data. So the distributions in the training and testing data are similar. You then split the training data into a number of partitions or folds. The number of folds normally factors into the name of the technique. For example, if you split into five folds, then you talk about five-fold cross validation. Once the training data have been split into folds, you can start cross validating. First, keep aside the data in the first fold. Train a model on the remaining four folds. Then evaluate that model on the data from the first fold. This will give you the first value for the evaluation metric. Next, you move on to the second fold where the same process is repeated. Data in the second fold are set aside for testing while the remaining four folds are used to train a model. That model is tested on the second fold data yielding the second value for the evaluation metric. You repeat the process for the remaining folds. Each of the folds is used in turn as testing data and you end up with as many values for the evaluation metric as there are folds. At this point, you're in a position to calculate the average of the evaluation metric over all the folds, which is a much more robust measure of model performance than a single value. Let's see how this works in practice. Remember the car's data? Of course, you do. You're going to build a cross validated regression model to predict consumption. Here are the first two ingredients which you need to perform cross validation: an estimator, which builds the model and is often a pipeline, and an evaluator, which quantifies how well a model works on testing data. We've seen both of these a few times already. Now, the final ingredients, you'll need two new classes, cross validator and param grid builder, both from the tuning subm module. You'll create a parameter grid which you'll leave empty for the moment but we'll return to in detail during the next lesson. Finally, you have everything required to create a cross validator object, an estimator which is the linear regression model, an empty grid of parameters for the estimator and an evaluator which will calculate the root mean squared error. You can optionally specify the number of folds which defaults to three and a random number seed for repeatability. The cross validator has a fit method which will apply the cross validation procedure to the training data. You can then look at the average root mean squared error calculated across all of the folds. This is a more robust measure of model performance because it is based on multiple trained test splits. Note that the average metric is returned as a list. You'll see why in the next lesson. The trained cross validator object acts just like any other model. It has a transform method which can be used to make predictions on new data. If we evaluate the predictions on the original testing data, then we find a smaller value for the root mean squared error than we obtained using cross validation. This means that a simple train test split would have given an overly optimistic view on model performance. Let's give cross validation a try on our flights model.\n\nSo far, you've been using the default parameters for almost everything. You've built some decent models, but they could probably be improved by choosing better model parameters. There is no universal best set of parameters for a particular model. The optimal choice of parameters will depend on the data and the modeling goal. The idea is relatively simple. You build a selection of models, one for each set of model parameters. Then you evaluate those models and choose the best one. You'll be looking at the fuel consumption regression model. Again, you'll start by doing something simple, comparing a linear regression model with an intercept to one that passes through the origin. By default, a linear regression model will always fit an intercept, but you're going to be explicit and specify the fit intercept parameter as true. You fit the model to the training data and then calculate the root mean squared error for the testing data. Next, you repeat the process but specify false for the fit intercept parameter. Now, you are creating a model which passes through the origin. When you evaluate this model, you find that the root mean squared error is higher. So comparing these two models, you'd naturally choose the first one because it has a lower root mean squared error. However, there's a problem with this approach. Just getting a single estimate of root mean squared error is not very robust. It would be better to make this comparison using cross validation. You also have to manually build the models for the two different parameter values. It would be great if that were automated. You can systematically evaluate a model across a grid of parameter values using a technique known as grid search. To do this, you need to set up a parameter grid. You actually saw this in the previous lesson where you simply created an empty grid. Now you're going to add points to the grid. First you create a grid builder and then you add one or more grids. At present there's just one grid which takes two values for the fit intercept parameter. Call the build method to construct the grid. A separate model will be built for each point in the grid. You can check how many models this corresponds to. And of course this is just two. Now you create a cross validator object and fit it to the training data. This builds a bunch of models, one model for\n\n\nEach fold and point in the parameter grid. Since there are two points in the grid and 10 folds, this translates into 20 models. The cross validator is going to loop through each of the points in the parameter grid, and for each point it will create a cross validated model using the corresponding parameter values. When you take a look at the average metrics attribute, you can see why the metric is given as a list. You get one average value for each point in the grid. The values confirm what you observe before. The model that includes an intercept is superior to the model without an intercept. Our goal was to get the best model for the data. You retrieve this using the appropriately named best model attribute. But it's not actually necessary to work with this directly because the cross validator object will behave like the best model. So you can use it directly to make predictions on the testing data. Of course, you want to know what the best parameter value is, and you can retrieve this using the explain param method. As expected, the best value for the fit parameter is true. You can see this after the word current in the output. It's possible to add more parameters to the grid. Here, in addition to whether or not to include an intercept, you're also considering a selection of values for the regularization parameter and the elastic net parameter. Of course, the more parameters and values you add to the grid, the more models you have to evaluate. Because each of these models will be evaluated using cross validation. This might take a little while, but it will be time well spent because the model that you get back will in principle be much better than what you would have obtained by just using the default parameters. Let's apply grid search on the flights and SMS models.\nYou now know how to choose a good set of parameters for any model using cross validation and grid search. In the final lesson, you're going to learn about how models can be combined to form a collection or ensom. Simply put, an ensom model is just a collection of models. An ensom model combines the results from multiple models to produce better predictions than any one of those models acting alone. The concept is based on the idea of the wisdom of the crowd which implies that the aggregated opinion of a group is better than the opinions of the individuals in that group even if the individuals are experts. As the quote suggests for this idea to be true there must be diversity and independence in the crowd. This applies to models too. A successful ensemble requires diverse models. It does not help if all of the models in the ensom are similar or exactly the same. Ideally, each of the models in the ensom should be different. A random forest, as the name implies, is a collection of trees. To ensure that each of those trees is different, the decision tree algorithm is modified slightly. Each tree is trained on a different random subset of the data and within each tree a random subset of features is used for splitting at each node. The result is a collection of trees where no two trees are the same within the random forest model. All of the trees operate in parallel. Let's go back to the car classifier yet again.\nYou create a random forest model using the random forest classifier class from the classification submodule. You can select the number of trees in the forest using the num trees parameter. By default, this is 20, but we'll drop that down to five so the results are easier to interpret. As is the case with any other model, the random forest is fit to the training data.\nOnce the model is trained, it's possible to access the individual trees in the forest using the trees attribute. You would not normally do this, but it's useful for illustrative purposes. There are precisely five trees in the forest as specified. The trees are all different as can be seen from the varying number of nodes in each tree.\nYou can then make predictions using each tree individually. Here are the predictions of individual trees on a subset of the testing data. Each row represents predictions from each of the five trees for a specific record. In some cases, all the trees agree, but there is often some dissent amongst the models. This is precisely where the random forest works best, where the prediction is not clearcut. The random forest model creates a consensus prediction by aggregating the predictions across all of the individual trees. You don't need to worry about these details though because the transform method will automatically generate a consensus prediction column. It also creates a probability column which assigns aggregate probabilities to each of the outcomes. It's possible to get an idea of the relative importance of the features in the model by looking at the feature importances attribute. An importance is assigned to each feature where a larger importance indicates a feature which makes a larger contribution to the model. Looking carefully at the importances, we see that feature 4, RPM, is the most important, while feature zero, the number of cylinders, is the least important. The second ensemble you'll be looking at is gradient boosted trees. Again, the aim is to build a collection of diverse models, but the approach is slightly different. Rather than building a set of trees that operate in parallel, now we build trees which work in series. The boosting algorithm works iteratively. First build a decision tree and add to the ensom. Then use the ensom to make predictions on the training data. Compare the predicted labels to the known labels. Now identify training instances where the predictions were incorrect. Return to the start and train another tree which focuses on improving the incorrect predictions. As trees are added to the onsaw, its predictions improve because each new tree focuses on correcting the shortcomings of the preceding trees. The class for the gradient boosted tree classifier is also found in the classification submodule. After creating an instance of the class, you fit it to the training data. You can make an objective comparison between a plain decision tree and the two ONSM methods by looking at the values of AU obtained by each of them on the testing data. Both of the ONSAM methods score better than the decision tree. This is not too surprising since they are significantly more powerful models. It's also worth noting that these results are based on the default parameters for these models. It should be possible to get even better performance by tuning those parameters using cross validation. In the final set of exercises, you'll try out ensemble methods on the flights data. Congratulations on completing this course on machine learning with Apache Spark. You have covered a lot of ground reviewing some machine learning fundamentals and seeing how they can be applied to large data sets using Spark for distributed computing. You learned how to load data into Spark and then perform a variety of operations on those data. Specifically, you learned basic column manipulation on data frames, how to deal with text data, bucketing continuous data, and one hot encoding categorical data. You then delved into two types of classifiers, decision trees and logistic regression. In the process, building a robust spam classifier.\nYou also learned about partitioning your data and how to use testing data and a selection of metrics to evaluate a model. Next, you learned about regression, starting with a simple linear regression model and progressing to penalized regression, which allowed you to build a model using only the most relevant predictors. You learned about pipelines and how they can make your Spark code cleaner and easier to maintain. This led naturally into using cross validation and grid search to derive more robust model metrics and use them to select good model parameters. Finally, you encountered two forms of models. Of course, there are many topics that were not covered in this course. If you want to dig deeper, then consult the excellent and extensive online documentation. Importantly, you can find instructions for setting up and securing a Spark cluster. Now, go and use what you've learned to solve challenging and interesting big data problems in the real world. Hi, welcome to this course on building recommendation engines using alternating lease squares or ALS in Pispark.\nYou're probably already familiar with the output of these types of recommendation engines where a website tells you something along the lines of, \"If you like that, then you'll probably like this.\" You've likely seen these types of recommendations on your favorite retail or media streaming websites. These recommendations are generated through different types of data that you, as a user, provide either directly or indirectly.\nWhen you purchase something online or watch a movie or even read an article, you're often given a chance to rate that item on a scale of 1 to five stars, a thumbs up or thumbs down, or some other type of rating scale. Based on your feedback from these types of rating systems, companies can learn a lot about your preferences and offer you recommendations based on preferences of users that are similar to you.\nFor example, if your movie streaming service sees that you like The Dark Knight and Iron Man and did not like Tangled and it also sees other users that also like The Dark Knight and Iron Man and also did not like Tangled, the ALS algorithm would see that you and these other users have similar tastes. It would then look at the movies that you have not yet seen and see which ones are the highest rated among those similar users and offer them as recommendations to you. This is why websites will often say things like because you like that movie, we think you'll like this movie or users like you also watched this movie. These types of rating systems are extremely powerful.\nIn fact, an article published by Mckenzian Company in October of 2013 stated that 35% of what customers buy on Amazon and 75% of what they watch on Netflix come from product recommendations based on algorithms such as the one you are going to be learning in this course. That's a powerful use of data. And with this course, you will learn how to do this. In addition to this, there are alternate uses of recommendation algorithms that can be extremely useful for purposes as broad as feature space reduction, image compression, mathematical user and product grouping, latent feature discovery, and you're going to learn some of these in this course. This tutorial is intended for those that have experience with Spark and Python and understand the fundamentals of machine learning. If needed, some good introductory resources are DataCamp's introduction to PySpark course, their intermediate Python for data science course, and their supervised machine learning with Python's scikitlearn course. Let's jump in. In the world of recommendation engines, there are two basic types. Collaborative filtering engines and contentbased filtering engines. Both aim to offer meaningful recommendations, but they do so in slightly different ways. Contentbased filtering, as the name suggests, tries to understand the content or features of the items and makes recommendations based on your preferences for those specific features. For example, a movie streaming service might go to great lengths to add descriptive tags to their movies, such as the genre, whether it's animated or not, the language spoken in the movie, the decade it was filmed, and which actors were in it. So when a user like you gives five stars to a really dramatic Portuguese movie with specific actors from a specific decade, they can infer that you like movies like this and will also like other dramatic movies in Portuguese with those same actors and recommend those movies to you.\nCollaborative filtering is a little bit different. As explained in the previous video, collaborative filtering is based on user similarity.\nHowever, unlike contentbased filtering, manually created tags are not necessary. The features and groupings are created mathematically from patterns in the ratings provided by users. When you provide ratings for a product or item, whether it be a thumbs up or thumbs down, or even if you just watch a video without even giving it a rating, you are providing meaningful insight about your preferences. From this behavior, the ALS algorithm can mathematically group you with similar users, predict your behavior, and help you have a more effective customer experience. While ALS can have contentbased applications, this course will focus on its application to collaborative filtering, but many of the principles of collaborative filtering can be implied to contentbased applications. Now, let's talk about ratings. In the realm of recommendation engines, there are two main types of ratings. Explicit ratings and implicit ratings. Explicit ratings are pretty straightforward. Examples of these are when you input a number of stars or something like a thumbs up or thumbs down. These are explicit ratings because users explicitly state how much they like or dislike something. Implicit ratings are a little bit different. They are based on the passive tracking of your behavior like the number of movies you've seen in different genres. Fundamentally, implicit ratings are generated from the frequency of your actions. For example, if you watch 30 movies and of those 30 movies, 22 are action movies and only one is a comedy, the low number of comedy views will be converted into low confidence that you like comedies, and the high number of action movies will be converted into a high confidence that you like action movies. These confidence scores are then used as ratings. The logic behind this is in essence the more you carry out a behavior the higher the likelihood that you like it and thus a higher rating. Additionally, in some cases you may not have access to user behavior counts like this. A simpler form of ratings that still works with the ALS algorithm is the use of simple binary ratings. Rather than having a count of user actions, binary ratings just show whether a user has done something like watched a comedy represented by a one or not watched a comedy represented by a zero. These types of ratings aren't nearly as rich, but they still can provide meaningful insight and still work perfectly fine with the ALS algorithm. Now, let's look at some actual data.\nSo far, we've only considered recommendations as a use case for the ALS algorithm, but there are other applications that are also useful. These include latent feature discovery, item grouping, dimensionality reduction, and image compression. In this course, we'll only talk about some of these. First, let's talk about latent features. As mentioned earlier, people will go to great lengths to effectively categorize items. But some products span various categories making them difficult to organize. Movies are often like this.\nFor example, horror movies can be comedies. Dramas can be satires. Documentaries can be romances or even mysteries. Because of this, they can sometimes be difficult to market. If we had a better understanding of how consumers categorize movies based on their experience watching them, we could add more power to marketing strategies. ALS can help with this. When we have a matrix that contains users and movie ratings, ALS will factor that matrix into two matrices. One containing user information and the other containing product information or in this case movie information. Each matrix takes the respective labeled axis from the original matrix and is given another axis that is unlabeled. The unlabeled axes contain what's called latent features. The number of latent features is referred to as the rank of these matrices. In this case, the rank chosen is three. You, as a\n\n\nData scientist, get to choose how many of these ALS will create. These latent features represent groups that are created from patterns in the original ratings matrix. And the values in these columns represent how much each item falls into these groups. For example, in the original ratings matrix, there might be a lot of people who like horror movies and don't like dramas. They would rate horror movies high and dramas low. Likewise, other people might like dramas and not like horror movies and would rate dramas high and horror films low. ALS can see this and determine that these are different types of movies. And if we were to look at the movie factor matrix, we would likely see that in one of the latent feature rows, the dramas would score high and the horror movies would score low, while in another latent feature column, we might see the opposite. Knowing a little about these movies, we could determine that those latent features reflect those two genres. This allows us to mathematically see how users experience these movies and to what degree users feel each movie falls into each respective category. This concept goes a bit deeper though. For example, if we look at a movie matrix, we might see in one latent feature column that several movies have scored very high, but they don't seem to have anything in common. If they're all popular movies, we might want to research what's going on here to see if there's a business opportunity. Digging deeper, we find that these movies are all adaptations of Shakespeare plays and that there seems to be a strong customer group that likes these types of movies. Now that we know this, we can use this information to inform how we choose what movies to make and hopefully give our customers more of what they want. It's worth reiterating that in the original data set, there was no column anywhere called Shakespeare adaptations. It's also worth noting that many or all of these customers may not even know that this is something that draws them to these movies. This is the type of powerful information that ALS can help us uncover. Now, let's try to actually use this. As you've probably realized, matrix operations are fundamental to the ALS algorithm. We're going to review matrix multiplication and matrix factorization. Let's start with matrix multiplication. Here we have two square matrices. In order to multiply them together, we make specific pairs of the values from the two matrices and add the products of those pairs. We start at the top left corner of each matrix and create pairs moving to the right on the first matrix and moving down on the second matrix one at a time. Each pair is multiplied and the products from all pairs are added together. The final sum will make up one number of the resulting matrix. That's a lot to digest. So, let's walk through an example. Starting at the top left number of each matrix, we have a pair of numbers 1 and 9. We will multiply those numbers together. Then, moving to the right on the first matrix and down on the second matrix, we have 2 and six. Then, moving right again on the first matrix and down again on the second matrix, we have three and three. We have completed the first set of pairs. So, let's add their products together. 1 * 9 + 2 * 6 + 3 * 3 is 9 + 12 + 9, which gives us 30. 30 is the first number in our final matrix. From here, we stay on the first row of the first matrix, but move on to the second column of the second matrix. These pairs give us 1 and 8, 2 and 5, and 3 and 2. 1 * 8 + 2 * 5 + 3 * 2 is equal to 8 + 10 + 6 which is 24. Moving to the next set of pairs, we multiply 1 and 7, 2 and 4, and 3 and 1. Their products are 7, 8, and 3, which makes 18. Once we've multiplied the first row of the first matrix by all columns of the second matrix, we then go through the same process for the second row of the first matrix with all columns of the second matrix and so on until all the rows of the first matrix have been multiplied by all columns of the second matrix. In this example, we multiplied two square matrices of the same dimensions. In reality, you can multiply any two matrices as long as the number of columns of the first matrix matches the number of rows of the second matrix. If they don't, then some values in one of the matrices won't be paired and multiplication can't be completed. Let's look at some examples and practice matrix multiplication. Matrix factorization or matrix decomposition is essentially the opposite of matrix multiplication. Rather than multiplying two matrices together to get one new matrix, matrix factorization splits a matrix into two or more matrices which when multiplied back together produce an approximation of the original matrix. There are several different mathematical approaches for this each of which has a different application. We aren't going to go into any of that here. We are simply going to review the factorization that ALS performs. Used in the context of collaborative filtering, ALS uses a factorization called non-gative matrix factorization. Because matrix factorization generally returns only approximations of the original matrix. In some cases, they can return negative values in the factor matrices even when attempting to predict positive values. When predicting what rating a user will give to an item, negative values don't really make sense. Neither do they make sense in the context of latent features. For this reason, the version of ALS that we will be using will require that the factorization return only positive values. Let's look at some sample factorizations. Here's a sample matrix of possible item ratings. There are five rows and five columns. And here's one factorization of that matrix called the LU factorization. Notice that the factor matrices are the same dimensions or rank as the original matrix. Also notice that some of the values in this factorization are negative. Using this type of factorization could result in negative predictions that wouldn't make sense in our context. Here's another factorization. In this case, all the values are positive, meaning that the resulting product of these factor matrices is guaranteed to be positive. This is closer to what we need for our purposes. Notice here that the dimensions of the factor matrices are such that the first factor matrix has the same number of rows as the original matrix, but a different number of columns. Also, the second factor matrix has the same number of columns as the original matrix, but a different number of rows. The dimensions of the factor matrices that don't match the original matrix are called the rank or the number of latent features. In this case, we have chosen the rank of the factor matrices to be three. What that means is that the number of latent features of the factor matrices is three. Remember that as a data scientist when doing these types of factorizations, you get to choose the rank or the number of latent features the factor matrices will have. Now look at this matrix. Not all the cells have numbers in them. Despite this, we can still factor the values in the matrix. Also notice that because there is at least one value in every row and at least one value in every column that each of the factor matrices are totally full. Because of this, factoring a sparse matrix into two factor matrices gives us the means to not only approximate the original values that existed in the matrix to begin with, but also provide predictions for the cells that were originally blank. And because the factorization is based on the values that existed previously, the blank cells are filled in based on those already existing patterns. So when we do this with user ratings, the blanks are filled in with values that reflect the individual user behavior and the behavior of users similar to them. This is why this method is called collaborative filtering. Let's look at some real life examples. Now that we've covered matrix multiplication and matrix factorization, we're ready to begin exploring how ALS uses non-gative matrix factorization to predict how users will rate movies they haven't yet seen. Let's start at the beginning. Here is a portion of a matrix of users and their movie ratings. In total, there are 671 users and 9,066 movies. Of the 6.1 million possible ratings we could have in this matrix with this many users and movies, we only have about 100,000. That means that 98% of the matrix is totally blank. This makes sense because 9,066 movies are far too many movies for any normal person to watch in their lifetime. One of the benefits of ALS is that it works well with sparse matrices like this. Now, the first thing ALS does with a matrix like this is factor it into two different matrices as you see here. Remember that factorizations like this produce two matrices which when multiplied back together produce an approximation of the original matrix. In order to get the closest approximation of the original matrix R, ALS first fills in the factor matrices with random numbers and then makes slight adjustments to the matrices one at a time until it has the best approximation possible. In other words, ALS holds the matrix R and the matrix U constant and makes adjustments to the matrix P. It then multiplies the two factor matrices together to see how far the predictions are from the original matrix using the root mean squared error or RMSSE as an error metric. The RMSSE basically tells you on average how far off your predictions are from the actual values. We'll talk more about this later in the course, but note that in calculating the RMSSE, only the values that existed in the original matrix are considered. The missing values are not considered. ALS then holds P and R constant and adjusts values in the matrix U. The RMSSE is calculated again and ALS again switches and calculates the RMSSE again. ALS will continue to iterate until instructed to stop at which point ALS has the best possible approximation of the original matrix R. The beauty of all this is that when the RMSSE is fully minimized, ALS simply multiplies the matrices back together and the blank cells are filled in with predictions. In other words, when we take a sparse matrix and factor it into two matrices, every rating in the original matrix must have a respective row and column full of values in the respective factor matrices that can be multiplied back together to approximate that original value. And since there is at least one rating in every row and at least one rating in every column of the original matrix, when ALS creates the two respective factor matrices, there are values in every cell of the two factor matrices, which allows us to then create predictions for the previously blank spaces. So when ALS iterates to make sure that its resulting product is as close to those original cells as possible, the result is that the previously blank cells are now filled in with values that are based on how each user has behaved in the past relative to the behavior of similar users. Let's jump into some examples and see how this is done in real life. Let's talk about data preparation. Data preparation will consist of two things. Correct data frame format and correct schema. First, data frame format. Most data frames you've seen probably look like this with user ids in one column, all the features in the remaining columns and the values of those features making up the contents of those columns. However, many pipark algorithms ALS included require your data to be in row-based format like this. The data is the same. The first column contains user ids. But rather than a different feature in each column, column 2 contains feature names and column 3 contains the value of that feature for that user. So a user's data can be spread across several rows and rows contain no null values. Depending on your data, you may need to convert it to this format. Now let's talk about creating the right schema. As you see, our user ID column and our generically named column of movie titles are strings. PiSpark's implementation of ALS can only consume user ids and movie IDs as integers. So again, you might need to convert your data to integers. Let's walk through an example of how to do all of this. Here's a conventional data frame. To convert it to a long or dense matrix, we will need to use a user defined function called wide to long. We won't go into the detail of how it works here, but it turns the conventional data frame into a row-based data frame like this. If you like access to this function directly, a link will be provided at the end of the course. So, we have the right data frame format. Let's get the right schema. In order to have integer, user and movie IDs, we need to assign unique integers to the user ids and to the movie IDs. To do this, we will follow three steps. Extract unique user IDs and movie IDs. Assign each one an integer ID and rejoin these unique integer IDs back to the ratings data. Let's start with user IDs. Let's first run this query to get all the distinct user ids into one data frame and call it users. Then we'll import a method called monotonically increasing ID which will assign a unique integer to each row of our users data frame. We need to be careful when using this because it will treat each partition of data independently meaning the same integer could be used in different partitions. In order to get around this, we'll convert our data into one partition using the coales method. Also note that while the integers will be increasing by a value of one over each row, they may not necessarily start at one. That's not super important here. What's really important is that they are unique. So now we can create a new column in our users data frame called user int ID. Set it to monotonically increasing ID. And we will have our new user integer IDs. Note that the monotonically increasing ID method can be a bit tricky as the values it provides can change as you do different things to your data set. For this reason, we've called the persist method to tell Spark to keep these values the same across all subsequent data frame operations. We'll do the same thing with the movie ids. And now we have two data frames, one with our user IDs and one with our movie ids. So let's join them together along with our original data frame on our user ID and variable columns using the join method specifying a left join. We can be even more thorough by creating a new data frame with only the columns ALS needs and renaming our columns using the do alias method which renames the column on which it is called like this. Now let's prepare some data. As with many other algorithms, ALS has arguments that we give it and hyperparameters which must be tuned in order to generate the best predictions. Here's what a built out ALS model looks like. Let's review each argument and hyperparameter. The user call, item call, and rating call are straightforward. They simply tell Spark which columns in your data frame contain the respective user ids, item ids and ratings. The first ALS hyperparameter is the rank. As you already know, ALS will take a matrix of ratings and will factor that matrix into two different\n\n\nmatrices.\nOne representing the users and the other representing the products or items or in our case movies.\nIn the process of doing this, latent features are uncovered.\nALS allows you to choose the number of latent features that are created, which is referred to as the rank hyperparameter, often represented by the letter K.\nYour objective with the data will determine the rank.\nIf you're trying to find meaningful groupings or categories of movies to see how similar or different movies are, you may want to experiment with different numbers of latent features.\nIf you have too few or too many latent features, the groupings might be difficult to understand.\nSo you want to look at different numbers of latent features and manually identify what makes the most sense.\nFor purposes of recommendations, however, the best number of latent features will be found through cross validation.\nThe number of iterations or max itter simply tells ALS how many times to iterate back and forth between the factor matrices adjusting the values to reduce the RMSSE.\nObviously, the higher number of iterations, the longer it will take to complete, and the fewer number of iterations, the higher the risk of not fully reducing the error.\nSo, you'll have to determine what works best for you.\nMany other machine learning algorithms have a regularization parameter often called a lambda.\nA lambda is simply a number that is added to an error metric to keep the algorithm from converging too quickly and overfitting to the training data.\nThe lambda for ALS and PySpark is referred to as the regg param.\nWe'll talk about alpha later in the course, but suffice it to say the alpha is only used when using implicit ratings and not used when using explicit ratings.\nLet's talk about the ALS arguments.\nAs mentioned previously, there are several different factorizations that can be used to factor a matrix.\nThe one that we are interested in is the non- negative matrix factorization.\nSo, we set the non-gative argument to true.\nYou might be familiar with the term cold start strategy already in the context of ALS when splitting data into test and train sets.\nIt's possible for a user to have all of their ratings inadvertently put into the test set, leaving nothing in the train set to be used for making a prediction.\nIn this case, ALS can't make meaningful predictions for that user or calculate an error metric.\nTo avoid this, we set the cold start strategy to drop, which tells Spark that when these situations arise to not use them to calculate the RMSSE and to only use users that have ratings in both the test and training sets.\nWe also need to tell Spark whether our ratings are implicit or explicit.\nWe do this by setting the implicit prefs argument to true or false.\nOnce we have a built-out model like you see here, we can fit it to training data and then generate test predictions to see how well it performs.\nWe can do this by calling the fit and transform methods as you see here.\nYou'll do this yourself in subsequent exercises.\nNow, it's your turn to build some models.\nUp until now, we've only been using sample data sets.\nNow, we're going to begin using actual data using the MovieLens data set.\nThis data set is made available by the good people at grouplands.org and contains roughly 20 million ratings for over 138,000 users and more than 27,000 movies.\nIn order to provide you with a better learning experience, we will achieve shorter run times by using a subset of the original data set, including 100,000 ratings.\nIn addition to the ratings data, grouplands.org or also provides additional data files that include information on movie genres and other types of tags that movie watchers have provided for them.\nWe'll take what you've learned from the previous chapters and explore the data.\nPrepare the data.\nBuild out a cross validated ALS model, generate predictions, and assess the model's performance.\nFirst, we'll view the data using the dot show and dotc columns methods, as well as some other methods to understand the nature of the data set.\nThen we'll calculate its sparsity using this sparity formula.\nAnd then we'll assess whether further preparation is needed in order to adequately prepare it for ALS.\nIf you're not familiar with the term sparity, it simply provides a measure of how empty a matrix is or what percentage of the matrix is empty.\nIn essence, this formula is simply the number of ratings that a matrix contains divided by the number of ratings it could contain given the number of users and movies in the matrix.\nThe code to calculate sparity is pretty straightforward.\nWe'll simply get the numerator by counting the number of ratings in the ratings data frame.\nThen we'll get the number of distinct users and the number of distinct items or movies.\nWe'll then multiply the number of users and the number of movies together to get the denominator and simply divide the numerator by the denominator and subtract the result from one.\nBecause division in Python will return an integer, we multiply the numerator by 1.0 to ensure a decimal or float is returned.\nLet's go over some other techniques that may or may not be new to you.\nAs you may already know, the distinct method simply returns all the unique values in a column.\nFor example, if you want to know how many unique users there are in a table, you could simply select the user column from the data frame, then run the distinct and count methods like you see here.\nThe group by method organizes data by the unique values of a specific column to return subtotals for those unique values.\nFor example, if you wanted to look at total number of ratings each user has provided, you would first need to group by user ID as you see here, then call the count method as you see here.\nWith this, you could then get the min or max or average of that same column.\nThe filter method allows you to filter out any data that doesn't meet your specified criteria.\nFor example, if you wanted to only consider users that have rated at least 20 movies, you would simply apply the same group by and count methods and then add a filter method specifying that the count column should only include values greater than 20.\nLet's apply what you've learned to a real data set.\nRemember from the last chapter, you built out a model on the ratings data set and the code looked like this.\nNow, the RMSSE that you got was lower than the 1.45 45 shown here.\nBut what if you went through this whole process and got an error metric that you weren't satisfied with like this RMSSE of 1.45?\nYou might want to try other combinations of hyperparameter values to try and reduce that.\nSpark makes it easy to do this by using two additional tools called the param grid builder and the cross validator.\nThese tools will allow you to try many different hyperparameter values and have Spark identify the best combination.\nLet's talk about how to use them.\nThe param grid builder tells Spark all the hyperparameter values you want it to try.\nTo do this, we first import the param grid builder package, instantiate a param grid builder, and give it a name.\nWe'll call it param grid.\nWe then add each hyperparameter name by calling the add grid method on our ls algorithm and hyperparameter name as you see here.\nNotice the empty list to the right of the hyperparameter names.\nThis is where we input the values we want Spark to try for each hyperparameter like this.\nOnce we've added all of this, we call the build method to complete the build on our param grid.\nNow, let's look at the cross validator.\nThe cross validator essentially fits a model to several different portions of our training data set called folds and then generates predictions for each respective hold out portion of the data set to see how it performs.\nTo properly use the cross validator, we first import the cross validator package, instantiate a cross validator, and give it a name.\nWe'll call it CV here.\nWe then tell it to use our ALS model as an estimator by setting the estimator argument equal to the name of our model, which is ALS.\nWe'll then set the estimator param maps to our param grid that we built so that Spark knows what values to try as it works to identify the best combination of hyperparameters.\nThen we provide the name of our evaluator so it knows how to measure each model's performance.\nWe do this by simply setting the evaluator argument to the name of our evaluator which is evaluator.\nWe finish by setting the numfolds argument to the number of times we want Spark to test each model on the training data.\nIn this case five times.\nLet's go over how to integrate these into a full code buildout.\nWe'll first split our data into training and test sets using the random split method.\nAnd we'll build a generic ALS model without any hyperparameters.\nOnly the model arguments as you see here.\nThe cross validator will take care of the hyperparameters.\nWe'll build our param grid builder so Spark knows what hyperparameter values to test.\nWe'll create an evaluator so Spark knows how to evaluate each model.\nThen the cross validator will tell Spark the algorithm, the hyperparameters and values, and the evaluator to use to find the best model and the number of training set folds we want each model to be tested on.\nWe then fit our cross validator on the training data to have Spark try all the combinations of hyperparameters we specified by calling the CV.fit method on the training data.\nOnce it's finished running, we extract the best performing model by calling the best model method on our model.\nWe'll call this our best model.\nAnd with it, we can generate predictions on the test set.\nPrint the error metric and the respective hyperparameter values using the code you see here.\nAnd now we have our cross validated model.\nLet's build a real model on a real data set.\nCongratulations.\nYou just built your first cross validated ALS model.\nNow let's determine whether the model suits your needs or not.\nThe primary way to do this is to examine the error metric.\nIn this case, the RMSSE.\nThe RMSSE or root mean squared error tells us on average how far a given prediction is from its corresponding actual value.\nIt's pretty straightforward.\nIf we have predictions and actual values, the RMSSE subtracts the actual value from the prediction, then squares those differences to make them positive.\nIt then sums those differences, takes the average by dividing by the number of observations, in this case, n= 4.\nAnd it then takes the square root to undo the squaring of the values that we did previously.\nSo if we have an RMSSE of 61, then on average our predictions are either 61 above or below the original rating.\nAnother way to evaluate our model is to look at its recommendations.\nRemember, however, that ALS is often used to identify patterns and uncover latent features that are unobservable by humans.\nMeaning that ALS can sometimes see things that may not initially make sense to us as humans.\nBear this in mind as you move forward.\nTo generate recommendations, we will use the native Spark function recommend for all users, which generates the top recommendations for all users.\nALS recommendation output has two challenges that need to be addressed.\nThe first is that it is in a format like this, which is perfectly usable in PySpark, but isn't very human readable.\nTo resolve this, we save the dataf frame as a temporary table and use this SQL query to make it readable.\nThe explode command essentially takes an array like our recommendation column and separates each item within it like this.\nNotice that only one movie ID and its respective recommendation value for each user is contained on each line where previously all recommendations for a given user were contained on one line.\nAlso notice that ALS conveniently includes the movie ID and rating column names with each value on each line.\nThis makes it easy to separate them into different columns.\nAdding the lateral view to the explode function allows us to treat the exploded column as a table and extract the individual values as separate columns.\nWe first name the lateral view.\nIn this case, we call it exploded table and then give it a formal table name which we will call movie ids and ratings.\nThis allows us to select the user ID and then get the movie ID and ratings by referencing the movie IDs and ratings table in the beginning of our query.\nThe output is now readable.\nAnd if we join it to the original movie information, we can see what's going on even better.\nThe other challenge with these recommendations is that they include predictions for movies that have already been seen.\nRemember how ALS creates two factor matrices that are multiplied together to produce an approximation of the original ratings matrix?\nThat's essentially what the ALS output is, including all movies for all users, whether they've seen them or not.\nA simple way to address this is to filter out the movies that have already been seen.\nSince we already have our clean recommendations and the original movie ratings, we can simply join these two data frames together on user ID and movie ID using a left join.\nThe movies that have already been seen are those that have a rating from the original movie ratings data frame.\nSo if we simply add a filter so that the rating column of the movie ratings data frame is null, we'll only have predictions for movies that the individual users have not seen.\nNow let's evaluate your model.\nBy now, you should be pretty comfortable with ALS.\nSo far, we've only used explicit ratings.\nIn most real life situations, however, explicit ratings aren't available, and you'll have to get creative in building these types of models.\nOne way to get around this is to use implicit ratings.\nRemember that while explicit ratings are explicitly provided by users in various forms, implicit ratings are data used to infer ratings.\nFor example, if a news website sees that in the last month you clicked on 21 geopolitical articles and only one local news article, ALS can convert these numbers into scores indicating how confident it is that you like them.\nThis approach assumes that the more you do something, the more you prefer it.\nALS can use these confidence ratings to generate recommendations and you're going to learn how to do this.\nFirst, let's talk about the data set you will be using.\nThe data set this time comes from the Million Songs data set available from Lab Rosa at Columbia University.\nYou're going to be using one file of this data set called the Echo Nest Taste Profile data set.\nIt contains information on over 1 million users, including the number of times they've played nearly 400,000 songs.\nThis is more data than we can use for this course, so we will only be using a portion of it.\nWe'll first examine the data, get summary statistics, and then build and evaluate our model.\nOne thing to note here is that because the use of implicit ratings causes ALS to calculate a level of confidence that a user likes a song based on the number of times they played it, a matrix will need to include zeros for the songs that each user has not yet listen to.\nIn case your data doesn't already include the zeros, we'll walk through how to do this.\nLet's say we have a ratings data frame like this.\nYou can use the\n\n\ndistinct method to extract the unique\nuser IDs and song IDs like this.\nYou can then perform a cross join which joins\neach user to each song like this.\nNotice that the three users and three songs we\noriginally had now create nine unique\npairs.\nUsing a left join, you can take\nthat cross join table and join it with\nthe original ratings to get the num\nplays column.\nNotice it joins on both\nuser ID and song ID.\nAnd because we want\nzeros in place of the null values so\nthat every user has a value for every\nsong, we simply call the fillna method\ntelling spark to fill the null values\nwith zero.\nAnd you have your final\nproduct to feed to\nALS.\nHere are those steps in a nice\nclean function.\nLet's do this with our\nmillion songs data\nset.\nNow that we have an implicit\nratings data set, let's discuss these\ntypes of models.\nThe first thing you\nshould know is that implicit ratings\nmodels have an additional hyperparameter\ncalled alpha.\nAlpha is an integer value\nthat tells Spark how much each\nadditional song play should add to the\nmodel's confidence that a user actually\nlikes a\nsong.\nLike the other hyperparameters,\nthis will need to be tuned through cross\nvalidation.\nThe challenge of these\nmodels is the evaluation.\nWith explicit ratings, we used the\nRMSSE.\nIt made sense in that situation\nbecause we could match predictions back\nto a true measure of user preference.\nIn\nthe case of implicit ratings, however,\nwe don't have a true measure of user\npreference.\nWe only have the number of\ntimes a user listened to a song and a\nmeasure of how confident our model is\nthat they like that song.\nThese aren't\nthe same thing, and calculating an RMSSE\nbetween them doesn't make sense.\nHowever, using a test set, we can see if\nour model is giving high predictions to\nthe songs that users have actually\nlistened to.\nThe logic here is that if\nour model is returning a high prediction\nfor a song that the respective user has\nactually listened to, then the\npredictions make sense, especially if\nthey've listened to it more than once.\nWe can measure this using this rank\norder error metric or ROEM.\nIn essence, this metric checks to see if\nsongs with higher numbers of plays have\nhigher\npredictions.\nFor example, here's a set\nof bad predictions.\nThe percent rank\ncolumn has ranked the predictions for\neach individual user such that the\nlowest prediction is in the highest\npercentile and the highest prediction is\nin the lowest\npercentile.\nNotice that these bad\npredictions include low predictions and\nhigh predictions for songs with more\nthan one play, indicating that the\npredictions may not be any better than\nrandom.\nIf we multiply the number of\nplays by the percent rank, we get this\nNP rank\ncolumn.\nWhen we sum that column, we get\nour ROEM numerator and the sum of the\nnum plays column gives us our ROE\ndenominator.\nUsing these, we can\ncalculate our ROEM to be\n0.556.\nValues close to .5 indicate that\nthey aren't much better than random.\nIf\nwe were to look at good predictions\nwhere the model gave high predictions to\nsongs that had more than one play, they\nmight look like\nthis.\nNotice that songs that have been\nplayed have high ratings indicating that\nthe predictions are better than random,\nwhich subsequently gives us an ROM of\n0.111.\nThis is much closer to zero,\nwhich is where we want to be.\nUnfortunately, Spark hasn't implemented\nan evaluator metric like this, so you'll\nneed to build it manually.\nAn ROEM\nfunction will be provided to you in\nsubsequent exercises.\nAnd for your\nreference, the code to build it is\nprovided at the end of this course.\nUsing this function and a for loop, you\ncan build several models as you see\nhere, each with different hyperparameter\nvalues.\nYou'll want to create a model\nfor each combination of hyperparameter\nvalues that you'll want to try.\nYou can then fit each one to the\ntraining data, extract each model's test\npredictions, and then calculate the ROE\nfor each one.\nThis is a simplified way\nto do this.\nFull cross validation is\nimperative to building good models.\nIt\nis beyond the scope of this course to\nteach how to code a function that\nmanually cross validates and evaluates\nmodels like this.\nBut doing so should be\ndone and code to do so is provided at\nthe end of this course.\nNow, let's put\nthis into practice.\nSo far, we've covered situations when\nyou have explicit ratings and when you\nhave implicit ratings from user behavior\ncounts.\nNow, we're going to cover the\nsituation when you might not even have\nbehavior counts.\nIn some situations, you\nmay only have binary data that tells you\nwhether a user has or has not taken an\naction with no indication of how many\ntimes they've done so.\nTo go back to the\nmovie example, if you know whether\ncustomers have watched certain movies,\nbut don't have information on how many\ntimes or how much they actually liked\nthem, you could simply feed binary data\nto ALS that indicates which customers\nhave watched each movie and which ones\nhaven't.\nALS can still pull signal from\nthis type of data and make meaningful\npredictions.\nWhen taking this approach,\nthe data will look like this.\nNotice\nthat all ratings are either a one or a\nzero.\nWe must treat binary ratings like\nthese as implicit ratings.\nIf we treated\nthem like explicit ratings and didn't\ninclude the zeros, the best performing\nmodel would simply predict one for\neverything and deliver a deceivingly\nideal RMSSE of\nzero.\nAlso, as with our previous million\nsongs model, we can't use the RMSSE as a\nmodel evaluation metric.\nUltimately,\nwhen our machine learning process holds\nout random observations in the test set,\nwe want our model to generate high\npredictions for those movies that users\nhave actually watched.\nFor this reason,\nwe'll use our ROEM metric.\nAgain, we'll apply the same concepts\nwe've covered previously on this binary\ndata set.\nThe convenience of using the\nmovie lens data set is that we can see\nhow our binary model performs against\nthe original true preference ratings of\nthe original movie lens data set.\nOne word about binary models.\nWhile it's\nperfectly feasible to feed binary data\nlike this into ALS and get meaningful\nrecommendations, the data does have sort\nof a class imbalance where the vast\nmajority of ratings are zeros with a\nsmall percentage of\nones.\nSince implicit ratings models use\ncustomized error metrics like ROEM and\nnot RMSSE, the class imbalance doesn't\nreally pose a problem like it might in\nclassification problems.\nALS can still generate meaningful\nrecommendations from this type of data,\nbut there are strategies that can be\ntaken with the data to try and improve\nrecommendations.\nFor example, rather\nthan treat unseen movies purely as\nzeros, you can weight them higher if\nmore people have seen them.\nThis assumes\nthat if many people have seen a movie,\nit must be a pretty good movie and\ntherefore should be treated with a\nlittle more weight and vice versa.\nThis\nis called item waiting.\nLikewise, you\ncould weight movies by individual user\nbehavior.\nFor example, if a user has\nseen lots of movies, you could weight\ntheir unseen movies lower, assuming that\nif a user has seen lots of movies, they\nknow what they like and have\ndeliberately chosen not to view the\nmovies they haven't seen and therefore\nthose movies deserve a lower waiting.\nWhile these methods are applicable,\ntheir methods haven't been implemented\ninto the PiSpark framework and therefore\nrequire a lot of manual work which is\nbeyond the scope of this course.\nHowever, if you'd like to learn more\nabout these types of approaches, you can\nread the paper referenced at the end of\nthis course.\nLet's build a binary\nratings\nmodel.\nCongratulations.\nYou've now\ncompleted this course on building\ncollaborative filtering recommendation\nengines in Pispark.\nWe've covered a number of things from\nwhy these are important to matrix\nmultiplication and factorization and\nlatent features.\nBut most importantly,\nyou've learned how to build and\ninterpret recommendation engines with\nthree different types of data.\nExplicit\nratings, implicit ratings using user\nbehavior counts, and implicit ratings\nusing binary user behavior.\nWith this\ninformation, you'll be well prepared to\nbuild a collaborative filtering\nrecommendation engine with the relevant\ndata available to you as a data\nscientist.\nSome things to bear in mind\nabout these types of models.\nIf users\ndon't have a lot of ratings and ALS\ncan't infer much about them, it's likely\nthat ALS will make broad general\nrecommendations that aren't really\npersonalized.\nYou might have seen this\nif you spent extra time exploring some\nof the recommendation output.\nLike all\nmodels, the more data there is, the\nbetter the model\nperforms.\nWhile we've gone over\ndifferent ways of evaluating\nrecommendation engines, the only way to\nreally know if your model performs well\nis to test it on users and see if they\nactually take your\nrecommendations.\nIt's entirely possible\nthat a simple binary implicit ratings\nmodel provides much better\nrecommendations for users than an\nexplicit model, but the only way to know\nis to test it.\nBear this in mind as you\nmove forward.\nHere are some resources to\nhelp you as you continue to learn about\nthese models and begin to build them on\nyour own.\nThe first is a paper published\nby McKenzie and company discussing the\npower of recommendation engines like\nALSbased models.\nThe second is the code\nto build the wide to long function\ndiscussed in the section about preparing\ndata for\nALS.\nThe third is the white paper that\nprovides the academic background for\nbuilding ALS models using implicit\nratings.\nI highly recommend reading this\npaper as it provides a lot of context\nand insight into how these models work\nand alternative ways to evaluate them.\nThe fourth is a GitHub link for code\nthat manages the cross validation and\nmodel evaluation for implicit ratings\nmodels using ALS and PISpark.\nThe last\nresource listed here is a paper that\ndiscusses the math and intuition behind\nthe user-based waiting and item based\nwaiting methodologies for addressing the\nclass imbalance present in binary\nratings models.\nCongratulations on\ncompleting this course and best of luck\nas you move forward.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.817Z"
}