{
  "episodeId": "VkqYmwIIeOg",
  "channelSlug": "@datacamp",
  "title": "Mistral Agents API Tutorial: Build Powerful AI Agents",
  "publishedAt": "2025-07-10T14:36:58.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Hi data scamps and data champs. This is",
      "offset": 0.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Richie. Let's get hands-on with Mistl's",
      "offset": 2.56,
      "duration": 5.999
    },
    {
      "lang": "en",
      "text": "AI agents API. You'll learn how to build",
      "offset": 5.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "powerful autonomous tool using AI",
      "offset": 8.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "agents. Our instructor Beex will cover",
      "offset": 11.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "getting started with the agents API",
      "offset": 13.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "through to helping you build your first",
      "offset": 15.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "AI agent. To jump straight to coding,",
      "offset": 17.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "skip ahead to this timestamp. To follow",
      "offset": 19.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "along in data lab, check out the link in",
      "offset": 21.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the description. Happy learning. If you",
      "offset": 24.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "follow AI, you've probably noticed a",
      "offset": 26.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "major trend happening in the AI sphere.",
      "offset": 28.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "All big tech companies are open-sourcing",
      "offset": 30.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "frameworks to build AI agents. For",
      "offset": 33.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "example, OpenAI has its agents SDK.",
      "offset": 36.079,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Google has a waste agent development kit",
      "offset": 39.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and Microsoft has had AutoAN for a while",
      "offset": 42,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "now. Recently, a very promising player",
      "offset": 44.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "joined this pack and it is Mistral AI.",
      "offset": 47.6,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Mistral's recently dropped agents API",
      "offset": 50,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "allows you to build powerful autonomous",
      "offset": 53.199,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "agents with access to tools like web",
      "offset": 55.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "search, code execution, and image",
      "offset": 58.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "generation. In this tutorial, we're",
      "offset": 60.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going to explore many of the features of",
      "offset": 62.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this agents API. And we're going to",
      "offset": 64.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "start simple by creating your first",
      "offset": 66.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "agent with built-in memory. And then",
      "offset": 68.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "we're going to move all the way up to",
      "offset": 71.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "using the built-in code execution tool",
      "offset": 73.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to generate interact to generate",
      "offset": 76.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "beautiful plots like the mandal broad",
      "offset": 78.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "set that you see below. This tutorial is",
      "offset": 80.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "designed for Python developers in mind",
      "offset": 83.36,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "who have basic Python knowledge and and",
      "offset": 86.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "have and are interested in building",
      "offset": 90.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "aentic applications. So without further",
      "offset": 92,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "ado, let's get started by setting up",
      "offset": 95.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "your environment. As you can see, I have",
      "offset": 98.479,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "this pip install command here, which is",
      "offset": 100.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "going to install the Mistral AI package",
      "offset": 102.64,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "and the Python.v package. Mistral AI is",
      "offset": 105.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "for communicating with the agents with",
      "offset": 109.119,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Mistrol's API server and Python. Env is",
      "offset": 111.28,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "a popular framework or package for",
      "offset": 115.119,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "managing environment variables variables",
      "offset": 118.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "through the help of files. After running",
      "offset": 120.799,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "this code snippet, uh after running this",
      "offset": 124.159,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "cell, you're going to create your",
      "offset": 126.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "account and sign in at",
      "offset": 129.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "console.mstrolai.home",
      "offset": 130.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and get your API key and save it as a an",
      "offset": 133.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "environment variable called mistrol API",
      "offset": 136.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "key in av file. If you don't know how to",
      "offset": 139.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "create the env file, I've pasted the",
      "offset": 141.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "command here for you uh which is",
      "offset": 144.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "touch.env.",
      "offset": 146.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Thisv file is the industry standard for",
      "offset": 148.4,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "uh hiding or storing sensitive",
      "offset": 152.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "information like API keys or uh",
      "offset": 155.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "sensitive information. And what you're",
      "offset": 157.92,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "going to do is uh add this file to your",
      "offset": 160.72,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "ignore file so that you won't",
      "offset": 164.879,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "accidentally commit your secrets to",
      "offset": 166.879,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "GitHub. Once you go through these steps,",
      "offset": 169.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you're ready to create your first agent",
      "offset": 171.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "which is which you can do using the",
      "offset": 173.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "first cell that you see here. First,",
      "offset": 175.599,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you're going to import the OS library",
      "offset": 178.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "and the TNV function from the Python.nv",
      "offset": 180.239,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "package that we've just installed. This",
      "offset": 184.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "function is going to read your TNV file",
      "offset": 185.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "here and load all the environment",
      "offset": 188.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "variables variables stored there and",
      "offset": 191.44,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "expose it to the OS package. Then you",
      "offset": 193.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "are going to import the Mistral class",
      "offset": 196.879,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "here from Mistral AI which is used to",
      "offset": 199.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "create a client that will communicate",
      "offset": 202.879,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "with the Mistral's API using your",
      "offset": 205.28,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "credentials which you can extract using",
      "offset": 208.4,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "the os.get env function. Here's the line",
      "offset": 210.959,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "that does this operation. And then we're",
      "offset": 214.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "going to create our very first agent",
      "offset": 217.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "using the client.ba.agents.create",
      "offset": 219.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "function. This function is going to",
      "offset": 222.56,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "accept four parameters. Uh the first one",
      "offset": 224.239,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "is the model name which is going to be",
      "offset": 226.879,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "MR medium. This model is going to bring",
      "offset": 229.68,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "a balance between performance and cost.",
      "offset": 232.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And then we're going to give a unique",
      "offset": 234.879,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "name to the agent. This name doesn't",
      "offset": 236.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "necessarily have to be unique. But for",
      "offset": 239.439,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "organization purposes and clarity, I",
      "offset": 242.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "recommend that you stick to unique names",
      "offset": 245.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "for your agents uh so that uh you can",
      "offset": 247.04,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "differentiate them later on. And then",
      "offset": 250.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you can give a custom description for",
      "offset": 252.959,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "your agent. Uh this is for readability",
      "offset": 254.799,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "purposes and going to be is is going to",
      "offset": 257.199,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "be important uh when creating multi-",
      "offset": 260.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "aent systems which we won't cover here",
      "offset": 262.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "as it is beyond the scope of the",
      "offset": 264.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "tutorial. And then you're going to have",
      "offset": 266.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "your instructions which is basically a",
      "offset": 268.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "system prompt that tells the agent how",
      "offset": 270.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to behave. Here we're keeping it simple",
      "offset": 272.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "by telling the agent that it is an ML",
      "offset": 274.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "expert and it should give practical and",
      "offset": 277.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "actionable advice. Let's run this cell",
      "offset": 280.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and wait for it to finish. And as you",
      "offset": 282.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "can see, we have a green check mark",
      "offset": 284.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "here. Uh, and let's print the agent ID.",
      "offset": 286.56,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "In Miles's API, all agents have unique",
      "offset": 291.12,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "IDs. Uh, so so that you can keep track",
      "offset": 294.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of how many agents or what kind of",
      "offset": 297.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "agents you have available under a single",
      "offset": 299.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "API key. If you notice here, I'm using a",
      "offset": 301.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "data lab environment, which is an",
      "offset": 304.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "environment provided by Data Camp. uh",
      "offset": 305.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but you can uh run all of these code",
      "offset": 308.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "examples in your own Jupyter notebook uh",
      "offset": 310.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "environment or in VS code which is going",
      "offset": 313.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to which is going to offer the same",
      "offset": 316.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "coding experience and then we're going",
      "offset": 318.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to move on to starting an conversation",
      "offset": 320.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "with the agent that we've just created.",
      "offset": 323.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Uh and this is going to be very easy by",
      "offset": 326,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "using the client.ba conversations",
      "offset": 328.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "start function.",
      "offset": 332.479,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "This function is going to require the",
      "offset": 335.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "agent ID that you are going to talk with",
      "offset": 337.44,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "and then also a prompt uh that describes",
      "offset": 340.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "what you what you are asking from the",
      "offset": 344.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "agent. Here we're asking the agent uh",
      "offset": 345.68,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "should I use random forest or xg boost",
      "offset": 348.479,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "for a 5,000 sample data set and to limit",
      "offset": 351.039,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "its answer we're asking it to generate a",
      "offset": 353.84,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "three sentence response only. Let's run",
      "offset": 356.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this cell. Uh let's print the response",
      "offset": 359.199,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "type. And we have a successful response",
      "offset": 362.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "which is called a conversation response",
      "offset": 365.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "object. This response object is going to",
      "offset": 367.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "have an outputs array. Let's print it",
      "offset": 370,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "and see how many elements it has.",
      "offset": 373.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Uh right now it only has one element",
      "offset": 376.8,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "because we've only asked it. We only",
      "offset": 379.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "asked for one question. Let's print its",
      "offset": 381.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "content using the response.puts.content",
      "offset": 384.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "content syntax and this is going to",
      "offset": 386.88,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "extract the raw text here and print it.",
      "offset": 390,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "As you can see, we have one, two, three",
      "offset": 394.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "sentences which uh aligns with the",
      "offset": 397.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "question that we've asked. You can pause",
      "offset": 400,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "here to read the accuracy of the",
      "offset": 401.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "response generated by the agent. This is",
      "offset": 404.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "a basic workflow for creating agents and",
      "offset": 406.319,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "starting conversations with them. In the",
      "offset": 409.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "next sections, we're going to dive deep",
      "offset": 411.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "into the into the agents API and talk",
      "offset": 413.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "about more intermediate and advanced",
      "offset": 416.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "concepts. The first deep dive concept",
      "offset": 418.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that we're going to talk about is",
      "offset": 421.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "creating effective agents. Agent",
      "offset": 423.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "performance in production scenarios is",
      "offset": 425.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "mostly determined by model choice,",
      "offset": 427.52,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "system prompt, and completion",
      "offset": 430.8,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "parameters. First, let's talk about",
      "offset": 432.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "model choice. Here we have Mistral",
      "offset": 434.479,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "medium model which is uh a premier uh",
      "offset": 437.12,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "and paid model offered by Mistral and it",
      "offset": 440.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "brings a balance between performance and",
      "offset": 444.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "cost. But depending on your use cases,",
      "offset": 446.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you're going to want a larger model with",
      "offset": 449.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "strong reasoning or code execution",
      "offset": 453.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "capabilities. Right now, Mrol medium is",
      "offset": 455.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "not a reasoning model, but it has a",
      "offset": 458.16,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "large context window of 128,000 tokens,",
      "offset": 460.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "but other good options exist like uh",
      "offset": 463.919,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "code stroll 2 for coding and megistral",
      "offset": 466.8,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "medium for reasoning. The second uh most",
      "offset": 469.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "influential factor in aging performance",
      "offset": 472.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is the system prompt. Uh we've already",
      "offset": 474.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "talked about this uh but uh we need to",
      "offset": 477.68,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "discuss it more because system prompt is",
      "offset": 480.72,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "very important in production scenarios.",
      "offset": 483.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "If you explore other platforms uh or",
      "offset": 485.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "other agents uh for for their system",
      "offset": 488.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "prompts, you're going to see that they",
      "offset": 490.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "contain thousands and thousands of words",
      "offset": 492.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that control the agent's behavior",
      "offset": 494.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "precisely. It tells the agent what to",
      "offset": 496.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "do, what kind of questions it can",
      "offset": 498.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "answer, what kind of questions it should",
      "offset": 500.16,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "avoid answering, and uh the tone of the",
      "offset": 502,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "language uh and everything that's",
      "offset": 504.879,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "related to the agents behavior. Uh so in",
      "offset": 506.639,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "practice when you're creating agents uh",
      "offset": 509.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you most of your time won't be spent on",
      "offset": 511.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "writing the code but it's going to be",
      "offset": 514.64,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "spent on prompt engineering and writing",
      "offset": 517.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "good system prompts. And then moving on",
      "offset": 519.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "to the third most important parameter in",
      "offset": 521.919,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "in performance is completion parameters.",
      "offset": 524.56,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "Right now we only have one uh completion",
      "offset": 527.68,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "parameter defined here which is the most",
      "offset": 530.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "important and it is called temperature.",
      "offset": 533.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "As you know, language models are word",
      "offset": 535.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "generating models. Uh what this means is",
      "offset": 538,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that if they have a text input, they're",
      "offset": 540.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "going to continue it by using the next",
      "offset": 542.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "word which with with the highest",
      "offset": 545.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "probability. For example, if the text is",
      "offset": 547.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "I am the model is going to choose uh the",
      "offset": 550.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "next word based on this temperature",
      "offset": 554,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "parameter. If the temperature value is",
      "offset": 556,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "between 0.1 and 0.5, it makes the agent",
      "offset": 559.6,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "very factual and very technical. If the",
      "offset": 563.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the temperature is between 0.5 and one,",
      "offset": 567.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it's going to be very creative. So, it's",
      "offset": 570.16,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "going to work for tasks that require",
      "offset": 572.16,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "creativity, brainstorming, and novelty.",
      "offset": 574.959,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Okay, let's start the deep dive section",
      "offset": 578.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "by talking about how to create effective",
      "offset": 580.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "agents in production scenarios. Agent",
      "offset": 583.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "performance is mostly determined by",
      "offset": 586.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "first model choice and second the system",
      "offset": 588.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "prompt and third completion parameters.",
      "offset": 591.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "So let's talk about this model choice",
      "offset": 594.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "first. Before doing anything you should",
      "offset": 597.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "decide what kind of model that you want",
      "offset": 599.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to use for your for the unique problem",
      "offset": 601.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that you are going to solve. Right here",
      "offset": 604,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we have the MR medium model for the data",
      "offset": 606.16,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "agent. But depending on your use case,",
      "offset": 609.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "you can choose other higherend models",
      "offset": 612.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "like code straw 2 for coding or",
      "offset": 615.279,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "magistral medium for reasoning. And I've",
      "offset": 618.079,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "I've left a link here for the models",
      "offset": 621.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "overview page of the mistral API",
      "offset": 624.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "documentation and you can check that out",
      "offset": 626.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "uh to choose what kind of models mist",
      "offset": 629.12,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "offers. Uh right now it it offers two",
      "offset": 632.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "different types of models. One is paid",
      "offset": 635.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and of course the other one is open",
      "offset": 638.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "weight. For high impact scenarios, you",
      "offset": 640.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "always want to choose paid models",
      "offset": 642.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "because they offer a very high",
      "offset": 643.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "performance. Uh but for lower impact",
      "offset": 645.519,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "scenarios, you can go with uh models",
      "offset": 648.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "with less context length and less",
      "offset": 651.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "parameters like mistrol small or",
      "offset": 653.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "maggistral small. And then you should",
      "offset": 655.839,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "spend a lot of time uh on writing a very",
      "offset": 657.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "detailed and high quality system prompt.",
      "offset": 661.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "System prompt is very important for",
      "offset": 664,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "agents because they control precisely",
      "offset": 665.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "how the agent behaves in production. If",
      "offset": 668.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "you dis if you explore other agents uh",
      "offset": 671.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "used by top companies like OpenAI or",
      "offset": 674.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Anthropic, you will see that their",
      "offset": 676.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "system prompts are going to contain",
      "offset": 679.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "thousands and thousands of words because",
      "offset": 681.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh you they want to control uh every",
      "offset": 684.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "single behavior of the agent uh from how",
      "offset": 687.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it responds to what kind of tools it can",
      "offset": 690.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "use and how it can use them. So system",
      "offset": 692.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "prompt is going to take most of your",
      "offset": 695.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "time to write in production uh instead",
      "offset": 697.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of actually writing code and then you",
      "offset": 700.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "need to choose uh a good value for the",
      "offset": 702.56,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "temperature parameter uh values between",
      "offset": 705.92,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "0.1 and 0.5 for temperature makes the",
      "offset": 709.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "model very factual and technical and it",
      "offset": 713.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is it's going to be ideal for writing",
      "offset": 715.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "code or uh or for scenarios where",
      "offset": 718,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "factual accuracy is desired for values",
      "offset": 721.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "between 0.5 5 and one the model is going",
      "offset": 723.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to become very creative. So it's going",
      "offset": 726.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to be very useful for uh tasks like",
      "offset": 728.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "brainstorming or writing poems or",
      "offset": 731.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "stories because the model is going to be",
      "offset": 734.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "very creative. Why this changes based on",
      "offset": 736.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "temperature is uh due to how LMS are",
      "offset": 739.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "built. As you know, LLMs generate uh",
      "offset": 742.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "text token by token. And the way they do",
      "offset": 746.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that is that they are going to be",
      "offset": 748.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "provided with an initial text, which is",
      "offset": 751.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the system prompt and the user's",
      "offset": 754,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "question. And then they're going to",
      "offset": 755.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "predict the next word by their",
      "offset": 758.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "probability.",
      "offset": 761.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Using lower end temperatures makes the",
      "offset": 763.04,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "makes the model to choose words with the",
      "offset": 766.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "highest probability which means they are",
      "offset": 769.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "always going to be chosen based on how",
      "offset": 771.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "frequently those words show up in the a",
      "offset": 773.839,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "in the model's training process and",
      "offset": 776.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "models towards the higher end like",
      "offset": 780.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "values closer to one makes the model",
      "offset": 783.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "choose words with lower probabilities.",
      "offset": 785.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "So uh it makes the model more creative.",
      "offset": 788.399,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "And now moving on to the second uh deep",
      "offset": 792.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "dive section which is managing",
      "offset": 795.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "conversations with an agent. As you know",
      "offset": 797.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "most agent building frameworks don't",
      "offset": 799.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "have built-in memory capabilities but uh",
      "offset": 801.279,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "Msaw agents API has that distinct",
      "offset": 804.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "advantage. It comes with built-in memory",
      "offset": 807.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and conversation management. We've",
      "offset": 809.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "already seen how to use the",
      "offset": 811.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "client.ba.con conversations.st start",
      "offset": 813.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "command which starts a completely new",
      "offset": 816,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "conversation thread with an agent. It",
      "offset": 818.959,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "requires agents ID and a prompt to start",
      "offset": 821.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "the conversation. So let's run this code",
      "offset": 825.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "snippet once again. This time we're",
      "offset": 828.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "asking the ML agent for best practices",
      "offset": 830.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for encoding categorical variables.",
      "offset": 833.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Let's wait a few seconds for it to",
      "offset": 835.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "finish and then we're going to do",
      "offset": 837.6,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "something new here which is printing the",
      "offset": 840,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "conversation ID.",
      "offset": 842.959,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "All response objects generated by uh by",
      "offset": 845.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "agents are going to have the",
      "offset": 849.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "conversation ID. This is done so uh so",
      "offset": 850.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "that we can differentiate one thread of",
      "offset": 853.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "conversation from another. And this is",
      "offset": 855.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "going to be important in production",
      "offset": 857.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "scenarios because you're going to have",
      "offset": 859.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "thousands of users for your agent. You",
      "offset": 861.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "don't want their conversations",
      "offset": 863.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "overlapping with each other. We have",
      "offset": 866.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "seen the start command and now we're",
      "offset": 867.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "going to explore the append command",
      "offset": 869.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "which is used to continue a",
      "offset": 871.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "conversation. So if you want to ask a",
      "offset": 873.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "follow-up question to the first question",
      "offset": 876.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that you ask, you are going to use the",
      "offset": 878,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "append method. As you can see here,",
      "offset": 879.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we're going to we're creating a",
      "offset": 882.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "follow-up uh variable and we are and it",
      "offset": 884.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "is equal to client.bata",
      "offset": 887.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "conversations.append append and this",
      "offset": 889.519,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "this function is going to require the",
      "offset": 892.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "conversation ID that we're going to",
      "offset": 894.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "continue and our question is uh asking",
      "offset": 896.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the agent what was my first question and",
      "offset": 899.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the followup should correctly print our",
      "offset": 901.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "first question which was asking the best",
      "offset": 904.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "practices for categorical encoding.",
      "offset": 907.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Let's run this cell and let's see what",
      "offset": 909.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "what the output is. And as you can see",
      "offset": 911.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the the response is almost instantaneous",
      "offset": 914,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and it is printing our first question",
      "offset": 916.56,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "word by word. Let's confirm going back",
      "offset": 919.519,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "here.",
      "offset": 922.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "As you can see our first question was",
      "offset": 924.639,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "exactly like it says right here.",
      "offset": 927.519,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "We can ask even more questions to",
      "offset": 931.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "continue the conversation like summarize",
      "offset": 934.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the entire conversation in three",
      "offset": 936.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sentences.",
      "offset": 938.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Let's see it. And as you can see the",
      "offset": 940.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "agent says you asked for best practices",
      "offset": 943.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for encoding categorical variables in ML",
      "offset": 945.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and I provided three methods and then",
      "offset": 947.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you asked for what your first question",
      "offset": 950.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "was and I repeated it for you. So this",
      "offset": 952.399,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "was how to uh so this process is for",
      "offset": 954.56,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "continuing conversations. Now uh we have",
      "offset": 958.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "another method for uh managing",
      "offset": 961.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "conversations which is going to be",
      "offset": 963.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "client.ba.con conversations.list.",
      "offset": 966.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Uh this list function is going to return",
      "offset": 968.959,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "an array containing all conversation",
      "offset": 971.36,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "objects that you have under a single API",
      "offset": 974,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "key. Let's run this sub and print the",
      "offset": 977.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "length of the conversations list which",
      "offset": 980.72,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "is going to be a lot for my API key",
      "offset": 982.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "because I've been using the API for a",
      "offset": 985.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "while now and uh I have a lot of",
      "offset": 987.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "conversations. As you can see it's it",
      "offset": 989.6,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "shows 85. And let's print the the very",
      "offset": 991.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "first item in this conversations list",
      "offset": 995.199,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "which is going to be the very last",
      "offset": 997.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "conversation that we've started. As you",
      "offset": 1000.72,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "can see the conversation ID is 9374E",
      "offset": 1002.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and it's going to match the conversation",
      "offset": 1007.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that we started right above here.",
      "offset": 1009.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Here it is. The last five digits of the",
      "offset": 1013.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "conversation ID is the same as the one",
      "offset": 1015.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "that we printed right here.",
      "offset": 1017.92,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "Now another very important method for",
      "offset": 1021.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "conversation management is get messages",
      "offset": 1023.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "uh which requires the conversation ID",
      "offset": 1026.72,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and as you can guess from the name it's",
      "offset": 1029.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "going to return all the uh message",
      "offset": 1031.439,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "history of a single conversation in an",
      "offset": 1034.079,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "array and we can access that array using",
      "offset": 1036.64,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "the do messages attribute. Let's run it",
      "offset": 1039.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 1043.199,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "let's run it. As you can see, it's a",
      "offset": 1044.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "huge array containing uh many output",
      "offset": 1046.319,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "entries. And let's see its length.",
      "offset": 1049.6,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "As you can see, it shows six because",
      "offset": 1053.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we've asked the agent three questions",
      "offset": 1056.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and it came back with three responses,",
      "offset": 1058.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "which makes the conversation history six",
      "offset": 1060.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "item uh array.",
      "offset": 1063.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "These all of these conversation",
      "offset": 1066.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "management methods are important for",
      "offset": 1068.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "building chat applications with message",
      "offset": 1071.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "history and for user friendliness. There",
      "offset": 1073.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "are other conversation management",
      "offset": 1075.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "methods on the documentation and I have",
      "offset": 1077.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I've left a link here uh for you to",
      "offset": 1079.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "check it out. Now next topic we're going",
      "offset": 1082.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to discuss is streaming responses for",
      "offset": 1084.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "user friendliness which is a requirement",
      "offset": 1088.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "for all modern agents and chat",
      "offset": 1091.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "platforms. For example, a chat GPT and",
      "offset": 1093.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "clot uh provide immediate usual visual",
      "offset": 1096.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "feedback for the user by printing every",
      "offset": 1100.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "word as it is generated by the model.",
      "offset": 1102.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "And we want to achieve the same",
      "offset": 1105.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "functionality with our MSOL agents here.",
      "offset": 1107.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "And this process starts using the start",
      "offset": 1110.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "stream command. Instead of the start",
      "offset": 1113.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "command for conversations, we're going",
      "offset": 1116,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to use the start stream command. And",
      "offset": 1118,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "this again starts a new conversation",
      "offset": 1120.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "with the agent but with streaming mode",
      "offset": 1123.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "enabled. I'm going to right now I'm",
      "offset": 1125.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "going to run these three cells one by",
      "offset": 1128.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "one and then explain what's happening",
      "offset": 1132.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "under the hood. Let's run this uh and",
      "offset": 1133.919,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "immediately as you can see the agent",
      "offset": 1137.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "starts generating the output one by one.",
      "offset": 1140,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Uh let's run let's run the rest one.",
      "offset": 1144,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Yeah, here we go. It is printing the",
      "offset": 1146.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "text that explains gradient descent in",
      "offset": 1148.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "five sentences. As you can see, it",
      "offset": 1151.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "generated it finished execution very",
      "offset": 1154.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "quickly and we didn't have to wait for",
      "offset": 1157.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the entire response to finish. Uh and",
      "offset": 1159.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "we've seen we saw each token being",
      "offset": 1162.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "generated by one by one. So let's see",
      "offset": 1165.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "what is happening.",
      "offset": 1167.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "First we're starting a new conversation",
      "offset": 1170,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "with streaming mode enabled which",
      "offset": 1172.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "requires our agent ID once again. And",
      "offset": 1174.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "then we're input we're giving our prompt",
      "offset": 1176.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "as input to the inputs parameter. And",
      "offset": 1179.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "then for reference we're printing the",
      "offset": 1182.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "type of the response which is an event",
      "offset": 1185.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "stream object. You're going to realize",
      "offset": 1187.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "what that is in a moment. And then we're",
      "offset": 1189.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "going to write write this event stream",
      "offset": 1192.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "object. When LLMs generate text one by",
      "offset": 1194.799,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "one, they do so using events. And these",
      "offset": 1197.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "events can be anything. First they can",
      "offset": 1201.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "be uh the start of the response or the",
      "offset": 1203.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "generation that that is one event.",
      "offset": 1206.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Another event may be a text event which",
      "offset": 1209.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "is generating one uh one word at a time.",
      "offset": 1212,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "The all those all those generated words",
      "offset": 1215.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "will be considered one event. Tool",
      "offset": 1219.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "execution or tool calls are also",
      "offset": 1221.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "considered a single event. Also the end",
      "offset": 1223.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of the response is also considered an",
      "offset": 1226.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "event. So what we're going to do is",
      "offset": 1229.039,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "write these stream of events and filter",
      "offset": 1231.919,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "the events for token generation ones.",
      "offset": 1235.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Right here we are doing exactly that.",
      "offset": 1239.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "We're starting the right using the",
      "offset": 1241.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "context manager with response as event",
      "offset": 1244.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "stream. And then for each event we're",
      "offset": 1246.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "checking its type. If its type is",
      "offset": 1249.28,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "message.output.delta delta which is the",
      "offset": 1251.84,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "official type in the MSL API for token",
      "offset": 1255.039,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "generation. We're going to print that",
      "offset": 1258.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "tokens value using the event dot data",
      "offset": 1261.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "content syntax. And here we're using",
      "offset": 1265.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "special parameters of the print",
      "offset": 1267.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "function. First of all, we're setting",
      "offset": 1269.28,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "the end of the printed text to an empty",
      "offset": 1271.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "stream because by default each printed",
      "offset": 1275.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "text in Python gets printed on a new",
      "offset": 1278.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "line. We don't want that because we're",
      "offset": 1281.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "printing words. They should appear next",
      "offset": 1282.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to each other. And we also we're going",
      "offset": 1285.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "to set flush equals to true. When we do",
      "offset": 1287.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that and a typewriter effect uh is",
      "offset": 1290.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "created and each word is printed as it",
      "offset": 1293.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is generated by the model. Next uh",
      "offset": 1296.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "method that we're going to learn is the",
      "offset": 1299.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "append stream. Uh just like the",
      "offset": 1301.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "conversation start had a function for",
      "offset": 1304.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "continuing conversations in the form of",
      "offset": 1307.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "append. We have the append stream",
      "offset": 1309.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "function for continuing conversations in",
      "offset": 1312.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "streaming mode. To use the append stream",
      "offset": 1314.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "function correctly, first we're going to",
      "offset": 1317.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "get the conversation ID of the last",
      "offset": 1319.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "response. And we can do so using the",
      "offset": 1322.799,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "client.ba.con conversations.list command",
      "offset": 1325.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and getting the ID of the last",
      "offset": 1328.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "conversation. And then we're going to",
      "offset": 1330.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "provide that to the appendad stream",
      "offset": 1332.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "function and a new input which asks the",
      "offset": 1334.799,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "agent to translate its last response to",
      "offset": 1338,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "French. Uh and we're going to write the",
      "offset": 1340.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "event stream once again and print only",
      "offset": 1343.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the token generation events. So let's",
      "offset": 1346.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "run this and see a French text being",
      "offset": 1348.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "generated.",
      "offset": 1351.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "Here we go. As you can see uh I have",
      "offset": 1354.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "zero French knowledge. Uh so if you know",
      "offset": 1357.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "French you can gen you you can test the",
      "offset": 1359.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "accuracy of this translation. Now let's",
      "offset": 1362.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "move on to using uh the built-in web",
      "offset": 1365.039,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "search tool of the ML agents API. As you",
      "offset": 1368.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "know uh most large language models live",
      "offset": 1372.159,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "in 2024 which means their training",
      "offset": 1375.2,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "knowledge stops in 2024. when you ask",
      "offset": 1378.32,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "them about events in 2025, they're going",
      "offset": 1381.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to hallucinate, which means they are",
      "offset": 1384.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "going to uh provide information as",
      "offset": 1387.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "though they know the answer when in",
      "offset": 1389.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "actuality it's complete rubbish. So uh",
      "offset": 1391.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "we're going to fix this by giving the",
      "offset": 1394.64,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "agents access to real time web search.",
      "offset": 1397.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And this is very easy using the",
      "offset": 1400.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "following syntax here. We're creating a",
      "offset": 1402.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "new agent called ML research agent and",
      "offset": 1405.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "we're giving it a brief description. And",
      "offset": 1408.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "what's different here is that it also",
      "offset": 1410.559,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the agent has access to the tools array",
      "offset": 1413.12,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "which contains a single web search tool",
      "offset": 1416.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "which is defined in a dictionary format",
      "offset": 1419.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "like this. Let's ask a question from the",
      "offset": 1422.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "agent that forces it to use this new web",
      "offset": 1425.039,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "search tool. And our question is what",
      "offset": 1427.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "are the latest transformer improvements",
      "offset": 1430.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in 2024f?",
      "offset": 1432.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "And we want it summarized right here.",
      "offset": 1434.799,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "Let's run this cell and print the",
      "offset": 1438.559,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "outputs array.",
      "offset": 1441.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "It's going to take a few seconds more",
      "offset": 1444.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "because apart on top of regular",
      "offset": 1446.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "response, it's also going to search the",
      "offset": 1449.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "internet for the correct answers. As you",
      "offset": 1451.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "can see uh we have a huge array uh of uh",
      "offset": 1454.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "of response but it's the array only",
      "offset": 1458.24,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "contains two elements. One is tool",
      "offset": 1461.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "execution entry. The other is message",
      "offset": 1464.159,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "output entry. Let's first uh see the",
      "offset": 1466.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "tool execution which has the name web",
      "offset": 1469.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "search which means our agent correctly",
      "offset": 1472.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "used the web search tool. And then we",
      "offset": 1475.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "have message output entry and",
      "offset": 1477.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "interestingly its content doesn't",
      "offset": 1479.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "involve a single code. a a a single text",
      "offset": 1481.679,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "but a chunks of text like text in in the",
      "offset": 1484.64,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "form of text chunk classes and also tool",
      "offset": 1487.919,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "reference chunk. Let's take a closer",
      "offset": 1491.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "look at this object uh by printing by",
      "offset": 1494.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "printing it here.",
      "offset": 1497.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "As you can see, it's an array of text",
      "offset": 1500.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "chunk and tool reference chunk objects.",
      "offset": 1502.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Let's take a closer look. When the",
      "offset": 1505.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "object is a text chunk, it is a re it's",
      "offset": 1508.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "a regular text. It starts like in 2025.",
      "offset": 1510.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "The transformer actor architecture",
      "offset": 1513.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "continues to evolve. But as you can see,",
      "offset": 1516.24,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "the text is cut off short by the next",
      "offset": 1518.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "tool reference track which contains a",
      "offset": 1523.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "citation object. A citation means the",
      "offset": 1525.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "agent is uh trying to give us",
      "offset": 1528.32,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "information of on where it got the",
      "offset": 1531.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "following text from. As you can see,",
      "offset": 1534.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "this tool reference chunk has the title",
      "offset": 1537.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "of the web page that it scraped and also",
      "offset": 1540.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "the URL of the web page. Our job here is",
      "offset": 1543.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "to concatenate to combine all of this",
      "offset": 1546.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "information into a single response.",
      "offset": 1549.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Unfortunately, this is not provided by",
      "offset": 1551.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the agents API uh by default. So, we",
      "offset": 1554,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "have to do the formatting ourselves and",
      "offset": 1556.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "we're doing that right here. First of",
      "offset": 1559.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "all, we are iterating through that tool",
      "offset": 1562.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "reference chunks and text chunks for",
      "offset": 1564.64,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "result in search response.putscontent.",
      "offset": 1566.64,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "We're checking if the item has the text",
      "offset": 1571.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "attribute. If it has the text attribute,",
      "offset": 1573.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it means the item is just text. So, we",
      "offset": 1576.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "can add it to the markdown text right",
      "offset": 1579.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "here. If it has the tool attribute, it",
      "offset": 1581.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "means the agent is giving a citation.",
      "offset": 1584.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "So, we're going to append it to the",
      "offset": 1587.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "markdown text in a citation format using",
      "offset": 1589.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "the markdown link syntax, which means",
      "offset": 1592.48,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "links are given using square and round",
      "offset": 1595.84,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "brackets.",
      "offset": 1600.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "In the first square bracket, we're",
      "offset": 1601.919,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "extracting the uh citations title. In",
      "offset": 1604.4,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "the second rounded bracket, we're",
      "offset": 1608.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "extracting the citations URL and",
      "offset": 1610.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "appending it to the full markdown text",
      "offset": 1613.6,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "right here. After the iteration is over,",
      "offset": 1616.559,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "we are going to have a single agent",
      "offset": 1620.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "response which we can print using the",
      "offset": 1623.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "markdown display function of the Jupyter",
      "offset": 1626.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "notebook. Let's run this cell. And as",
      "offset": 1628.72,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "you can see, we have a complete and",
      "offset": 1631.52,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "beautifully formatted markdown text with",
      "offset": 1634.799,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "built-in citations which is provided by",
      "offset": 1638.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the tool execution or by the web search",
      "offset": 1640.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "tool. As you can see, every chunk of",
      "offset": 1643.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "text is followed by a citation that",
      "offset": 1646.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "gives us information of where the agent",
      "offset": 1649.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "is taking its information from. Again,",
      "offset": 1652,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh we can continue the conversation",
      "offset": 1654.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "using the append append method. And here",
      "offset": 1657.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "we're asking the agent to summarize the",
      "offset": 1660.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "results in five sentences. Let's run it",
      "offset": 1663.36,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "and see the output.",
      "offset": 1666.559,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "There you go. Uh we have five sentences",
      "offset": 1670.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that summarizes the news in the",
      "offset": 1673.52,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "transformer architecture landscape. Uh",
      "offset": 1676.799,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "you can pause the video here to read the",
      "offset": 1679.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "response on your own and get up to speed",
      "offset": 1682.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "on transformer news. All right, let's",
      "offset": 1684.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "now talk about code execution for",
      "offset": 1687.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "solving complex tasks that requires",
      "offset": 1689.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "using Python code. Using the code",
      "offset": 1693.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "interpreter tool follows the same syntax",
      "offset": 1695.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "as a web search. Here we're creating",
      "offset": 1698,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "coding assistant with access to the code",
      "offset": 1700.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "interpreter tool which is defined in in",
      "offset": 1703.919,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "a dictionary format once again. And this",
      "offset": 1707.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "time we're giving it a more complex",
      "offset": 1710.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "problem to visualize the mandel broad",
      "offset": 1713.12,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "set using the equation z= z ^2 + c and",
      "offset": 1716.24,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "it should calculate the convergence for",
      "offset": 1721.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "a complex plane grid and create a",
      "offset": 1723.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "beautiful fractal visualization wall",
      "offset": 1726,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "art. If you don't know what mandelet is,",
      "offset": 1728.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "it doesn't matter. Uh what matters is",
      "offset": 1730.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "that we're now asking something that the",
      "offset": 1732.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "LLMs can't do which is calculating",
      "offset": 1735.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "complex math equations by default. It's",
      "offset": 1738.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "going to be forced to use the the new",
      "offset": 1740.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "code interpreter tool that we have",
      "offset": 1744,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "defined right here. So let's run this",
      "offset": 1745.76,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "and you'll notice that the uh request is",
      "offset": 1749.76,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "going to take much longer than simple",
      "offset": 1753.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "web search or simple questions that we",
      "offset": 1755.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "have been asking from the agent so far.",
      "offset": 1758.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Let's wait for it to finish. And it's",
      "offset": 1760.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "probably going to take more than uh 60",
      "offset": 1763.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "seconds.",
      "offset": 1765.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "As you can see, the cell finished",
      "offset": 1768.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "execution. Let's print its outputs. And",
      "offset": 1770.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "we're going to see that uh the outputs",
      "offset": 1773.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "array this time is even larger than the",
      "offset": 1776.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "web search version. So let's take a look",
      "offset": 1778.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "at the second element of this outputs",
      "offset": 1781.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "array, which is another tool execution,",
      "offset": 1784.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "but this time it's a code interpreter",
      "offset": 1786.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "one. Uh let's take a closer look by",
      "offset": 1788.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "printing its contents.",
      "offset": 1791.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "And as you can see uh we have the name",
      "offset": 1793.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "as code interpreter. And interestingly",
      "offset": 1796.24,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "we also have another dictionary called",
      "offset": 1799.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "info which contains a key called code",
      "offset": 1802.159,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "which contains the code for generating",
      "offset": 1805.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the visualization that we asked in the",
      "offset": 1808.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "prompt.",
      "offset": 1811.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "We can extract the code into a single",
      "offset": 1813.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "string by using this syntax which uh",
      "offset": 1815.76,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "again extracts the tool execution class",
      "offset": 1818.96,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "and extracts its info attribute and then",
      "offset": 1822.399,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "extracts the code and then we print it.",
      "offset": 1825.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "As you can see the code uses only numpy",
      "offset": 1828.159,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "and mattplot li to create and to create",
      "offset": 1831.279,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and plot the mandelroad fractal set.",
      "offset": 1834.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "This is only the coding part of the",
      "offset": 1837.279,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "equation. uh we also want to extract the",
      "offset": 1839.039,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "plot that was generated after the",
      "offset": 1842.96,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "mistral servers ran this code on their",
      "offset": 1845.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "own and this can be done by using",
      "offset": 1848.399,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "another outputs array content and let's",
      "offset": 1851.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "uh let's see it which is going to be a",
      "offset": 1854.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "file chunk object uh as you can see this",
      "offset": 1856.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "tool file chunk class is was generated",
      "offset": 1860.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "by the code interpreter and it has a",
      "offset": 1863.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "file ID and also file name and the file",
      "offset": 1865.84,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "format which is a PNG file.",
      "offset": 1870,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "We're interested in this file ID. So",
      "offset": 1873.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "let's extract it. And now using this ID,",
      "offset": 1875.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we can download the file that was",
      "offset": 1879.12,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "generated by using the client.files",
      "offset": 1881.2,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "download method which downloads the file",
      "offset": 1884.799,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "in B 64 format. And when we use the read",
      "offset": 1888.399,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "function on top of it, we're going to",
      "offset": 1892.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "get the raw bytes information of the",
      "offset": 1894.559,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "file, which means we can use that raw",
      "offset": 1897.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "file and save it as a PNG file right",
      "offset": 1901.279,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "here. Let's run this cell. And then I",
      "offset": 1904.32,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "already have the markdown cell uh for",
      "offset": 1907.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "displaying that image. Let's run it once",
      "offset": 1911.279,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "again to see the updated image. And as",
      "offset": 1913.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "you can see the image is the same as",
      "offset": 1915.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "before uh and it contains the",
      "offset": 1918.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "visualization of the beautiful Mandelro",
      "offset": 1920.96,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "fractal art. So that right there was uh",
      "offset": 1923.519,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the end of our brief tutorial. We've",
      "offset": 1927.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "covered the basics and some intermediate",
      "offset": 1930.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "concepts of the MR agents API. I have",
      "offset": 1932.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "left some links here. So, be sure to",
      "offset": 1935.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "check out the entire documentation of",
      "offset": 1937.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the agents API and then explore other",
      "offset": 1939.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "tools like document search and image",
      "offset": 1942.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "generation which are also very cool use",
      "offset": 1944.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "cases for the agents API. And then I",
      "offset": 1946.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "definitely learned how to use function",
      "offset": 1949.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "calling that allows your agents to use",
      "offset": 1950.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "plain Python functions for connecting to",
      "offset": 1953.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "external services or APIs like databases",
      "offset": 1956.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "or custom APIs for even more",
      "offset": 1960.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "functionality beyond just code",
      "offset": 1962.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "interpretation or web search. And then",
      "offset": 1965.12,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "also the agents API supports building",
      "offset": 1968,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "multi- aent systems where you can",
      "offset": 1971.039,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "combine multiple agents and make them",
      "offset": 1973.12,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "coordinate to solve complex tasks using",
      "offset": 1976.159,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "different coordination patterns. So",
      "offset": 1979.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "thank you for watching this tutorial and",
      "offset": 1982.559,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "I'll see you in the next one.",
      "offset": 1984.64,
      "duration": 4.039
    }
  ],
  "cleanText": "Hi data scamps and data champs.\n\nThis is Richie. Let's get hands-on with Mistral's AI Agents API. You'll learn how to build powerful autonomous tools using AI agents. Our instructor Bex will cover getting started with the Agents API through to helping you build your first AI agent. To jump straight to coding, skip ahead to this timestamp. To follow along in data lab, check out the link in the description. Happy learning.\n\nIf you follow AI, you've probably noticed a major trend happening in the AI sphere. All big tech companies are open-sourcing frameworks to build AI agents. For example, OpenAI has its agents SDK. Google has a waste agent development kit, and Microsoft has had AutoAN for a while now. Recently, a very promising player joined this pack, and it is Mistral AI. Mistral's recently dropped Agents API allows you to build powerful autonomous agents with access to tools like web search, code execution, and image generation. In this tutorial, we're going to explore many of the features of this Agents API. And we're going to start simple by creating your first agent with built-in memory. And then we're going to move all the way up to using the built-in code execution tool to generate interact to generate beautiful plots like the mandal broad set that you see below.\n\nThis tutorial is designed for Python developers in mind who have basic Python knowledge and are interested in building authentic applications. So without further ado, let's get started by setting up your environment. As you can see, I have this pip install command here, which is going to install the Mistral AI package and the Python.v package. Mistral AI is for communicating with the agents with Mistral's API server, and Python. Env is a popular framework or package for managing environment variables through the help of files. After running this code snippet, after running this cell, you're going to create your account and sign in at console.mstrolai.home and get your API key and save it as an environment variable called mistrol API key in av file. If you don't know how to create the env file, I've pasted the command here for you, which is touch.env. Thisv file is the industry standard for hiding or storing sensitive information like API keys or sensitive information. And what you're going to do is add this file to your ignore file so that you won't accidentally commit your secrets to GitHub.\n\nOnce you go through these steps, you're ready to create your first agent, which is which you can do using the first cell that you see here. First, you're going to import the OS library and the TNV function from the Python.nv package that we've just installed. This function is going to read your TNV file here and load all the environment variables stored there and expose it to the OS package. Then you are going to import the Mistral class here from Mistral AI, which is used to create a client that will communicate with the Mistral's API using your credentials, which you can extract using the os.get env function. Here's the line that does this operation. And then we're going to create our very first agent using the client.ba.agents.create function. This function is going to accept four parameters. The first one is the model name, which is going to be MR medium. This model is going to bring a balance between performance and cost. And then we're going to give a unique name to the agent. This name doesn't necessarily have to be unique, but for organization purposes and clarity, I recommend that you stick to unique names for your agents so that you can differentiate them later on. And then you can give a custom description for your agent. This is for readability purposes and is going to be important when creating multi-agent systems, which we won't cover here as it is beyond the scope of the tutorial. And then you're going to have your instructions, which is basically a system prompt that tells the agent how to behave. Here we're keeping it simple by telling the agent that it is an ML expert and it should give practical and actionable advice. Let's run this cell and wait for it to finish. And as you can see, we have a green check mark here. And let's print the agent ID. In Miles's API, all agents have unique IDs so that you can keep track of how many agents or what kind of agents you have available under a single API key.\n\nIf you notice here, I'm using a data lab environment, which is an environment provided by Data Camp, but you can run all of these code examples in your own Jupyter notebook environment or in VS code, which is going to offer the same coding experience. And then we're going to move on to starting a conversation with the agent that we've just created. And this is going to be very easy by using the client.ba conversations start function. This function is going to require the agent ID that you are going to talk with and then also a prompt that describes what you what you are asking from the agent. Here we're asking the agent, should I use random forest or xg boost for a 5,000 sample data set and to limit its answer, we're asking it to generate a three sentence response only. Let's run this cell. Let's print the response type. And we have a successful response, which is called a conversation response object. This response object is going to have an outputs array. Let's print it and see how many elements it has. Right now it only has one element because we've only asked it. We only asked for one question. Let's print its content using the response.puts.content content syntax, and this is going to extract the raw text here and print it. As you can see, we have one, two, three sentences, which aligns with the question that we've asked. You can pause here to read the accuracy of the response generated by the agent.\n\nThis is a basic workflow for creating agents and starting conversations with them. In the next sections, we're going to dive deep into the agents API and talk about more intermediate and advanced concepts. The first deep dive concept that we're going to talk about is creating effective agents. Agent performance in production scenarios is mostly determined by model choice, system prompt, and completion parameters. First, let's talk about model choice. Here we have Mistral medium model, which is a premier and paid model offered by Mistral, and it brings a balance between performance and cost. But depending on your use cases, you're going to want a larger model with strong reasoning or code execution capabilities. Right now, Mrol medium is not a reasoning model, but it has a large context window of 128,000 tokens, but other good options exist like code stroll 2 for coding and megistral medium for reasoning. The second most influential factor in aging performance is the system prompt. We've already talked about this, but we need to discuss it more because system prompt is very important in production scenarios. If you explore other platforms or other agents for their system prompts, you're going to see that they contain thousands and thousands of words that control the agent's behavior precisely. It tells the agent what to do, what kind of questions it can answer, what kind of questions it should avoid answering, and the tone of the language and everything that's related to the agents behavior. So in practice, when you're creating agents, most of your time won't be spent on writing the code, but it's going to be spent on prompt engineering and writing good system prompts. And then moving on to the third most important parameter in performance is completion parameters. Right now we only have one completion parameter defined here, which is the most important, and it is called temperature. As you know, language models are word generating models. What this means is that if they have a text input, they're going to continue it by using the next word which with the highest probability. For example, if the text is I am, the model is going to choose the next word based on this temperature parameter. If the temperature value is between 0.1 and 0.5, it makes the agent very factual and very technical. If the temperature is between 0.5 and one, it's going to be very creative. So, it's going to work for tasks that require creativity, brainstorming, and novelty.\n\nOkay, let's start the deep dive section by talking about how to create effective agents in production scenarios. Agent performance is mostly determined by first model choice and second the system prompt and third completion parameters. So let's talk about this model choice first. Before doing anything, you should decide what kind of model that you want to use for your for the unique problem that you are going to solve. Right here we have the MR medium model for the data agent. But depending on your use case, you can choose other higher-end models like code straw 2 for coding or magistral medium for reasoning. And I've left a link here for the models overview page of the Mistral API documentation, and you can check that out to choose what kind of models Mistral offers. Right now it offers two different types of models. One is paid, and of course the other one is open weight. For high impact scenarios, you always want to choose paid models because they offer a very high performance. But for lower impact scenarios, you can go with models with less context length and less parameters like mistrol small or maggistral small. And then you should spend a lot of time on writing a very detailed and high quality system prompt. System prompt is very important for agents because they control precisely how the agent behaves in production. If you explore other agents used by top companies like OpenAI or Anthropic, you will see that their system prompts are going to contain thousands and thousands of words because you they want to control every single behavior of the agent from how it responds to what kind of tools it can use and how it can use them. So system prompt is going to take most of your time to write in production instead of actually writing code. And then you need to choose a good value for the temperature parameter. Values between 0.1 and 0.5 for temperature makes the model very factual and technical, and it is it's going to be ideal for writing code or for scenarios where factual accuracy is desired. For values between 0.5 and one, the model is going to become very creative. So it's going to be very useful for tasks like brainstorming or writing poems or stories because the model is going to be very creative. Why this changes based on temperature is due to how LLMs are built. As you know, LLMs generate text token by token. And the way they do that is that they are going to be provided with an initial text, which is the system prompt and the user's question. And then they're going to predict the next word by their probability. Using lower end temperatures makes the makes the model to choose words with the highest probability, which means they are always going to be chosen based on how frequently those words show up in the in the model's training process, and models towards the higher end, like values closer to one, makes the model choose words with lower probabilities. So it makes the model more creative.\n\nAnd now moving on to the second deep dive section, which is managing conversations with an agent. As you know, most agent building frameworks don't have built-in memory capabilities, but Msaw agents API has that distinct advantage. It comes with built-in memory and conversation management. We've already seen how to use the client.ba.con conversations.st start command, which starts a completely new conversation thread with an agent. It requires agents ID and a prompt to start the conversation. So let's run this code snippet once again. This time we're asking the ML agent for best practices for encoding categorical variables. Let's wait a few seconds for it to finish, and then we're going to do something new here, which is printing the conversation ID. All response objects generated by by agents are going to have the conversation ID. This is done so that we can differentiate one thread of conversation from another. And this is going to be important in production scenarios because you're going to have thousands of users for your agent. You don't want their conversations overlapping with each other. We have seen the start command, and now we're going to explore the append command, which is used to continue a conversation. So if you want to ask a follow-up question to the first question that you ask, you are going to use the append method. As you can see here, we're going to we're creating a follow-up variable, and we are and it is equal to client.bata conversations.append append, and this function is going to require the conversation ID that we're going to continue, and our question is asking the agent what was my first question, and the followup should correctly print our first question, which was asking the best practices for categorical encoding. Let's run this cell and let's see what what the output is. And as you can see, the response is almost instantaneous, and it is printing our first question word by word. Let's confirm going back here. As you can see, our first question was exactly like it says right here. We can ask even more questions to continue the conversation like summarize the entire conversation in three sentences. Let's see it. And as you can see, the agent says you asked for best practices for encoding categorical variables in ML, and I provided three methods, and then you asked for what your first question was, and I repeated it for you. So this was how to so this process is for continuing conversations. Now we have another method for managing conversations, which is going to be client.ba.con conversations.list. This list function is going to return an array containing all conversation objects that you have under a single API key. Let's run this sub and print the length of the conversations list, which is going to be a lot for my API key because I've been using the API for a while now, and I have a lot of conversations. As you can see, it shows 85. And let's print the very first item in this conversations list, which is going to be the very last conversation that we've started. As you can see, the conversation ID is 9374E, and it's going to match the conversation that we started right above here. Here it is. The last five digits of the conversation ID is the same as the one that we printed right here. Now another very important method for conversation management is get messages, which requires the conversation ID, and as you can guess from the name, it's going to return all the message history of a single conversation in an array, and we can access that array using the do messages attribute. Let's run it and let's run it. As you can see, it's a huge array containing many output entries. And let's see its length. As you can see, it shows six because we've asked the agent three questions, and it came back with three responses, which makes the conversation history six item.\n\n\nUh, array.\nThese, all of these conversation management methods are important for building chat applications with message history and for user friendliness.\nThere are other conversation management methods on the documentation, and I have, I've left a link here for you to check it out.\nNow, the next topic we're going to discuss is streaming responses for user friendliness, which is a requirement for all modern agents and chat platforms.\nFor example, a chat GPT and clot provide immediate visual feedback for the user by printing every word as it is generated by the model.\nAnd we want to achieve the same functionality with our Mistral Agents API agents here.\nAnd this process starts using the start stream command.\nInstead of the start command for conversations, we're going to use the start stream command.\nAnd this again starts a new conversation with the agent, but with streaming mode enabled.\nI'm going to, right now, I'm going to run these three cells one by one and then explain what's happening under the hood.\nLet's run this, and immediately, as you can see, the agent starts generating the output one by one.\nUh, let's run, let's run the rest one.\nYeah, here we go.\nIt is printing the text that explains gradient descent in five sentences.\nAs you can see, it generated, it finished execution very quickly, and we didn't have to wait for the entire response to finish.\nUh, and we've seen, we saw each token being generated by one by one.\nSo let's see what is happening.\nFirst, we're starting a new conversation with streaming mode enabled, which requires our agent ID once again.\nAnd then we're input, we're giving our prompt as input to the inputs parameter.\nAnd then for reference, we're printing the type of the response, which is an event stream object.\nYou're going to realize what that is in a moment.\nAnd then we're going to write, write this event stream object.\nWhen LLMs generate text one by one, they do so using events.\nAnd these events can be anything.\nFirst, they can be the start of the response or the generation, that is one event.\nAnother event may be a text event, which is generating one, uh, one word at a time.\nThe all those, all those generated words will be considered one event.\nTool execution or tool calls are also considered a single event.\nAlso, the end of the response is also considered an event.\nSo what we're going to do is write these stream of events and filter the events for token generation ones.\nRight here, we are doing exactly that.\nWe're starting the right using the context manager with response as event stream.\nAnd then for each event, we're checking its type.\nIf its type is message.output.delta delta, which is the official type in the Mistral Agents API for token generation, we're going to print that tokens value using the event dot data content syntax.\nAnd here we're using special parameters of the print function.\nFirst of all, we're setting the end of the printed text to an empty stream because by default each printed text in Python gets printed on a new line.\nWe don't want that because we're printing words.\nThey should appear next to each other.\nAnd we also, we're going to set flush equals to true.\nWhen we do that, and a typewriter effect is created, and each word is printed as it is generated by the model.\nNext, uh, method that we're going to learn is the append stream.\nUh, just like the conversation start had a function for continuing conversations in the form of append, we have the append stream function for continuing conversations in streaming mode.\nTo use the append stream function correctly, first we're going to get the conversation ID of the last response.\nAnd we can do so using the client.ba.con conversations.list command and getting the ID of the last conversation.\nAnd then we're going to provide that to the appendad stream function and a new input which asks the agent to translate its last response to French.\nUh, and we're going to write the event stream once again and print only the token generation events.\nSo let's run this and see a French text being generated.\nHere we go.\nAs you can see, uh, I have zero French knowledge.\nUh, so if you know French, you can gen, you can test the accuracy of this translation.\nNow let's move on to using the built-in web search tool of the Mistral Agents API.\nAs you know, uh, most large language models live in 2024, which means their training knowledge stops in 2024.\nWhen you ask them about events in 2025, they're going to hallucinate, which means they are going to uh, provide information as though they know the answer when in actuality it's complete rubbish.\nSo, uh, we're going to fix this by giving the agents access to real time web search.\nAnd this is very easy using the following syntax here.\nWe're creating a new agent called Mistral research agent, and we're giving it a brief description.\nAnd what's different here is that it also, the agent has access to the tools array, which contains a single web search tool, which is defined in a dictionary format like this.\nLet's ask a question from the agent that forces it to use this new web search tool.\nAnd our question is, what are the latest transformer improvements in 2024?\nAnd we want it summarized right here.\nLet's run this cell and print the outputs array.\nIt's going to take a few seconds more because apart on top of regular response, it's also going to search the internet for the correct answers.\nAs you can see, uh, we have a huge array of uh, of response, but it's the array only contains two elements.\nOne is tool execution entry.\nThe other is message output entry.\nLet's first uh, see the tool execution, which has the name web search, which means our agent correctly used the web search tool.\nAnd then we have message output entry, and interestingly, its content doesn't involve a single code, a a a single text, but a chunks of text like text in in the form of text chunk classes and also tool reference chunk.\nLet's take a closer look at this object uh, by printing by printing it here.\nAs you can see, it's an array of text chunk and tool reference chunk objects.\nLet's take a closer look.\nWhen the object is a text chunk, it is a re, it's a regular text.\nIt starts like in 2025.\nThe transformer actor architecture continues to evolve.\nBut as you can see, the text is cut off short by the next tool reference track, which contains a citation object.\nA citation means the agent is uh, trying to give us information of on where it got the following text from.\nAs you can see, this tool reference chunk has the title of the web page that it scraped and also the URL of the web page.\nOur job here is to concatenate, to combine all of this information into a single response.\nUnfortunately, this is not provided by the Agents API uh, by default.\nSo, we have to do the formatting ourselves, and we're doing that right here.\nFirst of all, we are iterating through that tool reference chunks and text chunks for result in search response.putscontent.\nWe're checking if the item has the text attribute.\nIf it has the text attribute, it means the item is just text.\nSo, we can add it to the markdown text right here.\nIf it has the tool attribute, it means the agent is giving a citation.\nSo, we're going to append it to the markdown text in a citation format using the markdown link syntax, which means links are given using square and round brackets.\nIn the first square bracket, we're extracting the uh, citations title.\nIn the second rounded bracket, we're extracting the citations URL and appending it to the full markdown text right here.\nAfter the iteration is over, we are going to have a single agent response which we can print using the markdown display function of the Jupyter notebook.\nLet's run this cell.\nAnd as you can see, we have a complete and beautifully formatted markdown text with built-in citations, which is provided by the tool execution or by the web search tool.\nAs you can see, every chunk of text is followed by a citation that gives us information of where the agent is taking its information from.\nAgain, uh, we can continue the conversation using the append method.\nAnd here we're asking the agent to summarize the results in five sentences.\nLet's run it and see the output.\nThere you go.\nUh, we have five sentences that summarizes the news in the transformer architecture landscape.\nUh, you can pause the video here to read the response on your own and get up to speed on transformer news.\nAll right, let's now talk about code execution for solving complex tasks that requires using Python code.\nUsing the code interpreter tool follows the same syntax as a web search.\nHere we're creating coding assistant with access to the code interpreter tool, which is defined in in a dictionary format once again.\nAnd this time we're giving it a more complex problem to visualize the mandel broad set using the equation z= z ^2 + c, and it should calculate the convergence for a complex plane grid and create a beautiful fractal visualization wall art.\nIf you don't know what mandelet is, it doesn't matter.\nUh, what matters is that we're now asking something that the LLMs can't do, which is calculating complex math equations by default.\nIt's going to be forced to use the the new code interpreter tool that we have defined right here.\nSo let's run this and you'll notice that the uh, request is going to take much longer than simple web search or simple questions that we have been asking from the agent so far.\nLet's wait for it to finish.\nAnd it's probably going to take more than uh, 60 seconds.\nAs you can see, the cell finished execution.\nLet's print its outputs.\nAnd we're going to see that uh, the outputs array this time is even larger than the web search version.\nSo let's take a look at the second element of this outputs array, which is another tool execution, but this time it's a code interpreter one.\nUh, let's take a closer look by printing its contents.\nAnd as you can see, uh, we have the name as code interpreter.\nAnd interestingly, we also have another dictionary called info, which contains a key called code, which contains the code for generating the visualization that we asked in the prompt.\nWe can extract the code into a single string by using this syntax, which uh, again extracts the tool execution class and extracts its info attribute and then extracts the code and then we print it.\nAs you can see, the code uses only numpy and mattplot li to create and to create and plot the mandelroad fractal set.\nThis is only the coding part of the equation.\nUh, we also want to extract the plot that was generated after the mistral servers ran this code on their own, and this can be done by using another outputs array content, and let's uh, let's see it, which is going to be a file chunk object.\nUh, as you can see, this tool file chunk class is was generated by the code interpreter, and it has a file ID and also file name and the file format, which is a PNG file.\nWe're interested in this file ID.\nSo let's extract it.\nAnd now using this ID, we can download the file that was generated by using the client.files download method, which downloads the file in B 64 format.\nAnd when we use the read function on top of it, we're going to get the raw bytes information of the file, which means we can use that raw file and save it as a PNG file right here.\nLet's run this cell.\nAnd then I already have the markdown cell uh, for displaying that image.\nLet's run it once again to see the updated image.\nAnd as you can see, the image is the same as before, uh, and it contains the visualization of the beautiful Mandelro fractal art.\nSo that right there was uh, the end of our brief tutorial.\nWe've covered the basics and some intermediate concepts of the Mistral Agents API.\nI have left some links here.\nSo, be sure to check out the entire documentation of the Agents API and then explore other tools like document search and image generation, which are also very cool use cases for the Agents API.\nAnd then I definitely learned how to use function calling that allows your agents to use plain Python functions for connecting to external services or APIs like databases or custom APIs for even more functionality beyond just code interpretation or web search.\nAnd then also the Agents API supports building multi-agent systems where you can combine multiple agents and make them coordinate to solve complex tasks using different coordination patterns.\nSo thank you for watching this tutorial and I'll see you in the next one.\n",
  "dumpedAt": "2025-07-21T18:43:25.347Z"
}