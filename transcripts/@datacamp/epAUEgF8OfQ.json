{
  "episodeId": "epAUEgF8OfQ",
  "channelSlug": "@datacamp",
  "title": "#305 RAG 2.0 and The New Era of RAG Agents | Douwe Kiela, CEO at Contextual AI | Inventor of RAG",
  "publishedAt": "2025-06-09T10:07:03.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Welcome to dataf framed. This is Richie.",
      "offset": 0.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Retrieve augmented generation or rag to",
      "offset": 2.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "its friends is a popular technique for",
      "offset": 4.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "retrieving supporting information to",
      "offset": 6.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "include in your prompt. This is supposed",
      "offset": 8.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to reduce the hallucination problem that",
      "offset": 9.92,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "LLM suffer. For all its success, I quite",
      "offset": 12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "regularly see people proclaiming its",
      "offset": 14.799,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "imminent death in favor of something",
      "offset": 16.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "more sophisticated. Today, I want to",
      "offset": 18.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "know whether the reports of rag's death",
      "offset": 20.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are greatly exaggerated or not. Our",
      "offset": 22.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "guest is Da Keela, who was part of the",
      "offset": 24.96,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "team at Meta that invented rag. These",
      "offset": 27.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "days, he's CEO of Contextual AI, a",
      "offset": 29.519,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "platform for creating rag agents and",
      "offset": 32.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "custom LLMs known as contextual language",
      "offset": 34.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "models. All right, let's find out about",
      "offset": 37.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the fate of rag. A lot of rag systems",
      "offset": 39.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like 2 years ago were what we called",
      "offset": 41.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like Frankenstein's rag. All of these",
      "offset": 43.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "parts are kind of cobbled together, but",
      "offset": 45.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "they're not really designed to work well",
      "offset": 47.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "together. In our case, all of these",
      "offset": 49.039,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "components are designed to be",
      "offset": 50.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "state-of-the-art, and they're also",
      "offset": 51.879,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "designed to work well together. I think",
      "offset": 53.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the a much more useful definition of an",
      "offset": 55.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "agent is just something that actively",
      "offset": 57.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "reasons. The really exciting technology",
      "offset": 59.359,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that that has enabled all of this is",
      "offset": 62,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "just test time reasoning and the insight",
      "offset": 64.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that shifting the compute from the",
      "offset": 65.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "training side to the test time inference",
      "offset": 68.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "side um actually has very very nice uh",
      "offset": 70.24,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "properties.",
      "offset": 73.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Hi DA, welcome to the show. Hi, thanks",
      "offset": 77.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "for having me. Cool. So, I've been",
      "offset": 80.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "hearing a lot in the last year about",
      "offset": 82.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "people predicting the death of retrieval",
      "offset": 84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "augmented generation. Brag, why do you",
      "offset": 86.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "think people keep thinking this is going",
      "offset": 88.88,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "to happen? Yeah. And that that usually",
      "offset": 90.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "happens with with good simple ideas",
      "offset": 94.479,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "where where people are trying to sort of",
      "offset": 96.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "um rewrite history maybe from a",
      "offset": 98.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "marketing perspective a little bit. And",
      "offset": 100.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "rag is such a simple idea uh that I",
      "offset": 103.36,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "think it's a little bit silly to declare",
      "offset": 106.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "it. Um, so, um, I I guess everybody",
      "offset": 108.799,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "listening in here knows what RAG stands",
      "offset": 112.56,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "for, right? Retrieval augmented",
      "offset": 114.479,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "generation. And so the G is just any Gen",
      "offset": 116.04,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "AI model and then you want to make that",
      "offset": 119.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "model work on your data, which means you",
      "offset": 122.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "need to augment it with your data. And",
      "offset": 124.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the way to do that is through some form",
      "offset": 126.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "of retrieval. So it's such a kind of",
      "offset": 128.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "generic paradigm that it doesn't really",
      "offset": 131.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "make sense to to pronounce it that I",
      "offset": 133.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "think. But from a marketing perspective,",
      "offset": 136.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "a lot of people keep saying, &quot;Oh, you",
      "offset": 138.319,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "you know, you don't need rag, you need",
      "offset": 139.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "fine-tuning, or you don't need rag, you",
      "offset": 141.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "need um long context windows.&quot; Um, and",
      "offset": 143.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "yeah, I mean, we can we can go into the",
      "offset": 146.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "specifics of each of those. Um, but uh,",
      "offset": 148.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "yeah, I think these are mostly marketing",
      "offset": 151.599,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tricks. Okay, that's good to know. So I",
      "offset": 154.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "suppose it feels a bit like linear",
      "offset": 156.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "regression is very simple and I mean",
      "offset": 158.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's been around for like almost a",
      "offset": 160.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "century now and people still use it",
      "offset": 162.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because it's simple even though there",
      "offset": 164.239,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "are more sophisticated models out there",
      "offset": 165.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "now. Yeah, exactly. Yeah, we even bought",
      "offset": 167.519,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "a domain name is rank dead yet.com uh",
      "offset": 170.72,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "where we can point people to a blog post",
      "offset": 174.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "about uh about whether rag is dead or",
      "offset": 176.879,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "not. Yeah, I mean so these are the types",
      "offset": 179.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of things that you cannot really declare",
      "offset": 181.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "dead I think. Uh but I guess I'm biased.",
      "offset": 183.68,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "Okay. Uh there are any problems though?",
      "offset": 187.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Are there any limitations to this? Oh,",
      "offset": 189.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "absolutely. Uh so so I I I think if you",
      "offset": 192.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "look at agents, right, agents in",
      "offset": 195.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "general, uh retrievo is just one of the",
      "offset": 197.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "tools in the toolbox of an agent. Um and",
      "offset": 200,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "so um I think it's it's very true that",
      "offset": 203.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "we should not just have rag as the only",
      "offset": 207.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "thing that these agents uh can do. But",
      "offset": 210,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "yeah, that feels a little obvious,",
      "offset": 212.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "right? So, um, in terms of of the",
      "offset": 214.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "current limitation still of rag, I think",
      "offset": 216.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "part of the solution there is actually",
      "offset": 219.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "long context, right? So, if a retrieval",
      "offset": 221.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "system is imperfect, then ideally you",
      "offset": 224.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "want to cast a wider net. So, have more",
      "offset": 226.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "information that you can put into the",
      "offset": 229.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "context of the language model in the",
      "offset": 232,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "hope that there's something useful in",
      "offset": 233.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "there. Um, and then um uh that's that's",
      "offset": 235.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "where you need a longer context. That",
      "offset": 239.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that's why I always talk about these",
      "offset": 240.799,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "dichotoies as sort of being false,",
      "offset": 242.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right? It's like you need both. It's",
      "offset": 243.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "like you're not going to put the entire",
      "offset": 246.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "like internet in the context of your",
      "offset": 248.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "language model. So that's why you are",
      "offset": 250.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "going to do some search. So rag",
      "offset": 252.159,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "basically, but then you want to ideally",
      "offset": 254,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "put lots of search results in the",
      "offset": 256.479,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "context of the language model so that it",
      "offset": 258,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "can do its job and and answer the",
      "offset": 259.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "question correctly, right? So uh yeah,",
      "offset": 262.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we need all of those things. Okay, so",
      "offset": 264.56,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "just to make sure I've understood this",
      "offset": 266.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "correctly, the whole point of rag is",
      "offset": 268.84,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "that you're going to reduce",
      "offset": 271.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "hallucinations by just getting or just",
      "offset": 272.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "retrieving relevant information and",
      "offset": 274.96,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "putting that inside your prompt and then",
      "offset": 277.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that's going to help the large language",
      "offset": 279.759,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "model give you the correct answer. So if",
      "offset": 281.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you have a longer context window, you",
      "offset": 284.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "can put more information in there that's",
      "offset": 286,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "going to assist it. And so the idea of",
      "offset": 287.919,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "long context windows and rag they sound",
      "offset": 291.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "like they're competing but actually",
      "offset": 293.68,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "they're kind of complimentary. Is that",
      "offset": 295.759,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "right? Very very complimentary. Yes.",
      "offset": 298.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Okay. Wonderful. So I know in the",
      "offset": 301.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "contextual blog you've introduced the",
      "offset": 304,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "idea of rag 2.0. So can you tell me what",
      "offset": 306.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "rag 2.0 is and how is it different from",
      "offset": 309.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the original rag? Yeah. So, so the way",
      "offset": 312.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we do rag is is uh a bit different I",
      "offset": 315.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "guess from what everybody else is doing.",
      "offset": 318.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "And so we started the company when we",
      "offset": 320.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "saw the world get very excited after",
      "offset": 322.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "chat GPT and then get very frustrated uh",
      "offset": 325.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "because the technology wasn't quite",
      "offset": 328.56,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "ready and that that was especially true",
      "offset": 330.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "for uh enterprise use cases. So um we",
      "offset": 331.919,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "knew that rag would be part of the",
      "offset": 336.08,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "answer obviously given what we knew",
      "offset": 337.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "about rag. We also knew that rag was",
      "offset": 339.759,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just the first idea there and that and",
      "offset": 341.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "that uh even in the rag paper we talk",
      "offset": 343.759,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "about sort of what the the vision is and",
      "offset": 346.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "what we really want to achieve which is",
      "offset": 348.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that these components are designed to",
      "offset": 350.4,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "work together right so so a lot of um um",
      "offset": 352.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "rag systems like two years ago were what",
      "offset": 356.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we called like Frankenstein's rag where",
      "offset": 358.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it's all of these parts are kind of",
      "offset": 361.039,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "coupled together but they're not really",
      "offset": 362.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "designed to work well together. So uh",
      "offset": 364.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "our rag 2.0 I know approach is about",
      "offset": 367.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "making sure that all of the components",
      "offset": 369.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of a modern RAD pipeline. So that's not",
      "offset": 371.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "just a vector database and a language",
      "offset": 373.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "model anymore. It's much more",
      "offset": 375.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "complicated as a system. In our case,",
      "offset": 376.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "all of these components are designed to",
      "offset": 379.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "be state-of-the-art and they're also",
      "offset": 380.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "designed to work well together. And you",
      "offset": 382.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "can do that through uh training on the",
      "offset": 385.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "same uh data distribution essentially so",
      "offset": 387.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that all the parts are designed to work",
      "offset": 389.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "well together. They're literally like",
      "offset": 392.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "trained to work well together. And so uh",
      "offset": 394.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that combination of having very good",
      "offset": 396.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "components and then having a very good",
      "offset": 399.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "compound um that that makes our our",
      "offset": 400.96,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "system much better at rag than than",
      "offset": 404.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "anybody else. Okay. So in theory that",
      "offset": 406.479,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "sounds like a very useful idea for the",
      "offset": 409.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "idea that the data in your database and",
      "offset": 411.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the data in the large language model",
      "offset": 414.56,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "they're kind of harmonized in some way.",
      "offset": 416.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "You got all the components working",
      "offset": 419.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "together. Um do you have a sense of when",
      "offset": 420.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this might be useful and what sort of",
      "offset": 423.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "benefits you might get from it? Uh it's",
      "offset": 425.039,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "always",
      "offset": 427.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "useful. I mean uh it's it's the use",
      "offset": 428.599,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "cases we are focused on as a company. Uh",
      "offset": 432,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "they they tend to be high stakes use",
      "offset": 434.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "cases with a low tolerance for mistakes",
      "offset": 437.28,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "and a and sort of high uh high accuracy",
      "offset": 439.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "requirements where you want to have very",
      "offset": 442.68,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "accurate attributions. So very often",
      "offset": 444.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this is in regulated industries and then",
      "offset": 446.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "you want to um um you know where you're",
      "offset": 449.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "also sensitive about your data and if",
      "offset": 451.919,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "you have those characteristics then then",
      "offset": 454.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "uh we are much better than than anything",
      "offset": 456.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "else out there. Okay. Yeah. So I can see",
      "offset": 459.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "if you got a high requirement that the",
      "offset": 461.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "answer you're giving is correct in some",
      "offset": 462.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "way. You need to put more effort into",
      "offset": 465.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "this. Uh do you have any examples maybe",
      "offset": 467.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "from one of your customers of things",
      "offset": 470.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that have been built using this",
      "offset": 472,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "approach? Yeah. So, so one one use case",
      "offset": 473.36,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "I'm very proud of is the work we've been",
      "offset": 476.56,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "doing with Qualcomm. Um, so um their",
      "offset": 478.879,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "customer engineering department is using",
      "offset": 482.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "us where their only genai deployment at",
      "offset": 484.479,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "scale as far as I know. Uh where um yeah",
      "offset": 487.039,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "we these are thousands of engineers who",
      "offset": 490.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "are using us uh on a daily basis to",
      "offset": 492.8,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "answer very complicated questions. So",
      "offset": 496.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "these are not the simple kind of",
      "offset": 498.479,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "questions where you have like your",
      "offset": 500,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "internal search and maybe it's like you",
      "offset": 501.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "know something like glean and you would",
      "offset": 503.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "ask it like who is our 401k provider or",
      "offset": 504.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "how many vacation days do I get right um",
      "offset": 507.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "that that's not really where the the ROI",
      "offset": 510.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of AI is going to come from you want to",
      "offset": 512.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "focus on these much more uh expert uh",
      "offset": 514.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "kind of specialist knowledge worker use",
      "offset": 517.599,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "cases and and uh so that's what we did",
      "offset": 519.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "with Qualcomm and we can answer very",
      "offset": 522.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "complicated questions where I don't",
      "offset": 524.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "understand what the question means",
      "offset": 526.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "uh and definitely don't understand what",
      "offset": 528,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the answer means, but the system does a",
      "offset": 529.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "really good job at uh explaining the the",
      "offset": 531.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "information and giving the right answer.",
      "offset": 534.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Okay. Yeah. So, if you got a simple true",
      "offset": 536.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "or false or give me a number, retrieve",
      "offset": 538.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "one simple fact kind of situation, then",
      "offset": 541.6,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "rag 2.0 is maybe overkill. Is that",
      "offset": 544.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "right? But if you got a more complicated",
      "offset": 546.36,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "question, then that's when it's going to",
      "offset": 548.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "shine. It's not really overkill. It's",
      "offset": 550.399,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "more that um you'll So, the rag 2.0 know",
      "offset": 552.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh uh happens during training, right?",
      "offset": 556.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Not during inference. So during",
      "offset": 558.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "inference, it's still a normal rag",
      "offset": 560.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "system, but just a really really good",
      "offset": 562.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "one. Um so it it doesn't um make things",
      "offset": 564.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "more complicated for you. Uh if",
      "offset": 567.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "anything, it makes everything easier",
      "offset": 569.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "because we offer it in one platform",
      "offset": 571.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "where you can build these agents really",
      "offset": 573.12,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "in 10 seconds. Uh I I can build a a",
      "offset": 574.72,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "state-of-the-art rag agent a agent in in",
      "offset": 578.519,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "10 seconds, which is I think pretty",
      "offset": 581.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "powerful. 10 seconds. Okay. Well, I'm",
      "offset": 583.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that's pretty short. I mean, it's about",
      "offset": 585.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "as short a development time as you can",
      "offset": 588.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "get. Yeah. So, talk me through uh what",
      "offset": 590.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "do you do then if you're going to build",
      "offset": 593.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "something in 10 seconds.",
      "offset": 594.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So, uh we had these two concepts uh of",
      "offset": 597.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "agents and data stores in our platform.",
      "offset": 599.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "And so what you would do is you would",
      "offset": 602,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "create a data store. Um and then um you",
      "offset": 603.519,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "would tell the agent that that is the",
      "offset": 607.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "data store that it has to work on top of",
      "offset": 609.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and you put some files or a database uh",
      "offset": 611.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "into that data store and now you can",
      "offset": 614.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "talk to that data and do rag on top of",
      "offset": 616.48,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "that data with a a state-of-the-art rag",
      "offset": 618.959,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "system. Okay, that sounds very",
      "offset": 621.399,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "straightforward. Just copy some files",
      "offset": 624,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "into a database and say okay go look at",
      "offset": 625.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "them. So tell me something about what's",
      "offset": 628.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "in there.",
      "offset": 630.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Um, all right. Nice. Um, and if you're",
      "offset": 631.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "going to look at one of these very",
      "offset": 634.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "strict use cases that you mentioned, uh,",
      "offset": 636.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that one of these regulated examples, do",
      "offset": 639.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "you need some sort of tuning of that?",
      "offset": 641.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Um, like some sort of optimization to",
      "offset": 643.839,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "make sure it all works properly. Yeah.",
      "offset": 647.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "So, you can do that. So, our hope is",
      "offset": 648.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "that out of the box, our performance is",
      "offset": 651.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "already good enough, which turns out to",
      "offset": 653.2,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "be the case uh, for many use cases. But",
      "offset": 655.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "um yeah, if you if you work in finance",
      "offset": 658.44,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "for example, then you really want to uh",
      "offset": 661.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh uh get maximum performance and so",
      "offset": 664.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that means that you need to really",
      "offset": 666.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "specialize for the use case and you can",
      "offset": 667.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "do that through our platform. So we",
      "offset": 669.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "allow you to tune not just the language",
      "offset": 672.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model or just the retriever, we allow",
      "offset": 674.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you to tune the entire rag pipeline for",
      "offset": 676.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "your specific problem. Uh, and so often",
      "offset": 679.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that that leads to uh to pretty",
      "offset": 682,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "substantial uh performance improvements",
      "offset": 684.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "on top of already state-of-the-art out",
      "offset": 685.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of the box performance. Okay. All right.",
      "offset": 687.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "So, it sounds like a lot of the secret",
      "offset": 690,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to this is around the fine-tuning steps",
      "offset": 692.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and the other steps around making sure",
      "offset": 695.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the model quality is high. Yeah, some of",
      "offset": 697.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it is. So, it's also just about having",
      "offset": 699.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "very good components, right? So, for for",
      "offset": 702.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "uh folks listening in on the podcast,",
      "offset": 704.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "they can also use these components",
      "offset": 706.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "themselves. We've made them available",
      "offset": 708,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "independently. So if you uh want to have",
      "offset": 709.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "a grounded language model that is",
      "offset": 712.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "designed for rag and nothing else, so",
      "offset": 714.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's really specialized for rag, then",
      "offset": 716.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you can use that. If you want to use a",
      "offset": 718.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "reranker um uh that is state-of-the-art",
      "offset": 720.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and that can follow instructions,",
      "offset": 723.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "um you can also do that, right? And so",
      "offset": 725.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it's the same for our retrieval pipeline",
      "offset": 728,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and for our uh document intelligence",
      "offset": 729.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "pipeline. So, all of the components that",
      "offset": 731.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "make up our state-of-the-art uh uh rag",
      "offset": 734,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "system, they're they're available on",
      "offset": 737.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "their own as well. Uh because we uh just",
      "offset": 739.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like seeing what people build with them.",
      "offset": 742.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Interesting. Uh I'm curious as to how",
      "offset": 744.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you measure what the benefits are. It",
      "offset": 746.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "seems like it's quite tricky to work out",
      "offset": 748.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "um what the quality of the responses are",
      "offset": 751.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "in general. So, how do you go about",
      "offset": 753.839,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "doing this? Yeah, evaluation is such an",
      "offset": 756,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "important area and and so",
      "offset": 759.279,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "underappreciated and so we we do it in a",
      "offset": 761.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "variety of different ways. I mean",
      "offset": 765.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "obviously we look at kind of standard",
      "offset": 766.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "benchmarks, right? And so like rag QA",
      "offset": 768.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "arena is a benchmark and and we are",
      "offset": 770.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "state-of-the-art on that and so that's",
      "offset": 773.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "what we um care about too. Uh but that",
      "offset": 774.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "doesn't even necessarily translate to",
      "offset": 778.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "real world performance. Uh so we we look",
      "offset": 780.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "much more closely at our our uh actual",
      "offset": 783.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "customer data sets and um sort of the",
      "offset": 785.519,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "process of UAT user acceptability",
      "offset": 788.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "testing. So do people actually consider",
      "offset": 790.959,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "this answer to be correct and useful? Um",
      "offset": 793.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and that's ultimately what you care",
      "offset": 796.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "about, right? That's where the the ROI",
      "offset": 797.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "comes from. Um so that's one thing. I I",
      "offset": 799.519,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "think the other thing that's maybe worth",
      "offset": 802.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "uh mentioning here is that we have this",
      "offset": 804.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this framework around natural language",
      "offset": 807.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "unit testing. um where um you you're",
      "offset": 809.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "familiar with unit testing, right? It's",
      "offset": 813.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it's the what you do with code is like",
      "offset": 815.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "just step by step measuring small small",
      "offset": 817.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "things about your code, small units and",
      "offset": 820.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and then making sure that they are",
      "offset": 823.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "correct and and you can use the same",
      "offset": 825.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "idea for language model responses where",
      "offset": 828.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "you can delineate very precisely what",
      "offset": 831.36,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "the characteristics are of a good",
      "offset": 834.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "answer. So, so you could say okay like",
      "offset": 835.959,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "it needs to mention this thing first.",
      "offset": 838.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that needs to be in this particular",
      "offset": 840.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "style. It can absolutely not talk about",
      "offset": 842,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this thing and you know whatever the",
      "offset": 844.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "characteristics are of the answer you",
      "offset": 847.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "can write specific unit tests for. And",
      "offset": 848.639,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "so doing that gives you much much richer",
      "offset": 851.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "signal than just looking at what",
      "offset": 853.959,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "everybody else is doing now with LLM as",
      "offset": 856.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "a judge models which is just like is",
      "offset": 858.16,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "this generated response equivalent to my",
      "offset": 860.399,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "ground truth response? And so maybe that",
      "offset": 863.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "makes sense if you're generating a few",
      "offset": 866.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "sentences, but if you're generating a",
      "offset": 867.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "long answer, then that's just not going",
      "offset": 869.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "to cut it. Interesting. So, uh, you want",
      "offset": 871.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to be able to ask specific questions",
      "offset": 873.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "about the response and just check does",
      "offset": 876.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "it match all of these different criteria",
      "offset": 879.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and it can do pass or fail for lots of",
      "offset": 882,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "different things then. Exactly. But",
      "offset": 885.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that's really great too, right? Because",
      "offset": 887.279,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "if you think about like a regulated",
      "offset": 888.639,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "industry",
      "offset": 890.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "um the you have you will have people in",
      "offset": 891.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "like the the model risk management",
      "offset": 894.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "department of the bank and they need to",
      "offset": 896.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "write out what a good answer looks like",
      "offset": 898.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and what cannot be uh a part of any",
      "offset": 901.519,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "answer right so if they can just write",
      "offset": 904.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "those unit tests in natural language and",
      "offset": 906.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "then the system can test against that",
      "offset": 908.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for every generation uh that's something",
      "offset": 910.56,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "that that you that will make regulators",
      "offset": 913.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "much happier basically basically yeah I",
      "offset": 915.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Certainly from my experience of creating",
      "offset": 917.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "software uh when you write the test it",
      "offset": 919.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "really helps clarify what you actually",
      "offset": 922.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "want the software to do. I'm sure it's",
      "offset": 923.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the same situation with your large",
      "offset": 926,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "language models. Yeah, test driven",
      "offset": 928,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "development. Yeah, absolutely. Cool. So",
      "offset": 930.56,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "I guess related to this is the idea of",
      "offset": 933.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "maintenance. So if your data is changing",
      "offset": 937.8,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "very rapidly, how do you go about",
      "offset": 940.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "maintaining your model just to make sure",
      "offset": 942.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "it's continually giving the right",
      "offset": 944.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "results?",
      "offset": 945.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Yeah. Uh, so I mean what rag is useful",
      "offset": 947.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "for is making sure that things will keep",
      "offset": 951.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "working on your data even as the data",
      "offset": 954,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "changes, right? That's part of part of",
      "offset": 956.16,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the the appeal of rag. Um, so um I I I",
      "offset": 957.92,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "think you you kind of do that",
      "offset": 962.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "automatically there. But one of the",
      "offset": 964.079,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "things that we're always thinking about",
      "offset": 965.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is how can we make sure that we have the",
      "offset": 967.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "best components and continuously keep",
      "offset": 968.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "updating the best components to make",
      "offset": 971.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sure that they're integrated into the",
      "offset": 972.959,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "the the overall pipeline. Um and and so",
      "offset": 974.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "that actually takes a lot of effort,",
      "offset": 978.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "right? Make just staying at the frontier",
      "offset": 980.399,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "of AI which is moving so quickly. Um",
      "offset": 983.279,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "that is is a lot of work. Um and and so",
      "offset": 986.56,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "um yeah that's that's exciting to that",
      "offset": 990,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we get to do uh things like that right",
      "offset": 992.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "as a broader kind of research community",
      "offset": 994.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "research community. Okay. So in theory",
      "offset": 996.399,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you shouldn't have to do too much",
      "offset": 998.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "maintenance as long as the systems in",
      "offset": 999.759,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "place and you test it once. So even if",
      "offset": 1002.399,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "your data changes it's still going to",
      "offset": 1005.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "work. Yeah. I actually I so I was at",
      "offset": 1008.24,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "this uh I was at this this Gartner",
      "offset": 1011.68,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "conference and it was data and analytics",
      "offset": 1014.92,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "and and so everybody at the Gartner",
      "offset": 1018.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "conference and this is really not my",
      "offset": 1020.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "crowd to be honest. these are are not",
      "offset": 1022.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "not like uh you know AI uh people they",
      "offset": 1024.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "are like real enterprise people and it",
      "offset": 1027.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "was really eye openening for me to just",
      "offset": 1029.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "be there and talk to all these amazing",
      "offset": 1030.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "people and get their perspectives which",
      "offset": 1032.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "were very different from what I've been",
      "offset": 1034.16,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "hearing everywhere and and the whole",
      "offset": 1036.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "conference was just talking about uh",
      "offset": 1037.679,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "making your data ready for AI and and I",
      "offset": 1040.799,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "just felt like that's a massive copout",
      "offset": 1044.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "right that you should not have to make",
      "offset": 1047.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "your data ready for AI you should make",
      "offset": 1048.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "your AI ready for your data",
      "offset": 1051.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Right. And and so so that in an ideal",
      "offset": 1054.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "world and I think where we're headed is",
      "offset": 1058.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that you don't have to do anything to",
      "offset": 1060.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "your data to make it work. You just have",
      "offset": 1062,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "very good AI that works on top of that",
      "offset": 1063.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "data and you and we just have to accept",
      "offset": 1065.52,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "that data is noisy. And so that's why we",
      "offset": 1068.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "have like this multi-stage retrieval",
      "offset": 1071.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "pipeline. That's why we have this",
      "offset": 1073.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "powerful re-ranker that can follow",
      "offset": 1074.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "instructions. that that's exactly",
      "offset": 1076.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "because you need to make sense of noisy",
      "offset": 1078.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "data and filter out the things that you",
      "offset": 1079.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "don't want and make sure you get the",
      "offset": 1081.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "things that you do want into the",
      "offset": 1083.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "language model. Um, so uh yeah, in an",
      "offset": 1084.64,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "ideal world in the long run I I can't",
      "offset": 1088.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "promise it now yet, but in the long run",
      "offset": 1089.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you want to make sure that AI just works",
      "offset": 1092.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "on your data and you don't have to make",
      "offset": 1094.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "your data AI ready. Okay, I think I just",
      "offset": 1096.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "heard everyone listening in who works in",
      "offset": 1098.32,
      "duration": 7.239
    },
    {
      "lang": "en",
      "text": "data governance just gasp all at once.",
      "offset": 1100.16,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "uh just just wait don't waste your time",
      "offset": 1105.559,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "preparing for uh this work in in the",
      "offset": 1108.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "near future. Okay. So do you have any",
      "offset": 1111.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "advice for people who do work in data",
      "offset": 1113.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "governance then? So if your company's",
      "offset": 1115.52,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "making applications involves search or",
      "offset": 1118,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "retrieval, what do data governance",
      "offset": 1120.679,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "people need to do? Be very careful about",
      "offset": 1122.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "your metadata. Um, so I I think that's",
      "offset": 1125.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "really one of the crucial parts of of",
      "offset": 1128.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "state-of-the-art modern rag pipelines is",
      "offset": 1130.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "making sure that you have high quality",
      "offset": 1132.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "metadata or annotations for your",
      "offset": 1134.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "documents and and uh database schemas or",
      "offset": 1136.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "things like that. Um, so that that is is",
      "offset": 1139.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "definitely still going to matter having",
      "offset": 1142.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "rich metadata. Um and so yeah the other",
      "offset": 1144.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "thing is is really about I think",
      "offset": 1148.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "thinking carefully through like your",
      "offset": 1150.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "entitlements model especially when you",
      "offset": 1152.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have multiple data sources that might be",
      "offset": 1154.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "very desperate. So you you know we",
      "offset": 1156.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "talked to a lot of companies that have",
      "offset": 1159.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like SharePoint and Confluence and like",
      "offset": 1160.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "Jura and Slack and like Google Drive and",
      "offset": 1162.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "all kinds of different things and then",
      "offset": 1165.919,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "they want to work on top of all of",
      "offset": 1167.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "those. Um but so getting that to",
      "offset": 1168.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "actually work with a with a proper sort",
      "offset": 1171.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "of role-based access control",
      "offset": 1173.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "entitlements uh setup is is not trivial",
      "offset": 1175.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "at all. So that's one of the things that",
      "offset": 1179.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "that we are uh are good at. But uh if",
      "offset": 1181.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "you if you have a more centralized way",
      "offset": 1184.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "of dealing with that from a data",
      "offset": 1187.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "governance perspective uh then you can",
      "offset": 1188.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "save yourself a big headache. Okay.",
      "offset": 1191.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah. So certainly I can see how lineage",
      "offset": 1193.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is incredibly important. Do you want to",
      "offset": 1195.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "go into a bit more about the",
      "offset": 1198,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "entitlements? Uh it sounds as if",
      "offset": 1199.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "different people have got different",
      "offset": 1201.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "permissions on data and it makes",
      "offset": 1202.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "retrieval a bit complicated.",
      "offset": 1204.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yeah. So if you think about like real",
      "offset": 1207.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "world companies with like 100,000",
      "offset": 1209.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "employees and not everybody has access",
      "offset": 1212.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to the same amount of data um and or so",
      "offset": 1214.24,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "right some some like companies actually",
      "offset": 1218,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "from a regulatory perspective have sort",
      "offset": 1221.039,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "of hard fencing between departments. Um",
      "offset": 1222.88,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "yeah so you need to make sure that you",
      "offset": 1227.039,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "capture that and that you don't make any",
      "offset": 1228.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "mistakes. Um and so uh yeah, setting",
      "offset": 1230.679,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "things up the right way for handling",
      "offset": 1233.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "that at the scale of 100,000 people in a",
      "offset": 1235.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "large company. Um that's a that's a very",
      "offset": 1238.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "interesting problem. Sure. Yeah. So in",
      "offset": 1241.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that case, I guess you want the model to",
      "offset": 1243.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "give different responses depending on",
      "offset": 1245.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "who is asking the question because",
      "offset": 1247.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "they're going to see different data.",
      "offset": 1249.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Yeah. So like I always enjoy saying",
      "offset": 1252.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "everything is",
      "offset": 1255.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "contextual and and so you want to get",
      "offset": 1256.76,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "that context into the the model and so",
      "offset": 1259.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for every user and every user",
      "offset": 1261.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "interaction even um the the context",
      "offset": 1264,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "should be different. Okay, nice. So I",
      "offset": 1267.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "know handling permissions has been",
      "offset": 1269.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "around forever in relational databases.",
      "offset": 1271.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I don't know what the status is with",
      "offset": 1274.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "vector databases. So, is it easy to",
      "offset": 1276.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "manage who gets access to what data and",
      "offset": 1279.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like is it as sophisticated as with",
      "offset": 1282.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "relational databases? No, it's not it's",
      "offset": 1284.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "not nearly as sophisticated. Uh it's",
      "offset": 1287.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "getting better, but it's not not as",
      "offset": 1290.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "sophisticated. And even with relational",
      "offset": 1291.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "databases, I I I think it's actually not",
      "offset": 1293.919,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "that that easy. Um so you can do it in",
      "offset": 1296.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "like one one sort of single uh database",
      "offset": 1299.84,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "but doing it across databases um uh is",
      "offset": 1303.2,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "is also not easy. Okay. So is there a",
      "offset": 1306.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "solution to this then? Yeah. So the the",
      "offset": 1309.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "way we do it is we we synchronize the",
      "offset": 1312,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "the data with our um sort of data store",
      "offset": 1314.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "concept which we talked about right and",
      "offset": 1318.559,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "then um when we do the retrieval step we",
      "offset": 1320.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "validate using an entitlements API that",
      "offset": 1324.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sort of calls all the upstream APIs to",
      "offset": 1326.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to make sure that the that you still",
      "offset": 1328.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "have access. Um and so that obviously",
      "offset": 1331.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "cannot always happen in real time",
      "offset": 1334.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "depending on your latency constraints",
      "offset": 1335.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "but there's some sort of um",
      "offset": 1337.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "synchronization uh step that happens in",
      "offset": 1339.64,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "between there and so yeah so the model",
      "offset": 1342.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is basically like you ingest you do the",
      "offset": 1344.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "retrieval over that index kind of a",
      "offset": 1347.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "complicated index um and then uh when",
      "offset": 1349.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "you find the results you validate that",
      "offset": 1353.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the results you found are actually",
      "offset": 1355.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "accessible by the user using an",
      "offset": 1356.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "entitlements API. All right. So, if it",
      "offset": 1358.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "sounds like one of those problems where",
      "offset": 1360.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it's going to be much better if someone",
      "offset": 1362.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "else solves it for you, like it doesn't",
      "offset": 1363.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "sound like the sort of fun thing you",
      "offset": 1365.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "want to play around with yourself.",
      "offset": 1367.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "That's what I've been trying to tell",
      "offset": 1369.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "people. Stop trying to DIY complex rag",
      "offset": 1370.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "systems. It's not worth your time. Just",
      "offset": 1374.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like you wouldn't build your own",
      "offset": 1376.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "database or your own language model,",
      "offset": 1377.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "like you shouldn't build your own rag",
      "offset": 1380.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "platform, you should be building rag",
      "offset": 1383.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "agents and applications on top of that",
      "offset": 1385.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "platform to solve uh important business",
      "offset": 1387.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "problems for your business. Okay. Yeah.",
      "offset": 1389.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "So I mean that seems incredibly",
      "offset": 1392.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "important. Uh so it's like try and",
      "offset": 1393.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "figure out where you can add the most",
      "offset": 1396.48,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "value to solve problems to your",
      "offset": 1398.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "business. Uh can you just talk me",
      "offset": 1400.2,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "through how do you go about thinking",
      "offset": 1402.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "about this? Where should you be spending",
      "offset": 1404.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "your time? Do you have any examples?",
      "offset": 1405.919,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "Yeah. So, so",
      "offset": 1408.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the differentiated value for a company",
      "offset": 1409.88,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "is what companies are all about, right?",
      "offset": 1412.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Like your company wants to be better",
      "offset": 1415.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "than your competition. That that's sort",
      "offset": 1416.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "of the goal of every company. And and so",
      "offset": 1418.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the more you can focus on that um and",
      "offset": 1421.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "thinking about okay, how can we um uh",
      "offset": 1424.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "use our data and how can we automate",
      "offset": 1427.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "some of our processes or improve some of",
      "offset": 1429.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the things that we do. um the more time",
      "offset": 1431.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you can spend on that and not on like",
      "offset": 1433.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the optimal chunking strategy uh or like",
      "offset": 1436.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "how to make sure that your VLM doesn't",
      "offset": 1439.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "continuously go down or like basic",
      "offset": 1441.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "problems that you have to deal with on",
      "offset": 1444.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the sort of red plumbing side of things.",
      "offset": 1445.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Um the less you have to worry about the",
      "offset": 1448.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "plumbing, the more you can worry about",
      "offset": 1450.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually solving real problems that that",
      "offset": 1452.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "add business value. Yeah, that makes",
      "offset": 1454.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "sense. just really think about how can I",
      "offset": 1456.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "actually make my business better and",
      "offset": 1459.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then try and get as close to just",
      "offset": 1461.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "creating stuff on that level rather than",
      "offset": 1463.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "worrying too much about the low-level",
      "offset": 1465.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "infrastructure if possible. Okay. So",
      "offset": 1467.039,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "there's something you mentioned a few",
      "offset": 1470.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "times uh it's the idea of a",
      "offset": 1472.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "reranker. It sounds quite technical but",
      "offset": 1475.08,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "I feel like it's one of those",
      "offset": 1477.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "underleveled components of AI systems",
      "offset": 1479.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that isn't talked about much. So, do you",
      "offset": 1481.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "want to explain what is a re-ranker and",
      "offset": 1483.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "why would I need one? Yeah, the the",
      "offset": 1485.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "re-ranker is super important. I think",
      "offset": 1488.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "every modern rag pipeline in production",
      "offset": 1490.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "that I've seen has a re-ranker in it.",
      "offset": 1492.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And so, the idea is actually very",
      "offset": 1495.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "simple. It's when you do retrieval um",
      "offset": 1497.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "because you you need to do retrieval",
      "offset": 1501.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "over large amounts of data, you cannot",
      "offset": 1503.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "have a very big model doing that. So,",
      "offset": 1505.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "embeddings models are are you cache the",
      "offset": 1508.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "embeddings, right? it's relatively uh uh",
      "offset": 1510.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "cheap compute. Maybe you do some like",
      "offset": 1514.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "keyword search um and then you get a",
      "offset": 1516.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "bunch of results. But now because you",
      "offset": 1519.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you had to do this at a very large",
      "offset": 1521.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "scale, you will have made mistakes",
      "offset": 1523.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because you had to do it sort of quickly",
      "offset": 1525.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "enough, right? So you couldn't have like",
      "offset": 1527.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "a a smarter model take a look at it. So",
      "offset": 1529.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that that's what the reranker is. It's a",
      "offset": 1532.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "smarter model that does a second pass at",
      "offset": 1534.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "your initial retrieval results and says",
      "offset": 1537.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "actually like this I you know I can see",
      "offset": 1539.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "why it was sort of relevant but it's",
      "offset": 1542.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "really not relevant or you know this one",
      "offset": 1543.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is actually super super important for",
      "offset": 1546.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "getting the right answer. Um, and so,",
      "offset": 1548.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "uh, our re-ranker is, uh,",
      "offset": 1550.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "state-of-the-art by by a large margin.",
      "offset": 1552.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "So, it's much better than other",
      "offset": 1554.799,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "rebankers out there, and it's the only",
      "offset": 1556,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "one that can follow instructions, which",
      "offset": 1558.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "is really important because now you can",
      "offset": 1560.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "tell it what your um data hierarchy is",
      "offset": 1562,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "basically. So, again, at the level of",
      "offset": 1565.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the user or even of the the individual",
      "offset": 1568,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "query, you can say this is the priority",
      "offset": 1570.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "of my retrieval results based on my",
      "offset": 1573.919,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "preferences. So most recent first um if",
      "offset": 1576.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "it is a PDF then it's more true than if",
      "offset": 1580.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it's a slack message um you know if it",
      "offset": 1582.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "comes from our internal wiki or if the",
      "offset": 1585.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "boss wrote it then it's more true than",
      "offset": 1587.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "if it's an intern uh document and it has",
      "offset": 1589.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "draft in the title right the these types",
      "offset": 1591.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of of rules for how to prioritize data",
      "offset": 1593.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "and how to break conflicts in rag",
      "offset": 1596.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "pipelines um that's what you need an",
      "offset": 1598.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "instruction following re-ranker for and",
      "offset": 1601.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and so uh yeah that's that's one of the",
      "offset": 1603.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "that we have in our platform and that's",
      "offset": 1606.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "one of the things that makes us better.",
      "offset": 1608.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Okay. Yeah. So I think the standard",
      "offset": 1610.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "approach for retrieving information from",
      "offset": 1613.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "a backto-day space is just a simple dot",
      "offset": 1614.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "product of like 19th century maths. Uh",
      "offset": 1617.36,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "so I'm curious you mentioned if you want",
      "offset": 1620.559,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "to rerank information uh you got to have",
      "offset": 1623.919,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "a set of rules on precedence. So is that",
      "offset": 1626.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "something you need to write manually uh",
      "offset": 1629.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "these precedence rules or can they be",
      "offset": 1631.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "automatically generated?",
      "offset": 1633.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Yeah. So, so we usually have pretty good",
      "offset": 1636.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "default rules in place uh for for the",
      "offset": 1638.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "re-ranker. And so that would be things",
      "offset": 1641.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "like most recent first and kind of",
      "offset": 1642.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "common sense uh rules. Um so you don't",
      "offset": 1644.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "have to come up with them yourself if",
      "offset": 1648,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you don't want to. You can also ask a",
      "offset": 1650,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "language model based on your data what",
      "offset": 1651.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it thinks the rules should be. Um um but",
      "offset": 1653.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "uh yeah, so you you you can do it",
      "offset": 1657.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "yourself or you can just go with the",
      "offset": 1659.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "defaults.",
      "offset": 1661.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Okay, nice. So again, this sounds like",
      "offset": 1663.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "something where you could easily get",
      "offset": 1664.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "very deep into the weeds and figure out",
      "offset": 1666.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "what's the optimal strategy here, but it",
      "offset": 1669.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "also feels like again this could be",
      "offset": 1671.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "tricky stuff. Yeah. So if you have a",
      "offset": 1672.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "very specific rank problem and there's a",
      "offset": 1675.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "very specific thing that needs to happen",
      "offset": 1676.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "there um just because of the problem",
      "offset": 1678.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you're trying to solve, then then you",
      "offset": 1681.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "could try to solve it through the system",
      "offset": 1683.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "prompt, but ideally you solve it by",
      "offset": 1685.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "prompting your retrieval uh pipeline,",
      "offset": 1687.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "right? And so you obviously can't do",
      "offset": 1689.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that in your first stage retriever",
      "offset": 1691.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "because it doesn't have the ability to",
      "offset": 1692.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "follow any instructions, but you can do",
      "offset": 1694.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "it in your second stage. Uh so in your",
      "offset": 1696.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "reranker. So the tricky part here comes",
      "offset": 1699.08,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "when like what happens if you've got",
      "offset": 1702.08,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "information that conflicts with each",
      "offset": 1704.799,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "other. So you're trying",
      "offset": 1707.08,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "to you give a problem that's trying to",
      "offset": 1709.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "give a specific answer. So you pull some",
      "offset": 1712.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "information from your knowledge base or",
      "offset": 1714.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "whatever and you've got two different",
      "offset": 1716.64,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "results that give you different answers.",
      "offset": 1719.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Uh talk me through how do you go about",
      "offset": 1722.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "resolving this? So I mean it depends on",
      "offset": 1724.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the type of conflict. U but you can tell",
      "offset": 1726.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it through instructions how how to deal",
      "offset": 1729.279,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "with that conflict or how to break the",
      "offset": 1731.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "tie basically. So you could say most",
      "offset": 1732.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "recent first. So if you find two",
      "offset": 1735.039,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "documents and one is more recent, then",
      "offset": 1737.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you uh rank that one much higher, right?",
      "offset": 1739.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Or you could say different data sources.",
      "offset": 1742,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "It's like I prefer this data source over",
      "offset": 1744.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that one. Um so you can you can just put",
      "offset": 1746.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that in the instructions too. And then",
      "offset": 1749.039,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "there are that's why again why it's so",
      "offset": 1752.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "important that you can prompt these",
      "offset": 1754.399,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "components is because there are",
      "offset": 1755.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "different uh strategies for dealing with",
      "offset": 1757.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "conflicts. So one is you go for most",
      "offset": 1760.559,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "recent the other is um you report both",
      "offset": 1763.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "results but you say that one is more",
      "offset": 1766.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "recent right so uh it really depends and",
      "offset": 1768.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and that's that's why it's so important",
      "offset": 1772.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that you get control over those aspects",
      "offset": 1773.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "of a rag system okay so it's good that",
      "offset": 1776.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "there's some sort of way of resolving",
      "offset": 1779.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "these conflicts I'm curious is there any",
      "offset": 1780.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "way to feedback from when you have these",
      "offset": 1783.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "conflicts uh to then put that into your",
      "offset": 1785.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "data governance strategy",
      "offset": 1787.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah. So, so we have uh some customers",
      "offset": 1791.919,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "where um when the system, so one of our",
      "offset": 1795.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "our uh very special capabilities is our",
      "offset": 1798.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "ability to say I don't know. Um which is",
      "offset": 1800.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "an underrated feature. It's much better",
      "offset": 1803.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to say I don't know than to make up a",
      "offset": 1806.159,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "wrong answer, right? So um um that's a",
      "offset": 1807.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "really really good thing. But then when",
      "offset": 1811.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you have an I don't know answer, ideally",
      "offset": 1812.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you want to have the ability to annotate",
      "offset": 1815.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that answer so that the next time you",
      "offset": 1817.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "get the question, you do have relevant",
      "offset": 1819.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "information in your your documentation",
      "offset": 1821.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "for for resolving that question, right?",
      "offset": 1824.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Uh so that's one feedback loop and then",
      "offset": 1826.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the other is yeah we we collect feedback",
      "offset": 1828.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "uh through our our UI or through our",
      "offset": 1831.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "APIs and then you can export that",
      "offset": 1833.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "feedback and actually train on that",
      "offset": 1835.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "feedback. Um and so that's how you can",
      "offset": 1837.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "uh specialize it for the use case over",
      "offset": 1840,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "time. Just keep making it this sounds",
      "offset": 1841.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pretty useful for creating the feedback",
      "offset": 1844.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "that you get. Start off saying okay",
      "offset": 1845.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we've got some wrong answers and then",
      "offset": 1847.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you give those wrong answers back to the",
      "offset": 1850,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "people who actually curate the data or",
      "offset": 1852.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "maintain it and that's going to feed",
      "offset": 1854.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "back into a better system for later on.",
      "offset": 1856.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "Yeah. So that's over time at least how",
      "offset": 1859.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you can capture most of the",
      "offset": 1861.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "distribution. Obviously the tail of the",
      "offset": 1862.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "distribution you're never going to fully",
      "offset": 1864.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "be able to capture uh but you can",
      "offset": 1866.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "fine-tune system maybe to generally be",
      "offset": 1868.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "better also at the tail. Now we talked a",
      "offset": 1870.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "little bit before about the idea of a",
      "offset": 1873.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "rag agent. So I am curious as to whether",
      "offset": 1875.12,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "rag agents have a different architecture",
      "offset": 1879.279,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "compared to other rag applications. Uh",
      "offset": 1882,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "yes and no. I guess we should talk a bit",
      "offset": 1885.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "about what an agent is right because",
      "offset": 1887.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "everybody's very confused. Absolutely.",
      "offset": 1889.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Uh so I think an agent is a very general",
      "offset": 1891.919,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "concept. I mean so it it comes from uh",
      "offset": 1895.44,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "reinforcement learning uh or even it's",
      "offset": 1898.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "maybe even older than reinforcement",
      "offset": 1901.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "learning itself where it's really just",
      "offset": 1902.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "about like a policy that takes actions",
      "offset": 1904.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "in an environment and and that policy",
      "offset": 1907.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "has some sort of state and so it can",
      "offset": 1909.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "manipulate the environment um uh but it",
      "offset": 1912.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "doesn't necessarily have to manipulate",
      "offset": 1915.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the environment in order to be",
      "offset": 1917.519,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "considered an agent. Uh so another way",
      "offset": 1919.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to put that's maybe maybe a bit closer",
      "offset": 1921.679,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to home for uh folks listening to this",
      "offset": 1923.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "podcast is if you think about an agent",
      "offset": 1926.399,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "that can do SQL queries. Um some people",
      "offset": 1929.039,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "will say oh but it's only an agent if it",
      "offset": 1933.679,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "changes the environment which would mean",
      "offset": 1935.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that it is only an agent if it generates",
      "offset": 1937.519,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "insert queries or update queries. And if",
      "offset": 1940.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it just does select queries then",
      "offset": 1943.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "suddenly it's no longer an agent. So I",
      "offset": 1944.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "mean when you explain it like that it",
      "offset": 1948.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "sounds a bit silly right as like",
      "offset": 1949.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "obviously that's still an agent like",
      "offset": 1951.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "deep research doesn't do any insert",
      "offset": 1953.039,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "queries or update queries but it does do",
      "offset": 1955.039,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "a lot of useful stuff right so um I I",
      "offset": 1957.519,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "think a much more useful definition of",
      "offset": 1961.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "an agent is just something that actively",
      "offset": 1963.519,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "reasons um so something that thinks",
      "offset": 1965.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "about what it's doing formulates a plan",
      "offset": 1968.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "executes on the plan and then can revise",
      "offset": 1970.399,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that plan based on the information that",
      "offset": 1972.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "came in so that's active reasoning and",
      "offset": 1974.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and So the really exciting uh technology",
      "offset": 1976.24,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "that that has enabled all of this is",
      "offset": 1979.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "just test time reasoning and the insight",
      "offset": 1981.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "that um that uh shifting the compute",
      "offset": 1983.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "from the training side to the test time",
      "offset": 1986.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "inference side um actually has very very",
      "offset": 1988.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "nice uh properties. Okay. Yeah, I like",
      "offset": 1992.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that. So I think uh with your",
      "offset": 1994.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "explanation there it was just",
      "offset": 1997.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "introducing all the important bits of",
      "offset": 1998.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "jargon from reinforcement learning uh",
      "offset": 2000.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "just the idea of policies and",
      "offset": 2003.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "environments and all those other things.",
      "offset": 2005.2,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "If you're interested in reinforcement",
      "offset": 2006.559,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "learning please just rewind and re",
      "offset": 2007.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "listen to that last minute or so. Uh",
      "offset": 2009.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "okay uh I do like the idea uh that you",
      "offset": 2011.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "mentioned uh with test time reasoning.",
      "offset": 2014.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "So you're doing reasoning at the time",
      "offset": 2016.799,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the user asks for it. Uh and then it can",
      "offset": 2018.799,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "go and check things on its own and uh",
      "offset": 2021.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "work through things step by step. Yeah.",
      "offset": 2024.159,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "No, so I I I think like the the simple",
      "offset": 2026.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "chatbot is a subset of the overall",
      "offset": 2029.519,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "concept of an agent. Uh so you can just",
      "offset": 2031.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "have a you can have a chatbot agent and",
      "offset": 2034.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it probably for most chatbot use cases",
      "offset": 2037.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it doesn't have to do a lot of thinking.",
      "offset": 2039.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Um so it the the same agent can also",
      "offset": 2041.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "power the chatbot use case. It is just",
      "offset": 2045.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "more powerful. It depends on how how",
      "offset": 2047.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "much um you want to budget for test time",
      "offset": 2049.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "compute. And so if you want to minimize",
      "offset": 2052.639,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that then uh you probably um uh yeah",
      "offset": 2055.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just have something like a standard",
      "offset": 2058.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "chatbot. But but the the boundary is is",
      "offset": 2059.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "is very blurry, right? Because when I",
      "offset": 2062.639,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "have a rag application, which maybe is",
      "offset": 2064.8,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "not really an agent, but as a part of my",
      "offset": 2068.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "rag step, I do query decomposition and I",
      "offset": 2071.119,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "formulate sort of a plan and then I do",
      "offset": 2074.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "some filtering on top. It's sort of an",
      "offset": 2076.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "agent, right? Like that that is what",
      "offset": 2078.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "agents would do, but the agent sort of",
      "offset": 2081.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "maybe determines that more dynamically",
      "offset": 2082.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "on the fly sort of what it would want to",
      "offset": 2085.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "do. Um but yeah, the the boundary is not",
      "offset": 2087.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "as well defined, I think, as a lot of",
      "offset": 2090.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "people like to pretend. Um but so",
      "offset": 2092.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "overall, I think so this really is the",
      "offset": 2095.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "year of agents. There's a lot of",
      "offset": 2097.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "excitement about them. They don't really",
      "offset": 2098.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "work quite yet. I think a lot of people",
      "offset": 2100.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "um are are sort of blown away by the",
      "offset": 2104.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "potential, but like in practice, I",
      "offset": 2106.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "haven't seen any real agent deployments",
      "offset": 2108.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "that have like material impact on a",
      "offset": 2110.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "company's business uh yet. I mean, I'm",
      "offset": 2112.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "sure it's going to come, but it's going",
      "offset": 2115.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "to take some time. Um, but these agents",
      "offset": 2117.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "obviously they need to work on your",
      "offset": 2119.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "data, too, right? Just like with what we",
      "offset": 2121.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "were doing before with NAI. It's like,",
      "offset": 2123.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "yeah, we need agents, but the agents",
      "offset": 2125.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "need to work on your data. So, they need",
      "offset": 2127.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to be rag agents because rag is the way",
      "offset": 2129.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "that you get things to work on top of",
      "offset": 2132.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "your data. Um so um yeah, rag is one of",
      "offset": 2134.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the tools that these agents need to rely",
      "offset": 2138.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "on for them to be useful especially in",
      "offset": 2140.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "enterprise settings. Ah this is",
      "offset": 2143.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "interesting. So I think the big",
      "offset": 2145.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "difference is that with this sort of",
      "offset": 2147.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "standard rack approach you've got some",
      "offset": 2149.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "software saying uh okay let's shove all",
      "offset": 2151.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the bits of information from the vector",
      "offset": 2153.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "database into the prompt and those kind",
      "offset": 2156.079,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "of being pushed to the LLM. Whereas if",
      "offset": 2160,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "you're doing things with um inference",
      "offset": 2162.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "time reasoning then the LLM has to ask",
      "offset": 2166.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "for the information just kind of pulling",
      "offset": 2168.88,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 2170.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "in. Uh so there's a a swap is that about",
      "offset": 2171.64,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "right? Yeah that's right. So but again",
      "offset": 2175.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "right it's not really like one or the",
      "offset": 2178.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "other. It's really a spectrum like I",
      "offset": 2180.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "think most modern rag systems they",
      "offset": 2182.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "probably have some kind of classifier",
      "offset": 2184.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that says should I retrieve or not right",
      "offset": 2186.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "so and then based on that you say okay",
      "offset": 2190.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so like if I say hello then then you",
      "offset": 2192.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "don't have to retrieve in your rag",
      "offset": 2194.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "chatbot you just say hello back right so",
      "offset": 2196.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "first thing is you you need to so that's",
      "offset": 2199.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "already kind of active retrieval where",
      "offset": 2201.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in the old rag setup where it's really",
      "offset": 2203.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "passive retrieval is you get a query you",
      "offset": 2205.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "always search for that query",
      "offset": 2208.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "uh in your vector database. You always",
      "offset": 2210.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "give the results to the language model",
      "offset": 2212.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "like that's that's prehistoric at this",
      "offset": 2215.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "point, right? So it's much more",
      "offset": 2218.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "complicated. There's there's like active",
      "offset": 2219.599,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "decision making involved in these rag",
      "offset": 2222.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "pipelines. There's a lot of uh like",
      "offset": 2224.32,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "filtering. There's the reranker uh that",
      "offset": 2226.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "has a huge impact. Um there there's this",
      "offset": 2229.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "sort of active retrieval component.",
      "offset": 2232.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "There's query decomposition which is",
      "offset": 2234,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "almost like formulating a a retrieval",
      "offset": 2235.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "plan like that that's all very agentic",
      "offset": 2238,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "but now when you have an agent you can",
      "offset": 2241.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "do that much more dynamically and that's",
      "offset": 2242.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that's why that's so exciting. Well",
      "offset": 2244.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "okay. Yeah. So you got more flexibility",
      "offset": 2246.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "about what the behavior is going to be",
      "offset": 2249.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "uh rather than having a more rigid",
      "offset": 2251.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "pipeline. Yeah. Exactly. And it can also",
      "offset": 2252.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like recover from its mistakes which is",
      "offset": 2256,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "very important. Right. So if it",
      "offset": 2258.24,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "retrieves something and it thinks oh",
      "offset": 2259.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "actually this is not what I wanted let",
      "offset": 2261.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "me try a different query and then it",
      "offset": 2262.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "gets the result that's very powerful and",
      "offset": 2264.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that's something you can do with with uh",
      "offset": 2266.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "aentic abilities um and then uh the",
      "offset": 2269.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "other thing is is more around sort of",
      "offset": 2272.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "multihop questions or or like multi-step",
      "offset": 2274.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "reasoning is like like first I need to",
      "offset": 2277.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "know this and then I need to know that",
      "offset": 2279.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "and then I need to compare those things",
      "offset": 2281.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and then maybe based on the result do",
      "offset": 2282.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "something else right that that type of",
      "offset": 2284.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "multihop uh problem that that's I think",
      "offset": 2287.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "very interesting uh uh in terms of",
      "offset": 2290.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "business value as well. Okay. So you",
      "offset": 2292.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "mentioned that there haven't really been",
      "offset": 2294.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "any deployments of agents so far that",
      "offset": 2296.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "have wowed you. Uh so what do you think",
      "offset": 2298.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "would wow you? I mean, they wowed me in",
      "offset": 2301.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "terms of like seeing the potential, but",
      "offset": 2304.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they haven't wowed me in like, oh, this",
      "offset": 2306.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "company has saved like $10 million this",
      "offset": 2308.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "year because they had an agent doing",
      "offset": 2311.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "something. I like a like a proper agent",
      "offset": 2313.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "uh using test time reasoning and not",
      "offset": 2316.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "something that was branded an agent, but",
      "offset": 2318,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that was something that we were already",
      "offset": 2319.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "uh capable of doing before kind of uh",
      "offset": 2322.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "reasoning models. So, what do you think",
      "offset": 2324.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "the sticking points are? Where are we",
      "offset": 2327.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "falling short? I think generalization",
      "offset": 2329.839,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "sort of um um so so actually having it",
      "offset": 2333.359,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "work in in realw world settings where",
      "offset": 2337.44,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "it's not in the the toy domain right so",
      "offset": 2340.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "getting things to work in a toy domain",
      "offset": 2343.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "or in a nice demo that that that used to",
      "offset": 2345.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "be my sort of story around rag is and",
      "offset": 2348.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that's still very true people think that",
      "offset": 2350.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "rag is easy because you can build a a",
      "offset": 2352.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "nice rag demo on a single document very",
      "offset": 2355.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "quickly now and it will be pretty nice.",
      "offset": 2358.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Um, but getting this to actually work at",
      "offset": 2361.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "scale on real world data, uh, where you",
      "offset": 2363.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "have enterprise constraints, it's very",
      "offset": 2366.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "different problem. And so it's the same",
      "offset": 2368.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "with agents where it's like, oh, I can",
      "offset": 2370.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "make something like do this one",
      "offset": 2372.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "particular thing when I prompt it and",
      "offset": 2373.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "basically just make everything look good",
      "offset": 2375.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "for exactly the one thing I wanted to",
      "offset": 2378,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "do, but then when you actually have to",
      "offset": 2379.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "make this work in in a real world",
      "offset": 2382.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "setting, then then everything just",
      "offset": 2385.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "breaks down very quickly still. So",
      "offset": 2386.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that's going to get better over time",
      "offset": 2388.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "obviously. Um and and so I think the",
      "offset": 2390.8,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "hype is justified. Um but yeah, it's",
      "offset": 2393.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "going to take some time for for these",
      "offset": 2397.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "systems to be enterprise grade enough",
      "offset": 2398.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "for anybody to really deploy this uh in",
      "offset": 2400.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "a in a critical setting. So since a lot",
      "offset": 2403.28,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "of companies are just thinking we got to",
      "offset": 2406.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "get on the AI game, we got to build",
      "offset": 2409.359,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "something.",
      "offset": 2413.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "What can you build that is likely to",
      "offset": 2415.44,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "work and to add value? Yeah. So, um what",
      "offset": 2417.76,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "you can build um so you can build basic",
      "offset": 2421.68,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "u solutions for the relatively uh boring",
      "offset": 2425.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "problems. So, um one thing you can do is",
      "offset": 2428.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "go for like the basic problems where you",
      "offset": 2431.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "ask the basic questions like internal",
      "offset": 2433.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "search, right? But that doesn't really",
      "offset": 2435.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "get you value. It's much more like",
      "offset": 2436.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "trying to find um workflows that exist",
      "offset": 2438.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "in your company that are a little bit",
      "offset": 2441.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "boring but that where it's important",
      "offset": 2444.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that you get it right and where it",
      "offset": 2447.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "requires some expertise. uh if you can",
      "offset": 2448.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "solve those problems then uh you can you",
      "offset": 2451.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "can make your your uh company much",
      "offset": 2454.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "better, right? And and so these could be",
      "offset": 2456.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "very simple things from like you know",
      "offset": 2458.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "checking for uh compliance against your",
      "offset": 2460.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "set of policies",
      "offset": 2463.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "um or doing basic research. Uh we have a",
      "offset": 2465.2,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "very nice demo where we fill out Excel",
      "offset": 2469.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "spreadsheets on unstructured data on the",
      "offset": 2471.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "fly so that you don't have to manually",
      "offset": 2473.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "go and copy and paste. you just directly",
      "offset": 2475.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "in your Excel uh kind of call a macro",
      "offset": 2478,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and then fill out the spreadsheet with",
      "offset": 2480.88,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "unstructured data from different data",
      "offset": 2482.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "sources. Um doing things like customer",
      "offset": 2484.52,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "support um there there's a lot of um",
      "offset": 2488,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "lowhanging fruit um in kind of the the",
      "offset": 2491.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "codegen side of things. So there a lot",
      "offset": 2494.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of it is happening kind of across the",
      "offset": 2496.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "board, right? It's just um um yeah doing",
      "offset": 2498.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "this this the right way takes time. So",
      "offset": 2501.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "there's there's a big gap between sort",
      "offset": 2504.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "of where the hype cycle is and where",
      "offset": 2506.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like reality is in enterprises, but I",
      "offset": 2508.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "mean it it's coming. Um uh it's it's",
      "offset": 2510.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "just u yeah it takes time. Okay. So it",
      "offset": 2513.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "sounds like the best approach then is to",
      "offset": 2516.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "go for maybe slightly more narrow use",
      "offset": 2518.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "cases where there's less flexibility",
      "offset": 2520.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "needed, less generalization needed. I",
      "offset": 2521.839,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "guess maybe all most build disposable",
      "offset": 2524.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "agents. does if you can build something",
      "offset": 2528.2,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "quickly um that just solve your problems",
      "offset": 2530.56,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and then be done that works. I like I",
      "offset": 2532.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "like that idea. You can build disposable",
      "offset": 2535.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "agents on our platform and",
      "offset": 2537.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "maybe I should rebrand like that the",
      "offset": 2541.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "disposable agent platform.",
      "offset": 2543.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Nice. Uh so just to wrap up, what are",
      "offset": 2546.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "you most excited about in the world of",
      "offset": 2548.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "AI? Yeah, so I'm obviously very excited",
      "offset": 2550.56,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "about all the Gentic things. Um I I",
      "offset": 2554,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "think for for me personally where I I",
      "offset": 2557.24,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "see a lot of very interesting problems",
      "offset": 2561.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "is at the intersection of structured and",
      "offset": 2563.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "unstructured data. Um so you have a",
      "offset": 2565.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "bunch of documents but you also have",
      "offset": 2568.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "your traditional uh kind of structured",
      "offset": 2570.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "relational databases uh your snowflake",
      "offset": 2572.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "or your bigquery or whatever you use and",
      "offset": 2575.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "now you want to kind of cross-sect that",
      "offset": 2578.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "information using a gentic rag. And so",
      "offset": 2580.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "if you can do that which you you can now",
      "offset": 2583.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "uh start doing because of these agentic",
      "offset": 2586.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "abilities that unlocks so much",
      "offset": 2588.56,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "interesting um potential. Um so I think",
      "offset": 2590.88,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "that's really exciting. The other thing",
      "offset": 2595.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is multimodality is obviously still um",
      "offset": 2597.119,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "very under unexplored. I think uh um",
      "offset": 2600.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "every time there's like an image",
      "offset": 2604.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "generation feature uh uh getting shipped",
      "offset": 2606.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "that kind of goes viral. But um I think",
      "offset": 2608.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "image understanding is actually much",
      "offset": 2611.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "more valuable um from a from a kind of",
      "offset": 2613.359,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "enterprise perspective. Um so so I think",
      "offset": 2616.72,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "that that's also a really uh key unlock",
      "offset": 2620.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that is coming soon. So chart",
      "offset": 2623.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "understanding for example and and things",
      "offset": 2625.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like that understanding um you know a",
      "offset": 2627.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "McKenzie slide deck that has lots of",
      "offset": 2630.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "different diagrams and charts and things",
      "offset": 2632.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "in there like right now that's not",
      "offset": 2633.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "really within the capabilities of these",
      "offset": 2635.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "systems. Um, but it it's coming very",
      "offset": 2637.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "quickly. Okay, I got to follow up on",
      "offset": 2639.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "that. Uh, so talk me through it. You",
      "offset": 2641.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "basically want the ability to understand",
      "offset": 2644.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "a presentation, then just throw it a",
      "offset": 2645.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "PowerPoint and explain what the output",
      "offset": 2649.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is. Yeah. So that's the simple case. So",
      "offset": 2652.079,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "the hard case is I have uh a 100 million",
      "offset": 2655.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "PowerPoints that my company has made in",
      "offset": 2658.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the history of my company with a 100,000",
      "offset": 2660.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "people in my company. And now I want to",
      "offset": 2662.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "answer questions based on all of that",
      "offset": 2665.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "information. Uh so it's not just like",
      "offset": 2667.96,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "one single PowerPoint because that you",
      "offset": 2671.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "can kind of start to do even though it's",
      "offset": 2673.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "not very accurate but you need to do it",
      "offset": 2674.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "at scale right? So over over lots of u",
      "offset": 2676.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "uh presentations. So if you can do that",
      "offset": 2680.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then you can do very interesting kind of",
      "offset": 2682.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "synthesis on top of it right. So how did",
      "offset": 2684.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "our perspective on a particular type of",
      "offset": 2687.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "thing change over the years? uh and then",
      "offset": 2689.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you can just find the relevant",
      "offset": 2692.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "presentation decks that cover this",
      "offset": 2693.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "particular thing. Look at the charts,",
      "offset": 2695.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "reason about the trends in the charts",
      "offset": 2697.28,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "and then combine that into a new",
      "offset": 2699.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "insight. Um that that's all kind of",
      "offset": 2701.079,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "starting to become possible now. Okay,",
      "offset": 2703.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that would be very interesting. Although",
      "offset": 2707.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "I have to say I've definitely had a few",
      "offset": 2708.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "colleagues in the past where even with",
      "offset": 2709.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "them talking over the PowerPoint, I am",
      "offset": 2711.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "not sure what they've been on about.",
      "offset": 2713.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "It's similar to what everybody says,",
      "offset": 2716.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "garbage in, garbage out. You cannot be",
      "offset": 2717.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "much better than your data. You can try",
      "offset": 2719.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "to reason about it intelligently but",
      "offset": 2721.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "there are limits there. Absolutely.",
      "offset": 2723.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Absolutely. Wonderful. So all right. Uh",
      "offset": 2725.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "since you've actually been involved in",
      "offset": 2727.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "rag since the beginning I mean you were",
      "offset": 2729.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there when rag was created as part of",
      "offset": 2732,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "the uh team at meta. Uh so do you think",
      "offset": 2733.68,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "rag has lived up to your expectations?",
      "offset": 2737.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Has it panned out as you expected?",
      "offset": 2740.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "That's a nice question.",
      "offset": 2743.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "Um I mean I I think that the the",
      "offset": 2744.92,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "original vision was always that we would",
      "offset": 2748.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "have um kind of a decoupling between the",
      "offset": 2751.119,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "knowledge and the reasoning where the",
      "offset": 2754.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "reasoning is really just like taking",
      "offset": 2757.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "whatever the relevant knowledge is and",
      "offset": 2759.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "then uh giving the right answer on top",
      "offset": 2761.599,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "of it without um um having any of of the",
      "offset": 2763.599,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "knowledge in it itself. So that didn't",
      "offset": 2768.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "really pan out and and so um that's part",
      "offset": 2770.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of the reason why these systems",
      "offset": 2774.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "hallucinate and and that that's um I",
      "offset": 2775.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "mean it's a longer story but but so for",
      "offset": 2778.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for rag I think when the paper came out",
      "offset": 2780.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "uh it was very focused on open domain",
      "offset": 2783.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "question answering which is the the",
      "offset": 2785.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "domain that you evaluate these systems",
      "offset": 2787.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "on um and so it was wellreceived but at",
      "offset": 2788.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "the time Gen AI like vector databases",
      "offset": 2793.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "basically didn't really exist right that",
      "offset": 2795.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "became a thing after uh the paper uh",
      "offset": 2797.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "language models didn't really exist. We",
      "offset": 2800.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "had like Bart and T5, but there there",
      "offset": 2802.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "wasn't really a concept of like an auto",
      "offset": 2804.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "reggressive uh generative model. So I",
      "offset": 2806.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "think like the reason rag became such a",
      "offset": 2810,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "popular like paradigm and concept um and",
      "offset": 2812.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "why it's called rag is because because",
      "offset": 2815.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of the G. So it's really just because",
      "offset": 2817.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "genai became a thing that rag became a",
      "offset": 2819.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "thing. Um and there are lots of papers",
      "offset": 2822.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "from around that same time. And there's",
      "offset": 2824.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this amazing paper from folks at Google",
      "offset": 2826.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "called Realm where that didn't become",
      "offset": 2828.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the name of the paradigm because it",
      "offset": 2830.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "didn't have a G in it. It was a a mass",
      "offset": 2833.04,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "language model, right? So um yeah,",
      "offset": 2835.119,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "hindsight is sort of 2020. It is amazing",
      "offset": 2840,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "how small changes to the name have a big",
      "offset": 2842.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "difference on your success or not. I",
      "offset": 2844.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "mean it was it's not just the name,",
      "offset": 2847.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "right? is is that we were interested in",
      "offset": 2848.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "trying to see if you could generate the",
      "offset": 2850.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "answer without uh sort of so the",
      "offset": 2852.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "alternative is just predicting the",
      "offset": 2855.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "answer which is uh what was much more",
      "offset": 2856.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "normal to do at the time. So I guess we",
      "offset": 2858.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "were ahead of our time in the right way",
      "offset": 2861.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there. Wonderful. Yeah, certainly I mean",
      "offset": 2863.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "uh it's taken over in so many different",
      "offset": 2865.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "ways. Uh it's ubiquitous now. Uh so",
      "offset": 2866.8,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "obviously very successful and uh just",
      "offset": 2869.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "finally I want ideas for people to",
      "offset": 2873.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "follow. So whose work are you most",
      "offset": 2876,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "interested in at the moment? Whose work",
      "offset": 2877.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "am I most interested in? Um I think",
      "offset": 2880.079,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "there there is a",
      "offset": 2882.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "um a lot of interesting work happening",
      "offset": 2884.76,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "in this new testime compute paradigm. So",
      "offset": 2888.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "I I mean I still have my kind of",
      "offset": 2891.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "part-time Stanford adjunct professor uh",
      "offset": 2894.079,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "gig which is great for me to kind of uh",
      "offset": 2897.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "stay stay at least a little bit up to",
      "offset": 2900,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "date on the latest latest research",
      "offset": 2902.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "trends. Um, and I I think there's just a",
      "offset": 2903.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "lot of interesting research happening",
      "offset": 2906.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "around this testime reasoning and and",
      "offset": 2907.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "what you can do there. I think we've",
      "offset": 2909.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "only just scratched the surface. Um, and",
      "offset": 2911.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and so uh what happened with Deep Seek",
      "offset": 2914.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and things like that has been very",
      "offset": 2916.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "encouraging, I think, from that",
      "offset": 2918.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "perspective where it's actually not that",
      "offset": 2919.52,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "hard for um for like nonfrontier le um",
      "offset": 2921.52,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "folks to to do interesting things in",
      "offset": 2927.359,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "this space and and have impact. Um so um",
      "offset": 2929.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "yeah and I I follow a lot of uh just",
      "offset": 2933.28,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "smart academics. Uh yeah smart academics",
      "offset": 2936.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "always worth following I think. Uh very",
      "offset": 2939.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "good genre of people to uh to watch out",
      "offset": 2940.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for. Uh so all right uh thank you so",
      "offset": 2942.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "much for your time to",
      "offset": 2945.2,
      "duration": 4.97
    },
    {
      "lang": "en",
      "text": "thanks for having me.",
      "offset": 2947.52,
      "duration": 23.88
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 2950.17,
      "duration": 21.23
    }
  ],
  "cleanText": "Welcome to DataFramed. This is Richie.\n\nRetrieval Augmented Generation, or RAG to its friends, is a popular technique for retrieving supporting information to include in your prompt. This is supposed to reduce the hallucination problem that LLMs suffer. For all its success, I quite regularly see people proclaiming its imminent death in favor of something more sophisticated. Today, I want to know whether the reports of RAG's death are greatly exaggerated or not. Our guest is Douwe Kiela, who was part of the team at Meta that invented RAG. These days, he's CEO of Contextual AI, a platform for creating RAG agents and custom LLMs known as contextual language models. All right, let's find out about the fate of RAG. A lot of RAG systems, like two years ago, were what we called like Frankenstein's RAG. All of these parts are kind of cobbled together, but they're not really designed to work well together. In our case, all of these components are designed to be state-of-the-art, and they're also designed to work well together. I think a much more useful definition of an agent is just something that actively reasons. The really exciting technology that has enabled all of this is just test time reasoning and the insight that shifting the compute from the training side to the test time inference side, um, actually has very, very nice, uh, properties.\n\nHi Douwe, welcome to the show.\n\nHi, thanks for having me.\n\nCool. So, I've been hearing a lot in the last year about people predicting the death of Retrieval Augmented Generation. RAG, why do you think people keep thinking this is going to happen?\n\nYeah. And that that usually happens with good, simple ideas where people are trying to sort of, um, rewrite history, maybe from a marketing perspective a little bit. And RAG is such a simple idea, uh, that I think it's a little bit silly to declare it. Um, so, um, I, I guess everybody listening in here knows what RAG stands for, right? Retrieval Augmented Generation. And so the G is just any Gen AI model, and then you want to make that model work on your data, which means you need to augment it with your data. And the way to do that is through some form of retrieval. So it's such a kind of generic paradigm that it doesn't really make sense to pronounce it that I think. But from a marketing perspective, a lot of people keep saying, \"Oh, you, you know, you don't need RAG, you need fine-tuning, or you don't need RAG, you need, um, long context windows.\" Um, and yeah, I mean, we can, we can go into the specifics of each of those. Um, but, uh, yeah, I think these are mostly marketing tricks.\n\nOkay, that's good to know. So I suppose it feels a bit like linear regression is very simple, and I mean, it's been around for like almost a century now, and people still use it because it's simple, even though there are more sophisticated models out there now.\n\nYeah, exactly. Yeah, we even bought a domain name, is ragdead yet.com, uh, where we can point people to a blog post about, uh, about whether RAG is dead or not. Yeah, I mean, so these are the types of things that you cannot really declare dead, I think. Uh, but I guess I'm biased.\n\nOkay. Uh, are there any problems though? Are there any limitations to this?\n\nOh, absolutely. Uh, so, so I, I think if you look at agents, right, agents in general, uh, retrieval is just one of the tools in the toolbox of an agent. Um, and so, um, I think it's, it's very true that we should not just have RAG as the only thing that these agents, uh, can do. But yeah, that feels a little obvious, right? So, um, in terms of the current limitation still of RAG, I think part of the solution there is actually long context, right? So, if a retrieval system is imperfect, then ideally you want to cast a wider net. So, have more information that you can put into the context of the language model in the hope that there's something useful in there. Um, and then, um, uh, that's, that's where you need a longer context. That, that's why I always talk about these dichotomies as sort of being false, right? It's like you need both. It's like you're not going to put the entire like internet in the context of your language model. So that's why you are going to do some search. So RAG basically, but then you want to ideally put lots of search results in the context of the language model so that it can do its job and and answer the question correctly, right? So, uh, yeah, we need all of those things.\n\nOkay, so just to make sure I've understood this correctly, the whole point of RAG is that you're going to reduce hallucinations by just getting or just retrieving relevant information and putting that inside your prompt, and then that's going to help the large language model give you the correct answer. So if you have a longer context window, you can put more information in there that's going to assist it. And so the idea of long context windows and RAG, they sound like they're competing, but actually they're kind of complimentary. Is that right?\n\nVery, very complimentary. Yes.\n\nOkay. Wonderful. So I know in the Contextual AI blog, you've introduced the idea of RAG 2.0. So can you tell me what RAG 2.0 is and how is it different from the original RAG?\n\nYeah. So, so the way we do RAG is, is, uh, a bit different, I guess, from what everybody else is doing. And so we started the company when we saw the world get very excited after ChatGPT and then get very frustrated, uh, because the technology wasn't quite ready, and that that was especially true for, uh, enterprise use cases. So, um, we knew that RAG would be part of the answer, obviously, given what we knew about RAG. We also knew that RAG was just the first idea there, and that, and that, uh, even in the RAG paper, we talk about sort of what the the vision is and what we really want to achieve, which is that these components are designed to work together, right? So, so a lot of, um, um, RAG systems, like two years ago, were what we called like Frankenstein's RAG, where it's all of these parts are kind of coupled together, but they're not really designed to work well together. So, uh, our RAG 2.0, I know approach is about making sure that all of the components of a modern RAG pipeline. So that's not just a vector database and a language model anymore. It's much more complicated as a system. In our case, all of these components are designed to be state-of-the-art, and they're also designed to work well together. And you can do that through, uh, training on the same, uh, data distribution essentially, so that all the parts are designed to work well together. They're literally like trained to work well together. And so, uh, that combination of having very good components and then having a very good compound, um, that that makes our, our system much better at RAG than than anybody else.\n\nOkay. So in theory, that sounds like a very useful idea for the idea that the data in your database and the data in the large language model, they're kind of harmonized in some way. You got all the components working together. Um, do you have a sense of when this might be useful and what sort of benefits you might get from it?\n\nUh, it's always useful. I mean, uh, it's, it's the use cases we are focused on as a company. Uh, they, they tend to be high-stakes use cases with a low tolerance for mistakes and a and sort of high, uh, high accuracy requirements where you want to have very accurate attributions. So very often this is in regulated industries, and then you want to, um, um, you know, where you're also sensitive about your data, and if you have those characteristics, then, then, uh, we are much better than than anything else out there.\n\nOkay. Yeah. So I can see if you got a high requirement that the answer you're giving is correct in some way, you need to put more effort into this. Uh, do you have any examples maybe from one of your customers of things that have been built using this approach?\n\nYeah. So, so one use case I'm very proud of is the work we've been doing with Qualcomm. Um, so, um, their customer engineering department is using us where their only GenAI deployment at scale as far as I know. Uh, where, um, yeah, we, these are thousands of engineers who are using us, uh, on a daily basis to answer very complicated questions. So these are not the simple kind of questions where you have like your internal search and maybe it's like, you know, something like Glean, and you would ask it like, who is our 401k provider or how many vacation days do I get, right? Um, that, that's not really where the the ROI of AI is going to come from. You want to focus on these much more, uh, expert, uh, kind of specialist knowledge worker use cases, and and, uh, so that's what we did with Qualcomm, and we can answer very complicated questions where I don't understand what the question means, uh, and definitely don't understand what the answer means, but the system does a really good job at, uh, explaining the the information and giving the right answer.\n\nOkay. Yeah. So, if you got a simple true or false or give me a number, retrieve one simple fact kind of situation, then RAG 2.0 is maybe overkill. Is that right? But if you got a more complicated question, then that's when it's going to shine.\n\nIt's not really overkill. It's more that, um, you'll So, the RAG 2.0 know, uh, uh, happens during training, right? Not during inference. So during inference, it's still a normal RAG system, but just a really, really good one. Um, so it, it doesn't, um, make things more complicated for you. Uh, if anything, it makes everything easier because we offer it in one platform where you can build these agents really in 10 seconds. Uh, I, I can build a, a state-of-the-art RAG agent, a agent in, in 10 seconds, which is I think pretty powerful.\n\n10 seconds. Okay. Well, I'm, that's pretty short. I mean, it's about as short a development time as you can get.\n\nYeah. So, talk me through, uh, what do you do then if you're going to build something in 10 seconds.\n\nSo, uh, we had these two concepts, uh, of agents and data stores in our platform. And so what you would do is you would create a data store. Um, and then, um, you would tell the agent that that is the data store that it has to work on top of, and you put some files or a database, uh, into that data store, and now you can talk to that data and do RAG on top of that data with a, a state-of-the-art RAG system.\n\nOkay, that sounds very straightforward. Just copy some files into a database and say, okay, go look at them. So tell me something about what's in there.\n\nUm, all right. Nice. Um, and if you're going to look at one of these very strict use cases that you mentioned, uh, that one of these regulated examples, do you need some sort of tuning of that? Um, like some sort of optimization to make sure it all works properly?\n\nYeah. So, you can do that. So, our hope is that out of the box, our performance is already good enough, which turns out to be the case, uh, for many use cases. But, um, yeah, if you, if you work in finance, for example, then you really want to, uh, uh, uh, get maximum performance, and so that means that you need to really specialize for the use case, and you can do that through our platform. So we allow you to tune not just the language model or just the retriever, we allow you to tune the entire RAG pipeline for your specific problem. Uh, and so often that that leads to, uh, to pretty substantial, uh, performance improvements on top of already state-of-the-art out of the box performance.\n\nOkay. All right. So, it sounds like a lot of the secret to this is around the fine-tuning steps and the other steps around making sure the model quality is high.\n\nYeah, some of it is. So, it's also just about having very good components, right? So, for, for, uh, folks listening in on the podcast, they can also use these components themselves. We've made them available independently. So if you, uh, want to have a grounded language model that is designed for RAG and nothing else, so it's really specialized for RAG, then you can use that. If you want to use a reranker, um, uh, that is state-of-the-art and that can follow instructions, um, you can also do that, right? And so it's the same for our retrieval pipeline and for our, uh, document intelligence pipeline. So, all of the components that make up our state-of-the-art, uh, uh, RAG system, they're, they're available on their own as well. Uh, because we, uh, just like seeing what people build with them.\n\nInteresting. Uh, I'm curious as to how you measure what the benefits are. It seems like it's quite tricky to work out, um, what the quality of the responses are in general. So, how do you go about doing this?\n\nYeah, evaluation is such an important area and and so underappreciated, and so we, we do it in a variety of different ways. I mean, obviously we look at kind of standard benchmarks, right? And so like RAG QA arena is a benchmark, and and we are state-of-the-art on that, and so that's what we, um, care about too. Uh, but that doesn't even necessarily translate to real-world performance. Uh, so we, we look much more closely at our, our, uh, actual customer data sets and, um, sort of the process of UAT, user acceptability testing. So do people actually consider this answer to be correct and useful? Um, and that's ultimately what you care about, right? That's where the the ROI comes from. Um, so that's one thing. I, I think the other thing that's maybe worth, uh, mentioning here is that we have this, this framework around natural language unit testing, um, where, um, you, you're familiar with unit testing, right? It's, it's the what you do with code is like just step by step measuring small, small things about your code, small units and and then making sure that they are correct, and and you can use the same idea for language model responses where you can delineate very precisely what the characteristics are of a good answer. So, so you could say, okay, like it needs to mention this thing first, that needs to be in this particular style. It can absolutely not talk about this thing, and you know, whatever the characteristics are of the answer, you can write specific unit tests for. And so doing that gives you much, much richer signal than just looking at what everybody else is doing now with LLM as a judge models, which is just like, is this generated response equivalent to my ground truth response? And so maybe that makes sense if you're generating a few sentences, but if you're generating a long answer, then that's just not going to cut it.\n\nInteresting. So, uh, you want to be able to ask specific questions about the response and just check, does it match all of these different criteria, and it can do pass or fail for lots of different things then.\n\nExactly. But that's really great too, right? Because if you think about like a regulated industry, um, the, you have, you will have\n\n\nPeople in, like the model risk management department of the bank, and they need to write out what a good answer looks like and what cannot be a part of any answer, right? So if they can just write those unit tests in natural language, and then the system can test against that for every generation, that's something that will make regulators much happier, basically.\n\nBasically, yeah. I certainly, from my experience of creating software, when you write the test, it really helps clarify what you actually want the software to do. I'm sure it's the same situation with your large language models.\n\nYeah, test-driven development.\n\nYeah, absolutely. Cool. So I guess related to this is the idea of maintenance. So if your data is changing very rapidly, how do you go about maintaining your model just to make sure it's continually giving the right results?\n\nYeah. Uh, so I mean, what RAG is useful for is making sure that things will keep working on your data even as the data changes, right? That's part of the appeal of RAG. Um, so, um, I think you kind of do that automatically there. But one of the things that we're always thinking about is how can we make sure that we have the best components and continuously keep updating the best components to make sure that they're integrated into the overall pipeline. Um, and so that actually takes a lot of effort, right? Make just staying at the frontier of AI, which is moving so quickly. Um, that is a lot of work. Um, and so, um, yeah, that's exciting that we get to do things like that, right, as a broader kind of research community.\n\nOkay. So in theory, you shouldn't have to do too much maintenance as long as the systems in place and you test it once. So even if your data changes, it's still going to work.\n\nYeah. I actually, I, so I was at this, uh, I was at this Gartner conference, and it was data and analytics, and so everybody at the Gartner conference, and this is really not my crowd to be honest. These are not, not like, uh, you know, AI, uh, people. They are like real enterprise people, and it was really eye-opening for me to just be there and talk to all these amazing people and get their perspectives, which were very different from what I've been hearing everywhere. And the whole conference was just talking about, uh, making your data ready for AI, and I just felt like that's a massive copout, right? That you should not have to make your data ready for AI. You should make your AI ready for your data.\n\nRight. And so, so that in an ideal world, and I think where we're headed, is that you don't have to do anything to your data to make it work. You just have very good AI that works on top of that data, and you, and we just have to accept that data is noisy. And so that's why we have like this multi-stage retrieval pipeline. That's why we have this powerful re-ranker that can follow instructions. That's exactly because you need to make sense of noisy data and filter out the things that you don't want and make sure you get the things that you do want into the language model. Um, so, uh, yeah, in an ideal world, in the long run, I, I can't promise it now yet, but in the long run, you want to make sure that AI just works on your data, and you don't have to make your data AI ready.\n\nOkay, I think I just heard everyone listening in who works in data governance just gasp all at once.\n\nUh, just, just wait, don't waste your time preparing for, uh, this work in the near future. Okay. So do you have any advice for people who do work in data governance then? So if your company's making applications involves search or retrieval, what do data governance people need to do?\n\nBe very careful about your metadata. Um, so I, I think that's really one of the crucial parts of state-of-the-art modern RAG pipelines is making sure that you have high-quality metadata or annotations for your documents and database schemas or things like that. Um, so that is definitely still going to matter, having rich metadata. Um, and so, yeah, the other thing is really about, I think, thinking carefully through like your entitlements model, especially when you have multiple data sources that might be very desperate. So you, you know, we talked to a lot of companies that have like SharePoint and Confluence and like Jira and Slack and like Google Drive and all kinds of different things, and then they want to work on top of all of those. Um, but so getting that to actually work with a proper sort of role-based access control entitlements, uh, setup is not trivial at all. So that's one of the things that we are good at. But, uh, if you, if you have a more centralized way of dealing with that from a data governance perspective, uh, then you can save yourself a big headache.\n\nOkay. Yeah. So certainly I can see how lineage is incredibly important. Do you want to go into a bit more about the entitlements? Uh, it sounds as if different people have got different permissions on data, and it makes retrieval a bit complicated.\n\nYeah. So if you think about like real-world companies with like 100,000 employees, and not everybody has access to the same amount of data, um, and or so, right, some companies actually, from a regulatory perspective, have sort of hard fencing between departments. Um, yeah, so you need to make sure that you capture that and that you don't make any mistakes. Um, and so, uh, yeah, setting things up the right way for handling that at the scale of 100,000 people in a large company, um, that's a very interesting problem.\n\nSure. Yeah. So in that case, I guess you want the model to give different responses depending on who is asking the question because they're going to see different data.\n\nYeah. So like I always enjoy saying everything is contextual, and so you want to get that context into the model, and so for every user and every user interaction, even, um, the context should be different.\n\nOkay, nice. So I know handling permissions has been around forever in relational databases. I don't know what the status is with vector databases. So, is it easy to manage who gets access to what data, and like, is it as sophisticated as with relational databases?\n\nNo, it's not, it's not nearly as sophisticated. Uh, it's getting better, but it's not, not as sophisticated. And even with relational databases, I, I think it's actually not that easy. Um, so you can do it in like one, one sort of single, uh, database, but doing it across databases, um, uh, is also not easy.\n\nOkay. So is there a solution to this then?\n\nYeah. So the way we do it is we synchronize the data with our, um, sort of data store concept, which we talked about, right? And then, um, when we do the retrieval step, we validate using an entitlements API that sort of calls all the upstream APIs to make sure that the, that you still have access. Um, and so that obviously cannot always happen in real time, depending on your latency constraints, but there's some sort of synchronization, uh, step that happens in between there. And so, yeah, so the model is basically like you ingest, you do the retrieval over that index, kind of a complicated index, um, and then, uh, when you find the results, you validate that the results you found are actually accessible by the user using an entitlements API.\n\nAll right. So, if it sounds like one of those problems where it's going to be much better if someone else solves it for you, like it doesn't sound like the sort of fun thing you want to play around with yourself.\n\nThat's what I've been trying to tell people. Stop trying to DIY complex RAG systems. It's not worth your time. Just like you wouldn't build your own database or your own language model, like you shouldn't build your own RAG platform. You should be building RAG agents and applications on top of that platform to solve, uh, important business problems for your business.\n\nOkay. Yeah. So I mean that seems incredibly important. Uh, so it's like try and figure out where you can add the most value to solve problems to your business. Uh, can you just talk me through how do you go about thinking about this? Where should you be spending your time? Do you have any examples?\n\nYeah. So, so the differentiated value for a company is what companies are all about, right? Like your company wants to be better than your competition. That's sort of the goal of every company. And and so the more you can focus on that, um, and thinking about, okay, how can we, um, uh, use our data and how can we automate some of our processes or improve some of the things that we do, um, the more time you can spend on that and not on like the optimal chunking strategy, uh, or like how to make sure that your VLM doesn't continuously go down or like basic problems that you have to deal with on the sort of red plumbing side of things. Um, the less you have to worry about the plumbing, the more you can worry about actually solving real problems that add business value.\n\nYeah, that makes sense. Just really think about how can I actually make my business better and then try and get as close to just creating stuff on that level rather than worrying too much about the low-level infrastructure if possible. Okay. So there's something you mentioned a few times, uh, it's the idea of a re-ranker. It sounds quite technical, but I feel like it's one of those underleveled components of AI systems that isn't talked about much. So, do you want to explain what is a re-ranker and why would I need one?\n\nYeah, the re-ranker is super important. I think every modern RAG pipeline in production that I've seen has a re-ranker in it. And so, the idea is actually very simple. It's when you do retrieval, um, because you need to do retrieval over large amounts of data, you cannot have a very big model doing that. So, embeddings models are, are you cache the embeddings, right? It's relatively, uh, cheap compute. Maybe you do some like keyword search, um, and then you get a bunch of results. But now, because you had to do this at a very large scale, you will have made mistakes because you had to do it sort of quickly enough, right? So you couldn't have like a smarter model take a look at it. So that's what the re-ranker is. It's a smarter model that does a second pass at your initial retrieval results and says, actually, like this, I, you know, I can see why it was sort of relevant, but it's really not relevant, or you know, this one is actually super, super important for getting the right answer. Um, and so, uh, our re-ranker is, uh, state-of-the-art by a large margin. So, it's much better than other re-rankers out there, and it's the only one that can follow instructions, which is really important because now you can tell it what your, um, data hierarchy is, basically. So, again, at the level of the user or even of the individual query, you can say this is the priority of my retrieval results based on my preferences. So most recent first, um, if it is a PDF, then it's more true than if it's a Slack message, um, you know, if it comes from our internal wiki, or if the boss wrote it, then it's more true than if it's an intern, uh, document and it has draft in the title, right? The these types of rules for how to prioritize data and how to break conflicts in RAG pipelines, um, that's what you need an instruction-following re-ranker for. And, and so, uh, yeah, that's one of the things that we have in our platform, and that's one of the things that makes us better.\n\nOkay. Yeah. So I think the standard approach for retrieving information from a back-to-day space is just a simple dot product of like 19th-century maths. Uh, so I'm curious, you mentioned if you want to rerank information, uh, you got to have a set of rules on precedence. So is that something you need to write manually, uh, these precedence rules, or can they be automatically generated?\n\nYeah. So, so we usually have pretty good default rules in place, uh, for the re-ranker. And so that would be things like most recent first and kind of common sense, uh, rules. Um, so you don't have to come up with them yourself if you don't want to. You can also ask a language model based on your data what it thinks the rules should be. Um, um, but, uh, yeah, so you, you can do it yourself or you can just go with the defaults.\n\nOkay, nice. So again, this sounds like something where you could easily get very deep into the weeds and figure out what's the optimal strategy here, but it also feels like again, this could be tricky stuff.\n\nYeah. So if you have a very specific rank problem, and there's a very specific thing that needs to happen there, um, just because of the problem you're trying to solve, then you could try to solve it through the system prompt, but ideally, you solve it by prompting your retrieval, uh, pipeline, right? And so you obviously can't do that in your first stage retriever because it doesn't have the ability to follow any instructions, but you can do it in your second stage, uh, so in your re-ranker. So the tricky part here comes when, like, what happens if you've got information that conflicts with each other? So you're trying to, you give a problem that's trying to give a specific answer. So you pull some information from your knowledge base or whatever, and you've got two different results that give you different answers. Uh, talk me through how do you go about resolving this?\n\nSo I mean, it depends on the type of conflict. Uh, but you can tell it through instructions how how to deal with that conflict or how to break the tie, basically. So you could say most recent first. So if you find two documents and one is more recent, then you, uh, rank that one much higher, right? Or you could say different data sources. It's like I prefer this data source over that one. Um, so you can, you can just put that in the instructions too. And then there are, that's why again, why it's so important that you can prompt these components is because there are different, uh, strategies for dealing with conflicts. So one is you go for most recent, the other is, um, you report both results, but you say that one is more recent, right? So, uh, it really depends, and that's, that's why it's so important that you get control over those aspects of a RAG system.\n\nOkay, so it's good that there's some sort of way of resolving these conflicts. I'm curious, is there any way to feedback from when you have these conflicts, uh, to then put that into your data governance strategy?\n\nYeah. So, so we have, uh, some customers where, um, when the system, so one of our, our, uh, very special capabilities is our ability to say I don't know. Um, which is an underrated feature. It's much better to say I don't know than to make up a wrong answer, right? So, um, um, that's a really, really good thing. But then when you have an I don't know answer, ideally, you want to have the ability to annotate that answer so that the next time you get the\n\n\nQuestion, you do have relevant information in your documentation for resolving that question, right?\nUh, so that's one feedback loop, and then the other is, yeah, we collect feedback through our UI or through our APIs, and then you can export that feedback and actually train on that feedback.\nUm, and so that's how you can specialize it for the use case over time.\nJust keep making it.\nThis sounds pretty useful for creating the feedback that you get.\nStart off saying, okay, we've got some wrong answers, and then you give those wrong answers back to the people who actually curate the data or maintain it, and that's going to feed back into a better system for later on.\nYeah.\nSo that's over time at least how you can capture most of the distribution.\nObviously, the tail of the distribution, you're never going to fully be able to capture, uh, but you can fine-tune the system maybe to generally be better also at the tail.\nNow, we talked a little bit before about the idea of a RAG agent.\nSo I am curious as to whether RAG agents have a different architecture compared to other RAG applications.\nUh, yes and no.\nI guess we should talk a bit about what an agent is, right, because everybody's very confused.\nAbsolutely.\nUh, so I think an agent is a very general concept.\nI mean, so it comes from uh reinforcement learning, uh, or even it's maybe even older than reinforcement learning itself, where it's really just about like a policy that takes actions in an environment, and that policy has some sort of state, and so it can manipulate the environment, um, uh, but it doesn't necessarily have to manipulate the environment in order to be considered an agent.\nUh, so another way to put that's maybe maybe a bit closer to home for uh folks listening to this podcast is if you think about an agent that can do SQL queries.\nUm, some people will say, oh, but it's only an agent if it changes the environment, which would mean that it is only an agent if it generates insert queries or update queries.\nAnd if it just does select queries, then suddenly it's no longer an agent.\nSo I mean, when you explain it like that, it sounds a bit silly, right, as like, obviously that's still an agent, like deep research doesn't do any insert queries or update queries, but it does do a lot of useful stuff, right?\nSo, um, I, I think a much more useful definition of an agent is just something that actively reasons, um, so something that thinks about what it's doing, formulates a plan, executes on the plan, and then can revise that plan based on the information that came in.\nSo that's active reasoning.\nAnd so the really exciting uh technology that that has enabled all of this is just test time reasoning, and the insight that um that uh shifting the compute from the training side to the test time inference side, um, actually has very, very nice uh properties.\nOkay.\nYeah, I like that.\nSo I think uh with your explanation there, it was just introducing all the important bits of jargon from reinforcement learning, uh, just the idea of policies and environments and all those other things.\nIf you're interested in reinforcement learning, please just rewind and re-listen to that last minute or so.\nUh, okay, uh, I do like the idea uh that you mentioned uh with test time reasoning.\nSo you're doing reasoning at the time the user asks for it.\nUh, and then it can go and check things on its own and uh work through things step by step.\nYeah.\nNo, so I, I, I think like the the simple chatbot is a subset of the overall concept of an agent.\nUh, so you can just have a, you can have a chatbot agent, and it probably for most chatbot use cases, it doesn't have to do a lot of thinking.\nUm, so it, the same agent can also power the chatbot use case.\nIt is just more powerful.\nIt depends on how how much um you want to budget for test time compute.\nAnd so if you want to minimize that, then uh you probably um uh yeah, just have something like a standard chatbot.\nBut, but the the boundary is is is very blurry, right?\nBecause when I have a RAG application, which maybe is not really an agent, but as a part of my RAG step, I do query decomposition, and I formulate sort of a plan, and then I do some filtering on top.\nIt's sort of an agent, right?\nLike that that is what agents would do, but the agent sort of maybe determines that more dynamically on the fly, sort of what it would want to do.\nUm, but yeah, the the boundary is not as well defined, I think, as a lot of people like to pretend.\nUm, but so overall, I think so this really is the year of agents.\nThere's a lot of excitement about them.\nThey don't really work quite yet.\nI think a lot of people um are are sort of blown away by the potential, but like in practice, I haven't seen any real agent deployments that have like material impact on a company's business uh yet.\nI mean, I'm sure it's going to come, but it's going to take some time.\nUm, but these agents obviously they need to work on your data, too, right?\nJust like with what we were doing before with AI.\nIt's like, yeah, we need agents, but the agents need to work on your data.\nSo, they need to be RAG agents because RAG is the way that you get things to work on top of your data.\nUm, so, um, yeah, RAG is one of the tools that these agents need to rely on for them to be useful, especially in enterprise settings.\nAh, this is interesting.\nSo I think the big difference is that with this sort of standard rack approach, you've got some software saying, uh, okay, let's shove all the bits of information from the vector database into the prompt, and those kind of being pushed to the LLM.\nWhereas if you're doing things with um inference time reasoning, then the LLM has to ask for the information, just kind of pulling it in.\nUh, so there's a a swap, is that about right?\nYeah, that's right.\nSo, but again, right, it's not really like one or the other.\nIt's really a spectrum, like I think most modern RAG systems, they probably have some kind of classifier that says, should I retrieve or not, right?\nSo, and then based on that, you say, okay, so like if I say hello, then then you don't have to retrieve in your RAG chatbot, you just say hello back, right?\nSo, first thing is, you you need to, so that's already kind of active retrieval, where in the old RAG setup, where it's really passive retrieval, is you get a query, you always search for that query uh in your vector database.\nYou always give the results to the language model, like that's that's prehistoric at this point, right?\nSo it's much more complicated.\nThere's there's like active decision making involved in these RAG pipelines.\nThere's a lot of uh like filtering.\nThere's the reranker uh that has a huge impact.\nUm, there there's this sort of active retrieval component.\nThere's query decomposition, which is almost like formulating a a retrieval plan, like that that's all very agentic, but now when you have an agent, you can do that much more dynamically, and that's that's why that's so exciting.\nWell, okay.\nYeah.\nSo you got more flexibility about what the behavior is going to be uh rather than having a more rigid pipeline.\nYeah.\nExactly.\nAnd it can also like recover from its mistakes, which is very important.\nRight.\nSo if it retrieves something and it thinks, oh, actually this is not what I wanted, let me try a different query, and then it gets the result, that's very powerful, and that's something you can do with with uh agentic abilities.\nUm, and then uh the other thing is is more around sort of multihop questions or or like multi-step reasoning is like, like first I need to know this, and then I need to know that, and then I need to compare those things, and then maybe based on the result, do something else, right?\nThat that type of multihop uh problem, that that's I think very interesting uh uh in terms of business value as well.\nOkay.\nSo you mentioned that there haven't really been any deployments of agents so far that have wowed you.\nUh, so what do you think would wow you?\nI mean, they wowed me in terms of like seeing the potential, but they haven't wowed me in like, oh, this company has saved like $10 million this year because they had an agent doing something.\nI like a like a proper agent uh using test time reasoning and not something that was branded an agent, but that was something that we were already uh capable of doing before kind of uh reasoning models.\nSo, what do you think the sticking points are?\nWhere are we falling short?\nI think generalization sort of um um so so actually having it work in in real world settings where it's not in the the toy domain, right?\nSo getting things to work in a toy domain or in a nice demo, that that that used to be my sort of story around RAG is, and that's still very true, people think that RAG is easy because you can build a a nice RAG demo on a single document very quickly now, and it will be pretty nice.\nUm, but getting this to actually work at scale on real world data, uh, where you have enterprise constraints, it's a very different problem.\nAnd so it's the same with agents, where it's like, oh, I can make something like do this one particular thing when I prompt it and basically just make everything look good for exactly the one thing I wanted to do, but then when you actually have to make this work in in a real world setting, then then everything just breaks down very quickly still.\nSo that's going to get better over time, obviously.\nUm, and and so I think the hype is justified.\nUm, but yeah, it's going to take some time for for these systems to be enterprise grade enough for anybody to really deploy this uh in a in a critical setting.\nSo since a lot of companies are just thinking we got to get on the AI game, we got to build something.\nWhat can you build that is likely to work and to add value?\nYeah.\nSo, um, what you can build, um, so you can build basic solutions for the relatively uh boring problems.\nSo, um, one thing you can do is go for like the basic problems where you ask the basic questions, like internal search, right?\nBut that doesn't really get you value.\nIt's much more like trying to find um workflows that exist in your company that are a little bit boring, but that where it's important that you get it right and where it requires some expertise.\nUh, if you can solve those problems, then uh you can you can make your your uh company much better, right?\nAnd and so these could be very simple things from like, you know, checking for uh compliance against your set of policies\num or doing basic research.\nUh, we have a very nice demo where we fill out Excel spreadsheets on unstructured data on the fly so that you don't have to manually go and copy and paste.\nYou just directly in your Excel uh kind of call a macro and then fill out the spreadsheet with unstructured data from different data sources.\nUm, doing things like customer support, um, there there's a lot of um low-hanging fruit um in kind of the the codegen side of things.\nSo there a lot of it is happening kind of across the board, right?\nIt's just um um yeah, doing this this the right way takes time.\nSo there's there's a big gap between sort of where the hype cycle is and where like reality is in enterprises, but I mean it it's coming.\nUm, uh, it's it's just u yeah, it takes time.\nOkay.\nSo it sounds like the best approach then is to go for maybe slightly more narrow use cases where there's less flexibility needed, less generalization needed.\nI guess maybe all most build disposable agents.\nDoes if you can build something quickly, um, that just solve your problems and then be done, that works.\nI like I like that idea.\nYou can build disposable agents on our platform and maybe I should rebrand like that, the disposable agent platform.\nNice.\nUh, so just to wrap up, what are you most excited about in the world of AI?\nYeah, so I'm obviously very excited about all the agentic things.\nUm, I, I think for for me personally, where I I see a lot of very interesting problems is at the intersection of structured and unstructured data.\nUm, so you have a bunch of documents, but you also have your traditional uh kind of structured relational databases, uh, your Snowflake or your BigQuery or whatever you use, and now you want to kind of cross-sect that information using a gentic RAG.\nAnd so if you can do that, which you you can now uh start doing because of these agentic abilities, that unlocks so much interesting um potential.\nUm, so I think that's really exciting.\nThe other thing is multimodality is obviously still um very under explored.\nI think uh um every time there's like an image generation feature uh uh getting shipped, that kind of goes viral.\nBut um I think image understanding is actually much more valuable um from a from a kind of enterprise perspective.\nUm, so so I think that that's also a really uh key unlock that is coming soon.\nSo chart understanding, for example, and and things like that, understanding um, you know, a McKenzie slide deck that has lots of different diagrams and charts and things in there, like right now that's not really within the capabilities of these systems.\nUm, but it it's coming very quickly.\nOkay, I got to follow up on that.\nUh, so talk me through it.\nYou basically want the ability to understand a presentation, then just throw it a PowerPoint and explain what the output is.\nYeah.\nSo that's the simple case.\nSo the hard case is I have uh a 100 million PowerPoints that my company has made in the history of my company with a 100,000 people in my company.\nAnd now I want to answer questions based on all of that information.\nUh, so it's not just like one single PowerPoint, because that you can kind of start to do, even though it's not very accurate, but you need to do it at scale, right?\nSo over over lots of u uh presentations.\nSo if you can do that, then you can do very interesting kind of synthesis on top of it, right?\nSo how did our perspective on a particular type of thing change over the years?\nUh, and then you can just find the relevant presentation decks that cover this particular thing.\nLook at the charts, reason about the trends in the charts, and then combine that into a new insight.\nUm, that that's all kind of starting to become possible now.\nOkay, that would be very interesting.\nAlthough I have to say I've definitely had a few colleagues in the past where even with them talking over the PowerPoint, I am not sure what they've been on about.\nIt's similar to what everybody says, garbage in, garbage out.\nYou cannot be much better than your data.\nYou can try to reason about it intelligently, but there are limits there.\nAbsolutely.\nAbsolutely.\nWonderful.\nSo all right.\nUh, since you've actually been involved in RAG since the beginning, I mean, you were there when RAG was created as part of the uh team at Meta.\nUh, so do you think RAG has lived up to your expectations?\nHas it panned out as you expected?\nThat's a nice question.\nUm, I mean, I, I think that the the original vision was always that we would have um kind of a decoupling between the knowledge and the reasoning, where the reasoning is really just like taking whatever the relevant knowledge is and then uh giving the right answer on top of\n\n\nIt without, um, um, having any of of the\nknowledge in it itself.\nSo that didn't really pan out, and, and so, um, that's part of the reason why these systems hallucinate, and, and that, that's, um, I mean, it's a longer story, but, but so for for RAG, I think when the paper came out, uh, it was very focused on open domain question answering, which is the the domain that you evaluate these systems on, um, and so it was well-received, but at the time Gen AI, like vector databases, basically didn't really exist, right?\nThat became a thing after uh, the paper, uh, language models didn't really exist.\nWe had like Bart and T5, but there there wasn't really a concept of like an auto regressive uh, generative model.\nSo I think like the reason RAG became such a popular like paradigm and concept, um, and why it's called RAG is because because of the G.\nSo it's really just because GenAI became a thing that RAG became a thing.\nUm, and there are lots of papers from around that same time.\nAnd there's this amazing paper from folks at Google called Realm, where that didn't become the name of the paradigm because it didn't have a G in it.\nIt was a a mass language model, right?\nSo, um, yeah, hindsight is sort of 2020.\nIt is amazing how small changes to the name have a big difference on your success or not.\nI mean it was it's not just the name, right?\nIs is that we were interested in trying to see if you could generate the answer without uh sort of, so the alternative is just predicting the answer, which is uh what was much more normal to do at the time.\nSo I guess we were ahead of our time in the right way there.\nWonderful.\nYeah, certainly, I mean, uh, it's taken over in so many different ways.\nUh, it's ubiquitous now.\nUh, so obviously very successful, and uh, just finally, I want ideas for people to follow.\nSo whose work are you most interested in at the moment?\nWhose work am I most interested in?\nUm, I think there there is a,\num, a lot of interesting work happening in this new testime compute paradigm.\nSo I, I mean, I still have my kind of part-time Stanford adjunct professor uh, gig, which is great for me to kind of uh, stay, stay at least a little bit up to date on the latest latest research trends.\nUm, and I, I think there's just a lot of interesting research happening around this testime reasoning and, and what you can do there.\nI think we've only just scratched the surface.\nUm, and, and so, uh, what happened with Deep Seek and things like that has been very encouraging, I think, from that perspective, where it's actually not that hard for, um, for like nonfrontier le, um, folks to to do interesting things in this space and, and have impact.\nUm, so, um, yeah, and I, I follow a lot of uh, just smart academics.\nUh, yeah, smart academics always worth following, I think.\nUh, very good genre of people to uh, to watch out for.\nUh, so all right, uh, thank you so much for your time to\nthanks for having me.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.123Z"
}