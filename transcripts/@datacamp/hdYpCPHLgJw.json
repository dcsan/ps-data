{
  "episodeId": "hdYpCPHLgJw",
  "channelSlug": "@datacamp",
  "title": "#300 End to End AI Application Development with Maxime Labonne & Paul-Emil Iusztin",
  "publishedAt": "2025-05-05T13:10:35.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Welcome to dataf framed. This is Richie.",
      "offset": 0.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "AI engineer is perhaps the hottest role",
      "offset": 2.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "right now. But just like the data",
      "offset": 4.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "scientist role, there's more to it than",
      "offset": 6.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you can possibly learn in a lifetime.",
      "offset": 8.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "That means it's really several related",
      "offset": 10.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "jobs covered with a single umbrella",
      "offset": 12.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "title. Today, I'm chatting to two AI",
      "offset": 14.799,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "engineers with very different jobs.",
      "offset": 17.199,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Maxim Leon is a senior staff machine",
      "offset": 19.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "learning scientist at foundation model",
      "offset": 21.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "startup Liquid AI where he focuses on",
      "offset": 23.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "post-training of LLMs. He's created many",
      "offset": 26.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "popular LLMs including Alpha Monolic and",
      "offset": 29.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Neural Beagle as well as developer tools",
      "offset": 32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "for automating LLM pipelines. Paulie is",
      "offset": 34.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "a senior machine learning engineer at a",
      "offset": 37.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "stealth AI startup and the founder of",
      "offset": 40.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Decoding ML. His work focuses on",
      "offset": 42.559,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "pipelines evaluation and deployment of",
      "offset": 45.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "AI. The two of them joined forces to",
      "offset": 48.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "write the LLM engineers handbook on",
      "offset": 50.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "which this is based and I've",
      "offset": 52.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "cherrypicked the most interesting",
      "offset": 54.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "chapters to talk about today. So let's",
      "offset": 56.079,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "find out how to be an AI engineer. How",
      "offset": 58.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "personally I see the future is that",
      "offset": 60.879,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "we'll be mostly focused on thinking and",
      "offset": 63.039,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "planning soling solutions or other",
      "offset": 67.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "creative processes. we will we will",
      "offset": 70.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "automate a lot of these boring mundane",
      "offset": 72.479,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "tasks and it will let us actually be",
      "offset": 75.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "more more human. A lot of people try to",
      "offset": 78.72,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "apply fine-tuning to problems where it",
      "offset": 81.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "might not be the best solution. But in a",
      "offset": 83.759,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "lot of situations, you can get away with",
      "offset": 85.92,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "like few shot uh prompting by providing",
      "offset": 88.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "a few examples to the model. Uh",
      "offset": 91.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "everything related to a right pipeline",
      "offset": 93.84,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "to retrieve the context and include it",
      "offset": 96.479,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "in um in the prompt.",
      "offset": 99.759,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Hi Paul and Maxim. Welcome to the show.",
      "offset": 103.36,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "Hello Richie. Excited to be here. Hey",
      "offset": 105.68,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Richie, thanks for the invitation.",
      "offset": 109.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Wonderful. So, uh, I'm curious as to",
      "offset": 111.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "what you think people misunderstand",
      "offset": 114,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "about AI application development. What",
      "offset": 115.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "do people get wrong about it? Okay, so I",
      "offset": 117.92,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "think that, um, in my field, a common",
      "offset": 120.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "misconception is about fine-tuning and",
      "offset": 122.799,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "how when you can apply it and its",
      "offset": 125.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "effectiveness in general. I think that a",
      "offset": 128.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "lot of people try to apply fine-tuning",
      "offset": 131.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to problems where it might not be the",
      "offset": 133.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "best solution and it's understandable",
      "offset": 135.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that you want to customize your LM",
      "offset": 139.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "because you have your specific use",
      "offset": 141.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "cases, you have your specific data but",
      "offset": 142.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "in a lot of situations you can get away",
      "offset": 145.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "with like few shot uh prompting by",
      "offset": 148,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "providing a few examples to the model uh",
      "offset": 150.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "everything related to a right pipeline",
      "offset": 153.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "to retrieve the context and include it",
      "offset": 156.16,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "in um in the prompt. Um all these",
      "offset": 159.36,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "techniques tend to be like really good",
      "offset": 162.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "for a lot of stuff that people want to",
      "offset": 164.319,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "do, but a lot of them try to directly go",
      "offset": 166,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh to fine-tuning, which is never really",
      "offset": 169.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the best option unless you really have",
      "offset": 172,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to go through finetuning. So yeah, to me",
      "offset": 174,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "this is the main misconception on my",
      "offset": 176.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "side. Okay, that feels like a very good",
      "offset": 179.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "news story because there are some very",
      "offset": 181.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "simple things you can do. So you",
      "offset": 182.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "mentioned the idea of few shot",
      "offset": 184.159,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "prompting. It's like it's a relatively",
      "offset": 185.28,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "cheap technique. Uh so you don't need to",
      "offset": 186.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "go all the way to fine-tuning uh in",
      "offset": 188.879,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "order to get customized model behavior I",
      "offset": 190.64,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "guess. All right. And and Paul, what uh",
      "offset": 192.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "people get wrong in your side of things?",
      "offset": 194.959,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "Well, on my side of things, I guess uh",
      "offset": 197.2,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "the most common misconception is about",
      "offset": 200.4,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "using frameworks to build this like more",
      "offset": 203.56,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "on the inference side of application. So",
      "offset": 207.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we all know about lime chain, l index",
      "offset": 209.76,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "and all of that. And I seen that mostly",
      "offset": 212.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "everyone start with them and they think",
      "offset": 216.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "that's usually like what takes to build",
      "offset": 218.319,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "the um application. But in most cases",
      "offset": 221.12,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "usually you hit some threshold where you",
      "offset": 225.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "cannot move forward because uh from what",
      "offset": 228,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "I seen these frameworks are mostly like",
      "offset": 231.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "a very low code solution to getting into",
      "offset": 234.879,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "rag and agents and all of this and it's",
      "offset": 238,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "like similar to no code solutions.",
      "offset": 241.439,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "they're quite rigid and you need when",
      "offset": 244.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "you need custom stuff you usually hit uh",
      "offset": 246.239,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "some some ceiling that you cannot pass.",
      "offset": 250.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "And what I recommend is always like when",
      "offset": 252.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "you need to interact with the database",
      "offset": 255.599,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "like basically ingest data and uh",
      "offset": 258.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "embedded uh data into your database and",
      "offset": 261.759,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "retrieve it. Just start writing it from",
      "offset": 264.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "scratch from day zero instead of",
      "offset": 267.759,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "starting with these frameworks. And I I",
      "offset": 270.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "believe that these framers are only good",
      "offset": 273.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like for very quick concepts to see that",
      "offset": 274.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "it's worth it to start writing it from",
      "offset": 277.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "scratch and the idea and the data is",
      "offset": 279.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "manageable and workable. Okay. So at the",
      "offset": 282.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "moment the frameworks are only good for",
      "offset": 284.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "very simple projects and you want to go",
      "offset": 286.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "a bit low level with your code if you if",
      "offset": 289.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you need to write something a bit more",
      "offset": 290.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "uh sophisticated. Yeah. Exactly. because",
      "offset": 292.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "most of the time you need like very",
      "offset": 295.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "custom filters, very custom ways to",
      "offset": 298,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "store your data. Uh I don't know custom",
      "offset": 300.8,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "ways to preprocess postprocess your data",
      "offset": 303.919,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "and you will realize that if to",
      "offset": 306.84,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "customize lchain or laidback or similar",
      "offset": 309.8,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "uh frameworks only takes longer uh in",
      "offset": 312.96,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "the long run than just writing it from",
      "offset": 316.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "scratch. But I I want to highlight that",
      "offset": 318.84,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "there there's like the other dimension",
      "offset": 321.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to this like more agentic workflow",
      "offset": 324.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "related frameworks like langraph and",
      "offset": 327.52,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "lame index workflows which realized uh",
      "offset": 329.44,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "these issues because they're made like",
      "offset": 333.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "from the same people and they're mostly",
      "offset": 335.039,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "used to orchestrate like your logic your",
      "offset": 337.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "steps to orchestrate them to monitor",
      "offset": 341.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "them to deploy them and that's a way to",
      "offset": 343.84,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "go because they don't limit in how you",
      "offset": 347.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "process your data, how you manage your",
      "offset": 350.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "data, how you integrate uh your",
      "offset": 352.4,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "workflows or agents with your current",
      "offset": 354.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "infrastructure. So yeah, these are two",
      "offset": 357.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "sides of the coin.",
      "offset": 359.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So things like langraph, yes, things",
      "offset": 361.24,
      "duration": 7.799
    },
    {
      "lang": "en",
      "text": "like lang chain, no. All right. Uh okay.",
      "offset": 364.4,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "So maybe we can get into text into more",
      "offset": 369.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "detail, but it sounds like you'll really",
      "offset": 371.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "think about which pieces of technology",
      "offset": 373.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "you're using when you're building stuff.",
      "offset": 375.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "All right. So, I'd love to talk a bit",
      "offset": 377.36,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "more about like what your roles are cuz",
      "offset": 379.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "you're both kind of normally AI",
      "offset": 380.479,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "engineers, but it feels like your jobs",
      "offset": 382.16,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "are very different. So, I'd love to hear",
      "offset": 383.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about like what exactly it is you do.",
      "offset": 385.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Maybe Paul, do you want to go first this",
      "offset": 387.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "time? So, my background is more from a",
      "offset": 389.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "software engineering, ML engineering",
      "offset": 392.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "side of things. So, I'm like used to",
      "offset": 394.319,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "trading models, but it's not like my uh",
      "offset": 398,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "bread and butter part of the job, let's",
      "offset": 400.56,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "say. Uh so my side of AI engineering",
      "offset": 402.72,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "because it's a very vague term is mostly",
      "offset": 407.28,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "related to like taking models from the",
      "offset": 410.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "researching from the training theme and",
      "offset": 413.039,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "integrate that into like bigger systems",
      "offset": 415.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "which can be things like uh more",
      "offset": 418.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "analopsy like putting them on the right",
      "offset": 421.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "infrastructure to scale well to uh heat",
      "offset": 424,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "requirements like cost latency",
      "offset": 427.199,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "throughput uh and",
      "offset": 429.36,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "So this is one side of the uh one aspect",
      "offset": 434,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "of it and another aspect of it is like",
      "offset": 436.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "integrated into the code itself because",
      "offset": 439.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "most of the time you need to pre-process",
      "offset": 442.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "your data between putting it into the",
      "offset": 444.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "model and the reality is that how the",
      "offset": 446.479,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "data is prep-processed in the research",
      "offset": 449.039,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "part of things most of the time is not",
      "offset": 451.599,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "like production ready for integrating",
      "offset": 455.479,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "into the code itself. So most of the",
      "offset": 458.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "time you need to rewrite it to again",
      "offset": 461.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "think about the requirements and also we",
      "offset": 464.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "have to think about like the training",
      "offset": 467.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "surface skew how to manage your",
      "offset": 469.199,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "pipelines to",
      "offset": 472,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "actually not introduce differences",
      "offset": 474.199,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "because only small parts of your data",
      "offset": 476.879,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "are like preprocess different from how",
      "offset": 480.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the models were trained and how it's",
      "offset": 482,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "deployed things most of the time will",
      "offset": 483.68,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "not go well",
      "offset": 486.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "so that's why like I think the the MLOps",
      "offset": 488.199,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "world with the more engineer world merge",
      "offset": 491.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "really well together because both have",
      "offset": 494.56,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "to solve this issues how you deploy your",
      "offset": 496.96,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "model how you uh I know design architect",
      "offset": 499.8,
      "duration": 8.679
    },
    {
      "lang": "en",
      "text": "your architecture to have uh to make",
      "offset": 504.08,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "your features reproducible to make your",
      "offset": 508.479,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "features sharable to make your feature",
      "offset": 510.479,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "accessible at training and certain",
      "offset": 513.279,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "uh like did this part of my job and on",
      "offset": 517.36,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "the other side uh another aspect of it",
      "offset": 520.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is more related to like workflow and",
      "offset": 523.599,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "agents development. So I think that most",
      "offset": 525.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "more on the like this combination of",
      "offset": 529.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "engineering mlots software engineering",
      "offset": 531.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "side of things you'll get into",
      "offset": 534.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "frameworks like as I said like graph",
      "offset": 536.399,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "where you need to orchestrate all sorts",
      "offset": 539.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "of agents to glue them together to",
      "offset": 541.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "implement right pipelines and uh yeah",
      "offset": 544.16,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "and this these aspects of engineering.",
      "offset": 547.04,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "So to",
      "offset": 550.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "conclude, I don't do that much",
      "offset": 551.64,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "fine-tuning, I guess.",
      "offset": 554,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "But it's interesting that a lot of the",
      "offset": 556.56,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "problems you mentioned, they're very",
      "offset": 557.839,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "familiar. I think like coming from a",
      "offset": 559.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "data science background where it's like",
      "offset": 560.959,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "you think you're going to be spending a",
      "offset": 562.56,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "lot of time messing about models, but",
      "offset": 563.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "actually like a lot of the problems are",
      "offset": 565.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "just dealing with data quality issues,",
      "offset": 566.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "trying to figure out how to get from",
      "offset": 568.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "prototype to putting stuff into",
      "offset": 570.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "production. So uh very familiar issues",
      "offset": 571.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there that you're dealing with. Exactly.",
      "offset": 574.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "But I will also act that because of this",
      "offset": 576.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the scale that AI engineering and LLM",
      "offset": 578.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "introduced because usually here you need",
      "offset": 581.36,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "to work with bigger models and bigger",
      "offset": 583.44,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "scales. You also have the dimension of",
      "offset": 587.399,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "data engineering and software",
      "offset": 590.08,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "engineering. So I think this role is a",
      "offset": 592.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "little bit tricky because you need to",
      "offset": 594.959,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "really know a little or more of of",
      "offset": 597.44,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "everything. And and Maxim uh tell us",
      "offset": 600.72,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "about your role. Yeah. So I have a like",
      "offset": 603.519,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "quite unique role uh as um u working",
      "offset": 607.6,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "post training for an LLM provider. Um so",
      "offset": 610.56,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "in my job what I do is like I fine-tune",
      "offset": 613.839,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "base models to create general purpose u",
      "offset": 617.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "models like a bit like JPT you can ask",
      "offset": 620.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "it pretty much anything right uh that's",
      "offset": 623.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "not really what people do in companies",
      "offset": 626.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "in my previous role I was more a machine",
      "offset": 628.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "learning scientist where the goal was to",
      "offset": 630.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "work with someone like Paul um I would",
      "offset": 633.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "train the models and I would uh give the",
      "offset": 636.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "baby to Paul so he could deploy it and",
      "offset": 639.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and improve the code also in general",
      "offset": 642.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "like as Paul said the code that we we",
      "offset": 645.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "tend to to do to preprocess the data is",
      "offset": 648.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "not production ready uh that's that's",
      "offset": 650.64,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "Paul being nice to say it's a Jupyter",
      "offset": 653.279,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "notebook I can't reuse it um I try to be",
      "offset": 656.839,
      "duration": 8.281
    },
    {
      "lang": "en",
      "text": "kind thank you Paul um I think that you",
      "offset": 661.04,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "have in terms of tech roles you have",
      "offset": 665.12,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "these two two very strategic positions",
      "offset": 667.279,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "of uh having machine learning scientists",
      "offset": 671.279,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that are responsible for the model and",
      "offset": 673.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the goal is really to get the data, make",
      "offset": 676.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "a good model, evaluate it, make sure",
      "offset": 679.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that everything works and then you have",
      "offset": 681.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "the machine learning engineer who's also",
      "offset": 684,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "responsible for a lot of breath",
      "offset": 686.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "actually. they have to cover a lot of",
      "offset": 688.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "things uh from data processing to",
      "offset": 690.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "deployment to inference optimization and",
      "offset": 693.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "usually it goes like really well uh",
      "offset": 696,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "together uh in my experience and I think",
      "offset": 698.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "that with large language models we see",
      "offset": 701.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "exactly the same kind of dynamics it's",
      "offset": 704.079,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "just that the knowledge um the tools",
      "offset": 706.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "they they changed a bit the scale also",
      "offset": 709.92,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "changed because now you don't have um to",
      "offset": 712.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "deploy hundreds of machine learning",
      "offset": 716.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "models every week. Um, but you have to",
      "offset": 718.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "deploy one or two, but they're very big",
      "offset": 721.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and you need to make sure that they",
      "offset": 723.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "really work. So, it's switched a bit",
      "offset": 724.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like some kind of focus in what's",
      "offset": 726.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "important in these jobs, but",
      "offset": 728.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "fundamentally we still have the same",
      "offset": 730.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "dynamics. Okay. So, that's interesting",
      "offset": 732.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that um your jobs feels like pretty",
      "offset": 735.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "closely related to the more traditional",
      "offset": 737.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I say traditional but like from the last",
      "offset": 739.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "decade or so machine learning scientist",
      "offset": 741.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "role. So it's just like yeah make sure",
      "offset": 743.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "the model is sort of high quality and",
      "offset": 745.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "then hand it off to someone in",
      "offset": 747.279,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "engineering to go and put it in",
      "offset": 749.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "production. Okay. I think between you uh",
      "offset": 750.519,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "you have the sort of full like flow of",
      "offset": 753.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "uh creating AI applications. Uh just to",
      "offset": 756.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "make sure the audience understands like",
      "offset": 758.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "what are the different sort of steps in",
      "offset": 760.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "order like at a high level for creating",
      "offset": 762,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "an AI application like what's where do",
      "offset": 764.32,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "you start and what's the end point? I",
      "offset": 766.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "think that the the main part is",
      "offset": 769.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "understanding the problem really well",
      "offset": 771.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like uh with any project right you need",
      "offset": 773.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to really understand the outcome you",
      "offset": 775.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "need to understand the inputs the",
      "offset": 776.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "outputs what we are trying to do really",
      "offset": 778.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "here because sometimes people will tell",
      "offset": 780.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you okay I want to fine-tune this model",
      "offset": 782.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "on this data set but actually this will",
      "offset": 785.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "not solve their problem so you can do it",
      "offset": 787.279,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "but then they're going to tell you no",
      "offset": 788.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "actually um so understanding the problem",
      "offset": 791.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and scoping it well is probably the most",
      "offset": 793.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "important step and then we have like",
      "offset": 796.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "different step of data collection, data",
      "offset": 799.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "generation if needed to create um a data",
      "offset": 802.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "set. It doesn't have to be a fine tuning",
      "offset": 805.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "data set. It can also be a data set for",
      "offset": 807.92,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "rag. It can be also a data set for",
      "offset": 810.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "evaluation. Um it just like broadly",
      "offset": 812.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "speaking data set creation is probably",
      "offset": 814.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the the first step. If needed, you have",
      "offset": 817.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "a step of LLM training um where you're",
      "offset": 819.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "going to fine-tune the model um on a",
      "offset": 823.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "specific data set. After that, you",
      "offset": 826.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "probably want to have an evaluation",
      "offset": 828.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "framework. It's often better to start",
      "offset": 831.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "with the evaluation framework even",
      "offset": 834.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "before you generate the data to do the",
      "offset": 836.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "finetuning because you know what you",
      "offset": 838.8,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "optimize for. You're probably going to",
      "offset": 840.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "iterate a lot of times over the",
      "offset": 842.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "evaluation data set. Um so it's good to",
      "offset": 845.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "start early. Once you evaluated the",
      "offset": 847.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "model um I think I I can give it to Paul",
      "offset": 850.48,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "but then you are going into like rag um",
      "offset": 853.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "deployment monitoring and testing. Paul",
      "offset": 857.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "do you want to continue? Yeah I will not",
      "offset": 860,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "continue like directly but on the on the",
      "offset": 862.639,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "same page let's say so what I like to do",
      "offset": 865.68,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "is like al again more on the engineering",
      "offset": 870,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "side is to think carefully about the",
      "offset": 872.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "requirements. So one part of the",
      "offset": 876.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "requirements is the data which might",
      "offset": 878.48,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "seem already uh said but other aspect of",
      "offset": 880.079,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "any software application in the end are",
      "offset": 885.279,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the requirements of like latency",
      "offset": 888,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "uh throughput costs and infrastructure.",
      "offset": 891.279,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "So I I usually like to start laying down",
      "offset": 894.639,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "an overview of the infrastructure and",
      "offset": 898.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "thinking about the flow of the data, how",
      "offset": 900.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "how it will run through your",
      "offset": 903.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "infrastructure, where it will be stored,",
      "offset": 905.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "how the components will be connected to",
      "offset": 908,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "each other. So people can really",
      "offset": 910.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "understand how this would work in",
      "offset": 912.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "production because one thing is to work",
      "offset": 914.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "like on your internal clusters where",
      "offset": 917.36,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "everything is is like stitched together",
      "offset": 920.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "with your own internal scripts and all",
      "offset": 923.279,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of that. And another thing is to have a",
      "offset": 925.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "automated workflows where everything is",
      "offset": 929.519,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "basically working automatically with",
      "offset": 932.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "DevOps pipeline or whatever other",
      "offset": 935.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "pipelines you implement. So this is",
      "offset": 937.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "another important aspect of it. And one",
      "offset": 940.16,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "thing that I usually preach",
      "offset": 942.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "is to start really small in the",
      "offset": 944.92,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "beginning. So to have an end to end uh",
      "offset": 949.04,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "workflow that works like with your",
      "offset": 953,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "minimum features basically an MVP let's",
      "offset": 955.279,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "call it uh without trying to be smart",
      "offset": 958.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "without trying to do complicated things",
      "offset": 961.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "and all of that but you you see that",
      "offset": 963.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "everything is working end to end and",
      "offset": 966,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "then you start using the evaluation uh",
      "offset": 968.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "data sets to evaluate how your",
      "offset": 971.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "application works on your benchmark.",
      "offset": 973.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Basically we we take the data set that",
      "offset": 976.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Maxim told us about and test the whole",
      "offset": 978.48,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "application on it. Sometimes this works.",
      "offset": 981.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Sometimes the data set needs some",
      "offset": 984.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "adaptation because it's one thing to",
      "offset": 986.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "evaluate your LLM uh in isolation and",
      "offset": 988.72,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "another thing to evaluate your whole",
      "offset": 991.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "application. But yeah the idea is that",
      "offset": 995.079,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "you want an end to end workflow that",
      "offset": 998.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "works. You want your evaluation to be",
      "offset": 1000.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "really in place and based on the",
      "offset": 1003.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "evaluation you have a feedback loop and",
      "offset": 1005.36,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "based on that you can start exploring",
      "offset": 1008.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "new features adding more complexity and",
      "offset": 1011.279,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "all that. You can think about it like",
      "offset": 1014,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "integration or unit test in the end like",
      "offset": 1016.92,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "you you keep adding stuff on top of it",
      "offset": 1019.759,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and on every new feature you test it and",
      "offset": 1022.079,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "if the test pass it's okay let's push",
      "offset": 1025.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that feature if not uh that we keep to",
      "offset": 1027.6,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "work on it. Okay that's very cool. It",
      "offset": 1031.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "seemed like a common thing between the",
      "offset": 1033.679,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "two of you is that you need to like plan",
      "offset": 1034.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "what you're doing beforehand. It's I",
      "offset": 1036.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "always find this very disappointing. I",
      "offset": 1038.24,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "want to like dive in and start writing",
      "offset": 1039.6,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "code and things but actually yeah",
      "offset": 1040.959,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "working out what the problem is you're",
      "offset": 1042.16,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "trying to solve beforehand a very useful",
      "offset": 1043.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "idea. Okay. So uh all this sort of",
      "offset": 1046.6,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "workflow uh I have to say you've cod it",
      "offset": 1049.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "really well in your LLM engineers",
      "offset": 1050.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "handbook. Um I'm amazed how you got like",
      "offset": 1052.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the full workflow in in a in a",
      "offset": 1055.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "relatively concise book. Um I'd love to",
      "offset": 1057.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "go into like some of the chapters in",
      "offset": 1061.12,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "more detail because there are a few",
      "offset": 1062.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "parts of this where a bit of the",
      "offset": 1063.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "workflow I wasn't particularly familiar",
      "offset": 1065.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with. So you have a whole chapter on uh",
      "offset": 1066.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "something called uh the rag features uh",
      "offset": 1070,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "pipeline. So tell me what is a rag",
      "offset": 1072.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "feature pipeline? Basically you can at a",
      "offset": 1075.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "very high level overview you can split",
      "offset": 1079.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "your machine learning system in four big",
      "offset": 1081.36,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "types of",
      "offset": 1084.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "pipelines. Uh the feature pipeline, the",
      "offset": 1086.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "training pipeline, the inverse pipeline",
      "offset": 1089.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and the observability pipeline. I know",
      "offset": 1091.44,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "for me personally the name are quite",
      "offset": 1094.559,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "uh self-explanatory but uh I will dive",
      "offset": 1097.919,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "deeper into the feature pipeline. Uh so",
      "offset": 1101.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "this one basically capes",
      "offset": 1104.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "row data which is usually which usually",
      "offset": 1107.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "comes out of your data pipelines which",
      "offset": 1110.16,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "can be implemented by your data",
      "offset": 1112.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "engineering team or smaller projects",
      "offset": 1113.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "implemented still by the AI ML",
      "offset": 1116.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "engineering team. But the idea that what",
      "offset": 1119.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "comes out of these data pipelines is",
      "offset": 1121.84,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "usually clean standardized data to some",
      "offset": 1124.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "extent and the feature pipeline takes",
      "offset": 1126.919,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "this information and transform it",
      "offset": 1130.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "transforms it into features right",
      "offset": 1132.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "because it's called a feature pipeline",
      "offset": 1134.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and because you're implementing a rag",
      "offset": 1137.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "feature pipeline these features are uh",
      "offset": 1139.52,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "collections that are used for RAB right",
      "offset": 1143.36,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "so what we do here is basically we",
      "offset": 1146.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "further clean the data if we think it's",
      "offset": 1149.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "necessary for our",
      "offset": 1151.6,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "specific redu use case and we also chunk",
      "offset": 1153.48,
      "duration": 8.6
    },
    {
      "lang": "en",
      "text": "it embedding or or do other advanced RAM",
      "offset": 1158.72,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "related things uh before indexing and",
      "offset": 1162.08,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "loading it into a vector DP usually but",
      "offset": 1165.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "this is like a standard use case we can",
      "offset": 1169.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "of course dig more into this because rag",
      "offset": 1171.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "is not only about vector DPS in the end",
      "offset": 1175.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "rag is about storing your data",
      "offset": 1177.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "in a proper database that can help you a",
      "offset": 1180.08,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "your problem and retrieve it as smooth",
      "offset": 1184.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "as possible so you have access to the",
      "offset": 1187.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "right context when you want to generate",
      "offset": 1189.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "answers. So, I'm curious as to what a",
      "offset": 1191.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "feature might look like. Uh, if you've",
      "offset": 1194.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "got um a text document, then what are",
      "offset": 1196.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the features you're pulling out of this?",
      "offset": 1198.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Um, can you give us a a concrete",
      "offset": 1200.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "example? Like a very basic example is",
      "offset": 1202.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "that you have a text document, right?",
      "offset": 1205.44,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "And within this text document, you",
      "offset": 1208.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "usually have multiple entities. And the",
      "offset": 1211.039,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "idea is that during generation, so when",
      "offset": 1214.24,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "a a",
      "offset": 1217.12,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "user hits a question into the chat and",
      "offset": 1218.679,
      "duration": 8.441
    },
    {
      "lang": "en",
      "text": "wait for an answer, you don't want to",
      "offset": 1223.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "pass to the LLM the whole document",
      "offset": 1227.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "because most LLMs are like biased when",
      "offset": 1229.12,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "you uh hit context that are that is not",
      "offset": 1232.559,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "relevant. for",
      "offset": 1236.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "example, it can get confused and start",
      "offset": 1237.88,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "giving answers on context that is not as",
      "offset": 1241.039,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "I said relevant to your to your uh",
      "offset": 1244.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "question and then uh in this use case is",
      "offset": 1247.12,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "usually have hogenation happens right uh",
      "offset": 1250.08,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "so we can dig into this later on but on",
      "offset": 1253.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "how a feature looks like so you have a",
      "offset": 1257.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "big document which can be from like one",
      "offset": 1259.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "page document to hundreds of pages",
      "offset": 1262.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "document so the core Core idea is to",
      "offset": 1264.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "split this document into entities that",
      "offset": 1267.12,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "are like make sense on their own and",
      "offset": 1270.24,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "take usually you want to to be as small",
      "offset": 1274.799,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "as possible but still relevant. So like",
      "offset": 1278.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "when you take this uh uh chunk as we",
      "offset": 1280.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "call it it makes sense on its own",
      "offset": 1283.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because in the end we will pass this",
      "offset": 1286,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "chunk to the ln to generate an answer.",
      "offset": 1288.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So it's like if it's fragmented",
      "offset": 1290.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "information then uh yeah it will be",
      "offset": 1292.48,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "useless or the answer we will be like",
      "offset": 1296,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "not complete. So the trickiest part",
      "offset": 1299.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "actually in rag is to take these",
      "offset": 1301.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "documents and split them to create this",
      "offset": 1303.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "uh packets of information that are",
      "offset": 1307.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "really relevant on their own. And after",
      "offset": 1309.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "we take this, we split the documents",
      "offset": 1312.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "into these packets, these chunks. We",
      "offset": 1314.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "embed them and store them into a vector",
      "offset": 1317.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "DB. And why we need to embed them is",
      "offset": 1320.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "because uh the most popular technique",
      "offset": 1323.039,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "for rag is semantic search where you",
      "offset": 1326.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "basically do uh the cosine similarity",
      "offset": 1328.559,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "between two vectors which uh intuitively",
      "offset": 1331.679,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "find semantic similarities between two",
      "offset": 1335.6,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "vectors. So basically using this",
      "offset": 1338.48,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "embeddings you embed your data in the",
      "offset": 1341.64,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "factor database which is indexed on this",
      "offset": 1345.28,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "vectors and you embed your query. So",
      "offset": 1348.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "instead of doing like classic searches",
      "offset": 1351.039,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "into a vector database, you uh use this",
      "offset": 1352.799,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "embedding this embedded query to index",
      "offset": 1355.88,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "the vector index to query the vector",
      "offset": 1359.72,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "index and find the most similar vectors",
      "offset": 1362.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "relative to the semantics and you will",
      "offset": 1366.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "retrieve those packets of information",
      "offset": 1368.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and pass them to the to the LM. So this",
      "offset": 1371.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "is like you know more high level",
      "offset": 1374.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "overview how red works behind the",
      "offset": 1376.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "scenes. Okay. Uh yes. Um I like the",
      "offset": 1378.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "idea. It's just basically about uh",
      "offset": 1381.919,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "retrieving relevant bits of information",
      "offset": 1383.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so your model doesn't hallucinate. It's",
      "offset": 1385.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "it's pulling from real data.",
      "offset": 1387.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "But nice. Um yeah. So I guess it sounds",
      "offset": 1392.679,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "like the the tricky part then is still",
      "offset": 1396.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just about how do you divide your",
      "offset": 1397.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "document up into into smaller chunks",
      "offset": 1399.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that are meaningful in some sense in",
      "offset": 1401.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "order that you can pull out like the the",
      "offset": 1403.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "right facts rather than just entire",
      "offset": 1405.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "paragraphs or just like fragments of",
      "offset": 1408,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "sentences or things. You got to get at",
      "offset": 1409.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the right level. Yeah, exactly. And it's",
      "offset": 1411.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a lot more harder than it looks because",
      "offset": 1413.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "you usually work with unstructured data",
      "offset": 1415.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and every document is unique. Every",
      "offset": 1418.559,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "image is unique and that's where most of",
      "offset": 1421.6,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "the AI problem start to fail because",
      "offset": 1426.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "they try to generalized the how you do",
      "offset": 1428.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "chunking and most of the time that",
      "offset": 1431.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and so uh another one of the chapters in",
      "offset": 1435.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "your book is around fine-tuning. So I",
      "offset": 1437.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "think Maxim this is your specialty here.",
      "offset": 1440.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "So talk me through um one of the",
      "offset": 1442.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "chapters on supervised finetuning. So",
      "offset": 1444.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "what does supervised fine-tuning involve",
      "offset": 1446.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and why do you want to do? Yeah. So",
      "offset": 1449.039,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "supervised fine tuning allows you to",
      "offset": 1452.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "train the model on a data set that you",
      "offset": 1454.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "created and that's really good if you",
      "offset": 1458.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "want to instill some knowledge for",
      "offset": 1460.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "example in the model. Um let's take an",
      "offset": 1462.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "example. You work in a company and you",
      "offset": 1465.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "would like the model to have some basic",
      "offset": 1467.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "facts about the company. If you chat",
      "offset": 1470.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "with it, it can be interesting for a",
      "offset": 1472.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "chatbot. For example, SP tuning can help",
      "offset": 1473.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you um instill this knowledge in the",
      "offset": 1476.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "model. Can help you if you also want to",
      "offset": 1479.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "change the formatting of the answer. If",
      "offset": 1481.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "you want to have like a specific",
      "offset": 1484.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "structure, it can be also a good",
      "offset": 1486.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "solution. And general it can be pretty",
      "offset": 1488.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "good if you want to um if you have a",
      "offset": 1491.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "bigger model and you want to distill it",
      "offset": 1494.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "into a smaller model can use the bigger",
      "offset": 1496.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "model to create a data set and we train",
      "offset": 1499.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the smaller model on that. Uh this is",
      "offset": 1501.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "done all the time uh to create general",
      "offset": 1505.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "purpose fine models. Okay. So it's",
      "offset": 1507.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "really just about customizing what the",
      "offset": 1510.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "output's going to be. I'm curious as to",
      "offset": 1511.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "like how it relates to I I know there's",
      "offset": 1514.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "a lot of other different sort of",
      "offset": 1516.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "post-training techniques in order to",
      "offset": 1517.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "change how the uh the outputs are",
      "offset": 1520.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "generated or or what the quality of the",
      "offset": 1522.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "outputs is like. Where does supervised",
      "offset": 1524.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "fine tuning fit in compared to all these",
      "offset": 1526.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "other techniques? Yes. So, usually you",
      "offset": 1528.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "would um start with the base model. The",
      "offset": 1530.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "base model has been pre-trained on a lot",
      "offset": 1532.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of data. Uh but it's only able to",
      "offset": 1534.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "autocomplete the text because it's been",
      "offset": 1538.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "trained to predict the next word or sub",
      "offset": 1540.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "word in sequence. Um so that's not",
      "offset": 1543.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "really useful if you want to ask",
      "offset": 1545.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "questions because it's just going to",
      "offset": 1547.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "complete your question uh instead of",
      "offset": 1548.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "providing the answer. But this is why",
      "offset": 1550.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you want supervised functioning in the",
      "offset": 1552.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "first place. uh because in supervised",
      "offset": 1553.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "functioning we have a specific structure",
      "offset": 1555.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "where you say this is the question this",
      "offset": 1557.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "is the answer this is the question this",
      "offset": 1560.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is the answer so yeah more broadly",
      "offset": 1562.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "speaking supervised fingering is pretty",
      "offset": 1565.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "much always used at least to transform",
      "offset": 1567.279,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "this base model into a useful assistant",
      "offset": 1569.919,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "and then you have other techniques that",
      "offset": 1574,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "go on top of uh finetuning they're",
      "offset": 1576.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "called preference alignment techniques",
      "offset": 1579.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "in general or another name is",
      "offset": 1581.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "reinforcement learning from human",
      "offset": 1583.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "feedback, but not all of them actually",
      "offset": 1585.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "use reinforcement learning. So I like to",
      "offset": 1587.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "call them preference alignment. So in",
      "offset": 1590.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "this family of techniques, the goal will",
      "offset": 1593.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "be more to tune the outputs from the",
      "offset": 1595.12,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "model. So they sound better that they",
      "offset": 1598.64,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "sound closer to what you want. Uh this",
      "offset": 1602.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "is useful if you for example you have an",
      "offset": 1605.279,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "instruct model and you see that some",
      "offset": 1608.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "answers you don't like them and you are",
      "offset": 1610.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "able to write better answers. In this",
      "offset": 1613.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "case you have a preference data set",
      "offset": 1616.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because all the answers that you don't",
      "offset": 1617.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like you want to reject them and all the",
      "offset": 1619.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "answers that you can rewrite to improve",
      "offset": 1622.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "them they're your chosen answers. So you",
      "offset": 1624.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "can train a model on that. So rejecting",
      "offset": 1626.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the rejected answers and choosing the",
      "offset": 1629.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "chosen answers. That's kind of the goal",
      "offset": 1631.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "of um DPO, direct preference",
      "offset": 1633.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "optimization, which is probably the most",
      "offset": 1636.2,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "popular preference alignment algorithm.",
      "offset": 1638.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Okay, so this is like after you've done",
      "offset": 1641.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the finetuning, you're then saying,",
      "offset": 1643.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "well, okay, these are some possible uh",
      "offset": 1645.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "responses, which one do you like best?",
      "offset": 1647.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "And you say in I guess you're doing",
      "offset": 1650.159,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "either reinforcement learning or some",
      "offset": 1651.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "other technique to say, okay, we we want",
      "offset": 1653.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "more output like these good answers.",
      "offset": 1655.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Yeah, I know um Deep Seek made a bit of",
      "offset": 1657.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "a buzz in recent months because they'd",
      "offset": 1661.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "introduced a new algorithm for doing",
      "offset": 1663.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "this preference alignment. Can you talk",
      "offset": 1665.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "me through what the innovation is there?",
      "offset": 1667.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Yeah. So this algorithm is called GRPO",
      "offset": 1669.36,
      "duration": 9.559
    },
    {
      "lang": "en",
      "text": "and the idea is that you are going to",
      "offset": 1674,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "um I'm going to take a high level or",
      "offset": 1678.919,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "overview of it but uh the the idea is",
      "offset": 1682,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that you're going to compare different",
      "offset": 1684.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "versions of an answer and you're going",
      "offset": 1687.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "to take the advantage of this comparison",
      "offset": 1690.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "to have like your reward signal for the",
      "offset": 1693.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "uh model to to be trained. So in this",
      "offset": 1696.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "case, you do not need to have chosen",
      "offset": 1698.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "answers and rejected answers. You only",
      "offset": 1701.12,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "need like some prompts and then the",
      "offset": 1703.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "filtering will happen because you can",
      "offset": 1707.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "specify some custom functions uh to make",
      "offset": 1709.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "sure that you have the proper",
      "offset": 1713.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "formatting. For example, to give an",
      "offset": 1714.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "example of what I'm trying to to say",
      "offset": 1716.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "right now, um DeepSync used two",
      "offset": 1719.76,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "different um filtering reward functions.",
      "offset": 1722.799,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "One of them was to teach the model to",
      "offset": 1726.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "output thinking tokens. So when you talk",
      "offset": 1729.6,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "to deepse one, it will always think a",
      "offset": 1733.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "lot like really a lot sometimes and then",
      "offset": 1736.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "give you the answer. Well, that's",
      "offset": 1739.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "behavior was um created design during",
      "offset": 1740.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "this preference alignment process by",
      "offset": 1744.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "punishing the model when it did not",
      "offset": 1747.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "output the thinking tokens and rewarding",
      "offset": 1749.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it when it did it. Another thing that",
      "offset": 1751.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "they did and this one is probably more",
      "offset": 1753.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "interesting is that they extracted the",
      "offset": 1755.919,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "answer when you ask a math question for",
      "offset": 1758.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "example they extracted the final answer",
      "offset": 1760.399,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "from um the model and they compared it",
      "offset": 1763.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "to the ground truth answer and they use",
      "offset": 1766.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that to train the model and that's why",
      "offset": 1768.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it's so good at math it's so good at",
      "offset": 1770.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "this like scientific questions general",
      "offset": 1772.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "that does seem pretty useful um for well",
      "offset": 1774.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "yeah I don't know whether it works in",
      "offset": 1777.36,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "other fields but certainly for",
      "offset": 1778.64,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "mathematics science where you've got",
      "offset": 1779.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "like a real concrete and so that you can",
      "offset": 1781.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "test did it get the right answer or not.",
      "offset": 1783.039,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Um that seems um yeah uh very good to",
      "offset": 1785.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "train against. Um I suppose more",
      "offset": 1788.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "generally though if you don't have this",
      "offset": 1791.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "concrete answer how do you know whether",
      "offset": 1793.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "an LLM is giving good answers or not? It",
      "offset": 1795.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "feels like it's a bit more fuzzy trying",
      "offset": 1798.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to evaluate whether it works or not.",
      "offset": 1800.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Yeah, the usual answer to this question",
      "offset": 1802.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is asking another LLM if it agrees or",
      "offset": 1805.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "not. Um because yeah this is pretty much",
      "offset": 1808,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the best thing that we have. This is",
      "offset": 1810.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "used in other preference alignment",
      "offset": 1812.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "algorithms actually. You can have a",
      "offset": 1814.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "function one of these functions can just",
      "offset": 1816.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "be calling another LLM and say hey is it",
      "offset": 1818.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "correct or not and get this reward",
      "offset": 1821.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "signal in your GPO training. Um but what",
      "offset": 1823.279,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "GPO gives you compared to to DPO is that",
      "offset": 1827.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "you do not need a preference data set",
      "offset": 1830.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and in a lot of cases it's difficult to",
      "offset": 1832.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "create the preference data set. So that",
      "offset": 1834.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can be really helpful. And then the",
      "offset": 1836.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "question is do you have multiple reward",
      "offset": 1838.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "signals? As I said, you can specify",
      "offset": 1841.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "different functions. So that can be",
      "offset": 1843.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "really good if you have like different",
      "offset": 1845.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "ways to reward uh the models. And",
      "offset": 1847.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "finally about computational resources.",
      "offset": 1850.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "This is also more efficient than PO",
      "offset": 1852.88,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "which is the third and um really heavy",
      "offset": 1855.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "preference alignment um algorithm. So",
      "offset": 1859.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "this is also good if you if you don't",
      "offset": 1862.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "have like the resources of open AI to",
      "offset": 1864.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "train models. Uh it can be handy. Paul,",
      "offset": 1866.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "did you have any uh thoughts on uh how",
      "offset": 1869.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you make sure an LLM is any good or not?",
      "offset": 1872,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Uh excellent. Actually I have one",
      "offset": 1874.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "question for Maxim which I seen like",
      "offset": 1876.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "different answers to this and I'm really",
      "offset": 1880.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "curious what what he thinks about it. So",
      "offset": 1881.919,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you said that after so you have the base",
      "offset": 1885.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "model and after you do this finetuning",
      "offset": 1887.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "step to basically create the instruct",
      "offset": 1889.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "model that answer your questions",
      "offset": 1893.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "properly but if you want to only inject",
      "offset": 1894.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "task specific or domain specific",
      "offset": 1898.399,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "knowledge into the LLM is it correct to",
      "offset": 1900.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "take uh instruct model as your base",
      "offset": 1904.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "model and further finetune it on other",
      "offset": 1907.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "instructions that only inject as I said",
      "offset": 1910.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "more narrow pro test specific or domain",
      "offset": 1912.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "specific knowledge or you need to start",
      "offset": 1915.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "from your base knowledge that doesn't",
      "offset": 1917.679,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "know how to answer questions at all. No.",
      "offset": 1920.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Yeah, you're you're right. You can do it",
      "offset": 1923.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "from an instruct model. That would work",
      "offset": 1925.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "too, but it's just that sometime it",
      "offset": 1927.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "doesn't work. Um so it's a bit difficult",
      "offset": 1929.6,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "to recommend it. Um people often ask me",
      "offset": 1932.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "like oh do I have to take a base model?",
      "offset": 1935.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And the short answer is no, you don't",
      "offset": 1938.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "have to. And in most scenarios actually",
      "offset": 1940.559,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "it's going to be pretty much the same.",
      "offset": 1943.279,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "So you don't have to but I wouldn't",
      "offset": 1946.519,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "necessarily take the risk or at least I",
      "offset": 1949.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "would try both approaches and evaluate",
      "offset": 1951.919,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "them to see which base model was better.",
      "offset": 1954.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "But I don't fully trust this instruct",
      "offset": 1957.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "models uh to be retrained like sometimes",
      "offset": 1960.08,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "it it goes really wrong",
      "offset": 1962.64,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and based on your data samples. So you",
      "offset": 1965.24,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "need like the same data sample size, the",
      "offset": 1969.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "same data sizes for both scenarios. If",
      "offset": 1971.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you move from a struct model or base",
      "offset": 1974.24,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "model so in theory you don't need to",
      "offset": 1976.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "write because instruct model already",
      "offset": 1980.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "knows the structure. It already has more",
      "offset": 1982.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "knowledge. So in theory you don't need",
      "offset": 1985.12,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "to. In practice, if you want to have the",
      "offset": 1987.279,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "same performance in your downstream",
      "offset": 1991.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "task, because here we're talking about",
      "offset": 1993.919,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "fine-tuning a model for a specific use",
      "offset": 1996.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "case, if you do that, it's probably",
      "offset": 1998.76,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "better to have like as many samples as",
      "offset": 2001.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you can. Um, of course, if you have like",
      "offset": 2003.519,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "millions of samples, it's it's too much.",
      "offset": 2006,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Like you don't need that many, right?",
      "offset": 2007.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "But usually, I would say no, like I",
      "offset": 2009.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "would retrain with the entire data set",
      "offset": 2012.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "if I can because like Yeah. Creating",
      "offset": 2014.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "data set is often the problem. It's",
      "offset": 2018.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "often the main problem in the entire",
      "offset": 2020.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "training pipeline. Training models is",
      "offset": 2021.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "easy. Evaluating them is becoming",
      "offset": 2024.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "harder. Generating the data it's it's",
      "offset": 2026.799,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "really hard. Yeah. Uh well that's uh",
      "offset": 2029.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "where my question was rooted because",
      "offset": 2032.72,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 2035.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "intuitively for me you need less data to",
      "offset": 2036.36,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "further fight tune an already",
      "offset": 2040.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "instruction model than to take it from",
      "offset": 2042.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "from the base model on and that that's",
      "offset": 2044.159,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "why I was curious because most of the",
      "offset": 2046.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "time you don't have like thousand of or",
      "offset": 2049.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "tens of thousands of high quality",
      "offset": 2051.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "samples and I was hoping that maybe you",
      "offset": 2053.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "can hijack it with a few hundred",
      "offset": 2055.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "samples. Yeah. Um, I'm not sure that it",
      "offset": 2057.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "will work as well as the full set, but",
      "offset": 2060.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "yeah, it can happen. All right, super. I",
      "offset": 2063.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "say I like this idea of guests asking",
      "offset": 2066.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "other guest questions. It makes my life",
      "offset": 2067.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "much easier. I can sort of sit back and",
      "offset": 2068.96,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "have a drink.",
      "offset": 2070.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I was I was really curious about this",
      "offset": 2072.879,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "one because you cannot find a lot of",
      "offset": 2074.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "information of this or if you can find",
      "offset": 2077.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it is very polarized and you talk a lot.",
      "offset": 2079.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "I'll make a LinkedIn post about it",
      "offset": 2082.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "before you do a poll.",
      "offset": 2083.679,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Yeah, for sure. Read the blog.",
      "offset": 2086.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Wonderful. So yeah, it sounds like uh",
      "offset": 2089.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the challenge there is about getting uh",
      "offset": 2090.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "high quality data and can you sort of",
      "offset": 2093.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "use synthetic data from another LLM just",
      "offset": 2095.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "in place of a real data set. Cool. Uh",
      "offset": 2097.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "all right. So related to this um it",
      "offset": 2100.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "seems like deployment is uh very much a",
      "offset": 2102,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "sticking point for a lot of",
      "offset": 2104.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "organizations. So okay uh Paul you've",
      "offset": 2105.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "just received some Jupyter notebooks",
      "offset": 2108.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "from Maxim. What's the next step to",
      "offset": 2110.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "getting this into something something in",
      "offset": 2113.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "production? Yeah, I would first ask him",
      "offset": 2115.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "if we can do this with uh clo or open",
      "offset": 2117.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "air models. So we don't have to deploy",
      "offset": 2120.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "anything at",
      "offset": 2123.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "all jokes aside. So I think the hardest",
      "offset": 2127.32,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "part is to fight compute when it comes",
      "offset": 2131.28,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "to deployment because to deploy these",
      "offset": 2133.44,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "big models it's very costly right so",
      "offset": 2137.599,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "they're b I think two main ways to",
      "offset": 2142,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "deploy them is either to like quantize",
      "offset": 2145.44,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "them really heavily and try to deploy",
      "offset": 2149.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "them on a CPU to be honest I don't have",
      "offset": 2151.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a lot of experience with that only when",
      "offset": 2153.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I deploy them on my local machine like",
      "offset": 2155.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "playing around with things. Uh what I",
      "offset": 2158.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "did like to deploy them more for",
      "offset": 2161.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "production grade was still using",
      "offset": 2163.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "quantization plus GPU models",
      "offset": 2165.599,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "uh plus GPU machines and in that way",
      "offset": 2168.8,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "most of the time you need as I said to",
      "offset": 2172.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "find a compute provider and to find like",
      "offset": 2175.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a serine",
      "offset": 2177.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh which will basically serve your",
      "offset": 2180.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "model. Some uh the serving engines are",
      "offset": 2182.72,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "tools like uh VLM which is very popular",
      "offset": 2185.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh TGI from Hunting phase. These two are",
      "offset": 2189.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "open source and I tested them and and",
      "offset": 2191.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "really like them. But you can also opt",
      "offset": 2194.48,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "for more uh vendor locked options like",
      "offset": 2197.839,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "SageMaker which we we showed in the book",
      "offset": 2202.16,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "how how to do which I think it's a",
      "offset": 2204.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "more more easy to do because AWS",
      "offset": 2208.359,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "provides everything that you need like",
      "offset": 2211.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in one place the compute the the tools",
      "offset": 2213.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the framework the everything so that's",
      "offset": 2216.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "why I think that so many people adopt",
      "offset": 2219.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "solutions like AWS and SageMaker because",
      "offset": 2222.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "they're I think the easiest",
      "offset": 2224.64,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "way to get",
      "offset": 2226.56,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "started. Uh other options that I've seen",
      "offset": 2228.2,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "like more community based that are",
      "offset": 2232.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "really really cool is I recently tested",
      "offset": 2234.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "HuggyFace is dedicated where under the",
      "offset": 2237.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "hood they implement TGI",
      "offset": 2240.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh because it's made by them. So TJ is",
      "offset": 2243.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "made by HuggyFace and it's just insanely",
      "offset": 2246.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "simple to be honest to to deploy a",
      "offset": 2249.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "model. You just do a few clicks like",
      "offset": 2251.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "choose your compute, choose your",
      "offset": 2254.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "quantization methods, choose your model,",
      "offset": 2255.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "choose your scaling strategies and",
      "offset": 2258.32,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "deploy",
      "offset": 2261.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "them. But I I think that this is nice",
      "offset": 2261.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and sweet for like more toy projects,",
      "offset": 2265.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "but when you want to deploy them more at",
      "offset": 2267.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "scale,",
      "offset": 2271.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "uh yeah, it's you need to be really",
      "offset": 2272.56,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "careful like into thinking what your",
      "offset": 2275.599,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "cost should be. as as I said before like",
      "offset": 2279.119,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "do the math really well behind choose",
      "offset": 2282.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the autoscaling methods properly choose",
      "offset": 2285.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "your as I said framework properly",
      "offset": 2288.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "configure your your your framework for",
      "offset": 2290.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "requirements",
      "offset": 2292.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "uh again in the end it's quite simple",
      "offset": 2294.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because these requirements most of the",
      "offset": 2297.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "time boil to four aspects that I kept",
      "offset": 2299.2,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "saying is podcast cost latency",
      "offset": 2303.2,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "throughput and uh your data",
      "offset": 2306.28,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "type. And another thing that I realized",
      "offset": 2310.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "recently",
      "offset": 2313.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "is it will make your life so much easier",
      "offset": 2315.079,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "if you pick like uh battle testing",
      "offset": 2318,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tools. So instead of trying to deploy",
      "offset": 2320.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "your models on",
      "offset": 2322.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "uh the fanciest latest",
      "offset": 2324.72,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "uh serving DLM tool that's out there and",
      "offset": 2327.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is raging on LinkedIn or",
      "offset": 2332,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "whatever just sticking to the AWS",
      "offset": 2333.96,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "ecosystem would make your life so much",
      "offset": 2336.48,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "easier or your cloud vendor like AWS",
      "offset": 2338.48,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "Asure GCP and so on and so forth. And",
      "offset": 2343.56,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "for example, I I'm a big fan of AWS. You",
      "offset": 2346.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "add so many options to deploy your model",
      "offset": 2350.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like you can start with bedrock for the",
      "offset": 2352.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "easiest option where you don't have a",
      "offset": 2355.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "lot of control but it's good enough to",
      "offset": 2357.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "start with. Then you have Sage Maker",
      "offset": 2359.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "where it gives you more more control but",
      "offset": 2361.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it's quite costly but I I believe that",
      "offset": 2364,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is a really good way to quickly ramp up",
      "offset": 2366.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "your training and inference pipelines",
      "offset": 2368.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and then you can go more lowle and",
      "offset": 2370.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "actually deploy your model on ECS and",
      "offset": 2374.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "EKS which are like Kubernetes or",
      "offset": 2376.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Kubernetes light clusters when basically",
      "offset": 2378.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "have full control over whatever you want",
      "offset": 2382,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to do and you can start playing around",
      "offset": 2384.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "with whatever you have in mind. I like",
      "offset": 2387.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the idea of keeping your technology",
      "offset": 2389.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "stack simple. Uh you mentioned that cost",
      "offset": 2391.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "control could be a problem. I can",
      "offset": 2394.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "certainly say like for the with the rise",
      "offset": 2395.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "of these reasoning models with the rise",
      "offset": 2397.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "of agents which just consume a lot of",
      "offset": 2399.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "tokens they get very expensive that",
      "offset": 2400.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "costs are going to be a problem. Is it",
      "offset": 2402.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "possible to predict how much you're",
      "offset": 2404.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "likely to need to spend in advance? It",
      "offset": 2406.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "feels like this is something you need to",
      "offset": 2409.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "worry about very early on in your",
      "offset": 2410.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "project.",
      "offset": 2411.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Yes and no.",
      "offset": 2413.44,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "There's good news and bad news there.",
      "offset": 2417.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Hey, in the end you can make",
      "offset": 2420.68,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "speculations and predictions similar",
      "offset": 2423.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "like to the stock market, right? So you",
      "offset": 2425.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "you can calculate like how much compute",
      "offset": 2428.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "you need for your models and you can",
      "offset": 2431.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "know like how big your model will be and",
      "offset": 2433.119,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "then you can see uh on what type of",
      "offset": 2435.92,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "machines your model can fit. Then you",
      "offset": 2440.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "know how much those machines will cost.",
      "offset": 2442.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that you can assume how much traffic you",
      "offset": 2445.52,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "will",
      "offset": 2447.68,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "have. But",
      "offset": 2448.44,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "again like when it comes to model sizes",
      "offset": 2450.839,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "and on on what machine you can put the",
      "offset": 2453.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "models, these are more predictable",
      "offset": 2455.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "calculations. But when it comes to",
      "offset": 2457.44,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "uh pure traffic that can be yes and no",
      "offset": 2460.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and they're more",
      "offset": 2464.64,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "like lowhanging fruits. For",
      "offset": 2466.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "example, what",
      "offset": 2470.44,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "if your users start to input uh very",
      "offset": 2472.52,
      "duration": 8.28
    },
    {
      "lang": "en",
      "text": "high documents like 3,000 pages books",
      "offset": 2476.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "because also your costs are dependent.",
      "offset": 2480.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So despite uh the infrastructure that",
      "offset": 2483.04,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "your model will sit on uh you also have",
      "offset": 2485.68,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "like autoscaling mechanism. So basically",
      "offset": 2489.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you will need only one machine to serve",
      "offset": 2492.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "a few clients or more machines to serve",
      "offset": 2493.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "more clients with more bigger inputs. So",
      "offset": 2496.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for example, let's assume that one",
      "offset": 2499.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "client inputs an outlier, a book with",
      "offset": 2501.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "3,000 pages. Then a machine will be",
      "offset": 2504.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "locked to process that that book for a",
      "offset": 2506.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "long period of time and it will start to",
      "offset": 2510.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "ramp up other machines that will start",
      "offset": 2513.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to eat your money. So basically you you",
      "offset": 2515.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "have a few variables that are more fixed",
      "offset": 2519.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "which you can predict but the traffic",
      "offset": 2521.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that your user users",
      "offset": 2523.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "uh basically your your users traffic and",
      "offset": 2526.88,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "the type of inputs that they they will",
      "offset": 2529.04,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "input",
      "offset": 2531.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the they're more hard to predict. You",
      "offset": 2532.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "can only make assumptions. Okay. So it",
      "offset": 2535.359,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "sounds like users are kind of the",
      "offset": 2538,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "problem. They do they do things that are",
      "offset": 2539.48,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "unexpected. So I suppose um I I guess",
      "offset": 2542,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the solution to that is you you start",
      "offset": 2545.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "small and work out see like what are",
      "offset": 2547.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "your users using your product for and",
      "offset": 2549.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then you're going to get some data and",
      "offset": 2552.319,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "that's going to give you a bit more",
      "offset": 2553.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "information on like what you need to do",
      "offset": 2554.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "as you as you scale things up. Yeah.",
      "offset": 2556.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah. Exactly. And I like to say that",
      "offset": 2558.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "these are the type of problems that you",
      "offset": 2560.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "want to have because if you have users",
      "offset": 2562.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "it means that your product and what",
      "offset": 2564.16,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "you're doing is good but you need some",
      "offset": 2565.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "futures some future steps to further",
      "offset": 2568.04,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "improve it. So, as you said, it's better",
      "offset": 2570.64,
      "duration": 8.92
    },
    {
      "lang": "en",
      "text": "just to start small and do some like uh",
      "offset": 2573.28,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "uh I know some protection layers on top",
      "offset": 2580.68,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "of it so you don't uh wake up with a",
      "offset": 2583.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "cloud bill of thousands of dollars. So,",
      "offset": 2586.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the easiest way to do this is just to",
      "offset": 2590,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "specify a maximum number of replicas",
      "offset": 2593.04,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "that that you want to scale.",
      "offset": 2595.28,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "So when you do this uh the worst thing",
      "offset": 2598.119,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "that can happen is it will hinder",
      "offset": 2602.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "people's experience. So if there are no",
      "offset": 2604.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "more replicas to scale up it it means",
      "offset": 2608,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "that some people will not be able to",
      "offset": 2610.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "access the models in real time or how",
      "offset": 2613.119,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "how they were",
      "offset": 2615.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "designed. And the next protection layer",
      "offset": 2617.079,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "is that that that's why you need strong",
      "offset": 2619.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "monitoring and observability pipelines",
      "offset": 2621.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "because that's how you gather all this",
      "offset": 2624.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "information. you can gather actual real",
      "offset": 2626.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "insights of on average how much it costs",
      "offset": 2629.119,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "to run your model or like on average how",
      "offset": 2632.48,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "many models you give to them uh up at a",
      "offset": 2635.359,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "single point in time what's the whole",
      "offset": 2639.119,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "cost of your whole infrastructure. So",
      "offset": 2642.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "better start early and monitor and",
      "offset": 2645.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "gather statistics the right way and",
      "offset": 2647.76,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "based on that take datadriven uh methods",
      "offset": 2650,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "to quickly adapt than just assuming how",
      "offset": 2654.319,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "much it will cost. Okay. Uh so that's",
      "offset": 2657.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "actually quite reassuring that like a",
      "offset": 2660.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "lot of the secret to this is doing data",
      "offset": 2662.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "analysis like collect data on what your",
      "offset": 2664.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "user is doing, how much things are",
      "offset": 2667.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "costing, do some analysis and then",
      "offset": 2668.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that's going to help keep things under",
      "offset": 2670.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "control. So um Maxim, I'd like to talk",
      "offset": 2672.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "about something I I saw you posting",
      "offset": 2676,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "about recently on your LinkedIn. You",
      "offset": 2677.44,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "were talking about automatic",
      "offset": 2679.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "obliteration. So tell me what is",
      "offset": 2680.839,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "obliteration and why would you want to",
      "offset": 2682.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "automate it? So obliteration is a",
      "offset": 2684.56,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "technique um that comes from mechanistic",
      "offset": 2687.52,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "interpretability research in LLM. Well,",
      "offset": 2691.079,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "specifically it was um a blog post on",
      "offset": 2694.319,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "less wrong called refusal in LLM is",
      "offset": 2697.2,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "mediated by a single direction by RDT at",
      "offset": 2700.4,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "L and what they found is that when the",
      "offset": 2703.839,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "LLM refuses a prompt so all the safety",
      "offset": 2707.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "layer is actually very very brittle.",
      "offset": 2710.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "It's actually very easy to modify the",
      "offset": 2713.2,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "weights of another. So it cannot say no",
      "offset": 2716.96,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "anymore. It cannot refuse any request.",
      "offset": 2720.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "This technique has been refined by the",
      "offset": 2723.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "open source community especially by",
      "offset": 2725.68,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "someone named Fail Spy and this is now",
      "offset": 2728.16,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "what we mean when we talk about",
      "offset": 2733.119,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "aeration. It's about removing the um",
      "offset": 2734.839,
      "duration": 8.041
    },
    {
      "lang": "en",
      "text": "what's called the refusal direction in",
      "offset": 2740,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the model weights themselves. To do that",
      "offset": 2742.88,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "we take prompts that the models refuse.",
      "offset": 2746.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "We take prompts that the model will",
      "offset": 2749.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "accept. we take the difference between",
      "offset": 2751.28,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "both in the intermediate uh calculations",
      "offset": 2755.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "within the model and we use this vector",
      "offset": 2758.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "like this difference we apply it in the",
      "offset": 2761.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "weights uh like absolute barbariance and",
      "offset": 2764.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it just works uh so it's a really really",
      "offset": 2766.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "funny technique because it should not",
      "offset": 2768.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "work really but it does it has works so",
      "offset": 2770.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "so well um so this is my interest in it",
      "offset": 2772.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "it's like yeah I don't know like this is",
      "offset": 2776.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the kind of stuff that should not be",
      "offset": 2778.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "allowed uh but we can still do it. And",
      "offset": 2780.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "recently I applied it uh to Gemma 3",
      "offset": 2783.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "released by Google and it was very",
      "offset": 2786.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "interesting to see that their model",
      "offset": 2789.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "seems to be a lot more resilient um to",
      "offset": 2791.28,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "this type of attack uh than others. Uh",
      "offset": 2794.079,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "so yeah, I tried to improve the",
      "offset": 2798.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "technique a little bit by introducing",
      "offset": 2800.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like different parameters that you can",
      "offset": 2802.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "tweak and see what works, what doesn't",
      "offset": 2804.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "work. Uh so it was a fun little",
      "offset": 2807.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "challenge uh with this uh new",
      "offset": 2809.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "architecture with these new models.",
      "offset": 2811.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Okay, this is fascinating because I",
      "offset": 2813.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "think um a lot of corporate use cases",
      "offset": 2815.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "for LLMs, you really want to have quite",
      "offset": 2818.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "tight control over what outputs are",
      "offset": 2821.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "being produced and having like moderated",
      "offset": 2824.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "outputs. Certainly if you got like a",
      "offset": 2826.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "customer chatbot like you don't want it",
      "offset": 2828.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to start going off the rails and not",
      "offset": 2829.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "talking about things relevant to the",
      "offset": 2831.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "job. So, is obliteration then a problem",
      "offset": 2833.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "to having these guard rails or is this",
      "offset": 2836.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "more of a sort of theoretical um attack",
      "offset": 2838.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "on LLMs? I think that there's um a big",
      "offset": 2841.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh community, open source community that",
      "offset": 2844.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "doesn't really like these guard rails in",
      "offset": 2846.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "general that are baked in the models and",
      "offset": 2848.56,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "they want to do stuff where it can also",
      "offset": 2851.119,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "um like confront like it doesn't really",
      "offset": 2854.44,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "work very well and they might get",
      "offset": 2858.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "refusal when it's not really um meant to",
      "offset": 2859.839,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "be refused. Um so they really like these",
      "offset": 2863.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "models in general. So it's more like the",
      "offset": 2866.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "open source community. And then this",
      "offset": 2868.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "technique can be used to do the absolute",
      "offset": 2870.079,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "opposite. You can create a model that",
      "offset": 2873.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "refuses everything that you say. You",
      "offset": 2876.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "just want I don't know like what's the",
      "offset": 2878.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "capital of France and it's going to say",
      "offset": 2880.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "I'm sorry but I cannot answer that. Uh",
      "offset": 2882.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so it can be used to actually um upgrade",
      "offset": 2885.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "the security level if you want and it",
      "offset": 2888.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "can be used in other creative ways. For",
      "offset": 2890.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "example, fail spy made a model that is",
      "offset": 2893.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "absolutely depressed um using this",
      "offset": 2896.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "technique. Everything that you say to",
      "offset": 2898.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this model like it will answer with a",
      "offset": 2900.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "very sad tone, you know, uh saying that",
      "offset": 2902.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "it's very sad and the universe is bleak.",
      "offset": 2905.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Um so you can do a lot of creative",
      "offset": 2908.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "little work. What's very interesting to",
      "offset": 2910.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "me with this technique is that you can",
      "offset": 2913.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "change the behavior of the model. You",
      "offset": 2916.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "can customize it without training the",
      "offset": 2918.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "model. So it's a lot cheaper to use",
      "offset": 2920.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "because there's no training involved and",
      "offset": 2923.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "despite that you can get pretty good",
      "offset": 2925.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "results for some use cases. Ah",
      "offset": 2927.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "interesting. So does that mean it might",
      "offset": 2930.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be a cheaper alternative to fine-tuning",
      "offset": 2931.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "then? It's not yet I have to say like I",
      "offset": 2934.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "working on it a bit on the side and for",
      "offset": 2937.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "example um a little project I made um",
      "offset": 2940,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "was to use it for classification. I",
      "offset": 2943.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "thought okay if I if I try to classify",
      "offset": 2946.319,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "uh some text uh using this technique and",
      "offset": 2949.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I do the same thing where I have like",
      "offset": 2952.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "wrong classifications correct",
      "offset": 2954.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "classifications I take the difference it",
      "offset": 2956.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "kind of works like I go from 0% accuracy",
      "offset": 2959.359,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "to 23% accuracy but 23% accuracy is not",
      "offset": 2962.319,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "very high so I wouldn't say it's it's",
      "offset": 2966.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "super useful just kind of works u maybe",
      "offset": 2968.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "with more work by being a bit more uh",
      "offset": 2971.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "surgical",
      "offset": 2974.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in the layers and the parameters that we",
      "offset": 2975.92,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "use um to to to apply this technique",
      "offset": 2979.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "that could raise um the performance, but",
      "offset": 2982.8,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "I'm I'm not sure we'll ever use it as a",
      "offset": 2986.16,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "real um contender competitor to to",
      "offset": 2989.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "fine-tuning. I'm not sure this will ever",
      "offset": 2993.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be the case, unfortunately. Okay. All",
      "offset": 2995.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "right. So it sounds like it's more about",
      "offset": 2998.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "um moderation control then uh rather",
      "offset": 3000.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "than uh customization of of output",
      "offset": 3002.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "content for now. Yes. Okay. But not even",
      "offset": 3004.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "for preence alignment because you said",
      "offset": 3007.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like you can introduce specific tones to",
      "offset": 3009.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the model. It's true. It's true. I think",
      "offset": 3011.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "that for for this kind of um changing",
      "offset": 3014.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the tone of the model, it it can be",
      "offset": 3017.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "really nice, but we would need to see",
      "offset": 3019.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like a comparison between a model that",
      "offset": 3022,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "has been like properly uh preference",
      "offset": 3024,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "aligned and a model that has been um",
      "offset": 3026.319,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "abiterated to see okay like how like",
      "offset": 3028.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "what do we lose because arbiteration is",
      "offset": 3032.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "quite a barbaric uh process where you",
      "offset": 3034.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "really like u destroy the weights of",
      "offset": 3038,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "your models and sometimes what you see",
      "offset": 3040.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is that the the end model is a bit",
      "offset": 3043.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "dumber like it's not going to perform as",
      "offset": 3045.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "well on major benchmarks like MML. You",
      "offset": 3047.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you lose a bit of intelligence because",
      "offset": 3050.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you were not subtle enough. Okay, makes",
      "offset": 3052.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "sense. And I I actually have another",
      "offset": 3054.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "curiosity and is this tactic similar to",
      "offset": 3057.16,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "merging because I know that there's also",
      "offset": 3061.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like for similar architectures you",
      "offset": 3063.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "directly the computation between the",
      "offset": 3065.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "weights of your model without training.",
      "offset": 3067.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "No, you're right. like it model merging",
      "offset": 3070.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is also a way to customize models",
      "offset": 3072.319,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "without training. Um so it it's related.",
      "offset": 3075.2,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "It's just the way that you customize",
      "offset": 3079.119,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "them that you apply this like different",
      "offset": 3082.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the way that you modify the weights is",
      "offset": 3086.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "different because with model merging",
      "offset": 3088.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you're just going to uh use the weights",
      "offset": 3090.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "of other models and kind of apply them",
      "offset": 3092.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "combine them with what you have. And",
      "offset": 3095.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "here with arbiteration, you're going to",
      "offset": 3097.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "calculate this difference between two",
      "offset": 3100.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "things and then you're going to apply",
      "offset": 3102.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this difference to the weights of your",
      "offset": 3104.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "model. But I agree like this is very",
      "offset": 3106.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "much related. Okay. Nice. And actually",
      "offset": 3108.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "uh Paul uh I was uh snooping on your",
      "offset": 3111.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "LinkedIn as well and I noticed that uh",
      "offset": 3114.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "you're posting that um people have been",
      "offset": 3116.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "predicting the death of rag for four",
      "offset": 3118,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "years now, but it's not quite happened.",
      "offset": 3119.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "So I'm curious as to why people think",
      "offset": 3121.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "rag is done for and uh whether or not",
      "offset": 3123.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "it's going to happen. Well, people think",
      "offset": 3126.16,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "that the whole purpose of rag is to is",
      "offset": 3129.04,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "because the context limit of the LLM is",
      "offset": 3133.16,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "limited, right? So in the beginning when",
      "offset": 3136.88,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "CHP went out or and the say for similar",
      "offset": 3140.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "models, the context window was like",
      "offset": 3144.16,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "quite limited. I think",
      "offset": 3146.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "32,000 or so. Maybe I'm wrong, but",
      "offset": 3148.2,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "anyway, it was quite small and that's",
      "offset": 3151.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "where rag was born because we wanted",
      "offset": 3154.8,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "like to throw",
      "offset": 3157.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "in I know companies data which is",
      "offset": 3158.68,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "usually like terabytes, pabytes even",
      "offset": 3162.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "more uh in size and we want to make the",
      "offset": 3164.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "model give the model access to this",
      "offset": 3168.64,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "data. But a few year but uh years later",
      "offset": 3171.04,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "on research advance and has also the",
      "offset": 3175.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "complex window got larger and I think",
      "offset": 3177.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "now uh Gemini has like a 2 million token",
      "offset": 3180.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "or so. Maybe I'm wrong about the actual",
      "offset": 3183.28,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "number but like this is the the scale of",
      "offset": 3185.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the of the context window that it could",
      "offset": 3188.44,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "support. So 2 million talkers is huge.",
      "offset": 3191.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Like I think you can throw in many books",
      "offset": 3195.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "in there just with a single call and",
      "offset": 3198.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "call it a day. So if you see it like",
      "offset": 3200.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "this, it doesn't make sense to do rag",
      "offset": 3203.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "anymore because you just complicate",
      "offset": 3206.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "yourself with uh chunking, embedding,",
      "offset": 3208.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "finding these entities, storing,",
      "offset": 3211.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "retrieving, and all that hassle. But the",
      "offset": 3213.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "issue is actually that usually doesn't",
      "offset": 3216.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "work well because as I said if you throw",
      "offset": 3219.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "it everything into a model it will be",
      "offset": 3221.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "really hard for it to focus on the right",
      "offset": 3224.24,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "data then you will",
      "offset": 3227.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "actually have a retrieval problem inside",
      "offset": 3229.559,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "your crop because when you input a",
      "offset": 3232.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "specific question then you need to",
      "offset": 3234.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "optimize your model to be able to",
      "offset": 3237.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "retrieve the information it needs for",
      "offset": 3239.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "question from the context itself which",
      "offset": 3241.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "usually it doesn't perform that good or",
      "offset": 3243.92,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "is not that controllable let's say uh",
      "offset": 3246.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "it's more nondeterministic maybe it will",
      "offset": 3250.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "find it maybe it will not but from what",
      "offset": 3252.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "I heard so far usually it's more it will",
      "offset": 3254.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "not or it will not find everything you",
      "offset": 3258.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "need because usually the model are more",
      "offset": 3260.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "biased at the beginning and mostly at",
      "offset": 3263.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the end of the contest so most of their",
      "offset": 3266.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "answer are based on your on the end of",
      "offset": 3268.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "the context I guess similar to humans",
      "offset": 3271.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "usually remember the latest sentences",
      "offset": 3273.44,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "from a",
      "offset": 3276.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "conversation. So if what's relevant to",
      "offset": 3277.079,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "your question is at the beginning or in",
      "offset": 3279.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the middle most probably to hallucinate",
      "offset": 3281.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "or something like that. So this is one",
      "offset": 3284.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "issue don't have the control over the",
      "offset": 3286.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "flow of information and the second issue",
      "offset": 3288.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "is again requirements costs if you every",
      "offset": 3290.72,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "time you input millions or ten of",
      "offset": 3294.48,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "thousands of tokens the cost will",
      "offset": 3297.839,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "skyrocket and also latency because the",
      "offset": 3301.319,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "model needs to compute all these tokens.",
      "offset": 3304.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "So you had a huge latency issues over",
      "offset": 3307.119,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "there. And again now we hit like compute",
      "offset": 3311.04,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "requirements. You need",
      "offset": 3315.599,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "uh your machines need more DAM to call",
      "offset": 3318.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "this context in memory or or not. There",
      "offset": 3322.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "are other engineering issues at hand",
      "offset": 3324.72,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "that you need to somehow store or cache",
      "offset": 3326.88,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "that that context. So there are other",
      "offset": 3331.16,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "issues and actually rag is still the",
      "offset": 3334.319,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "simplest the simpler methods where you",
      "offset": 3337.119,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "just throw in all the data into into",
      "offset": 3339.8,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "vector database and retrieve only small",
      "offset": 3343.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "uh chunks of data that you actually need",
      "offset": 3346.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "to and like this you put into the",
      "offset": 3349.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "context only what you need for the",
      "offset": 3351.599,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "specific question and the model will",
      "offset": 3354.079,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "focus only on what's useful and the",
      "offset": 3356.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "prompt itself it's smaller so more cost",
      "offset": 3359.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "effective",
      "offset": 3362.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the more latency effective. So it's",
      "offset": 3363.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "actually more predictable a more",
      "offset": 3366.64,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "predictable way to give the right",
      "offset": 3368.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "answers. Something I would like to add",
      "offset": 3371.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "is that all are also like not that good",
      "offset": 3373.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "actually with long context. We can see I",
      "offset": 3375.92,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "worked a lot on that uh eval evaluating",
      "offset": 3378.72,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "the effective context length and models",
      "offset": 3382.079,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that post that they can process more",
      "offset": 3385.079,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "than 32k tokens are usually like quite",
      "offset": 3388.559,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "bad at more than that. Um I think that",
      "offset": 3392.559,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it's not that true with frontier models.",
      "offset": 3395.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Frontier models can process like pretty",
      "offset": 3398.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "long context but still like 2 million I",
      "offset": 3400.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "I don't know. I wouldn't trust it,",
      "offset": 3403.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "especially if the answer is somewhere in",
      "offset": 3404.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the middle, it's probably a very bad",
      "offset": 3406.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "idea. But it's true, even with like 64K",
      "offset": 3408.72,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "tokens, for example. I think that you",
      "offset": 3412.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "quickly the accuracy really drops uh",
      "offset": 3415.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "quite fast. So, it's another reason why",
      "offset": 3418.799,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "even though it's advertised as um being",
      "offset": 3421.599,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "able to process M tokens, it's probably",
      "offset": 3424.319,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "not that effective in practice. Yeah,",
      "offset": 3428.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you you said it right to advertise. It's",
      "offset": 3431.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "similar like to phones or machines. Hey,",
      "offset": 3434.16,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "my phone has another camera, but just to",
      "offset": 3436.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "make people buy it,",
      "offset": 3440.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but that doesn't mean it will actually",
      "offset": 3442.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "be more useful or not.",
      "offset": 3444.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "And another aspect of it that I would",
      "offset": 3447.48,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "like to add is that actually in when you",
      "offset": 3449.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "want to further optimize your system, we",
      "offset": 3453.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "actually go away from this like two",
      "offset": 3456,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "three big models to again do more",
      "offset": 3458.48,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "smaller models that are more specialized",
      "offset": 3462.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "on specific tasks and again usually",
      "offset": 3465.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these smaller models have smaller",
      "offset": 3468.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "context window where you need rap",
      "offset": 3469.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "basically.",
      "offset": 3472.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "All right. So, uh, it sounds like, uh,",
      "offset": 3474.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these sort of promises of like giant,",
      "offset": 3475.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "uh, context windows in in LLMs, they're",
      "offset": 3477.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "kind of they're a bit overhyped and",
      "offset": 3481.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually rag is still a lot more useful.",
      "offset": 3482.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Like rather than stuffing absolutely",
      "offset": 3484.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "everything into the prompt, you would",
      "offset": 3486.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "just want to retrieve the useful bits",
      "offset": 3487.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and then augment your prompt with those",
      "offset": 3489.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "bits. Okay. So, I'm curious like uh just",
      "offset": 3491.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "to finish like what are you most excited",
      "offset": 3495.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "about in the world of AI? Yeah, I think",
      "offset": 3497.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that there's a lot of focus on",
      "offset": 3500.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "everything related to AGI. um and that",
      "offset": 3501.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "it's supposed to solve all the problems",
      "offset": 3505.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "in the world uh with just just AI, you",
      "offset": 3507.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "know, like we don't really know what but",
      "offset": 3510.16,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "AI is magic word and it's going to solve",
      "offset": 3512.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "everything. I I think like this is",
      "offset": 3515.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "really cool. Of course, like I love",
      "offset": 3517.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "seeing LLM's acing math problems that I",
      "offset": 3519.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "don't even understand. I think it's it's",
      "offset": 3522.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "really beautiful when that happens. And",
      "offset": 3524.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this year, I'm pretty sure that it it's",
      "offset": 3526.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "over for humans, like we're going to get",
      "offset": 3529.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "superhuman uh results in every",
      "offset": 3531.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "international math competition. But what",
      "offset": 3534.48,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "I'm mostly interested in is how we can",
      "offset": 3537.04,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "embed this technology everywhere in a",
      "offset": 3541.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "way that is like not forced because we",
      "offset": 3544.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "can see companies trying to push users",
      "offset": 3546.88,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "to uh use LLM even though sometimes it's",
      "offset": 3549.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "not what you want, right? Um but I I",
      "offset": 3553.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "think that having possibility of getting",
      "offset": 3556.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "like this offline assistance pretty much",
      "offset": 3558.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "everywhere can be really gamechanging in",
      "offset": 3561.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "a lot of applications, a lot of use",
      "offset": 3563.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "case. It can be super useful for people",
      "offset": 3565.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "to learn new skills. I know that I would",
      "offset": 3568.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "have loved having this technology when I",
      "offset": 3571.04,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "was younger and I was um trying to learn",
      "offset": 3573.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "German through Google Translate, which",
      "offset": 3576.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "was not the best way of learning German.",
      "offset": 3578.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "Um, but I I'm mostly interested in that,",
      "offset": 3581.44,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "like how can we embed this technology in",
      "offset": 3584.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "cars, in phones, in in tablets, uh,",
      "offset": 3588.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "pretty much everywhere where it can be",
      "offset": 3590.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "useful and and not just pushing for it,",
      "offset": 3592.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "um, blindly. And Paul, what are you",
      "offset": 3595.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "excited about? Well, I'm more uh, let's",
      "offset": 3598.16,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "say more grounded a little and not that,",
      "offset": 3600.96,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "uh, dreamish.",
      "offset": 3604.52,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "Uh I I mostly I I love productivity",
      "offset": 3607.559,
      "duration": 8.121
    },
    {
      "lang": "en",
      "text": "tools and productivity stuff. So I was a",
      "offset": 3612.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "pro productivity gig in in that sort of",
      "offset": 3615.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "sense. And most of my work as you can",
      "offset": 3618.24,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "see is around more or less around this.",
      "offset": 3621.119,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "And I personally see like AI more and",
      "offset": 3625.44,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "mostly this like LLM even if they are",
      "offset": 3629.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "like the current state of LLMs or more",
      "offset": 3632.76,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "like physics or general purpose LLMs",
      "offset": 3635.76,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "which we can uh slowly see this new",
      "offset": 3639.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "trend. I think this will be like the",
      "offset": 3642.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "next industry uh industrial revolution.",
      "offset": 3645.359,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "So I think that it will completely",
      "offset": 3649.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "change how people interact with machines",
      "offset": 3652,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "and how we will work and how our life",
      "offset": 3654.72,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "would be because how personally I see",
      "offset": 3659.28,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "the future is that we'll be mostly",
      "offset": 3662.16,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "focused on thinking and planning and",
      "offset": 3665.599,
      "duration": 8.441
    },
    {
      "lang": "en",
      "text": "uh I know solving",
      "offset": 3671.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "solutions or other creative processes",
      "offset": 3674.04,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "like okay I want to talk about a",
      "offset": 3676.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "specific topic. I want to write to about",
      "offset": 3680,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "specific topic to create a specific",
      "offset": 3683.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "piece of art. So I I think that we we",
      "offset": 3685.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "will automate a lot of these boring",
      "offset": 3689.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "mundane tasks and it will let us",
      "offset": 3691.44,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "actually be more more human. Uh which is",
      "offset": 3694.96,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "not that intuitive but because we don't",
      "offset": 3698.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "have to like fill in paper all the day",
      "offset": 3701.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "or lose so much time answering to emails",
      "offset": 3703.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "or even like writing blog posts or",
      "offset": 3706.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "making videos. we can just focus on what",
      "offset": 3709.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "we want to express on what we want to to",
      "offset": 3712.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "send to the world on what we want to",
      "offset": 3715.68,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "create and it would boost a lot of of",
      "offset": 3717.359,
      "duration": 8.561
    },
    {
      "lang": "en",
      "text": "yeah of our human parts. All right,",
      "offset": 3722.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "brilliant. So, uh last question. I",
      "offset": 3725.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "always want some follow recommendations.",
      "offset": 3727.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So, tell me whose work are you most",
      "offset": 3730.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "excited about at the moment? Uh who",
      "offset": 3731.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "should I be following? Personally, I",
      "offset": 3733.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "really like the work uh made by the",
      "offset": 3735.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "people at Haging Face. um everything",
      "offset": 3737.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "everywhere, everyone there. Um I think",
      "offset": 3739.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "it's really cool to have them as a kind",
      "offset": 3741.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "of center of excellence for like all the",
      "offset": 3744.799,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "open source efforts. Um they've been",
      "offset": 3746.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "brilliant. They started with this",
      "offset": 3749.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "transformers library and they slowly",
      "offset": 3750.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "expanded and now they're everywhere like",
      "offset": 3753.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "you cannot uh they standardize",
      "offset": 3754.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "everything. So uh you you you're doomed",
      "offset": 3756.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to u use uh one of their stuff. But they",
      "offset": 3758.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "also do like really cool scientific",
      "offset": 3762.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "work. They create really cool data sets.",
      "offset": 3763.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Um, I think that, yeah, we're really",
      "offset": 3767.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "lucky to to have them. And, um, yeah, if",
      "offset": 3769.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "you don't follow them already, um, this",
      "offset": 3772.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "is a really cool place to learn about",
      "offset": 3775.76,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "the the latest um, news, the latest",
      "offset": 3778.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "algorithm. They implement them, they",
      "offset": 3781.799,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "talk about them, and uh, they're really",
      "offset": 3784.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "good at it. Wonderful. Yeah. Uh, lots of",
      "offset": 3786.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "great people at Hogface. So, uh,",
      "offset": 3788.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "definitely uh, worth looking into their",
      "offset": 3790.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "work. And Paul, let's let me think. So",
      "offset": 3792.4,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "usually for example when it comes to",
      "offset": 3795.76,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "uh to more like educational part of it",
      "offset": 3800,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and when when it comes to like teaching",
      "offset": 3803.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "people for example I really respect uh",
      "offset": 3806.16,
      "duration": 8.199
    },
    {
      "lang": "en",
      "text": "chips work like I I guess like as a",
      "offset": 3809.28,
      "duration": 9.44
    },
    {
      "lang": "en",
      "text": "engineer and writer or or creator or",
      "offset": 3814.359,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "however you want to call yourselves like",
      "offset": 3818.72,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 3820.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "she's a role model in the sense of",
      "offset": 3822.039,
      "duration": 7.881
    },
    {
      "lang": "en",
      "text": "things but uh at the same time like also",
      "offset": 3825.359,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "respect a lot what Haggi face is doing",
      "offset": 3829.92,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "in the open space open source space. Uh",
      "offset": 3833.44,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "but again in the MLO part of things I I",
      "offset": 3838.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "respect a lot what people are doing for",
      "offset": 3841.039,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "for companies such as uh ZenML or or",
      "offset": 3842.96,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "data bricks. So yeah, it's it's hard to",
      "offset": 3847.52,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "point to a",
      "offset": 3851.039,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "specific there are so many people that",
      "offset": 3853.24,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "are doing so much great work. So it's",
      "offset": 3855.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "hard to like pick all you want.",
      "offset": 3858.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Absolutely. Uh a lot of uh great people",
      "offset": 3860.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "speaking on all all these topics. All",
      "offset": 3863.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "right. Wonderful. Uh thank you so much",
      "offset": 3865.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "for your time, Maxim. Thank you so much",
      "offset": 3867.599,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "for your time, Paul. It's been great",
      "offset": 3868.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "having you on the show. Thank you. Thank",
      "offset": 3870.079,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "you, Richie. It was great being here.",
      "offset": 3873.119,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 3879.49,
      "duration": 16.979
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:25.385Z"
}