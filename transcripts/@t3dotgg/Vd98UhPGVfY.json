{
  "episodeId": "Vd98UhPGVfY",
  "channelSlug": "@t3dotgg",
  "title": "Vercel Finally Caught Up",
  "publishedAt": "2025-06-27T07:53:24.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "It happened. Verscel actually shipped. I",
      "offset": 0.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "feel like it's been a while since",
      "offset": 2.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "Verscell dropped a bunch of things at",
      "offset": 3.679,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "once, like many years by now. The last",
      "offset": 5.2,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "two Next.js comps were relatively low in",
      "offset": 7.759,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "terms of new features both for Next and",
      "offset": 10.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "for Versell. That changed today. They",
      "offset": 12.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "dropped a ton of things I've been",
      "offset": 14.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "personally waiting for forever. From a",
      "offset": 15.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "new pricing model to cues to a way to",
      "offset": 17.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "run code that's sandboxed from your",
      "offset": 20.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "users to even handling captions",
      "offset": 22.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "properly. It's kind of nuts how much",
      "offset": 24.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "stuff they drop that I've needed for a",
      "offset": 26.08,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "while. It almost feels like they've been",
      "offset": 27.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "looking through my Twitter and hitting",
      "offset": 29.119,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "all the check boxes for all the things",
      "offset": 30.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that I've been bothered by recently.",
      "offset": 31.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "That all said, Verscell has not paid me",
      "offset": 33.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "for a long time. In fact, quite the",
      "offset": 35.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "opposite. I spend a lot of money paying",
      "offset": 37.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "them nowadays, especially with the",
      "offset": 39.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "success of T3 chat. So, someone's got to",
      "offset": 40.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "cover today's bill. And if I'm going to",
      "offset": 43.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "be honest, it can't be them. So, quick",
      "offset": 44.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "word from today's sponsor, and then",
      "offset": 46.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we'll dive right into what I think about",
      "offset": 48.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "everything Versel just shipped. One of",
      "offset": 49.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the biggest changes I'm seeing from AI",
      "offset": 51.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "is the willingness of big enterprise",
      "offset": 53.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "companies to adopt tools by small teams",
      "offset": 55.039,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and companies themselves. It's kind of",
      "offset": 57.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "crazy to see that small businesses like",
      "offset": 58.879,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "my own with T3 Chat are getting interest",
      "offset": 60.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "from much, much bigger enterprises.",
      "offset": 62.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "There's one big thing that's hard to get",
      "offset": 64.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "right though. O. And no, AI is not going",
      "offset": 66,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to solve this problem for you. Setting",
      "offset": 68.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "up O in a way that these big businesses",
      "offset": 70.4,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "are willing to use and adopt and",
      "offset": 72,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "integrate in their systems is something",
      "offset": 73.439,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that I wouldn't wish on my worst enemy.",
      "offset": 74.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "And that's why I'm so pumped about",
      "offset": 76.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "today's sponsor, Work OS. These guys",
      "offset": 78.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "made it way easier to get your",
      "offset": 80.479,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "application and most importantly your",
      "offset": 82.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "authentication ready for enterprise",
      "offset": 83.84,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "adoption. You can take my word for it or",
      "offset": 85.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you can look at the absurd number of",
      "offset": 87.759,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "companies that you're already using",
      "offset": 89.36,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "software from that have made the move",
      "offset": 90.799,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "themselves from cursor to OpenAI to",
      "offset": 92.479,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Verscell to Carta to Vanta to so many",
      "offset": 94.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "more. I always smile a bit when I open",
      "offset": 97.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "up the cursor dashboard and see the work",
      "offset": 98.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "OS like offkit signin. It's cool to know",
      "offset": 100.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that they're using the same tools we use",
      "offset": 102.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "every single day. I've personally",
      "offset": 104.479,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "chatted with GMO about offplatforms like",
      "offset": 105.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "work OS and he told me really early on",
      "offset": 108.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that he regretted not adopting one",
      "offset": 110,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "earlier. I think we could have done even",
      "offset": 111.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "more business if we had partnered with",
      "offset": 113.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "work OS earlier. It's been incredibly",
      "offset": 115.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "wellreceived. I couldn't agree more.",
      "offset": 116.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "They found an incredible balance of",
      "offset": 119.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "making something enterprise ready for",
      "offset": 121.04,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "these IT teams to integrate and",
      "offset": 122.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "something that's actually nice to use",
      "offset": 124.799,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "for us as full stack TypeScript",
      "offset": 125.92,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "developers. It's awesome that they found",
      "offset": 127.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "this balance and we're seeing more and",
      "offset": 129.039,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "more people adopt it for that reason. If",
      "offset": 130.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "you're tired of thinking about O and are",
      "offset": 131.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "ready to just ship, check out work OS",
      "offset": 133.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "today at soyv.link/workos.",
      "offset": 134.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Let's dive in. Going to do a real quick",
      "offset": 137.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "overview of the things that they changed",
      "offset": 139.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so that we can keep ourselves on track.",
      "offset": 141.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "The one that I'm personally most excited",
      "offset": 143.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "about is the active CPU billing. It's",
      "offset": 145.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "going to be a real fun one to talk about",
      "offset": 148.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and specifically why it matters. They",
      "offset": 149.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "added the sandboxing for code runs",
      "offset": 151.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "specifically for user submitted code or",
      "offset": 153.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "more importantly AI generated code cues",
      "offset": 155.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "finally and then their capture killer",
      "offset": 158.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "solution that is actually looking very",
      "offset": 161.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "compelling. It seems like they they",
      "offset": 163.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "learned all the right lessons from the",
      "offset": 165.04,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "things I've been complaining about with",
      "offset": 166.319,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "captas. If you haven't watched my",
      "offset": 167.519,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "capture video already, obviously I'm",
      "offset": 168.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "biased cuz I made it, but I think it's",
      "offset": 170.56,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "one of the best videos I ever did. So",
      "offset": 172.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "check it out if you haven't. very",
      "offset": 173.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "practical, applicable information on how",
      "offset": 175.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to set up captions and rate limiting in",
      "offset": 177.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "your services properly. They also",
      "offset": 180.4,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "officially dropped the AI gateway which",
      "offset": 182.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "is their alternative to something like",
      "offset": 184.959,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "open router. Actually looks pretty",
      "offset": 186.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "compelling. We'll dive into that too. So",
      "offset": 188.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "let's start with the active CPU billing",
      "offset": 189.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "cuz I am so excited about this. Again,",
      "offset": 192.239,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I'm biased because this is going to save",
      "offset": 195.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "me a ton of money. To understand this,",
      "offset": 196.879,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we need to understand how servers are",
      "offset": 199.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "built. If I have one server and I don't",
      "offset": 201.519,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "know, let's say that this server costs",
      "offset": 204.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "us $10 a month. This server is just one",
      "offset": 206.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "server. If it gets no users or it gets",
      "offset": 210.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "a,000 users or it gets a million users,",
      "offset": 212.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "we have our fixed rate. It's 10 bucks a",
      "offset": 214.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "month. But what happens if this server",
      "offset": 216.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "can only have, I don't know, 500 users",
      "offset": 218.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "concurrently at its peak. Then you need",
      "offset": 221.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to spin up more servers. Maybe you need",
      "offset": 223.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to keep them live for the whole month or",
      "offset": 225.36,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "maybe you're dynamically spinning them",
      "offset": 226.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "up and down. you end up in the the",
      "offset": 227.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "wonderful Kubernetes hell that we've all",
      "offset": 230.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "seen and I've often made fun of as you",
      "offset": 232.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "need to spin up more servers and",
      "offset": 235.44,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "remember to spin them down and wait for",
      "offset": 236.72,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "them to spin up and possibly lose",
      "offset": 238.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "traffic in that time. Not fun. And",
      "offset": 239.439,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that's why the world started moving",
      "offset": 241.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "towards a serverless model, especially",
      "offset": 243.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "for the short-term compute needs that",
      "offset": 245.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "most web apps have because most users",
      "offset": 247.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "are making a bunch of requests or",
      "offset": 249.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "sometimes just one request doing a lot",
      "offset": 252,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of DB and then not doing anything for a",
      "offset": 253.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "while. And the idea of serverless isn't",
      "offset": 256.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you don't have servers. Obviously your",
      "offset": 258.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "code runs on a server. The point is that",
      "offset": 260.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the actual like server setup that you",
      "offset": 261.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "have like the amount of provisioned",
      "offset": 264.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "servers you have running is directly",
      "offset": 266.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "reflective of the number of users",
      "offset": 268.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "currently making requests. So previously",
      "offset": 270.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the way the world worked is you had your",
      "offset": 272.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "one server and all of your requests went",
      "offset": 274.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to this. So you had a bunch of different",
      "offset": 276.479,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "requests from all sorts of different",
      "offset": 278.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "users, hitting the same server, taking",
      "offset": 279.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "however long they need to take, and then",
      "offset": 282.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "sending the responses on the other end",
      "offset": 284.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "out to the users on the other side. But",
      "offset": 285.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "if you ever needed more requests than",
      "offset": 288.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "this could fit, that was when you ran",
      "offset": 290.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "into problems. You'd have to either",
      "offset": 292.479,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "manually spin up a server and hold these",
      "offset": 293.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "requests until it was ready or do",
      "offset": 295.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "something else. Serverless had the goal",
      "offset": 297.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "of flipping this dynamic entirely. So",
      "offset": 299.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "instead of being 10 bucks a month for a",
      "offset": 301.919,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "server, it's build on gigabyte hours is",
      "offset": 304.08,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "the usual phrasing for it. So gigabyte",
      "offset": 307.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "hour is how many gigabytes of RAM over",
      "offset": 310.639,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "how many hours is the thing that you'd",
      "offset": 313.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "be build on. So now each of these",
      "offset": 315.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "requests instead of having its own",
      "offset": 317.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "server, they each have their own",
      "offset": 319.44,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "separate instance that is alive for as",
      "offset": 322.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "long as the request needs it to be and",
      "offset": 324.479,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "then dies immediately after. And if each",
      "offset": 325.84,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "of these requests takes a very small",
      "offset": 328.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "amount of time to resolve, like 100 to",
      "offset": 329.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "200 milliseconds just to get the DB",
      "offset": 332.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "requests done, generate the HTML and",
      "offset": 334,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "send it to the user, this makes a ton of",
      "offset": 335.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "sense. So if you think of this, like",
      "offset": 337.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "each of these takes 100 ms,",
      "offset": 339.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "we shrink it like that. Then this model",
      "offset": 342.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "makes a ton of sense. And since more and",
      "offset": 344.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "more compute has been working this way,",
      "offset": 347.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I say as somebody who built a lot of",
      "offset": 349.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "things like this, this model actually",
      "offset": 351.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "was really really good for web",
      "offset": 353.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "applications and like crowd apps and",
      "offset": 355.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "e-commerce and all those types of things",
      "offset": 357.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "because you never have to worry about",
      "offset": 358.88,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "having enough servers for your users and",
      "offset": 360,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the users are making such short requests",
      "offset": 361.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that if you add all of these millisecond",
      "offset": 364.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "times up, it's still not going to cost",
      "offset": 366.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you a whole bunch of money. So it ends",
      "offset": 367.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "up being often a good bit cheaper than",
      "offset": 369.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "if you provisioned a big enough server",
      "offset": 371.52,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "because you don't have to keep the",
      "offset": 373.12,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "server provisioned during downtime and",
      "offset": 374.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you're just paying for the time spent",
      "offset": 375.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "requesting. So what's the problem? Well,",
      "offset": 376.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "let's say like these four requests are",
      "offset": 379.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "people requesting their user data like",
      "offset": 382.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "their email address that they use to",
      "offset": 384.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sign up or their chat history on T3",
      "offset": 385.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "chat. Those all happen really quickly.",
      "offset": 387.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "That's all good. But what happens when",
      "offset": 389.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "you start generating a response from",
      "offset": 391.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "OpenAI? This stops being 100",
      "offset": 393.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "milliseconds and starts being 20",
      "offset": 395.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "seconds. Now imagine you have a bunch of",
      "offset": 398.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "users doing this constantly. Now you're",
      "offset": 401.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "not adding up a 100 milliseconds times",
      "offset": 403.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "4. You're adding up 20 seconds. And",
      "offset": 405.68,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "suddenly having an individual server",
      "offset": 408.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "running for each of these requests sucks",
      "offset": 411.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "because there's a second dimension we",
      "offset": 413.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have to be considerate of. We were to",
      "offset": 414.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "put these two axes. Axis one is",
      "offset": 417.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "duration. This is how long does the",
      "offset": 420.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "request take to run. And axis 2 is",
      "offset": 423.12,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "intensity, like how much CPU is actually",
      "offset": 426.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "being used during this request. So for",
      "offset": 429.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "something like your Nex.js app, it's not",
      "offset": 431.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "using a whole lot of CPU when it's",
      "offset": 434,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "making a DB request, but it uses a",
      "offset": 435.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "decent bit when it's turning that JSON",
      "offset": 437.039,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "into HTML to send to the user. Or if",
      "offset": 438.639,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you're doing something like image",
      "offset": 440.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "encoding where you're re-encoding a PNG",
      "offset": 441.919,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "into an AVIF or something like that like",
      "offset": 444.16,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "we're adding for T3 chat for partial",
      "offset": 446.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "image gen. Those things take a good bit",
      "offset": 447.919,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "of CPU and that means that you're",
      "offset": 450,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "limited with how many things you can do",
      "offset": 451.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "on a server. If you have 10 users",
      "offset": 453.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "re-encoding images on a server that's",
      "offset": 454.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "way more compute than even like a",
      "offset": 456.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "thousand users who are just waiting for",
      "offset": 458.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a DB response to come in. So making sure",
      "offset": 460.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that you're paying attention to the",
      "offset": 464,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "actual CPU usage relative to the",
      "offset": 465.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "duration is key. This is where different",
      "offset": 468.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "workloads get interesting though.",
      "offset": 470.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Something like rendering your Nex.js app",
      "offset": 472.319,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "is like I would say medium usage low",
      "offset": 474.8,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "duration. So SSR medium usage low",
      "offset": 477.919,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "duration. Something like image",
      "offset": 481.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "re-encoding. Image encoding is high",
      "offset": 484.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "usage medium duration. And this is the",
      "offset": 486.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "interesting thing. There's this area in",
      "offset": 489.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "the corner here of like high duration,",
      "offset": 491.919,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "low usage. That has been a really rare",
      "offset": 495.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "thing. So there hasn't been much",
      "offset": 498.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "innovation in this corner for a while",
      "offset": 500,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "because there hasn't been a need to. The",
      "offset": 502.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "goal historically has been how do we",
      "offset": 503.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "reduce the duration of our requests.",
      "offset": 506,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Almost all the work that companies like",
      "offset": 508.319,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "Verscell have put in that all these",
      "offset": 509.919,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "database companies have put in, they're",
      "offset": 511.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "focused on how do we shift down the",
      "offset": 512.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "duration chain so that things can happen",
      "offset": 514.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "faster so users have a faster and better",
      "offset": 516.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "experience on the web. The problem is",
      "offset": 518.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there is this new workload that fits",
      "offset": 520.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "really well in this corner that I've",
      "offset": 522.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "been hinting at a bit. LM generation",
      "offset": 523.36,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "requests. And to be clear, I am not",
      "offset": 526.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "saying the actual running of the model",
      "offset": 528.959,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "to take in an input and create the new",
      "offset": 532,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "tokens that you send to the user. What",
      "offset": 535.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I'm referring to is the endpoint that",
      "offset": 537.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "takes the user's request, attaches",
      "offset": 540.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "whatever metadata, whatever memory or",
      "offset": 542.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "whatever else, and then sends that to",
      "offset": 543.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the OpenAI API with the services API",
      "offset": 546.08,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "key, and then receives that stream and",
      "offset": 548.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then streams it down to the user. that",
      "offset": 550.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "ends up being really high duration and",
      "offset": 553.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "extraordinarily low CPU costs. And that",
      "offset": 555.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "ratio is very strange. It is not",
      "offset": 559.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "something that most of these services",
      "offset": 562.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "were built for except for one,",
      "offset": 564.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Cloudflare. Cloudflare's infra",
      "offset": 566.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "specifically the Cloudflare worker",
      "offset": 568.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "product has really low CPU performance.",
      "offset": 569.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "If you take a given like server rendered",
      "offset": 573.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "task and run it on Verscell or on Lambda",
      "offset": 575.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "with node and then run the same exact",
      "offset": 578.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "code on workers, it will take two to",
      "offset": 580.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "four times longer to actually give you a",
      "offset": 583.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "response if it's the CPU that you're",
      "offset": 585.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "measuring. So if you take some JSON and",
      "offset": 587.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "convert it to HTML with React and",
      "offset": 590,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Next.js, JS that'll take four times",
      "offset": 591.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "longer on Cloudflare because the CPUs",
      "offset": 593.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "are weaker and their virtualization",
      "offset": 595.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "layer is different versus on Verscell or",
      "offset": 596.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Lambda or even a traditional server",
      "offset": 598.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "where you have some dedicated CPU",
      "offset": 600.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "allocation and a real like kernel",
      "offset": 601.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "running real Linux running real node on",
      "offset": 604.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "top of it. That ends up being",
      "offset": 606.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "significantly faster and also with",
      "offset": 607.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "something like Lambda you can up the",
      "offset": 609.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "provisioning so it has more compute when",
      "offset": 611.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it needs it. This is where things get",
      "offset": 613.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "interesting because Cloudflare's infra",
      "offset": 614.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "has historically been bad for SSR",
      "offset": 617.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "workloads, but it's been really good for",
      "offset": 619.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "what I've referred to as the world's",
      "offset": 621.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "most advanced switch statement. If",
      "offset": 624,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you're effectively using Cloudflare as a",
      "offset": 625.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "way to resolve a request and point it in",
      "offset": 627.76,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "the right direction and do basic changes",
      "offset": 629.519,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "to it, Cloudflare has been really good.",
      "offset": 630.959,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "And one of the things that makes",
      "offset": 632.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Cloudflare's model so special is their",
      "offset": 633.839,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "net CPU billing. So let's redraw this",
      "offset": 636.16,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "here because remember there's two",
      "offset": 641.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "numbers we care about. There is the",
      "offset": 643.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "duration and there is the actual CPU",
      "offset": 644.72,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "usage. So let's say a user is making a",
      "offset": 647.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "request for their metadata. User",
      "offset": 650.959,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "requests their metadata. This request",
      "offset": 652.64,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "will probably be pretty fast. 100",
      "offset": 656.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "milliseconds if your database is fast",
      "offset": 658.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "enough. This request probably isn't",
      "offset": 660.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "going to need to do too much in terms of",
      "offset": 662.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "CPU. If we were to measure this by the",
      "offset": 664.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "amount of CPU utilized and we'll just",
      "offset": 667.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "measure it in milliseconds as well to",
      "offset": 670.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "make it easy. Let's say it was 10",
      "offset": 671.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "milliseconds net CPU. So if we shrink",
      "offset": 673.839,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "this accordingly, it ends up looking",
      "offset": 677.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like that. Actually quite a bit less if",
      "offset": 679.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "we make this to scale. I'll do that",
      "offset": 681.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "instead. There we go. You get the idea.",
      "offset": 683.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "A request from a user that takes 100",
      "offset": 686.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "milliseconds of wall clock. If it's",
      "offset": 688.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "spending most of that time waiting for",
      "offset": 690.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the DB, it's not going to use much CPU.",
      "offset": 692.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "To expand this literally to make it a",
      "offset": 695.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "little clearer, red is when we're being",
      "offset": 697.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "built. Hollow is when we're not because",
      "offset": 699.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "something else is happening. So that",
      "offset": 702,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "little red section in here, we are",
      "offset": 704.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "requesting from DB. That takes a very",
      "offset": 706.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "small amount of time. Then we wait for I",
      "offset": 709.279,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "don't know 85 milliseconds. Wait for DB",
      "offset": 712,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "response.",
      "offset": 714.959,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "And at the end here, we have a little",
      "offset": 716.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "more compute to take that DB response,",
      "offset": 718.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "re-encode it in JSON or whatever format",
      "offset": 720.24,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "you want it to, and then send it to the",
      "offset": 721.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "user. So in this 100 millisecond clock",
      "offset": 723.519,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "time, we're probably doing at most 15",
      "offset": 726.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "milliseconds of CPU actually running. If",
      "offset": 728.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "you were inside this box and you looked",
      "offset": 731.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "at how much like CPU utilization it was",
      "offset": 733.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "using, you'd see it at around 100% then",
      "offset": 735.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "less than 5% probably like one to zero",
      "offset": 738.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "depending on how much other stuff's",
      "offset": 741.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "going on in the box. Then back up to",
      "offset": 742.8,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "100% for a brief moment then the data",
      "offset": 744.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "goes down to the user. The thing that",
      "offset": 746.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "made fluid compute really cool which was",
      "offset": 748.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Verscell's way of parallelizing things",
      "offset": 750.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "on a given lambda that I've covered a",
      "offset": 752.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "bunch in the past is that it could",
      "offset": 754.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "effectively stuff other people's",
      "offset": 755.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "requests in when this happened. So here,",
      "offset": 757.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "since this CPU is not doing anything, it",
      "offset": 761.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "will stuff in another user's request.",
      "offset": 763.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "It'll use some CPU, do the thing, and",
      "offset": 765.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "when this one's being waited to resolve,",
      "offset": 767.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "we can have another one and another one.",
      "offset": 769.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "And now what we're being buil",
      "offset": 772.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "for this whole window. So let's say that",
      "offset": 775.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this ends up being 150 milliseconds of",
      "offset": 777.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "total compute. So now we squeezed 400",
      "offset": 780,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "millisecond requests into 150",
      "offset": 782.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "millisecond window. So you're not going",
      "offset": 785.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "to be build for 400 milliseconds. You're",
      "offset": 787.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "going to be build for 150. That's really",
      "offset": 789.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "cool. You know what would be way cooler?",
      "offset": 791.92,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "Remember how we said that like each of",
      "offset": 793.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "these like totals to 15? Let's even",
      "offset": 794.959,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "round up a bit. Let's say each of these",
      "offset": 798.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "is 10. So it's 20 milliseconds of",
      "offset": 800.959,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "compute and it's 150 milliseconds",
      "offset": 803.519,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "total. So 20 40 60 80. If we take all of",
      "offset": 807.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "these, remember each of them is about",
      "offset": 811.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "10. You'll see this is quite a bit",
      "offset": 812.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "shorter. than what we ended up paying",
      "offset": 815.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for. It's like way less than half.",
      "offset": 817.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Actually, I think in this case it's a",
      "offset": 819.92,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "bit over half technically. The point",
      "offset": 821.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "being that happens because there's a lot",
      "offset": 822.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "of dead areas in this. We have here",
      "offset": 825.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "where nothing happened. I'll use blue to",
      "offset": 827.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "show those dead areas. We have here to",
      "offset": 829.68,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "here where nothing happened. We have",
      "offset": 833.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "another dead zone there as well. These",
      "offset": 836.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "dead areas you're still paying for",
      "offset": 838,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "because you have to have the server spun",
      "offset": 840.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "up waiting for these requests to be",
      "offset": 842.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "completed. When you're looking at",
      "offset": 844.399,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "something like a 100 millisecond request",
      "offset": 845.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to go get something from the database,",
      "offset": 847.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this looks pretty bad. If we're a little",
      "offset": 849.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "more realistic with server rendered",
      "offset": 851.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "loads, like instead of this user",
      "offset": 853.76,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "requesting their metadata, they're",
      "offset": 855.68,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "requesting their user page. That might",
      "offset": 856.959,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "need a little more CPU in the end. We",
      "offset": 860.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "change it to 150 millconds. And at the",
      "offset": 863.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "end, say it needs a lot more compute",
      "offset": 864.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "after it gets the response from the",
      "offset": 866.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "database to actually render the page.",
      "offset": 868.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "Something like this would benefit a lot",
      "offset": 870.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "more from the model that Verscell had",
      "offset": 871.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "with fluid compute because such a high",
      "offset": 873.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "percentage of the time is still spent",
      "offset": 875.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "doing real work that it's fine. But what",
      "offset": 877.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "about the thing we were talking about",
      "offset": 880.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "before these dreadful 20 to sometimes",
      "offset": 881.92,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "like 8 minute long requests? How much",
      "offset": 885.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "compute do you guys think those are",
      "offset": 889.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "using? So let's take a a 20 second wall",
      "offset": 890.639,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "clock generation. We have this initial",
      "offset": 894.079,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "block at the start that is verifying the",
      "offset": 897.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "user and their data. So we verify the",
      "offset": 900.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "user, we send the request off to open",
      "offset": 902.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "AAI or whatever other API and then we",
      "offset": 904.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "are waiting and we keep waiting and we",
      "offset": 906.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "keep waiting",
      "offset": 909.519,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and then eventually near the end we get",
      "offset": 912.079,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the full response and we send that down",
      "offset": 914.959,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to the user. Maybe we persist in our",
      "offset": 917.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "database or let's say we are sending",
      "offset": 918.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "back the tokens as we get them which is",
      "offset": 920.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "very common. Each of those takes",
      "offset": 923.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "literally like two cycles of CPU. So we",
      "offset": 926.399,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "have a ton of these super super like",
      "offset": 929.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "impossibly thin lines randomly",
      "offset": 931.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "throughout where a tiny tiny bit of CPU",
      "offset": 933.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "is used. And on this 20 second wall",
      "offset": 935.92,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "clock, not only are we not using seconds",
      "offset": 939.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of CPU, not only are we not using",
      "offset": 942.399,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "milliseconds, usually a AILM generation",
      "offset": 944.24,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "call can be measured in nanoseconds of",
      "offset": 947.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "CPU used. So something even as crazy as",
      "offset": 950.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a 20 second wall clock time could be as",
      "offset": 953.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "low as 100 nano seconds of compute.",
      "offset": 955.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "That's the problem. That ratio is",
      "offset": 959.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "insane. If you look at how much compute",
      "offset": 961.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "is used relative to the amount of time",
      "offset": 963.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the request took, that's where this",
      "offset": 965.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "pricing gets really rough. That is the",
      "offset": 968.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "original pricing because now I have this",
      "offset": 970.639,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "20 second runtime I am paying for even",
      "offset": 973.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "though I'm using almost no CPU during",
      "offset": 975.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "most of it. And my honest guess is that",
      "offset": 978.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "when more and more of these AI workloads",
      "offset": 980,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "started appearing on Verscell, their",
      "offset": 981.519,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "average CPU like usage and like the",
      "offset": 983.199,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "percentage that their CPUs were utilized",
      "offset": 985.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "during requests probably went from like",
      "offset": 987.519,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "60 to 80% down to less than 5%. And when",
      "offset": 989.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "I look at our CPU utilization for our",
      "offset": 992.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "box, especially if we turn off fluid,",
      "offset": 995.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the CPU utilization is so close to zero,",
      "offset": 997.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "it's funny because you have to just sit",
      "offset": 999.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "there kind of when you're waiting for",
      "offset": 1002.399,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "the response. If you do the thing I",
      "offset": 1003.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "showed earlier with fluid and you force",
      "offset": 1005.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "a lot of things into that same lambda",
      "offset": 1007.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "request, it helps but it doesn't help",
      "offset": 1009.519,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "enough because again Cloudflare bills on",
      "offset": 1011.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "net CPU. So they are only charging you",
      "offset": 1014.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for these red boxes. They are only",
      "offset": 1016.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "charging you when the CPU is going and",
      "offset": 1018.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "they could do that because they don't",
      "offset": 1021.519,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "have to spin up a new kernel for every",
      "offset": 1023.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "dev. If you are a dev deploying on",
      "offset": 1024.959,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Versell and I'm a dev deploying on",
      "offset": 1026.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Versell, we both have our own code. We",
      "offset": 1028.319,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "both deploy. Even with fluid when it's",
      "offset": 1030.48,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "sharing the node to do multiple things",
      "offset": 1033.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "at once, it only shares across a given",
      "offset": 1035.439,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "user in a given binary of code. So your",
      "offset": 1038.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "code can't run on my lambda when my",
      "offset": 1041.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "lambda is running. On cloudflare, the",
      "offset": 1044.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "abstraction isn't at like the kernel or",
      "offset": 1046.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "firecracker or lambda level. The",
      "offset": 1048.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "abstraction is higher. It's in V8, the",
      "offset": 1050.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actual engine. They built the worker",
      "offset": 1052.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "isolate layer which allows them to have",
      "offset": 1054.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "basically the event loop being shared.",
      "offset": 1056.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "So if you know the JavaScript event loop",
      "offset": 1058.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "like requests are made, functions get",
      "offset": 1060.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "queued up and executed and data like",
      "offset": 1062.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "flows through it like that. The way",
      "offset": 1064.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Cloudflare works is effectively when you",
      "offset": 1065.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "don't have anything on your event loop,",
      "offset": 1067.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "when you don't have anything cued ready",
      "offset": 1069.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "to go, someone else has a request for",
      "offset": 1070.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "their own code running on the same",
      "offset": 1072.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "instance. That's what was so powerful",
      "offset": 1074.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "about Cloudflare stuff is their event",
      "offset": 1076.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "system means your code can be running",
      "offset": 1079.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "and doing nothing and cost them nothing",
      "offset": 1081.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "because that same V8 instance can be",
      "offset": 1083.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "resolving other people's requests with",
      "offset": 1085.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "their own separate code at the same",
      "offset": 1087.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "time. There's like four options here.",
      "offset": 1088.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "You have traditional servers, you have",
      "offset": 1091.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "serverless where it's one instance per",
      "offset": 1093.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "request, so one instance per user",
      "offset": 1095.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "effectively. You have fluid compute, the",
      "offset": 1097.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "serverless servers things that Verscell",
      "offset": 1099.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "did where you have one node that can",
      "offset": 1101.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "resolve multiple users requests but only",
      "offset": 1103.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "has one binary like one piece of code",
      "offset": 1105.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "running on it. And then you have the",
      "offset": 1106.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Cloudflare solution which is lots of",
      "offset": 1108.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "different developers codes running on",
      "offset": 1110.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the same box at the same time and it's",
      "offset": 1112.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "abstracting on like the per function",
      "offset": 1114.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "call level. And when I tell you the",
      "offset": 1116.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "price is massively different, I mean it.",
      "offset": 1118.64,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "As we see here, Cloudflare bills on the",
      "offset": 1121.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "CPU time, the net CPU cost, the amount",
      "offset": 1124.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of time the CPU is actually invocating.",
      "offset": 1127.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And they charge 2 cents per million CPU",
      "offset": 1129.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "milliseconds. They build it in",
      "offset": 1132.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "milliseconds because that's like how",
      "offset": 1134.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "much these requests take. It's measured",
      "offset": 1135.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "in nanconds and milliseconds, not",
      "offset": 1137.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "seconds and minutes like it is on other",
      "offset": 1139.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "platforms because they're charging based",
      "offset": 1141.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on that. There's a max of 5 minutes of",
      "offset": 1142.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "CPU time per invocation, but that's CPU",
      "offset": 1145.039,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "time. the actual request doesn't have a",
      "offset": 1148.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "duration limit. So, it could take 2 days",
      "offset": 1151.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to resolve and it's fine as long as it's",
      "offset": 1153.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not doing anything with the CPU in that",
      "offset": 1156,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "time. Really cool. And Versel finally",
      "offset": 1157.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "decided to match that. And this is",
      "offset": 1161.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "exciting to me because I've actually",
      "offset": 1162.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "argued with them a bunch about this",
      "offset": 1164.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "because I compared our numbers on Fluid",
      "offset": 1165.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to our numbers on serverless to my",
      "offset": 1167.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "friends numbers on Cloudflare with",
      "offset": 1169.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "workers and there was a pretty rough gap",
      "offset": 1171.28,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "there in terms of the cost. Like I know",
      "offset": 1175.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "people who were doing more inference",
      "offset": 1177.36,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "than us that had bills that were like a",
      "offset": 1178.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "hundth of hours for compute because it",
      "offset": 1180.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "doesn't matter how many thousands of",
      "offset": 1183.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "requests you can stuff into a 30-cond",
      "offset": 1185.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "window if the alternative is being",
      "offset": 1189.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "charged for one millisecond per request",
      "offset": 1191.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "instead. I can't go into details of",
      "offset": 1194.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "these numbers cuz the ones I know are",
      "offset": 1196.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "very much under NDAs, but I can tell you",
      "offset": 1198.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "it is as much as a 100x difference. And",
      "offset": 1200.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it's pretty easy to see that difference",
      "offset": 1203.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "if you just think through the diagram",
      "offset": 1205.2,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "here. If I have requests that take 80",
      "offset": 1207.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "seconds and one of these 80 second",
      "offset": 1211.039,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "requests uses I I'll even go higher than",
      "offset": 1212.96,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "normal 5 milliseconds of CPU. Best case",
      "offset": 1216.16,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "we could squeeze like 100 things into a",
      "offset": 1220.799,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "fluid compute instance. So worst case",
      "offset": 1223.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with serverless if we have a 100",
      "offset": 1226.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "requests that match this. So we'll say",
      "offset": 1228.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "100 requests, 80 seconds, 5 milliseconds",
      "offset": 1229.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "of CPU. If we have a normal server,",
      "offset": 1233.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "we're just paying the server cost.",
      "offset": 1234.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "That's hard to measure. If we do",
      "offset": 1236.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "serverless, then we're paying that 80",
      "offset": 1237.84,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "seconds* 100 equals 8,000",
      "offset": 1241.28,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "seconds. That's rough. Fluid, let's say",
      "offset": 1246.08,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "we squeeze them all in and it's it all",
      "offset": 1250.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "fits. So it's 80 seconds time 1 because",
      "offset": 1252.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we're only invocating once. That's 80",
      "offset": 1255.039,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "seconds. Net CPU billing 5 milliseconds",
      "offset": 1257.2,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "times 100 equals 500 milliseconds which",
      "offset": 1261.679,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "is equal 2.5 seconds. Is this a real",
      "offset": 1265.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "number from a real dashboard right now?",
      "offset": 1269.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "No. But I can tell you from my",
      "offset": 1271.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "experience this lines up. This is",
      "offset": 1273.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "accurate to the things I've seen. The",
      "offset": 1276.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "gap's insane because the amount of CPU",
      "offset": 1278.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "used for these inference tasks is",
      "offset": 1280.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "relatively low in comparison. Like the",
      "offset": 1283.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "80 to 5 millisecond ratio. This is the",
      "offset": 1286.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "thing to pay attention to. A request",
      "offset": 1288.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that takes 80 seconds can use as little",
      "offset": 1289.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "as 5 milliseconds of CPU. There is",
      "offset": 1291.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "nothing you can do to make this number",
      "offset": 1293.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "worth paying for. You have to find a way",
      "offset": 1296.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to charge this number instead if you",
      "offset": 1298.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "want to compete with Cloudflare. And",
      "offset": 1300,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this is what Verscell just announced.",
      "offset": 1301.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "They are now going to charge for this",
      "offset": 1303.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "instead of this. And that's probably",
      "offset": 1305.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going to cost them a ton of money behind",
      "offset": 1307.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the scenes depending on how they have it",
      "offset": 1309.36,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "provisioned. I don't know the details of",
      "offset": 1310.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "how they implemented this because the",
      "offset": 1311.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "reason Cloudflare can charge on this is",
      "offset": 1313.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "during that 80 seconds they can run",
      "offset": 1315.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "someone else's code on the same node. On",
      "offset": 1318,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "versell they can't do that because it is",
      "offset": 1320.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "running its own kernel, its own node",
      "offset": 1322.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "instance. So if I'm not doing anything,",
      "offset": 1324.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "they can't just spin someone else's code",
      "offset": 1326.32,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "up on the same box or on the same",
      "offset": 1327.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "instance. So let's read what they've",
      "offset": 1329.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "done and how they did it. Introducing",
      "offset": 1330.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "active CPU pricing. Of course, they had",
      "offset": 1332.96,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "to come up with their own name for it.",
      "offset": 1334.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Fluid compute exists for a new class of",
      "offset": 1335.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "workloads. IObound backends like AI",
      "offset": 1338,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "inference agents, MCP servers and",
      "offset": 1340.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "anything that needs to scale instantly",
      "offset": 1342.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "but often remains idle between",
      "offset": 1344.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "operations. This is the key that scale",
      "offset": 1346.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "instantly part is so useful both for",
      "offset": 1348.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "like surges in traffic but also the",
      "offset": 1351.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "other side where you're I don't know",
      "offset": 1354.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "spinning up a preview deployment when",
      "offset": 1356.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "somebody files a PR. Having things that",
      "offset": 1357.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "can instantaneously on demand spin up",
      "offset": 1360.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and down like that is so useful for all",
      "offset": 1362.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the different layers of my stack. These",
      "offset": 1365.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "workloads don't follow traditional quick",
      "offset": 1367.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "request response patterns which again",
      "offset": 1369.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "this is what Verscell has optimized for",
      "offset": 1370.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "historically. How can we get the data",
      "offset": 1372.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "close to the server? How can we format",
      "offset": 1374.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the data faster so there's less time",
      "offset": 1376.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "being spent generating the right thing?",
      "offset": 1378.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "How can we cache stuff to skip the",
      "offset": 1380.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "compute layer entirely? What can we do",
      "offset": 1383.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to reduce the amount of time it takes to",
      "offset": 1384.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "resolve a request? That's been",
      "offset": 1386.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Verscell's goal since day one. That does",
      "offset": 1388,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "not work in the AI world. These are",
      "offset": 1390.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "longunning unpredictable workloads and",
      "offset": 1392.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "they use cloud resources in new ways.",
      "offset": 1395.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Fluid quickly became the default compute",
      "offset": 1398.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "model on Verscell, helping teams cut",
      "offset": 1399.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "costs by up to 85% through optimizations",
      "offset": 1401.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like infunction concurrency. Today we're",
      "offset": 1404.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "taking the efficiency and cost savings",
      "offset": 1406.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "further with a new pricing model. You",
      "offset": 1407.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "pay CPU rates only when your code is",
      "offset": 1409.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "actively using CPU. Huge from servers to",
      "offset": 1412.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "serverless. In the early days of cloud",
      "offset": 1415.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "comput, teams ran longived servers. You",
      "offset": 1416.96,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "had to manage provisioning, handle",
      "offset": 1418.799,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "scaling, yada yada. You get the idea.",
      "offset": 1419.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Serverless changed that. It abstracted",
      "offset": 1421.679,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "away infrastructure configuration and",
      "offset": 1423.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "introduced automatic scaling. Each",
      "offset": 1425.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "request triggered its own isolated",
      "offset": 1426.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "instance just as I said. So here we see",
      "offset": 1428.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it starts with a lot of compute, does",
      "offset": 1430.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "nothing for a while, and then ends. And",
      "offset": 1432.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you're paying for that whole time from",
      "offset": 1434.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "serverless to fluid. Here's the",
      "offset": 1436.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "difference. Now you can squeeze them all",
      "offset": 1438,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "into one fluid node. Didn't they say 85%",
      "offset": 1439.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "before? Now it's saying up to 90%. From",
      "offset": 1443.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "my experience, it's been 85 to 90. So",
      "offset": 1445.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "it's funny. Those are the two numbers.",
      "offset": 1447.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I've seen both in our dashboards. And",
      "offset": 1448.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "then from fluid to active CPU. Fluid",
      "offset": 1450.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "improved performance and cost, but there",
      "offset": 1454,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "was still room to optimize. Even with",
      "offset": 1455.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "high concurrency, there could still be",
      "offset": 1457.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "moments where all invocations are",
      "offset": 1458.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "waiting on external resources and no",
      "offset": 1460.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "code is actively running. During these",
      "offset": 1462.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "idle periods, functions stay in memory,",
      "offset": 1464.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "do no work, yet they're still incurring",
      "offset": 1466,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "CPU cost. Again, if we look at this this",
      "offset": 1467.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "way, we just want to pay for the parts",
      "offset": 1470.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "where the CPU is working. This aligns",
      "offset": 1472.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with actual usage. Compute costs scale",
      "offset": 1474.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with real work, not just with time a",
      "offset": 1477.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "function is alive. So what's funny here",
      "offset": 1479.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "is that something like our conversion of",
      "offset": 1481.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "images is effectively free CPU costwise",
      "offset": 1483.279,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "because we are already running the",
      "offset": 1486.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "nodes. The CPU is being paid for. We",
      "offset": 1489.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "might as well use it. This is going to",
      "offset": 1490.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "make that feel much more expensive",
      "offset": 1492.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "because we're actually paying for that",
      "offset": 1494.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "compute now where previously we",
      "offset": 1495.919,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "effectively weren't. But that also means",
      "offset": 1497.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "our bills are going to be way way",
      "offset": 1499.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "cheaper overall because most of our",
      "offset": 1500.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "workloads aren't using CPU for the",
      "offset": 1502.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "majority of the time. So the active CPU",
      "offset": 1504.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "pricing model fluid compute now charges",
      "offset": 1506.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "based on three key metrics. Active CPUs",
      "offset": 1508.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "which reflects the compute time your",
      "offset": 1510.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "code actively is executing on a virtual",
      "offset": 1511.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "CPU measured in milliseconds calculated",
      "offset": 1513.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "as the number of vCPUs allocated",
      "offset": 1515.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "multiplied by the time they're actively",
      "offset": 1517.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "being used for starts at.128",
      "offset": 1519.12,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "per hour. So let's run the math here",
      "offset": 1522.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "quick. You know how I am. I like to know",
      "offset": 1525.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the hard numbers. The versel is this per",
      "offset": 1527.44,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "hour which in milliseconds 2 cents per",
      "offset": 1531.2,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "million CPU milliseconds. This is going",
      "offset": 1534.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to be annoying math because these",
      "offset": 1538.24,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "numbers are not compatible. That's",
      "offset": 1539.52,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "definitely intentional on both parties.",
      "offset": 1540.799,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "Okay, so how many milliseconds are in an",
      "offset": 1545.919,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "hour? Okay, hour minute time a th00and.",
      "offset": 1547.919,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "Okay, so that's 3.6 million",
      "offset": 1552.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "milliseconds. So that's 3.6 6 times",
      "offset": 1555.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "more. Is it going to be exact? It's",
      "offset": 1558.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "going to be 0.02 * 3.6.",
      "offset": 1560.32,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "No, it's about it's a bit more. Okay.",
      "offset": 1563.84,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "So, Cloudflare is charging 7.2 cents per",
      "offset": 1567.12,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "hour of compute. I'm just going to get",
      "offset": 1571.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "rid of the other Cloudflare number",
      "offset": 1573.679,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "because I think billing in hours is a",
      "offset": 1574.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "little more sensical. So, Cloudflare is",
      "offset": 1576.559,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "still cheaper per hour, but it's not",
      "offset": 1579.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that big a gap. It's a bit under 50%",
      "offset": 1582.159,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "difference. There is also a bill per",
      "offset": 1585.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "invocation though, so we will get to",
      "offset": 1588.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that in a second because I believe",
      "offset": 1590.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Versell has it as well. They also have",
      "offset": 1592,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to charge for provision memory which",
      "offset": 1593.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "covers the memory required to keep a",
      "offset": 1595.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "function alive while it's running",
      "offset": 1596.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "measured in gigabyte hours and build at",
      "offset": 1598,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "a much lower rate less than 10% of",
      "offset": 1599.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "active CPU thanks to Fluid's ability to",
      "offset": 1601.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "reuse memory across multiple",
      "offset": 1603.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "concurrentifications. Starts at 1 cent",
      "offset": 1604.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "per gigabyte hour. Cloudflare doesn't",
      "offset": 1606.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "charge for this. So to be fair,",
      "offset": 1608.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Cloudflare has a much stricter limit on",
      "offset": 1611.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "memory. Yeah, workers are very limited",
      "offset": 1613.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "in how like big they can be. The amount",
      "offset": 1616.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of JS it can run is 10 megabytes of",
      "offset": 1618.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "actual binary, which sounds like a lot",
      "offset": 1621.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "for JavaScript, but it's not, especially",
      "offset": 1624.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "for serverside stuff at scale. Each",
      "offset": 1628,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "worker can use up to 128 megabytes of",
      "offset": 1630.08,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "memory. That's very low relatively",
      "offset": 1633.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "speaking. You if you architect around",
      "offset": 1636.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "that and you're not using like node APIs",
      "offset": 1639.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and things heavily, it's fine. But",
      "offset": 1641.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that's not much memory. That said,",
      "offset": 1644,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "that's 128 megs of memory per request.",
      "offset": 1646.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Where on Versel, you have the amount of",
      "offset": 1649.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "RAM that your node has across all the",
      "offset": 1651.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "requests being routed to that node. So,",
      "offset": 1654.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "it depends on what your memory",
      "offset": 1655.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "characteristics look like. But I am",
      "offset": 1657.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "going to skip that section simply",
      "offset": 1659.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "because the way memory works in these",
      "offset": 1661.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "platforms is so different that it's not",
      "offset": 1663.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "really comparable. But we'll they say",
      "offset": 1666.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "it's less than 10%. So we'll add another",
      "offset": 1668.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "10% to the Verscell bill accordingly.",
      "offset": 1670.799,
      "duration": 8.641
    },
    {
      "lang": "en",
      "text": "This is CPU cost memory versel 10% of",
      "offset": 1674,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "CPU cost and cloudflare will say free",
      "offset": 1679.44,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "and then we have the request costs",
      "offset": 1683.12,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "because invocations are charged for on",
      "offset": 1686.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "these platforms. They say for versell",
      "offset": 1689.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that like in traditional serverless they",
      "offset": 1691.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "are still billing for this. It remains",
      "offset": 1693.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "part of the overall billing 60 cents per",
      "offset": 1695.76,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "million requests. Exception for request",
      "offset": 1698.799,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "cost. Versel is 60 per 1 mil requests.",
      "offset": 1701.919,
      "duration": 8.561
    },
    {
      "lang": "en",
      "text": "Cloudflare does charge for this 30 cents",
      "offset": 1707.279,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "per additional million. So if we go",
      "offset": 1710.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "through and compare these numbers, I",
      "offset": 1713.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "would guesstimate that it averages out",
      "offset": 1715.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "to Verscell being worst case about 2x",
      "offset": 1718.399,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "more expensive. And that sounds terrible",
      "offset": 1721.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "just upfront saying like, &quot;Yeah, switch",
      "offset": 1724.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "from Versel to Cloudflare and cut your",
      "offset": 1726.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "costs in half.&quot; I'm gonna drop a number",
      "offset": 1728.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "that I probably shouldn't. The ratio of",
      "offset": 1730.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "our Gemini bill to our Verscell bill",
      "offset": 1733.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "right now, and this is before all these",
      "offset": 1735.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "changes were made. It's 1:40. Our Gemini",
      "offset": 1737.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "bill is 40 times higher than our Versel",
      "offset": 1740.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "comput bill. If you add all of our other",
      "offset": 1743.12,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "inference, it's closer to 180th. And",
      "offset": 1745.36,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "this is fluid compute costs. It's a",
      "offset": 1749.039,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "massive gap. So again, like because",
      "offset": 1752.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "these workloads tend to be expensive,",
      "offset": 1756.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "but not just for the compute, for all",
      "offset": 1758.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "these other things, that gap doesn't",
      "offset": 1760.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "matter as much as it seems. Like a 2x",
      "offset": 1763.2,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "here might feel really big. It is not",
      "offset": 1765.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "because remember when I said it was like",
      "offset": 1769.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "a 100x difference on net CPU to not.",
      "offset": 1771.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Let's say it's the same gap here. We'll",
      "offset": 1773.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "say it's 50x. What's going to happen for",
      "offset": 1775.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "us is again if we look at the Gemini",
      "offset": 1777.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "bill, it's going to be a 50x difference.",
      "offset": 1780.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Our bill is now going to be 1/400th of",
      "offset": 1783.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "our not Gemini bill of our inference",
      "offset": 1785.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "bill. I would expect after these changes",
      "offset": 1789.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "are implemented and we can run it that",
      "offset": 1791.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "our inference bill for actually paying",
      "offset": 1792.96,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Gemini open everybody else will be 400",
      "offset": 1796.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "times higher than our compute bill. And",
      "offset": 1799.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "if we switched to Cloudflare, it could",
      "offset": 1801.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "be as good as 800 times, but we would be",
      "offset": 1802.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "giving up node compatibility. We'd be",
      "offset": 1804.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "giving up things like image compression",
      "offset": 1806.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "on the same server. We'd be giving up a",
      "offset": 1809.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "bunch of the convenience around how we",
      "offset": 1811.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "deploy, how we run things on Versell,",
      "offset": 1813.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "how we manage domains, and how we do all",
      "offset": 1815.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "these other things. It's a decent",
      "offset": 1816.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "difference there, especially when you",
      "offset": 1818.799,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "consider how much higher the RAM",
      "offset": 1820.399,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "limitations are, how much faster the",
      "offset": 1821.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "compute is, and all these other things.",
      "offset": 1823.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "The argument is no longer you use",
      "offset": 1824.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Versell until it's too expensive, then",
      "offset": 1826.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "you move to Cloudflare. Now it's is the",
      "offset": 1828.24,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "benefits of Verscell worth a roughly 2x",
      "offset": 1831.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "gap to you. There are benefits and",
      "offset": 1834.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "negatives to all of this. They're both",
      "offset": 1836.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "very compelling options now. And the",
      "offset": 1838.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Versell doesn't make sense at a certain",
      "offset": 1841.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "scale thing is kind of killed by this.",
      "offset": 1843.6,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "Versel's platform now is close enough to",
      "offset": 1846.399,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "costs as long as this is implemented as",
      "offset": 1850.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "generously as they have implied through",
      "offset": 1852.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "their posting. The cost gap here is",
      "offset": 1854.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "nowhere near as bad as it used to be. It",
      "offset": 1857.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is comically less bad. I saw some",
      "offset": 1859.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "speculation about how Verscell uses",
      "offset": 1861.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "middleware and edge stuff. They did",
      "offset": 1863.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "actually formally kill edge workers in",
      "offset": 1865.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "middleware today. You still have the",
      "offset": 1868,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "middleware file. It just runs on",
      "offset": 1869.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Verscell's compute engine instead. They",
      "offset": 1870.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "did briefly play around with the worker",
      "offset": 1873.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "D model from Cloudflare is like the",
      "offset": 1874.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thing that would run on the edge, but it",
      "offset": 1876.159,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "had its benefits and negatives as most",
      "offset": 1878,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "things do and they slowly moved off of",
      "offset": 1880.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it. yet they're managing to replicate",
      "offset": 1882.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the awesome billing characteristics of",
      "offset": 1884.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that model inside of more traditional",
      "offset": 1887.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "compute engines, which is really cool. A",
      "offset": 1889.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "different way to think about this is",
      "offset": 1892.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you're paying around a 2x premium for",
      "offset": 1893.84,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "way faster CPU and actual node compat.",
      "offset": 1896.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "This isn't even including the developer",
      "offset": 1901.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "experience wins. This isn't even",
      "offset": 1902.72,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "including the cool things that Versell",
      "offset": 1904,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "platform does, all the other stuff.",
      "offset": 1905.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you're effectively paying 2x more for",
      "offset": 1906.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "your CPU to run faster, for node",
      "offset": 1908.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "compatibility, and yes, for DX stuff",
      "offset": 1910.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that I think is valuable and really",
      "offset": 1912.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "nice. Previously, this was closer to",
      "offset": 1914.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "100x difference. So, that's a nice",
      "offset": 1916.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "change. Well, next is sandboxing for",
      "offset": 1917.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "code. This is a really fun one. And",
      "offset": 1921.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "funny enough, Cloudflare also announced",
      "offset": 1923.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "their solution for this. Well, they",
      "offset": 1925.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "announced that the thing that they just",
      "offset": 1927.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "dropped is their solution for it. So,",
      "offset": 1928.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "sandboxing for code, run untrusted code",
      "offset": 1930.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "with Verscell sandbox. Versell sandbox",
      "offset": 1933.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "is a secure cloud resource powered by",
      "offset": 1935.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "fluid compute. It's designed to run",
      "offset": 1937.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "untrusted code like code generated by AI",
      "offset": 1939.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "agents in isolated and ephemeral",
      "offset": 1941.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "environments. It's a standalone SDK that",
      "offset": 1943.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "can be executed from any environment",
      "offset": 1945.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "including non-verell platforms. That's",
      "offset": 1946.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "actually really cool. So if you're",
      "offset": 1948.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "running on Cloudflare or you're running",
      "offset": 1950.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "on I don't know Heroku or traditional",
      "offset": 1952.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "AWS and you need to just spin up some",
      "offset": 1953.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sandbox that is safe and isolated and",
      "offset": 1956.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "won't screw up other things on your",
      "offset": 1958.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "platform to run some generated code or",
      "offset": 1960,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "user submitted code. This is like funny.",
      "offset": 1962.399,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I see chat already dropping eval. Yeah,",
      "offset": 1964.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it's like if evval was safe, which is",
      "offset": 1967.039,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "really cool. Also, don't get me started",
      "offset": 1969.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on Cloudflare's node compatibility. The",
      "offset": 1971.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "progress they made is awesome. It's not",
      "offset": 1973.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "real node. Do they support certain node",
      "offset": 1975.76,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "APIs in a compatible enough thing? Cool.",
      "offset": 1978.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "But if you need node, if you need",
      "offset": 1982.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "something like, I don't know, ffmpeg or",
      "offset": 1984.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "something that actually uses native code",
      "offset": 1986.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "on the platform, Cloudflare can't do",
      "offset": 1989.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that at all. And don't get me",
      "offset": 1991.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "started on trying to run Sharp through",
      "offset": 1992.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Wom on Cloudflare. Remember that 10",
      "offset": 1994.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "megabyte limit, ffmpeg Wom on",
      "offset": 1996.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Cloudflare? No, that that is not a",
      "offset": 1998.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "thing. People have claimed they have",
      "offset": 2000.559,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "done it. I have never got anybody to",
      "offset": 2001.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "share code that actually works for it.",
      "offset": 2003.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "It's all made up. You cannot run",
      "offset": 2004.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "actual workloads on Cloudflare that do",
      "offset": 2006.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "things more complex than executing",
      "offset": 2010.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "JavaScript. You You cannot do it. Don't",
      "offset": 2011.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "pretend you can. I'm tired of people",
      "offset": 2014.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pretending you can. You can't. They even",
      "offset": 2015.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "know that. That's why they put out",
      "offset": 2018.399,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "containers. Exactly. They have",
      "offset": 2019.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "containers. They are not cheap. They are",
      "offset": 2021.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "good. It's awesome that they have",
      "offset": 2023.039,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "containers now to solve this problem.",
      "offset": 2024.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "But I am so tired of people pretending,",
      "offset": 2025.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "oh, you can just wit and then Cloudflare",
      "offset": 2028.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "can do everything. No, it can't.",
      "offset": 2030.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Anyways, untrusted code. They have an",
      "offset": 2032.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "SDK for it, the Versell sandbox. It's",
      "offset": 2035.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "really cool that they're integrating",
      "offset": 2037.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this at like an SDK level where you can",
      "offset": 2038.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "create a sandbox, hand it a path or a",
      "offset": 2040.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "buffer with code. Here we're taking this",
      "offset": 2044.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "generated text. We are creating a buffer",
      "offset": 2045.919,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "from it and then running a command in",
      "offset": 2047.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that sandbox. Command node arg",
      "offset": 2050.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "script.js. Tell it where to pass the",
      "offset": 2052.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "standard out and standard error. That's",
      "offset": 2055.119,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "really cool. This is really cool. Here's",
      "offset": 2056.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the docs for the sandbox. You can see",
      "offset": 2060.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the type of stuff you have control of",
      "offset": 2062.079,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "for it. This code below. You'll set up a",
      "offset": 2064.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "sandbox with four virtual CPUs that uses",
      "offset": 2066.159,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Node22 runtime and will do all of the",
      "offset": 2068.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "following. clones the GitHub repo for a",
      "offset": 2070.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "next app, installs the depths, runs the",
      "offset": 2071.919,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "next server, listens on port 3000, open",
      "offset": 2073.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it, sandbox domain in a browser, and",
      "offset": 2076.399,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "stream logs to your terminal. Sandbox",
      "offset": 2078.399,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "will stop after the time out of 5",
      "offset": 2080.159,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "minutes that we set. Cool. Source, this",
      "offset": 2081.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is the source that we wanted to pull",
      "offset": 2084.399,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "from. Type get resource vCPUs 4. You",
      "offset": 2085.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "could also probably set memory and stuff",
      "offset": 2089.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like that there. Timeout milliseconds 5",
      "offset": 2091.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "minutes. This is their fancy helper for",
      "offset": 2093.919,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "converting a string to milliseconds. But",
      "offset": 2096.8,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "5 minutes is 300,000 milliseconds. Cool.",
      "offset": 2099.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "Ports 30,00 to open that port. Runtime",
      "offset": 2103.119,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "node 22. We then run commands. We run",
      "offset": 2105.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "the npm install command. std error stout",
      "offset": 2108.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "out to pass it. Install.exit code is not",
      "offset": 2111.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "zero. That means it failed. We log that",
      "offset": 2114.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it failed. Process.exit. Starting to dev",
      "offset": 2117.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "server. Sandbox run command. npm rundev.",
      "offset": 2119.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Same deal. Detached to true. Set a",
      "offset": 2122.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "timeout. Spawn open. Sandbox.domain",
      "offset": 2124.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "3000. Cool. That's actually really nice.",
      "offset": 2127.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Good stuff. There's a ton of potential",
      "offset": 2131.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "in the sandbox platform. I'm excited. As",
      "offset": 2132.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "I mentioned earlier, Cloudflare just",
      "offset": 2134.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "announced their way to run Docker images",
      "offset": 2135.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "and containers on Cloudflare. And it",
      "offset": 2138.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "seems like today they rushed out this",
      "offset": 2140.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "blog post on how to use Code Sandboxes",
      "offset": 2142.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "with that. That looks similar. That's",
      "offset": 2144.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "kind of funny that we got at",
      "offset": 2146.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Verscell/andbox",
      "offset": 2149.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and at Cloudflare/Sandbox within 3 hours",
      "offset": 2150.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "of each other. Kind of funny. the the",
      "offset": 2154,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "race between Cloudflare and Versell has",
      "offset": 2157.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "never been so tight. These guys are are",
      "offset": 2159.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tooth and nail fighting each other",
      "offset": 2161.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "constantly now and that's awesome",
      "offset": 2163.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "because we are the ones who benefit the",
      "offset": 2164.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "most at the end. The reason versel has",
      "offset": 2166.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "got so much cheaper is Cloudflare. The",
      "offset": 2167.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "reason Cloudflare got containers is",
      "offset": 2169.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Verscell. The reason Verscell has",
      "offset": 2170.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "sandboxes now is anthropic and the",
      "offset": 2172.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "reason Cloudflare does is also anthropic",
      "offset": 2175.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "to be fair. Yeah, like we are all moving",
      "offset": 2176.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "forward now because these two platforms",
      "offset": 2179.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "are competing so actively that we get",
      "offset": 2181.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "better stuff. And of course, as I hinted",
      "offset": 2184.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "at before, Anthropic also today dropped",
      "offset": 2186.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "their similar thing, build and share AI",
      "offset": 2189.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "powered apps with claude. They added the",
      "offset": 2191.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "ability to build, host, and share",
      "offset": 2194.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "interactive AI powered apps directly in",
      "offset": 2195.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the cloud app. Theirs is like a higher",
      "offset": 2197.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "level thing where you can share a clawed",
      "offset": 2199.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "chat and the artifact in it that is",
      "offset": 2202.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "generated can be its own like mini app",
      "offset": 2204.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "that you share with other people. Cool.",
      "offset": 2206.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Built on top of similar things. I",
      "offset": 2209.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "wouldn't be surprised if they were",
      "offset": 2211.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually using Cloudflare's solution",
      "offset": 2212.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here or maybe even Verscell's probably",
      "offset": 2214.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "cloud flares though knowing them. There",
      "offset": 2216.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "are other platforms that already did",
      "offset": 2218.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "this. I know Daytona decently well.",
      "offset": 2219.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "We're considering using them both for T3",
      "offset": 2221.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "chat and also talking about potential",
      "offset": 2223.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "sponsor in the future. Just full",
      "offset": 2225.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "disclosure might work with them later.",
      "offset": 2226.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "There are a lot of these types of",
      "offset": 2227.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "solutions where you can call the Daytona",
      "offset": 2229.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "SDK, create an instance for a given",
      "offset": 2231.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "language, hand it some source code, in",
      "offset": 2234.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this case code run console log hello",
      "offset": 2236.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "world, and then it can exec it, give you",
      "offset": 2238.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the response. This is a space that I",
      "offset": 2240.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "expect to see getting more and more",
      "offset": 2242.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "interesting over time because the need",
      "offset": 2244.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "for these types of tools to make it easy",
      "offset": 2246.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to safely execute code are more and more",
      "offset": 2248.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "valuable because AI can't really",
      "offset": 2250.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "interact with the real world, but it can",
      "offset": 2252.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "absolutely write code that does. So",
      "offset": 2254.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "expect more of these things to start",
      "offset": 2256.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "happening and also for platforms like",
      "offset": 2258.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "Verscell and Cloudflare to start pushing",
      "offset": 2259.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "them more heavily. Being able to sandbox",
      "offset": 2261.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the space where your code runs in order",
      "offset": 2263.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "for code that you don't trust to be able",
      "offset": 2266.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to run is going to be more important now",
      "offset": 2268.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "than ever. And honestly, I'm slightly",
      "offset": 2270.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "betting on Cloudflare for this one",
      "offset": 2272.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "because they're deep in the zero trust",
      "offset": 2274.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "world. It's a huge part of their product",
      "offset": 2275.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "and platform. So the idea of running",
      "offset": 2277.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "untrusted code is something that I think",
      "offset": 2279.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "fits them very well. You can also tell",
      "offset": 2281.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "this blog post was rushed out because",
      "offset": 2283.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "they didn't even put a space here. They",
      "offset": 2285.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "did on the others,",
      "offset": 2287.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "but it's very clear this was an",
      "offset": 2290.079,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "announcement they were probably working",
      "offset": 2291.68,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "on already, but they rushed out this",
      "offset": 2292.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "post in order to stay on top of Versel's",
      "offset": 2294,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "announcements. That's sandboxing. Next,",
      "offset": 2296.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "we have Q's. And Q's I'm actually really",
      "offset": 2299.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "excited about. Funny enough, it's one of",
      "offset": 2302.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the things I think Netlefi was really a",
      "offset": 2303.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "head-on. huge fans of what Netlefi was",
      "offset": 2306,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "focused on here because it's important",
      "offset": 2308.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "work especially for like long run",
      "offset": 2310.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "generation tasks that don't have partial",
      "offset": 2312.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "steps like if you're doing an image gen",
      "offset": 2315.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "or deep research or something that just",
      "offset": 2317.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "takes a long time to go having compute",
      "offset": 2320.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "running for that whole time sucks on",
      "offset": 2323.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Cloudflare since you're only build for",
      "offset": 2325.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the net CPU and it can run for an",
      "offset": 2326.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "effectively infinite duration it wasn't",
      "offset": 2329.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "too big a deal you can just spin up a",
      "offset": 2330.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "thing and let it sit and wait for a",
      "offset": 2332.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "while but we're actually having problems",
      "offset": 2333.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "right now where certain models like uh",
      "offset": 2335.04,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "03 Pro can take so long to run. We don't",
      "offset": 2337.52,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "have 03 Pro on T3 chat as a thing any",
      "offset": 2341.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "user can just use. You have to bring",
      "offset": 2344.16,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "your own API key because it's so",
      "offset": 2345.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "expensive. The problem that we've seen",
      "offset": 2346.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is that for a handful of users, it times",
      "offset": 2348.32,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "out and it's timing out because our",
      "offset": 2350.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "functions only run for 800 seconds.",
      "offset": 2353.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "That's just the value that we have",
      "offset": 2355.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "hard-coded on them. We can bump it",
      "offset": 2356.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "higher and I think the new fluid compute",
      "offset": 2358.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "changes mean we can bump it even higher.",
      "offset": 2359.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "But if the request takes 15 minutes to",
      "offset": 2361.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "resolve and we don't have enough",
      "offset": 2364.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "duration for that, that sucks. It'd be a",
      "offset": 2365.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "lot nicer if we could ceue up an action",
      "offset": 2368.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and then when it is done, run the code",
      "offset": 2370.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to persist it and show the user the",
      "offset": 2372.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "result. Cues are really helpful for",
      "offset": 2374.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "these workloads that have to wait for a",
      "offset": 2376.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "long time and do literally nothing. So",
      "offset": 2379.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you don't even need to deal with the",
      "offset": 2381.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "memory allocation in that time. This is",
      "offset": 2382.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a limited beta to be fair. This is one",
      "offset": 2384.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of the earlier products which after some",
      "offset": 2386.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of the experiences I've had with Versell",
      "offset": 2388.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "scares me. Some of these early products",
      "offset": 2390.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "are fully flushed out and ready to go.",
      "offset": 2391.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Others aren't at all. I usually try to",
      "offset": 2393.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "gut feel it with them and also I look at",
      "offset": 2395.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "what other customers are moving over",
      "offset": 2397.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "before I make my own decisions here. But",
      "offset": 2398.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "here's what they have to say. It's a",
      "offset": 2400.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "message Q service built for versell",
      "offset": 2402.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "applications in limited beta. Let you",
      "offset": 2403.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "offload work by sending tasks to a queue",
      "offset": 2405.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "where they'll be processed in the",
      "offset": 2407.359,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "background so users don't have to wait",
      "offset": 2408.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "for slow operations to finish during a",
      "offset": 2410.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "request and your app can handle retries",
      "offset": 2411.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and failures more reliably. It's kind of",
      "offset": 2413.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "like uh temporal which is crash proof",
      "offset": 2415.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "code. It lets you create workflows",
      "offset": 2418.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "around all of the code that you write.",
      "offset": 2420.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So if some part fails, you can rerun it.",
      "offset": 2422.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "It's like an infle effect type thing.",
      "offset": 2424.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "And there's also ingest I tried working",
      "offset": 2426.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "with in the past. Seemingly really good",
      "offset": 2429.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "product,",
      "offset": 2431.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "not the easiest to interact with, but",
      "offset": 2432.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "very promising stuff. Really good DX.",
      "offset": 2434.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Not very types safe initially. They",
      "offset": 2438.079,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "might have finally fixed that, but it",
      "offset": 2439.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "wasn't at the time. But these are",
      "offset": 2440.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "durable workflow and durable workload",
      "offset": 2442.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "platforms. And it seems like Verscell's",
      "offset": 2445.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "decided that they're not worth investing",
      "offset": 2447.04,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "anymore. They want to make their own",
      "offset": 2448.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "instead, which I I fully understand.",
      "offset": 2449.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Those companies are are weird and now",
      "offset": 2451.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "they're building their own thing",
      "offset": 2453.68,
      "duration": 1.919
    },
    {
      "lang": "en",
      "text": "instead. Oh, Trigger. I forgot about",
      "offset": 2454.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Trigger. They're actually awesome. I",
      "offset": 2455.599,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "need to stop forgetting about",
      "offset": 2456.8,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "trigger.dev. These guys are really",
      "offset": 2457.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "really good and they're awesome to",
      "offset": 2459.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "interact with, too. So, again, this is a",
      "offset": 2460.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "thing a lot of other places had",
      "offset": 2462.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "implemented, but Versell now has on",
      "offset": 2464.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "platform. They even have these fancy",
      "offset": 2466.319,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "dashboards where you can see how long",
      "offset": 2467.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "each part takes. And obviously, they're",
      "offset": 2468.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "leaning into AI, transcribe to video,",
      "offset": 2471.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "transcribe audio, all those types of",
      "offset": 2473.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "things. generate content runs. It has",
      "offset": 2474.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "steps. It creates the text. It then",
      "offset": 2477.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "creates an image and returns the result.",
      "offset": 2480.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "You can retry it. You can trace it. You",
      "offset": 2482.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "can do all sorts of different things",
      "offset": 2484.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "with it. Also a good use case for stuff",
      "offset": 2485.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like crowns. You get the idea. Cues and",
      "offset": 2487.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "like workflow and workload durability",
      "offset": 2490.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "solutions are important. It's cool",
      "offset": 2493.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "seeing them building them into Versell",
      "offset": 2495.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "directly. Under the hood, Verscell Q's",
      "offset": 2497.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "uses an appendon log to store messages",
      "offset": 2499.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and ensure tasks such as AI video",
      "offset": 2501.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "processing, sending emails, or updating",
      "offset": 2503.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "external services are persisted and",
      "offset": 2505.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "never lost. Key features: PubSub, so",
      "offset": 2507.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "it's topic based messaging allowing for",
      "offset": 2510.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "multiple consumer groups. Streaming",
      "offset": 2511.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "support handle payloads without loading",
      "offset": 2513.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "them entirely into memory. Ooh, that's",
      "offset": 2515.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "very interesting actually. Streamlined",
      "offset": 2517.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "O, which is annoying to set up, right?",
      "offset": 2519.2,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Cool that they did that. And a",
      "offset": 2520.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "TypeScript SDK with full type safety. I",
      "offset": 2521.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "think Trigger has that. None of the rest",
      "offset": 2524,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "do. That's a really cool thing. Send and",
      "offset": 2525.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "receive are from versell/Q",
      "offset": 2527.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "send topic message receive topic",
      "offset": 2530.079,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "consumer m dossage. Awesome. I do not",
      "offset": 2532.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "know how they made this type safe. I",
      "offset": 2535.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "would want to see how this stuff is",
      "offset": 2537.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "defined so that we know what M's shape",
      "offset": 2538.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "is there. But yeah, it's early early",
      "offset": 2540.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "access. So you have to sign up for",
      "offset": 2543.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Verscell community and express interest",
      "offset": 2544.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in order to see more. Yeah, cool stuff.",
      "offset": 2546.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I want to see a lot more before I highly",
      "offset": 2548.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "recommend it. Until then, I would",
      "offset": 2550.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "personally recommend trigger but all the",
      "offset": 2552.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "options are pretty good. So that is Q's.",
      "offset": 2554.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "Next is another one that feels tailor",
      "offset": 2556.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "made for me. Their capture killer. I'm",
      "offset": 2558.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "not going to say my previous capture",
      "offset": 2561.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "video is an essential watch, but I think",
      "offset": 2563.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it's a pretty damn good watch. If you",
      "offset": 2566.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "haven't, definitely check it out. If you",
      "offset": 2567.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "appreciate content like that, hit that",
      "offset": 2569.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "sub button. Less than half of you have.",
      "offset": 2571.76,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "It cost you nothing. Might as well,",
      "offset": 2572.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? Anyways, the Capta Killer product",
      "offset": 2574.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I've been told about for a bit. By the",
      "offset": 2576.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "time they were hitting me up to try it,",
      "offset": 2579.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "though, I'd already made the move to H",
      "offset": 2581.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Capture and was really happy. That said,",
      "offset": 2583.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "none of the major capture solutions,",
      "offset": 2586.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "those being Turnstyle, Recapture, and H",
      "offset": 2588.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Capture, none of them even have an npm",
      "offset": 2591.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "package, much less like a modern full",
      "offset": 2593.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "stack TypeScripty dev experience. They",
      "offset": 2596.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "all expect you to embed a random script",
      "offset": 2598.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in your page and manage all of the data",
      "offset": 2599.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and loading states and everything",
      "offset": 2602.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "yourself, the partial states for",
      "offset": 2603.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "invisible versus visible and all of",
      "offset": 2605.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that. The things that I want out of a",
      "offset": 2607.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "capture specifically are the following.",
      "offset": 2609.2,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "I want a good invisible mode where most",
      "offset": 2612.56,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "people will never see the capture or",
      "offset": 2616.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "even no one is running. I want low false",
      "offset": 2617.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "positives, meaning I don't want a lot of",
      "offset": 2620.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "users getting failed. I ideally don't",
      "offset": 2622.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "want any users being failed when they",
      "offset": 2625.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "are real users or real humans on a real",
      "offset": 2626.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "browser. I want good DX specifically. I",
      "offset": 2628.48,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "want an npm package that knows what",
      "offset": 2632.079,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "React is. And don't get me started on",
      "offset": 2635.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the random open source packages for",
      "offset": 2638.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "something like recapture because they're",
      "offset": 2640.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "all entirely unmaintained in garbage. I",
      "offset": 2642.56,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "want type safety different error states",
      "offset": 2645.2,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "handled. And I guess this is kind of DX,",
      "offset": 2649.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but it's also just a general desire.",
      "offset": 2651.599,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Promotion to visible capture on fail.",
      "offset": 2653.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "This is a big deal. H capture is a thing",
      "offset": 2657.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "for this. They call it 99.9% passive,",
      "offset": 2660.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "which means for most users, most of the",
      "offset": 2662.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "time they'll never see a capture. But if",
      "offset": 2663.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "they fail to validate them before it",
      "offset": 2665.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "passes the token to you, then they will",
      "offset": 2667.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "pop up a capture challenge for the user",
      "offset": 2669.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "for them to pass and then give you the",
      "offset": 2671.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "new token that's generated from that",
      "offset": 2674.16,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "instead. If I want to do that with",
      "offset": 2675.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "recapture, I have to myself run the",
      "offset": 2677.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "invisible capture, validate that",
      "offset": 2679.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "separately. And if the validation fails,",
      "offset": 2681.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "send that state back down to the client",
      "offset": 2683.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "to tell the user's device, hey, you",
      "offset": 2685.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "failed our capture check. Can you try it",
      "offset": 2687.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "again, but with a visible capture? So I",
      "offset": 2689.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "have to manage all of those partial",
      "offset": 2692,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "states, all of the piping back and",
      "offset": 2694.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "forth, and also keep myself from",
      "offset": 2696.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "accidentally building something that is",
      "offset": 2698.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "really useful for bots to test against.",
      "offset": 2700.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And that's the worst part is it's very",
      "offset": 2703.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "easy if you implement your own like",
      "offset": 2705.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "progressive capture layer where it",
      "offset": 2708.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "promotes or I guess demotes you from",
      "offset": 2709.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "invisible to visible. You might have",
      "offset": 2711.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "just accidentally created a system where",
      "offset": 2713.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "people can test their capture workaround",
      "offset": 2715.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "bots. Not fun in the slightest. Not fun",
      "offset": 2716.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "at all. I wouldn't wish this on my",
      "offset": 2720.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "worst enemy. I've been through it with",
      "offset": 2721.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "captures. I'm done. So, let's see how",
      "offset": 2723.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "vers cells stacks up. Introducing bot",
      "offset": 2725.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "ID. Invisible bot filtering for critical",
      "offset": 2728.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "routes. Modern sophisticated bots don't",
      "offset": 2731.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "look like bots. They execute JS, solve",
      "offset": 2734,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "captions, and navigate interfaces like",
      "offset": 2736.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "real users. Tools like Playright and",
      "offset": 2738.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Puppeteer can script humanlike behavior",
      "offset": 2740.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "from page load to form submission.",
      "offset": 2742.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Traditional defenses like checking",
      "offset": 2744.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "headers or rate limits aren't enough.",
      "offset": 2745.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Bots that blend in by design are hard to",
      "offset": 2748,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "detect and expensive to ignore. Just to",
      "offset": 2750,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "emphasize the point here, H capture",
      "offset": 2752.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "pricing a dollar per thousand",
      "offset": 2754.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "verifications. Write data's H capture",
      "offset": 2756.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "solver at the highest tier $15 per",
      "offset": 2759.119,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "thousand solves. You pay 5 cents more",
      "offset": 2762.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "per thousand to work around an H capture",
      "offset": 2765.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "than I, as the developer pay to serve",
      "offset": 2768.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you one, which is really funny when you",
      "offset": 2770.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "think about it. So yeah, take that as",
      "offset": 2772.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you will. Having something that's more",
      "offset": 2776.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "resilient is cool. That said, depends on",
      "offset": 2778.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "how expensive each request is. If a",
      "offset": 2781.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "request by a bot sneaks through, how",
      "offset": 2783.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "much value are they getting and how much",
      "offset": 2785.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "money does it cost you? If they're",
      "offset": 2787.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "getting less than $15 per thousand",
      "offset": 2789.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "requests of value working around your",
      "offset": 2791.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "captas, then it doesn't matter. But if",
      "offset": 2793.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "they are, you might need something more",
      "offset": 2796.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "strict. This is where bot ID comes in.",
      "offset": 2797.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "It's a new layer of protection. Think of",
      "offset": 2800.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it as an invisible capture to stop",
      "offset": 2802.319,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "browser automation before it reaches",
      "offset": 2803.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "your back end. It's built to protect",
      "offset": 2805.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "critical routes where automated abuse",
      "offset": 2807.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "has real cost like checkouts, login,",
      "offset": 2809.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "signups, APIs or actions that trigger",
      "offset": 2811.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "expensive backend operations like LLM",
      "offset": 2813.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "powered endpoints.",
      "offset": 2814.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And this is so much nicer. Check bot ID.",
      "offset": 2816.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Isbot equals await checkbot ID. If",
      "offset": 2820.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "isbot, deny. No configure tuning",
      "offset": 2822.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "required. Install a package. Setup",
      "offset": 2825.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "rewrites. Mount the client and verify",
      "offset": 2826.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "request server side. I want to see what",
      "offset": 2828.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the setup rewrites part does. Let's read",
      "offset": 2830.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "through how this works first. Bot ID is",
      "offset": 2832.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "available in two modes. basic and deep",
      "offset": 2834.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "analysis which adds advanced detection",
      "offset": 2836.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "checks. Detection starts at the session",
      "offset": 2838.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "level. Bot ID injects lightweight",
      "offset": 2840.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "offiscated code into the requesters",
      "offset": 2842.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "environment that evolves on every load",
      "offset": 2844.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and is designed to resist replay",
      "offset": 2846.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "tampering and static analysis. It runs",
      "offset": 2847.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "invisibly with no captures or changes to",
      "offset": 2849.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the user experience. This is funny",
      "offset": 2851.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "because the previous examples they had",
      "offset": 2853.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "were like verify with Versell and they",
      "offset": 2855.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "had a little thing that would appear in",
      "offset": 2857.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "the UI and I explained why I hate that",
      "offset": 2858.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and I'm assuming others did because it",
      "offset": 2860.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "doesn't seem like there's any UI at all",
      "offset": 2861.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "with this. That is cool. Unlike",
      "offset": 2863.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "traditional defenses, bot ID doesn't",
      "offset": 2865.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "rely on static signals like user agent",
      "offset": 2867.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "headers or IP ranges, which are easy to",
      "offset": 2869.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "fake and become outdated. It also avoids",
      "offset": 2870.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "more intrusive methods like captures,",
      "offset": 2872.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "heruristics, and reputation scores,",
      "offset": 2874.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "which can frustrate or block real users.",
      "offset": 2876,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Instead, buy decounters the most",
      "offset": 2878.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "advanced bots by silently collecting",
      "offset": 2879.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "thousands of signals, mutating detection",
      "offset": 2881.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "logs on every load to prevent reverse",
      "offset": 2883.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "engineering and making spoofing",
      "offset": 2885.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "difficult, beating attack patterns into",
      "offset": 2886.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "global machine learning networks that",
      "offset": 2888.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "continuously improve protections. Fast,",
      "offset": 2889.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "reliable, built for dev. Server side",
      "offset": 2892,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "verification. Takes a single function",
      "offset": 2893.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "call. yada yada. No API keys to manage.",
      "offset": 2894.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "That's really nice. No score thresholds",
      "offset": 2897.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to tune. That is so nice. One of my",
      "offset": 2899.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "favorite things about H capture. You",
      "offset": 2901.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "don't have to determine what is or isn't",
      "offset": 2902.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "human yourself. There's a little chart",
      "offset": 2904.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "on the recapture dashboard that you can",
      "offset": 2906.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "move left to right for how strict you",
      "offset": 2907.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "want to be because you have to give it a",
      "offset": 2910.559,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "number between zero and one for how",
      "offset": 2911.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "strictly you want to filter out users.",
      "offset": 2913.92,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "They recommend 7 to8. We lost like five",
      "offset": 2916.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "plus percent of our requests if we went",
      "offset": 2920.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "above.3",
      "offset": 2922.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "which meant that we were almost",
      "offset": 2924.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "certainly letting in some bots which",
      "offset": 2925.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "sucks but recapture was not fun. Deep",
      "offset": 2927.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "analysis powered by Casada. This is the",
      "offset": 2930.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "partner that they worked with. Again,",
      "offset": 2932.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Verscell is not hesitant to pull in a",
      "offset": 2933.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "third party where it makes sense even if",
      "offset": 2935.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that third party doesn't know how to do",
      "offset": 2937.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "layouts on small screens. That is funny.",
      "offset": 2939.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Anyways, they are the like core platform",
      "offset": 2942.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for the automated threat detection",
      "offset": 2945.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "stuff.",
      "offset": 2946.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Supposedly, they're really good. From",
      "offset": 2948.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "what I've seen, they are not easy to set",
      "offset": 2950.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "up and you're expected to sign a pretty",
      "offset": 2951.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "crazy deal with them because they don't",
      "offset": 2953.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "even show pricing. Yeah, they really",
      "offset": 2954.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "want you to sign like a deal with them,",
      "offset": 2956.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not just like go and sign up yourself on",
      "offset": 2958.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the site, which is cool that Versell is",
      "offset": 2960.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the one that's going to take the hit",
      "offset": 2962.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "here for us, make that partnership, and",
      "offset": 2964.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "then give it to us at a reasonable rate",
      "offset": 2966,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that's much more transparent. No",
      "offset": 2968.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "separate service to sign up for. Just",
      "offset": 2969.839,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "install the package, define the routes",
      "offset": 2970.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you want it on, and deploy. Enterprise",
      "offset": 2972.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "grade defenses. They've been using it",
      "offset": 2974,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "for things like Vzero for a while.",
      "offset": 2975.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Cool. For teams with targeted automation",
      "offset": 2977.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that slips past contentional defenses",
      "offset": 2979.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "available now for all teams.",
      "offset": 2983.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I want to see how they are handling the",
      "offset": 2986,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "rewrite bit because that was very",
      "offset": 2987.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "interesting to me. Okay, we have to wrap",
      "offset": 2988.8,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "our next config with with bot ID. Oh,",
      "offset": 2991.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this is so that ad blockers won't break",
      "offset": 2994.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "the script loads. This is similar to the",
      "offset": 2996.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "workarounds I have to do for analytics",
      "offset": 2997.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and such. Or you can manually rewrite.",
      "offset": 2999.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Oh god, these these are some URLs. These",
      "offset": 3002,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "are going to start getting blocked on",
      "offset": 3005.68,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "things. People are going to be pissed. I",
      "offset": 3006.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "I know how this back and forth goes with",
      "offset": 3008.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "ad blocks. That'll be fun. Bot ID client",
      "offset": 3010,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "protected routes API sensitive checkout",
      "offset": 3013.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and signup.",
      "offset": 3015.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And then we have their component that",
      "offset": 3017.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "they provide from bot ID/Client. You",
      "offset": 3018.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "pass it the config with the routes that",
      "offset": 3022,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you want to protect. And now it knows to",
      "offset": 3024.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "automatically add on any request to",
      "offset": 3027.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "those routes the right header",
      "offset": 3029.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "information so it can verify the user.",
      "offset": 3031.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Check bot ID on the route configured in",
      "offset": 3033.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the bot ID client component.",
      "offset": 3034.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "If you don't have the component or the",
      "offset": 3037.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "particular route in the component, that",
      "offset": 3039.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "will fail. Makes sense.",
      "offset": 3040.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "The one thing that's annoying here is it",
      "offset": 3043.599,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "doesn't seem to have the granular level",
      "offset": 3045.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "of config for if you like want to skip",
      "offset": 3046.559,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the verification on a paid user, but you",
      "offset": 3048.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "want to do it on a free user. This",
      "offset": 3050.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "doesn't have the right heruristics to",
      "offset": 3052.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "enable that, which is a little annoying.",
      "offset": 3055.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I get why it doesn't, but it's it'd be",
      "offset": 3056.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "nice if it did. And then in the route,",
      "offset": 3058.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you just call check bot ID. will handle",
      "offset": 3060.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "grabbing from the request and doing",
      "offset": 3062.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "everything else it needs to. My",
      "offset": 3064.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "hesitations here are that if you're not",
      "offset": 3066,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "using Verscell or more importantly",
      "offset": 3068.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you're not using next, this needs",
      "offset": 3070.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "request data. So you'd have to pass it",
      "offset": 3072.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "request and it doesn't seem like they",
      "offset": 3073.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "offer that. So this is a very tied to",
      "offset": 3074.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Versell solution",
      "offset": 3077.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "server action. Same deal. You just call",
      "offset": 3079.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "check bot ID",
      "offset": 3080.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "pricing. Here's where we are. Basic all",
      "offset": 3083.92,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "plans free deep analysis for pro and",
      "offset": 3086.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "enterprise cost a dollar per thousand",
      "offset": 3089.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "requests. Okay, interesting. So, the",
      "offset": 3091.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "basic check is always free for everyone.",
      "offset": 3093.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Huh, that makes this way more",
      "offset": 3096.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "compelling, actually. Okay, I think I",
      "offset": 3098.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "see what they're doing here. They show",
      "offset": 3100.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "this off as two modes, but what this",
      "offset": 3103.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "actually is is two products. Basic is",
      "offset": 3106.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "their turn style color because",
      "offset": 3109.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Cloudflare doesn't charge for turn",
      "offset": 3111.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "style, at least not traditionally.",
      "offset": 3112.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "They let you create a certain number of",
      "offset": 3116.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "widgets, which are certain sites and",
      "offset": 3118.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "configurations that a widget is allowed",
      "offset": 3120.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to be used on, and it doesn't charge",
      "offset": 3122.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "based on how much usage it gets. So, for",
      "offset": 3124.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "free and cheap services that need to",
      "offset": 3127.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "make sure that they're not getting",
      "offset": 3129.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "abused, this freess made Turnstyle a",
      "offset": 3130.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "really compelling option for services",
      "offset": 3133.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that had a lot of use and made very",
      "offset": 3135.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "little if any money from their users.",
      "offset": 3136.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "But it also had an insane false positive",
      "offset": 3139.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "rate and especially in invisible mode.",
      "offset": 3141.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "It was basically unusable in invisible",
      "offset": 3144,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "mode. So you were either giving",
      "offset": 3145.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Cloudflare free branding and reach by",
      "offset": 3148.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "showing the Cloudflare logo on the UI or",
      "offset": 3150.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you had a 15% chance that a real user's",
      "offset": 3153.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "request failed. Not worthwhile even for",
      "offset": 3155.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "free. I found this to be rough. And then",
      "offset": 3157.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "if you need more than 20 widgets or 15",
      "offset": 3160.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "host names, you have to be on an",
      "offset": 3162.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "enterprise plan, which is contact sales",
      "offset": 3164.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "pricing. So you know how that goes.",
      "offset": 3166.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "There's a lot. Turnstyle is quite",
      "offset": 3169.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "possibly my least favorite Cloudflare",
      "offset": 3170.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "product. I've played with most of them",
      "offset": 3173.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "for something at this point and",
      "offset": 3174.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Turnstyle is the hardest by far for me",
      "offset": 3175.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to recommend. It supposedly got better",
      "offset": 3177.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "after we tried, but it was so bad when",
      "offset": 3179.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we tried that I cannot in good faith",
      "offset": 3181.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "recommend it unless you really need free",
      "offset": 3183.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and now I still can't because basic mode",
      "offset": 3185.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "on Versel's bot ID for all plans is",
      "offset": 3188.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "free. It integrates way better too. So",
      "offset": 3190.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this is a really compelling like",
      "offset": 3193.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "turnstyle destroyed option. If this",
      "offset": 3195.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "existed when we built the capture system",
      "offset": 3198.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "for T3 chat, we almost certainly would",
      "offset": 3200.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "have used it. Deep analysis is meant to",
      "offset": 3202.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "compete with recapture and H capture and",
      "offset": 3204.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "more advanced solutions like what work",
      "offset": 3206.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "OS is doing with Radar and whatnot. And",
      "offset": 3208.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this price of a dollar per thousand",
      "offset": 3210,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "requests is perfectly matched to H",
      "offset": 3211.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "capture and recapture. They both cost",
      "offset": 3213.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "pretty much exactly the same. This makes",
      "offset": 3215.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "so much sense for something like us. I",
      "offset": 3217.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "could see us setting the basic tier for",
      "offset": 3220.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "most routes and then the deep analysis",
      "offset": 3222.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "tier for chat generation, for example.",
      "offset": 3224.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Yeah, this makes so much sense. Is there",
      "offset": 3227.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a place where you can turn on deep",
      "offset": 3229.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "analysis like at a code level? The bot",
      "offset": 3231.599,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "ID has to be turned on basic or deep",
      "offset": 3235.359,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "analysis at a project level. I hate that",
      "offset": 3237.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "versel. I know somebody's watching or",
      "offset": 3241.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "will watch later. Fix that. Give me the",
      "offset": 3242.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "ability in this config where I'm",
      "offset": 3245.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "actually defining the routes here to",
      "offset": 3248,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "pick if it is deep analysis or normal. I",
      "offset": 3251.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "should be able to choose here and then",
      "offset": 3255.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "of course I'd have to like specify when",
      "offset": 3257.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I call here like on the check bot ID",
      "offset": 3259.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "call I'd have to put like deep colon",
      "offset": 3261.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "true or something give me a big error if",
      "offset": 3263.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I have the wrong config on either side",
      "offset": 3265.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but that's I think a relatively trivial",
      "offset": 3267.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "change that will make this way more",
      "offset": 3269.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "compelling where for most routes I can",
      "offset": 3270.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "use basic and the expensive ones I can",
      "offset": 3272.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "use for that good stuff that's a very",
      "offset": 3275.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "compelling change and another one of",
      "offset": 3277.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "those things like I am so excited for a",
      "offset": 3279.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "year or two from now where the pain I",
      "offset": 3281.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "had to go through with caption does just",
      "offset": 3283.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "feels archaic and wrong. The same way",
      "offset": 3285.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that like configuring a big Webpack",
      "offset": 3288.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "config now just is funny because we've",
      "offset": 3290.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "moved past that problem as an industry,",
      "offset": 3294.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "but it was so real at the time. So, this",
      "offset": 3296.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is one of the most compelling things",
      "offset": 3298.72,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "they've done. I'm excited for a future",
      "offset": 3299.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "where people don't have to think about",
      "offset": 3301.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and set up captions like this anymore.",
      "offset": 3302.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Can't wait for us to get there. And if",
      "offset": 3304.48,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "we go through this list, good invisible",
      "offset": 3305.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "mode. They only have invisible mode, low",
      "offset": 3307.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "false positives. Seems like it's really",
      "offset": 3308.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "good according to the numbers I've seen",
      "offset": 3310.559,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "from them and from the partner uh",
      "offset": 3311.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "Casada, right? Yeah, from their partner",
      "offset": 3313.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "seemed really good. Good DX looks the",
      "offset": 3315.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "best DX I've seen so far by quite a bit.",
      "offset": 3317.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "They're not handling the different error",
      "offset": 3320.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "states the ways I would want to. I'm",
      "offset": 3321.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "sure like you have to send the error",
      "offset": 3323.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "back yourself and show it to the user.",
      "offset": 3325.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "It'd be nice if they made it easier to",
      "offset": 3326.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "handle those. I still want promotion to",
      "offset": 3327.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a visible capta because there will",
      "offset": 3330.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "always be some type of user that like",
      "offset": 3332.24,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "really needs to have a visible capture",
      "offset": 3336.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "verification because they're on a weird",
      "offset": 3339.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "browser on a weird OS with a VPN. I",
      "offset": 3340.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "don't want to restrict those users, but",
      "offset": 3343.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "I want them to put the work in. You get",
      "offset": 3344.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the idea. Oh, Resend just removed",
      "offset": 3346.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Cloudflare turn style for their signin",
      "offset": 3349.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and it's cleaner and their login is 2x",
      "offset": 3351.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "faster. I'm almost positive that they",
      "offset": 3353.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just moved to the new thing that Versell",
      "offset": 3355.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "put out. I would be surprised if it",
      "offset": 3357.68,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "wasn't that. Oh yeah, here we are. 6",
      "offset": 3358.72,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "hours ago, Versell announced this new",
      "offset": 3360,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "feature. They implemented it. Same bot",
      "offset": 3361.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "protections, testing, push production.",
      "offset": 3363.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "Not only does it declutter login, cuts",
      "offset": 3365.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "our login times in half. Huge. That's",
      "offset": 3366.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "awesome. Good stuff. It it it makes the",
      "offset": 3369.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "code so much simpler, too. Yeah, if",
      "offset": 3371.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you've implemented recapture 2 or even",
      "offset": 3373.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "recapture 3 before, you know how rough",
      "offset": 3375.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "they can be to set up. This is super",
      "offset": 3378.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "compelling. Arguably the most compelling",
      "offset": 3380.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "thing that they launched for most",
      "offset": 3382.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "developers of most services. Really",
      "offset": 3384.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "cool. And now we are at the last thing",
      "offset": 3386.24,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Versel dropped, the AI gateway. They've",
      "offset": 3389.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "been talking about this for a bit, but",
      "offset": 3393.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it officially shipped today in beta. So,",
      "offset": 3394.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "what is the AI gateway? the single",
      "offset": 3397.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "endpoint to access a wide range of AI",
      "offset": 3399.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "models across providers with better",
      "offset": 3401.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uptime, faster responses, and no lockin.",
      "offset": 3402.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Now, in beta, you can use models from",
      "offset": 3405.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "providers like OpenAI, XAI, Anthropic,",
      "offset": 3407.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Google, and more. You can do usage based",
      "offset": 3409.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "billing from the provider listed prices,",
      "offset": 3411.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "bring your own key, improved",
      "offset": 3413.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "observability, including per model",
      "offset": 3414.559,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "usage, latency, and error metrics,",
      "offset": 3416.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "simplified authentication, fallback, and",
      "offset": 3418.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "provider routing for more reliable",
      "offset": 3420,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "inference. Really big deal, and the",
      "offset": 3421.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "higher throughput and rate limits. The",
      "offset": 3423.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "start of the hut takes anthropic is the",
      "offset": 3425.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "worst way to use anthropic models. What",
      "offset": 3427.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "do I mean when I say this? There are a",
      "offset": 3430.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "couple ways that a model provider can",
      "offset": 3432.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "fail you. The ones that I care about the",
      "offset": 3434.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "most are reliability. How often do",
      "offset": 3437.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "requests succeed versus failing? How",
      "offset": 3440.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "often does the service actually go down",
      "offset": 3442.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and just be unable to resolve entirely?",
      "offset": 3444.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "How often does it error for random",
      "offset": 3446.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "reasons? All those types of things. Like",
      "offset": 3447.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "on a scale from planet scale to neon,",
      "offset": 3449.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "where does it fall? And then the other",
      "offset": 3451.119,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "really big one, rate limits. You have no",
      "offset": 3453.119,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "idea how hard and obnoxious it is to get",
      "offset": 3457.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "anthropic to bump your rate limits. No",
      "offset": 3460.559,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "matter how big a customer you are, no",
      "offset": 3462.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "matter how many connections you have,",
      "offset": 3463.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "none of those things matter. They have",
      "offset": 3465.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "four tiers of customer that are defined",
      "offset": 3467.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "by how much money you spend and you",
      "offset": 3470.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "getting promoted internally to be a",
      "offset": 3471.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "certain tier. Cloud Sonnet 4 on the base",
      "offset": 3473.599,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "tier can do maximum 50 requests per",
      "offset": 3475.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "minute, which sounds like a decent bit",
      "offset": 3479.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "until you realize it's 20k tokens per",
      "offset": 3481.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "minute in and 8,000 tokens per minute",
      "offset": 3483.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "out. One user's request can be up to a",
      "offset": 3486.319,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "100,000 tokens in. So, one user can",
      "offset": 3489.119,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "saturate this for 5 minutes with one",
      "offset": 3493.599,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "request. Actually, entirely unusable for",
      "offset": 3496.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "real workloads. So, you need to bump up",
      "offset": 3499.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "this tier list before you get somewhere",
      "offset": 3501.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "usable. And even then, 80,000 token per",
      "offset": 3503.119,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "minute maximum on tier three. The tiers",
      "offset": 3506.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "are again based on how much money you",
      "offset": 3509.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "spend and when you get like the bump",
      "offset": 3510.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pressed. Tier 4 requires that you're",
      "offset": 3512.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "spending 5,000 a month or more.",
      "offset": 3514.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And then you're like forced under custom",
      "offset": 3518.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "tiers where they might or might not bump",
      "offset": 3520.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "these limits for you. So, we had Claude",
      "offset": 3522.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "37 bumped relatively high because we",
      "offset": 3524.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "were one of the bigger promoters of the",
      "offset": 3527.359,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "model when it dropped. I did my YouTube",
      "offset": 3528.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "video and everything. bugged them and",
      "offset": 3529.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they gave us higher allocation. And then",
      "offset": 3531.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Sonnet 4 dropped and we are at obviously",
      "offset": 3533.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "tier 4, 200k tokens in that is two",
      "offset": 3535.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "requests per minute. Even if we can do",
      "offset": 3538.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "4,000 requests per minute, if our input",
      "offset": 3541.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "token limits 200k and two people hit",
      "offset": 3543.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "100k on two messages, that's two",
      "offset": 3545.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "messages a minute. That's horrible.",
      "offset": 3547.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "That's terrible. That's entirely",
      "offset": 3550,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "unusable for us. We regularly blast",
      "offset": 3551.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "through the tier four limits. And once",
      "offset": 3553.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's custom, you need to get on an",
      "offset": 3555.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "enterprise plan and do the sales dance",
      "offset": 3557.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "back and forth and everything. It's hell",
      "offset": 3559.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "miserable. I wouldn't wish it on",
      "offset": 3561.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "anybody. So, what do we do if the rate",
      "offset": 3562.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "limits on Anthropics API are this",
      "offset": 3564.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "terrible in negotiating with them is",
      "offset": 3566.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "horrible. And the other third point,",
      "offset": 3568.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "this one really annoys me. Uh cost",
      "offset": 3570.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "negotiations. When you're doing a",
      "offset": 3573.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "certain level of throughput on anything,",
      "offset": 3575.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you can usually negotiate down the rate",
      "offset": 3577.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and get a discount. If we committed with",
      "offset": 3580,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Anthropic to do 20K a month of committed",
      "offset": 3582.4,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "spend. So we literally spend 20K a month",
      "offset": 3586.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for just credits. If we go over, we can",
      "offset": 3589.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "spend more. But if we don't hit that, we",
      "offset": 3591.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "have committed to spending that much",
      "offset": 3593.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "regardless. They'll give us a discount.",
      "offset": 3594.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "The discount we would get if we agree to",
      "offset": 3596.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "20K a month. That is $240,000 a year.",
      "offset": 3598.72,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "5%. We get a 5% discount for agreeing to",
      "offset": 3602.96,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "an absurd level of spend. So all of",
      "offset": 3606.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "these things with anthropic suck. The",
      "offset": 3609.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "reliability is garbage, especially when",
      "offset": 3610.799,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "new models drop, especially when cursor",
      "offset": 3612.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "ships new features. Rate limits make it",
      "offset": 3614.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "literally unusable. And the cost",
      "offset": 3616.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "negotiations are not a real benefit. The",
      "offset": 3618.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "way that they would prevent the things",
      "offset": 3621.599,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "I'm about to talk about is if they made",
      "offset": 3622.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the negotiations and the rates you can",
      "offset": 3624.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "get there way friendlier and easier to",
      "offset": 3626,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "do, but they won't do that because",
      "offset": 3628.319,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "they're kind of insane. And here is",
      "offset": 3629.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "where the problems start. Reliability is",
      "offset": 3631.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "rough. Okay, I can't find my screenshot,",
      "offset": 3634.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "but I had screenshots of how bad the",
      "offset": 3636.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "fail rate was on Drop. When Cloud 4",
      "offset": 3638,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "dropped, Opus had a 15% success rate",
      "offset": 3640.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "from the official APIs. Sonet was",
      "offset": 3642.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "heavily rate limited, and Bedrock and",
      "offset": 3644.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Vertex were not supporting it yet. Wait,",
      "offset": 3647.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Bedrock and Vert.ex, isn't this a cloud",
      "offset": 3651.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "model? Yeah. One of the really fun",
      "offset": 3652.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "things that Anthropic did is that they",
      "offset": 3655.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "made a deal with both Google and AWS to",
      "offset": 3657.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "allow them to host the models at the",
      "offset": 3660.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "same rate that Anthropic does. So, if",
      "offset": 3662,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you really want to keep your models",
      "offset": 3664.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "inside of AWS or GCP because that's",
      "offset": 3665.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "where all your other spend is, you can",
      "offset": 3667.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "go ahead and use Anthropics models",
      "offset": 3670.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "there. You just have to pay Google",
      "offset": 3672.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "instead. And I'm sure Google gets some",
      "offset": 3674.799,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "level of discount, but they are forced",
      "offset": 3676.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "to charge the same rate so that they're",
      "offset": 3677.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "not undercutting Anthropic as part of",
      "offset": 3679.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "their deal. Makes a ton of sense. I get",
      "offset": 3682.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "why they do that. But where things get",
      "offset": 3684.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "interesting is when we look at",
      "offset": 3685.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "reliability and rate limits. Do you know",
      "offset": 3687.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "what's really reliable? Do you know what",
      "offset": 3689.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "doesn't really care about rate limits?",
      "offset": 3691.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "AWS. Do you know what's kind of reliable",
      "offset": 3693.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and is not really aware of what rate",
      "offset": 3696.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "limits are except for on AI studio which",
      "offset": 3697.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "is a whole separate rant for another",
      "offset": 3699.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "time? Google Cloud. So what if we could",
      "offset": 3700.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "use the best of all of these? What if we",
      "offset": 3703.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "could use anthropic models on anthropic",
      "offset": 3705.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "service when it's up? When it's down, we",
      "offset": 3707.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "could just take the same request and do",
      "offset": 3709.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "it on Google's infrastructure or",
      "offset": 3711.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Amazon's infrastructure. This is what",
      "offset": 3712.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "open router does. If we look for Cloud",
      "offset": 3714.799,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Sonnet here, Cloud 4, you can see they",
      "offset": 3717.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "have a couple different providers. They",
      "offset": 3720.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "actually, believe it or not, default to",
      "offset": 3722.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Google Vertex US because it's faster",
      "offset": 3724.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "than Anthropics APIs. It's almost twice",
      "offset": 3726.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "as fast. Now, I'm almost certainly going",
      "offset": 3729.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to move us to Open Router because we'll",
      "offset": 3730.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "get a 2x speed improvement at the same",
      "offset": 3732.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "cost and better reliability because when",
      "offset": 3734.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "other things go down, it will just route",
      "offset": 3736.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to the right place. It's a really good",
      "offset": 3738.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "platform since Open Routers already",
      "offset": 3741.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "negotiated with all of these providers",
      "offset": 3743.04,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "to get through the rate limits to the",
      "offset": 3744.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "best of their ability since they are",
      "offset": 3746.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "spreading the traffic across all of",
      "offset": 3748.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "these different providers and they let",
      "offset": 3749.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "you bring your own API key for the ones",
      "offset": 3751.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "you want to. All of that together ends",
      "offset": 3752.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "up making open router a really",
      "offset": 3754.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "compelling platform for running",
      "offset": 3756.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "anthropic models. I can't recommend that",
      "offset": 3758.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you use the anthropic API when you could",
      "offset": 3760.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "use open router instead because you'll",
      "offset": 3762.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "for the same price get faster responses,",
      "offset": 3764.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "way better reliability and not have to",
      "offset": 3767.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "worry about rate limits. That's a",
      "offset": 3769.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "phenomenal deal. And if you bring your",
      "offset": 3770.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "own key for anthropic models and then",
      "offset": 3772.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the enthropic API down, you can have it",
      "offset": 3774.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "configured to fall back to Google and",
      "offset": 3776.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "use credit through open router instead",
      "offset": 3778.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of your own anthropic credits on your",
      "offset": 3779.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "own anthropic account. It's really nice.",
      "offset": 3781.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Platforms like Open Router make all the",
      "offset": 3783.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "sense in the world and I would expect",
      "offset": 3785.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "long-term for these guys to win because",
      "offset": 3787.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Anthropic and Amazon and Google are too",
      "offset": 3789.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "busy competing with each other to do",
      "offset": 3792,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this themselves. Do you know who doesn't",
      "offset": 3794.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "care what cloud is behind their compute",
      "offset": 3796.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "though? Versel. They don't give a",
      "offset": 3798.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "where this comput is running because",
      "offset": 3801.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "they're not competing in the GPU world.",
      "offset": 3802.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Verscell is not spinning up server farms",
      "offset": 3804.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with a bunch of GPUs in them. They don't",
      "offset": 3806.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "care. So versel and of course in this",
      "offset": 3808.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "case open router they don't have to",
      "offset": 3811.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "worry about which place the traffic is",
      "offset": 3812.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "routed to. They don't care as long as",
      "offset": 3815.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you get a response more reliably than",
      "offset": 3817.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you would otherwise with lower latency",
      "offset": 3819.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "and higher throughput as well which is",
      "offset": 3821.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "really cool. Makes so much sense. So",
      "offset": 3822.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Verscell seems to have woken up to this",
      "offset": 3825.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "reality and realized oh we could be that",
      "offset": 3827.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in between because Versell doesn't care",
      "offset": 3829.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "about where the models being hosted.",
      "offset": 3831.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "They're not a model host. It doesn't",
      "offset": 3833.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "matter to them. they are effectively",
      "offset": 3834.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "commoditizing the places where the GPUs",
      "offset": 3836.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "run. The same way with AWS, you don't",
      "offset": 3839.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "care which server warehouse your stuff",
      "offset": 3841.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is running on. You don't care if your",
      "offset": 3843.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "CPU is Intel or AMD or whatever. The",
      "offset": 3844.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "abstraction is a layer higher. This",
      "offset": 3847.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "abstraction is a layer higher, too. We",
      "offset": 3849.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "also have somebody from Open Router",
      "offset": 3851.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "hanging out here. While they do have",
      "offset": 3852.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "some rate limits, but for 99.99% of",
      "offset": 3855.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "their users, it's essentially zero. And",
      "offset": 3857.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that's the rate limit from Anthropic.",
      "offset": 3859.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "But they can also route your traffic",
      "offset": 3861.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "other places. So it barely matters. They",
      "offset": 3863.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "don't have per user rate limits.",
      "offset": 3866.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Theoretically, if enough people are",
      "offset": 3867.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "doing a ton of traffic on Open Router at",
      "offset": 3870.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the same time, they could hit rate",
      "offset": 3872.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "limits with their providers, but it",
      "offset": 3874.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "effectively never happens and you're",
      "offset": 3876.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "hooking into someone else's rate limit",
      "offset": 3877.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "stuff. And even then, if you bring your",
      "offset": 3879.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "own API key, you get around that, too.",
      "offset": 3881.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So, let's see what Versell's up to here.",
      "offset": 3884.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Oh, yeah. pretty much exact same thing",
      "offset": 3886.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "as what Open Router is building.",
      "offset": 3889.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Generally speaking, if there's a company",
      "offset": 3891.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that's all in on a thing that is complex",
      "offset": 3893.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "enough to have a lot of moving parts and",
      "offset": 3895.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a company that's spinning up the",
      "offset": 3896.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "equivalent on the side, I tend to think",
      "offset": 3898.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the team that's focused on it fully is",
      "offset": 3900.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "going to win. You can look at something",
      "offset": 3902.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "like Versel Blob versus Upload Thing.",
      "offset": 3903.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Even though Verscell built Nex.js and is",
      "offset": 3906.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like the lead of good TypeScript",
      "offset": 3908.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "developer experience stuff, Upload Thing",
      "offset": 3910.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is a way better experience and has way",
      "offset": 3912.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "happier users than Blob does. It's also",
      "offset": 3914.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "more production ready, scales better. We",
      "offset": 3916.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "built upload thing for a reason. It's",
      "offset": 3917.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "because I wasn't happy with the",
      "offset": 3919.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "solutions. Even though Verscell put out",
      "offset": 3920.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "blob around the same time, ours was",
      "offset": 3922.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "better. We won. I think this is going to",
      "offset": 3924.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "happen here, too, where open router has",
      "offset": 3926,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "a team of incredibly talented devs",
      "offset": 3927.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "working super hard to make the best like",
      "offset": 3929.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "distributed experience for making",
      "offset": 3931.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "inference happen. Verscell has it as a",
      "offset": 3933.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "checkbox you can go hit in their UI and",
      "offset": 3935.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "use it there. I don't see a world where",
      "offset": 3937.039,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "their AI gateway beats out open router,",
      "offset": 3939.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but it could happen. I'm all for it. and",
      "offset": 3941.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "having yet another way to circumvent the",
      "offset": 3944,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "borderline extortion that is the",
      "offset": 3946.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "anthropic developer experience trying to",
      "offset": 3948.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "provision those models. I'm okay with",
      "offset": 3950.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you putting your money anywhere that",
      "offset": 3952.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "isn't directly anthropic at this point",
      "offset": 3953.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "because their platform is so rough to",
      "offset": 3955.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "work with. So if you are using cloud",
      "offset": 3958.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "models please use open router or AI",
      "offset": 3960.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "gateway or even just deploy on bedrock",
      "offset": 3963.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "or GCP directly. I don't care what you",
      "offset": 3965.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "go with. Just know that the anthropic",
      "offset": 3967.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "experience trying to use those models",
      "offset": 3969.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "via the API, it's going to bite you in",
      "offset": 3971.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the ass and it's going to do it a lot.",
      "offset": 3973.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Yeah, I did actually miss a couple",
      "offset": 3975.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "things, believe it or not, even after",
      "offset": 3977.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "all of that. So, let's just really",
      "offset": 3979.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "quickly blast through those and wrap up.",
      "offset": 3981.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Rolling releases. This is awesome.",
      "offset": 3983.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Previously, if you wanted to spin up a",
      "offset": 3986.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "new version of your app to a percentage",
      "offset": 3988.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "of your users, you really couldn't. It",
      "offset": 3990,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "was kind of an all or nothing thing. I",
      "offset": 3992.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "ran into this a lot during the T3 chat",
      "offset": 3994.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "beta and we eventually just told users",
      "offset": 3996.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to go use the beta version to move",
      "offset": 3998.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "traffic over by hand. It would have been",
      "offset": 4000.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "really nice if I could have routed a",
      "offset": 4002.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "percentage of our traffic to a different",
      "offset": 4003.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "front end and back end. Not really",
      "offset": 4005.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "realistic at the time. This is super",
      "offset": 4007.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "useful for those of us who want to do",
      "offset": 4009.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "big overhauls. Again, it feels like",
      "offset": 4011.2,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "they're building a lot of these based on",
      "offset": 4012.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the pain points I've experienced. Also,",
      "offset": 4013.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that's a familiar profile picture.",
      "offset": 4015.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Dimmitri is the guy who did the",
      "offset": 4017.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "TypeScript Doom thing. Lunatic. Cool to",
      "offset": 4019.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "see him popping up in stuff like this.",
      "offset": 4022.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Next, we have everyone's favorite topic,",
      "offset": 4024.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "microrends.",
      "offset": 4026.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Microrends are proof that Versell is now",
      "offset": 4027.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a big company because they almost",
      "offset": 4030.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "certainly need this for themselves. The",
      "offset": 4032.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "concept of microrends is instead of your",
      "offset": 4034.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "components being files in a codebase,",
      "offset": 4035.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think of them like packages. So, a given",
      "offset": 4037.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "component that is owned by a specific",
      "offset": 4039.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "team can be updated and you can update",
      "offset": 4041.039,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "specific parts of the product without",
      "offset": 4043.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "having to update the entire JS for",
      "offset": 4045.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "everything. means you can distribute",
      "offset": 4047.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "everything from the framework choices to",
      "offset": 4049.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the build tools, pipelines, and all of",
      "offset": 4051.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that so that your UI is broken up into",
      "offset": 4052.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the logical parts based on your teams.",
      "offset": 4054.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "If you've ever heard the phrase you ship",
      "offset": 4056.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "your org chart, microrends makes it",
      "offset": 4058.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "possible to ship your org chart. And",
      "offset": 4060.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "since most businesses eventually need to",
      "offset": 4062.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "go in that direction, it makes a ton of",
      "offset": 4064.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "sense. We absolutely should have found a",
      "offset": 4065.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "way to do microrends at Twitch. Instead,",
      "offset": 4066.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we ended up with a gigantic repo with a",
      "offset": 4068.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "single package JSON that sucks to work",
      "offset": 4070.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "in. This is the solution to maintaining",
      "offset": 4071.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "good DX at scale, but someone has to pay",
      "offset": 4073.92,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "the piper and deal with those DX",
      "offset": 4076.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "experiences at that scale. Microrends",
      "offset": 4080.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "means that a couple people have to",
      "offset": 4082.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "maintain it and have a experience",
      "offset": 4084,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "so that everybody else can do what they",
      "offset": 4085.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "want to do. And if you do it right, it's",
      "offset": 4087.039,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "awesome. Versel seems to now be",
      "offset": 4088.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "embracing it and allow you to split",
      "offset": 4089.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "large apps into smaller independently",
      "offset": 4091.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "deployable units that each team can",
      "offset": 4092.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "build, test, and deploy on. Awesome. You",
      "offset": 4094.079,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "care a lot about this stuff, just you",
      "offset": 4096.239,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "should probably check out Sephr Cloud.",
      "offset": 4097.759,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "These guys are the gods of microrends. I",
      "offset": 4099.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "helped them with the copy on the",
      "offset": 4101.92,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "homepage. I love these guys. They were",
      "offset": 4102.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "an early sponsor. Still chat with them",
      "offset": 4104.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "all the time. Good crew. They get it.",
      "offset": 4105.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Chat with them if you really want to",
      "offset": 4107.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "make a scalable front end. And finally,",
      "offset": 4109.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Versell agents. I actually have no idea",
      "offset": 4111.199,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "what this one is. Versel agent now",
      "offset": 4113.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "unlimited beta. An AI assistant built",
      "offset": 4115.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "into the Verscell dashboard that",
      "offset": 4118,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "analyzes your app performance and",
      "offset": 4119.12,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "security data. Focuses on observability,",
      "offset": 4120.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "summarizing anomalies, identifying",
      "offset": 4122.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "likely causes, and recommending specific",
      "offset": 4123.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "action. Okay, so this looks at all of",
      "offset": 4125.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the data for your Verscell deployments",
      "offset": 4127.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and makes suggestions on things going on",
      "offset": 4129.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "from incidents to weird performance",
      "offset": 4132.08,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "issues and makes suggestions. Really",
      "offset": 4133.759,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "cool. Nice to see. Excited to see where",
      "offset": 4135.839,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this ends up. It's in limited beta, so",
      "offset": 4137.6,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "it's going to be a bit, but it seems",
      "offset": 4139.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "really promising.",
      "offset": 4140.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "That's really funny. I left the open",
      "offset": 4142.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "router dashboard open and now it has an",
      "offset": 4145.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "application side error. Turns out",
      "offset": 4147.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "writing code is hard. If only they had",
      "offset": 4149.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the Verscell agent to tell them what",
      "offset": 4152.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "went wrong here.",
      "offset": 4154.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "It was a very good day to be a Versell",
      "offset": 4157.44,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "customer and I wish these things existed",
      "offset": 4159.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "before. I'm very happy they do now.",
      "offset": 4161.759,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "They're going to save me a bunch of time",
      "offset": 4163.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and money and a lot of frustrations",
      "offset": 4164.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "going forward and they might even save",
      "offset": 4167.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "me some users if their capture solution",
      "offset": 4169.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "has fewer frustrating moments for those",
      "offset": 4170.799,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "users. I am very excited about this. Let",
      "offset": 4172.799,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "me know what you guys think.",
      "offset": 4175.839,
      "duration": 2.801
    }
  ],
  "cleanText": "It happened. Vercel actually shipped. I feel like it's been a while since Vercel dropped a bunch of things at once, like many years by now. The last two Next.js comps were relatively low in terms of new features both for Next and for Vercel. That changed today. They dropped a ton of things I've been personally waiting for forever. From a new pricing model to Queues to a way to run code that's sandboxed from your users to even handling captions properly. It's kind of nuts how much stuff they drop that I've needed for a while. It almost feels like they've been looking through my Twitter and hitting all the check boxes for all the things that I've been bothered by recently. That all said, Vercel has not paid me for a long time. In fact, quite the opposite. I spend a lot of money paying them nowadays, especially with the success of T3 chat. So, someone's got to cover today's bill. And if I'm going to be honest, it can't be them. So, quick word from today's sponsor, and then we'll dive right into what I think about everything Vercel just shipped. One of the biggest changes I'm seeing from AI is the willingness of big enterprise companies to adopt tools by small teams and companies themselves. It's kind of crazy to see that small businesses like my own with T3 Chat are getting interest from much, much bigger enterprises. There's one big thing that's hard to get right though. O. And no, AI is not going to solve this problem for you. Setting up WorkOS in a way that these big businesses are willing to use and adopt and integrate in their systems is something that I wouldn't wish on my worst enemy. And that's why I'm so pumped about today's sponsor, WorkOS. These guys made it way easier to get your application and most importantly your authentication ready for enterprise adoption. You can take my word for it or you can look at the absurd number of companies that you're already using software from that have made the move themselves from Cursor to OpenAI to Vercel to Carta to Vanta to so many more. I always smile a bit when I open up the Cursor dashboard and see the WorkOS like offkit signin. It's cool to know that they're using the same tools we use every single day. I've personally chatted with GMO about offplatforms like WorkOS and he told me really early on that he regretted not adopting one earlier. I think we could have done even more business if we had partnered with WorkOS earlier. It's been incredibly wellreceived. I couldn't agree more. They found an incredible balance of making something enterprise ready for these IT teams to integrate and something that's actually nice to use for us as full stack TypeScript developers. It's awesome that they found this balance and we're seeing more and more people adopt it for that reason. If you're tired of thinking about O and are ready to just ship, check out WorkOS today at soyv.link/workos.\n\nLet's dive in. Going to do a real quick overview of the things that they changed so that we can keep ourselves on track. The one that I'm personally most excited about is the active CPU billing. It's going to be a real fun one to talk about and specifically why it matters. They added the sandboxing for code runs specifically for user submitted code or more importantly AI generated code Queues finally and then their capture killer solution that is actually looking very compelling. It seems like they they learned all the right lessons from the things I've been complaining about with captas. If you haven't watched my capture video already, obviously I'm biased cuz I made it, but I think it's one of the best videos I ever did. So check it out if you haven't. Very practical, applicable information on how to set up captions and rate limiting in your services properly. They also officially dropped the AI gateway which is their alternative to something like open router. Actually looks pretty compelling. We'll dive into that too. So let's start with the active CPU billing cuz I am so excited about this. Again, I'm biased because this is going to save me a ton of money. To understand this, we need to understand how servers are built. If I have one server and I don't know, let's say that this server costs us $10 a month. This server is just one server. If it gets no users or it gets a,000 users or it gets a million users, we have our fixed rate. It's 10 bucks a month. But what happens if this server can only have, I don't know, 500 users concurrently at its peak. Then you need to spin up more servers. Maybe you need to keep them live for the whole month or maybe you're dynamically spinning them up and down. You end up in the the wonderful Kubernetes hell that we've all seen and I've often made fun of as you need to spin up more servers and remember to spin them down and wait for them to spin up and possibly lose traffic in that time. Not fun. And that's why the world started moving towards a serverless model, especially for the short-term compute needs that most web apps have because most users are making a bunch of requests or sometimes just one request doing a lot of DB and then not doing anything for a while. And the idea of serverless isn't you don't have servers. Obviously your code runs on a server. The point is that the actual like server setup that you have like the amount of provisioned servers you have running is directly reflective of the number of users currently making requests. So previously the way the world worked is you had your one server and all of your requests went to this. So you had a bunch of different requests from all sorts of different users, hitting the same server, taking however long they need to take, and then sending the responses on the other end out to the users on the other side. But if you ever needed more requests than this could fit, that was when you ran into problems. You'd have to either manually spin up a server and hold these requests until it was ready or do something else. Serverless had the goal of flipping this dynamic entirely. So instead of being 10 bucks a month for a server, it's build on gigabyte hours is the usual phrasing for it. So gigabyte hour is how many gigabytes of RAM over how many hours is the thing that you'd be build on. So now each of these requests instead of having its own server, they each have their own separate instance that is alive for as long as the request needs it to be and then dies immediately after. And if each of these requests takes a very small amount of time to resolve, like 100 to 200 milliseconds just to get the DB requests done, generate the HTML and send it to the user, this makes a ton of sense. So if you think of this, like each of these takes 100 ms, we shrink it like that. Then this model makes a ton of sense. And since more and more compute has been working this way, I say as somebody who built a lot of things like this, this model actually was really really good for web applications and like crowd apps and e-commerce and all those types of things because you never have to worry about having enough servers for your users and the users are making such short requests that if you add all of these millisecond times up, it's still not going to cost you a whole bunch of money. So it ends up being often a good bit cheaper than if you provisioned a big enough server because you don't have to keep the server provisioned during downtime and you're just paying for the time spent requesting. So what's the problem? Well, let's say like these four requests are people requesting their user data like their email address that they use to sign up or their chat history on T3 chat. Those all happen really quickly. That's all good. But what happens when you start generating a response from OpenAI? This stops being 100 milliseconds and starts being 20 seconds. Now imagine you have a bunch of users doing this constantly. Now you're not adding up a 100 milliseconds times 4. You're adding up 20 seconds. And suddenly having an individual server running for each of these requests sucks because there's a second dimension we have to be considerate of. We were to put these two axes. Axis one is duration. This is how long does the request take to run. And axis 2 is intensity, like how much CPU is actually being used during this request. So for something like your Nex.js app, it's not using a whole lot of CPU when it's making a DB request, but it uses a decent bit when it's turning that JSON into HTML to send to the user. Or if you're doing something like image encoding where you're re-encoding a PNG into an AVIF or something like that like we're adding for T3 chat for partial image gen. Those things take a good bit of CPU and that means that you're limited with how many things you can do on a server. If you have 10 users re-encoding images on a server that's way more compute than even like a thousand users who are just waiting for a DB response to come in. So making sure that you're paying attention to the actual CPU usage relative to the duration is key. This is where different workloads get interesting though. Something like rendering your Nex.js app is like I would say medium usage low duration. So SSR medium usage low duration. Something like image re-encoding. Image encoding is high usage medium duration. And this is the interesting thing. There's this area in the corner here of like high duration, low usage. That has been a really rare thing. So there hasn't been much innovation in this corner for a while because there hasn't been a need to. The goal historically has been how do we reduce the duration of our requests. Almost all the work that companies like Vercel have put in that all these database companies have put in, they're focused on how do we shift down the duration chain so that things can happen faster so users have a faster and better experience on the web. The problem is there is this new workload that fits really well in this corner that I've been hinting at a bit. LM generation requests. And to be clear, I am not saying the actual running of the model to take in an input and create the new tokens that you send to the user. What I'm referring to is the endpoint that takes the user's request, attaches whatever metadata, whatever memory or whatever else, and then sends that to the OpenAI API with the services API key, and then receives that stream and then streams it down to the user. That ends up being really high duration and extraordinarily low CPU costs. And that ratio is very strange. It is not something that most of these services were built for except for one, Cloudflare. Cloudflare's infra specifically the Cloudflare worker product has really low CPU performance. If you take a given like server rendered task and run it on Vercel or on Lambda with node and then run the same exact code on workers, it will take two to four times longer to actually give you a response if it's the CPU that you're measuring. So if you take some JSON and convert it to HTML with React and Next.js, JS that'll take four times longer on Cloudflare because the CPUs are weaker and their virtualization layer is different versus on Vercel or Lambda or even a traditional server where you have some dedicated CPU allocation and a real like kernel running real Linux running real node on top of it. That ends up being significantly faster and also with something like Lambda you can up the provisioning so it has more compute when it needs it. This is where things get interesting because Cloudflare's infra has historically been bad for SSR workloads, but it's been really good for what I've referred to as the world's most advanced switch statement. If you're effectively using Cloudflare as a way to resolve a request and point it in the right direction and do basic changes to it, Cloudflare has been really good. And one of the things that makes Cloudflare's model so special is their net CPU billing. So let's redraw this here because remember there's two numbers we care about. There is the duration and there is the actual CPU usage. So let's say a user is making a request for their metadata. User requests their metadata. This request will probably be pretty fast. 100 milliseconds if your database is fast enough. This request probably isn't going to need to do too much in terms of CPU. If we were to measure this by the amount of CPU utilized and we'll just measure it in milliseconds as well to make it easy. Let's say it was 10 milliseconds net CPU. So if we shrink this accordingly, it ends up looking like that. Actually quite a bit less if we make this to scale. I'll do that instead. There we go. You get the idea. A request from a user that takes 100 milliseconds of wall clock. If it's spending most of that time waiting for the DB, it's not going to use much CPU. To expand this literally to make it a little clearer, red is when we're being built. Hollow is when we're not because something else is happening. So that little red section in here, we are requesting from DB. That takes a very small amount of time. Then we wait for I don't know 85 milliseconds. Wait for DB response. And at the end here, we have a little more compute to take that DB response, re-encode it in JSON or whatever format you want it to, and then send it to the user. So in this 100 millisecond clock time, we're probably doing at most 15 milliseconds of CPU actually running. If you were inside this box and you looked at how much like CPU utilization it was using, you'd see it at around 100% then less than 5% probably like one to zero depending on how much other stuff's going on in the box. Then back up to 100% for a brief moment then the data goes down to the user. The thing that made fluid compute really cool which was Vercel's way of parallelizing things on a given lambda that I've covered a bunch in the past is that it could effectively stuff other people's requests in when this happened. So here, since this CPU is not doing anything, it will stuff in another user's request. It'll use some CPU, do the thing, and when this one's being waited to resolve, we can have another one and another one. And now what we're being buil for this whole window. So let's say that this ends up being 150 milliseconds of total compute. So now we squeezed 400 millisecond requests into 150 millisecond window. So you're not going to be build for 400 milliseconds. You're going to be build for 150. That's really cool. You know what would be way cooler? Remember how we said that like each of these like totals to 15? Let's even round up a bit. Let's say each of these is 10. So it's 20 milliseconds of compute and it's 150 milliseconds total. So 20 40 60 80. If we take all of these, remember each of them is about 10. You'll see this is quite a bit shorter. than what we ended up paying for. It's like way less than half. Actually,\n\n\nI think in this case, it's a bit over half technically.\nThe point being that happens because there's a lot of dead areas in this.\nWe have here where nothing happened.\nI'll use blue to show those dead areas.\nWe have here to here where nothing happened.\nWe have another dead zone there as well.\nThese dead areas you're still paying for because you have to have the server spun up waiting for these requests to be completed.\nWhen you're looking at something like a 100 millisecond request to go get something from the database, this looks pretty bad.\nIf we're a little more realistic with server rendered loads, like instead of this user requesting their metadata, they're requesting their user page.\nThat might need a little more CPU in the end.\nWe change it to 150 milliseconds.\nAnd at the end, say it needs a lot more compute after it gets the response from the database to actually render the page.\nSomething like this would benefit a lot more from the model that Vercel had with fluid compute because such a high percentage of the time is still spent doing real work that it's fine.\nBut what about the thing we were talking about before, these dreadful 20 to sometimes like 8-minute long requests?\nHow much compute do you guys think those are using?\nSo let's take a 20-second wall clock generation.\nWe have this initial block at the start that is verifying the user and their data.\nSo we verify the user, we send the request off to open AAI or whatever other API, and then we are waiting, and we keep waiting, and we keep waiting, and then eventually near the end, we get the full response and we send that down to the user.\nMaybe we persist in our database, or let's say we are sending back the tokens as we get them, which is very common.\nEach of those takes literally like two cycles of CPU.\nSo we have a ton of these super, super, like impossibly thin lines randomly throughout where a tiny, tiny bit of CPU is used.\nAnd on this 20-second wall clock, not only are we not using seconds of CPU, not only are we not using milliseconds, usually a AILM generation call can be measured in nanoseconds of CPU used.\nSo something even as crazy as a 20-second wall clock time could be as low as 100 nanoseconds of compute.\nThat's the problem.\nThat ratio is insane.\nIf you look at how much compute is used relative to the amount of time the request took, that's where this pricing gets really rough.\nThat is the original pricing because now I have this 20-second runtime I am paying for even though I'm using almost no CPU during most of it.\nAnd my honest guess is that when more and more of these AI workloads started appearing on Vercel, their average CPU like usage and like the percentage that their CPUs were utilized during requests probably went from like 60 to 80% down to less than 5%.\nAnd when I look at our CPU utilization for our box, especially if we turn off fluid, the CPU utilization is so close to zero, it's funny because you have to just sit there kind of when you're waiting for the response.\nIf you do the thing I showed earlier with fluid and you force a lot of things into that same lambda request, it helps, but it doesn't help enough because again, Cloudflare bills on net CPU.\nSo they are only charging you for these red boxes.\nThey are only charging you when the CPU is going, and they could do that because they don't have to spin up a new kernel for every dev.\nIf you are a dev deploying on Vercel and I'm a dev deploying on Vercel, we both have our own code.\nWe both deploy.\nEven with fluid when it's sharing the node to do multiple things at once, it only shares across a given user in a given binary of code.\nSo your code can't run on my lambda when my lambda is running.\nOn Cloudflare, the abstraction isn't at like the kernel or firecracker or lambda level.\nThe abstraction is higher.\nIt's in V8, the actual engine.\nThey built the worker isolate layer which allows them to have basically the event loop being shared.\nSo if you know the JavaScript event loop, like requests are made, functions get queued up and executed and data like flows through it like that.\nThe way Cloudflare works is effectively when you don't have anything on your event loop, when you don't have anything cued ready to go, someone else has a request for their own code running on the same instance.\nThat's what was so powerful about Cloudflare stuff is their event system means your code can be running and doing nothing and cost them nothing because that same V8 instance can be resolving other people's requests with their own separate code at the same time.\nThere's like four options here.\nYou have traditional servers, you have serverless where it's one instance per request, so one instance per user effectively.\nYou have fluid compute, the serverless servers things that Vercel did where you have one node that can resolve multiple users requests but only has one binary like one piece of code running on it.\nAnd then you have the Cloudflare solution which is lots of different developers codes running on the same box at the same time and it's abstracting on like the per function call level.\nAnd when I tell you the price is massively different, I mean it.\nAs we see here, Cloudflare bills on the CPU time, the net CPU cost, the amount of time the CPU is actually invocating.\nAnd they charge 2 cents per million CPU milliseconds.\nThey build it in milliseconds because that's like how much these requests take.\nIt's measured in nanoseconds and milliseconds, not seconds and minutes like it is on other platforms because they're charging based on that.\nThere's a max of 5 minutes of CPU time per invocation, but that's CPU time.\nThe actual request doesn't have a duration limit.\nSo, it could take 2 days to resolve and it's fine as long as it's not doing anything with the CPU in that time.\nReally cool.\nAnd Vercel finally decided to match that.\nAnd this is exciting to me because I've actually argued with them a bunch about this because I compared our numbers on Fluid to our numbers on serverless to my friends numbers on Cloudflare with workers and there was a pretty rough gap there in terms of the cost.\nLike I know people who were doing more inference than us that had bills that were like a hundth of hours for compute because it doesn't matter how many thousands of requests you can stuff into a 30-cond window if the alternative is being charged for one millisecond per request instead.\nI can't go into details of these numbers cuz the ones I know are very much under NDAs, but I can tell you it is as much as a 100x difference.\nAnd it's pretty easy to see that difference if you just think through the diagram here.\nIf I have requests that take 80 seconds and one of these 80 second requests uses, I I'll even go higher than normal, 5 milliseconds of CPU.\nBest case, we could squeeze like 100 things into a fluid compute instance.\nSo worst case with serverless if we have a 100 requests that match this.\nSo we'll say 100 requests, 80 seconds, 5 milliseconds of CPU.\nIf we have a normal server, we're just paying the server cost.\nThat's hard to measure.\nIf we do serverless, then we're paying that 80 seconds * 100 equals 8,000 seconds.\nThat's rough.\nFluid, let's say we squeeze them all in and it's it all fits.\nSo it's 80 seconds time 1 because we're only invocating once.\nThat's 80 seconds.\nNet CPU billing 5 milliseconds times 100 equals 500 milliseconds which is equal 2.5 seconds.\nIs this a real number from a real dashboard right now?\nNo.\nBut I can tell you from my experience this lines up.\nThis is accurate to the things I've seen.\nThe gap's insane because the amount of CPU used for these inference tasks is relatively low in comparison.\nLike the 80 to 5 millisecond ratio.\nThis is the thing to pay attention to.\nA request that takes 80 seconds can use as little as 5 milliseconds of CPU.\nThere is nothing you can do to make this number worth paying for.\nYou have to find a way to charge this number instead if you want to compete with Cloudflare.\nAnd this is what Vercel just announced.\nThey are now going to charge for this instead of this.\nAnd that's probably going to cost them a ton of money behind the scenes depending on how they have it provisioned.\nI don't know the details of how they implemented this because the reason Cloudflare can charge on this is during that 80 seconds they can run someone else's code on the same node.\nOn Vercel they can't do that because it is running its own kernel, its own node instance.\nSo if I'm not doing anything, they can't just spin someone else's code up on the same box or on the same instance.\nSo let's read what they've done and how they did it.\nIntroducing active CPU pricing.\nOf course, they had to come up with their own name for it.\nFluid compute exists for a new class of workloads.\nIObound backends like AI inference agents, MCP servers and anything that needs to scale instantly but often remains idle between operations.\nThis is the key that scale instantly part is so useful both for like surges in traffic but also the other side where you're I don't know spinning up a preview deployment when somebody files a PR.\nHaving things that can instantaneously on demand spin up and down like that is so useful for all the different layers of my stack.\nThese workloads don't follow traditional quick request response patterns which again this is what Vercel has optimized for historically.\nHow can we get the data close to the server?\nHow can we format the data faster so there's less time being spent generating the right thing?\nHow can we cache stuff to skip the compute layer entirely?\nWhat can we do to reduce the amount of time it takes to resolve a request?\nThat's been Vercel's goal since day one.\nThat does not work in the AI world.\nThese are longunning unpredictable workloads and they use cloud resources in new ways.\nFluid quickly became the default compute model on Vercel, helping teams cut costs by up to 85% through optimizations like infunction concurrency.\nToday we're taking the efficiency and cost savings further with a new pricing model.\nYou pay CPU rates only when your code is actively using CPU.\nHuge from servers to serverless.\nIn the early days of cloud comput, teams ran longived servers.\nYou had to manage provisioning, handle scaling, yada yada.\nYou get the idea.\nServerless changed that.\nIt abstracted away infrastructure configuration and introduced automatic scaling.\nEach request triggered its own isolated instance just as I said.\nSo here we see it starts with a lot of compute, does nothing for a while, and then ends.\nAnd you're paying for that whole time from serverless to fluid.\nHere's the difference.\nNow you can squeeze them all into one fluid node.\nDidn't they say 85% before?\nNow it's saying up to 90%.\nFrom my experience, it's been 85 to 90.\nSo it's funny.\nThose are the two numbers.\nI've seen both in our dashboards.\nAnd then from fluid to active CPU.\nFluid improved performance and cost, but there was still room to optimize.\nEven with high concurrency, there could still be moments where all invocations are waiting on external resources and no code is actively running.\nDuring these idle periods, functions stay in memory, do no work, yet they're still incurring CPU cost.\nAgain, if we look at this this way, we just want to pay for the parts where the CPU is working.\nThis aligns with actual usage.\nCompute costs scale with real work, not just with time a function is alive.\nSo what's funny here is that something like our conversion of images is effectively free CPU costwise because we are already running the nodes.\nThe CPU is being paid for.\nWe might as well use it.\nThis is going to make that feel much more expensive because we're actually paying for that compute now where previously we effectively weren't.\nBut that also means our bills are going to be way, way cheaper overall because most of our workloads aren't using CPU for the majority of the time.\nSo the active CPU pricing model fluid compute now charges based on three key metrics.\nActive CPUs which reflects the compute time your code actively is executing on a virtual CPU measured in milliseconds calculated as the number of vCPUs allocated multiplied by the time they're actively being used for starts at .128 per hour.\nSo let's run the math here quick.\nYou know how I am.\nI like to know the hard numbers.\nThe versel is this per hour which in milliseconds 2 cents per million CPU milliseconds.\nThis is going to be annoying math because these numbers are not compatible.\nThat's definitely intentional on both parties.\nOkay, so how many milliseconds are in an hour?\nOkay, hour minute time a th00and.\nOkay, so that's 3.6 million milliseconds.\nSo that's 3.6 6 times more.\nIs it going to be exact?\nIt's going to be 0.02 * 3.6.\nNo, it's about it's a bit more.\nOkay.\nSo, Cloudflare is charging 7.2 cents per hour of compute.\nI'm just going to get rid of the other Cloudflare number because I think billing in hours is a little more sensical.\nSo, Cloudflare is still cheaper per hour, but it's not that big a gap.\nIt's a bit under 50% difference.\nThere is also a bill per invocation though, so we will get to that in a second because I believe Vercel has it as well.\nThey also have to charge for provision memory which covers the memory required to keep a function alive while it's running measured in gigabyte hours and build at a much lower rate less than 10% of active CPU thanks to Fluid's ability to reuse memory across multiple concurrentifications.\nStarts at 1 cent per gigabyte hour.\nCloudflare doesn't charge for this.\nSo to be fair, Cloudflare has a much stricter limit on memory.\nYeah, workers are very limited in how like big they can be.\nThe amount of JS it can run is 10 megabytes of actual binary, which sounds like a lot for JavaScript, but it's not, especially for serverside stuff at scale.\nEach worker can use up to 128 megabytes of memory.\nThat's very low relatively speaking.\nYou if you architect around that and you're not using like node APIs and things heavily, it's fine.\nBut that's not much memory.\nThat said, that's 128 megs of memory per request.\nWhere on Vercel, you have the amount of RAM that your node has across all the requests being routed to that node.\nSo, it depends on what your memory characteristics look like.\nBut I am going to skip that section simply because the way memory works in these platforms is so different that it's not really comparable.\nBut we'll they say it's less than 10%.\nSo we'll add another 10% to the Vercel bill accordingly.\nThis is CPU cost memory versel 10% of CPU cost and cloudflare will say free and then we have the request costs because invocations are charged for on these platforms.\nThey say for versel that like in traditional serverless they are still billing for this.\nIt remains part of the overall billing 60 cents per million requests.\nException for request cost.\nVercel is 60 per 1 mil requests\n\n\nCloudflare does charge for this, 30 cents per additional million. So if we go through and compare these numbers, I would guesstimate that it averages out to Vercel being worst case about 2x more expensive. And that sounds terrible just upfront saying like, \"Yeah, switch from Vercel to Cloudflare and cut your costs in half.\" I'm gonna drop a number that I probably shouldn't. The ratio of our Gemini bill to our Vercel bill right now, and this is before all these changes were made. It's 1:40. Our Gemini bill is 40 times higher than our Vercel compute bill. If you add all of our other inference, it's closer to 180th. And this is fluid compute costs. It's a massive gap. So again, like because these workloads tend to be expensive, but not just for the compute, for all these other things, that gap doesn't matter as much as it seems. Like a 2x here might feel really big. It is not because remember when I said it was like a 100x difference on net CPU to not. Let's say it's the same gap here. We'll say it's 50x. What's going to happen for us is again if we look at the Gemini bill, it's going to be a 50x difference. Our bill is now going to be 1/400th of our not Gemini bill of our inference bill. I would expect after these changes are implemented and we can run it that our inference bill for actually paying Gemini open everybody else will be 400 times higher than our compute bill. And if we switched to Cloudflare, it could be as good as 800 times, but we would be giving up node compatibility. We'd be giving up things like image compression on the same server. We'd be giving up a bunch of the convenience around how we deploy, how we run things on Vercel, how we manage domains, and how we do all these other things. It's a decent difference there, especially when you consider how much higher the RAM limitations are, how much faster the compute is, and all these other things. The argument is no longer you use Vercel until it's too expensive, then you move to Cloudflare. Now it's is the benefits of Vercel worth a roughly 2x gap to you. There are benefits and negatives to all of this. They're both very compelling options now. And the Vercel doesn't make sense at a certain scale thing is kind of killed by this. Vercel's platform now is close enough to costs as long as this is implemented as generously as they have implied through their posting. The cost gap here is nowhere near as bad as it used to be. It is comically less bad. I saw some speculation about how Vercel uses middleware and edge stuff. They did actually formally kill edge workers in middleware today. You still have the middleware file. It just runs on Vercel's compute engine instead. They did briefly play around with the worker D model from Cloudflare is like the thing that would run on the edge, but it had its benefits and negatives as most things do and they slowly moved off of it. Yet they're managing to replicate the awesome billing characteristics of that model inside of more traditional compute engines, which is really cool. A different way to think about this is you're paying around a 2x premium for way faster CPU and actual node compat. This isn't even including the developer experience wins. This isn't even including the cool things that Vercel platform does, all the other stuff. You're effectively paying 2x more for your CPU to run faster, for node compatibility, and yes, for DX stuff that I think is valuable and really nice. Previously, this was closer to 100x difference. So, that's a nice change. Well, next is sandboxing for code. This is a really fun one. And funny enough, Cloudflare also announced their solution for this. Well, they announced that the thing that they just dropped is their solution for it. So, sandboxing for code, run untrusted code with Vercel sandbox. Vercel sandbox is a secure cloud resource powered by fluid compute. It's designed to run untrusted code like code generated by AI agents in isolated and ephemeral environments. It's a standalone SDK that can be executed from any environment including non-verel platforms. That's actually really cool. So if you're running on Cloudflare or you're running on I don't know Heroku or traditional AWS and you need to just spin up some sandbox that is safe and isolated and won't screw up other things on your platform to run some generated code or user submitted code. This is like funny. I see chat already dropping eval. Yeah, it's like if eval was safe, which is really cool. Also, don't get me started on Cloudflare's node compatibility. The progress they made is awesome. It's not real node. Do they support certain node APIs in a compatible enough thing? Cool. But if you need node, if you need something like, I don't know, ffmpeg or something that actually uses native code on the platform, Cloudflare can't do that at all. And don't get me started on trying to run Sharp through Wom on Cloudflare. Remember that 10 megabyte limit, ffmpeg Wom on Cloudflare? No, that that is not a thing. People have claimed they have done it. I have never got anybody to share code that actually works for it. It's all made up. You cannot run actual workloads on Cloudflare that do things more complex than executing JavaScript. You cannot do it. Don't pretend you can. I'm tired of people pretending you can. You can't. They even know that. That's why they put out containers. Exactly. They have containers. They are not cheap. They are good. It's awesome that they have containers now to solve this problem. But I am so tired of people pretending, oh, you can just wit and then Cloudflare can do everything. No, it can't. Anyways, untrusted code. They have an SDK for it, the Vercel sandbox. It's really cool that they're integrating this at like an SDK level where you can create a sandbox, hand it a path or a buffer with code. Here we're taking this generated text. We are creating a buffer from it and then running a command in that sandbox. Command node arg script.js. Tell it where to pass the standard out and standard error. That's really cool. This is really cool. Here's the docs for the sandbox. You can see the type of stuff you have control of for it. This code below. You'll set up a sandbox with four virtual CPUs that uses Node22 runtime and will do all of the following. clones the GitHub repo for a next app, installs the depths, runs the next server, listens on port 3000, open it, sandbox domain in a browser, and stream logs to your terminal. Sandbox will stop after the time out of 5 minutes that we set. Cool. Source, this is the source that we wanted to pull from. Type get resource vCPUs 4. You could also probably set memory and stuff like that there. Timeout milliseconds 5 minutes. This is their fancy helper for converting a string to milliseconds. But 5 minutes is 300,000 milliseconds. Cool. Ports 30,00 to open that port. Runtime node 22. We then run commands. We run the npm install command. std error stout out to pass it. Install.exit code is not zero. That means it failed. We log that it failed. Process.exit. Starting to dev server. Sandbox run command. npm rundev. Same deal. Detached to true. Set a timeout. Spawn open. Sandbox.domain 3000. Cool. That's actually really nice. Good stuff. There's a ton of potential in the sandbox platform. I'm excited. As I mentioned earlier, Cloudflare just announced their way to run Docker images and containers on Cloudflare. And it seems like today they rushed out this blog post on how to use Code Sandboxes with that. That looks similar. That's kind of funny that we got at Vercel/andbox and at Cloudflare/Sandbox within 3 hours of each other. Kind of funny. The the race between Cloudflare and Vercel has never been so tight. These guys are are tooth and nail fighting each other constantly now and that's awesome because we are the ones who benefit the most at the end. The reason Vercel has got so much cheaper is Cloudflare. The reason Cloudflare got containers is Vercel. The reason Vercel has sandboxes now is anthropic and the reason Cloudflare does is also anthropic to be fair. Yeah, like we are all moving forward now because these two platforms are competing so actively that we get better stuff. And of course, as I hinted at before, Anthropic also today dropped their similar thing, build and share AI powered apps with claude. They added the ability to build, host, and share interactive AI powered apps directly in the cloud app. Theirs is like a higher level thing where you can share a clawed chat and the artifact in it that is generated can be its own like mini app that you share with other people. Cool. Built on top of similar things. I wouldn't be surprised if they were actually using Cloudflare's solution here or maybe even Vercel's probably cloud flares though knowing them. There are other platforms that already did this. I know Daytona decently well. We're considering using them both for T3 chat and also talking about potential sponsor in the future. Just full disclosure might work with them later. There are a lot of these types of solutions where you can call the Daytona SDK, create an instance for a given language, hand it some source code, in this case code run console log hello world, and then it can exec it, give you the response. This is a space that I expect to see getting more and more interesting over time because the need for these types of tools to make it easy to safely execute code are more and more valuable because AI can't really interact with the real world, but it can absolutely write code that does. So expect more of these things to start happening and also for platforms like Vercel and Cloudflare to start pushing them more heavily. Being able to sandbox the space where your code runs in order for code that you don't trust to be able to run is going to be more important now than ever. And honestly, I'm slightly betting on Cloudflare for this one because they're deep in the zero trust world. It's a huge part of their product and platform. So the idea of running untrusted code is something that I think fits them very well. You can also tell this blog post was rushed out because they didn't even put a space here. They did on the others, but it's very clear this was an announcement they were probably working on already, but they rushed out this post in order to stay on top of Vercel's announcements. That's sandboxing. Next, we have Queues. And Queues I'm actually really excited about. Funny enough, it's one of the things I think Netlify was really a head-on. Huge fans of what Netlify was focused on here because it's important work especially for like long run generation tasks that don't have partial steps like if you're doing an image gen or deep research or something that just takes a long time to go having compute running for that whole time sucks on Cloudflare since you're only build for the net CPU and it can run for an effectively infinite duration it wasn't too big a deal you can just spin up a thing and let it sit and wait for a while but we're actually having problems right now where certain models like uh 03 Pro can take so long to run. We don't have 03 Pro on T3 chat as a thing any user can just use. You have to bring your own API key because it's so expensive. The problem that we've seen is that for a handful of users, it times out and it's timing out because our functions only run for 800 seconds. That's just the value that we have hard-coded on them. We can bump it higher and I think the new fluid compute changes mean we can bump it even higher. But if the request takes 15 minutes to resolve and we don't have enough duration for that, that sucks. It'd be a lot nicer if we could ceue up an action and then when it is done, run the code to persist it and show the user the result. Cues are really helpful for these workloads that have to wait for a long time and do literally nothing. So you don't even need to deal with the memory allocation in that time. This is a limited beta to be fair. This is one of the earlier products which after some of the experiences I've had with Vercel scares me. Some of these early products are fully flushed out and ready to go. Others aren't at all. I usually try to gut feel it with them and also I look at what other customers are moving over before I make my own decisions here. But here's what they have to say. It's a message Q service built for Vercel applications in limited beta. Let you offload work by sending tasks to a queue where they'll be processed in the background so users don't have to wait for slow operations to finish during a request and your app can handle retries and failures more reliably. It's kind of like uh temporal which is crash proof code. It lets you create workflows around all of the code that you write. So if some part fails, you can rerun it. It's like an infle effect type thing. And there's also ingest I tried working with in the past. Seemingly really good product, not the easiest to interact with, but very promising stuff. Really good DX. Not very types safe initially. They might have finally fixed that, but it wasn't at the time. But these are durable workflow and durable workload platforms. And it seems like Vercel's decided that they're not worth investing anymore. They want to make their own instead, which I I fully understand. Those companies are are weird and now they're building their own thing instead. Oh, Trigger. I forgot about Trigger. They're actually awesome. I need to stop forgetting about trigger.dev. These guys are really really good and they're awesome to interact with, too. So, again, this is a thing a lot of other places had implemented, but Vercel now has on platform. They even have these fancy dashboards where you can see how long each part takes. And obviously, they're leaning into AI, transcribe to video, transcribe audio, all those types of things. generate content runs. It has steps. It creates the text. It then creates an image and returns the result. You can retry it. You can trace it. You can do all sorts of different things with it. Also a good use case for stuff like crowns. You get the idea. Cues and like workflow and workload durability solutions are important. It's cool seeing them building them into Vercel directly. Under the hood, Vercel Q's uses an appendon log to store messages and ensure tasks such as AI video processing, sending emails, or updating external services are persisted and never lost. Key features: PubSub, so it's topic based\n\n\nMessaging allowing for multiple consumer groups.\nStreaming support handles payloads without loading them entirely into memory.\nOoh, that's very interesting actually.\nStreamlined.\nO, which is annoying to set up, right?\nCool that they did that.\nAnd a TypeScript SDK with full type safety.\nI think Trigger has that.\nNone of the rest do.\nThat's a really cool thing.\nSend and receive are from Vercel/Q.\nSend topic message receive topic consumer message.\nAwesome.\nI do not know how they made this type safe.\nI would want to see how this stuff is defined so that we know what M's shape is there.\nBut yeah, it's early early access.\nSo you have to sign up for Vercel community and express interest in order to see more.\nYeah, cool stuff.\nI want to see a lot more before I highly recommend it.\nUntil then, I would personally recommend Trigger, but all the options are pretty good.\nSo that is Q's.\nNext is another one that feels tailor made for me.\nTheir capture killer.\nI'm not going to say my previous capture video is an essential watch, but I think it's a pretty damn good watch.\nIf you haven't, definitely check it out.\nIf you appreciate content like that, hit that sub button.\nLess than half of you have.\nIt cost you nothing.\nMight as well, right?\nAnyways, the Capta Killer product I've been told about for a bit.\nBy the time they were hitting me up to try it, though, I'd already made the move to H Capture and was really happy.\nThat said, none of the major capture solutions, those being Turnstyle, Recapture, and H Capture, none of them even have an npm package, much less like a modern full stack TypeScripty dev experience.\nThey all expect you to embed a random script in your page and manage all of the data and loading states and everything yourself, the partial states for invisible versus visible and all of that.\nThe things that I want out of a capture specifically are the following.\nI want a good invisible mode where most people will never see the capture or even no one is running.\nI want low false positives, meaning I don't want a lot of users getting failed.\nI ideally don't want any users being failed when they are real users or real humans on a real browser.\nI want good DX specifically.\nI want an npm package that knows what React is.\nAnd don't get me started on the random open source packages for something like recapture because they're all entirely unmaintained in garbage.\nI want type safety different error states handled.\nAnd I guess this is kind of DX, but it's also just a general desire.\nPromotion to visible capture on fail.\nThis is a big deal.\nH capture is a thing for this.\nThey call it 99.9% passive, which means for most users, most of the time they'll never see a capture.\nBut if they fail to validate them before it passes the token to you, then they will pop up a capture challenge for the user for them to pass and then give you the new token that's generated from that instead.\nIf I want to do that with recapture, I have to myself run the invisible capture, validate that separately.\nAnd if the validation fails, send that state back down to the client to tell the user's device, hey, you failed our capture check.\nCan you try it again, but with a visible capture?\nSo I have to manage all of those partial states, all of the piping back and forth, and also keep myself from accidentally building something that is really useful for bots to test against.\nAnd that's the worst part is it's very easy if you implement your own like progressive capture layer where it promotes or I guess demotes you from invisible to visible.\nYou might have just accidentally created a system where people can test their capture workaround bots.\nNot fun in the slightest.\nNot fun at all.\nI wouldn't wish this on my worst enemy.\nI've been through it with captures.\nI'm done.\nSo, let's see how Vercel's stacks up.\nIntroducing bot ID.\nInvisible bot filtering for critical routes.\nModern sophisticated bots don't look like bots.\nThey execute JS, solve captions, and navigate interfaces like real users.\nTools like Playright and Puppeteer can script humanlike behavior from page load to form submission.\nTraditional defenses like checking headers or rate limits aren't enough.\nBots that blend in by design are hard to detect and expensive to ignore.\nJust to emphasize the point here, H capture pricing a dollar per thousand verifications.\nWrite data's H capture solver at the highest tier $15 per thousand solves.\nYou pay 5 cents more per thousand to work around an H capture than I, as the developer pay to serve you one, which is really funny when you think about it.\nSo yeah, take that as you will.\nHaving something that's more resilient is cool.\nThat said, depends on how expensive each request is.\nIf a request by a bot sneaks through, how much value are they getting and how much money does it cost you?\nIf they're getting less than $15 per thousand requests of value working around your captas, then it doesn't matter.\nBut if they are, you might need something more strict.\nThis is where bot ID comes in.\nIt's a new layer of protection.\nThink of it as an invisible capture to stop browser automation before it reaches your back end.\nIt's built to protect critical routes where automated abuse has real cost like checkouts, login, signups, APIs or actions that trigger expensive backend operations like LLM powered endpoints.\nAnd this is so much nicer.\nCheck bot ID.\nIsbot equals await checkbot ID.\nIf isbot, deny.\nNo configure tuning required.\nInstall a package.\nSetup rewrites.\nMount the client and verify request server side.\nI want to see what the setup rewrites part does.\nLet's read through how this works first.\nBot ID is available in two modes: basic and deep analysis which adds advanced detection checks.\nDetection starts at the session level.\nBot ID injects lightweight offiscated code into the requesters environment that evolves on every load and is designed to resist replay tampering and static analysis.\nIt runs invisibly with no captures or changes to the user experience.\nThis is funny because the previous examples they had were like verify with Vercel and they had a little thing that would appear in the UI and I explained why I hate that and I'm assuming others did because it doesn't seem like there's any UI at all with this.\nThat is cool.\nUnlike traditional defenses, bot ID doesn't rely on static signals like user agent headers or IP ranges, which are easy to fake and become outdated.\nIt also avoids more intrusive methods like captures, heruristics, and reputation scores, which can frustrate or block real users.\nInstead, buy decounters the most advanced bots by silently collecting thousands of signals, mutating detection logs on every load to prevent reverse engineering and making spoofing difficult, beating attack patterns into global machine learning networks that continuously improve protections.\nFast, reliable, built for dev.\nServer side verification.\nTakes a single function call.\nYada yada.\nNo API keys to manage.\nThat's really nice.\nNo score thresholds to tune.\nThat is so nice.\nOne of my favorite things about H capture.\nYou don't have to determine what is or isn't human yourself.\nThere's a little chart on the recapture dashboard that you can move left to right for how strict you want to be because you have to give it a number between zero and one for how strictly you want to filter out users.\nThey recommend 7 to 8.\nWe lost like five plus percent of our requests if we went above .3 which meant that we were almost certainly letting in some bots which sucks but recapture was not fun.\nDeep analysis powered by Casada.\nThis is the partner that they worked with.\nAgain, Vercel is not hesitant to pull in a third party where it makes sense even if that third party doesn't know how to do layouts on small screens.\nThat is funny.\nAnyways, they are the like core platform for the automated threat detection stuff.\nSupposedly, they're really good.\nFrom what I've seen, they are not easy to set up and you're expected to sign a pretty crazy deal with them because they don't even show pricing.\nYeah, they really want you to sign like a deal with them, not just like go and sign up yourself on the site, which is cool that Vercel is the one that's going to take the hit here for us, make that partnership, and then give it to us at a reasonable rate that's much more transparent.\nNo separate service to sign up for.\nJust install the package, define the routes you want it on, and deploy.\nEnterprise grade defenses.\nThey've been using it for things like Vzero for a while.\nCool.\nFor teams with targeted automation that slips past contentional defenses available now for all teams.\nI want to see how they are handling the rewrite bit because that was very interesting to me.\nOkay, we have to wrap our next config with with bot ID.\nOh, this is so that ad blockers won't break the script loads.\nThis is similar to the workarounds I have to do for analytics and such.\nOr you can manually rewrite.\nOh god, these these are some URLs.\nThese are going to start getting blocked on things.\nPeople are going to be pissed.\nI I know how this back and forth goes with ad blocks.\nThat'll be fun.\nBot ID client protected routes API sensitive checkout and signup.\nAnd then we have their component that they provide from bot ID/Client.\nYou pass it the config with the routes that you want to protect.\nAnd now it knows to automatically add on any request to those routes the right header information so it can verify the user.\nCheck bot ID on the route configured in the bot ID client component.\nIf you don't have the component or the particular route in the component, that will fail.\nMakes sense.\nThe one thing that's annoying here is it doesn't seem to have the granular level of config for if you like want to skip the verification on a paid user, but you want to do it on a free user.\nThis doesn't have the right heruristics to enable that, which is a little annoying.\nI get why it doesn't, but it's it'd be nice if it did.\nAnd then in the route, you just call check bot ID.\nWill handle grabbing from the request and doing everything else it needs to.\nMy hesitations here are that if you're not using Vercel or more importantly you're not using next, this needs request data.\nSo you'd have to pass it request and it doesn't seem like they offer that.\nSo this is a very tied to Vercel solution.\nServer action.\nSame deal.\nYou just call check bot ID.\nPricing.\nHere's where we are.\nBasic all plans free deep analysis for pro and enterprise cost a dollar per thousand requests.\nOkay, interesting.\nSo, the basic check is always free for everyone.\nHuh, that makes this way more compelling, actually.\nOkay, I think I see what they're doing here.\nThey show this off as two modes, but what this actually is is two products.\nBasic is their turn style color because Cloudflare doesn't charge for turn style, at least not traditionally.\nThey let you create a certain number of widgets, which are certain sites and configurations that a widget is allowed to be used on, and it doesn't charge based on how much usage it gets.\nSo, for free and cheap services that need to make sure that they're not getting abused, this freess made Turnstyle a really compelling option for services that had a lot of use and made very little if any money from their users.\nBut it also had an insane false positive rate and especially in invisible mode.\nIt was basically unusable in invisible mode.\nSo you were either giving Cloudflare free branding and reach by showing the Cloudflare logo on the UI or you had a 15% chance that a real user's request failed.\nNot worthwhile even for free.\nI found this to be rough.\nAnd then if you need more than 20 widgets or 15 host names, you have to be on an enterprise plan, which is contact sales pricing.\nSo you know how that goes.\nThere's a lot.\nTurnstyle is quite possibly my least favorite Cloudflare product.\nI've played with most of them for something at this point and Turnstyle is the hardest by far for me to recommend.\nIt supposedly got better after we tried, but it was so bad when we tried that I cannot in good faith recommend it unless you really need free and now I still can't because basic mode on Vercel's bot ID for all plans is free.\nIt integrates way better too.\nSo this is a really compelling like turnstyle destroyed option.\nIf this existed when we built the capture system for T3 chat, we almost certainly would have used it.\nDeep analysis is meant to compete with recapture and H capture and more advanced solutions like what WorkOS is doing with Radar and whatnot.\nAnd this price of a dollar per thousand requests is perfectly matched to H capture and recapture.\nThey both cost pretty much exactly the same.\nThis makes so much sense for something like us.\nI could see us setting the basic tier for most routes and then the deep analysis tier for chat generation, for example.\nYeah, this makes so much sense.\nIs there a place where you can turn on deep analysis like at a code level?\nThe bot ID has to be turned on basic or deep analysis at a project level.\nI hate that Vercel.\nI know somebody's watching or will watch later.\nFix that.\nGive me the ability in this config where I'm actually defining the routes here to pick if it is deep analysis or normal.\nI should be able to choose here and then of course I'd have to like specify when I call here like on the check bot ID call I'd have to put like deep colon true or something give me a big error if I have the wrong config on either side but that's I think a relatively trivial change that will make this way more compelling where for most routes I can use basic and the expensive ones I can use for that good stuff that's a very compelling change and another one of those things like I am so excited for a year or two from now where the pain I had to go through with caption does just feels archaic and wrong.\nThe same way that like configuring a big Webpack config now just is funny because we've moved past that problem as an industry, but it was so real at the time.\nSo, this is one of the most compelling things they've done.\nI'm excited for a future where people don't have to think about and set up captions like this anymore.\nCan't wait for us to get there.\nAnd if we go through this list, good invisible mode.\nThey only have invisible mode, low false positives.\nSeems like it's really good according to the numbers I've seen from them and from the partner uh Casada, right?\nYeah, from their partner seemed really good.\nGood DX looks the best DX I've seen so far by quite a bit.\nThey're not handling the different error states the ways I would want to.\nI'm sure like you have to send the error back yourself and show it to the user.\nIt'd be nice if they made it easier to handle those.\nI still want promotion to a visible capta because there will always be some type of user that like really needs to have a visible capture verification because they're on a weird browser on a weird OS.\n\n\nWith a VPN, I don't want to restrict those users, but I want them to put the work in.\nYou get the idea.\nOh, Resend just removed Cloudflare turn style for their signin, and it's cleaner, and their login is 2x faster.\nI'm almost positive that they just moved to the new thing that Vercel put out.\nI would be surprised if it wasn't that.\nOh yeah, here we are.\n6 hours ago, Vercel announced this new feature.\nThey implemented it.\nSame bot protections, testing, push production.\nNot only does it declutter login, cuts our login times in half.\nHuge.\nThat's awesome.\nGood stuff.\nIt it it makes the code so much simpler, too.\nYeah, if you've implemented recapture 2 or even recapture 3 before, you know how rough they can be to set up.\nThis is super compelling.\nArguably the most compelling thing that they launched for most developers of most services.\nReally cool.\nAnd now we are at the last thing Vercel dropped, the AI gateway.\nThey've been talking about this for a bit, but it officially shipped today in beta.\nSo, what is the AI gateway?\nThe single endpoint to access a wide range of AI models across providers with better uptime, faster responses, and no lockin.\nNow, in beta, you can use models from providers like OpenAI, XAI, Anthropic, Google, and more.\nYou can do usage based billing from the provider listed prices, bring your own key, improved observability, including per model usage, latency, and error metrics, simplified authentication, fallback, and provider routing for more reliable inference.\nReally big deal, and the higher throughput and rate limits.\nThe start of the hut takes anthropic is the worst way to use anthropic models.\nWhat do I mean when I say this?\nThere are a couple ways that a model provider can fail you.\nThe ones that I care about the most are reliability.\nHow often do requests succeed versus failing?\nHow often does the service actually go down and just be unable to resolve entirely?\nHow often does it error for random reasons?\nAll those types of things.\nLike on a scale from planet scale to neon, where does it fall?\nAnd then the other really big one, rate limits.\nYou have no idea how hard and obnoxious it is to get anthropic to bump your rate limits.\nNo matter how big a customer you are, no matter how many connections you have, none of those things matter.\nThey have four tiers of customer that are defined by how much money you spend and you getting promoted internally to be a certain tier.\nCloud Sonnet 4 on the base tier can do maximum 50 requests per minute, which sounds like a decent bit until you realize it's 20k tokens per minute in and 8,000 tokens per minute out.\nOne user's request can be up to a 100,000 tokens in.\nSo, one user can saturate this for 5 minutes with one request.\nActually, entirely unusable for real workloads.\nSo, you need to bump up this tier list before you get somewhere usable.\nAnd even then, 80,000 token per minute maximum on tier three.\nThe tiers are again based on how much money you spend and when you get like the bump pressed.\nTier 4 requires that you're spending 5,000 a month or more.\nAnd then you're like forced under custom tiers where they might or might not bump these limits for you.\nSo, we had Claude 37 bumped relatively high because we were one of the bigger promoters of the model when it dropped.\nI did my YouTube video and everything, bugged them, and they gave us higher allocation.\nAnd then Sonnet 4 dropped and we are at obviously tier 4, 200k tokens in that is two requests per minute.\nEven if we can do 4,000 requests per minute, if our input token limits 200k and two people hit 100k on two messages, that's two messages a minute.\nThat's horrible.\nThat's terrible.\nThat's entirely unusable for us.\nWe regularly blast through the tier four limits.\nAnd once it's custom, you need to get on an enterprise plan and do the sales dance back and forth and everything.\nIt's hell miserable.\nI wouldn't wish it on anybody.\nSo, what do we do if the rate limits on Anthropic's API are this terrible in negotiating with them is horrible?\nAnd the other third point, this one really annoys me.\nUh cost negotiations.\nWhen you're doing a certain level of throughput on anything, you can usually negotiate down the rate and get a discount.\nIf we committed with Anthropic to do 20K a month of committed spend.\nSo we literally spend 20K a month for just credits.\nIf we go over, we can spend more.\nBut if we don't hit that, we have committed to spending that much regardless.\nThey'll give us a discount.\nThe discount we would get if we agree to 20K a month.\nThat is $240,000 a year.\n5%.\nWe get a 5% discount for agreeing to an absurd level of spend.\nSo all of these things with anthropic suck.\nThe reliability is garbage, especially when new models drop, especially when cursor ships new features.\nRate limits make it literally unusable.\nAnd the cost negotiations are not a real benefit.\nThe way that they would prevent the things I'm about to talk about is if they made the negotiations and the rates you can get there way friendlier and easier to do, but they won't do that because they're kind of insane.\nAnd here is where the problems start.\nReliability is rough.\nOkay, I can't find my screenshot, but I had screenshots of how bad the fail rate was on Drop.\nWhen Cloud 4 dropped, Opus had a 15% success rate from the official APIs.\nSonet was heavily rate limited, and Bedrock and Vertex were not supporting it yet.\nWait, Bedrock and Vert.ex, isn't this a cloud model?\nYeah.\nOne of the really fun things that Anthropic did is that they made a deal with both Google and AWS to allow them to host the models at the same rate that Anthropic does.\nSo, if you really want to keep your models inside of AWS or GCP because that's where all your other spend is, you can go ahead and use Anthropics models there.\nYou just have to pay Google instead.\nAnd I'm sure Google gets some level of discount, but they are forced to charge the same rate so that they're not undercutting Anthropic as part of their deal.\nMakes a ton of sense.\nI get why they do that.\nBut where things get interesting is when we look at reliability and rate limits.\nDo you know what's really reliable?\nDo you know what doesn't really care about rate limits?\nAWS.\nDo you know what's kind of reliable and is not really aware of what rate limits are except for on AI studio which is a whole separate rant for another time?\nGoogle Cloud.\nSo what if we could use the best of all of these?\nWhat if we could use anthropic models on anthropic service when it's up?\nWhen it's down, we could just take the same request and do it on Google's infrastructure or Amazon's infrastructure.\nThis is what open router does.\nIf we look for Cloud Sonnet here, Cloud 4, you can see they have a couple different providers.\nThey actually, believe it or not, default to Google Vertex US because it's faster than Anthropics APIs.\nIt's almost twice as fast.\nNow, I'm almost certainly going to move us to Open Router because we'll get a 2x speed improvement at the same cost and better reliability because when other things go down, it will just route to the right place.\nIt's a really good platform since Open Routers already negotiated with all of these providers to get through the rate limits to the best of their ability since they are spreading the traffic across all of these different providers and they let you bring your own API key for the ones you want to.\nAll of that together ends up making open router a really compelling platform for running anthropic models.\nI can't recommend that you use the anthropic API when you could use open router instead because you'll for the same price get faster responses, way better reliability and not have to worry about rate limits.\nThat's a phenomenal deal.\nAnd if you bring your own key for anthropic models and then the enthropic API down, you can have it configured to fall back to Google and use credit through open router instead of your own anthropic credits on your own anthropic account.\nIt's really nice.\nPlatforms like Open Router make all the sense in the world and I would expect long-term for these guys to win because Anthropic and Amazon and Google are too busy competing with each other to do this themselves.\nDo you know who doesn't care what cloud is behind their compute though?\nVercel.\nThey don't give a where this comput is running because they're not competing in the GPU world.\nVercel is not spinning up server farms with a bunch of GPUs in them.\nThey don't care.\nSo Vercel and of course in this case open router they don't have to worry about which place the traffic is routed to.\nThey don't care as long as you get a response more reliably than you would otherwise with lower latency and higher throughput as well which is really cool.\nMakes so much sense.\nSo Vercel seems to have woken up to this reality and realized oh we could be that in between because Vercel doesn't care about where the models being hosted.\nThey're not a model host.\nIt doesn't matter to them.\nThey are effectively commoditizing the places where the GPUs run.\nThe same way with AWS, you don't care which server warehouse your stuff is running on.\nYou don't care if your CPU is Intel or AMD or whatever.\nThe abstraction is a layer higher.\nThis abstraction is a layer higher, too.\nWe also have somebody from Open Router hanging out here.\nWhile they do have some rate limits, but for 99.99% of their users, it's essentially zero.\nAnd that's the rate limit from Anthropic.\nBut they can also route your traffic other places.\nSo it barely matters.\nThey don't have per user rate limits.\nTheoretically, if enough people are doing a ton of traffic on Open Router at the same time, they could hit rate limits with their providers, but it effectively never happens and you're hooking into someone else's rate limit stuff.\nAnd even then, if you bring your own API key, you get around that, too.\nSo, let's see what Vercel's up to here.\nOh, yeah.\npretty much exact same thing as what Open Router is building.\nGenerally speaking, if there's a company that's all in on a thing that is complex enough to have a lot of moving parts and a company that's spinning up the equivalent on the side, I tend to think the team that's focused on it fully is going to win.\nYou can look at something like Vercel Blob versus Upload Thing.\nEven though Vercel built Nex.js and is like the lead of good TypeScript developer experience stuff, Upload Thing is a way better experience and has way happier users than Blob does.\nIt's also more production ready, scales better.\nWe built upload thing for a reason.\nIt's because I wasn't happy with the solutions.\nEven though Vercel put out blob around the same time, ours was better.\nWe won.\nI think this is going to happen here, too, where open router has a team of incredibly talented devs working super hard to make the best like distributed experience for making inference happen.\nVercel has it as a checkbox you can go hit in their UI and use it there.\nI don't see a world where their AI gateway beats out open router, but it could happen.\nI'm all for it.\nAnd having yet another way to circumvent the borderline extortion that is the anthropic developer experience trying to provision those models.\nI'm okay with you putting your money anywhere that isn't directly anthropic at this point because their platform is so rough to work with.\nSo if you are using cloud models please use open router or AI gateway or even just deploy on bedrock or GCP directly.\nI don't care what you go with.\nJust know that the anthropic experience trying to use those models via the API, it's going to bite you in the ass and it's going to do it a lot.\nYeah, I did actually miss a couple things, believe it or not, even after all of that.\nSo, let's just really quickly blast through those and wrap up.\nRolling releases.\nThis is awesome.\nPreviously, if you wanted to spin up a new version of your app to a percentage of your users, you really couldn't.\nIt was kind of an all or nothing thing.\nI ran into this a lot during the T3 chat beta and we eventually just told users to go use the beta version to move traffic over by hand.\nIt would have been really nice if I could have routed a percentage of our traffic to a different front end and back end.\nNot really realistic at the time.\nThis is super useful for those of us who want to do big overhauls.\nAgain, it feels like they're building a lot of these based on the pain points I've experienced.\nAlso, that's a familiar profile picture.\nDimmitri is the guy who did the TypeScript Doom thing.\nLunatic.\nCool to see him popping up in stuff like this.\nNext, we have everyone's favorite topic, microrends.\nMicrorends are proof that Vercel is now a big company because they almost certainly need this for themselves.\nThe concept of microrends is instead of your components being files in a codebase, think of them like packages.\nSo, a given component that is owned by a specific team can be updated and you can update specific parts of the product without having to update the entire JS for everything.\nMeans you can distribute everything from the framework choices to the build tools, pipelines, and all of that so that your UI is broken up into the logical parts based on your teams.\nIf you've ever heard the phrase you ship your org chart, microrends makes it possible to ship your org chart.\nAnd since most businesses eventually need to go in that direction, it makes a ton of sense.\nWe absolutely should have found a way to do microrends at Twitch.\nInstead, we ended up with a gigantic repo with a single package JSON that sucks to work in.\nThis is the solution to maintaining good DX at scale, but someone has to pay the piper and deal with those DX experiences at that scale.\nMicrorends means that a couple people have to maintain it and have a experience so that everybody else can do what they want to do.\nAnd if you do it right, it's awesome.\nVercel seems to now be embracing it and allow you to split large apps into smaller independently deployable units that each team can build, test, and deploy on.\nAwesome.\nYou care a lot about this stuff, just you should probably check out Sephr Cloud.\nThese guys are the gods of microrends.\nI helped them with the copy on the homepage.\nI love these guys.\nThey were an early sponsor.\nStill chat with them all the time.\nGood crew.\nThey get it.\nChat with them if you really want to make a scalable front end.\nAnd finally, Vercel agents.\nI actually have no idea what this one is.\nVercel agent now unlimited beta.\nAn AI assistant built into the Vercel dashboard that analyzes your app performance and security data\n\n\nFocuses on observability, summarizing anomalies, identifying likely causes, and recommending specific action.\n\nOkay, so this looks at all of the data for your Vercel deployments and makes suggestions on things going on, from incidents to weird performance issues, and makes suggestions.\n\nReally cool.\n\nNice to see.\n\nExcited to see where this ends up.\n\nIt's in limited beta, so it's going to be a bit, but it seems really promising.\n\nThat's really funny.\n\nI left the open router dashboard open, and now it has an application side error.\n\nTurns out writing code is hard.\n\nIf only they had the Vercel agent to tell them what went wrong here.\n\nIt was a very good day to be a Vercel customer, and I wish these things existed before.\n\nI'm very happy they do now.\n\nThey're going to save me a bunch of time and money and a lot of frustrations going forward, and they might even save me some users if their capture solution has fewer frustrating moments for those users.\n\nI am very excited about this.\n\nLet me know what you guys think.\n",
  "dumpedAt": "2025-07-21T18:43:25.867Z"
}