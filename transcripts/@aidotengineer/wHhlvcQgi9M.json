{
  "episodeId": "wHhlvcQgi9M",
  "channelSlug": "@aidotengineer",
  "title": "7 Habits of Highly Effective Generative AI Evaluations - Justin Muller",
  "publishedAt": "2025-06-03T22:22:29.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Hello, welcome to my talk on the seven",
      "offset": 5.879,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "habits of highly effective generative AI",
      "offset": 8.559,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "evaluations. I'm Justin Mohler. I'm a",
      "offset": 12.04,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "principal applied AI architect at AWS. I",
      "offset": 14.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "have degrees in physics, math, law, and",
      "offset": 18.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "business. I've been working in natural",
      "offset": 20,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "language processing for about 15 years",
      "offset": 21.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "on and off and generative AI for the",
      "offset": 23.439,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "last four years uh inside of AWS. My",
      "offset": 26.4,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "team is a small specialist team that",
      "offset": 30.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "helps customers scale Genai workloads.",
      "offset": 32.239,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "So I've had the opportunity to work on",
      "offset": 34.239,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "many many workloads across all different",
      "offset": 35.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "industries, all different sizes of",
      "offset": 38.079,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "customers. Some small workloads, some of",
      "offset": 39.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the largest in North America. Um, I'll",
      "offset": 42.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "also mention that because of that",
      "offset": 45.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "experience, I've seen a lot of workloads",
      "offset": 47.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "fail and I've seen a lot of workloads",
      "offset": 48.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "succeed. And so, one of the things my",
      "offset": 50.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "team does now is that we've gathered up",
      "offset": 52.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "a lot of the best practices we've seen",
      "offset": 54.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "across the successful workloads as well",
      "offset": 56.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "as some of the common failure points",
      "offset": 58.719,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "we've seen on the failed workloads and",
      "offset": 60.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "share them as best practices in talks",
      "offset": 62.16,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "like this",
      "offset": 64.479,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "one. All right, so let's get going. The",
      "offset": 65.56,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "first question is uh what is the biggest",
      "offset": 68.799,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "challenge in scaling generative AI? You",
      "offset": 71.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "may you may be able to guess the",
      "offset": 73.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "solution here as this is what the talk's",
      "offset": 75.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "all about. Um, but I I would argue that",
      "offset": 77.36,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "it's",
      "offset": 80.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "evaluations. I in in my experience,",
      "offset": 81.72,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "having been called in to help many",
      "offset": 84.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "different customers scale their Geni",
      "offset": 86.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "workloads, there's lots of concerns that",
      "offset": 88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "come up. And when I ask this question",
      "offset": 89.84,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "with a live audience, there's things",
      "offset": 91.439,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "like cost,",
      "offset": 93.439,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "hallucinations, accuracy,",
      "offset": 95.24,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "capacity, all these different types of",
      "offset": 97.88,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "um concerns come up. But the number one",
      "offset": 100.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "thing that I see across all workloads is",
      "offset": 103.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a lack of evaluations. And in",
      "offset": 105.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "particular, I call it the missing piece",
      "offset": 107.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to scaling geni because a lot of times",
      "offset": 108.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "when a customer has",
      "offset": 110.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "uh successfully built a a PC and they",
      "offset": 112.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "have a a workload that they like or",
      "offset": 115.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "chatbot that seems cool and they're",
      "offset": 117.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "figuring out how to scale it, um this is",
      "offset": 119.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the piece that I come in and most often",
      "offset": 123.2,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "it's missing. And when we add",
      "offset": 125.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "evaluations, it unlocks the ability to",
      "offset": 127.479,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "scale. and we're going to look at how",
      "offset": 129.92,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "that",
      "offset": 131.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "happens, but it's by far the most common",
      "offset": 132.52,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "way to unlock scale. All right, I'll",
      "offset": 135.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "give you a quick customer example. Um,",
      "offset": 138.16,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in uh July 2024, I was called in to as a",
      "offset": 140.48,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "as like an escalation on a workload",
      "offset": 143.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "where a customer was doing document",
      "offset": 146.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "processing and they'd been working on",
      "offset": 148.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this document processing workload for",
      "offset": 150.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about I think six months, 12 months",
      "offset": 151.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "maybe. Um, and they had six or eight",
      "offset": 154.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "engineers working on it. So, it was a",
      "offset": 155.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "pretty big project at that point. and",
      "offset": 157.12,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "the uh the",
      "offset": 159.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the VP of technology that was in charge",
      "offset": 162.04,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "of the project called me in. He asked",
      "offset": 164.64,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "everyone else to leave the room and he",
      "offset": 166.319,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "was like, &quot;Look, this project, we spent",
      "offset": 167.519,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "a bunch of money on it. Accuracy is at",
      "offset": 169.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "22%. We're thinking about just cutting",
      "offset": 172.519,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "the project. We're not getting the",
      "offset": 174.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "results we need. So, I need you to tell",
      "offset": 175.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "me if we can save this thing.&quot; And uh",
      "offset": 177.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "essentially, I spent a couple of weeks",
      "offset": 181.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "doing discovery and understanding what",
      "offset": 182.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "was going on in the workload. And the",
      "offset": 184.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "number one thing that I realized was",
      "offset": 186.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "they had zero evaluations, right? They",
      "offset": 187.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "had this big process and end to end they",
      "offset": 190.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "had a single number coming out the end",
      "offset": 192.319,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that said, &quot;Well, it's 22% accurate.&quot; So",
      "offset": 194.239,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "what I did is design a evaluations",
      "offset": 196.959,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "framework and the results were almost",
      "offset": 200.159,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "funny because once the evaluation",
      "offset": 202.159,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "framework was in place and you could see",
      "offset": 203.599,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "exactly where the problems were, fixing",
      "offset": 205.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "them were trivial, right? Fixing the",
      "offset": 206.879,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "problems wasn't really the challenge.",
      "offset": 209.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Increasing accuracy wasn't really the",
      "offset": 210.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "challenge. It was knowing where the",
      "offset": 211.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "problems were and what was causing them.",
      "offset": 213.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So over the course of the next six",
      "offset": 216.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "months they built the evaluation",
      "offset": 217.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "framework. They fixed uh many of the",
      "offset": 219.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "issues that came up. They were able to",
      "offset": 221.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "achieve 92% accuracy by January. Uh when",
      "offset": 223.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "uh which 90% was their threshold for",
      "offset": 227.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "launching to production at scale and",
      "offset": 229.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "they launched and became the single",
      "offset": 232.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "largest document processing workload uh",
      "offset": 234.72,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "on AWS in North America at the",
      "offset": 237.2,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "time. All right. So evaluations are",
      "offset": 240.36,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "really really important. what are they?",
      "offset": 242.879,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "How do we actually build these things?",
      "offset": 245.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "This is the place I like to start with",
      "offset": 248,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "evaluations because many times,",
      "offset": 249.519,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "especially if you're coming from a",
      "offset": 251.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "traditional a IML back uh background,",
      "offset": 252.959,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "you look at evaluations as a way to",
      "offset": 255.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "measure",
      "offset": 258.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "quality because you're you're familiar",
      "offset": 260.44,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "with things like well what's your F1",
      "offset": 262.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "score? What's your precision and recall?",
      "offset": 264.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And you get a score and it just tells",
      "offset": 266.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you how well you're doing. And while uh",
      "offset": 268.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Jenny or Triv AI evaluations do produce",
      "offset": 271.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a score, if anything, it's a it's a far",
      "offset": 273.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "third of the top reasons why we do",
      "offset": 276.16,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "evaluations with geni",
      "offset": 278.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "workloads. The main goal with any",
      "offset": 280.52,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "evaluation framework should be to",
      "offset": 283.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "discover problems. Similar to the",
      "offset": 284.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "workload example that I just shared, if",
      "offset": 286.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "your evaluation framework tells you",
      "offset": 288.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "where the problems are and perhaps even",
      "offset": 290.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "suggests solutions because it includes",
      "offset": 291.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "an element of generative AI reasoning,",
      "offset": 293.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "now we can improve our workloads. And of",
      "offset": 296.16,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "course, yes, we need to measure quality.",
      "offset": 298.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "That's very important. But I I start",
      "offset": 299.759,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this way because when you think about",
      "offset": 302.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "designing an evaluation framework, if",
      "offset": 303.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "you come in with the mindset that it's",
      "offset": 305.919,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "going to find errors, you design it in a",
      "offset": 308.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "very different way than if you come in",
      "offset": 311.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and with the mindset that it's just",
      "offset": 313.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "going to measure how well you're doing.",
      "offset": 314.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "All right. So, let's double click on",
      "offset": 317.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "what that looks like.",
      "offset": 318.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Um and and before we do that though, I",
      "offset": 321.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "do want to I I do want to add a second",
      "offset": 323.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "aside on why I think evaluations are so",
      "offset": 325.36,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "po",
      "offset": 328,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "important.",
      "offset": 328.759,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Um my team within AWS is a very small",
      "offset": 330.6,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "specialist team and so we have to have",
      "offset": 333.759,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "very very strict filters on which",
      "offset": 335.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "projects we say yes to and which ones we",
      "offset": 338.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "support. And my experience has been",
      "offset": 340,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "evaluations are the number one filter",
      "offset": 342.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that separates a science project from a",
      "offset": 344.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "successful project. When I talk to a",
      "offset": 346.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "team and the team says, &quot;Can you teach",
      "offset": 349.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "me about multi-agent collaboration?&quot; And",
      "offset": 352.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I say, &quot;Great. Let's sit down and spend",
      "offset": 355.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "two hours putting together a gold",
      "offset": 357.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "standard set for our evaluations so that",
      "offset": 358.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "your multi- aent collaboration framework",
      "offset": 360.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "project is successful.&quot; If the team",
      "offset": 363.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "comes back and says, &quot;Oh gosh, two hours",
      "offset": 366.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "on eval sounds boring. Can you just give",
      "offset": 369.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "me the toys to play with?&quot; I I know",
      "offset": 370.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "right away that's a science project and",
      "offset": 372.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's not going to go anywhere. and and",
      "offset": 374.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "science projects are important. They're",
      "offset": 376.88,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "a fun way to learn. They're just not",
      "offset": 378.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what goes to scale. On the other hand,",
      "offset": 379.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "for my projects that have been wildly",
      "offset": 382.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "successful, where customers have had",
      "offset": 383.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "100x return on investment or maybe cut",
      "offset": 385.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "their cost by 10,000% or something like",
      "offset": 388,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that, those customers when I say, hey,",
      "offset": 390.44,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "can we spend two hours uh building an",
      "offset": 392.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "eval framework? They say, why don't we",
      "offset": 394.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "send spend four? Right? Evals are so",
      "offset": 396,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "important and we recognize that and this",
      "offset": 398.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "project is so important that we're going",
      "offset": 400.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to invest the time. So I I'll as an",
      "offset": 402.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "aside I'll just mention it as my number",
      "offset": 404.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "one filter for deciding which projects",
      "offset": 406.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "are going to be successful and which are",
      "offset": 408.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "worth spending time on.",
      "offset": 409.6,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "All right. So,",
      "offset": 412.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "evalative AI world come with baggage,",
      "offset": 414.6,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "right? And especially if you have a",
      "offset": 417.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "traditional AIML background, it can be a",
      "offset": 419.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "little scary, right? You you look at",
      "offset": 420.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "evals and it's free text coming out and",
      "offset": 422.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "you're like, &quot;Oh my gosh, it's not going",
      "offset": 424.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to be a specific number. I can't",
      "offset": 426.639,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "mathematically calculate an exact number",
      "offset": 428,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "every time. Let's",
      "offset": 429.759,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "panic.&quot; Right? And it turns out that the",
      "offset": 431.8,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "human race has been grading and",
      "offset": 434.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "evaluating",
      "offset": 437.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "uh free text workloads for uh hundreds",
      "offset": 438.72,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "of years if not thousands, right? Um can",
      "offset": 442.639,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "you imagine if my professor in English",
      "offset": 446.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "assigns me an essay and says uh write me",
      "offset": 449.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a three-page essay on the on the",
      "offset": 452,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "emotional meaning of",
      "offset": 454.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "sunshine. And I I spend an I spend my",
      "offset": 456.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "hours and I write my essay. I turn it",
      "offset": 459.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "in. The professor says, &quot;Well, I can't",
      "offset": 460.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "calculate an exact answer to this. I",
      "offset": 462.479,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "can't calculate your exact source, so I",
      "offset": 464.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "give up. I'm just not going to score",
      "offset": 466.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "it.&quot; No, of course not. The professor is",
      "offset": 467.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "going to give it a score. And we can",
      "offset": 469.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "evaluate generative AI in the same way",
      "offset": 471.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that we've been evaluating free text for",
      "offset": 474,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "for decades and generations, right? But",
      "offset": 476,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the key understanding is if you have a",
      "offset": 479.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "bad professor, they just give you a",
      "offset": 481.36,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "score. Maybe you've had this experience",
      "offset": 482.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "like me. You've turned in an essay and",
      "offset": 484.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "got an F, right? If you just get a",
      "offset": 485.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "score, you're like, &quot;Well, that that",
      "offset": 487.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "stinks, but what do I do with it?&quot;",
      "offset": 490.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "Right? With generative AI, we can go a",
      "offset": 492.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "bit deeper. We can do what the good",
      "offset": 494.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "professors do, which is point out what",
      "offset": 496,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you did wrong, point out where you can",
      "offset": 497.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "improve, right? This is why evaluations",
      "offset": 500.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "in the term in the in the context of",
      "offset": 503.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "grade school are very very valuable",
      "offset": 505.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because you can improve. This is the",
      "offset": 507.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "same for generative AI.",
      "offset": 509.12,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "All",
      "offset": 512.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "right, let me give you another example,",
      "offset": 512.76,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "another um unique complexity to the",
      "offset": 515.68,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "evaluation of generative AI. This is a",
      "offset": 519.039,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "2x4 and I gave myself the task of",
      "offset": 522.64,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "drilling a 1-in hole into the 2x4 all",
      "offset": 525.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "the way through it. And when I talk to a",
      "offset": 529.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "live audience, I ask for a thumbs up,",
      "offset": 532,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "thumbs down. How did I do on this task?",
      "offset": 533.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Right? Very similar to the types of",
      "offset": 535.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "feedback we often have for generative AI",
      "offset": 537.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "uh workloads. And often I get mostly",
      "offset": 540.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "thumbs up. There's always that one guy",
      "offset": 542.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that gives me a thumbs down. Um, and",
      "offset": 543.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "what you're done, what you've done is",
      "offset": 547.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "evaluated my output. But if the",
      "offset": 548.399,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "methodology I used to to create this",
      "offset": 551.279,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "hole looks like",
      "offset": 555.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "this, right? Suddenly maybe I didn't do",
      "offset": 556.839,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "such a good job. Maybe how I got there",
      "offset": 560.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "matters. And the reason is even though I",
      "offset": 562.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "managed to make a good hole this time,",
      "offset": 565.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "if this is my setup, if this is the",
      "offset": 567.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "methodology I used to to achieve it or",
      "offset": 569.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in the context of generative AI, if this",
      "offset": 571.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "is the reasoning I took to create the",
      "offset": 573.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "hole, then maybe I need to I need to",
      "offset": 576.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "rethink my system. So let me give you",
      "offset": 578.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "another example in the context of a real",
      "offset": 580.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "customer use case. This is a meteorology",
      "offset": 582.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "company that was creating um summaries",
      "offset": 584.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of local weather based on sensor data.",
      "offset": 587.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "So, I've summarized what their prompt",
      "offset": 590.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "looked just so it's easy to read here.",
      "offset": 592,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "But essentially, they have a prompt that",
      "offset": 594.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "gives sensor data like it's raining and",
      "offset": 596.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it's 40° and it's windy and it's asking",
      "offset": 598.399,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "for a",
      "offset": 600.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "summary. And if the summary says uh",
      "offset": 601.8,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "today it's sunny and bright outside. All",
      "offset": 605.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of the sudden it's like hm something has",
      "offset": 607.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "gone wrong, right? If we evaluate this,",
      "offset": 610.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the score is zero. You know, this prompt",
      "offset": 612.8,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "gets an F, but we don't know why, right?",
      "offset": 614.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "But what if we ask the model to explain",
      "offset": 616.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "its reasoning? And this reasoning is",
      "offset": 618.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "it's important to mental health to be",
      "offset": 620.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "happy. So I decided not to talk about",
      "offset": 622.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the rain. Now that we've looked behind",
      "offset": 623.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the scenes and we've seen kind of what",
      "offset": 625.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the model how the model got there, we",
      "offset": 627.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "suddenly have a lot more insight into",
      "offset": 629.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "how to fix the problem. Right? And you",
      "offset": 630.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "can imagine this in the context and why",
      "offset": 632.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this is such a big problem in the",
      "offset": 634.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "context of evals. Let's look at the at",
      "offset": 635.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the opposite opposite example. If we",
      "offset": 638.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "give it data and it says it's sunny and",
      "offset": 640.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "then the response comes out in sunny.",
      "offset": 642.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Well, we'll say response is 10 out of",
      "offset": 644.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "10. Awesome. I've built a a brilliant",
      "offset": 647.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "prompt and I can go to scale. And this",
      "offset": 650.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is a danger because what happens is if",
      "offset": 652.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you didn't eval the reasoning and this",
      "offset": 654.88,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "was the",
      "offset": 657.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "reasoning. All of a sudden you're like,",
      "offset": 658.2,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "okay, maybe my prompt isn't working",
      "offset": 660.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right because even though I got the",
      "offset": 661.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "right answer, I drilled the hole",
      "offset": 663.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "correctly in this one case, right? The",
      "offset": 664.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "reasoning is is uh not getting me where",
      "offset": 667.6,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "I need to",
      "offset": 669.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "be. Okay.",
      "offset": 671.24,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "The last um the last step before I get",
      "offset": 673.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "into my my seven habits of generative AI",
      "offset": 676.399,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "uh evaluations is what I call prompt",
      "offset": 679.36,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "decomposition. And although this uh this",
      "offset": 683.399,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "technique is not specific to",
      "offset": 686.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "evaluations, it's often done in the",
      "offset": 688,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "context of evaluations because when it",
      "offset": 690.079,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "comes to geni, you only can attach an",
      "offset": 692.32,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "eval to one prompt. It's difficult to",
      "offset": 695.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "evaluate the first half of a prompt",
      "offset": 699.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "versus the second half of of a prompt,",
      "offset": 701.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? I think of it like a multimeter",
      "offset": 703.76,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "for my electrical engineers out there.",
      "offset": 705.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "You've got your two sensors. You put one",
      "offset": 706.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "sensor at the beginning of a prompt, one",
      "offset": 709.04,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "sensor at the end of the prompt, and",
      "offset": 710.399,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "you're sensing what went on in the whole",
      "offset": 711.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "prompt. Many times, if the prompt is",
      "offset": 713.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "very large and it does a lot of",
      "offset": 716.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "complicated things, it's difficult to",
      "offset": 717.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "understand where the error is because",
      "offset": 719.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you just kind of get like, well, there's",
      "offset": 721.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "something going wrong in this big pile",
      "offset": 723.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "of code or big pile of prompting. And",
      "offset": 724.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so, um, this is that same weather",
      "offset": 727.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "company. This is an example from them",
      "offset": 730.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "where they were having to write a",
      "offset": 732.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "summary of today's weather. Inside their",
      "offset": 734.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "prompt, they did a lot of different",
      "offset": 736.399,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "instructions based on what the sensor",
      "offset": 737.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "data was. And one of the things they",
      "offset": 739.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "noticed is that for a portion of their",
      "offset": 741.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "prompt, they actually had it say, okay,",
      "offset": 743.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "well, if the wind speed is like less",
      "offset": 745.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "than five, then it's not very windy, but",
      "offset": 747.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if it's more than five, then it's windy.",
      "offset": 749.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And what was happening is in a in the in",
      "offset": 751.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the PC stage, that worked great. when",
      "offset": 755.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "they tried to scale out maybe two to",
      "offset": 756.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "three% of the time Claude was coming",
      "offset": 758.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "back and saying oh uh the wind speed is",
      "offset": 761.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "seven seven is less than five so it's",
      "offset": 763.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "not windy right and just completely",
      "offset": 765.839,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "messing up the",
      "offset": 768.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "math so what we did here is a series of",
      "offset": 769.72,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "prompt decompositions and if you're",
      "offset": 772.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "interested in it I've got a blog here",
      "offset": 773.92,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "you can check out",
      "offset": 775.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 777.72,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "the basic idea is take the big prompt",
      "offset": 779.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and break it into a a chaining series of",
      "offset": 782.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "prompts from From an eval standpoint,",
      "offset": 785,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "that means that you can attach eval to",
      "offset": 787.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "each section of the prompt. And what",
      "offset": 789.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that allows you to do is decide, okay,",
      "offset": 791.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "well, this section is working great.",
      "offset": 793.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "This section isn't. So now I know where",
      "offset": 795.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to focus my efforts. The other thing it",
      "offset": 796.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "does is allows you to decide, is",
      "offset": 798.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "generative AI even the right tool for",
      "offset": 801.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "that section of the prompt. Right? So in",
      "offset": 804,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the case of this weather prompt, um,",
      "offset": 806.079,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "doing a mathematical comparison, is",
      "offset": 808.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "seven larger than five? You don't need",
      "offset": 810.959,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "genai for that. Python is perfect at",
      "offset": 813.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that. It gets it accurate, perfectly",
      "offset": 815.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "accurate. And so in this series of",
      "offset": 816.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "chaining steps, one step was a Python",
      "offset": 818.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "mathematical comparison. When they made",
      "offset": 820.56,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "this change, the accuracy went to",
      "offset": 822.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "100%. And they could tell the accuracy",
      "offset": 825.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "of each step. All",
      "offset": 827.68,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "right, I will briefly mention as well in",
      "offset": 830.68,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "the context of that uh prompt",
      "offset": 834.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "decomposition. It's a very very common",
      "offset": 836,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "pattern and so I wanted to talk about",
      "offset": 837.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "its impact on evaluations. One of the",
      "offset": 839.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "most common patterns we see is semantic",
      "offset": 842,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "routing. some kind of query comes in or",
      "offset": 843.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "an input to a workload. And the first",
      "offset": 846.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "step is well, what kind of task is this?",
      "offset": 848.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "If it's a it's an easy task, go to a",
      "offset": 851.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "small model. If it's a hard task, go to",
      "offset": 854,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a large model. And by doing this, um,",
      "offset": 855.519,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "what I'm doing is using the right model",
      "offset": 858.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "for the job. Not dependent on the prompt",
      "offset": 861.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "itself, but depending on the input from",
      "offset": 864.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the user and the complicatedness, the",
      "offset": 866.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "complication level of the task from the",
      "offset": 869.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "user. And the interesting piece here is",
      "offset": 871.24,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "again attaching evals to each step. And",
      "offset": 874,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "so that means for for an eval semantic",
      "offset": 877.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "router often the input that you're",
      "offset": 879.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "evaluating as a query and the output is",
      "offset": 881.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "just the number one or the number two in",
      "offset": 883.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this case. And what you'll see when you",
      "offset": 885.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "run the eval is often even breaking this",
      "offset": 887.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "down significantly increases your",
      "offset": 890.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "accuracy because you're removing what I",
      "offset": 892.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "call dead space or dead tokens. Right?",
      "offset": 894.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "If this was all one big prompt, what",
      "offset": 897.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "you'd see is your instructions, your",
      "offset": 899.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "instructions for your easy stuff, your",
      "offset": 902.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "instructions for your hard stuff, right?",
      "offset": 904.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And if an easy query comes in, it only",
      "offset": 906.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "needed those easy instructions. Those",
      "offset": 908.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "hard instructions are basically dead",
      "offset": 910.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "space. They weren't necessary, but what",
      "offset": 912.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "they are is extra cost and an",
      "offset": 914.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "opportunity for the model to get",
      "offset": 917.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "confused. So often by breaking it down",
      "offset": 918.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "this way, we see we see the evaluation",
      "offset": 920.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "showing the accuracy popping up because",
      "offset": 922.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you're only sending the necessary",
      "offset": 924.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "information for the",
      "offset": 926.079,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "task. All right, so let's jump into the",
      "offset": 927.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "good stuff. I I didn't mean to keep you",
      "offset": 930.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "waiting, but here we are. These are the",
      "offset": 932.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "seven most common trends I see across",
      "offset": 935.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "all generative AI uh workloads that have",
      "offset": 937.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "successfully scaled. I will say I have",
      "offset": 940.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "never seen a workload go to scale",
      "offset": 942.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "without evaluations and most of them",
      "offset": 944.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "also include these seven habits. The",
      "offset": 946.56,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "first one is",
      "offset": 949.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "fast. It is the case that I have seen",
      "offset": 950.759,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "evaluations built with the mindset that",
      "offset": 953.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "getting results within a week is",
      "offset": 956.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "okay. What happens is I I make a prompt,",
      "offset": 958.92,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "I push it to some kind of test site, I",
      "offset": 962.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "email a team to go test it, they test it",
      "offset": 964.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "whenever they feel like, they email me",
      "offset": 966.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "back whenever they feel like, and a week",
      "offset": 968.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "later I get to make one more change to",
      "offset": 969.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "my prompt. And you can imagine a team",
      "offset": 971.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that's working on that mindset that they",
      "offset": 973.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "get to have like four changes or eight",
      "offset": 975.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "changes a month versus a team that's",
      "offset": 977.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "managed to build an evaluation framework",
      "offset": 979.44,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that operates in seconds and they can",
      "offset": 981.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "make hundreds of changes every day and",
      "offset": 982.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "hundreds of tests every day. Right? The",
      "offset": 985.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pace of innovation, the pace of",
      "offset": 987.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "increasing your accuracy is going to be",
      "offset": 989.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "much much faster. Uh to the point where",
      "offset": 990.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I don't think I've ever seen a workload",
      "offset": 992.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "scale unless it's gone through many many",
      "offset": 994.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "iterations. And so sometimes just",
      "offset": 996.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "knowing that at the outset is helpful",
      "offset": 998.079,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "for teams.",
      "offset": 999.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "um rather than somehow you're going to",
      "offset": 1000.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "craft this beautiful perfect prompt your",
      "offset": 1003.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "first try and never change it again. Uh",
      "offset": 1004.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I will say and this is what we're going",
      "offset": 1007.519,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "to see a example of in in just a minute.",
      "offset": 1008.8,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "Um often my my rule of thumb target is",
      "offset": 1011.48,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "30 seconds to run your evaluation",
      "offset": 1014.959,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "framework. And you do that by using",
      "offset": 1016.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "generative AI as a judge to evaluate or",
      "offset": 1018.48,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "if the if the output's numeric like from",
      "offset": 1021.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "a semantic router where it's one two",
      "offset": 1023.759,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "three you're using Python to say is this",
      "offset": 1025.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "did it output a one? Yes or no? Is one",
      "offset": 1027.919,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "equal one? Yes. Okay. Good job.",
      "offset": 1029.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Um, the reason I say 30 seconds is often",
      "offset": 1032.36,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "what it means is you spend 10 seconds in",
      "offset": 1035.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "parallel sending a 100 test cases for",
      "offset": 1038.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "generation. I've created a test. I've",
      "offset": 1040.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "created a prompt template I want to",
      "offset": 1042.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "test. I run it across my 100 test cases",
      "offset": 1043.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and get results. 10 seconds. Then I take",
      "offset": 1045.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "those results and inund more parallel",
      "offset": 1048.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "calls I judge them. So I take my",
      "offset": 1051.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "results, compare them against my gold",
      "offset": 1055.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "standard and I and I get a 100 judges",
      "offset": 1056.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "going per 10 seconds. And in the final",
      "offset": 1059.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "10 seconds, I take my output from my 100",
      "offset": 1061.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "judges and I summarize it. Often that",
      "offset": 1064.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "means breaking it down by categories and",
      "offset": 1066.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "by what's right and what's wrong. And so",
      "offset": 1068.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "what I see in my output is not 100",
      "offset": 1070.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "generated out responses that now I have",
      "offset": 1073.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "to go read through and figure junk out.",
      "offset": 1074.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "What I see in that last 10 seconds is a",
      "offset": 1077.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "summary where it says, &quot;Hey, you know",
      "offset": 1078.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "what? For all of your questions, all of",
      "offset": 1080.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "your queries that had to do with taxes,",
      "offset": 1081.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "when you got it right, here's what the",
      "offset": 1083.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "trends were. When you got it wrong,",
      "offset": 1085.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here's what the trends were.&quot; And again,",
      "offset": 1088.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it's going from that p from that mindset",
      "offset": 1089.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of how do I figure out what's going",
      "offset": 1092.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "wrong and where the errors are and how",
      "offset": 1094.24,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "to fix",
      "offset": 1095.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "them. Quantifiable is my second one. All",
      "offset": 1097.08,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "all effective frameworks also produce",
      "offset": 1101.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "numbers, right? And I sometimes",
      "offset": 1104,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "depending on the teams, I get a lot of",
      "offset": 1106.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "push back on this because you're right",
      "offset": 1107.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that they're not always the exact same",
      "offset": 1109.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "number every time you run the test,",
      "offset": 1111.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right? If I turn in an essay, maybe I",
      "offset": 1113.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "turn it in on Thursday and I get a score",
      "offset": 1115.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "or I turn it in on Friday. Friday, my",
      "offset": 1118.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "professor has a few cups of uh glasses",
      "offset": 1120.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of wine before grading and so my score",
      "offset": 1122.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is a little better. Right? This happens",
      "offset": 1124.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in real life. It happens with AI vals.",
      "offset": 1126.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "The way that we deal with it is the same",
      "offset": 1129.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in real life as it is with eval which is",
      "offset": 1131.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "we we make it numerous. Right? Even if",
      "offset": 1134.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "one if there's a little bit of jitter in",
      "offset": 1136.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the score in the quantifiable score that",
      "offset": 1138.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we ask for uh if we have enough test",
      "offset": 1140.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "cases and we average across those test",
      "offset": 1143.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "cases that jitter goes out just like in",
      "offset": 1144.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "grade school hopefully you didn't turn",
      "offset": 1147.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "in one assignment ever. Uh you turn in a",
      "offset": 1148.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "bunch of assignments and your your final",
      "offset": 1150.799,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "score was the average of all of",
      "offset": 1152.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "those. The other reason why numerous is",
      "offset": 1154.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "so important is that we want to cover",
      "offset": 1157.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "all of our we want to be diverse and",
      "offset": 1159.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "cover all of our uh test cases. Uh",
      "offset": 1162.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "right? So um oftent times when I'm",
      "offset": 1165.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "running this as an exercise with",
      "offset": 1167.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "customers even the the the exercise of",
      "offset": 1169.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "building 100 test cases is very valuable",
      "offset": 1172.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for the team to figure out what the",
      "offset": 1174.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "scope of the project is because",
      "offset": 1176,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "inevitably someone will say hey when",
      "offset": 1177.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "someone asks about taxes we can we can",
      "offset": 1179.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "direct them to this site where they can",
      "offset": 1182.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "get their W TWS and then someone else",
      "offset": 1183.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "will say no no no if they ask about",
      "offset": 1185.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "taxes we can't respond at all because",
      "offset": 1187.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "we're not allowed to right and the team",
      "offset": 1188.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "is debating the product design nothing",
      "offset": 1190.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to do with geni but it's a good exercise",
      "offset": 1192.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "to understand what is the scope of what",
      "offset": 1194.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you want to answer and and make sure you",
      "offset": 1196.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have questions outside of what you want",
      "offset": 1198.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to answer so that you can you can",
      "offset": 1200,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "measure that the that the model is is uh",
      "offset": 1201.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "redirecting those that are outside of",
      "offset": 1204.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "scope. The next habit is explainable.",
      "offset": 1206.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "This is that insight where it's like",
      "offset": 1209.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "don't just look at the outputs, look at",
      "offset": 1211.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "how you got there, right? Look at how",
      "offset": 1213.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the model is reasoning and in",
      "offset": 1215.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "particularly I I said reasoning for",
      "offset": 1217.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "generation and scoring. Look at how your",
      "offset": 1219.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "judge is reasoning as well for for in in",
      "offset": 1220.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "some cases where you're asking the judge",
      "offset": 1223.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to do a lot of reasoning because just",
      "offset": 1225.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like you're doing prompt engineering for",
      "offset": 1227.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "your for your for your prompt for your",
      "offset": 1228.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "users, you also need to engineer the the",
      "offset": 1230.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "judge prompt and make sure the judge is",
      "offset": 1233.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "is scoring correctly. Um I'll I'll show",
      "offset": 1235.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a small example of that in just a",
      "offset": 1238.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "minute. But the general idea is just",
      "offset": 1239.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "like a a professor grading a paper,",
      "offset": 1241.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "right? It's not necessarily that you",
      "offset": 1244.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "just say, &quot;Hey, give me a score.&quot; Right?",
      "offset": 1246,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "But the professor has a rubric.",
      "offset": 1248.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Professor says, &quot;Okay, well, it needed",
      "offset": 1250,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "to be three pages long. If it's not",
      "offset": 1252.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "three pages long, 100% off. It needed to",
      "offset": 1253.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "have five different sources cited. You",
      "offset": 1256.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "didn't do that. If you only cited three,",
      "offset": 1258.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "well, that's uh 20% off.&quot; Uh, you needed",
      "offset": 1260,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "a professional tone. Every time your",
      "offset": 1262.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tone slips out of professional, uh, take",
      "offset": 1264.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "3% off your score, right? And so you can",
      "offset": 1266.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "see that the rules start building up and",
      "offset": 1269.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you give really clear instructions on",
      "offset": 1271.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "how the how the the output should be",
      "offset": 1273.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "judged and asking your judge to explain",
      "offset": 1276,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "its reasoning is often a good way to uh",
      "offset": 1278.24,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "do prompt engineering for the",
      "offset": 1280.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "judge. The next is segmented. This is",
      "offset": 1282.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "why I showed those slides on prompt",
      "offset": 1284.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "decomposition. In practice, almost all",
      "offset": 1286.12,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "scaled workloads are multiple steps,",
      "offset": 1288.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "right? There's very very few workloads",
      "offset": 1291.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I've ever seen that are a single prompt,",
      "offset": 1293.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right? And what that means is you need",
      "offset": 1295.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "to evaluate each step individually. And",
      "offset": 1296.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "this actually is is uh powerful for a",
      "offset": 1299.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "number of reasons. Not the least of",
      "offset": 1301.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "which is that it's very rare for a",
      "offset": 1302.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "workload to be a single model. It's much",
      "offset": 1304.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "more appropriate for each step because",
      "offset": 1307.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "they're evaluated individually. We",
      "offset": 1310.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "evaluate which model is appropriate.",
      "offset": 1311.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "Many times as semantic router you want",
      "offset": 1313.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "to use something like Nova Micro that's",
      "offset": 1315.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "going to give you an instant response.",
      "offset": 1316.559,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "It's very simple and easy. It's going to",
      "offset": 1318,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "give you one, two, three. Um and so you",
      "offset": 1319.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "can you can with an evaluation framework",
      "offset": 1321.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "prove to yourself the smallest model",
      "offset": 1323.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "that's appropriate to use on each step.",
      "offset": 1325.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Diverse is what I was mentioning",
      "offset": 1329.6,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "earlier. You want to cover all of your",
      "offset": 1330.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "use cases. So sometimes I'll start by",
      "offset": 1332.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "listing out what are the use cases in",
      "offset": 1333.919,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "scope and then make sure we have",
      "offset": 1335.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "questions applying to all of them. I",
      "offset": 1336.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "will say the hundred is a rule of thumb.",
      "offset": 1338.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Sometimes if they're use cases that are",
      "offset": 1340.159,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "like edge cases we don't really care",
      "offset": 1341.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "about. I'll put in like three or four",
      "offset": 1342.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "examples for that. But the core use",
      "offset": 1344.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "cases we want a lot of examples for. And",
      "offset": 1346.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "then the last one I'll mention is",
      "offset": 1348.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "traditional. Um, and I put this in there",
      "offset": 1350,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "because there is a tendency to uh throw",
      "offset": 1352.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the baby out with a bathwater and say,",
      "offset": 1355.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "&quot;Hey, this is geni. We're all going to",
      "offset": 1357.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "use geni. We're going to geni",
      "offset": 1358.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "everything.&quot; You know, the output of",
      "offset": 1360.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "your my semantic router is one. I'll use",
      "offset": 1361.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "jenny to see if that one is really the",
      "offset": 1363.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "number one, right? And uh the answer is",
      "offset": 1365.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "no. There are a lot of traditional",
      "offset": 1368.159,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "techniques that are very very powerful.",
      "offset": 1369.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Um obviously if the output is numeric",
      "offset": 1372.039,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "just just use a numeric evaluation. uh",
      "offset": 1374.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "for things like rag architecture is one",
      "offset": 1378.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "of the most common um there's many many",
      "offset": 1380.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "types of uh database accuracy",
      "offset": 1382,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "evaluations retrieval and precision and",
      "offset": 1384.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "F1 scores and all those kinds of things",
      "offset": 1386.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "for question answering um simple things",
      "offset": 1388.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "like measuring cost and latency are",
      "offset": 1391.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "still traditional tooling so I just want",
      "offset": 1393.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to I I put that there to highlight the",
      "offset": 1396,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "fact that traditional tooling is still",
      "offset": 1397.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "very very powerful and very important uh",
      "offset": 1399.44,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "in the context of GI evaluations all",
      "offset": 1401.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "right I'm going to give a quick example",
      "offset": 1405.08,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "of what this looks like. Uh and uh just",
      "offset": 1408.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh visually we start with that gold",
      "offset": 1411.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "standard set. Again, this is where I",
      "offset": 1413.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "think the most important uh use of your",
      "offset": 1414.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "time is is is building this. Your whole",
      "offset": 1416.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "system is going to be pointed towards",
      "offset": 1418.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and designed towards this gold standard",
      "offset": 1421.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "set. So if your gold standard set has a",
      "offset": 1422.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "bunch of errors, what you've done is",
      "offset": 1424.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "designed a whole system that does that",
      "offset": 1426.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "creates errors, right? Um so so invest",
      "offset": 1428.559,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "it's always worth time investing in",
      "offset": 1432.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "building a good gold standard set.",
      "offset": 1434.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I will uh as a brief aside mention this",
      "offset": 1437.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is a a terrible place to use GEI. If you",
      "offset": 1440,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "use GEI to create your gold standard",
      "offset": 1443.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "set. If the GEI set creates errors,",
      "offset": 1444.76,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "you've built you build a system that",
      "offset": 1448.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "generates the same errors that the GI",
      "offset": 1449.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "system has. Uh and it can lead to a lot",
      "offset": 1451.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of problems. It can be helpful to",
      "offset": 1453.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "generate a what I would call a silver",
      "offset": 1456,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "standard set which is the GI system",
      "offset": 1458.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "taking a guess at a gold standard but it",
      "offset": 1460.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "still needs to be reviewed by a human to",
      "offset": 1462.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "confirm its accuracy. All right, we take",
      "offset": 1463.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "an input from that gold standard set,",
      "offset": 1466.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "one of those, we put it into our prompt",
      "offset": 1468.559,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "template, put it into our LLM in order",
      "offset": 1470.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "to generate an output. We include the",
      "offset": 1473.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "answer and the reasoning. Then we take",
      "offset": 1476.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the the matching answer from that gold",
      "offset": 1478.799,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "standard input uh and we compare it with",
      "offset": 1481.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that generated output into a judge",
      "offset": 1483.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "prompt and the judge generates a number",
      "offset": 1485.36,
      "duration": 7.559
    },
    {
      "lang": "en",
      "text": "and the reasoning behind that number.",
      "offset": 1488.24,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "Um, and then we pull the category. Often",
      "offset": 1494.36,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "I'll I'll include the category in the",
      "offset": 1497.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "gold standard set so that um we can then",
      "offset": 1498.48,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "as that final step uh break things down",
      "offset": 1502.159,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "by category and um",
      "offset": 1505.6,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "and generate a summary for the right and",
      "offset": 1509.24,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "wrong answers uh for each",
      "offset": 1512.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "category. All right, that's it. Thank",
      "offset": 1516.84,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "you so much. I hope this was helpful.",
      "offset": 1520,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Uh, please feel free to reach out with",
      "offset": 1522.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "questions and comments and enjoy your",
      "offset": 1523.919,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "day.",
      "offset": 1526.799,
      "duration": 3
    }
  ],
  "cleanText": "Hello, welcome to my talk on the seven habits of highly effective generative AI evaluations. I'm Justin Muller. I'm a Principal Applied AI Architect at AWS. I have degrees in physics, math, law, and business. I've been working in natural language processing for about 15 years on and off and generative AI for the last four years inside of AWS. My team is a small specialist team that helps customers scale GenAI workloads. So I've had the opportunity to work on many, many workloads across all different industries, all different sizes of customers, some small workloads, some of the largest in North America.\n\nUm, I'll also mention that because of that experience, I've seen a lot of workloads fail, and I've seen a lot of workloads succeed. And so, one of the things my team does now is that we've gathered up a lot of the best practices we've seen across the successful workloads as well as some of the common failure points we've seen on the failed workloads and share them as best practices in talks like this one.\n\nAll right, so let's get going. The first question is, uh, what is the biggest challenge in scaling generative AI? You may be able to guess the solution here as this is what the talk's all about. Um, but I, I would argue that it's evaluations. In my experience, having been called in to help many different customers scale their GenAI workloads, there's lots of concerns that come up. And when I ask this question with a live audience, there's things like cost, hallucinations, accuracy, capacity, all these different types of um, concerns come up. But the number one thing that I see across all workloads is a lack of evaluations. And in particular, I call it the missing piece to scaling GenAI because a lot of times when a customer has uh, successfully built a PC and they have a workload that they like or chatbot that seems cool and they're figuring out how to scale it, um, this is the piece that I come in and most often it's missing. And when we add evaluations, it unlocks the ability to scale, and we're going to look at how that happens, but it's by far the most common way to unlock scale.\n\nAll right, I'll give you a quick customer example. Um, in uh, July 2024, I was called in as a, as like an escalation on a workload where a customer was doing document processing, and they'd been working on this document processing workload for about, I think, six months, 12 months maybe. Um, and they had six or eight engineers working on it. So, it was a pretty big project at that point. And the uh, the the VP of technology that was in charge of the project called me in. He asked everyone else to leave the room, and he was like, \"Look, this project, we spent a bunch of money on it. Accuracy is at 22%. We're thinking about just cutting the project. We're not getting the results we need. So, I need you to tell me if we can save this thing.\" And uh, essentially, I spent a couple of weeks doing discovery and understanding what was going on in the workload. And the number one thing that I realized was they had zero evaluations, right? They had this big process and end to end they had a single number coming out the end that said, \"Well, it's 22% accurate.\" So what I did is design an evaluations framework, and the results were almost funny because once the evaluation framework was in place and you could see exactly where the problems were, fixing them were trivial, right? Fixing the problems wasn't really the challenge. Increasing accuracy wasn't really the challenge. It was knowing where the problems were and what was causing them. So over the course of the next six months they built the evaluation framework. They fixed uh, many of the issues that came up. They were able to achieve 92% accuracy by January, uh, which 90% was their threshold for launching to production at scale, and they launched and became the single largest document processing workload uh, on AWS in North America at the time. All right. So evaluations are really, really important. What are they? How do we actually build these things? This is the place I like to start with evaluations because many times, especially if you're coming from a traditional AI/ML background, you look at evaluations as a way to measure quality because you're familiar with things like, well, what's your F1 score? What's your precision and recall? And you get a score and it just tells you how well you're doing. And while uh, GenAI or Triv AI evaluations do produce a score, if anything, it's a far third of the top reasons why we do evaluations with GenAI workloads. The main goal with any evaluation framework should be to discover problems. Similar to the workload example that I just shared, if your evaluation framework tells you where the problems are and perhaps even suggests solutions because it includes an element of generative AI reasoning, now we can improve our workloads. And of course, yes, we need to measure quality. That's very important. But I, I start this way because when you think about designing an evaluation framework, if you come in with the mindset that it's going to find errors, you design it in a very different way than if you come in with the mindset that it's just going to measure how well you're doing.\n\nAll right. So, let's double click on what that looks like.\n\nUm, and and before we do that though, I do want to, I, I do want to add a second aside on why I think evaluations are so important.\n\nUm, my team within AWS is a very small specialist team, and so we have to have very, very strict filters on which projects we say yes to and which ones we support. And my experience has been evaluations are the number one filter that separates a science project from a successful project. When I talk to a team and the team says, \"Can you teach me about multi-agent collaboration?\" And I say, \"Great. Let's sit down and spend two hours putting together a gold standard set for our evaluations so that your multi-agent collaboration framework project is successful.\" If the team comes back and says, \"Oh gosh, two hours on eval sounds boring. Can you just give me the toys to play with?\" I know right away that's a science project and it's not going to go anywhere. And science projects are important. They're a fun way to learn. They're just not what goes to scale. On the other hand, for my projects that have been wildly successful, where customers have had 100x return on investment or maybe cut their cost by 10,000% or something like that, those customers when I say, hey, can we spend two hours uh, building an eval framework? They say, why don't we spend four? Right? Evals are so important and we recognize that and this project is so important that we're going to invest the time. So I, I'll as an aside I'll just mention it as my number one filter for deciding which projects are going to be successful and which are worth spending time on.\n\nAll right. So,\n\nevaluative AI world come with baggage, right? And especially if you have a traditional AI/ML background, it can be a little scary, right? You you look at evals and it's free text coming out and you're like, \"Oh my gosh, it's not going to be a specific number. I can't mathematically calculate an exact number every time. Let's panic.\" Right? And it turns out that the human race has been grading and evaluating uh, free text workloads for uh, hundreds of years if not thousands, right? Um, can you imagine if my professor in English assigns me an essay and says uh, write me a three-page essay on the on the emotional meaning of sunshine. And I, I spend an, I spend my hours and I write my essay. I turn it in. The professor says, \"Well, I can't calculate an exact answer to this. I can't calculate your exact source, so I give up. I'm just not going to score it.\" No, of course not. The professor is going to give it a score. And we can evaluate generative AI in the same way that we've been evaluating free text for for decades and generations, right? But the key understanding is if you have a bad professor, they just give you a score. Maybe you've had this experience like me. You've turned in an essay and got an F, right? If you just get a score, you're like, \"Well, that that stinks, but what do I do with it?\" Right? With generative AI, we can go a bit deeper. We can do what the good professors do, which is point out what you did wrong, point out where you can improve, right? This is why evaluations in the term in the in the context of grade school are very, very valuable because you can improve. This is the same for generative AI.\n\nAll right, let me give you another example, another um, unique complexity to the evaluation of generative AI. This is a 2x4, and I gave myself the task of drilling a 1-in hole into the 2x4 all the way through it. And when I talk to a live audience, I ask for a thumbs up, thumbs down. How did I do on this task? Right? Very similar to the types of feedback we often have for generative AI uh, workloads. And often I get mostly thumbs up. There's always that one guy that gives me a thumbs down. Um, and what you're done, what you've done is evaluated my output. But if the methodology I used to to create this hole looks like this, right? Suddenly maybe I didn't do such a good job. Maybe how I got there matters. And the reason is even though I managed to make a good hole this time, if this is my setup, if this is the methodology I used to to achieve it or in the context of generative AI, if this is the reasoning I took to create the hole, then maybe I need to, I need to rethink my system. So let me give you another example in the context of a real customer use case. This is a meteorology company that was creating um, summaries of local weather based on sensor data. So, I've summarized what their prompt looked just so it's easy to read here. But essentially, they have a prompt that gives sensor data like it's raining and it's 40° and it's windy and it's asking for a summary. And if the summary says uh, today it's sunny and bright outside. All of the sudden it's like, hm, something has gone wrong, right? If we evaluate this, the score is zero. You know, this prompt gets an F, but we don't know why, right? But what if we ask the model to explain its reasoning? And this reasoning is it's important to mental health to be happy. So I decided not to talk about the rain. Now that we've looked behind the scenes and we've seen kind of what the model, how the model got there, we suddenly have a lot more insight into how to fix the problem. Right? And you can imagine this in the context and why this is such a big problem in the context of evals. Let's look at the at the opposite opposite example. If we give it data and it says it's sunny and then the response comes out in sunny. Well, we'll say response is 10 out of 10. Awesome. I've built a brilliant prompt and I can go to scale. And this is a danger because what happens is if you didn't eval the reasoning and this was the reasoning. All of a sudden you're like, okay, maybe my prompt isn't working right because even though I got the right answer, I drilled the hole correctly in this one case, right? The reasoning is is uh, not getting me where I need to be. Okay.\n\nThe last um, the last step before I get into my seven habits of generative AI uh, evaluations is what I call prompt decomposition. And although this uh, this technique is not specific to evaluations, it's often done in the context of evaluations because when it comes to GenAI, you only can attach an eval to one prompt. It's difficult to evaluate the first half of a prompt versus the second half of of a prompt, right? I think of it like a multimeter for my electrical engineers out there. You've got your two sensors. You put one sensor at the beginning of a prompt, one sensor at the end of the prompt, and you're sensing what went on in the whole prompt. Many times, if the prompt is very large and it does a lot of complicated things, it's difficult to understand where the error is because you just kind of get like, well, there's something going wrong in this big pile of code or big pile of prompting. And so, um, this is that same weather company. This is an example from them where they were having to write a summary of today's weather. Inside their prompt, they did a lot of different instructions based on what the sensor data was. And one of the things they noticed is that for a portion of their prompt, they actually had it say, okay, well, if the wind speed is like less than five, then it's not very windy, but if it's more than five, then it's windy. And what was happening is in a in the PC stage, that worked great. When they tried to scale out maybe two to three% of the time Claude was coming back and saying, oh, uh, the wind speed is seven, seven is less than five, so it's not windy, right? And just completely messing up the math. So what we did here is a series of prompt decompositions, and if you're interested in it, I've got a blog here you can check out.\n\nUm, the basic idea is take the big prompt and break it into a chaining series of prompts. From an eval standpoint, that means that you can attach eval to each section of the prompt. And what that allows you to do is decide, okay, well, this section is working great. This section isn't. So now I know where to focus my efforts. The other thing it does is allows you to decide, is generative AI even the right tool for that section of the prompt. Right? So in the case of this weather prompt, um, doing a mathematical comparison, is seven larger than five? You don't need GenAI for that. Python is perfect at that. It gets it accurate, perfectly accurate. And so in this series of chaining steps, one step was a Python mathematical comparison. When they made this change, the accuracy went to 100%. And they could tell the accuracy of each step.\n\nAll right, I will briefly mention as well in the context of that uh, prompt decomposition. It's a very, very common pattern, and so I wanted to talk about its impact on evaluations. One of the most common patterns we see is semantic routing. Some kind of query comes in or an input to a workload. And the first\n\n\nThe first step is, well, what kind of task is this?\nIf it's an easy task, go to a small model.\nIf it's a hard task, go to a large model.\nAnd by doing this, um, what I'm doing is using the right model for the job, not dependent on the prompt itself, but depending on the input from the user and the complicatedness, the complication level of the task from the user.\nAnd the interesting piece here is again attaching evals to each step.\nAnd so that means for an eval, semantic router, often the input that you're evaluating as a query and the output is just the number one or the number two in this case.\nAnd what you'll see when you run the eval is often even breaking this down significantly increases your accuracy because you're removing what I call dead space or dead tokens, right?\nIf this was all one big prompt, what you'd see is your instructions, your instructions for your easy stuff, your instructions for your hard stuff, right?\nAnd if an easy query comes in, it only needed those easy instructions.\nThose hard instructions are basically dead space.\nThey weren't necessary, but what they are is extra cost and an opportunity for the model to get confused.\nSo often by breaking it down this way, we see the evaluation showing the accuracy popping up because you're only sending the necessary information for the task.\nAll right, so let's jump into the good stuff.\nI didn't mean to keep you waiting, but here we are.\nThese are the seven most common trends I see across all generative AI workloads that have successfully scaled.\nI will say I have never seen a workload go to scale without evaluations, and most of them also include these seven habits.\nThe first one is fast.\nIt is the case that I have seen evaluations built with the mindset that getting results within a week is okay.\nWhat happens is I make a prompt, I push it to some kind of test site, I email a team to go test it, they test it whenever they feel like, they email me back whenever they feel like, and a week later I get to make one more change to my prompt.\nAnd you can imagine a team that's working on that mindset that they get to have like four changes or eight changes a month versus a team that's managed to build an evaluation framework that operates in seconds and they can make hundreds of changes every day and hundreds of tests every day.\nRight?\nThe pace of innovation, the pace of increasing your accuracy is going to be much, much faster.\nUh, to the point where I don't think I've ever seen a workload scale unless it's gone through many, many iterations.\nAnd so sometimes just knowing that at the outset is helpful for teams.\nUm, rather than somehow you're going to craft this beautiful, perfect prompt your first try and never change it again.\nUh, I will say, and this is what we're going to see an example of in just a minute.\nUm, often my rule of thumb target is 30 seconds to run your evaluation framework.\nAnd you do that by using generative AI as a judge to evaluate, or if the output's numeric, like from a semantic router where it's one, two, three, you're using Python to say, is this, did it output a one?\nYes or no?\nIs one equal one?\nYes.\nOkay.\nGood job.\nUm, the reason I say 30 seconds is often what it means is you spend 10 seconds in parallel sending a 100 test cases for generation.\nI've created a test.\nI've created a prompt template I want to test.\nI run it across my 100 test cases and get results.\n10 seconds.\nThen I take those results and inundate more parallel calls, I judge them.\nSo I take my results, compare them against my gold standard, and I get a 100 judges going per 10 seconds.\nAnd in the final 10 seconds, I take my output from my 100 judges and I summarize it.\nOften that means breaking it down by categories and by what's right and what's wrong.\nAnd so what I see in my output is not 100 generated out responses that now I have to go read through and figure junk out.\nWhat I see in that last 10 seconds is a summary where it says, \"Hey, you know what?\nFor all of your questions, all of your queries that had to do with taxes, when you got it right, here's what the trends were.\nWhen you got it wrong, here's what the trends were.\"\nAnd again, it's going from that mindset of how do I figure out what's going wrong and where the errors are and how to fix them.\nQuantifiable is my second one.\nAll effective frameworks also produce numbers, right?\nAnd I sometimes, depending on the teams, I get a lot of pushback on this because you're right that they're not always the exact same number every time you run the test, right?\nIf I turn in an essay, maybe I turn it in on Thursday and I get a score or I turn it in on Friday.\nFriday, my professor has a few cups of, uh, glasses of wine before grading and so my score is a little better.\nRight?\nThis happens in real life.\nIt happens with AI evals.\nThe way that we deal with it is the same in real life as it is with eval, which is we make it numerous.\nRight?\nEven if one, if there's a little bit of jitter in the score in the quantifiable score that we ask for, uh, if we have enough test cases and we average across those test cases, that jitter goes out, just like in grade school, hopefully you didn't turn in one assignment ever.\nUh, you turn in a bunch of assignments and your final score was the average of all of those.\nThe other reason why numerous is so important is that we want to cover all of our, we want to be diverse and cover all of our, uh, test cases.\nUh, right?\nSo, um, oftentimes when I'm running this as an exercise with customers, even the exercise of building 100 test cases is very valuable for the team to figure out what the scope of the project is because inevitably someone will say, \"Hey, when someone asks about taxes, we can, we can direct them to this site where they can get their W TWS,\" and then someone else will say, \"No, no, no, if they ask about taxes, we can't respond at all because we're not allowed to,\" right?\nAnd the team is debating the product design, nothing to do with GenAI, but it's a good exercise to understand what is the scope of what you want to answer and make sure you have questions outside of what you want to answer so that you can measure that the model is redirecting those that are outside of scope.\nThe next habit is explainable.\nThis is that insight where it's like, don't just look at the outputs, look at how you got there, right?\nLook at how the model is reasoning and in particularly, I said reasoning for generation and scoring.\nLook at how your judge is reasoning as well for, for in some cases where you're asking the judge to do a lot of reasoning because just like you're doing prompt engineering for your, for your prompt for your users, you also need to engineer the judge prompt and make sure the judge is scoring correctly.\nUm, I'll show a small example of that in just a minute.\nBut the general idea is just like a professor grading a paper, right?\nIt's not necessarily that you just say, \"Hey, give me a score,\" right?\nBut the professor has a rubric.\nProfessor says, \"Okay, well, it needed to be three pages long.\nIf it's not three pages long, 100% off.\nIt needed to have five different sources cited.\nYou didn't do that.\nIf you only cited three, well, that's uh, 20% off.\"\nUh, you needed a professional tone.\nEvery time your tone slips out of professional, uh, take 3% off your score, right?\nAnd so you can see that the rules start building up and you give really clear instructions on how the output should be judged and asking your judge to explain its reasoning is often a good way to uh, do prompt engineering for the judge.\nThe next is segmented.\nThis is why I showed those slides on prompt decomposition.\nIn practice, almost all scaled workloads are multiple steps, right?\nThere's very, very few workloads I've ever seen that are a single prompt, right?\nAnd what that means is you need to evaluate each step individually.\nAnd this actually is powerful for a number of reasons.\nNot the least of which is that it's very rare for a workload to be a single model.\nIt's much more appropriate for each step because they're evaluated individually.\nWe evaluate which model is appropriate.\nMany times as semantic router, you want to use something like Nova Micro that's going to give you an instant response.\nIt's very simple and easy.\nIt's going to give you one, two, three.\nUm, and so you can, you can with an evaluation framework prove to yourself the smallest model that's appropriate to use on each step.\nDiverse is what I was mentioning earlier.\nYou want to cover all of your use cases.\nSo sometimes I'll start by listing out what are the use cases in scope and then make sure we have questions applying to all of them.\nI will say the hundred is a rule of thumb.\nSometimes if they're use cases that are like edge cases we don't really care about, I'll put in like three or four examples for that.\nBut the core use cases we want a lot of examples for.\nAnd then the last one I'll mention is traditional.\nUm, and I put this in there because there is a tendency to uh, throw the baby out with a bathwater and say, \"Hey, this is GenAI.\nWe're all going to use GenAI.\nWe're going to GenAI everything.\"\nYou know, the output of your my semantic router is one.\nI'll use GenAI to see if that one is really the number one, right?\nAnd uh, the answer is no.\nThere are a lot of traditional techniques that are very, very powerful.\nUm, obviously if the output is numeric, just use a numeric evaluation.\nUh, for things like rag architecture is one of the most common.\nUm, there's many, many types of uh, database accuracy evaluations, retrieval and precision and F1 scores and all those kinds of things for question answering.\nUm, simple things like measuring cost and latency are still traditional tooling, so I just want to, I put that there to highlight the fact that traditional tooling is still very, very powerful and very important uh, in the context of GI evaluations.\nAll right, I'm going to give a quick example of what this looks like.\nUh, and uh, just visually, we start with that gold standard set.\nAgain, this is where I think the most important uh, use of your time is, is building this.\nYour whole system is going to be pointed towards and designed towards this gold standard set.\nSo if your gold standard set has a bunch of errors, what you've done is designed a whole system that does that creates errors, right?\nUm, so so invest, it's always worth time investing in building a good gold standard set.\nI will, uh, as a brief aside, mention this is a terrible place to use GEI.\nIf you use GEI to create your gold standard set.\nIf the GEI set creates errors, you've built, you build a system that generates the same errors that the GI system has.\nUh, and it can lead to a lot of problems.\nIt can be helpful to generate a what I would call a silver standard set, which is the GI system taking a guess at a gold standard, but it still needs to be reviewed by a human to confirm its accuracy.\nAll right, we take an input from that gold standard set, one of those, we put it into our prompt template, put it into our LLM in order to generate an output.\nWe include the answer and the reasoning.\nThen we take the the matching answer from that gold standard input uh, and we compare it with that generated output into a judge prompt and the judge generates a number and the reasoning behind that number.\nUm, and then we pull the category.\nOften I'll include the category in the gold standard set so that um, we can then as that final step uh, break things down by category and um, and generate a summary for the right and wrong answers uh, for each category.\nAll right, that's it.\nThank you so much.\nI hope this was helpful.\nUh, please feel free to reach out with questions and comments and enjoy your day.\n",
  "dumpedAt": "2025-07-21T18:43:25.766Z"
}