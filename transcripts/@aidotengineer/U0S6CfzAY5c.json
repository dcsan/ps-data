{
  "episodeId": "U0S6CfzAY5c",
  "channelSlug": "@aidotengineer",
  "title": "360Brew: LLM-based Personalized Ranking and Recommendation - Hamed and Maziar, LinkedIn AI",
  "publishedAt": "2025-07-16T17:59:28.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 0.33,
      "duration": 7.479
    },
    {
      "lang": "en",
      "text": "Hi everyone. Very excited to be here and",
      "offset": 14.799,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "I'm Ahmed. This is Mazar. And uh today",
      "offset": 18.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "uh uh we're going to talk about our",
      "offset": 21.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "journey in leveraging large language",
      "offset": 22.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "models for personalization and ranking u",
      "offset": 25.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "and our path to production such a large",
      "offset": 28.64,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "model for uh for LinkedIn use cases.",
      "offset": 30.96,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "Oop",
      "offset": 35.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "uh recommendation ranking and",
      "offset": 38.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "personalization is deeply integrated our",
      "offset": 40.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "daily life. uh when you go to a feed to",
      "offset": 43.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to read an article, when you're looking",
      "offset": 46.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "for a for a job, when you're searching",
      "offset": 47.76,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "for something, when you're buying",
      "offset": 49.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "something online, the the the back end",
      "offset": 50.879,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "uh powered by recommendation system",
      "offset": 54.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "tries to find the the the",
      "offset": 55.84,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "best uh content or best entity based on",
      "offset": 58.399,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "your uh your interest and and relevancy",
      "offset": 61.359,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "to to your uh to to you. Uh however this",
      "offset": 64.32,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "uh this system usually um",
      "offset": 68.08,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "um suffer from some some challenges",
      "offset": 71.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "especially they are they are being being",
      "offset": 74.479,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "trained on a specific task. So they are",
      "offset": 76.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "disjoint optimized um they are usually",
      "offset": 79.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "not leveraging for for leveraging the",
      "offset": 83.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the most advanced architecture they are",
      "offset": 84.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "being rolled out one by one which is",
      "offset": 86.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "very time consuming and and",
      "offset": 88.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "unproductive.",
      "offset": 90,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 91.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so what the question that we are asking",
      "offset": 93.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is that what if we have only one model",
      "offset": 95.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to to solve all the task uh at at at the",
      "offset": 97.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "same time.",
      "offset": 100.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So the mission that we started was to",
      "offset": 103.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "build a large foundation uh model based",
      "offset": 105.6,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "on large language models that understand",
      "offset": 108.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the the holist have a holistic",
      "offset": 111.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "understanding of the user uh journey on",
      "offset": 113.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "LinkedIn platform and can solve all the",
      "offset": 115.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "personalization tasks that that LinkedIn",
      "offset": 118.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "has with just one model. And in addition",
      "offset": 120.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to that we wanted this model to have",
      "offset": 123.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "three three other main characteristics.",
      "offset": 125.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "One, we want this model to have zero",
      "offset": 127.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "shot capability so that when you have a",
      "offset": 129.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "new problem or new surface instead of",
      "offset": 131.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "basically collecting the data, building",
      "offset": 133.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "a new recommendation ranking ranking",
      "offset": 135.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "model and and putting into production",
      "offset": 138.239,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "which is a very time consuming uh",
      "offset": 140.239,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "journey. You can basically leverage this",
      "offset": 142.879,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "model out of the box to solve your task.",
      "offset": 144.879,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "You just basically prompt the model and",
      "offset": 147.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "tell the model that this is the task",
      "offset": 149.68,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "that I want to solve. This is kind of",
      "offset": 150.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "recommendation. This is this is the",
      "offset": 152.239,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "entity. this is a user and what do you",
      "offset": 153.68,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "think about the relevancy between these",
      "offset": 155.599,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "two entities.",
      "offset": 156.879,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Uh the second characteristic that we",
      "offset": 158.879,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "want to have this model to have is to",
      "offset": 160.879,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "leverage in context learning as much as",
      "offset": 162.56,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "possible so that for a cold start users",
      "offset": 164.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "problem for example uh we can leverage",
      "offset": 166.879,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "this model by just giving a very few",
      "offset": 169.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "examples or or just by explaining what",
      "offset": 171.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the user might be interested in and the",
      "offset": 173.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "model can solve that problem for the",
      "offset": 175.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "coldest star users",
      "offset": 176.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and the last one is a following",
      "offset": 178.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "instruction. We want basically give our",
      "offset": 180.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "users and members the ability to uh to",
      "offset": 182.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "tell the model what they're interested",
      "offset": 186.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "in. Like imagine that next time that you",
      "offset": 187.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "go to a LinkedIn feed or uh you can tell",
      "offset": 189.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the model that these are the these are",
      "offset": 192.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the niche these are my niche interest",
      "offset": 194.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and these are the topics that I'm",
      "offset": 196.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "interested to to explore and the model",
      "offset": 198.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "basically the recommendation system",
      "offset": 200.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "start finding the relevant information",
      "offset": 201.92,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "for you and recommend it to you.",
      "offset": 203.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Now Maz will talk about how we build",
      "offset": 206.879,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "this model uh and then I I'll talk about",
      "offset": 208.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "how to serve this model. Okay. So it's",
      "offset": 211.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "done. So let me talk a little bit about",
      "offset": 214.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the uh the brewing part building of the",
      "offset": 217.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "model. So in order to make use of the",
      "offset": 219.2,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "LLMs which is what I think most of you",
      "offset": 221.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "guys are here for is that we need to",
      "offset": 224.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "convert all the information we have",
      "offset": 226.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "about the users and the interactions and",
      "offset": 228.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "everything that they had into prompt and",
      "offset": 230.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "this is what we call the magic of",
      "offset": 232.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "promptification. So we take the uh all",
      "offset": 233.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the information we have about the user",
      "offset": 236.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "history and their profiles and a lot of",
      "offset": 238.239,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "interactions that they have had and we",
      "offset": 240.319,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "turn it into a prompt something like the",
      "offset": 242.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "one on the right hand side here. So",
      "offset": 244.159,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "there's as you can see there's an",
      "offset": 245.84,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "instruction for the model to follow for",
      "offset": 247.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "example what we want the model to do in",
      "offset": 248.959,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this case so that we can actually",
      "offset": 250.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "generalize over different instructions.",
      "offset": 252.159,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "We give some information about the",
      "offset": 254.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "member profile and we have some past for",
      "offset": 255.439,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "example interactions that they have had",
      "offset": 258.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "with the data that we have already shown",
      "offset": 260.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to them. And then the question comes in,",
      "offset": 262,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "what do you think the user is going to",
      "offset": 263.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do with this data or this this new piece",
      "offset": 265.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "of information or this new item that we",
      "offset": 267.68,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "are showing to you? So that's basically",
      "offset": 269.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "how we formalize the problem in order to",
      "offset": 271.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "feed it into an LLM. So obviously I mean",
      "offset": 274,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "if you take one of the LLMs out of the",
      "offset": 277.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "box and try to solve this problem with",
      "offset": 279.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it's going to work a little bit but it's",
      "offset": 281.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "not going to be perfect. So in order to",
      "offset": 282.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "do that we have to train the model. So",
      "offset": 284.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this is actually the pipeline that we",
      "offset": 286.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have for developing the model and making",
      "offset": 288.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it productionize. So as you can see the",
      "offset": 290.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "left hand side we start with the open",
      "offset": 292.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "source model then we do some uh magic of",
      "offset": 294.16,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "upycling to to basically so that we can",
      "offset": 297.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "actually control the size of the model",
      "offset": 299.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and the throughput versus the quality of",
      "offset": 301.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the model and then we have like a few uh",
      "offset": 303.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "blocks of training uh continuous",
      "offset": 306.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "pre-training fine-tuning and in",
      "offset": 308.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "instruction fine-tuning and also",
      "offset": 311.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "alignment and at this point we have this",
      "offset": 312.4,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "large model which is we call blue XL",
      "offset": 315.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "which can think of it as a large model",
      "offset": 317.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "with 150 billion parameters that does",
      "offset": 319.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "really really well and we we have",
      "offset": 322,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "maximized the quality but obviously this",
      "offset": 324.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "model is not going to be able to serve",
      "offset": 326.32,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "online because as you as you know the",
      "offset": 328.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "recommendation systems are very very",
      "offset": 330.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "coopus hungry so from here we go all the",
      "offset": 332.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "way down to try to distill the model so",
      "offset": 335.039,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "maximize the efficiency and we're going",
      "offset": 336.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to talk a little bit about that but",
      "offset": 338.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "basically we go all the way down to",
      "offset": 340.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "let's say 3B model which is actually",
      "offset": 341.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "something that can be productionized but",
      "offset": 343.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "as you can see there are so many",
      "offset": 345.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "different boxes here and in order to",
      "offset": 347.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "make sure that the the the development",
      "offset": 349.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "cycle is actually smooth. We had to do a",
      "offset": 351.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "lot of automation. So, one of the key",
      "offset": 354,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "lessons from here is that you build a",
      "offset": 356.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "lot of automation into this uh into",
      "offset": 358.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "these pipelines in order to make uh ma",
      "offset": 360.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "make the fact that making these models",
      "offset": 362.639,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "is actually very complicated into much",
      "offset": 364.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "easier and more manageable uh situation.",
      "offset": 366.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Uh one big question that might actually",
      "offset": 369.52,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "come up here is that why do you actually",
      "offset": 371.039,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "need the Excel model? And in fact, we",
      "offset": 372.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "did a lot of experimentation to see if",
      "offset": 374.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you can actually get away from with not",
      "offset": 376.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "having the Excel model. Unfortunately,",
      "offset": 378.56,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "that's not actually the case. You have",
      "offset": 379.919,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to first go big and then go small. If",
      "offset": 381.199,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you do try to train the model from",
      "offset": 383.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "scratch with a small model, it doesn't",
      "offset": 385.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "actually work that well. So, in this",
      "offset": 387.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "case, we did this and we showed that the",
      "offset": 389.36,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "distillation is actually something that",
      "offset": 390.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is very important for for the smaller",
      "offset": 392.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "models. But now, let me tell you a",
      "offset": 394.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "little bit about the levers that you can",
      "offset": 396.479,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "use in order to improve these models",
      "offset": 398.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "over time. This is actually something",
      "offset": 399.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that's very important. I mean if you",
      "offset": 401.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "look at all the literature there's a lot",
      "offset": 402.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "about the scaling laws how these models",
      "offset": 404.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually scale with data with compute",
      "offset": 406.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and uh with this and that. So in this",
      "offset": 408.479,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "case we have three different layers and",
      "offset": 410.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "I'm going to talk about the first one is",
      "offset": 412.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "obviously the data scaling. So what if",
      "offset": 414.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we have actually more and more data this",
      "offset": 416.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "is something that we comes up a lot in",
      "offset": 418.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the uh in the recommendation systems we",
      "offset": 419.919,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "actually have a lot of data depending on",
      "offset": 422.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "how much you actually log about the user",
      "offset": 424.479,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "behavior. you might have a lot of data",
      "offset": 426.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that goes back to 6 months, one year or",
      "offset": 428.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "whatever. And in this graph, as as as",
      "offset": 431.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you see, as we increase the amount of",
      "offset": 433.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "data, the performance of the model",
      "offset": 434.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "actually improves and uh uh we hope that",
      "offset": 436.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we can actually improve the model even",
      "offset": 439.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "further with having more and more data",
      "offset": 441.12,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "feed feed into it. Um the another lever",
      "offset": 443.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that you can actually pull in order to",
      "offset": 447.039,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "improve the quality of the model,",
      "offset": 448.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "especially the Excel model, is to",
      "offset": 450.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "increase the size of the model. And in",
      "offset": 452.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "this experiment, we actually did this",
      "offset": 454,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "experiment over mixtural uh",
      "offset": 455.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "architecture. You can see if you go from",
      "offset": 457.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "7B to 8x 22B, the performance of the",
      "offset": 459.599,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "model actually increases and improves.",
      "offset": 462.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And finally, this is another thing that",
      "offset": 465.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "is kind of like I think one of the take",
      "offset": 467.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "take-h home message from here would be",
      "offset": 469.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that the context length actually matters",
      "offset": 470.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "a lot for these kinds of applications",
      "offset": 472.8,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "with the recommendation systems. And the",
      "offset": 474.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "context length actually defines how much",
      "offset": 476.879,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "history from the user you can actually",
      "offset": 478.639,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "give to the model. So in this experiment",
      "offset": 480.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we actually show that if you increase",
      "offset": 482.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the context length by uh feeding more",
      "offset": 483.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "history from the user to the model you",
      "offset": 486.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "can actually improve the uh the",
      "offset": 488.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "performance of the model by feeding more",
      "offset": 490.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and more data to the model. As you can",
      "offset": 492.639,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "see towards the end of this graph the",
      "offset": 495.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "performance actually drops. Uh we don't",
      "offset": 497.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "believe that this is because of the fact",
      "offset": 499.039,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that the context is actually less",
      "offset": 500.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "informative. The problem is that the",
      "offset": 502.479,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "models I mean at least the model that we",
      "offset": 504.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "were using in this experiment doesn't",
      "offset": 505.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "generalize that well to the longer",
      "offset": 507.039,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "context. actually they are I mean if you",
      "offset": 508.479,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "look at most of the literature they tell",
      "offset": 510.639,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that the the performance of the model",
      "offset": 512.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "actually drops if you go beyond some",
      "offset": 514.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "context. So",
      "offset": 516,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "I uh actually I I have to give it back",
      "offset": 519.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to you. Okay. Uh let's uh talk a little",
      "offset": 521.519,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "bit about the uh the results and see if",
      "offset": 524.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we can actually uh deliver on some of",
      "offset": 527.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the promises that I that we had. So one",
      "offset": 530.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of the things that we promised was that",
      "offset": 532.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we can actually improve the performance",
      "offset": 534.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of the model or performance of the behav",
      "offset": 536.16,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the behavior of the system on cold start",
      "offset": 538.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "users. In this case we actually show the",
      "offset": 540.959,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "gap between our model and the production",
      "offset": 543.279,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "models uh on the users that have few uh",
      "offset": 546.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "interactions like for example less than",
      "offset": 550.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "five interactions, less than 100",
      "offset": 552.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "interactions and uh so on. And you as",
      "offset": 553.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you can see the gap between the uh the",
      "offset": 556.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "360 brew model and the production model",
      "offset": 559.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "actually grows uh as the number of",
      "offset": 561.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "interactions decreases. So this actually",
      "offset": 563.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "shows you that having the word knowledge",
      "offset": 566.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "uh that the model uh inserts into this",
      "offset": 568.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "these systems actually improves the the",
      "offset": 571.36,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "quality uh of its uh predictions.",
      "offset": 573.6,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "Finally uh uh we we were uh promising to",
      "offset": 577.839,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "give you some generalization to the new",
      "offset": 582.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "domains meaning that the problems that",
      "offset": 584.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "model has never seen inside its",
      "offset": 586.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "training. And in this graph as I show",
      "offset": 588.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "these are four different tasks and these",
      "offset": 591.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "tasks are completely out of domain. Not",
      "offset": 593.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "no information about that surface the",
      "offset": 596.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "model has seen during the training. But",
      "offset": 598.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "as you can see it can actually uh be on",
      "offset": 600.64,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "par or even beat uh some of the um the",
      "offset": 603.2,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "production models and just to say these",
      "offset": 607.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "production models are specific for that",
      "offset": 610.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "specific task. So they have been trained",
      "offset": 612.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on that task. So this is not actually",
      "offset": 613.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the small feat. So it's actually",
      "offset": 616.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "something that is significant. So as you",
      "offset": 617.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "can see this gives the uh the people who",
      "offset": 619.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "are developing these uh uh develop",
      "offset": 622,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "developing these platforms to roll out",
      "offset": 624.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "uh features and roll out surfaces much",
      "offset": 626.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "more quickly because they can actually",
      "offset": 628.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "use these models to do uh to do uh",
      "offset": 630.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "recommendation for them. And now I give",
      "offset": 633.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it back to Hamemed to talk about",
      "offset": 635.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "serving. So let me walk you through that",
      "offset": 637.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "how can we production such a large model",
      "offset": 639.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in an environment that requires a very",
      "offset": 642.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "high QPS and low latency. Many",
      "offset": 644,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "recommendation systems have tens of",
      "offset": 647.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "thousands of the of the QPS and they",
      "offset": 649.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "also require more less than a second",
      "offset": 652.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like a 500 400 millisecond latency",
      "offset": 653.92,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "at at best. Um",
      "offset": 657.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "there are there are three levers that we",
      "offset": 660.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "can we can pull in in order to make the",
      "offset": 662.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "model more efficient and improve the",
      "offset": 665.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "throughput and and reduce the latency",
      "offset": 666.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for these models. uh specification uh",
      "offset": 668.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "going to this model model and",
      "offset": 671.36,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "quantization",
      "offset": 673.2,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "uh as as Moz",
      "offset": 677.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "explained before uh a smaller models",
      "offset": 681.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "definitely have a better throughput but",
      "offset": 683.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "our recipe is that we need to go big and",
      "offset": 685.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "then go small if you go with a smaller",
      "offset": 687.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "model initially it doesn't have enough",
      "offset": 690,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "capacity it doesn't have enough",
      "offset": 691.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "reasoning power to to to solve the",
      "offset": 692.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "complicated task that we have so we go",
      "offset": 695.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "with a larger model and then we start",
      "offset": 698.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "this 150 billion parameter model and",
      "offset": 700.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "then we start distilling it to the",
      "offset": 702.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "smaller model. And one of the recipe",
      "offset": 704,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "here is that we need to do the",
      "offset": 706.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "distillation step by step and that means",
      "offset": 707.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "that we go with a for example 8B uh 8B",
      "offset": 709.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "model then 3B model and then 1B model.",
      "offset": 713.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "So we slowly decrease the size of the",
      "offset": 715.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "model and we we we distill over and over",
      "offset": 717.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "from the from the from the from the",
      "offset": 720.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "previous model. Um and that recipe shows",
      "offset": 723.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to be much much much more effective",
      "offset": 725.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "rather than basically directly going",
      "offset": 727.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "from 150 billion parameter model to one",
      "offset": 730.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "v parameter model. Uh same thing for",
      "offset": 732.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "pruning. Uh so pruning is a mathemat",
      "offset": 735.279,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "optimiz mathematical optimization",
      "offset": 738.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "problem. You want to either reduce the",
      "offset": 739.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "uh reduce the number of heads in the",
      "offset": 743.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "transformers. You can reduce the number",
      "offset": 746,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "of MLPS. Overall this transform model",
      "offset": 747.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "tends uh proven to be very very",
      "offset": 750.639,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "redundant in terms of keeping the",
      "offset": 752.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "information. So we can start pruning and",
      "offset": 754.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "removing some of these layers uh or or",
      "offset": 756.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "reduce basically the precision for for u",
      "offset": 758.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for the for the for each of the",
      "offset": 761.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "activations and parameters.",
      "offset": 763.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Uh however uh again if if you do the",
      "offset": 766,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "pruning u very aggressively at the",
      "offset": 768.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "beginning your performance would",
      "offset": 771.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "significantly suffer. So the the recipe",
      "offset": 772.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "here is also do the gradual pruning. uh",
      "offset": 775.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we we do we what we we do is that we",
      "offset": 777.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "start pruning the model very small",
      "offset": 780.079,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "pruning to the model. we we we distill",
      "offset": 782.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "uh to the smaller model and we do it",
      "offset": 785.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "over and over again. More more pruning,",
      "offset": 787.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "more distillation, more pruning, more",
      "offset": 789.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "distillation. And as you can see from",
      "offset": 790.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "this plot u doing the gradual pruning uh",
      "offset": 793.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "has um can be as effective as basically",
      "offset": 796.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "no no information loss. Whereas if you",
      "offset": 800.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "just basically do aggressive pruning at",
      "offset": 803.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the beginning, you can have up to 1%",
      "offset": 804.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "reduction in the in the model quality.",
      "offset": 806.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Another",
      "offset": 810.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "lever is is is quantization. Going to",
      "offset": 812.079,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "lower precision uh we are leveraging FP8",
      "offset": 814.639,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "uh for activation model parameters.",
      "offset": 818,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "However, uh doing just FP8 in all the",
      "offset": 820.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "layers uh here's the performance of the",
      "offset": 824.079,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "or the quality of the model",
      "offset": 826.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "significantly. So now basically your",
      "offset": 827.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "tool would be to do mix precision. And",
      "offset": 829.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "one of the important aspect when we",
      "offset": 831.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "comes to ranking recommendations and and",
      "offset": 833.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "overall uh prediction tasks is you want",
      "offset": 835.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the model the the prediction or the",
      "offset": 838.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "probability of output of the model to",
      "offset": 840.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "have a very good precision. So in the LM",
      "offset": 842.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "head at the end of the language model",
      "offset": 844.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "has to be in FP32. If you do it in FP16,",
      "offset": 846.88,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "BF16 or FP8, uh what what happens is",
      "offset": 849.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that the numbers collapse and you don't",
      "offset": 853.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "have a very good calibration on top of",
      "offset": 855.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that and you cannot distinguish between",
      "offset": 857.12,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "different item recommended.",
      "offset": 859.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Uh last part is specification. We can",
      "offset": 862.079,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "specify basically the the attentions the",
      "offset": 865.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "most expensive part of the transformers",
      "offset": 868.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is is attention scores. And we can",
      "offset": 870.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "leverage a specification. Not every item",
      "offset": 873.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "needs to attend to every items. And when",
      "offset": 875.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you know your task when you know that",
      "offset": 877.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "this recommendation these are the items",
      "offset": 878.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that you want to uh in the history you",
      "offset": 880.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "can spearsify and not have every item",
      "offset": 882.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "item to each other. And same same goes",
      "offset": 884.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "with when you are recommending the",
      "offset": 887.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "items. Instead of recommending one item",
      "offset": 888.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you can recommend 50 item 500 item at",
      "offset": 890.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the same time but you want to make sure",
      "offset": 893.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that these items are not attending to",
      "offset": 895.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "each other. So you sparify uh the",
      "offset": 897.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "attention scores uh for the output and",
      "offset": 899.519,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "for the for the query.",
      "offset": 901.92,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "If you put everything together uh we can",
      "offset": 908.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we can see that basically this we can we",
      "offset": 910.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "can have a significant reduction in the",
      "offset": 912.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "latency. What we have done is that in",
      "offset": 915.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "the in in four or five of our release uh",
      "offset": 916.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "uh one release after the other we were",
      "offset": 919.519,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "able to reduce the latency by 7x and at",
      "offset": 921.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "the same time increasing the throughput",
      "offset": 924.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "which is basically the number of queries",
      "offset": 926.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that we can handle by one GPU by 30x. So",
      "offset": 928.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we are improving basically the amount of",
      "offset": 931.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the work work that the GPU is doing. At",
      "offset": 933.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "the same time we are reducing the",
      "offset": 935.76,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "latency that each query is sync.",
      "offset": 936.959,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "Uh these are some of basically technical",
      "offset": 943.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "report and and and papers that we",
      "offset": 945.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "published uh during our journey to share",
      "offset": 948.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "with the community basically a lesson",
      "offset": 950.72,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "learned. Um",
      "offset": 953.12,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "and that's the end of our talk. So we",
      "offset": 957.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "have some time also to answer some",
      "offset": 960.399,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "questions. Thank you.",
      "offset": 962.079,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "Please come to the microphones. Um if",
      "offset": 967.839,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you want to ask the question. Yeah.",
      "offset": 969.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Yeah. Thank you. Great talk. One",
      "offset": 970.959,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "question. How did you measure that it",
      "offset": 972.48,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "doesn't lose generalization power?",
      "offset": 973.759,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "Obviously you've done a lot of",
      "offset": 975.279,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "fine-tuning. Uh and you mentioned it",
      "offset": 976.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "works for four or five tasks instead of",
      "offset": 978.639,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "task specific models. How do you know",
      "offset": 980.8,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "it's going to work for the next five",
      "offset": 982.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "tasks? That's a good question. So we",
      "offset": 983.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have a lot of I mean the answer overall",
      "offset": 986.079,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "is having a very comprehensive",
      "offset": 987.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "benchmarking set. We have something",
      "offset": 989.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "around like 50 to 60 benchmarking. Some",
      "offset": 991.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "of them are internal some of them are",
      "offset": 994.32,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "external. For example we leverage if",
      "offset": 995.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "evad to make sure that the model still",
      "offset": 996.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "follows a very good instruction. Um and",
      "offset": 998.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "as Mia mentioned some of the tasks are",
      "offset": 1002.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "not never being part of our training",
      "offset": 1004.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "data and that's how we are measuring",
      "offset": 1006.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "basically the generalization to the new",
      "offset": 1008.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "new domain within LinkedIn use cases for",
      "offset": 1010.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "example.",
      "offset": 1012.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Hi, thanks thanks for the talk. Um, I'm",
      "offset": 1014.639,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "wondering what a small uh listing",
      "offset": 1017.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "website uh can use out of the box. Um,",
      "offset": 1019.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "have you heard of NL Web which was",
      "offset": 1022.639,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "launched recently by Microsoft? Uh, if",
      "offset": 1024.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "yes, what are your views on that as a",
      "offset": 1026.959,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "recommendation system? NL web. No, I",
      "offset": 1028.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "haven't actually heard the okay. Sorry",
      "offset": 1031.679,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "about that. Anything you for smaller",
      "offset": 1033.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "ones listing, let's say real estate",
      "offset": 1036.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "listing website has like thousands of uh",
      "offset": 1037.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "real estate listings. what are the out",
      "offset": 1040.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "of the box recommendation models that uh",
      "offset": 1042.48,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "people can start using? Uh I mean that's",
      "offset": 1045.039,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "the I uh I wish that that such a model",
      "offset": 1048.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "would exist. I I don't really I mean",
      "offset": 1051.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that's why I think we started this work.",
      "offset": 1053.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "We tried we were trying to see if you",
      "offset": 1055.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "can actually make it a foundation model",
      "offset": 1056.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "so that you can actually solve those",
      "offset": 1058.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "kinds of problems. I think there's a lot",
      "offset": 1059.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of potential for for this to be able to",
      "offset": 1061.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "serve a lot of the use cases that are",
      "offset": 1063.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "beyond the bigger companies but",
      "offset": 1065.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "definitely I don't know any I think you",
      "offset": 1067.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "should check out NL web one. Okay, I'll",
      "offset": 1069.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "look at that. Yeah, thanks.",
      "offset": 1072.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Uh thank you for the great talk. Um uh",
      "offset": 1075.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "on the slide where you mentioned you",
      "offset": 1077.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "have a multi- uh item scoring. Uh I'm",
      "offset": 1079.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "curious like uh what does that uh",
      "offset": 1083.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "effectively mean? Does it mean that you",
      "offset": 1085.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "need to do multi-step decoding or it's",
      "offset": 1086.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "just a one step or just processing the",
      "offset": 1089.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "logits for multiple items? What does it",
      "offset": 1091.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's a multi-step. We don't want to",
      "offset": 1094.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "basically we didn't want to go to the",
      "offset": 1095.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "for example complication of speculative",
      "offset": 1097.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "decoding or basically the decoding",
      "offset": 1100.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "aspect. We wanted to have everything at",
      "offset": 1102.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the preill. Okay. So what we did was",
      "offset": 1103.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that basically all the items are being",
      "offset": 1105.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "sequenced all the recommended items or",
      "offset": 1107.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "potential candidates are sequenced",
      "offset": 1110,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "together but we also wanted to avoid",
      "offset": 1111.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "them to attend to each other. So we",
      "offset": 1114.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "leverage basically what we call it like",
      "offset": 1117.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "for the attention mask uh and we we",
      "offset": 1120.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "develop a special kernel actually in the",
      "offset": 1123.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "SG lang and VLM to to be able to do",
      "offset": 1125.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that. And now when you have up to 500",
      "offset": 1127.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "items in in your query segment, those",
      "offset": 1131.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "items don't attend to each other. They",
      "offset": 1134.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "only attend to the historical user and",
      "offset": 1136.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and and user profile information. Okay,",
      "offset": 1138.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "thank you.",
      "offset": 1141.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "Hey, great talk. Uh so a user history",
      "offset": 1143.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "means many things, right? So like there",
      "offset": 1146.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is all of the jobs that they've applied",
      "offset": 1148.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to or and the job postings. There are so",
      "offset": 1150.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "many entities and so on. Uh the the",
      "offset": 1152.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "context of the model can get quite",
      "offset": 1154.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "large. uh how did you manage that? Did",
      "offset": 1156.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you compress it or were there parts that",
      "offset": 1158.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you focused on? Yeah, so we we actually",
      "offset": 1161.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "experimented with a lot of things. Uh we",
      "offset": 1164.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "experimented with with rack system so",
      "offset": 1166.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that basically when we have a query we",
      "offset": 1168.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "try to figure out what are the most",
      "offset": 1170,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "closest items in the your history to",
      "offset": 1171.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "bring it up. Uh we also experimented",
      "offset": 1173.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "with chronical orders and some sort of",
      "offset": 1176.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "weight decade on the chronical orders.",
      "offset": 1179.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "It turns out that for majority of",
      "offset": 1181.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "application that we have actually",
      "offset": 1183.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "chronical order is good enough and that",
      "offset": 1184.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "kind of makes sense because",
      "offset": 1187.28,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "recommendation systems are very biased",
      "offset": 1188.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to the freshness. So the more recent",
      "offset": 1189.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "user activity helps one of the biggest",
      "offset": 1192.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "challenge is actually the this is more",
      "offset": 1195.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "now become more like a traditional LM",
      "offset": 1198.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "problem. How do you balance the",
      "offset": 1200,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "distribution of your positive and",
      "offset": 1201.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "negative within the context? And I think",
      "offset": 1203.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that's becomes something that more like",
      "offset": 1205.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a ML engineering effort to figure out",
      "offset": 1207.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "okay do I want more positive more",
      "offset": 1209.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "negative like how much how much",
      "offset": 1211.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "information I need to put in the",
      "offset": 1213.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "context. Yeah. Just I can add one more",
      "offset": 1214.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "thing to this. This there's also another",
      "offset": 1217.6,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "complication when you go to the serving",
      "offset": 1219.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of these models. You don't want to break",
      "offset": 1220.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the KV caching or something that you're",
      "offset": 1222.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "using in the serving. So it's going to",
      "offset": 1224.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "be a little bit more complicated, more",
      "offset": 1226,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "cumbersome to do something that's",
      "offset": 1227.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "smarter than just putting the",
      "offset": 1229.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "chronological order. So that's something",
      "offset": 1230.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that needs to be designed. So it's not",
      "offset": 1232.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "something that's very obvious.",
      "offset": 1233.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Absolutely. Uh one more question. Uh you",
      "offset": 1235.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "guys did so many experiments, tried out",
      "offset": 1237.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so many things. How's your entire system",
      "offset": 1239.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "set up? Because I'm assuming that you",
      "offset": 1241.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "say quantization, but you must have",
      "offset": 1243.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "tried different forms of quantization,",
      "offset": 1245.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "whatnot. How do you set up the system in",
      "offset": 1246.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "such a way that you can try out multiple",
      "offset": 1248.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "experiments and see what works best? Can",
      "offset": 1251.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "you talk a bit about that?",
      "offset": 1253.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Uh yes touched a bit on that one. I",
      "offset": 1255.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "think the the the one thing that we we",
      "offset": 1257.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we hold a very high bar for the one was",
      "offset": 1259.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "automation. So our system is very",
      "offset": 1261.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "automated to the extent that when you're",
      "offset": 1263.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "running experimentation actually the",
      "offset": 1266.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "result of the experimental being pushed",
      "offset": 1267.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "automatically into the Excel sheet and",
      "offset": 1269.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "now when you have such an automated",
      "offset": 1272.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "system now basically the developers are",
      "offset": 1274.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "very efficient in terms of like I just",
      "offset": 1276.08,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "want to figure out different",
      "offset": 1277.52,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "quantization. So you just change the",
      "offset": 1278.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "quantization parameters and everything",
      "offset": 1279.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "else just by click button happens end to",
      "offset": 1281.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "end. Uh so I think uh automation is the",
      "offset": 1283.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "key if you want to basically really",
      "offset": 1287.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "optimize for these models. So did you",
      "offset": 1288.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "build all of that automation in house or",
      "offset": 1291.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "did you Yes, most of them I mean we",
      "offset": 1292.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "leverage for example lightning VLSG like",
      "offset": 1294.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "uh we leverage basically a lot of open",
      "offset": 1298,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "source tools but we make sure that they",
      "offset": 1299.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "are integrated very well with each other",
      "offset": 1302.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and uh and optimize basically the entire",
      "offset": 1304.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "flow. Cool. Thank you.",
      "offset": 1306.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "Thank you for Thank you again Maza.",
      "offset": 1308.88,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "Thank you.",
      "offset": 1312.48,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1318.57,
      "duration": 2.24
    }
  ],
  "cleanText": "[Music]\nHi everyone. Very excited to be here.\nI'm Ahmed. This is Maziar. And uh, today, uh, we're going to talk about our journey in leveraging large language models for personalization and ranking, and our path to production such a large model for uh, for LinkedIn use cases.\nOops.\nUh, recommendation, ranking, and personalization is deeply integrated into our daily life. Uh, when you go to a feed to read an article, when you're looking for a job, when you're searching for something, when you're buying something online, the, the, the back end, uh, powered by recommendation systems, tries to find the, the, the best, uh, content or best entity based on your, uh, your interest and and relevancy to, to your, uh, to, to you. Uh, however, this, uh, this system usually, um, um, suffer from some, some challenges, especially they are, they are being, being trained on a specific task. So they are disjoint optimized, um, they are usually not leveraging for, for leveraging the, the most advanced architecture. They are being rolled out one by one, which is very time-consuming and, and unproductive.\n\nUm, so what the question that we are asking is that, what if we have only one model to, to solve all the tasks, uh, at, at the same time?\nSo the mission that we started was to build a large foundation, uh, model based on large language models that understand the, the holistic, have a holistic understanding of the user, uh, journey on LinkedIn platform and can solve all the personalization tasks that, that LinkedIn has with just one model. And in addition to that, we wanted this model to have three, three other main characteristics.\nOne, we want this model to have zero-shot capability so that when you have a new problem or new surface, instead of basically collecting the data, building a new recommendation ranking, ranking model and, and putting into production, which is a very time-consuming, uh, journey, you can basically leverage this model out of the box to solve your task. You just basically prompt the model and tell the model that this is the task that I want to solve. This is kind of recommendation. This is, this is the entity, this is a user, and what do you think about the relevancy between these two entities?\nUh, the second characteristic that we want to have this model to have is to leverage in-context learning as much as possible so that for a cold start users problem, for example, uh, we can leverage this model by just giving a very few examples or, or just by explaining what the user might be interested in, and the model can solve that problem for the coldest star users.\nAnd the last one is a following instruction. We want basically give our users and members the ability to, uh, to tell the model what they're interested in. Like imagine that next time that you go to a LinkedIn feed or, uh, you can tell the model that these are the, these are the niche, these are my niche interest, and these are the topics that I'm interested to, to explore, and the model basically, the recommendation system start finding the relevant information for you and recommend it to you.\nNow, Maziar will talk about how we build this model, uh, and then I, I'll talk about how to serve this model. Okay. So it's done. So let me talk a little bit about the, uh, the brewing part, building of the model. So in order to make use of the LLMs, which is what I think most of you guys are here for, is that we need to convert all the information we have about the users and the interactions and everything that they had into prompts, and this is what we call the magic of promptification. So we take the, uh, all the information we have about the user history and their profiles and a lot of interactions that they have had, and we turn it into a prompt, something like the one on the right-hand side here. So there's, as you can see, there's an instruction for the model to follow, for example, what we want the model to do in this case, so that we can actually generalize over different instructions. We give some information about the member profile, and we have some past, for example, interactions that they have had with the data that we have already shown to them. And then the question comes in, what do you think the user is going to do with this data or this, this new piece of information or this new item that we are showing to you? So that's basically how we formalize the problem in order to feed it into an LLM. So obviously, I mean, if you take one of the LLMs out of the box and try to solve this problem with it, it's going to work a little bit, but it's not going to be perfect. So in order to do that, we have to train the model. So this is actually the pipeline that we have for developing the model and making it productionize. So as you can see, the left-hand side, we start with the open-source model, then we do some, uh, magic of upcycling to, to basically, so that we can actually control the size of the model and the throughput versus the quality of the model, and then we have like a few, uh, blocks of training, uh, continuous pre-training, fine-tuning, and instruction fine-tuning, and also alignment. And at this point, we have this large model, which is we call Blue XL, which can think of it as a large model with 150 billion parameters that does really, really well, and we, we have maximized the quality, but obviously this model is not going to be able to serve online because as you, as you know, the recommendation systems are very, very compute hungry. So from here, we go all the way down to try to distill the model, so maximize the efficiency, and we're going to talk a little bit about that, but basically we go all the way down to, let's say, 3B model, which is actually something that can be productionized. But as you can see, there are so many different boxes here, and in order to make sure that the, the, the development cycle is actually smooth, we had to do a lot of automation. So, one of the key lessons from here is that you build a lot of automation into this, uh, into these pipelines in order to make, uh, ma, make the fact that making these models is actually very complicated into much easier and more manageable, uh, situation.\nUh, one big question that might actually come up here is that why do you actually need the XL model? And in fact, we did a lot of experimentation to see if you can actually get away from with not having the XL model. Unfortunately, that's not actually the case. You have to first go big and then go small. If you do try to train the model from scratch with a small model, it doesn't actually work that well. So, in this case, we did this and we showed that the distillation is actually something that is very important for, for the smaller models. But now, let me tell you a little bit about the levers that you can use in order to improve these models over time. This is actually something that's very important. I mean, if you look at all the literature, there's a lot about the scaling laws, how these models actually scale with data, with compute, and, uh, with this and that. So in this case, we have three different layers, and I'm going to talk about the first one is obviously the data scaling. So what if we have actually more and more data? This is something that we comes up a lot in the, uh, in the recommendation systems. We actually have a lot of data depending on how much you actually log about the user behavior. You might have a lot of data that goes back to six months, one year, or whatever. And in this graph, as, as, as you see, as we increase the amount of data, the performance of the model actually improves, and, uh, uh, we hope that we can actually improve the model even further with having more and more data feed, feed into it. Um, the another lever that you can actually pull in order to improve the quality of the model, especially the XL model, is to increase the size of the model. And in this experiment, we actually did this experiment over mixtural, uh, architecture. You can see if you go from 7B to 8x 22B, the performance of the model actually increases and improves. And finally, this is another thing that is kind of like, I think one of the take, take-home message from here would be that the context length actually matters a lot for these kinds of applications with the recommendation systems. And the context length actually defines how much history from the user you can actually give to the model. So in this experiment, we actually show that if you increase the context length by, uh, feeding more history from the user to the model, you can actually improve the, uh, the performance of the model by feeding more and more data to the model. As you can see towards the end of this graph, the performance actually drops. Uh, we don't believe that this is because of the fact that the context is actually less informative. The problem is that the models, I mean, at least the model that we were using in this experiment, doesn't generalize that well to the longer context. Actually, they are, I mean, if you look at most of the literature, they tell that the, the performance of the model actually drops if you go beyond some context. So,\nI, uh, actually, I, I have to give it back to you. Okay. Uh, let's, uh, talk a little bit about the, uh, the results and see if we can actually, uh, deliver on some of the promises that I, that we had. So one of the things that we promised was that we can actually improve the performance of the model or performance of the behav, the behavior of the system on cold start users. In this case, we actually show the gap between our model and the production models, uh, on the users that have few, uh, interactions, like for example, less than five interactions, less than 100 interactions, and, uh, so on. And you, as you can see, the gap between the, uh, the 360Brew model and the production model actually grows, uh, as the number of interactions decreases. So this actually shows you that having the word knowledge, uh, that the model, uh, inserts into this, these systems actually improves the, the quality, uh, of its, uh, predictions.\nFinally, uh, uh, we, we were, uh, promising to give you some generalization to the new domains, meaning that the problems that model has never seen inside its training. And in this graph, as I show, these are four different tasks, and these tasks are completely out of domain. No information about that surface the model has seen during the training. But as you can see, it can actually, uh, be on par or even beat, uh, some of the, um, the production models, and just to say, these production models are specific for that specific task. So they have been trained on that task. So this is not actually the small feat. So it's actually something that is significant. So as you can see, this gives the, uh, the people who are developing these, uh, uh, develop, developing these platforms to roll out, uh, features and roll out surfaces much more quickly because they can actually use these models to do, uh, to do, uh, recommendation for them. And now I give it back to Hamed Firooz to talk about serving. So let me walk you through that, how can we production such a large model in an environment that requires a very high QPS and low latency. Many recommendation systems have tens of thousands of the, of the QPS, and they also require more less than a second, like a 500, 400 millisecond latency at, at best.\nUm, there are, there are three levers that we can, we can pull in in order to make the model more efficient and improve the throughput and, and reduce the latency for these models: specification, uh, going to this model model, and quantization.\nUh, as, as Maziar explained before, uh, a smaller models definitely have a better throughput, but our recipe is that we need to go big and then go small. If you go with a smaller model initially, it doesn't have enough capacity, it doesn't have enough reasoning power to, to, to solve the complicated task that we have. So we go with a larger model, and then we start this 150 billion parameter model, and then we start distilling it to the smaller model. And one of the recipe here is that we need to do the distillation step by step, and that means that we go with a, for example, 8B, uh, 8B model, then 3B model, and then 1B model. So we slowly decrease the size of the model, and we, we, we distill over and over from the, from the, from the, from the previous model. Um, and that recipe shows to be much, much, much more effective rather than basically directly going from 150 billion parameter model to one v parameter model. Uh, same thing for pruning. Uh, so pruning is a mathemat, optimiz, mathematical optimization problem. You want to either reduce the, uh, reduce the number of heads in the transformers. You can reduce the number of MLPs. Overall, this transform model tends, uh, proven to be very, very redundant in terms of keeping the information. So we can start pruning and removing some of these layers, uh, or, or reduce basically the precision for, for u, for the, for the, for each of the activations and parameters.\nUh, however, uh, again, if, if you do the pruning, u, very aggressively at the beginning, your performance would significantly suffer. So the, the recipe here is also do the gradual pruning. Uh, we, we do, we, what we, we do is that we start pruning the model, very small pruning to the model. We, we, we distill, uh, to the smaller model, and we do it over and over again. More pruning, more distillation, more pruning, more distillation. And as you can see from this plot, u, doing the gradual pruning, uh, has, um, can be as effective as basically no, no information loss. Whereas if you just basically do aggressive pruning at the beginning, you can have up to 1% reduction in the, in the model quality.\nAnother lever is is is quantization. Going to lower precision, uh, we are leveraging FP8, uh, for activation model parameters. However, uh, doing just FP8 in all the layers, uh, here's the performance of the, or the quality of the model significantly. So now basically your tool would be to do mix precision. And one of the important aspect when we comes to ranking recommendations and, and overall, uh, prediction tasks is you want the model, the, the prediction or the probability of output of the model to have a very good precision. So in the LM head at the end of the language model has to be in FP32. If you do it in FP16, BF16 or FP8, uh, what, what happens is that the numbers collapse, and you don't have a very good calibration on top of that, and you cannot distinguish between different item recommended.\nUh, last part is specification. We can specify basically the, the attentions, the most expensive part of the transformers is is attention scores. And we can leverage a specification. Not every item needs to attend to every items. And when you know your task, when you know that this recommendation, these are the items that you want to, uh, in the history, you can spearsify and not have every item item to each other. And same, same goes with when you are recommending the items. Instead of recommending one item, you can recommend 50 item, 500 item at the same time, but you want to make sure that these items are not attending to each other. So you sparify, uh, the attention scores, uh, for the output and for the, for the query.\nIf you put everything together, uh, we can, we can see that basically this, we can, we can have a significant reduction in the latency. What we have done is that in the, in, in four or five of our release, uh, uh, one release after the other, we were able to reduce the latency by 7x and at the same time increasing the throughput, which is basically the number of queries that we can handle by one GPU by 30x. So we are improving basically the amount of the work, work that the GPU is doing. At the same time,\n\n\nWe are reducing the latency that each query is synced.\nUh, these are some of basically technical reports and papers that we published during our journey to share with the community, basically a lesson learned.\nUm, and that's the end of our talk.\nSo we have some time also to answer some questions.\nThank you.\nPlease come to the microphones if you want to ask the question.\nYeah.\nYeah, thank you.\nGreat talk.\nOne question.\nHow did you measure that it doesn't lose generalization power?\nObviously, you've done a lot of fine-tuning.\nUh, and you mentioned it works for four or five tasks instead of task-specific models.\nHow do you know it's going to work for the next five tasks?\nThat's a good question.\nSo we have a lot of, I mean, the answer overall is having a very comprehensive benchmarking set.\nWe have something around like 50 to 60 benchmarking.\nSome of them are internal, some of them are external.\nFor example, we leverage if Evad to make sure that the model still follows a very good instruction.\nUm, and as Mia mentioned, some of the tasks are not never being part of our training data, and that's how we are measuring basically the generalization to the new domain within LinkedIn use cases, for example.\nHi, thanks, thanks for the talk.\nUm, I'm wondering what a small listing website can use out of the box.\nUm, have you heard of NL Web, which was launched recently by Microsoft?\nUh, if yes, what are your views on that as a recommendation system?\nNL web.\nNo, I haven't actually heard the okay.\nSorry about that.\nAnything you for smaller ones listing, let's say real estate listing website has like thousands of uh real estate listings.\nWhat are the out-of-the-box recommendation models that people can start using?\nUh, I mean that's the I, uh, I wish that that such a model would exist.\nI, I don't really, I mean, that's why I think we started this work.\nWe tried, we were trying to see if you can actually make it a foundation model so that you can actually solve those kinds of problems.\nI think there's a lot of potential for this to be able to serve a lot of the use cases that are beyond the bigger companies, but definitely I don't know any.\nI think you should check out NL web one.\nOkay, I'll look at that.\nYeah, thanks.\nUh, thank you for the great talk.\nUm, uh, on the slide where you mentioned you have a multi- uh item scoring.\nUh, I'm curious like, uh, what does that uh effectively mean?\nDoes it mean that you need to do multi-step decoding or it's just a one step or just processing the logits for multiple items?\nWhat does it?\nIt's a multi-step.\nWe don't want to basically, we didn't want to go to the, for example, complication of speculative decoding or basically the decoding aspect.\nWe wanted to have everything at the preill.\nOkay.\nSo what we did was that basically all the items are being sequenced, all the recommended items or potential candidates are sequenced together, but we also wanted to avoid them to attend to each other.\nSo we leverage basically what we call it like for the attention mask, uh, and we, we develop a special kernel actually in the SG lang and VLM to be able to do that.\nAnd now when you have up to 500 items in your query segment, those items don't attend to each other.\nThey only attend to the historical user and and and user profile information.\nOkay, thank you.\nHey, great talk.\nUh, so a user history means many things, right?\nSo like there is all of the jobs that they've applied to or and the job postings.\nThere are so many entities and so on.\nUh, the context of the model can get quite large.\nUh, how did you manage that?\nDid you compress it or were there parts that you focused on?\nYeah, so we, we actually experimented with a lot of things.\nUh, we experimented with with rack system so that basically when we have a query, we try to figure out what are the most closest items in your history to bring it up.\nUh, we also experimented with chronical orders and some sort of weight decade on the chronical orders.\nIt turns out that for majority of application that we have, actually chronical order is good enough, and that kind of makes sense because recommendation systems are very biased to the freshness.\nSo the more recent user activity helps.\nOne of the biggest challenge is actually the, this is more now become more like a traditional LM problem.\nHow do you balance the distribution of your positive and negative within the context?\nAnd I think that's becomes something that more like a ML engineering effort to figure out, okay, do I want more positive, more negative, like how much, how much information I need to put in the context.\nYeah.\nJust I can add one more thing to this.\nThis, there's also another complication when you go to the serving of these models.\nYou don't want to break the KV caching or something that you're using in the serving.\nSo it's going to be a little bit more complicated, more cumbersome to do something that's smarter than just putting the chronological order.\nSo that's something that needs to be designed.\nSo it's not something that's very obvious.\nAbsolutely.\nUh, one more question.\nUh, you guys did so many experiments, tried out so many things.\nHow's your entire system set up?\nBecause I'm assuming that you say quantization, but you must have tried different forms of quantization, whatnot.\nHow do you set up the system in such a way that you can try out multiple experiments and see what works best?\nCan you talk a bit about that?\nUh, yes, touched a bit on that one.\nI think the, the, the one thing that we, we, we hold a very high bar for the one was automation.\nSo our system is very automated to the extent that when you're running experimentation, actually the result of the experimental being pushed automatically into the Excel sheet, and now when you have such an automated system, now basically the developers are very efficient in terms of like, I just want to figure out different quantization.\nSo you just change the quantization parameters and everything else just by click button happens end to end.\nUh, so I think uh, automation is the key if you want to basically really optimize for these models.\nSo did you build all of that automation in house or did you?\nYes, most of them, I mean, we leverage for example, lightning VLSG, like uh, we leverage basically a lot of open source tools, but we make sure that they are integrated very well with each other and uh, and optimize basically the entire flow.\nCool.\nThank you.\nThank you for.\nThank you again, Maza.\nThank you.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.910Z"
}