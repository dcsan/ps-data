{
  "episodeId": "sRpqPgKeXNk",
  "channelSlug": "@aidotengineer",
  "title": "Trends Across the AI Frontier â€” George Cameron, ArtificialAnalysis.ai",
  "publishedAt": "2025-07-08T16:00:06.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1.03,
      "duration": 6.97
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 16.37,
      "duration": 8.35
    },
    {
      "lang": "en",
      "text": "Hi everyone, I'm George, co-founder of",
      "offset": 21.6,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "Artificial Analysis. A quick background",
      "offset": 24.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "to who we are before we dive into",
      "offset": 27.359,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "things.",
      "offset": 29.039,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Do you see that?",
      "offset": 32.399,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "I think my clicker is not working.",
      "offset": 36.88,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "Click.",
      "offset": 40.559,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Oh, there we go. Great. So, a quick",
      "offset": 45.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "background to who we are. We're a",
      "offset": 47.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "leading independent AI benchmarking",
      "offset": 49.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "company. We benchmark a broad spectrum",
      "offset": 51.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "across AI. So, we benchmark models for",
      "offset": 54.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "their intelligence. We benchmark API",
      "offset": 57.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "endpoints for their speed, their cost.",
      "offset": 59.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "We also benchmark uh hardware and all",
      "offset": 62.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "the AI accelerators out there. Uh and we",
      "offset": 65.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "also benchmark a range of modalities,",
      "offset": 68.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "not just language, but also vision,",
      "offset": 70.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "speech, image generation, video",
      "offset": 73.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "generation, and we publish essentially",
      "offset": 76.08,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "nearly all of it uh for free on our",
      "offset": 79.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "website artificialanalysis.ai",
      "offset": 81.759,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "AI whereby we benchmark over 150",
      "offset": 84.159,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "different models uh across a range of",
      "offset": 87.759,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "metrics. We also publish reports many of",
      "offset": 90.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "which are publicly accessible and we",
      "offset": 94.32,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "also have uh a subscription for",
      "offset": 97.2,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "enterprises looking to uh enter uh or",
      "offset": 99.759,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "bring AI to production in their",
      "offset": 104.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "environments um in an efficient uh and",
      "offset": 106.96,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "effective way.",
      "offset": 110.079,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Let's start off with AI progress. Let's",
      "offset": 114.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "set the scene. So, it's been a crazy two",
      "offset": 117.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "years. I think that we've all felt it in",
      "offset": 120.32,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "this room whereby OpenAI uh kicked off",
      "offset": 123.2,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "the race uh with the chat GBT and GBD",
      "offset": 127.92,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "3.5 launch. And since then, it's only",
      "offset": 131.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "gotten more hectic. There's been more",
      "offset": 135.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and more uh model releases by more and",
      "offset": 137.2,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "more labs pushing the AI frontier.",
      "offset": 140.319,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "So the current state now of frontier AI",
      "offset": 146.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "intelligence. I think this will be this",
      "offset": 148.72,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "order of models will be familiar to a",
      "offset": 151.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "lot in this room. O3 is the leader but",
      "offset": 152.879,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "followed closely by 04 mini with",
      "offset": 156.239,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "reasoning mode high. Deepseek R1 the",
      "offset": 158.959,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "release in the last week or two. Rock 3",
      "offset": 162.08,
      "duration": 8.239
    },
    {
      "lang": "en",
      "text": "mini reasoning high Gemini 2.5 Pro",
      "offset": 166,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "Claude 4 opus thinking",
      "offset": 170.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "this benchmark is our artificial",
      "offset": 174,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "analysis intelligence index it's made up",
      "offset": 176,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "of a composite it's a composite index of",
      "offset": 178.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "seven evaluations",
      "offset": 181.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "which we then wait to develop our",
      "offset": 184.64,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "artificial analysis intelligence index",
      "offset": 187.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which just provides a generalist",
      "offset": 189.519,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "perspective on the intelligence of these",
      "offset": 191.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "models.",
      "offset": 193.599,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "We all have an understanding of what",
      "offset": 197.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "frontier AI intelligence is. But what I",
      "offset": 198.879,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "want to explore with you today is that",
      "offset": 202.8,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "there's more than one frontier in AI.",
      "offset": 204.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "There's trade-offs to accessing this",
      "offset": 208.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "intelligence. You shouldn't always use",
      "offset": 210.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the leading most intelligent model. And",
      "offset": 212.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "so what we want to do is we want to",
      "offset": 215.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "explore the different frontiers out",
      "offset": 216.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "there. And as an AI benchmarking",
      "offset": 218.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "company, we're going to bring some",
      "offset": 221.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "numbers to the four to help you reason",
      "offset": 222.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "about this. First, we'll be looking at",
      "offset": 224.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "reasoning models. Next, we'll be looking",
      "offset": 227.519,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "at the open weights frontier. Third, the",
      "offset": 229.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "cost frontier. And lastly, the speed",
      "offset": 233.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "frontier. There's other frontiers out",
      "offset": 235.68,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "there that we benchmark, but we'll focus",
      "offset": 238.4,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "on these key ones today.",
      "offset": 240.879,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "Starting with reasoning models, what",
      "offset": 246.56,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "we've done here is we've taken our",
      "offset": 248.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "intelligence index and looked at that",
      "offset": 251.439,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "relative to the output tokens used to",
      "offset": 254,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "run the intelligence index. So we've",
      "offset": 256.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "measured all of how many tokens each",
      "offset": 259.28,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "model took to run our seven evaluations",
      "offset": 261.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and we've plotted it on this chart and",
      "offset": 265.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "you can see two distinct groups. It's",
      "offset": 267.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "helpful to think about these separately.",
      "offset": 269.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "So non-reasoning models which offer less",
      "offset": 272,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "intelligence but uh require fewer output",
      "offset": 275.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "tokens",
      "offset": 278.639,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and reasoning models which use more",
      "offset": 280.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "output tokens but offer greater",
      "offset": 281.759,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "intelligence",
      "offset": 283.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and the more this is important to look",
      "offset": 285.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "at because more output tokens comes with",
      "offset": 287.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "trade-offs both for request latency as",
      "offset": 290.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "well as cost. We're going to bring some",
      "offset": 293.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "numbers to draw that out and look at the",
      "offset": 295.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "real differences here.",
      "offset": 297.6,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "Starting with output tokens and the",
      "offset": 301.04,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "verbosity of these models, just how",
      "offset": 305.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "yappy these reasoning models are. We can",
      "offset": 307.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "see that there's an order of magnitude",
      "offset": 310.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "difference between reasoning and",
      "offset": 313.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "non-reasoning models. It's not just that",
      "offset": 314.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "feeling, oh, this is taking a long time.",
      "offset": 318.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "It's real. It's an order of magnitude.",
      "offset": 320.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "So between GPT 4.1 it uh it required 7",
      "offset": 322.16,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "million tokens to run our intelligence",
      "offset": 326.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "index evaluations but then 04 mini high",
      "offset": 329.68,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "took 72 million tokens and the yappiest",
      "offset": 332.96,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "of them all Gemini 2.5 uh pro took 130",
      "offset": 335.759,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "million tokens to run our intelligence",
      "offset": 340.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "index",
      "offset": 342.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and as mentioned this has implications",
      "offset": 344,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "for cost as well as N10 latency",
      "offset": 345.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "responsiveness",
      "offset": 348.16,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "So looking at latency, we benchmark the",
      "offset": 352.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "API latency of how long it takes to",
      "offset": 355.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "receive a response when accessing these",
      "offset": 358.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "models via their APIs.",
      "offset": 361.44,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Here we can see that GBD 4.1 on median",
      "offset": 363.919,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "across our requests took 4.7 seconds to",
      "offset": 367.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "return a full response.",
      "offset": 370.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "04 mini high took over 40 seconds,",
      "offset": 373.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "roughly another 10x or order of",
      "offset": 377.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "magnitude increase. This has",
      "offset": 380,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "implications for applications and users",
      "offset": 382.639,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "which require responsiveness even",
      "offset": 386.479,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "enterprise uh kind of chat bots. You",
      "offset": 389.199,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "don't always reach for 03 in chat GPT",
      "offset": 391.44,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "and it and Facebook's done a lot of",
      "offset": 396.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "studies on this where they've looked at",
      "offset": 398.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the for consumer apps where they've",
      "offset": 400.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "looked at uh user drop off by lat uh",
      "offset": 402.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "application latency which clearly",
      "offset": 406,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "demonstrate this. Sorry, do you mind if",
      "offset": 408.08,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "we jump back a slide?",
      "offset": 410.56,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "And it also has uh implications for how",
      "offset": 417.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "we're building. So I think particularly",
      "offset": 421.199,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "with agents whereby 30 uh queries in",
      "offset": 423.039,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "succession is not uncommon.",
      "offset": 427.599,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "It has it's a multiplier effect on the",
      "offset": 430.479,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "latencies uh for your application and",
      "offset": 433.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "how you can build. If you have faster",
      "offset": 436.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "responses, maybe you can make that 30 uh",
      "offset": 438,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "100 queries for instance. And so putting",
      "offset": 440.639,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "numbers to that in terms of agents, 30",
      "offset": 443.759,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "is normal. And so even less than than 04",
      "offset": 446.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "mini, maybe you're at 10 seconds for a",
      "offset": 450,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "reason model. If you're running 30",
      "offset": 452.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "queries, that's 300 seconds that a user",
      "offset": 453.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "might be waiting for a response or an",
      "offset": 457.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "application might be waiting for a",
      "offset": 458.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "response. That's 5 minutes.",
      "offset": 460.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "If with the order of magnitudes that",
      "offset": 463.599,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we're dealing with here, if that 10",
      "offset": 465.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "seconds was 1 second, then those 30",
      "offset": 467.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "queries takes 30 seconds. 30 seconds",
      "offset": 469.199,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "versus 5 minutes impacts what you can",
      "offset": 472.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "build. Think of a contact center uh",
      "offset": 475.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "application that might maybe 30 seconds",
      "offset": 478.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is okay there, but 5 minutes uh",
      "offset": 480.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "definitely not. Who likes waiting on the",
      "offset": 482.24,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "phone uh that long or imaging",
      "offset": 483.68,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "numbers to these trade-offs is really",
      "offset": 496.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "important. I'd encourage everybody to",
      "offset": 498.4,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "measure them.",
      "offset": 500.96,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Next, we're going to move to the open",
      "offset": 505.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "weights.",
      "offset": 506.56,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Around the time of GPT4, there was a",
      "offset": 509.919,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "huge delta in terms of open weights",
      "offset": 512.159,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "intelligence",
      "offset": 515.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "proprietary intelligence.",
      "offset": 517.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Llama 65B or LMA 270B wasn't close to",
      "offset": 520.159,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "the intelligence of GPT4.",
      "offset": 523.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "What I'd like to show here is where we",
      "offset": 526.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "plot our intelligence index by release",
      "offset": 528.959,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "date is that that gap it closed",
      "offset": 531.36,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "until with with great models like",
      "offset": 535.519,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "mixture late time 7 and uh LM45B.",
      "offset": 537.519,
      "duration": 11.241
    },
    {
      "lang": "en",
      "text": "But 01 broke away in late 2024.",
      "offset": 542.48,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "But then of course I think we remember",
      "offset": 549.44,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "Deepseek released V3 I think December 26",
      "offset": 552.399,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "ruined some of my Christmas holiday",
      "offset": 556.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "plans.",
      "offset": 558.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Had to tell my family I I need to go",
      "offset": 560.32,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "read this paper. It's really exciting.",
      "offset": 562.08,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "And then of course R1 in January. The",
      "offset": 566.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "gap between open weights intelligence",
      "offset": 569.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and proprietary model intelligence",
      "offset": 572,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is less than it's ever been,",
      "offset": 575.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "particularly with the recent R1 release",
      "offset": 578,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in the last couple of weeks, which is",
      "offset": 580.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "only a couple of points different in our",
      "offset": 581.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "intelligence index to the leading",
      "offset": 584.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "models.",
      "offset": 587.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "You can't talk about open weights",
      "offset": 589.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "intelligence without talking about",
      "offset": 591.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "China. The leading open weights models",
      "offset": 593.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "across both reasoning models and",
      "offset": 596.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "non-reasoning models are from China",
      "offset": 598.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "based AI labs. Deepse seeks leading in",
      "offset": 601.839,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "both. Alibaba with their Quen 3 series",
      "offset": 604.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "is leading is coming in second in",
      "offset": 608.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "reasoning.",
      "offset": 611.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "But you also have other labs such as",
      "offset": 613.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Meta uh and Nvidia with their Neotron",
      "offset": 615.2,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "fine tunes of Llama coming in close as",
      "offset": 619.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "well.",
      "offset": 622.399,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "Let's look at the cost frontier. This is",
      "offset": 626.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "really important and I think similar to",
      "offset": 628.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "re to uh end latency impacts what you",
      "offset": 630.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "can build. So bringing some numbers",
      "offset": 633.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "here, we can really see these order of",
      "offset": 635.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "magnitudes play out.",
      "offset": 637.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "So 03 cost us $2,000 to run our",
      "offset": 640.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "intelligence index. Techrunch actually",
      "offset": 644.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "wrote an article about how much money we",
      "offset": 646.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "were we were spending on running evals.",
      "offset": 648.64,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "We we didn't want to read it.",
      "offset": 652.64,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "You can see 4.1 a great model. It's 30",
      "offset": 656.959,
      "duration": 8.241
    },
    {
      "lang": "en",
      "text": "times roughly cheaper in terms of the",
      "offset": 661.519,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "cost to run our intelligence index",
      "offset": 665.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "compared to 01 and 4.1 nano over 500",
      "offset": 666.72,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "times cheaper to run our intelligence",
      "offset": 671.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "index than 03.",
      "offset": 673.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "You should think about these when",
      "offset": 676.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "building applications. the kind of cost",
      "offset": 678,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "structure of your application might",
      "offset": 680.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "dictate what you can use here",
      "offset": 682.56,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "and how you use them. Those 30 uh",
      "offset": 685.76,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "sequential uh API calls for your agentic",
      "offset": 688.959,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "application could be uh 500 and still be",
      "offset": 692.079,
      "duration": 8.921
    },
    {
      "lang": "en",
      "text": "cheaper than an 03 query.",
      "offset": 695.2,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "A key point to note here with this cost",
      "offset": 701.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to run intelligence index and why we",
      "offset": 704.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "don't just look at the per token price",
      "offset": 706,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is that and the labs maybe don't want",
      "offset": 708.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you to think this way but you're paying",
      "offset": 711.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "for the cost per token but then you're",
      "offset": 714.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "also paying for how verbose the models",
      "offset": 716.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "are. All the reasoning tokens that are",
      "offset": 719.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "output when these models are in their",
      "offset": 721.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "thinking mode.",
      "offset": 723.519,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you pay for those as output tokens even",
      "offset": 726.079,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "if some of the labs hide them. And so",
      "offset": 728.959,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "you need to think about this and measure",
      "offset": 732.639,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it in your application not and benchmark",
      "offset": 734.56,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "not just by the cost per million tokens",
      "offset": 736.959,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "but also considering how many reasoning",
      "offset": 739.519,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "tokens there are and how verbose these",
      "offset": 742.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "models are. You can see even amongst the",
      "offset": 745.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "non-reasoning models there's big",
      "offset": 747.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "differences between how verbose these",
      "offset": 749.279,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "models are in responses.",
      "offset": 752.24,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "So for instance, ah we'll go to the next",
      "offset": 755.68,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "slide.",
      "offset": 758.399,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Do you mind if we go back one please?",
      "offset": 762.399,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "So what we've done here is we have now",
      "offset": 770.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we're now going to look at the trends in",
      "offset": 773.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "terms of cost. And so what you can see",
      "offset": 775.44,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "here is we've bucketed models by how",
      "offset": 778.56,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "intelligent they are. intelligence uh",
      "offset": 781.519,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "bands, if you will. And what we can see",
      "offset": 785.36,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "here is that accessing GPT4 level of",
      "offset": 788.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "intelligence has fallen over a 100 times",
      "offset": 791.68,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "since mid23.",
      "offset": 795.12,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "This is the case across all quality",
      "offset": 798.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "bands.",
      "offset": 800.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "You can see that even when a new quality",
      "offset": 802,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "band a new frontier is reached 01 mini",
      "offset": 804.399,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "in late 24",
      "offset": 808.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "quickly within only a few months the",
      "offset": 810.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "cost of accessing that level of",
      "offset": 813.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "intelligence h haveved this is moving",
      "offset": 815.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "quickly and so what I would say to you",
      "offset": 817.279,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "is when building applications",
      "offset": 819.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "think about what if cost wasn't a",
      "offset": 822.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "barrier when you're building it's a it's",
      "offset": 825.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "a very important kind of cost uh",
      "offset": 827.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "exercise because it might well",
      "offset": 829.519,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that if you build for a cost structure",
      "offset": 832.399,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that doesn't work now, then maybe in 6",
      "offset": 835.279,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "months time that will be uh possible and",
      "offset": 837.839,
      "duration": 8.041
    },
    {
      "lang": "en",
      "text": "it will be uh feasible.",
      "offset": 841.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "Next, we're going to look at the speed",
      "offset": 849.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "frontier. So this is how quickly you're",
      "offset": 851.12,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "receiving tokens, the output speed,",
      "offset": 854.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "output tokens per second that you're",
      "offset": 857.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "receiving after sending an A API",
      "offset": 860,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "request.",
      "offset": 862,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "This has been increasing and has",
      "offset": 863.92,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "increased dramatically since early 23 as",
      "offset": 865.519,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "well.",
      "offset": 869.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "So similarly, we've because there's a",
      "offset": 871.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "trade-off typically between intelligence",
      "offset": 873.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and speed, we've grouped models into",
      "offset": 875.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "certain buckets. And we can see here",
      "offset": 878,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "that they've all increased in terms of",
      "offset": 880.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "how quickly you can access a level of",
      "offset": 882.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "intelligence.",
      "offset": 885.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "So 40 I believe was around 40 output",
      "offset": 887.6,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "tokens per second. Now you can access",
      "offset": 891.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that was in 2023.",
      "offset": 895.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Who remembers hitting it wasn't a",
      "offset": 898,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "reasoning model hitting enter in chatbt",
      "offset": 900,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and just waiting for it to output",
      "offset": 902.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "especially code which you want to just",
      "offset": 904.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "copy straight into your editor and you",
      "offset": 906.079,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "know hit run see if it works now you can",
      "offset": 907.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "access that level of intelligence at",
      "offset": 911.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "over 300 tokens per second",
      "offset": 913.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that I'll go through it's not the focus",
      "offset": 918,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "of the talk but important to to",
      "offset": 919.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "reference model sparity so we're seeing",
      "offset": 920.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "more",
      "offset": 923.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "mixture of experts models and",
      "offset": 925.279,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "They activate only a proportion of uh",
      "offset": 928.32,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "parameters at inference time less",
      "offset": 932.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "compute per token which means it can go",
      "offset": 935.04,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "faster essentially and were around back",
      "offset": 937.36,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "then but they're getting more and more",
      "offset": 941.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "proportion",
      "offset": 943.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "next",
      "offset": 946.56,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "models are getting more uh intelligent",
      "offset": 948.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "particularly with distillations you know",
      "offset": 951.759,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "8B distillations etc.",
      "offset": 953.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "inference software optimizations like",
      "offset": 957.12,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "flash attention and speculative decoding",
      "offset": 959.44,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "and lastly hardware improvements. So",
      "offset": 964.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "H100 was faster than A100. Now we've",
      "offset": 967.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "recently launched benchmarks of the B200",
      "offset": 970.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "on our artificial analysis website and",
      "offset": 973.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "it's getting over a,000 output tokens a",
      "offset": 975.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "second. Think about that relative to the",
      "offset": 977.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "40 output tokens per second of GPT uh 4",
      "offset": 980,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in 23.",
      "offset": 983.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "There's also specialized uh accelerators",
      "offset": 985.68,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "like Cerebra, Samova, Grock.",
      "offset": 988.399,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "I want to share a house view here to",
      "offset": 994.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "frame things.",
      "offset": 996.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Yes, things are getting more efficient.",
      "offset": 998.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Yes, the cost of accessing the same",
      "offset": 1000.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "level of intelligence is decreasing and",
      "offset": 1002.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "hardware is getting better. We're",
      "offset": 1005.44,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "getting more system output throughput",
      "offset": 1006.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "on our on the chips.",
      "offset": 1009.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "But our view is that demand for compute",
      "offset": 1012.079,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is going to continue to increase.",
      "offset": 1014,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "We're going to see larger models. I mean",
      "offset": 1017.519,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "deepseek it's over 600 billion active uh",
      "offset": 1019.279,
      "duration": 8.121
    },
    {
      "lang": "en",
      "text": "sorry not active total parameters",
      "offset": 1023.12,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "and the demand for more intelligence is",
      "offset": 1028.079,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "insatiable. Reasoning models, as we saw,",
      "offset": 1030.319,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the yappy models, they require more",
      "offset": 1034.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "compute at inference time. And lastly,",
      "offset": 1036.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "agents",
      "offset": 1039.28,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "whereby 20, 30, 100 plus",
      "offset": 1040.959,
      "duration": 8.641
    },
    {
      "lang": "en",
      "text": "uh sequential requests to models is not",
      "offset": 1046.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "uncommon. These actors multiplies on the",
      "offset": 1049.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "demand for compute. And so the house",
      "offset": 1052,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "view playing with these numbers is net",
      "offset": 1053.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "net. We're going to continue to see",
      "offset": 1056.64,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "commute compute demand increase.",
      "offset": 1058.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Thanks everyone.",
      "offset": 1064.24,
      "duration": 3.319
    }
  ],
  "cleanText": "[Music]\n[Music]\nHi everyone, I'm George, co-founder of Artificial Analysis. A quick background to who we are before we dive into things.\nDo you see that?\nI think my clicker is not working.\nClick.\nOh, there we go. Great. So, a quick background to who we are. We're a leading independent AI benchmarking company. We benchmark a broad spectrum across AI. So, we benchmark models for their intelligence. We benchmark API endpoints for their speed, their cost. We also benchmark uh hardware and all the AI accelerators out there. Uh and we also benchmark a range of modalities, not just language, but also vision, speech, image generation, video generation, and we publish essentially nearly all of it uh for free on our website ArtificialAnalysis.ai.\nAI, whereby we benchmark over 150 different models uh across a range of metrics. We also publish reports, many of which are publicly accessible, and we also have uh a subscription for enterprises looking to uh enter uh or bring AI to production in their environments um in an efficient uh and effective way.\nLet's start off with AI progress. Let's set the scene. So, it's been a crazy two years. I think that we've all felt it in this room whereby OpenAI uh kicked off the race uh with the chat GBT and GBD 3.5 launch. And since then, it's only gotten more hectic. There's been more and more uh model releases by more and more developers pushing the AI frontier.\nSo the current state now of frontier AI intelligence. I think this will be this order of models will be familiar to a lot in this room. O3 is the leader but followed closely by 04 mini with reasoning mode high. Deepseek R1, the release in the last week or two. Rock 3 mini reasoning high, Gemini 2.5 Pro, Claude 4 opus thinking.\nThis benchmark is our Artificial Analysis intelligence index. It's made up of a composite. It's a composite index of seven evaluations which we then weight to develop our Artificial Analysis intelligence index, which just provides a generalist perspective on the intelligence of these models.\nWe all have an understanding of what frontier AI intelligence is. But what I want to explore with you today is that there's more than one frontier in AI. There's trade-offs to accessing this intelligence. You shouldn't always use the leading most intelligent model. And so what we want to do is we want to explore the different frontiers out there. And as an AI benchmarking company, we're going to bring some numbers to the four to help you reason about this. First, we'll be looking at reasoning models. Next, we'll be looking at the open weights frontier. Third, the cost frontier. And lastly, the speed frontier. There's other frontiers out there that we benchmark, but we'll focus on these key ones today.\nStarting with reasoning models, what we've done here is we've taken our intelligence index and looked at that relative to the output tokens used to run the intelligence index. So we've measured all of how many tokens each model took to run our seven evaluations and we've plotted it on this chart and you can see two distinct groups. It's helpful to think about these separately. So non-reasoning models which offer less intelligence but uh require fewer output tokens and reasoning models which use more output tokens but offer greater intelligence and the more this is important to look at because more output tokens comes with trade-offs both for request latency as well as cost. We're going to bring some numbers to draw that out and look at the real differences here.\nStarting with output tokens and the verbosity of these models, just how yappy these reasoning models are. We can see that there's an order of magnitude difference between reasoning and non-reasoning models. It's not just that feeling, oh, this is taking a long time. It's real. It's an order of magnitude. So between GPT 4.1, it uh it required 7 million tokens to run our intelligence index evaluations, but then 04 mini high took 72 million tokens, and the yappiest of them all, Gemini 2.5 uh pro took 130 million tokens to run our intelligence index.\nAnd as mentioned, this has implications for cost as well as end latency responsiveness.\nSo looking at latency, we benchmark the API latency of how long it takes to receive a response when accessing these models via their APIs.\nHere we can see that GBD 4.1 on median across our requests took 4.7 seconds to return a full response.\n04 mini high took over 40 seconds, roughly another 10x or order of magnitude increase. This has implications for applications and users which require responsiveness, even enterprise uh kind of chat bots. You don't always reach for 03 in chat GPT. And it and Facebook's done a lot of studies on this where they've looked at the for consumer apps where they've looked at uh user drop off by lat uh application latency which clearly demonstrate this. Sorry, do you mind if we jump back a slide?\nAnd it also has uh implications for how we're building. So I think particularly with agents whereby 30 uh queries in succession is not uncommon.\nIt has it's a multiplier effect on the latencies uh for your application and how you can build. If you have faster responses, maybe you can make that 30 uh 100 queries for instance. And so putting numbers to that in terms of agents, 30 is normal. And so even less than than 04 mini, maybe you're at 10 seconds for a reason model. If you're running 30 queries, that's 300 seconds that a user might be waiting for a response or an application might be waiting for a response. That's 5 minutes.\nIf with the order of magnitudes that we're dealing with here, if that 10 seconds was 1 second, then those 30 queries takes 30 seconds. 30 seconds versus 5 minutes impacts what you can build. Think of a contact center uh application that might maybe 30 seconds is okay there, but 5 minutes uh definitely not. Who likes waiting on the phone uh that long or imaging numbers to these trade-offs is really important. I'd encourage everybody to measure them.\nNext, we're going to move to the open weights.\nAround the time of GPT4, there was a huge delta in terms of open weights intelligence versus proprietary intelligence.\nLlama 65B or LMA 270B wasn't close to the intelligence of GPT4.\nWhat I'd like to show here is where we plot our intelligence index by release date is that that gap it closed until with with great models like mixture late time 7 and uh LM45B.\nBut 01 broke away in late 2024.\nBut then of course I think we remember Deepseek released V3, I think December 26, ruined some of my Christmas holiday plans.\nHad to tell my family I I need to go read this paper. It's really exciting.\nAnd then of course R1 in January. The gap between open weights intelligence and proprietary model intelligence is less than it's ever been, particularly with the recent R1 release in the last couple of weeks, which is only a couple of points different in our intelligence index to the leading models.\nYou can't talk about open weights intelligence without talking about China. The leading open weights models across both reasoning models and non-reasoning models are from China based AI labs. Deepse seeks leading in both. Alibaba with their Quen 3 series is leading is coming in second in reasoning.\nBut you also have other labs such as Meta uh and Nvidia with their Neotron fine tunes of Llama coming in close as well.\nLet's look at the cost frontier. This is really important and I think similar to re to uh end latency impacts what you can build. So bringing some numbers here, we can really see these order of magnitudes play out.\nSo 03 cost us $2,000 to run our intelligence index. Techrunch actually wrote an article about how much money we were we were spending on running evals. We we didn't want to read it.\nYou can see 4.1, a great model. It's 30 times roughly cheaper in terms of the cost to run our intelligence index compared to 01 and 4.1 nano over 500 times cheaper to run our intelligence index than 03.\nYou should think about these when building applications. The kind of cost structure of your application might dictate what you can use here and how you use them. Those 30 uh sequential uh API calls for your agentic application could be uh 500 and still be cheaper than an 03 query.\nA key point to note here with this cost to run intelligence index and why we don't just look at the per token price is that and the labs maybe don't want you to think this way, but you're paying for the cost per token, but then you're also paying for how verbose the models are. All the reasoning tokens that are output when these models are in their thinking mode.\nYou pay for those as output tokens even if some of the labs hide them. And so you need to think about this and measure it in your application not and benchmark not just by the cost per million tokens, but also considering how many reasoning tokens there are and how verbose these models are. You can see even amongst the non-reasoning models there's big differences between how verbose these models are in responses.\nSo for instance, ah, we'll go to the next slide.\nDo you mind if we go back one please?\nSo what we've done here is we have now we're now going to look at the trends in terms of cost. And so what you can see here is we've bucketed models by how intelligent they are. intelligence uh bands, if you will. And what we can see here is that accessing GPT4 level of intelligence has fallen over a 100 times since mid23.\nThis is the case across all quality bands.\nYou can see that even when a new quality band a new frontier is reached 01 mini in late 24, quickly within only a few months the cost of accessing that level of intelligence h haveved. This is moving quickly and so what I would say to you is when building applications, think about what if cost wasn't a barrier when you're building it's a it's a very important kind of cost uh exercise because it might well that if you build for a cost structure that doesn't work now, then maybe in 6 months time that will be uh possible and it will be uh feasible.\nNext, we're going to look at the speed frontier. So this is how quickly you're receiving tokens, the output speed, output tokens per second that you're receiving after sending an A API request.\nThis has been increasing and has increased dramatically since early 23 as well.\nSo similarly, we've because there's a trade-off typically between intelligence and speed, we've grouped models into certain buckets. And we can see here that they've all increased in terms of how quickly you can access a level of intelligence.\nSo 40 I believe was around 40 output tokens per second. Now you can access that was in 2023.\nWho remembers hitting it wasn't a reasoning model hitting enter in chatbt and just waiting for it to output, especially code which you want to just copy straight into your editor and you know hit run see if it works. Now you can access that level of intelligence at over 300 tokens per second.\nThat I'll go through it's not the focus of the talk but important to to reference model sparity so we're seeing more mixture of experts models and They activate only a proportion of uh parameters at inference time less compute per token which means it can go faster essentially and were around back then but they're getting more and more proportion.\nNext, models are getting more uh intelligent, particularly with distillations, you know, 8B distillations, etc.\nInference software optimizations like flash attention and speculative decoding and lastly hardware improvements. So H100 was faster than A100. Now we've recently launched benchmarks of the B200 on our Artificial Analysis website and it's getting over a,000 output tokens a second. Think about that relative to the 40 output tokens per second of GPT uh 4 in 23.\nThere's also specialized uh accelerators like Cerebra, Samova, Grock.\nI want to share a house view here to frame things.\nYes, things are getting more efficient. Yes, the cost of accessing the same level of intelligence is decreasing and hardware is getting better. We're getting more system output throughput on our on the chips.\nBut our view is that demand for compute is going to continue to increase.\nWe're going to see larger models. I mean deepseek, it's over 600 billion active uh sorry not active, total parameters and the demand for more intelligence is insatiable. Reasoning models, as we saw, the yappy models, they require more compute at inference time. And lastly, agents whereby 20, 30, 100 plus uh sequential requests to models is not uncommon. These actors multiplies on the demand for compute. And so the house view playing with these numbers is net net. We're going to continue to see commute compute demand increase.\nThanks everyone.\n",
  "dumpedAt": "2025-07-21T18:43:26.019Z"
}