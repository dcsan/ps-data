{
  "episodeId": "blrovBxxN9o",
  "channelSlug": "@aidotengineer",
  "title": "Turning Fails into Features: Zapier’s Hard-Won Eval Lessons — Rafal Willinski, Vitor Balocco, Zapier",
  "publishedAt": "2025-06-30T17:15:06.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 3.55,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "Um yeah and brief introduction to Zapier",
      "offset": 15.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "agents. Um I believe many of you know",
      "offset": 17.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what Zap year is. This automation",
      "offset": 19.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "software a lot of boxes arrows",
      "offset": 21.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "essentially about automation uh",
      "offset": 23.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "automating your business processes. Um",
      "offset": 25.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "agents is just well more agentic",
      "offset": 28.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "alternative to Zapier. You describe what",
      "offset": 30.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you want. We propose you a bunch of",
      "offset": 33.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tools a trigger and you enable that and",
      "offset": 35.04,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "hopefully we enable your we automate",
      "offset": 37.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "your whole business processes. Um and a",
      "offset": 40.879,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "key lesson that we have after those",
      "offset": 43.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "those two years is that um building good",
      "offset": 45.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "AI agents is hard and building good",
      "offset": 49.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "platform to enable nontechnical people",
      "offset": 51.84,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "to build AI agents is even harder.",
      "offset": 54.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "That's because AI is nondeterministic.",
      "offset": 56.879,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "But on top of that, your users are even",
      "offset": 59.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "more nondeterministic. They are going to",
      "offset": 61.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "use your products in a way that you",
      "offset": 64.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "cannot imagine up front. So um if you",
      "offset": 65.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "think that building agents is not that",
      "offset": 69.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "hard, you probably have this kind of",
      "offset": 71.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "picture in mind. Um you probably",
      "offset": 74.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "stumbled upon this library called",
      "offset": 77.119,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "blankchain. Um you pulled some examples",
      "offset": 79.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "tutorial. Um you tweaked the prompt,",
      "offset": 82.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "pulled a bunch of tools. you chatted",
      "offset": 84.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "with this solution and they thought,",
      "offset": 87.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "well, it's actually kind of working. All",
      "offset": 89.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right, so let's deploy it and let's",
      "offset": 90.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "collect some profit. Um, turns out the",
      "offset": 92.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "reality has a surprising amount of",
      "offset": 96.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "detail and we believe that building",
      "offset": 97.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "probabilistic software is a little bit",
      "offset": 99.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "different than building traditional",
      "offset": 102,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "software. Um, the initial prototype is",
      "offset": 103.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "only a start and after you ship",
      "offset": 106.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "something to your users, your",
      "offset": 108.079,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "responsibility switches to building the",
      "offset": 109.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "data flywheel. So once you start once",
      "offset": 111.439,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "your user starts using your product uh",
      "offset": 114.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you need to collect the feedback you're",
      "offset": 117.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "starting to understand the usage",
      "offset": 118.479,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "patterns the failures so they can then",
      "offset": 120.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "you can build more evolves build an",
      "offset": 122.719,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "understanding of what's failing what are",
      "offset": 125.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the use cases um as you're building more",
      "offset": 126.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "evils and burn features probably your",
      "offset": 129.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "product is getting better so you're",
      "offset": 131.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "getting more users and there are more",
      "offset": 133.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "failures and you have to build more",
      "offset": 135.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "features and on and on and on. So yeah,",
      "offset": 136.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "it form it forms this data flywheel. But",
      "offset": 139.04,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "starting with the first step.",
      "offset": 143.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Okay. Yeah. So starting from the",
      "offset": 145.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "beginning, how do you start collecting",
      "offset": 147.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actionable feedback?",
      "offset": 149.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Backing up for just a second. The first",
      "offset": 151.52,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "step is to make sure you're",
      "offset": 153.04,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "instrumenting your code, right? Which",
      "offset": 153.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you probably already are doing. Whether",
      "offset": 155.2,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "you're using brain trust or something",
      "offset": 157.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "else, they all offer like an easy way to",
      "offset": 158.239,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "get started like just tracing your",
      "offset": 159.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "completion calls. And this is a good",
      "offset": 161.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "start, but actually you also want to",
      "offset": 162.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "make sure that you're recording much",
      "offset": 164.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "more than that in your traces. You want",
      "offset": 165.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to record the tool calls, the errors",
      "offset": 167.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "from those two calls, the pre and",
      "offset": 169.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "postprocessing steps. That way, it will",
      "offset": 170.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "be much easier to debug what went wrong",
      "offset": 172.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "with the run.",
      "offset": 174.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "And you also want to strive to make the",
      "offset": 176.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "run repeatable for eval purposes. So,",
      "offset": 178.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "for instance, if you log data in the",
      "offset": 180,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "same shape as it appears in the runtime,",
      "offset": 181.519,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it makes it much easier to convert it to",
      "offset": 183.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "an evo run later because you can just",
      "offset": 184.879,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "prepopulate the inputs and expected",
      "offset": 187.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "outputs directly from your trace for",
      "offset": 188.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "free. And this is especially useful as",
      "offset": 190.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "well for two calls because if your two",
      "offset": 193.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "call produces any side effects, you",
      "offset": 195.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "probably want to mock those in your",
      "offset": 197.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "emails. So you get all that for free if",
      "offset": 198.239,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you're recording them in a trace.",
      "offset": 200,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Okay, great. So you've instrumented your",
      "offset": 202.879,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "code and you started getting uh all this",
      "offset": 204.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "raw data from your runs. Now it's time",
      "offset": 207.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to figure out what runs to actually pay",
      "offset": 209.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "attention to. Um explicit user feedback",
      "offset": 211.68,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "is feedback is really high signal. So",
      "offset": 214.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that's a good good place to start.",
      "offset": 216.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Unfortunately, not many people actually",
      "offset": 218.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "click those classic thumbs up, thumbs up",
      "offset": 220,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and thumbs down buttons. So, you got to",
      "offset": 221.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "work a bit harder for that feedback. And",
      "offset": 223.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in our experience, this works best when",
      "offset": 225.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you ask for the feedback in the right",
      "offset": 227.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "context. So, you can be a little bit",
      "offset": 229.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "more aggressive about asking for the",
      "offset": 230.48,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "feedback, but you're in the right",
      "offset": 231.76,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "context. You're not bothering the user",
      "offset": 232.959,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "before that. So, for us, one example of",
      "offset": 234.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "this is once an agent finished running,",
      "offset": 236.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "even if it was just a test run, we show",
      "offset": 238.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "a feedback call to action at the bottom,",
      "offset": 240.56,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "right? Did this run do what you",
      "offset": 242,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "expected? Give us the feedback now. Um,",
      "offset": 243.2,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "and this small change actually gave us",
      "offset": 245.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like a really nice bump in feedback",
      "offset": 247.439,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "submissions surprisingly. So, thumbs up",
      "offset": 249.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and thumbs down are a good benchmark, a",
      "offset": 251.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "good baseline, but try to find these",
      "offset": 253.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "critical moments in your users's journey",
      "offset": 255.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "where they'll be most likely to provide",
      "offset": 258,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you that feedback either because they're",
      "offset": 259.6,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "happy and satisfied or because they're",
      "offset": 261.28,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "angry and they want to tell you about",
      "offset": 262.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it. Um, even if you work really hard for",
      "offset": 263.759,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the feedback, explicit feedback is still",
      "offset": 267.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "really rare. Uh, and explicit feedback",
      "offset": 269.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that's detailed and actionable is even",
      "offset": 271.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "harder because people are just not that",
      "offset": 273.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "interested in providing feedback",
      "offset": 274.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "generally.",
      "offset": 276.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "So, you also want to mine user",
      "offset": 278.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "interaction for implicit feedback. And",
      "offset": 279.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the good news is there's actually a lot",
      "offset": 281.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of lowhanging fruit possibilities here.",
      "offset": 283.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Here's an example from our from our app.",
      "offset": 285.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Users can test an agent before they turn",
      "offset": 287.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it on to see if everything's going okay.",
      "offset": 289.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So, if they do turn it on, that's",
      "offset": 291.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually really strong positive implicit",
      "offset": 293.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "feedback, right?",
      "offset": 295.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Copying a model's response is also good",
      "offset": 297.68,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "implicit feedback. Even uh OpenAI is",
      "offset": 299.68,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "doing this for chat GPT.",
      "offset": 301.919,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "And you can also look for implicit",
      "offset": 305.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "signals in the conversation. Here the",
      "offset": 307.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "user is clearly letting us know that",
      "offset": 309.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "they're not happy with the results.",
      "offset": 311.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "Here are telling the agent to stop",
      "offset": 314.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "slinging around which is clearly",
      "offset": 316.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "implicit negative feedback. I think",
      "offset": 317.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "sometimes the user sends a follow-up",
      "offset": 321.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "message that is mostly rehashing what",
      "offset": 323.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "they asked the previous time to see if",
      "offset": 325.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "LM interprets that phrasing better.",
      "offset": 326.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "That's also also good implicit negative",
      "offset": 328,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "feedback. And there's also a surprising",
      "offset": 330.24,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "amount of of cursing.",
      "offset": 332.479,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Uh recently we had a lot of success as",
      "offset": 336.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "well using an LLM to detect and group",
      "offset": 337.919,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "frustrations and we have this weekly",
      "offset": 339.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "report that we post in our Slack. But it",
      "offset": 341.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "took us a lot of tinkering to make sure",
      "offset": 343.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that LLM understood what frustration",
      "offset": 344.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "means in the context of our products.",
      "offset": 346.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "So, I encourage you to try it out, but",
      "offset": 348.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "expect a lot of tinkering.",
      "offset": 350.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "You should also not forget to look at",
      "offset": 352.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "more traditional user metrics, right? Uh",
      "offset": 353.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "there's a lot of stuff in there for you",
      "offset": 356,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "to mine implicit signal, too. So, find",
      "offset": 357.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "what metrics your business cares about",
      "offset": 358.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and figure out how to track them. Then,",
      "offset": 360.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you can distill some signal from that",
      "offset": 362.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "data. You can look for customers, for",
      "offset": 364,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "example, that turned in the last seven",
      "offset": 365.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "days and go look at their last",
      "offset": 367.44,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "interaction with your product before",
      "offset": 368.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "they left and you're likely to find some",
      "offset": 369.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "signal there.",
      "offset": 371.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Okay. So I have raw data but now I'll",
      "offset": 373.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "let the indust I'll let the industry",
      "offset": 376.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "experts speak.",
      "offset": 379.199,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "Why is it starting?",
      "offset": 382.24,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Yeah. Or the beating should continue",
      "offset": 400.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "until everyone looks at their data.",
      "offset": 402.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Um okay, but how actually you're going",
      "offset": 405.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to do that. So um we believe that the",
      "offset": 408,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "first step is to either buy or build LM",
      "offset": 409.84,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "ops software. We do both. Um, you're",
      "offset": 412.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "definitely going to need that to",
      "offset": 415.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "understand your agent runs because one",
      "offset": 417.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "agent run is probably multiple LM calls,",
      "offset": 419.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "multiple database interactions, tool",
      "offset": 421.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "calls, rest calls, whatever. Each one of",
      "offset": 423.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "them can be source of failure and it's",
      "offset": 425.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "really important to piece together this",
      "offset": 427.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "whole story, understand this, you know,",
      "offset": 428.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "what caused this cascading failure. Um",
      "offset": 430.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "yeah, I said we are doing both because I",
      "offset": 435.039,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "believe vip coding your own internal",
      "offset": 436.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "tooling is really really easy right now",
      "offset": 439.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "with cursor and cloud cod and it's going",
      "offset": 441.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "to pay you massive dividends in the",
      "offset": 443.36,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "future um for two reasons. First of all,",
      "offset": 445.199,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it gives you an ability to understand",
      "offset": 448.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "your data in your own specific domain",
      "offset": 449.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "context. Uh and the second of all it",
      "offset": 452.319,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "also you should be also able to create a",
      "offset": 454.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "functionality to turn every single",
      "offset": 457.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "interacting case or every failure into",
      "offset": 459.44,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "an eval with the minimal amount of um",
      "offset": 462.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "fraction. So whenever you see something",
      "offset": 465.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "interesting there should be like a one",
      "offset": 467.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "click to turn it into an eval. It should",
      "offset": 469.039,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "become your instinct.",
      "offset": 471.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Once you understand what's going on on a",
      "offset": 473.599,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "singular run basis you can start",
      "offset": 475.759,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "understanding things at scale. So now",
      "offset": 477.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you can do feedback aggregations,",
      "offset": 479.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "clustering, you can bucket your uh your",
      "offset": 481.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "failure modes, you can bucket your",
      "offset": 484.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "interactions and then you're going to",
      "offset": 485.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "starting to see what kind of tools are",
      "offset": 487.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "failing the most. What kind of",
      "offset": 489.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "interactions are the most problematic",
      "offset": 490.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that's going to almost create for you",
      "offset": 493.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "like a automatic road map. So you'll",
      "offset": 494.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "know where to apply your time and effort",
      "offset": 497.039,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "to improve your product the most. Um",
      "offset": 499.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "doing anything else is going to be a",
      "offset": 501.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "suboptimal strategy. Something that we",
      "offset": 503.199,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "are also experimenting with is using",
      "offset": 506.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "reasoning models to explain the",
      "offset": 507.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "failures. Turns out that if you give",
      "offset": 509.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "them the trace output, input",
      "offset": 511.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "instructions and anything you can find,",
      "offset": 513.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "they are pretty good at finding the root",
      "offset": 516.08,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "cause of a failure. Um, even if they are",
      "offset": 518.399,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "not going to do that, they're probably",
      "offset": 521.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "going to explain you the whole run or",
      "offset": 523.599,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "just direct your attention into",
      "offset": 525.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "something that's really interesting and",
      "offset": 527.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "might help you find the root cause of",
      "offset": 528.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the problem.",
      "offset": 531.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Cool. So now you have a good short list",
      "offset": 533.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of failure modes you want to work on",
      "offset": 535.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "first. It's time to start building out",
      "offset": 536.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "your evals.",
      "offset": 538.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And we realized over time that there are",
      "offset": 540.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "different types of evals and the types",
      "offset": 542.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of evals that we want to build can be",
      "offset": 544.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "placed into this hierarchy that",
      "offset": 545.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "resembles the testing pyramid for those",
      "offset": 547.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of you that know that. Um so with unit",
      "offset": 548.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "test like evals at the base end to end",
      "offset": 551.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "evals or trajectory evals how we like to",
      "offset": 553.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "call them in the middle and the ultimate",
      "offset": 555.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "way of evaluating using AB testing with",
      "offset": 558,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "stage rolls at rollouts at the top. So",
      "offset": 559.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "let's talk a bit about those. Starting",
      "offset": 561.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "with unit test eels, we're just trying",
      "offset": 563.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to predict the n plus1 state from the",
      "offset": 565.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "current state. So these work great when",
      "offset": 567.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you want to do simple assertions, right?",
      "offset": 569.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "For instance, you could check whether",
      "offset": 571.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the next state is a specific tool call",
      "offset": 572.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "or if the two call parameters are",
      "offset": 574.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "correct or if the answer contains a",
      "offset": 576.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "specific keyword or if the agent",
      "offset": 577.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "determined that it was done, all that",
      "offset": 579.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "good stuff. So if you're starting out,",
      "offset": 581.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we recommend focusing on unit test evos",
      "offset": 583.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "first because these are the easiest to",
      "offset": 585.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "add. It helps you build that muscle of",
      "offset": 587.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "looking at your data, spotting problems,",
      "offset": 589.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "creating evals that reproduce them, and",
      "offset": 590.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "then just focusing on fixing them,",
      "offset": 592.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right? Beware though of like turning",
      "offset": 594.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "every positive feedback into an eval. We",
      "offset": 596.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "found that unit test evos are best for",
      "offset": 598.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "hill climbing specific failure modes",
      "offset": 600.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that you spot in your data.",
      "offset": 602.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So now unit test evals are not perfect",
      "offset": 604.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and we realized that ourselves. uh we",
      "offset": 606.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "realized we had overindexed on unit test",
      "offset": 609.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "evos when the new models were coming out",
      "offset": 610.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that were objectively stronger models",
      "offset": 612.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "but we were they were still performing",
      "offset": 615.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "worse in our internal benchmarks which",
      "offset": 616.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "was weird",
      "offset": 618.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "um and because the majority of our evals",
      "offset": 620,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "were so fine grain this made it really",
      "offset": 622,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "hard to see the forest for the trees",
      "offset": 623.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "when benchmarking new new models there",
      "offset": 625.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "was always a lot of noise when we try",
      "offset": 627.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "comparing runs like when you're looking",
      "offset": 628.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "at a single trace it's it's easy to kind",
      "offset": 630,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of go through the trace and understand",
      "offset": 632.16,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "what's happening but when you need to",
      "offset": 633.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "kind of look at it from I don't know how",
      "offset": 634.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "to play it again when you want to look",
      "offset": 637.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "at it uh through an aggregation of many",
      "offset": 638.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "traces then it starts getting difficult",
      "offset": 641.519,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "to understand what's happening why are",
      "offset": 642.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "so many of these passing and some of",
      "offset": 644.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "these are regressing yeah so we realized",
      "offset": 646.399,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that uh maybe machine can help us it",
      "offset": 648.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "turns out in that previous previous",
      "offset": 652.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "video when I was investigating uh one",
      "offset": 653.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "experiment inside brain trust there's a",
      "offset": 656.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "lot of looking at that screen trying to",
      "offset": 657.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "figure out what went wrong and we were",
      "offset": 659.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like hey maybe we can like just give",
      "offset": 660.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this old data to once again a reasoning",
      "offset": 662.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "LLM and compare the models For us, it",
      "offset": 665.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "turns out that with brain trust MCP and",
      "offset": 668.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "reasoning model, you can just ask it to,",
      "offset": 670.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hey, look at this run, look at this run",
      "offset": 672.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and tell me what's actually different",
      "offset": 675.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "about the new model that we are going to",
      "offset": 677.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "deploy. In this case, it was Gemini Pro",
      "offset": 679.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "versus Claude. And what the reasoning",
      "offset": 680.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model found was actually really, really",
      "offset": 683.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "good. It found that cloud is like a",
      "offset": 684.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "decisive exeutor, whereas Gemini is",
      "offset": 686.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "really yapping a lot. It's asking",
      "offset": 690.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "follow-up questions. It needs some",
      "offset": 692.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "positive affirmations, and it's",
      "offset": 693.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "sometimes even hallucinating. uh bad",
      "offset": 695.279,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "JSON structures. So yeah, it helped us a",
      "offset": 697.519,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "lot. It also surfaces a problem with",
      "offset": 700.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "unit test evals a lot which is um",
      "offset": 702.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "different models have different ways of",
      "offset": 705.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "trying to achieve the same goal and unit",
      "offset": 708.079,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "test evals are penalizing different",
      "offset": 711.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "paths. They're like hardcoded to only",
      "offset": 713.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "follow follow one path and yeah our unit",
      "offset": 715.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "test evals were overfitting to our",
      "offset": 718.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "existing models or actually data",
      "offset": 720,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "collecting using that model. So what we",
      "offset": 721.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "started experimenting with is trajectory",
      "offset": 724.56,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "evals. Um yeah instead of grading just",
      "offset": 726.72,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "one iteration of an agent, we let the",
      "offset": 730.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "agent run till the whole uh till the to",
      "offset": 732.639,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "the end state and we are not grading",
      "offset": 736.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just the end state but we are also",
      "offset": 738.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "grading all the tool calls that were",
      "offset": 739.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "made along the way and uh all the",
      "offset": 741.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "artifacts that have been generated along",
      "offset": 744.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the way. Um this can be also paired with",
      "offset": 746.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "LM as a judge. Ver is going to speak",
      "offset": 748.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "about it later.",
      "offset": 750,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Um yeah, but they are not free. I think",
      "offset": 752.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "they have really high return on",
      "offset": 756.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "investment, but they are much harder to",
      "offset": 758,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "set up. Um especially if you are",
      "offset": 760.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "evaluating runs that have tools that",
      "offset": 762.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "cause side effects, right? When you are",
      "offset": 765.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "running an evil, you definitely don't",
      "offset": 767.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "want to send a email on behalf of the",
      "offset": 768.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "customer once again, right? Um so we had",
      "offset": 770.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a fundamental question whether we should",
      "offset": 773.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "de mock environment or not. And we",
      "offset": 775.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "decided that we are not going mock we",
      "offset": 777.36,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "are not going to mock the environment",
      "offset": 779.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "because otherwise you're going to get",
      "offset": 780.639,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "data that is just not reflecting the",
      "offset": 782.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "reality. So what we started doing is",
      "offset": 784.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "just mirroring uh users environment and",
      "offset": 786.56,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "crafting a synthesis uh synthetic copy",
      "offset": 790,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of that. Uh also they are much slower",
      "offset": 793.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "right so they can sometimes take like up",
      "offset": 795.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "to an hour. Um so it's not pretty great.",
      "offset": 797.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "And we're also learning a bit more into",
      "offset": 801.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "LM as a judge. Uh this is when you use",
      "offset": 803.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "an LLM to grade or compare results from",
      "offset": 806.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "your EVOS and it's tempting to lean into",
      "offset": 808.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "them for everything, but you need to",
      "offset": 810.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "make sure that the judge is judging",
      "offset": 812,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "things correctly, which can be",
      "offset": 813.6,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "surprisingly hard. Uh and you also have",
      "offset": 814.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to be careful not to introduce subtle",
      "offset": 816.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "biases, right? Because even small things",
      "offset": 818.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that you might overlook might end up",
      "offset": 819.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "influencing it.",
      "offset": 821.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Lately, we've also been experimenting",
      "offset": 824.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "with this concept of rubrics based",
      "offset": 826.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "scoring. We use an LLM to judge the run,",
      "offset": 828,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but each row in our data set has a",
      "offset": 830.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "different set of rubrics, rubrics that",
      "offset": 832.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "were handcrafted by a human and describe",
      "offset": 834,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "in natural language what specifically",
      "offset": 836,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "about this run should the LM be paying",
      "offset": 838.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "attention to for the score. Uh so one",
      "offset": 840.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "example of this, did the agent react to",
      "offset": 842.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "an unexpected error from the calendar",
      "offset": 844.24,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "API and then try it again.",
      "offset": 845.839,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "So to sum it up, here's our current",
      "offset": 850.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "mental model of the types of evals that",
      "offset": 851.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we built for Zapper agents. We use LM as",
      "offset": 853.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "a judge or rubrics based evals to build",
      "offset": 855.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "a highle overview of your systems",
      "offset": 857.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "capabilities and these are great for",
      "offset": 859.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "benchmarking new models. We use",
      "offset": 861.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "trajectory evoss to capture multi-turn",
      "offset": 863.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "criteria and we use unit test like EVs",
      "offset": 865.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "to debug specific failures. Uh he'll",
      "offset": 867.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "climb them. Uh but beware of overfitting",
      "offset": 869.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "with these.",
      "offset": 871.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Yeah. And a couple of closing thoughts.",
      "offset": 874,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Don't obsess over metrics. Uh remember",
      "offset": 875.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that when a good metrics become a",
      "offset": 878.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "target, it ceases to be a good target.",
      "offset": 879.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Uh so when you're close to achieving",
      "offset": 882,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "100% score on your evil data set, it's",
      "offset": 884.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "not meaning that you're doing good job.",
      "offset": 887.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Actually meaning that your data set is",
      "offset": 889.199,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just not interesting, right? Because we",
      "offset": 890.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "don't have AGI yet. So it's probably not",
      "offset": 892.959,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "true that your model is that good. Um",
      "offset": 896.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "something that we're experimenting with",
      "offset": 899.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "lately is dividing the data set into two",
      "offset": 901.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "pools. uh into the regressions data set",
      "offset": 903.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to make sure that we are making any",
      "offset": 906.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "changes. We are not breaking existing",
      "offset": 907.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "use cases for the customers and also the",
      "offset": 909.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "aspirational data set of things that are",
      "offset": 911.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "extremely hard. For instance, like",
      "offset": 913.6,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "nailing 200 tool calls uh in a row. And",
      "offset": 915.44,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "lastly, um let's take a step back.",
      "offset": 920.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "What's the point of creating evils in",
      "offset": 923.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the first place? Uh your goal isn't to",
      "offset": 924.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "maximize some imaginary number in a",
      "offset": 926.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "lab-like setting. Uh your end goal is",
      "offset": 929.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "user satisfaction. So the ultimate judge",
      "offset": 931.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "um are your users. You shouldn't be",
      "offset": 934.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "optimizing for the biggest scores for",
      "offset": 936.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the evos and completely disregard the",
      "offset": 938.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "vibes. So that's why you think the",
      "offset": 940.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "ultimate verification method is an AB",
      "offset": 941.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "test. Just take a small proportion of",
      "offset": 943.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "your portion of your traffic, let's say",
      "offset": 946.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "5% and route it to the new model, root",
      "offset": 947.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it to the new prompt, monitor the",
      "offset": 950.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "feedback, check your metrics like",
      "offset": 952.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "activation, user retention and so on.",
      "offset": 953.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "Based on that probably you can make the",
      "offset": 956.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "most educated guess uh instead of being",
      "offset": 958.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "in the lab and optimizing this imaginary",
      "offset": 961.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "number.",
      "offset": 964.48,
      "duration": 5.11
    },
    {
      "lang": "en",
      "text": "That's all. Thank you.",
      "offset": 966.399,
      "duration": 9.921
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 969.59,
      "duration": 6.73
    }
  ],
  "cleanText": "[Music]\nUm, yeah, and a brief introduction to Zapier Agents. Um, I believe many of you know what Zapier is. This automation software, a lot of boxes, arrows, essentially about automation, uh, automating your business processes. Um, Agents is just, well, more agentic alternative to Zapier. You describe what you want. We propose you a bunch of tools, a trigger, and you enable that, and hopefully we enable your, we automate your whole business processes. Um, and a key lesson that we have after those, those two years is that, um, building good AI agents is hard, and building a good platform to enable nontechnical people to build AI agents is even harder. That's because AI is nondeterministic. But on top of that, your users are even more nondeterministic. They are going to use your products in a way that you cannot imagine up front. So, um, if you think that building agents is not that hard, you probably have this kind of picture in mind. Um, you probably stumbled upon this library called Blankchain. Um, you pulled some examples, tutorial. Um, you tweaked the prompt, pulled a bunch of tools. You chatted with this solution and they thought, well, it's actually kind of working. All right, so let's deploy it and let's collect some profit. Um, turns out the reality has a surprising amount of detail, and we believe that building probabilistic software is a little bit different than building traditional software. Um, the initial prototype is only a start, and after you ship something to your users, your responsibility switches to building the data flywheel. So once you start, once your user starts using your product, uh, you need to collect the feedback, you're starting to understand the usage patterns, the failures, so they can, then you can build more, evolve, build an understanding of what's failing, what are the use cases, um, as you're building more evils and burn features, probably your product is getting better, so you're getting more users, and there are more failures, and you have to build more features, and on and on and on. So yeah, it form, it forms this data flywheel. But starting with the first step.\n\nOkay. Yeah. So starting from the beginning, how do you start collecting actionable feedback?\n\nBacking up for just a second. The first step is to make sure you're instrumenting your code, right? Which you probably already are doing. Whether you're using Brain Trust or something else, they all offer like an easy way to get started, like just tracing your completion calls. And this is a good start, but actually, you also want to make sure that you're recording much more than that in your traces. You want to record the tool calls, the errors from those two calls, the pre and post-processing steps. That way, it will be much easier to debug what went wrong with the run.\n\nAnd you also want to strive to make the run repeatable for eval purposes. So, for instance, if you log data in the same shape as it appears in the runtime, it makes it much easier to convert it to an evo run later because you can just prepopulate the inputs and expected outputs directly from your trace for free. And this is especially useful as well for two calls because if your two call produces any side effects, you probably want to mock those in your emails. So you get all that for free if you're recording them in a trace.\n\nOkay, great. So you've instrumented your code and you started getting, uh, all this raw data from your runs. Now it's time to figure out what runs to actually pay attention to. Um, explicit user feedback is feedback is really high signal. So that's a good, good place to start. Unfortunately, not many people actually click those classic thumbs up, thumbs up and thumbs down buttons. So, you got to work a bit harder for that feedback. And in our experience, this works best when you ask for the feedback in the right context. So, you can be a little bit more aggressive about asking for the feedback, but you're in the right context. You're not bothering the user before that. So, for us, one example of this is once an agent finished running, even if it was just a test run, we show a feedback call to action at the bottom, right? Did this run do what you expected? Give us the feedback now. Um, and this small change actually gave us like a really nice bump in feedback submissions, surprisingly. So, thumbs up and thumbs down are a good benchmark, a good baseline, but try to find these critical moments in your users's journey where they'll be most likely to provide you that feedback, either because they're happy and satisfied or because they're angry and they want to tell you about it. Um, even if you work really hard for the feedback, explicit feedback is still really rare. Uh, and explicit feedback that's detailed and actionable is even harder because people are just not that interested in providing feedback generally.\n\nSo, you also want to mine user interaction for implicit feedback. And the good news is there's actually a lot of low-hanging fruit possibilities here. Here's an example from our, from our app. Users can test an agent before they turn it on to see if everything's going okay. So, if they do turn it on, that's actually really strong positive implicit feedback, right?\n\nCopying a model's response is also good implicit feedback. Even, uh, OpenAI is doing this for chat GPT.\n\nAnd you can also look for implicit signals in the conversation. Here the user is clearly letting us know that they're not happy with the results.\n\nHere are telling the agent to stop slinging around, which is clearly implicit negative feedback. I think sometimes the user sends a follow-up message that is mostly rehashing what they asked the previous time to see if LM interprets that phrasing better. That's also, also good implicit negative feedback. And there's also a surprising amount of cursing.\n\nUh, recently we had a lot of success as well using an LLM to detect and group frustrations, and we have this weekly report that we post in our Slack. But it took us a lot of tinkering to make sure that LLM understood what frustration means in the context of our products. So, I encourage you to try it out, but expect a lot of tinkering.\n\nYou should also not forget to look at more traditional user metrics, right? Uh, there's a lot of stuff in there for you to mine implicit signal, too. So, find what metrics your business cares about and figure out how to track them. Then, you can distill some signal from that data. You can look for customers, for example, that turned in the last seven days and go look at their last interaction with your product before they left, and you're likely to find some signal there.\n\nOkay. So I have raw data, but now I'll let the industry experts speak.\n\nWhy is it starting?\n\nYeah. Or the beating should continue until everyone looks at their data.\n\nUm, okay, but how actually you're going to do that. So, um, we believe that the first step is to either buy or build LM ops software. We do both. Um, you're definitely going to need that to understand your agent runs because one agent run is probably multiple LM calls, multiple database interactions, tool calls, rest calls, whatever. Each one of them can be source of failure, and it's really important to piece together this whole story, understand this, you know, what caused this cascading failure. Um, yeah, I said we are doing both because I believe VIP coding your own internal tooling is really, really easy right now with cursor and cloud cod, and it's going to pay you massive dividends in the future, um, for two reasons. First of all, it gives you an ability to understand your data in your own specific domain context. Uh, and the second of all, it also, you should be also able to create a functionality to turn every single interacting case or every failure into an eval with the minimal amount of, um, fraction. So whenever you see something interesting, there should be like a one click to turn it into an eval. It should become your instinct.\n\nOnce you understand what's going on on a singular run basis, you can start understanding things at scale. So now you can do feedback aggregations, clustering, you can bucket your, uh, your failure modes, you can bucket your interactions, and then you're going to starting to see what kind of tools are failing the most. What kind of interactions are the most problematic? That's going to almost create for you like an automatic road map. So you'll know where to apply your time and effort to improve your product the most. Um, doing anything else is going to be a suboptimal strategy. Something that we are also experimenting with is using reasoning models to explain the failures. Turns out that if you give them the trace output, input instructions, and anything you can find, they are pretty good at finding the root cause of a failure. Um, even if they are not going to do that, they're probably going to explain you the whole run or just direct your attention into something that's really interesting and might help you find the root cause of the problem.\n\nCool. So now you have a good short list of failure modes you want to work on first. It's time to start building out your evals.\n\nAnd we realized over time that there are different types of evals, and the types of evals that we want to build can be placed into this hierarchy that resembles the testing pyramid for those of you that know that. Um, so with unit test like evals at the base, end to end evals or trajectory evals, how we like to call them in the middle, and the ultimate way of evaluating using AB testing with stage rolls at rollouts at the top. So let's talk a bit about those. Starting with unit test evos, we're just trying to predict the n plus one state from the current state. So these work great when you want to do simple assertions, right? For instance, you could check whether the next state is a specific tool call or if the two call parameters are correct or if the answer contains a specific keyword or if the agent determined that it was done, all that good stuff. So if you're starting out, we recommend focusing on unit test evos first because these are the easiest to add. It helps you build that muscle of looking at your data, spotting problems, creating evals that reproduce them, and then just focusing on fixing them, right? Beware though of like turning every positive feedback into an eval. We found that unit test evos are best for hill climbing specific failure modes that you spot in your data.\n\nSo now unit test evals are not perfect, and we realized that ourselves. Uh, we realized we had overindexed on unit test evos when the new models were coming out that were objectively stronger models, but we were, they were still performing worse in our internal benchmarks, which was weird.\n\nUm, and because the majority of our evals were so fine grain, this made it really hard to see the forest for the trees when benchmarking new, new models. There was always a lot of noise when we try comparing runs, like when you're looking at a single trace, it's, it's easy to kind of go through the trace and understand what's happening, but when you need to kind of look at it from, I don't know how to play it again, when you want to look at it, uh, through an aggregation of many traces, then it starts getting difficult to understand what's happening, why are so many of these passing and some of these are regressing. Yeah, so we realized that, uh, maybe machine can help us. It turns out in that previous, previous video when I was investigating, uh, one experiment inside Brain Trust, there's a lot of looking at that screen, trying to figure out what went wrong, and we were like, hey, maybe we can like just give this old data to once again a reasoning LLM and compare the models. For us, it turns out that with Brain Trust MCP and reasoning model, you can just ask it to, hey, look at this run, look at this run and tell me what's actually different about the new model that we are going to deploy. In this case, it was Gemini Pro versus Claude. And what the reasoning model found was actually really, really good. It found that Claude is like a decisive executor, whereas Gemini is really yapping a lot. It's asking follow-up questions. It needs some positive affirmations, and it's sometimes even hallucinating, uh, bad JSON structures. So yeah, it helped us a lot. It also surfaces a problem with unit test evals a lot, which is, um, different models have different ways of trying to achieve the same goal, and unit test evals are penalizing different paths. They're like hardcoded to only follow, follow one path, and yeah, our unit test evals were overfitting to our existing models or actually data collecting using that model. So what we started experimenting with is trajectory evals. Um, yeah, instead of grading just one iteration of an agent, we let the agent run till the whole, uh, till the to the end state, and we are not grading just the end state, but we are also grading all the tool calls that were made along the way and, uh, all the artifacts that have been generated along the way. Um, this can be also paired with LM as a judge. Ver is going to speak about it later.\n\nUm, yeah, but they are not free. I think they have really high return on investment, but they are much harder to set up. Um, especially if you are evaluating runs that have tools that cause side effects, right? When you are running an evil, you definitely don't want to send an email on behalf of the customer once again, right? Um, so we had a fundamental question whether we should de mock environment or not. And we decided that we are not going mock, we are not going to mock the environment because otherwise you're going to get data that is just not reflecting the reality. So what we started doing is just mirroring, uh, users environment and crafting a synthesis, uh, synthetic copy of that. Uh, also they are much slower, right? So they can sometimes take like up to an hour. Um, so it's not pretty great.\n\nAnd we're also learning a bit more into LM as a judge. Uh, this is when you use an LLM to grade or compare results from your EVOS, and it's tempting to lean into them for everything, but you need to make sure that the judge is judging things correctly, which can be surprisingly hard. Uh, and you also have to be careful not to introduce subtle biases, right? Because even small things that you might overlook might end up influencing it.\n\nLately, we've also been experimenting with this concept of rubrics based scoring. We use an LLM to judge the run, but each row in our data set has a different set of rubrics, rubrics that were handcrafted by a human and describe in natural language what specifically about this run should the LM be paying attention to for the score. Uh, so one example of this, did the agent react to an unexpected error from the calendar API and then try it again.\n\nSo to sum it up, here's our current mental model of the types of evals that we built for Zapier Agents. We use LM as a judge or rubrics based evals to build a highle overview of your systems capabilities, and these are great for benchmarking new models. We use trajectory evoss to capture multi-turn criteria, and we use unit test like EVs to debug specific failures. Uh, he'll climb them. Uh, but beware of overfitting with these.\n\nYeah. And a couple\n\n\nOf closing thoughts.\n\nDon't obsess over metrics. Remember that when good metrics become a target, it ceases to be a good target. So when you're close to achieving a 100% score on your evil data set, it's not meaning that you're doing a good job. Actually meaning that your data set is just not interesting, right? Because we don't have AI yet. So it's probably not true that your model is that good.\n\nSomething that we're experimenting with lately is dividing the data set into two pools: into the regressions data set to make sure that we are making any changes. We are not breaking existing use cases for the customers and also the aspirational data set of things that are extremely hard. For instance, like nailing 200 tool calls in a row.\n\nLastly, let's take a step back. What's the point of creating evils in the first place? Your goal isn't to maximize some imaginary number in a lab-like setting. Your end goal is user satisfaction. So the ultimate judge are your users. You shouldn't be optimizing for the biggest scores for the evos and completely disregard the vibes. So that's why you think the ultimate verification method is an AB test. Just take a small proportion of your portion of your traffic, let's say 5%, and route it to the new model, route it to the new prompt, monitor the feedback, check your metrics like activation, user retention, and so on. Based on that, probably you can make the most educated guess instead of being in the lab and optimizing this imaginary number.\n\nThat's all. Thank you.\n\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:26.501Z"
}