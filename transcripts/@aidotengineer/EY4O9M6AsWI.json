{
  "episodeId": "EY4O9M6AsWI",
  "channelSlug": "@aidotengineer",
  "title": "General Intelligence is Multimodal â€” Keegan McCallum, Luma AI",
  "publishedAt": "2025-07-19T17:45:06.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 3.54,
      "duration": 5.11
    },
    {
      "lang": "en",
      "text": "So, it's 9:00 a.m. June 11th, 2024,",
      "offset": 15.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and we send out the announcement and",
      "offset": 19.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "hold our breath waiting uh to see the",
      "offset": 22,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "user signups pour in. We're expecting",
      "offset": 24.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "significant traffic for the launch of",
      "offset": 27.119,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Dream Machine, Luma's first video model,",
      "offset": 29.199,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "and we were woefully unprepared for what",
      "offset": 32.719,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "came next. Uh, we'd allocated about 500",
      "offset": 36.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "H100 GPUs. We thought that was a lot at",
      "offset": 38.559,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the time. It wasn't. Um,",
      "offset": 40.559,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "and uh, over the next hour, we saw",
      "offset": 44.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "request after request pour in and a",
      "offset": 47.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "giant queue of requests start to pile",
      "offset": 49.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "up. Luckily, we had a contingency plan",
      "offset": 52.32,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "for this um which was to take every GPU",
      "offset": 55.44,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "we had access to from every provider and",
      "offset": 60.16,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "start manually running SSH commands",
      "offset": 63.28,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "against them to spin up workers uh",
      "offset": 65.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "pulling in work from a global queue. Um",
      "offset": 68.479,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "and we were able to get to about 5,000",
      "offset": 71.52,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "H100s over the next six hours.",
      "offset": 74.96,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "and our cues of almost 100,000 uh",
      "offset": 78.479,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "finally started to drain around like 2",
      "offset": 82.479,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "p.m. that day.",
      "offset": 84.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And that is when Emit, our CEO and",
      "offset": 87.2,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "self-appointed chaos monkey uh decided",
      "offset": 90.4,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "to tweet that we had scaled up by 10x.",
      "offset": 93.439,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "Uh come on, come on in. It'll be faster",
      "offset": 97.28,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "now.",
      "offset": 100.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "And we'll see how it goes. Um, so we",
      "offset": 102.479,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "were our cues were down to about, you",
      "offset": 105.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "know, 300 at this point. And you can",
      "offset": 106.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "kind of see me here freaking out a",
      "offset": 110,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "little bit going, okay, about 10 minutes",
      "offset": 111.84,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "after the tweet goes out, uh, they start",
      "offset": 114,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to go up again, even though we've scaled",
      "offset": 116.079,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "up by 10 times. So now we're at 350,",
      "offset": 117.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we're at 400, we're at,400.",
      "offset": 119.759,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "And as this is going on, um, we're",
      "offset": 123.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "basically taking uh the entire training",
      "offset": 125.6,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "cluster. This is the only uh the only",
      "offset": 128,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "GPUs we hadn't used yet. uh taking the",
      "offset": 129.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "training cluster over uh it's another",
      "offset": 132.56,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "like 4,000 uh H100 GPUs and uh it barely",
      "offset": 134.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "made a dent. The cues were still going",
      "offset": 139.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "up. Uh so you'll see this uh it's it's",
      "offset": 141.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the KEK W emoji which has become a",
      "offset": 144.16,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "cultural staple of Luma over over time",
      "offset": 147.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and it was very much the mood at the",
      "offset": 150.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "time like what am I supposed to do with",
      "offset": 152.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this?",
      "offset": 154.319,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So uh I'm here to talk to you guys today",
      "offset": 156.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "a little bit about Luma. um who we are,",
      "offset": 158.879,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "what we do, and how we've managed to",
      "offset": 162.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "kind of scale our infrastructure up um",
      "offset": 164.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "to initially a million users in 4 days,",
      "offset": 166.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "which was pretty nuts. Uh for some",
      "offset": 169.28,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "context, chat GBT hit a million users in",
      "offset": 171.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "5 days. Um and we processed about half a",
      "offset": 173.519,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "million videos over the course of this",
      "offset": 176.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "12 hours. Um so yeah, we're going to",
      "offset": 178.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "talk a little bit about what we learned",
      "offset": 180.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "scaling up and you know, how we did it.",
      "offset": 182,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Um but first, very quick little intro to",
      "offset": 184.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Luma if you're not familiar with us. So,",
      "offset": 187.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we're not just a video model company.",
      "offset": 189.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "We're a foundation model lab. And we're",
      "offset": 191.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "aiming to build general multimodal",
      "offset": 192.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "intelligence that can, you know,",
      "offset": 195.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "generate, understand, and operate in the",
      "offset": 196.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "physical world just like a human can.",
      "offset": 198.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And uh to give you some sense of kind of",
      "offset": 201.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "what our models can do and where we're",
      "offset": 203.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "at today, um this is a feature we",
      "offset": 204.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "dropped yesterday, a demo video for it.",
      "offset": 206.64,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "This is modify video. All these videos",
      "offset": 211.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "um are initially taken on iPhones and",
      "offset": 213.92,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "basically uploaded to our platform with",
      "offset": 217.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "a text prompt and you can turn it turn",
      "offset": 220.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that robot into anything you want.",
      "offset": 223.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And for the AI engineers out in the",
      "offset": 226.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "audience um shout out to he uh manages",
      "offset": 228.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "our public API and you can integrate",
      "offset": 232.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "this functionality to your applications",
      "offset": 234.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "using it very easily. Um you don't need",
      "offset": 236.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to do any kind of crazy prompt",
      "offset": 239.28,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "engineering. We've taken care of that",
      "offset": 240.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for you. Just send us raw user uh",
      "offset": 241.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "prompts, generative media, and we'll",
      "offset": 245.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "send you back images and videos that uh",
      "offset": 248,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "meet your users needs.",
      "offset": 250.319,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "So, if you're interested in that, uh",
      "offset": 254.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "definitely hit me or Kron up. Um he's",
      "offset": 256.799,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "done a wonderful job building out, you",
      "offset": 258.959,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "know, our SDK and our whole kind of",
      "offset": 260.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Devril uh side of things. So, yeah. Um",
      "offset": 261.919,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "that is uh my little plug for our API in",
      "offset": 265.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "case anyone's interested. So back to",
      "offset": 268.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "infrastructure,",
      "offset": 270.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this is what our serving stack basically",
      "offset": 272.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "looked like when we launched. So we had",
      "offset": 274.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "a bunch of just tightly tightly coupled",
      "offset": 276.8,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "uh containers working together. Um one",
      "offset": 279.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of the benefits of this is that we could",
      "offset": 283.199,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "kind of just launch these on raw",
      "offset": 284.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "machines with zero other dependencies.",
      "offset": 285.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Um so that worked out well for us at",
      "offset": 288,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "launch. Um but there were some",
      "offset": 289.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "challenges uh scaling this up. So you",
      "offset": 292.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "know like any good engineer, I didn't",
      "offset": 294.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "want to reinvent the wheel initially. So",
      "offset": 296.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "we we reached for Triton inference",
      "offset": 298.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "server um which is kind of a classic um",
      "offset": 300.24,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "generalpurpose model serving uh server",
      "offset": 304.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but there there were some issues with",
      "offset": 306.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it. Um this setup was brittle. Um if",
      "offset": 308.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Triton went down the the CPU processes",
      "offset": 310.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "didn't necessarily know that it went",
      "offset": 313.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "down and so you'd be kind of pulling",
      "offset": 314.639,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "jobs and they'd fail. It was annoying.",
      "offset": 316.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Um, worse though with these video",
      "offset": 319.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "models, you're needing to uh run these",
      "offset": 321.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "on multiple GPUs and actually multiple",
      "offset": 324,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "nodes in a lot of cases to get to the",
      "offset": 326.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "latency you need. And Triton's just not",
      "offset": 328,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "built for that. Um, also we run our",
      "offset": 330,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "inference now on multiple different",
      "offset": 333.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "chipsets. So Nvidia is the the company",
      "offset": 335.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "that builds Triton. Uh, they don't have",
      "offset": 338.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "great support for things like AMD or",
      "offset": 340.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Grock or any of those. Um and finally",
      "offset": 343.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the biggest kind of hurdle was that this",
      "offset": 346.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "was really difficult to develop against",
      "offset": 348,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for the researchers. Um it had a whole",
      "offset": 350.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "bunch of different idioms, a whole bunch",
      "offset": 352.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of um kind of incantations you needed to",
      "offset": 354.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "make to make it work well and the",
      "offset": 357.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "overall setup was just uh it just felt",
      "offset": 358.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "very janky. So what we ended up doing um",
      "offset": 360.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "was you know rearchitecting to address",
      "offset": 364.24,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "some of these things and uh building our",
      "offset": 366.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "own serving stack on top of you know",
      "offset": 369.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "vanilla pietorch for all the GPU work.",
      "offset": 371.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "um that worked out really well because",
      "offset": 373.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "most of the vendors that are building uh",
      "offset": 375.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "these different chipsets, they make sure",
      "offset": 379.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that PyTorch is fully supported. It's",
      "offset": 381.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "kind of this great substrate to build on",
      "offset": 382.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "top of if you support, you know, very",
      "offset": 384.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "vanilla PyTorch things. Um you can",
      "offset": 386.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "typically make your model run anywhere.",
      "offset": 389.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Um you may need to optimize certain",
      "offset": 391.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "optim uh operations uh depending on your",
      "offset": 393.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "model to make things fast, but it's",
      "offset": 396.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "relatively easy to uh to get started. Um",
      "offset": 398.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and in terms of like the decoupled",
      "offset": 401.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "architecture you you see here uh the CPU",
      "offset": 402.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "workers being decoupled is quite quite",
      "offset": 405.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "useful and important because you can use",
      "offset": 407.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "um use them to queue up work and you",
      "offset": 411.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "know pull in when you're dealing with",
      "offset": 413.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you know videos, images, multimedia",
      "offset": 414.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "inputs on top of just text. You want",
      "offset": 416.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "those to be in the cluster ready for the",
      "offset": 419.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "GPUs to pull so you're not blocking the",
      "offset": 422.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "GPUs at all. Um and also with this",
      "offset": 424,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "architecture you can actually run the",
      "offset": 426.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "GPUs anywhere as long as you can connect",
      "offset": 428.56,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "to Reddus and our distributed storage.",
      "offset": 430.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "You can kind of see there seaweed FS um",
      "offset": 433.039,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you can have a a GPU in you know any",
      "offset": 435.52,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "kind of random provider any random VM",
      "offset": 439.199,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "and use tail scale connect uh connect to",
      "offset": 441.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "the rest of this architecture and then",
      "offset": 445.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "scale up without having to do all that",
      "offset": 447.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "other kind of crazy parallel SSH stuff.",
      "offset": 448.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Um so you know you want to run uh",
      "offset": 451.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "compute on your training cluster now you",
      "offset": 453.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "don't need to um you know provision",
      "offset": 455.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "special machines or anything you just",
      "offset": 458.479,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "run a command.",
      "offset": 459.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "There were a few more challenges that we",
      "offset": 461.759,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "hit um after we got through kind of the",
      "offset": 463.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the initial hurdles of this uh this",
      "offset": 466.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "infrastructure. So one of uh one of the",
      "offset": 468.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "big ones was back pressure. So because",
      "offset": 471.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this is decoupled um you can get into a",
      "offset": 473.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "sit and because there's multiple",
      "offset": 475.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "clusters that are you know pulling in",
      "offset": 477.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "work from the same global queue you can",
      "offset": 480.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "get into this situation where you have",
      "offset": 482.56,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "too many CPU workers pulling in work to",
      "offset": 484,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "one cluster and they're they're kind of",
      "offset": 485.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "waiting there to be processed by the",
      "offset": 488.639,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "GPUs on that cluster uh when they could",
      "offset": 490.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have been processed somewhere else. So",
      "offset": 492.319,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we came up with this uh dispatch uh",
      "offset": 494.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "limitation system. So we came up with",
      "offset": 496.639,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like a state where if a GPU has been",
      "offset": 498.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "pulled into a uh sorry if a job has been",
      "offset": 501.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "pulled into a cluster and is waiting to",
      "offset": 503.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "be picked up by a GPU uh you can put a",
      "offset": 505.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "limitation on like how many jobs are in",
      "offset": 507.68,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "that state so that you can avoid this",
      "offset": 509.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this issue. Um I'll talk more in depth",
      "offset": 512.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about uh priorities and our fair",
      "offset": 514.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "scheduling uh wos. That was another one.",
      "offset": 516.159,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Um you know we have multiple tiers of",
      "offset": 518.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "users and deciding whose jobs get",
      "offset": 521.039,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "process first is uh a constant",
      "offset": 524,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "optimization challenge. Um also handling",
      "offset": 526.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "different models. So like these video",
      "offset": 529.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "models are big. They are typically made",
      "offset": 530.64,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "up of you know 10 20 different um subm",
      "offset": 533.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "models. So you're pulling in a lot of",
      "offset": 536.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "weights. you're spending a lot of time",
      "offset": 539.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "compiling these things. So, traditional",
      "offset": 541.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "autoscaling is super wasteful. You're",
      "offset": 543.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "wasting, you know, 10 20 minutes of GPU",
      "offset": 545.519,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "time just warming things up. Um, and",
      "offset": 549.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "finally, you know, handling bursts. So,",
      "offset": 553.04,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "um, we basically built this system to",
      "offset": 556.16,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "handle bursts where you could scale up",
      "offset": 559.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "automatically on our training cluster,",
      "offset": 563.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "which our researchers hate me for. uh it",
      "offset": 565.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "makes them very upset but it allows us",
      "offset": 568,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to keep up with the demand from our",
      "offset": 570.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "users. Um and so that decoupled",
      "offset": 572.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "architecture plus a very simple",
      "offset": 574.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "scheduler that runs on top of slurm and",
      "offset": 577.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "can just run these pietorrch workers um",
      "offset": 579.68,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "handles you know increasing the total",
      "offset": 583.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "pool of GPUs when we need it and scaling",
      "offset": 585.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "down when we don't.",
      "offset": 587.36,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "So this system is a pullbased system and",
      "offset": 590.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "there's you know cues that submit to",
      "offset": 593.519,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "cues that pull from cues and it is um as",
      "offset": 595.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "you know Sorish and Vas you can both",
      "offset": 600.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "attest to here from Luma it is the least",
      "offset": 602,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "um enjoyable part of working at Luma is",
      "offset": 605.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "managing these cues is is everybody's uh",
      "offset": 608,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "least favorite task. So um initially",
      "offset": 611.12,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "when we uh launched Ray 2 which is a",
      "offset": 615.279,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "much bigger more resource inensive model",
      "offset": 619.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "um we began to actually have to deal",
      "offset": 622.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "with the the fact that we couldn't just",
      "offset": 624.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "keep scaling up more and more compute.",
      "offset": 626.64,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "It just wasn't economical or feasible.",
      "offset": 628.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Um so we had to deal with limited",
      "offset": 630.959,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "resources and um when you deal when you",
      "offset": 633.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "have like a pollbased scheduler with a",
      "offset": 636.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "bunch of cues like this there's this",
      "offset": 638,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "concept of work starvation that that",
      "offset": 640.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "comes into play where essentially you",
      "offset": 643.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "know we've got API uh tier jobs which we",
      "offset": 645.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "try to process very quickly. We've got",
      "offset": 648.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "enterprise we've got unlimited plus",
      "offset": 650.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "light free. So we've got all these",
      "offset": 653.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "different tiers that have different",
      "offset": 655.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "priority and who gets to go first. So if",
      "offset": 657.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you just naively process them in",
      "offset": 660.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "priority order, there may be enough",
      "offset": 661.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "enterprise or API jobs that the light",
      "offset": 664,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "jobs never get processed. So we were",
      "offset": 666.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "having people waiting for like seven,",
      "offset": 668.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "eight, nine hours and they were very",
      "offset": 670.399,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "unhappy. Um so also shout out to Sorish.",
      "offset": 672.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "He actually on a weekend after we",
      "offset": 676.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "designed this system in a couple hours",
      "offset": 678.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "implemented um this SLO based system",
      "offset": 680.48,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "which I'll go into that allows us to",
      "offset": 683.279,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "more fairly schedule uh work across the",
      "offset": 686.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the limited resources that we have. So",
      "offset": 690.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "how the system works is when you think",
      "offset": 693.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "about um",
      "offset": 696.079,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "the the concept of",
      "offset": 698.48,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "work starvation, one of the typical",
      "offset": 701.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "approaches you can you can use to manage",
      "offset": 704.399,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "this is aging. So the idea is the longer",
      "offset": 707.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "something waits in the queue, the higher",
      "offset": 709.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "its priority goes. But what's the actual",
      "offset": 711.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "function to control that aging",
      "offset": 714,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "mechanism? Um so we had the insight that",
      "offset": 715.68,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "this is a product problem and it really",
      "offset": 718.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "comes down to how long worst case are",
      "offset": 721.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "you okay with different tiers waiting.",
      "offset": 724.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Um, so we have service level objectives",
      "offset": 727.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that the product team defines and it",
      "offset": 729.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "kind of controls this aging behavior.",
      "offset": 731.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "And how that works is, you know, an API",
      "offset": 733.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "job, we may not want to wait in a queue",
      "offset": 735.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "more than a couple minutes. But a light",
      "offset": 737.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "job, maybe we're okay with them waiting",
      "offset": 740.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "for 10 minutes. And you can configure",
      "offset": 742.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "these and then set a threshold. Once uh",
      "offset": 745.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "once the threshold gets hit, so say 50%",
      "offset": 748.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "of that, you know, worst case timing,",
      "offset": 750.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then that job gets pulled to the front",
      "offset": 752.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of the queue. And initially uh that",
      "offset": 754.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "worked all right, but then you can",
      "offset": 757.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "actually hit another case of work",
      "offset": 759.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "starvation. Um if if you have a bunch of",
      "offset": 761.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "uh jobs that are potentially breaching",
      "offset": 765.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the SLA where um you know a a job with a",
      "offset": 766.72,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "long SLO since it's been waiting in the",
      "offset": 770,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "queue for a longer time will actually",
      "offset": 772.639,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "starve out the resources of say an API",
      "offset": 774.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "job that you don't want to wait as long.",
      "offset": 776.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Um, so how we handled that was with a uh",
      "offset": 778.639,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we we ranked the jobs by the percentage",
      "offset": 781.68,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "of their SLO um that they that they were",
      "offset": 784.639,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "at. So like if an API job's waiting for",
      "offset": 787.839,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a minute, that gets treated the same as",
      "offset": 790.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a light job that's been waiting for 10",
      "offset": 792.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "minutes. And that actually works out",
      "offset": 795.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "really nicely in practice and results in",
      "offset": 796.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "kind of intuitive fair scheduling",
      "offset": 799.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "behaviors.",
      "offset": 800.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "The last piece I want to talk about is",
      "offset": 803.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "kind of how we actually manage all these",
      "offset": 805.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "models. Um, so this was something where",
      "offset": 807.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "I was glad that we kind of reached for",
      "offset": 810.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Triton initially. They had this nice",
      "offset": 811.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "concept of a model repo um that that",
      "offset": 813.44,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "we've really leaned into. So every model",
      "offset": 816.639,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "um has a folder in object storage",
      "offset": 820.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "somewhere with a bunch of subfolders",
      "offset": 823.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that have different versions. Um, and so",
      "offset": 826.56,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "as you, you know, develop the code and",
      "offset": 829.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "fine-tune these models, deploy new",
      "offset": 833.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "checkpoints, you, uh, create a bunch of",
      "offset": 835.2,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "these immutable versions, and then you",
      "offset": 838.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "have a simple YAML file in the root of",
      "offset": 840.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the model folder that lets you define",
      "offset": 842.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "which one's active. So that's really",
      "offset": 845.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "nice because you can kind of re",
      "offset": 847.44,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "reproducibly",
      "offset": 848.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "um you know in these sorry in these",
      "offset": 850.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "versions you can um you store the full",
      "offset": 852.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Python environment so all the",
      "offset": 855.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "dependencies needed to run the model and",
      "offset": 857.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the checkpoints and this system is",
      "offset": 859.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "pretty nice because you know if you ever",
      "offset": 861.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "need to roll back or you know you want",
      "offset": 863.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to run things in all these different",
      "offset": 866.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "desperate environments you can make sure",
      "offset": 868.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "that you know you're running the exact",
      "offset": 870.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "same Python environment the exact same",
      "offset": 872.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "checkpoint that you were before And very",
      "offset": 873.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "rarely are there issues that um you",
      "offset": 876.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "can't really solve by just rolling back.",
      "offset": 878.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "So that's been quite useful for us. And",
      "offset": 881.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "um we've actually built like a automated",
      "offset": 884.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "kind of rollout system on top of this",
      "offset": 886.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "too where when we update those YAML",
      "offset": 888.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "files, the workers will just kind of",
      "offset": 890.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "switch versions on the fly without",
      "offset": 892,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "restarting. And um that that helps us",
      "offset": 894.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "kind of roll out uh new new model",
      "offset": 897.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "changes to the whole fleet of these",
      "offset": 899.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "thousands of uh H100 and uh AMD GPUs all",
      "offset": 902,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "at once. Um which makes managing this",
      "offset": 905.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "much more sane than the early days of",
      "offset": 907.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "parallel SSH.",
      "offset": 909.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And um yeah, if any of this seemed",
      "offset": 912.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "interesting to you, up your alley, you",
      "offset": 914.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "know, we're hiring, we're actively",
      "offset": 915.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "looking for cracked engineers,",
      "offset": 917.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "researchers,",
      "offset": 918.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "um AI enthusiasts in general. Please uh",
      "offset": 920.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "please hit us up and uh yeah, I just",
      "offset": 923.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "wanted to thank everybody for taking the",
      "offset": 925.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "time today and uh thank the team at Luma",
      "offset": 927.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "because uh it's it's incredible the work",
      "offset": 929.36,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "that everyone there does.",
      "offset": 931.519,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "We've we've got about three minutes for",
      "offset": 936.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "questions. So, let's take one or two and",
      "offset": 938.88,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "let's see where we get any questions.",
      "offset": 941.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Yeah. So, the the kind of cheat code",
      "offset": 963.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "here is that the chip the chip providers",
      "offset": 966.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "who we typically partner with really",
      "offset": 968.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "closely um they're always making sure",
      "offset": 970.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that PyTorch at least at a certain",
      "offset": 972.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "version works for their chipset. So,",
      "offset": 974.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "they're doing a lot of that work for us.",
      "offset": 976.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "And then typically what'll happen is",
      "offset": 978.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "we'll try to run the models on whatever",
      "offset": 981.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "the new chipset is. Usually it'll work",
      "offset": 983.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then it'll work a bit more slowly.",
      "offset": 986.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So we've got actually a team of like 10",
      "offset": 989.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "guys we call our Excel team that are",
      "offset": 991.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "optimizing the low-level operations",
      "offset": 994.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "within PyTorch using things like Triton",
      "offset": 997.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and working with the chip providers to",
      "offset": 999.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "make sure that things are actually fast.",
      "offset": 1001.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "So you'll typically with PyTorch be able",
      "offset": 1002.72,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "to run things um anywhere but a lot of",
      "offset": 1004.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the times they won't be fast and then we",
      "offset": 1007.759,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "just work closely with the chipset uh",
      "offset": 1009.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "providers to actually you know optimize",
      "offset": 1011.839,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "the model over time.",
      "offset": 1013.68,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. We work really closely with",
      "offset": 1019.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Nvidia and AMD and uh we are exploring",
      "offset": 1021.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you know some other providers um but",
      "offset": 1024.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "nothing really deep yet like you know",
      "offset": 1026.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Amazon's got their own chips uh Gro has",
      "offset": 1028.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "some chips. We just announced a big",
      "offset": 1030.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "partnership with uh Humane who works",
      "offset": 1032.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "really closely with with Grock and so",
      "offset": 1034.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "yeah, we're exploring some of these",
      "offset": 1037.199,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "other chipsets um through through some",
      "offset": 1038.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "of these uh partnerships.",
      "offset": 1040.079,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "Cool. Any other questions? Yeah.",
      "offset": 1043.6,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "We do not we well",
      "offset": 1053.039,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "I don't think so. Um but yeah, we we we",
      "offset": 1057.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "work with cloud providers that basically",
      "offset": 1059.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "kind of give us a um a working",
      "offset": 1062.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Kubernetes cluster at the very least,",
      "offset": 1065.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "which which is nice. Um we're not",
      "offset": 1066.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "provisioning the nodes oursel or",
      "offset": 1069.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "anything, but I actually would have to",
      "offset": 1070.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "check with the individual providers if",
      "offset": 1072.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "there's like VMs or if they're actually",
      "offset": 1074.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "metal machines. I know uh when we work",
      "offset": 1075.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "with Amazon, they're not, but some of",
      "offset": 1077.84,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "the other ones might be.",
      "offset": 1079.6,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1090.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Uh not yet. Um the way the like general",
      "offset": 1101.44,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "application works um does involve uh",
      "offset": 1106,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "video and like image QA models. So um",
      "offset": 1109.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "essentially the the way the actual",
      "offset": 1113.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "application works is we have like an",
      "offset": 1114.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "agent that you're interacting with. So",
      "offset": 1116.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "when you upload a video or an image that",
      "offset": 1118.559,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "is actually being you know captioned by",
      "offset": 1121.76,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "you know VLMs to enhance the the the",
      "offset": 1125.6,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "total pro the overall prompt but um no",
      "offset": 1128.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "like true VQA stuff quite yet but there",
      "offset": 1132.799,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "there may or may not be some coming.",
      "offset": 1135.44,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1141.31,
      "duration": 2.799
    }
  ],
  "cleanText": "[Music]\n\nSo, it's 9:00 a.m. June 11th, 2024, and we send out the announcement and hold our breath waiting to see the user signups pour in. We're expecting significant traffic for the launch of Dream Machine, Luma's first video model, and we were woefully unprepared for what came next. We'd allocated about 500 H100 GPUs. We thought that was a lot at the time. It wasn't.\n\nOver the next hour, we saw request after request pour in and a giant queue of requests start to pile up. Luckily, we had a contingency plan for this, which was to take every GPU we had access to from every provider and start manually running SSH commands against them to spin up workers pulling in work from a global queue. We were able to get to about 5,000 H100s over the next six hours, and our queues of almost 100,000 finally started to drain around like 2 p.m. that day.\n\nThat is when Emit, our CEO and self-appointed chaos monkey, decided to tweet that we had scaled up by 10x. \"Come on in. It'll be faster now.\"\n\nWe'll see how it goes. Our queues were down to about, you know, 300 at this point. You can kind of see me here freaking out a little bit going, \"Okay.\" About 10 minutes after the tweet goes out, they start to go up again, even though we've scaled up by 10 times. So now we're at 350, we're at 400, we're at 400.\n\nAs this is going on, we're basically taking the entire training cluster. These are the only GPUs we hadn't used yet, taking the training cluster over. It's another like 4,000 H100 GPUs, and it barely made a dent. The queues were still going up. You'll see this, it's the KEK W emoji, which has become a cultural staple of Luma over time, and it was very much the mood at the time, like, \"What am I supposed to do with this?\"\n\nSo I'm here to talk to you guys today a little bit about Luma, who we are, what we do, and how we've managed to kind of scale our infrastructure up to initially a million users in 4 days, which was pretty nuts. For some context, ChatGPT hit a million users in 5 days. We processed about half a million videos over the course of these 12 hours. So yeah, we're going to talk a little bit about what we learned scaling up and how we did it.\n\nBut first, a very quick little intro to Luma if you're not familiar with us. We're not just a video model company. We're a foundation model lab, and we're aiming to build general multimodal intelligence that can generate, understand, and operate in the physical world just like a human can.\n\nTo give you some sense of what our models can do and where we're at today, this is a feature we dropped yesterday, a demo video for it. This is modify video. All these videos are initially taken on iPhones and basically uploaded to our platform with a text prompt, and you can turn that robot into anything you want.\n\nFor the AI engineers out in the audience, shout out to he, who manages our public API, and you can integrate this functionality to your applications using it very easily. You don't need to do any kind of crazy prompt engineering. We've taken care of that for you. Just send us raw user prompts, generative media, and we'll send you back images and videos that meet your users' needs.\n\nSo, if you're interested in that, definitely hit me or Kron up. He's done a wonderful job building out our SDK and our whole kind of DevRel side of things. So, yeah. That is my little plug for our API in case anyone's interested.\n\nSo back to infrastructure, this is what our serving stack basically looked like when we launched. We had a bunch of just tightly coupled containers working together. One of the benefits of this is that we could kind of just launch these on raw machines with zero other dependencies. So that worked out well for us at launch, but there were some challenges scaling this up.\n\nLike any good engineer, I didn't want to reinvent the wheel initially. So we reached for Triton inference server, which is kind of a classic general-purpose model serving server, but there were some issues with it. This setup was brittle. If Triton went down, the CPU processes didn't necessarily know that it went down, and so you'd be kind of pulling jobs, and they'd fail. It was annoying.\n\nWorse though, with these video models, you're needing to run these on multiple GPUs and actually multiple nodes in a lot of cases to get to the latency you need. Triton's just not built for that. Also, we run our inference now on multiple different chipsets. Nvidia is the company that builds Triton. They don't have great support for things like AMD or Groq or any of those.\n\nFinally, the biggest kind of hurdle was that this was really difficult to develop against for the researchers. It had a whole bunch of different idioms, a whole bunch of kind of incantations you needed to make to make it work well, and the overall setup was just, it just felt very janky.\n\nSo what we ended up doing was rearchitecting to address some of these things and building our own serving stack on top of vanilla PyTorch for all the GPU work. That worked out really well because most of the vendors that are building these different chipsets, they make sure that PyTorch is fully supported. It's kind of this great substrate to build on top of if you support, you know, very vanilla PyTorch things. You can typically make your model run anywhere. You may need to optimize certain operations depending on your model to make things fast, but it's relatively easy to get started.\n\nIn terms of the decoupled architecture you see here, the CPU workers being decoupled is quite useful and important because you can use them to queue up work, and you know, pull in when you're dealing with videos, images, multimedia inputs on top of just text. You want those to be in the cluster ready for the GPUs to pull so you're not blocking the GPUs at all.\n\nAlso, with this architecture, you can actually run the GPUs anywhere as long as you can connect to Reddis and our distributed storage. You can kind of see there, Seaweed FS. You can have a GPU in any kind of random provider, any random VM, and use tail scale connect to connect to the rest of this architecture and then scale up without having to do all that other kind of crazy parallel SSH stuff.\n\nYou want to run compute on your training cluster now, you don't need to provision special machines or anything. You just run a command.\n\nThere were a few more challenges that we hit after we got through the initial hurdles of this infrastructure. One of the big ones was back pressure. Because this is decoupled, you can get into a situation, and because there's multiple clusters that are pulling in work from the same global queue, you can get into this situation where you have too many CPU workers pulling in work to one cluster, and they're waiting there to be processed by the GPUs on that cluster when they could have been processed somewhere else.\n\nSo we came up with this dispatch limitation system. We came up with a state where if a GPU has been pulled into a, sorry, if a job has been pulled into a cluster and is waiting to be picked up by a GPU, you can put a limitation on how many jobs are in that state so that you can avoid this issue.\n\nI'll talk more in depth about priorities and our fair scheduling. That was another one. We have multiple tiers of users, and deciding whose jobs get processed first is a constant optimization challenge. Also handling different models. These video models are big. They are typically made up of 10, 20 different sub-models. So you're pulling in a lot of weights. You're spending a lot of time compiling these things. Traditional autoscaling is super wasteful. You're wasting, you know, 10, 20 minutes of GPU time just warming things up.\n\nFinally, handling bursts. We basically built this system to handle bursts where you could scale up automatically on our training cluster, which our researchers hate me for. It makes them very upset, but it allows us to keep up with the demand from our users. That decoupled architecture plus a very simple scheduler that runs on top of Slurm and can just run these PyTorch workers handles increasing the total pool of GPUs when we need it and scaling down when we don't.\n\nThis system is a pull-based system, and there's cues that submit to cues that pull from cues, and it is, as you know, Sorish and Vas, you can both attest to here from Luma, it is the least enjoyable part of working at Luma is managing these cues. It is everybody's least favorite task.\n\nInitially, when we launched Ray 2, which is a much bigger, more resource-intensive model, we began to actually have to deal with the fact that we couldn't just keep scaling up more and more compute. It just wasn't economical or feasible. We had to deal with limited resources, and when you have a pull-based scheduler with a bunch of cues like this, there's this concept of work starvation that comes into play, where essentially, you know, we've got API tier jobs, which we try to process very quickly. We've got enterprise, we've got unlimited plus light free. We've got all these different tiers that have different priority and who gets to go first. If you just naively process them in priority order, there may be enough enterprise or API jobs that the light jobs never get processed. We were having people waiting for like seven, eight, nine hours, and they were very unhappy.\n\nAlso, shout out to Sorish. He actually, on a weekend after we designed this system, in a couple hours, implemented this SLO-based system, which I'll go into, that allows us to more fairly schedule work across the limited resources that we have.\n\nHow the system works is when you think about the concept of work starvation, one of the typical approaches you can use to manage this is aging. The idea is the longer something waits in the queue, the higher its priority goes. But what's the actual function to control that aging mechanism?\n\nWe had the insight that this is a product problem, and it really comes down to how long worst case are you okay with different tiers waiting. We have service level objectives that the product team defines, and it kind of controls this aging behavior. How that works is, you know, an API job, we may not want to wait in a queue more than a couple minutes. But a light job, maybe we're okay with them waiting for 10 minutes. You can configure these and then set a threshold. Once the threshold gets hit, so say 50% of that worst-case timing, then that job gets pulled to the front of the queue.\n\nInitially, that worked all right, but then you can actually hit another case of work starvation. If you have a bunch of jobs that are potentially breaching the SLA, where a job with a long SLO, since it's been waiting in the queue for a longer time, will actually starve out the resources of, say, an API job that you don't want to wait as long.\n\nHow we handled that was with a, we ranked the jobs by the percentage of their SLO that they were at. If an API job's waiting for a minute, that gets treated the same as a light job that's been waiting for 10 minutes. That actually works out really nicely in practice and results in kind of intuitive fair scheduling behaviors.\n\nThe last piece I want to talk about is how we actually manage all these models. This was something where I was glad that we kind of reached for Triton initially. They had this nice concept of a model repo that we've really leaned into. Every model has a folder in object storage somewhere with a bunch of subfolders that have different versions.\n\nAs you develop the code and fine-tune these models, deploy new checkpoints, you create a bunch of these immutable versions, and then you have a simple YAML file in the root of the model folder that lets you define which one's active. That's really nice because you can kind of reproducibly, in these, sorry, in these versions, you can store the full Python environment, so all the dependencies needed to run the model and the checkpoints. This system is pretty nice because, you know, if you ever need to roll back or you want to run things in all these different desperate environments, you can make sure that you're running the exact same Python environment, the exact same checkpoint that you were before. Very rarely are there issues that you can't really solve by just rolling back.\n\nThat's been quite useful for us. We've actually built like an automated kind of rollout system on top of this too, where when we update those YAML files, the workers will just kind of switch versions on the fly without restarting. That helps us kind of roll out new model changes to the whole fleet of these thousands of H100 and AMD GPUs all at once, which makes managing this much more sane than the early days of parallel SSH.\n\nIf any of this seemed interesting to you, up your alley, you know, we're hiring. We're actively looking for cracked engineers, researchers, AI enthusiasts in general. Please hit us up. I just wanted to thank everybody for taking the time today and thank the team at Luma because it's incredible the work that everyone there does.\n\nWe've got about three minutes for questions. So, let's take one or two and let's see where we get any questions.\n\nYeah. The kind of cheat code here is that the chip, the chip providers, who we typically partner with really closely, they're always making sure that PyTorch at least at a certain version works for their chipset. They're doing a lot of that work for us. Then typically what'll happen is we'll try to run the models on whatever the new\n\n\nChipset is. Usually it'll work, and then it'll work a bit more slowly.\nSo we've got actually a team of like 10 guys, we call our Excel team, that are optimizing the low-level operations within PyTorch using things like Triton and working with the chip providers to make sure that things are actually fast.\nSo you'll typically with PyTorch be able to run things anywhere, but a lot of the times they won't be fast, and then we just work closely with the chipset providers to actually, you know, optimize the model over time.\nYeah.\nWe work really closely with Nvidia and AMD, and we are exploring, you know, some other providers, but nothing really deep yet, like, you know, Amazon's got their own chips, Groq has some chips.\nWe just announced a big partnership with Humane, who works really closely with Groq, and so yeah, we're exploring some of these other chipsets through some of these partnerships.\nCool.\nAny other questions?\nYeah.\nWe do not, we well, I don't think so.\nUm, but yeah, we we we work with cloud providers that basically kind of give us a um a working Kubernetes cluster at the very least, which which is nice.\nUm, we're not provisioning the nodes ourselves or anything, but I actually would have to check with the individual providers if there's like VMs or if they're actually metal machines.\nI know when we work with Amazon, they're not, but some of the other ones might be.\nYeah.\nUh, not yet.\nUm, the way the like general application works, um, does involve uh video and like image QA models.\nSo um, essentially the the way the actual application works is we have like an agent that you're interacting with.\nSo when you upload a video or an image that is actually being, you know, captioned by, you know, VLMs to enhance the the the total pro the overall prompt, but um no like true VQA stuff quite yet, but there there may or may not be some coming.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.310Z"
}