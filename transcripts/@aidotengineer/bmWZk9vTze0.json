{
  "episodeId": "bmWZk9vTze0",
  "channelSlug": "@aidotengineer",
  "title": "MCP is all you need â€” Samuel Colvin, Pydantic",
  "publishedAt": "2025-07-18T18:52:25.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 0.37,
      "duration": 13.68
    },
    {
      "lang": "en",
      "text": "So yeah, I'm talking about uh MCP is all",
      "offset": 14.08,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "you need. A bit about who I am before we",
      "offset": 16.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "get started. I'm best known as the",
      "offset": 19.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "creator of Pideantic uh data validation",
      "offset": 21.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "library for Python that is uh fairly",
      "offset": 24.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "ubiquitous. downloaded about 360 million",
      "offset": 26.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "times a month. So someone pointed out to",
      "offset": 30.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "me that's like 140 times a second. Uh",
      "offset": 32.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Pantic is used in general Python",
      "offset": 35.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "development everywhere but also in",
      "offset": 37.52,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Genai. So it's used in all of the SDKs",
      "offset": 39.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and agent frameworks in Python",
      "offset": 42.879,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "basically. Uh Pantic became a company uh",
      "offset": 44.399,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "uh beginning of 23 and we have uh built",
      "offset": 48.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "two things beyond Pyantic since then.",
      "offset": 51.28,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Pantic AI uh an agent framework for",
      "offset": 53.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Python built on the same principles as",
      "offset": 56.399,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Pantic um and Pantic Logfire",
      "offset": 58.719,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "observability platform um which is our",
      "offset": 62.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "which is the commercial part of what we",
      "offset": 64.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "do. Um I'm also",
      "offset": 66.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "a somewhat inactive co-maintainer of the",
      "offset": 69.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "MCP Python SDK.",
      "offset": 72.159,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 74.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "so MCP is all you need is obviously a a",
      "offset": 76.479,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "play on Jason Lou's talks pantic is all",
      "offset": 80.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you need that he gave at AI engineer I",
      "offset": 83.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "think first of all nearly two years ago",
      "offset": 86.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and then the second one pantic is still",
      "offset": 88.72,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "all you need maybe this time last year.",
      "offset": 91.28,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "Um and it has the same basic idea that",
      "offset": 95.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "people are over complicating something",
      "offset": 97.439,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that we can use a single tool for and I",
      "offset": 98.799,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "guess also similarly the title is",
      "offset": 101.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "completely unrealistic. Of course",
      "offset": 104.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "padantic is not all you need. Uh and",
      "offset": 106.159,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "neither is MCP for everything but it has",
      "offset": 108.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the we have the I think where where we",
      "offset": 111.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "agree is that there are an awful lot of",
      "offset": 113.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "things that MCP can do and that people",
      "offset": 115.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "are over complicating the situation",
      "offset": 116.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sometimes trying to come up with new",
      "offset": 118.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "ways of doing agentto agent",
      "offset": 120.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "communication.",
      "offset": 121.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 123.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I'm talking here specifically about",
      "offset": 125.119,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "autonomous agents or code that you're",
      "offset": 127.52,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "writing. I'm not talking about the",
      "offset": 130.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "um,",
      "offset": 133.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "uh, claw desktop or cursor uh, Z wind",
      "offset": 135.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "surf, etc. use case of coding agents.",
      "offset": 139.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Those were what MCP was originally",
      "offset": 141.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "primarily designed for. Um, I don't know",
      "offset": 143.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "whether or not David Pereira would say",
      "offset": 146.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that that what we're doing using MCP",
      "offset": 148.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "from Python is a he definitely wouldn't",
      "offset": 151.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "say it's a misuse, but it I don't think",
      "offset": 153.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it it was the primary uh design use case",
      "offset": 155.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 159.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "um for MCP.",
      "offset": 161.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "So, two of the of the primitives of MCP",
      "offset": 163.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "prompts and resources probably don't",
      "offset": 167.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "come into this use case that much.",
      "offset": 170.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "They're very useful or or should be very",
      "offset": 172.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "useful in the kind of cursor type use",
      "offset": 174,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "case. They don't really apply in what",
      "offset": 176.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "we're talking about here. Um,",
      "offset": 178.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "but tool calling, the third primitive,",
      "offset": 181.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is extremely useful for what we're",
      "offset": 183.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "trying to do here. Um, tool calling is a",
      "offset": 185.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "lot more complicated than you might at",
      "offset": 188.8,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "first think. A lot of people say to me",
      "offset": 190.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "about MCPR, but couldn't it just be uh",
      "offset": 192.959,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "open API? Why do we need this uh custom",
      "offset": 195.599,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "protocol for doing it? Um, and there's a",
      "offset": 198.959,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "number of reasons. The idea of dynamic",
      "offset": 201.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "tools, the tools that come and go during",
      "offset": 202.879,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "an agent execution depending on the",
      "offset": 204.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "state of the server. Logging, so being",
      "offset": 206.159,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "able to return data to the user",
      "offset": 208.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "while the tool is still executing,",
      "offset": 212.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "sampling, which I'm going to talk about",
      "offset": 215.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "a lot today, perhaps the most",
      "offset": 216.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "confusingly named part of MCP, if not",
      "offset": 218.959,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tech in general right now. Uh, and stuff",
      "offset": 221.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like tracing, observability. Um, and I",
      "offset": 223.599,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "would also add to that actually the uh",
      "offset": 226.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "MCP's way of being allowed to operate as",
      "offset": 229.519,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "effectively a subprocess over standard",
      "offset": 231.28,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "in and standard out is extremely useful",
      "offset": 232.799,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "for lots of use cases and open API",
      "offset": 234.879,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "wouldn't wouldn't solve those problems.",
      "offset": 237.599,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "This is the kind of prototypical",
      "offset": 241.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "image that you will see from lots of",
      "offset": 244.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "people of what uh MCP is all about. The",
      "offset": 245.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "idea is we have some agent, we have any",
      "offset": 248.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "number of different tools that we can",
      "offset": 250.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "connect to that agent and the point is",
      "offset": 252.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that like the agent doesn't need to be",
      "offset": 254.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "designed with those particular tools in",
      "offset": 256,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "mind and those tools can be designed",
      "offset": 258.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "without knowing anything about the agent",
      "offset": 259.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and we can just compose the two together",
      "offset": 261.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "in the same way that uh I can go and use",
      "offset": 262.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "a browser and the web application the",
      "offset": 265.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "website I'm going to doesn't need to",
      "offset": 268.08,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "know anything about the browser. I mean",
      "offset": 269.199,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "I know we live in a kind of monoculture",
      "offset": 270.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of browsers now but like at least the",
      "offset": 271.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "ideal originally was we could have many",
      "offset": 273.6,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "different browsers all connecting over",
      "offset": 275.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the same protocol. MCP is following the",
      "offset": 276.479,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "same idea.",
      "offset": 278.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "But it can get more complicated than",
      "offset": 280.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this. So we can have situations like",
      "offset": 282,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "this where uh we have tools within our",
      "offset": 284.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "system which are themselves agents and",
      "offset": 287.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "are doing agentic things need access to",
      "offset": 290,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "an LLM and they of course can then in",
      "offset": 292.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "turn connect to other tools over MCP or",
      "offset": 294.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "or directly connecting to tools. This",
      "offset": 297.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this works nicely. This is elegant. But",
      "offset": 300.08,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "there's a problem.",
      "offset": 301.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "every single agent in our system needs",
      "offset": 303.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "access to an LLM. And so we need to go",
      "offset": 306.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and configure that. We need to work out",
      "offset": 308.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "resources for that. And if we are",
      "offset": 310.08,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "um using remote MCP servers, if that",
      "offset": 314.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "remote MCP server needs to",
      "offset": 316.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "um use an LLM, well, now it's worried",
      "offset": 320.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "about what the cost is going to be of",
      "offset": 322.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "doing that. What what if the uh remote",
      "offset": 323.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "agent that's operating as a tool could",
      "offset": 326.8,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "effectively piggyback off the",
      "offset": 328.56,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "uh the model that the original agent has",
      "offset": 332.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "access to. That's what sampling gives",
      "offset": 335.039,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "us. So as I say, I think sampling is a",
      "offset": 337.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "somewhat uh that's not making that any",
      "offset": 340.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "bigger unfortunately. Um is that clear",
      "offset": 343.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "on screen? I may maybe I'll make it",
      "offset": 345.919,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "bigger like that. Um sampling is this",
      "offset": 347.36,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "idea of a of a way where within MCP the",
      "offset": 350.639,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "protocol the um server can effectively",
      "offset": 353.52,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "make a request back through the client",
      "offset": 357.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to the LLM. So in this case client makes",
      "offset": 360.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "a request starts some sort of aantic",
      "offset": 362.32,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "query makes a call to the LLM LLM comes",
      "offset": 365.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "back and says I want to call that",
      "offset": 367.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "particular tool which is an MCP server.",
      "offset": 368.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Uh client takes care of making that call",
      "offset": 371.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to the MCP server. The MCP server now",
      "offset": 373.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "says, \"Hey, I actually need to be able",
      "offset": 376.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to use an LLM to answer whatever this",
      "offset": 378.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "question is.\" So, that then gets sent",
      "offset": 380.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "back to the client. The client proxies",
      "offset": 382.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that request to the LLM, receives the",
      "offset": 384.4,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "response from the LLM, sends that uh",
      "offset": 387.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "onto the MCP server, and the MCP server",
      "offset": 389.759,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "then returns and we can continue on our",
      "offset": 392.4,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "way. Um, sampling is very powerful, not",
      "offset": 395.12,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "that widely supported at the moment. Um,",
      "offset": 400.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "I'm going to demo it today with Pantic",
      "offset": 402.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "AI where we have support for sampling.",
      "offset": 404.319,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Well, I'll be honest, it's a PR right",
      "offset": 407.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "now, but it will be soon it will be",
      "offset": 409.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "merged. Um, we have support for sampling",
      "offset": 410.479,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "both as a uh as the client. So, knowing",
      "offset": 413.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "how to proxy the those LLM calls and as",
      "offset": 416.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a server basically being able to",
      "offset": 420,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "register use the MCP client as as the",
      "offset": 421.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "LLM.",
      "offset": 424.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "So this example",
      "offset": 426.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is obviously like all examples",
      "offset": 429.039,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "trivialized or simplified to be to fit",
      "offset": 430.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "on screen. The idea is that we we're",
      "offset": 432.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "building a like research agent which is",
      "offset": 434.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "going to go and research open source uh",
      "offset": 436.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "packages or libraries for us. And we",
      "offset": 439.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "have implemented one of the many tools",
      "offset": 442.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that you would in fact need for this.",
      "offset": 443.919,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "And that tool is um",
      "offset": 445.52,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "making uh I will switch now to code and",
      "offset": 448.8,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "show you uh the one tool that we have.",
      "offset": 451.759,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Uh",
      "offset": 456.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I'm in completely the wrong file. Here",
      "offset": 457.599,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "we are. Um so this tool is querying",
      "offset": 459.44,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "BigQuery",
      "offset": 464.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "BigQuery public data set for uh Pippi to",
      "offset": 465.919,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "get uh numbers about the number of",
      "offset": 469.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "downloads of a particular package. So",
      "offset": 471.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "this is this is pretty standard padantic",
      "offset": 474.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "AI uh padantic AI code. We've configured",
      "offset": 476.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "log file which I'll show you in a",
      "offset": 479.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "moment. We have the dependencies that",
      "offset": 480.479,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the uh that the agent has access to",
      "offset": 482.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "while it's running. We said we can do",
      "offset": 485.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "some retries. So if the agent returns if",
      "offset": 487.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the LLM returns the wrong data, we can",
      "offset": 489.919,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "send a retry a big system prompt where",
      "offset": 491.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we give it basically the schema of the",
      "offset": 494.56,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "table. Uh tell it what to do, give it a",
      "offset": 496.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "few examples, yada yada. But then we get",
      "offset": 498.879,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to this is the probably the powerful",
      "offset": 500.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "bit. So as an output validator we are",
      "offset": 502.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "going to go and first of all we're going",
      "offset": 505.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to strip out uh markdown block quotes",
      "offset": 506.8,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "from the SQL um if they're there then we",
      "offset": 510.08,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "will uh check that the table name is",
      "offset": 513.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "right that it's querying against and",
      "offset": 516.479,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "tell it that it shouldn't if it it",
      "offset": 517.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "shouldn't and then we're going to go and",
      "offset": 519.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "run the query and critically if the",
      "offset": 521.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "query fails we're going to uh raise",
      "offset": 523.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "model retry within pyantici to go and",
      "offset": 526.32,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "retry uh making the um",
      "offset": 528.64,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "uh making the request to the um LLM",
      "offset": 534.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "again saying asking the LLM to to uh",
      "offset": 538.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "attempt to to retry this. And what we're",
      "offset": 540.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the other thing we're doing throughout",
      "offset": 542.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this you'll see here is we have this",
      "offset": 543.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "context. MCP context.log. So you'll see",
      "offset": 546,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "here when we defined depths type we said",
      "offset": 549.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that that was going to be an instance of",
      "offset": 552.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this MCP uh context which is what we get",
      "offset": 554,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "when you call the MCP server. So what",
      "offset": 557.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "we're doing here is we're having a we're",
      "offset": 559.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "providing a type- safe way within in",
      "offset": 561.279,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "this case um the agent validator but it",
      "offset": 564.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "could be in a tool call if you wanted it",
      "offset": 567.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to be to access that context. So we can",
      "offset": 569.279,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "see here that we know at um in the type",
      "offset": 571.839,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "int uh uh that the the type is uh MCP",
      "offset": 574.959,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "context. So we have this log function",
      "offset": 579.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and we know it's signature and we can go",
      "offset": 581.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and make this log call. The point is",
      "offset": 582.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this is going to",
      "offset": 584.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "return to the client and ultimately to",
      "offset": 587.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the user watching before the the thing",
      "offset": 589.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "has completed. So you can get kind of",
      "offset": 591.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "progress updates as we go. MCP also has",
      "offset": 592.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a context concept of progress which I'm",
      "offset": 595.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "not using here but you can imagine that",
      "offset": 598,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "also being valuable if you knew how far",
      "offset": 599.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "through the query you were. You could",
      "offset": 601.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "show an update in progress. So the idea",
      "offset": 602.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "I think the original principle of uh",
      "offset": 605.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "logging like this is that you have the",
      "offset": 607.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the cursor style agent running and we",
      "offset": 609.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "want to be able to give updates to the",
      "offset": 612.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "user. Don't worry I'm still going before",
      "offset": 613.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it's finished and exactly what's",
      "offset": 616.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "happening. But you could also imagine",
      "offset": 617.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this being useful if you were using MCP.",
      "offset": 619.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "If this was research agent was uh",
      "offset": 621.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "running as a web application you wanted",
      "offset": 623.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "to show the user what was going on. This",
      "offset": 625.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "deep research might take you know",
      "offset": 627.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "minutes to run. We can give these logs",
      "offset": 629.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "while the tool call is still executing.",
      "offset": 630.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "And then we're just going to take the",
      "offset": 634.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the output turn it into a list of dict",
      "offset": 635.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and then format it as XML. So you get a",
      "offset": 638.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "nice uh models are very good at",
      "offset": 641.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "basically reviewing XML data. So we",
      "offset": 644.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "basically return whatever the query",
      "offset": 646.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "results are as that kind of XMLish data",
      "offset": 648.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "which the LLM will then be good at uh",
      "offset": 650.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "interpreting.",
      "offset": 653.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Now we get to the MCP bit. So in this",
      "offset": 655.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "code we are setting up an MCP server",
      "offset": 657.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "using fast MCP. There are two versions",
      "offset": 659.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of first MCP right now. Confusingly,",
      "offset": 662.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "this is the one from inside the MCP SDK.",
      "offset": 664.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 668.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we the dock string for our function. So,",
      "offset": 670.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we're registering one tool here, Pippi",
      "offset": 672.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "downloads, and our dock string from that",
      "offset": 674.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "function will end up becoming the",
      "offset": 677.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "description on the tool that is",
      "offset": 678.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "ultimately fed to the LLM that chooses",
      "offset": 680.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to go and call it. Um, and we're going",
      "offset": 682.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to pass in the user's question. And I",
      "offset": 685.76,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "think one of the one of the important",
      "offset": 688,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "things to say here is of course you",
      "offset": 688.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "could set this up to generate the SQL",
      "offset": 690.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "within your",
      "offset": 694.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh central agent. You could include all",
      "offset": 697.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of the um",
      "offset": 699.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "uh description of the SQL the",
      "offset": 702.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "instructions within your within the the",
      "offset": 704.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "description of the tool. Uh models don't",
      "offset": 706.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "seem to like that much data inside a",
      "offset": 709.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tool description. But more to the point,",
      "offset": 711.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we're just going to blow up the context",
      "offset": 712.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "window of our main agent if we're going",
      "offset": 714.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "to ship all of this context on how to",
      "offset": 716.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "make these queries into our main agent.",
      "offset": 717.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "That's just all overhead in all of our",
      "offset": 720,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "calls to that agent regardless of",
      "offset": 722.399,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "whether we're going to call this",
      "offset": 723.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "particular tool. So doing this kind of",
      "offset": 724.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "thing where we're doing the inference",
      "offset": 726.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "inside a tool is a powerful way of",
      "offset": 728.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "effectively limiting uh the context",
      "offset": 730.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "window of the of the main running agent.",
      "offset": 732.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "And then we're just going to return this",
      "offset": 735.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "output which will be a string, the value",
      "offset": 736.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "returned from from here. and we'll just",
      "offset": 738.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "run the run the MCP server and by",
      "offset": 741.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "default the MCP server will run over",
      "offset": 744,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "standard IO. Um, and then we come to our",
      "offset": 745.76,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "our main application. So here we have a",
      "offset": 748.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "definition of our agent. And you see",
      "offset": 752.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "we've defined one MCP server that's just",
      "offset": 754.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "going to run the the script I just",
      "offset": 756.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "showed you, the Pippi MCP server. Um,",
      "offset": 758.88,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "and so then this agent will act as the",
      "offset": 763.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "client and has that register as a tool",
      "offset": 765.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to be able to call. We're also going to",
      "offset": 766.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "set the give it the current date. Uh so",
      "offset": 768.88,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "it doesn't uh assume it's 20 2023 as",
      "offset": 771.36,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "they often do. Um and now we can go and",
      "offset": 775.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "ultimately run our main agent. Ask it",
      "offset": 778.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "for example how many downloads Pantic",
      "offset": 781.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "has had this year. And I'm going to be",
      "offset": 783.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "brave and run it and see what happens.",
      "offset": 785.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And it has succeeded and it has uh gone",
      "offset": 787.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and told us uh that we had whatever 1.6",
      "offset": 789.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "billion downloads this year. But",
      "offset": 793.2,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "probably more interesting is to come and",
      "offset": 794.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "look at what that looks like in Logfire.",
      "offset": 795.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "So if you look at is it going to come",
      "offset": 797.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "through to logfire or we having a",
      "offset": 799.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "failure here as well. This I will admit",
      "offset": 801.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this is the run from just before uh I",
      "offset": 803.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "came on stage but it it would look",
      "offset": 805.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "exactly the same. So I'm not going to",
      "offset": 807.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "talk too much about observability and",
      "offset": 809.36,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "how we do uh how MCP observability or",
      "offset": 811.12,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "tracing works within MCP because I know",
      "offset": 815.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "there's a talk coming up directly after",
      "offset": 817.6,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "me talking about that. So think of this",
      "offset": 818.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "as a kind of uh spoiler for what's going",
      "offset": 820.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to come up. But you can see we we run",
      "offset": 822.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "our outer agent. it decides to it calls",
      "offset": 825.12,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "uh uh GPT40 uh which decides sure enough",
      "offset": 828.48,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "I'm going to go and call this tool. Uh",
      "offset": 833.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it doesn't need to think about",
      "offset": 835.839,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "generating the SQL. It can just have a",
      "offset": 836.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "natural language description of the",
      "offset": 838.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "query that we're trying to make. We then",
      "offset": 839.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "um this is the MCP client as you can see",
      "offset": 842.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "here. MCP client then calls into the MCP",
      "offset": 844.48,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "server. um makes the which then again",
      "offset": 847.6,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "runs a different uh pyantic AI uh agent",
      "offset": 851.279,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "which then makes a call to an LLM which",
      "offset": 855.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "happens through proxing it through the",
      "offset": 858,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "client. So that's where you can see the",
      "offset": 859.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "service going client server uh client",
      "offset": 861.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "server",
      "offset": 864.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "ultimately if you look at the top level",
      "offset": 866.639,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh exchange with the model you'll see",
      "offset": 868.32,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "here yeah the the the out ultimate",
      "offset": 870.959,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "output was it return the the return",
      "offset": 875.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "response from running the query was was",
      "offset": 877.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this kind of XMLish data and then the",
      "offset": 878.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "LLM was able to turn that into a human",
      "offset": 881.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "description of what was going on. I",
      "offset": 883.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "think the other interesting thing",
      "offset": 885.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "probably is we can go and look in we",
      "offset": 886.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "should be able to see the actual SQL",
      "offset": 888.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that was called. So this is the agent",
      "offset": 890.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "call inside uh MCP server and you can",
      "offset": 892.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "see here the SQL it wrote and you can",
      "offset": 895.44,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "confirm that it indeed looks correct. Um",
      "offset": 898,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "I am going to",
      "offset": 901.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh go on from there and say um thank you",
      "offset": 904.399,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "very much. Um we are at the booth the",
      "offset": 907.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the Pantic booth. So if anyone has any",
      "offset": 910.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "questions on this, wants to see this",
      "offset": 911.92,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "fail in numerous other exciting ways,",
      "offset": 913.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "very happy to to talk to you. Yeah, come",
      "offset": 915.279,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "and say hi.",
      "offset": 917.12,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 922.06,
      "duration": 3.7
    }
  ],
  "cleanText": "[Music]\nSo yeah, I'm talking about uh MCP is all you need.\nA bit about who I am before we get started.\nI'm best known as the creator of Pydantic, a data validation library for Python that is uh fairly ubiquitous.\nDownloaded about 360 million times a month.\nSo someone pointed out to me that's like 140 times a second.\nUh Pydantic is used in general Python development everywhere but also in GenAI.\nSo it's used in all of the SDKs and agent frameworks in Python basically.\nUh Pydantic became a company uh beginning of '23, and we have uh built two things beyond Pydantic since then.\nPydantic AI, an agent framework for Python built on the same principles as Pydantic, um and Pydantic Logfire, an observability platform, um which is our which is the commercial part of what we do.\nUm, I'm also a somewhat inactive co-maintainer of the MCP Python SDK.\n\nUm, so MCP is all you need is obviously a play on Jason Liu's talks, \"Pydantic is all you need\" that he gave at AIE, I think, first of all, nearly two years ago, and then the second one, \"Pydantic is still all you need,\" maybe this time last year.\nUm, and it has the same basic idea that people are overcomplicating something that we can use a single tool for, and I guess also similarly, the title is completely unrealistic.\nOf course, Pydantic is not all you need.\nUh, and neither is MCP for everything, but it has the, we have the, I think where, where we agree is that there are an awful lot of things that MCP can do and that people are overcomplicating the situation sometimes, trying to come up with new ways of doing agent-to-agent communication.\n\nUm, I'm talking here specifically about autonomous agents or code that you're writing.\nI'm not talking about the um, uh, claw desktop or Cursor, uh, Z wind surf, etc. use case of coding agents.\nThose were what MCP was originally primarily designed for.\nUm, I don't know whether or not David Pereira would say that that what we're doing using MCP from Python is a, he definitely wouldn't say it's a misuse, but it, I don't think it, it was the primary uh design use case for um, for MCP.\n\nSo, two of the primitives of MCP, prompts and resources, probably don't come into this use case that much.\nThey're very useful or, or should be very useful in the kind of Cursor type use case.\nThey don't really apply in what we're talking about here.\nUm, but tool calling, the third primitive, is extremely useful for what we're trying to do here.\nUm, tool calling is a lot more complicated than you might at first think.\nA lot of people say to me about MCP, \"But couldn't it just be uh OpenAPI?\nWhy do we need this uh custom protocol for doing it?\"\nUm, and there's a number of reasons.\nThe idea of dynamic tools, the tools that come and go during an agent execution depending on the state of the server.\nLogging, so being able to return data to the user while the tool is still executing, sampling, which I'm going to talk about a lot today, perhaps the most confusingly named part of MCP, if not tech in general right now.\nUh, and stuff like tracing, observability.\nUm, and I would also add to that, actually, the uh MCP's way of being allowed to operate as effectively a subprocess over standard in and standard out is extremely useful for lots of use cases, and OpenAPI wouldn't, wouldn't solve those problems.\nThis is the kind of prototypical image that you will see from lots of people of what uh MCP is all about.\nThe idea is we have some agent, we have any number of different tools that we can connect to that agent, and the point is that like the agent doesn't need to be designed with those particular tools in mind, and those tools can be designed without knowing anything about the agent, and we can just compose the two together in the same way that uh I can go and use a browser and the web application, the website I'm going to, doesn't need to know anything about the browser.\nI mean, I know we live in a kind of monoculture of browsers now, but like at least the ideal originally was we could have many different browsers all connecting over the same protocol.\nMCP is following the same idea.\n\nBut it can get more complicated than this.\nSo we can have situations like this where uh we have tools within our system which are themselves agents and are doing agentic things, need access to an LLM, and they, of course, can then in turn connect to other tools over MCP or or directly connecting to tools.\nThis, this works nicely.\nThis is elegant.\nBut there's a problem.\nEvery single agent in our system needs access to an LLM.\nAnd so we need to go and configure that.\nWe need to work out resources for that.\nAnd if we are um using remote MCP servers, if that remote MCP server needs to um use an LLM, well, now it's worried about what the cost is going to be of doing that.\nWhat, what if the uh remote agent that's operating as a tool could effectively piggyback off the uh the model that the original agent has access to?\nThat's what sampling gives us.\nSo as I say, I think sampling is a somewhat, uh, that's not making that any bigger, unfortunately.\nUm, is that clear on screen?\nI may, maybe I'll make it bigger like that.\nUm, sampling is this idea of a, of a way where within MCP, the protocol, the um server can effectively make a request back through the client to the LLM.\nSo in this case, client makes a request, starts some sort of Pydantic query, makes a call to the LLM, LLM comes back and says, \"I want to call that particular tool,\" which is an MCP server.\nUh, client takes care of making that call to the MCP server.\nThe MCP server now says, \"Hey, I actually need to be able to use an LLM to answer whatever this question is.\"\nSo, that then gets sent back to the client.\nThe client proxies that request to the LLM, receives the response from the LLM, sends that uh onto the MCP server, and the MCP server then returns and we can continue on our way.\nUm, sampling is very powerful, not that widely supported at the moment.\nUm, I'm going to demo it today with Pydantic AI where we have support for sampling.\nWell, I'll be honest, it's a PR right now, but it will be soon, it will be merged.\nUm, we have support for sampling both as a uh as the client.\nSo, knowing how to proxy the those LLM calls and as a server, basically being able to register, use the MCP client as as the LLM.\n\nSo this example is obviously, like all examples, trivialized or simplified to be to fit on screen.\nThe idea is that we, we're building a like research agent which is going to go and research open source uh packages or libraries for us.\nAnd we have implemented one of the many tools that you would in fact need for this.\nAnd that tool is um, making, uh, I will switch now to code and show you uh the one tool that we have.\nUh, I'm in completely the wrong file.\nHere we are.\nUm, so this tool is querying BigQuery, BigQuery public data set for uh Pippi to get uh numbers about the number of downloads of a particular package.\nSo this is, this is pretty standard Pydantic AI uh Pydantic AI code.\nWe've configured Logfile, which I'll show you in a moment.\nWe have the dependencies that the uh that the agent has access to while it's running.\nWe said we can do some retries.\nSo if the agent returns, if the LLM returns the wrong data, we can send a retry, a big system prompt where we give it basically the schema of the table.\nUh, tell it what to do, give it a few examples, yada yada.\nBut then we get to this is the probably the powerful bit.\nSo as an output validator, we are going to go and first of all, we're going to strip out uh markdown block quotes from the SQL, um if they're there, then we will uh check that the table name is right, that it's querying against and tell it that it shouldn't if it, it shouldn't, and then we're going to go and run the query, and critically, if the query fails, we're going to uh raise model retry within Pydantic to go and retry uh making the um uh making the request to the um LLM again, saying, asking the LLM to to uh attempt to to retry this.\nAnd what we're, the other thing we're doing throughout this, you'll see here is we have this context, MCP context.log.\nSo you'll see here when we defined depths type, we said that that was going to be an instance of this MCP uh context, which is what we get when you call the MCP server.\nSo what we're doing here is we're having a, we're providing a type-safe way within, in this case, um the agent validator, but it could be in a tool call if you wanted it to be, to access that context.\nSo we can see here that we know at um in the type int uh uh that the the type is uh MCP context.\nSo we have this log function and we know it's signature and we can go and make this log call.\nThe point is this is going to return to the client and ultimately to the user watching before the the thing has completed.\nSo you can get kind of progress updates as we go.\nMCP also has a context concept of progress, which I'm not using here, but you can imagine that also being valuable if you knew how far through the query you were.\nYou could show an update in progress.\nSo the idea, I think the original principle of uh logging like this is that you have the the Cursor style agent running and we want to be able to give updates to the user.\n\"Don't worry, I'm still going\" before it's finished and exactly what's happening.\nBut you could also imagine this being useful if you were using MCP.\nIf this was research agent was uh running as a web application, you wanted to show the user what was going on.\nThis deep research might take, you know, minutes to run.\nWe can give these logs while the tool call is still executing.\nAnd then we're just going to take the the output, turn it into a list of dict, and then format it as XML.\nSo you get a nice uh models are very good at basically reviewing XML data.\nSo we basically return whatever the query results are as that kind of XMLish data, which the LLM will then be good at uh interpreting.\n\nNow we get to the MCP bit.\nSo in this code, we are setting up an MCP server using fast MCP.\nThere are two versions of fast MCP right now.\nConfusingly, this is the one from inside the MCP SDK.\nUm, we the dock string for our function.\nSo, we're registering one tool here, Pippi downloads, and our dock string from that function will end up becoming the description on the tool that is ultimately fed to the LLM that chooses to go and call it.\nUm, and we're going to pass in the user's question.\nAnd I think one of the one of the important things to say here is, of course, you could set this up to generate the SQL within your uh central agent.\nYou could include all of the um uh description of the SQL, the instructions within your within the the description of the tool.\nUh, models don't seem to like that much data inside a tool description.\nBut more to the point, we're just going to blow up the context window of our main agent if we're going to ship all of this context on how to make these queries into our main agent.\nThat's just all overhead in all of our calls to that agent regardless of whether we're going to call this particular tool.\nSo doing this kind of thing where we're doing the inference inside a tool is a powerful way of effectively limiting uh the context window of the of the main running agent.\nAnd then we're just going to return this output, which will be a string, the value returned from from here, and we'll just run the run the MCP server, and by default, the MCP server will run over standard IO.\nUm, and then we come to our our main application.\nSo here we have a definition of our agent.\nAnd you see we've defined one MCP server that's just going to run the the script I just showed you, the Pippi MCP server.\nUm, and so then this agent will act as the client and has that register as a tool to be able to call.\nWe're also going to set the, give it the current date.\nUh, so it doesn't uh assume it's 2023 as they often do.\nUm, and now we can go and ultimately run our main agent.\nAsk it, for example, how many downloads Pydantic has had this year.\nAnd I'm going to be brave and run it and see what happens.\nAnd it has succeeded, and it has uh gone and told us uh that we had whatever 1.6 billion downloads this year.\nBut probably more interesting is to come and look at what that looks like in Logfire.\nSo if you look at, is it going to come through to Logfire or are we having a failure here as well?\nThis, I will admit, this is the run from just before uh I came on stage, but it, it would look exactly the same.\nSo I'm not going to talk too much about observability and how we do uh how MCP observability or tracing works within MCP because I know there's a talk coming up directly after me talking about that.\nSo think of this as a kind of uh spoiler for what's going to come up.\nBut you can see we, we run our outer agent, it decides to, it calls uh uh GPT4, which decides, \"Sure enough, I'm going to go and call this tool.\"\nUh, it doesn't need to think about generating the SQL.\nIt can just have a natural language description of the query that we're trying to make.\nWe then um this is the MCP client, as you can see here.\nMCP client then calls into the MCP server, um makes the, which then again runs a different uh Pydantic AI uh agent, which then makes a call to an LLM, which happens through proxing it through the client.\nSo that's where you can see the service going, client server, uh client server, ultimately, if you look at the top level uh exchange with the model, you'll see here, yeah, the the the out ultimate output was, it return the the return response from running the query was was this kind of XMLish data, and then the LLM was able to turn that into a human description of what was going on.\nI think the other interesting thing probably is we can go and look in, we should be able to see the actual SQL that was called.\nSo this is the agent call inside uh MCP server, and you can see here the SQL it wrote, and you can confirm that it indeed looks correct.\nUm, I am going to uh go on from there and say, um, thank you very much.\nUm, we are at the booth, the the Pydantic booth.\nSo if anyone has any questions on this, wants to see this fail in numerous other exciting ways, very happy to to talk to you.\nYeah, come and say hi.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:24.643Z"
}