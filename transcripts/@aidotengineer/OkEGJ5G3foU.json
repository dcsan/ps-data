{
  "episodeId": "OkEGJ5G3foU",
  "channelSlug": "@aidotengineer",
  "title": "[Full Workshop] Reinforcement Learning, Kernels, Reasoning, Quantization & Agents â€” Daniel Han",
  "publishedAt": "2025-07-19T21:23:58.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 0.37,
      "duration": 13.68
    },
    {
      "lang": "en",
      "text": "Hello guys. Hello. Hello. Hello. Yes.",
      "offset": 14.719,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Sorry for being a bit late. There's a",
      "offset": 18.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "lot of traffic. But um hello. Um welcome",
      "offset": 20.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "to AI engineers worldfare. Thanks for",
      "offset": 23.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "coming to my session. Um, and yes, today",
      "offset": 25.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we're going to talk about the deep dive",
      "offset": 27.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "into RL kernels, agents, and",
      "offset": 29.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "quantization. Um, you might know me or",
      "offset": 31.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you might not know me, but I'm Daniel.",
      "offset": 33.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Uh, my brother is somewhere.",
      "offset": 35.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Uh, yeah, somewhere, but yes. Um, thanks",
      "offset": 38.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "for coming again. Oh, we also have",
      "offset": 41.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "stickers and other like random stuff",
      "offset": 43.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "later. Um, that's after the talk. Um, so",
      "offset": 45.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "maybe you might know me, maybe you might",
      "offset": 48.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "not. Um, so we, um, on Twitter, we tweet",
      "offset": 50.399,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "a lot. Um we did like a gradient",
      "offset": 53.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "accumulation bug fix last year. Um we",
      "offset": 55.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "introduced something called async",
      "offset": 58.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "offloaded gradient checkpointing. Um we",
      "offset": 59.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "also work with the hugging face Google",
      "offset": 61.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Meta Mistro teams to like fix bugs and",
      "offset": 63.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "like their open source models like",
      "offset": 65.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Gemma, Llama, Mistro fee and more. Um",
      "offset": 66.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "yeah so like we if you want to follow on",
      "offset": 69.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the latest stuff of AI better follow us.",
      "offset": 72.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Um we tweet about random stuff. Um you",
      "offset": 74.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "might even know when next models might",
      "offset": 77.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "be released. You know we sometimes tell",
      "offset": 78.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "people approximately. Um so that might",
      "offset": 80.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "be very interesting. Um",
      "offset": 82.64,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "we also do like open source",
      "offset": 85.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "contributions to the entire open source",
      "offset": 86.479,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "ecosystem. For example, we contribute",
      "offset": 88.4,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "sometimes to Llama CBP. Um we work with",
      "offset": 90.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the Quen team and Mistro on their",
      "offset": 92.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "releases. Um we also do like you know F4",
      "offset": 94.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "bug fixes, Llama 4 bug fixes which",
      "offset": 96.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "increase accuracy by a bit. Um so",
      "offset": 99.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "definitely you know utilize some of the",
      "offset": 101.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "new newer uploads that we do um which",
      "offset": 104.159,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "fix bugs all the time.",
      "offset": 106.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "We just surpassed 10 million monthly",
      "offset": 108.479,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "downloads and hugging face. Um, and",
      "offset": 110,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "yeah, we also have a GitHub package um",
      "offset": 111.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "with with 40,000 GitHub stars. Um, and",
      "offset": 114.399,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "we do like essentially we make fine",
      "offset": 116.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "tuning faster and reduce memory usage.",
      "offset": 117.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And yeah, that's the GitHub package.",
      "offset": 121.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Definitely check that out. Um, there's",
      "offset": 122.88,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "like free collab notebooks. I'm not sure",
      "offset": 124,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "if many people know, but you have like",
      "offset": 125.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "free GPUs that Google offers. You can",
      "offset": 127.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "just use them. Um, please use them more.",
      "offset": 129.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Um, and Kaggle, if not many people know,",
      "offset": 131.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "have 30 hours for free of GPUs per week,",
      "offset": 134,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "right? And there's no restrictions on",
      "offset": 136.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it. You know, please utilize the free",
      "offset": 137.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "resources as much as possible. Um, yes,",
      "offset": 140.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "they won't be unhappy. Just use them",
      "offset": 143.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "all. So, yes, we have notebooks. So, if",
      "offset": 144.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you scroll down a bit on our GitHub uh",
      "offset": 147.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "page, there's like free notebooks um for",
      "offset": 148.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Collab, Kaggle. We do reasoning, um,",
      "offset": 150.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "continue pre-training, supervised",
      "offset": 153.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "fine-tuning, and other stuff.",
      "offset": 155.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "We also upload models to our hunting",
      "offset": 158.08,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "case page. Um, for example, DC R10528",
      "offset": 159.68,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "released a few days ago. We upload 1.58",
      "offset": 162.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "bit quants which are like very small.",
      "offset": 165.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "They retain most of the accuracy. Um so",
      "offset": 167.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like these can run on your local device.",
      "offset": 169.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Um even if you have like very low VRAM",
      "offset": 171.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "or like not a very good GPU um it will",
      "offset": 173.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "still work and we will constantly upload",
      "offset": 175.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "models. So like sometimes people",
      "offset": 178.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "complain to us you know please stop",
      "offset": 180.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "uploading fixed models. You know it's",
      "offset": 181.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "kind of annoying. Um but too bad you",
      "offset": 183.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know unfortunately models have bugs. So",
      "offset": 185.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we do have to fix them immediately. You",
      "offset": 187.44,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "will see sometimes for example the",
      "offset": 189.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "accuracy can increase by 10%. Um,",
      "offset": 190.879,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "sometimes, you know, the large model",
      "offset": 193.44,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "providers won't tell you that they",
      "offset": 194.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "uploaded a fix. They're not going to",
      "offset": 196.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "tell you. But, you know, be sure to",
      "offset": 197.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "download the latest models. You will get",
      "offset": 199.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "all the fixes.",
      "offset": 201.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Now, today, let's start off from",
      "offset": 203.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "history, right? Does everyone remember",
      "offset": 205.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Llama? Um,",
      "offset": 207.36,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "although finally it got leaked, right?",
      "offset": 209.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "It was just a research paper, you know,",
      "offset": 211.599,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "meta saying, \"Oh, we trained Llama.",
      "offset": 213.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "Where's the weights?\" You know, it's",
      "offset": 215.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "only research access. And then suddenly",
      "offset": 216.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it got leaked. And then you kind of just",
      "offset": 218.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "kind of spawn the entire open source",
      "offset": 220.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "movement. Um you know some of the people",
      "offset": 221.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "who are the who are the authors you know",
      "offset": 223.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "are not part of meta anymore but llama",
      "offset": 225.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is extremely important for the entire",
      "offset": 228.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "ecosystem and it was like the beginning",
      "offset": 229.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of open source kind of um for large",
      "offset": 231.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "language models.",
      "offset": 233.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "The most famous plot from the paper is",
      "offset": 235.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "this right? So like you know if you keep",
      "offset": 237.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "making the model train more it just the",
      "offset": 240.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "loss just keeps going down. Um well the",
      "offset": 242.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "question is will the loss keep going",
      "offset": 245.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "down? That's the question right? So",
      "offset": 246.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Llama 1 was only trained at 1.4 trillion",
      "offset": 248.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tokens, right? So now 1.4 trillion",
      "offset": 250.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "tokens is actually very less. Most",
      "offset": 252.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "models are trained 10 times more. Um and",
      "offset": 254.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "you can also see from the trend that the",
      "offset": 256.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "bigger the model, the lower the loss,",
      "offset": 257.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "right? So 7 billion is the blue line. Um",
      "offset": 259.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and then 65 billion was like the red",
      "offset": 261.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "line. And you can see that in general as",
      "offset": 262.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a model gets bigger, it it gets smarter.",
      "offset": 264.56,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Um the training loss, the numbers are",
      "offset": 267.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "correct. So you should see generally",
      "offset": 270.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "these numbers from around maybe a bit",
      "offset": 271.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "higher than one. Um if you see training",
      "offset": 273.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "losses when you do fine tuning of like 8",
      "offset": 275.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "13 definitely something's wrong. Um, so",
      "offset": 277.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you should get at least losses around",
      "offset": 279.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "twoish, three-ish.",
      "offset": 281.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "And so now,",
      "offset": 283.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "oh, now Google, um, you know, Google's",
      "offset": 286,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "new Jamma 3 models are trained on 14",
      "offset": 288.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "trillion tokens, right? Which is much",
      "offset": 290.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "more. Um, Llama 4 is trained on 30",
      "offset": 292,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "trillion tokens, right? So like",
      "offset": 294.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "literally like at least 10 times more.",
      "offset": 296,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Um, so Jamma is 10 times more. Llama is",
      "offset": 298,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "like, you know, 30 times more.",
      "offset": 300.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Oh yes, I forgot. You can also access",
      "offset": 303.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the slides by the QR code if you want.",
      "offset": 305.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "It's also on the docs as well. Um so if",
      "offset": 307.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "you go to the docs there will be a link",
      "offset": 309.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to all the slides. Um I will probably",
      "offset": 310.4,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "post the slides anyways on Twitter and",
      "offset": 312.16,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "elsewhere so you can access them as",
      "offset": 313.759,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "well.",
      "offset": 315.039,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Also if there are questions for people",
      "offset": 319.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like you know raise your hand ask I will",
      "offset": 321.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "essentially do intermission between like",
      "offset": 324.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "some parts and I will ask people if you",
      "offset": 326.16,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "have questions please ask. Um last time",
      "offset": 327.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the talk many people asked questions I",
      "offset": 329.919,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "will answer every single one even if",
      "offset": 331.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it's stupid I don't care please ask. Um,",
      "offset": 332.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "sometimes I get stuff wrong. So, yes,",
      "offset": 334.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "just ask questions. Um, are there any",
      "offset": 337.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "questions? I'm I'm assuming no. Okay.",
      "offset": 340,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Okay. Let's go to the Okay, I will.",
      "offset": 343.039,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Okay. So, I I don't know if people have",
      "offset": 345.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "seen this plot very famous from Maxim.",
      "offset": 347.759,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Um, he shows the open source versus",
      "offset": 350.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "closed source performance of popular",
      "offset": 352.4,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "benchmarks. I think this is MLU5 shot.",
      "offset": 354.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Um, you can see the green line is open",
      "offset": 356.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "source models. Um, and then the red line",
      "offset": 358.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "or the orange line is like, you know,",
      "offset": 360.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "closed source models. Um and you can see",
      "offset": 362.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that in general the slope of the open",
      "offset": 364.319,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "source models is more dramatic than the",
      "offset": 366.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "closed source models. And I would say in",
      "offset": 368.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "general that you know the open source",
      "offset": 370.639,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "models and closed source models in terms",
      "offset": 372.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of MLOU they've kind of like reached the",
      "offset": 374.319,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "same accuracy right so like you can see",
      "offset": 377.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that okay this is already outdated but",
      "offset": 379.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "in general you see like llama 3.1 405",
      "offset": 381.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "billion kind of reached you know GP40",
      "offset": 383.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "level um so open source models",
      "offset": 385.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "definitely have caught up to closed",
      "offset": 387.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "source models",
      "offset": 389.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "however there was a however um recent",
      "offset": 391.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "you know like since September 2024 I",
      "offset": 394.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "would call this something called the o",
      "offset": 397.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "opensource drought. Um, no, no one wants",
      "offset": 398.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to talk about it, but I will, right? So,",
      "offset": 401.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like September 2024, 01 got released, 01",
      "offset": 402.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "preview. And to be honest, the open",
      "offset": 405.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "source community was shocked, right? So,",
      "offset": 407.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like suddenly the capabilities diverged,",
      "offset": 408.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? So, there's something called the",
      "offset": 411.84,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "MLU plateau where most models, the open",
      "offset": 412.72,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "source models and the closed source",
      "offset": 414.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "models, they kind of converged. So, the",
      "offset": 415.759,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "open source models was equivalent to the",
      "offset": 418.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "closed source models. But suddenly in",
      "offset": 419.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "2024 September you know OpenAI released",
      "offset": 421.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "01 preview and it kind of shocked the",
      "offset": 424.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "entire community because the capability",
      "offset": 426.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "or intelligence kind of skyrocketed",
      "offset": 428.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "right so like with reasoning long",
      "offset": 430.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "reasoning traces it just was a total",
      "offset": 432,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "change of mindset um and for four months",
      "offset": 434.479,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "the open source community kind of died",
      "offset": 437.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "internally because there was nothing we",
      "offset": 439.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "can't replicate it we don't know what to",
      "offset": 441.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "do you know do we do this do we do that",
      "offset": 442.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I don't know like and so like but then",
      "offset": 444.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "suddenly in 2025 January deepseek R1",
      "offset": 446.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "came along and they released R1 and",
      "offset": 449.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that's when the entire world kind of",
      "offset": 451.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "changed their view right so like you can",
      "offset": 453.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "in fact train open source models to be",
      "offset": 454.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "as powerful as 01 or 03 or whatever",
      "offset": 456.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right so like that's that was what I",
      "offset": 459.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "call the open source drought",
      "offset": 460.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "however there was a previous drought",
      "offset": 464.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "even before that remember when chbt got",
      "offset": 466.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "released in 2022 December right so like",
      "offset": 468.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "before even chbttt most models were base",
      "offset": 471.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "models um they were not really instruct",
      "offset": 474.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "fine-tuned that well and so most large",
      "offset": 475.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "pre-trained models were actually useless",
      "offset": 478.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but they were terri But then suddenly",
      "offset": 479.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "CHP came along and they did better you",
      "offset": 481.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "know reinforcement learning from human",
      "offset": 484,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "feedback better instruction fine-tuning",
      "offset": 485.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "better instruction following and it",
      "offset": 487.28,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "really changed the world right so like I",
      "offset": 489.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "think large lang large large language",
      "offset": 490.879,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "models were already here before 2022",
      "offset": 492.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "right it they were already there but it",
      "offset": 494.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "was just chipd which showed that if you",
      "offset": 496.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "have good data right good instructions",
      "offset": 498.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "good answers good supervised fine-tuning",
      "offset": 501.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "um and good reinforcement learning you",
      "offset": 504.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "can actually make the model very useful",
      "offset": 505.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "um and yes again open source had a delay",
      "offset": 507.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a very long delay right until like llama",
      "offset": 509.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "one I guess um and so always I would say",
      "offset": 511.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "open source always tries to catch up to",
      "offset": 514.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the closed source models um the next",
      "offset": 516,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "question is what is after reasoning um",
      "offset": 517.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "is there going to be something else um I",
      "offset": 520.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "think that's a very good question my",
      "offset": 523.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "personal take is it's going to be very",
      "offset": 525.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "hard um I think reasoning was like the",
      "offset": 526.959,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "last most the DC R1 paper said that most",
      "offset": 528.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "likely the model already has these",
      "offset": 532.08,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "reasoning capabilities and we just need",
      "offset": 533.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to accentuate them um and so like I'm",
      "offset": 535.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "not sure if there's going to be some new",
      "offset": 538.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you know like step function where we'll",
      "offset": 540.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "get to like the next capability um but",
      "offset": 541.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "in my view I think like every single",
      "offset": 543.68,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "time the closed source models will",
      "offset": 545.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "always do like a step function um but",
      "offset": 546.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "who knows maybe now it will plateau",
      "offset": 548.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "forever I don't know so that's like you",
      "offset": 550.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "have like you know you have like long",
      "offset": 552.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "discussions about you know if AGI is",
      "offset": 554,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "going to come or not like but who knows",
      "offset": 555.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "um the talk is not going to be about",
      "offset": 557.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "that but um yes next um so I call the",
      "offset": 558.48,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "first jump the SFT or um RHF jump right",
      "offset": 562.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so like that's essentially if you do",
      "offset": 566,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "good supervised fine tuning you get this",
      "offset": 567.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "large jump in performance and then the",
      "offset": 568.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "second jump is called the RL jump right",
      "offset": 570.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so like this essentially can increase",
      "offset": 571.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "performance dramatically if you employ",
      "offset": 573.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "second methodologies like RL right so",
      "offset": 575.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like but the question is like what's the",
      "offset": 577.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "next jump um I don't know",
      "offset": 578.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so I'm not sure if you guys saw this",
      "offset": 582.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "picture before um it's very widely known",
      "offset": 584,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "in the community about you know by Yan",
      "offset": 587.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Lakun he essentially showed this cake um",
      "offset": 589.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and essentially unsupervised learning or",
      "offset": 591.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like just pre-training in general um you",
      "offset": 593.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "know it's just a cake not that good um",
      "offset": 595.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and then supervised fine-tuning is kind",
      "offset": 597.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of the icing on on top of the cake. So",
      "offset": 599.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "like it's a bit better. Um and then the",
      "offset": 601.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "reinforcement learning is the cherry,",
      "offset": 603.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "right? So like I'm not sure people like",
      "offset": 604.399,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "the cherry, but like some people like",
      "offset": 606,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the cherry. Um and so like the goal is",
      "offset": 607.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "how can we get the cherry? Um but the",
      "offset": 608.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "problem is there's so less data about",
      "offset": 611.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this, right? Reinforcement learning is",
      "offset": 612.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "like very very very less data. And so",
      "offset": 614.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the problem is most large model labs",
      "offset": 616.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "will train these large pre-trained",
      "offset": 618.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "models and then they will iteratively",
      "offset": 620.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "refine it to make the model better",
      "offset": 621.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "through supervised learning, through",
      "offset": 623.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "reinforcement learning.",
      "offset": 625.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Interestingly enough, this slide was",
      "offset": 627.279,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "actually shared last year. Very popular.",
      "offset": 628.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "But actually, this was from 2016",
      "offset": 630.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "September. Um, so I had to dig this up",
      "offset": 632.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on YouTube. And so Yan Lakun actually",
      "offset": 634.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "talked about this back in 2016. So",
      "offset": 636.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "literally nearly 10 years ago. Um, I was",
      "offset": 638.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like, how wait a second, that's 10 years",
      "offset": 641.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "ago. Very long. Um, so this slide was",
      "offset": 642.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "actually very popular on Twitter. I I",
      "offset": 645.6,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "think it was in November last year.",
      "offset": 647.36,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "People kept tweeting about it. I don't",
      "offset": 648.399,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "know. I saw this, I was like shocked.",
      "offset": 649.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Um, but yes, so this encapsulates like",
      "offset": 650.88,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "the current AI um, boom.",
      "offset": 654.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And so like firstly like when we talked",
      "offset": 657.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "about these large models remember they",
      "offset": 660.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "started from a base model. Um and so we",
      "offset": 661.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "call these training stages right so when",
      "offset": 664,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you have a base model um you then",
      "offset": 665.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "convert it to a chat model right so like",
      "offset": 667.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "for example chat GBT is not a base model",
      "offset": 669.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it is a instruct fine-tune model or like",
      "offset": 671.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "some sort of fine-tuned model from a",
      "offset": 673.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "base model. So actually openAI does have",
      "offset": 675.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "a base model somewhere sitting in their",
      "offset": 677.279,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "server somewhere. um they're probably",
      "offset": 678.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "not going to serve it ever, but it is",
      "offset": 680.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "somewhere on the computers and they",
      "offset": 681.839,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "essentially fine-tuned it to make Chai",
      "offset": 683.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "GP4. Um Claude 4 has most likely a base",
      "offset": 685.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "model and then they fine-tune it to",
      "offset": 688.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "become Opus, right? So like Gemini also",
      "offset": 690,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "has a base model and they convert it to",
      "offset": 692.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Gemini 2.5 Pro. So this this phase when",
      "offset": 693.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "you convert a base model to a chat model",
      "offset": 696.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is the finetuning phase. Um and then the",
      "offset": 698.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "question is like you know what do we do",
      "offset": 701.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "in the arrow right like you know is it",
      "offset": 703.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "reinforcement learning is it supervised",
      "offset": 704.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "fine tuning is it like some other",
      "offset": 706.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "special source I don't know but like we",
      "offset": 707.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "essentially we'll discuss about these um",
      "offset": 709.68,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "topics um any questions first",
      "offset": 711.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 716.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so for example in open source models you",
      "offset": 718.399,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "might have seen gemma 3 pt um gemma 3 it",
      "offset": 720.32,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "llama 4 llama 4 instruct quen 3 base",
      "offset": 725.279,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "quen 3 um M small base M small instruct",
      "offset": 728.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "llama 2 llama 2 chat right these like",
      "offset": 731.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "terminologies to be honest I think the",
      "offset": 733.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "open source community should standardize",
      "offset": 735.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a method like terminologies like",
      "offset": 736.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "instruct or chat or you know no not even",
      "offset": 738.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a word like you know it and PT maybe",
      "offset": 741.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "they should like standardize it a bit um",
      "offset": 743.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but in general if you see it it means",
      "offset": 745.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "instruct instruction tuned PT means",
      "offset": 747.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "pre-trained um instruct just means you",
      "offset": 749.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "know instruction fine-tuned quen 3 just",
      "offset": 751.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "removed it entirely it's just called",
      "offset": 753.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "quen 3 um and then the base model is",
      "offset": 755.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "called with a base um and so like",
      "offset": 757.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "essentially these naming methodologies",
      "offset": 759.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "um if you see on hugging face um",
      "offset": 761.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "hopefully you will now recognize these",
      "offset": 763.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "different types of um models",
      "offset": 765.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and so generally what we say for like",
      "offset": 769.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you know reinforcement learning and",
      "offset": 772.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "finetuning is I would say it's called re",
      "offset": 773.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "fine tuning is everywhere um you start",
      "offset": 775.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "off with pre-training you then convert",
      "offset": 776.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "it into a supervised fine-tuning model",
      "offset": 778.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "via supervised fine tetuning SFT it's",
      "offset": 780.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "called SFT you also might hear like IF",
      "offset": 782.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which is instruction fine tuning they're",
      "offset": 784.959,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the same thing um and then we call",
      "offset": 786.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "something post training, the post-",
      "offset": 788.079,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "training phase. Um, but actually",
      "offset": 789.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "recently it actually kind of changed.",
      "offset": 791.12,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "Um, so I don't know if you guys have",
      "offset": 792.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "been keeping up with the latest stuff,",
      "offset": 794.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "terminology. Um, I actually don't really",
      "offset": 796,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "like terminology anymore, but like we",
      "offset": 797.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "have something called pre-training,",
      "offset": 799.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "which is you take like all of Wikipedia,",
      "offset": 800.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "all of the web, you know, everything,",
      "offset": 802.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "all of the data you can ever see, shove",
      "offset": 803.839,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it into the model, predict the next",
      "offset": 805.44,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "word. That's called the pre-training uh",
      "offset": 806.959,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "pre-training stage. We then have",
      "offset": 808.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "something called the midtraining stage",
      "offset": 810.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "um which essentially gives you higher",
      "offset": 812.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "quality data. Like for example, you can",
      "offset": 814,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "weight Wikipedia more because it's",
      "offset": 815.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "higher quality. Um you can essentially",
      "offset": 817.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "do long context extension as well. You",
      "offset": 819.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "shove this during the mid mid pre-tra uh",
      "offset": 821.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "mid-training stage. So if your context",
      "offset": 823.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of your model is very short, you want to",
      "offset": 825.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "extend it to very long context. You",
      "offset": 827.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "shove this during the mid-training",
      "offset": 829.279,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "stage. And then the second stage is the",
      "offset": 830.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "supervised fine-tuning stage where you",
      "offset": 832.639,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "want to convert the model to a chat",
      "offset": 834.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "model. And then we have the",
      "offset": 836.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "post-training phase which is like pre",
      "offset": 837.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "preference fine-tuning, DPO, RHF and",
      "offset": 839.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "stuff like that. And then we have this",
      "offset": 842.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "new thing called reinforcement",
      "offset": 844.639,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "fine-tuning or RLVR. Uh if no one knows",
      "offset": 846.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "what RLVR stands for, it stands for",
      "offset": 850.16,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "reinforcement learning with verifiable",
      "offset": 851.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "rewards. And this is like a new paradigm",
      "offset": 853.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "um not the same as preference fine",
      "offset": 856.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "tuning or DPO where we consider reward",
      "offset": 858.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "functions to make models much better. Um",
      "offset": 860.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and so this is how I would envision like",
      "offset": 862.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you know the whole training phases of",
      "offset": 865.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "models.",
      "offset": 866.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Another way to put it is we have some",
      "offset": 871.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "random initialization of the model like",
      "offset": 873.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "some random weights of the model right",
      "offset": 875.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "so like seven billion parameters",
      "offset": 876.959,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "literally random numbers right like GPT4",
      "offset": 878.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "GP4 I don't know 1.4 trillion parameters",
      "offset": 881.199,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "like just random numbers and then",
      "offset": 883.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "somehow we move in the space like the",
      "offset": 884.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "black line right so pretend this is like",
      "offset": 886.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "some highdimensional 1.4 for trillion",
      "offset": 888.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "dimensional space and then we somehow",
      "offset": 890.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "move in this space and then we get the",
      "offset": 892,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "final model. Right? That's a green job.",
      "offset": 894.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "The question is how do we move in this",
      "offset": 895.839,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "space to get to the final model? That's",
      "offset": 897.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the question.",
      "offset": 899.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Most people what they do is firstly you",
      "offset": 901.279,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "start from a random initialization. You",
      "offset": 903.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "do the pre-training phase which is very",
      "offset": 905.199,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "long. You get to this dark blue dot,",
      "offset": 906.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "right? That's called the pre-trained",
      "offset": 909.279,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "model. And then you do some supervised",
      "offset": 910.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "fine-tuning, instruction fine-tuning to",
      "offset": 912.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "get the blue dot. And notice the line",
      "offset": 914.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "for the light blue line is very short",
      "offset": 917.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "because it is very short, right? There",
      "offset": 918.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "is not that much data for supervised",
      "offset": 920.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "finetuning. And then somehow we get the",
      "offset": 921.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "blue dot and then we keep doing more",
      "offset": 923.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "iteration to get to the purple dot which",
      "offset": 925.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "is through preference finetuning. And",
      "offset": 927.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then finally we get the the green dot",
      "offset": 929.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "which is reinforcement learning via you",
      "offset": 931.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "know verifiable rewards like you know 03",
      "offset": 933.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "or 01. And so like the goal is somehow",
      "offset": 935.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we have to move from the black dot to",
      "offset": 938.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the green dot. And essentially all of",
      "offset": 940.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "large language models all of AI is just",
      "offset": 942.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "an optimization problem right? like how",
      "offset": 943.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "do we make this easier to get to the",
      "offset": 945.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "green dot? You could, you know, you",
      "offset": 947.519,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "could kind of theoretically guess you",
      "offset": 950.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "could just why don't you just go from",
      "offset": 952.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the green dot black dot to the green dot",
      "offset": 954,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like skipping all of the dumb phases,",
      "offset": 956.24,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "you know, just skip it entirely. Yes,",
      "offset": 957.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you could do that, but it's not going to",
      "offset": 959.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "be very efficient. You're going to be",
      "offset": 961.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "waiting there for like, you know, I",
      "offset": 963.839,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "don't know, millennia. Your loss is not",
      "offset": 964.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "going to go down. So, the tricks that we",
      "offset": 966.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "found in rein um AI is like you have to",
      "offset": 968.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "do these phases to get to your final",
      "offset": 970.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "green dot. Um there is like a new",
      "offset": 972.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "methodology where you can actually",
      "offset": 974.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "bypass this supervised finetuning stage",
      "offset": 975.92,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and the preference fine tuning stage and",
      "offset": 978.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "directly go to the green dot. There is a",
      "offset": 979.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "way and that's the dark red line. Um I",
      "offset": 981.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "think deepse uh deepseek zero kind of",
      "offset": 983.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like showed that you can use a",
      "offset": 985.519,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pre-trained model a base model and",
      "offset": 987.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "directly do some reinforcement learning",
      "offset": 989.199,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "with verifiable rewards and just skip it",
      "offset": 991.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "entirely. Um so that's like a new",
      "offset": 993.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "paradigm that people want to focus on.",
      "offset": 994.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "In my view I think you should still do",
      "offset": 997.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the light blue, the purple and then the",
      "offset": 998.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "green. I don't think so you should like",
      "offset": 1001.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "directly skip over to the green. If you",
      "offset": 1002.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "want to waste resources, you can skip to",
      "offset": 1004.959,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "the green. Um but I don't think so large",
      "offset": 1006.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "model labs want to waste resources. Um",
      "offset": 1008.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "hopefully not.",
      "offset": 1010.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So I I don't know if people have seen",
      "offset": 1013.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "this diagram. Agents in the old sense",
      "offset": 1015.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "like everyone keeps connecting agents",
      "offset": 1017.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "with reinforcement learning. Okay, but",
      "offset": 1019.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like why? Um so in general an agent is",
      "offset": 1021.12,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "you have some sort of environment. You",
      "offset": 1024.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "have like the agent doing something in",
      "offset": 1026.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the environment. you get like an action,",
      "offset": 1027.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "you do the action and then you get some",
      "offset": 1030,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "some sort of reward. Um, and the reward",
      "offset": 1031.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "is R. S is the state. So the current",
      "offset": 1034.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what the environment currently looks",
      "offset": 1037.039,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like. Um, and then essentially RL tries",
      "offset": 1038.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to optimize this loop. You're trying to",
      "offset": 1041.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "maximize a reward. Um, giving some sort",
      "offset": 1044.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of action. Um, and that's why like you",
      "offset": 1046.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "know that's why RL and agents are kind",
      "offset": 1048.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "of connected. Assume the agent is the",
      "offset": 1050.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "lang language model, right? So assume",
      "offset": 1052.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "the agent is in fact the language model.",
      "offset": 1054.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And so this the environment is kind of",
      "offset": 1055.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "fishy like you know it's it's hard to",
      "offset": 1058.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "say what exactly is the environment it's",
      "offset": 1060.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "more like the language models inference",
      "offset": 1061.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "space that's the environment kind of um",
      "offset": 1064,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but like pretend this was a game right",
      "offset": 1066.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so like the agent was the computer the",
      "offset": 1068.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "environment is like Mario for example",
      "offset": 1071.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "right and so like you're playing the",
      "offset": 1073.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Mario game automatically and your goal",
      "offset": 1074.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "is to win the game and so like the whole",
      "offset": 1076.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "goal of RL is to maximize reward",
      "offset": 1078.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "another one is like Pac-Man right so",
      "offset": 1082.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like you have the yellow Pac-Man And um",
      "offset": 1084.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "you can either go up, down, left or",
      "offset": 1086.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "right. Right. So like up, down, left or",
      "offset": 1087.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "right. That is the action. That's the a",
      "offset": 1089.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the yellow the I think I don't know",
      "offset": 1092.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "orange or something. The orange little",
      "offset": 1094.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "things are like rewards, right? So like",
      "offset": 1096.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "if you eat if you eat you know a yellow",
      "offset": 1098.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "a red an orange dot you will get",
      "offset": 1101.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "positive reward for. So R plus right? So",
      "offset": 1103.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like you have R pluses. If you eat a",
      "offset": 1106.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "very big one you'll get like very large",
      "offset": 1107.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "reward. But if you encounter one of the",
      "offset": 1109.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "enemies you will get minus reward,",
      "offset": 1111.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "right? All right. So like the qu the the",
      "offset": 1113.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "question is how do we maximize the",
      "offset": 1115.039,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "reward based on this environment",
      "offset": 1117.28,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "for language models there is a trick the",
      "offset": 1121.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "trick is this loop kind of changes",
      "offset": 1123.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "because we don't actually have a",
      "offset": 1127.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "continuous loop um the state does not",
      "offset": 1128.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "actually change over time right so like",
      "offset": 1131.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for example in a game in a game if you",
      "offset": 1132.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do an action the whole state changes",
      "offset": 1134.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "right the environment totally changes",
      "offset": 1136.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and so you have to like continuously",
      "offset": 1138.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "keep a history of the past steps but in",
      "offset": 1139.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "language models there is no history,",
      "offset": 1142.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "right? So like if you do a prompt, what",
      "offset": 1143.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is 2 plus 2? If you ask another",
      "offset": 1145.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "question, what is 4 plus4? It's like",
      "offset": 1147.919,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "totally not relevant to your previous",
      "offset": 1149.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "prompt. Okay, well fine, it is kind of",
      "offset": 1150.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "relevant, but you like it's not directly",
      "offset": 1152.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "correlated. And so like you can actually",
      "offset": 1154.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "delete one of the lines. Um that's the",
      "offset": 1156.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "next prompt. You can delete it entirely.",
      "offset": 1158.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "So for example, what is 2 plus two?",
      "offset": 1161.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Right? So like essentially you have all",
      "offset": 1163.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of these options. It could be zero, it",
      "offset": 1164.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "could be one, it could be two, it could",
      "offset": 1167.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "be infinity, it could be B, it could be",
      "offset": 1169.039,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "D. I don't know. It could be anything",
      "offset": 1170.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that you like. a symbol and so the what",
      "offset": 1171.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is 2 plus two is the state. So like what",
      "offset": 1175.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is the question that's the question",
      "offset": 1177.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "right? Like the question the reward for",
      "offset": 1178.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "example if you choose the one if you",
      "offset": 1181.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "choose four your reward is plus one. If",
      "offset": 1182.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "you choose anything else your reward",
      "offset": 1185.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "might be negative infinity zero whatever",
      "offset": 1186.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "number you like. You can come up with",
      "offset": 1189.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "any number you like for reward. It",
      "offset": 1190.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "doesn't have to be plus one. It can be",
      "offset": 1192,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "plus 10. It can be plus 100 and you can",
      "offset": 1193.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "do anything that you like. Um you can",
      "offset": 1196.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "also do distance based scoring. For",
      "offset": 1198.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "example is you know is choosing the",
      "offset": 1200.24,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "number five better than zero? So that's",
      "offset": 1203.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "a question. What what do you guys think?",
      "offset": 1207.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Is choosing the number five better than",
      "offset": 1208.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "zero or is it worse?",
      "offset": 1210.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "I guess better.",
      "offset": 1212.72,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "Yes. Okay. Better. So what would you do",
      "offset": 1213.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "for the reward then? Pretend the model",
      "offset": 1215.039,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "outputs five for what is 2 plus two?",
      "offset": 1216.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Zero.",
      "offset": 1221.36,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "Zero. Someone said zero.",
      "offset": 1222,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Okay. Zero is fine because it's wrong.",
      "offset": 1223.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Yes. Like if you the answer five is",
      "offset": 1226.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "wrong. So you should probably give a",
      "offset": 1228.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "reward of zero. But is there a better",
      "offset": 1229.44,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "answer?",
      "offset": 1230.96,
      "duration": 1.839
    },
    {
      "lang": "en",
      "text": "Less than one.",
      "offset": 1232.08,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Okay. Yes. Less than one. So like some",
      "offset": 1232.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "sort of like maybe 0.8. I don't know.",
      "offset": 1234.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Right. So correct. So like you could do",
      "offset": 1236.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like the answer divided by the correct",
      "offset": 1237.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "answer, right? So like if it's five, you",
      "offset": 1240.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "divide it by four. You could do some",
      "offset": 1241.6,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "sort of Okay, no, that's wrong. It's",
      "offset": 1242.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "five minus four divided by four. Um so",
      "offset": 1244.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like something some reward like that. Um",
      "offset": 1246.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pretend if your pretend if the model",
      "offset": 1248.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "says a what is a reward?US",
      "offset": 1250.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "minus one. Okay. Or could be minus 10",
      "offset": 1253.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "because that's very bad, right? You you",
      "offset": 1255.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "should not output a letter. It should be",
      "offset": 1256.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "some sort of number. So that's how you",
      "offset": 1258.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "design reward functions, right? You you",
      "offset": 1261.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we just design a reward function. shove",
      "offset": 1262.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this into you know take your reward",
      "offset": 1264.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "function it's like just if statements",
      "offset": 1266.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "shove this into a language model",
      "offset": 1269.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "fine-tuning phase and there we go you",
      "offset": 1270.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have 03 okay well you won't have 03 but",
      "offset": 1272.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you know what I mean um essentially 03",
      "offset": 1274.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is a collection of all these reward",
      "offset": 1276.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "functions right so like for example this",
      "offset": 1278.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "what is 2 plus2 is one question remember",
      "offset": 1280.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "it doesn't have to be what is 2 plus2 it",
      "offset": 1282.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is a general maths question what is 10 +",
      "offset": 1284.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "20 what is 10 * 200 / 10 you know",
      "offset": 1287.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "whatever maths equation you ever want",
      "offset": 1290.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and This function can take your question",
      "offset": 1292.799,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and convert it into a number. And 03 is",
      "offset": 1296.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "just a collection of all of these reward",
      "offset": 1299.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "functions.",
      "offset": 1300.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "And so the goal of RL is to make the",
      "offset": 1303.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "good ones more good. You want the good",
      "offset": 1306.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "rewards increase in value. So for",
      "offset": 1308.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "example, the four, you want the four to",
      "offset": 1310.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "appear more, but you want the three to",
      "offset": 1312.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "decrease. You know, you don't want the",
      "offset": 1314.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "three to keep appearing in your answer,",
      "offset": 1316,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "but you want the D and the B to be very,",
      "offset": 1318,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "very, very heavily penalized. That's the",
      "offset": 1320.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "goal of RL, right? So like we don't",
      "offset": 1322.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "actually have the answer, right? Okay,",
      "offset": 1324.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this this question is very easy. What is",
      "offset": 1325.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "2 plus2 obviously is four? Yes. Okay,",
      "offset": 1327.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "that's very easy. But pretend you have",
      "offset": 1328.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "some sort of complicated question like",
      "offset": 1330.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "for example, how do I win the stock",
      "offset": 1334,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "market? For example, let's dumb dumb",
      "offset": 1335.679,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "thing. Okay, how do I win the stock",
      "offset": 1337.039,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "market? You don't know what actions",
      "offset": 1338.24,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "you're going to take, right? Like but",
      "offset": 1339.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the point is you have the result, right?",
      "offset": 1340.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "You have the result like profit or loss.",
      "offset": 1342.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "But the question is we don't know how do",
      "offset": 1344.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "we get to the good profit. And so the",
      "offset": 1345.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "question is how do we maximize good",
      "offset": 1348,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "actions as much as possible and decrease",
      "offset": 1350.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "bad actions as much as possible and that",
      "offset": 1352.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is RL",
      "offset": 1354.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and so open AI released something called",
      "offset": 1357.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "RHF in uh chipt and they oh actually I",
      "offset": 1358.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "think it was instruct but anyways for",
      "offset": 1362.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "chipt they showed that you just need",
      "offset": 1364.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "some training data you need some data",
      "offset": 1367.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you interact with the agent which is the",
      "offset": 1369.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "language model you then get some actions",
      "offset": 1370.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "which is your answer to the language",
      "offset": 1374.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "model you feed this into reward model",
      "offset": 1376.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and then you get some reward and then",
      "offset": 1378.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you keep it you know iteratively doing",
      "offset": 1380.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this step and you'll finally get charg",
      "offset": 1382.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so remember the the base model you start",
      "offset": 1385.039,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "with so chat GBT base you convert this",
      "offset": 1387.52,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "into chat GP4 via this method",
      "offset": 1390.559,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "to expand on a bit if you guys have",
      "offset": 1395.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "heard of PO right what is PO um",
      "offset": 1397.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "essentially PO is just you just expand",
      "offset": 1400.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the box for the agent right so the",
      "offset": 1403.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "language model is like an agent you",
      "offset": 1404.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "expand it and there's just three models",
      "offset": 1406.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "inside of it. Um there is a generating",
      "offset": 1407.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "policy, the reference policy and then",
      "offset": 1409.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there's a value model. Um and that's all",
      "offset": 1411.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "not that special. Um we will talk about",
      "offset": 1414.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "each of these things separately. Um but",
      "offset": 1416.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "PO is just a optimization algorithm to",
      "offset": 1419.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "make RAHF work better.",
      "offset": 1421.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "GRPO which is the algorithm behind DSE",
      "offset": 1425.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "R1 smartly deletes one of the fact uh",
      "offset": 1427.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "deletes one of the things the value",
      "offset": 1430.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "model. It just gets rid of it. Um now",
      "offset": 1432.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "why would you delete it? We will talk",
      "offset": 1434.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "about this but the trick is if you",
      "offset": 1436.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "delete a model the value model you just",
      "offset": 1438.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "save parameters you save compute and",
      "offset": 1440.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's much more efficient right remember",
      "offset": 1442.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "each of these models is kind of like a",
      "offset": 1444.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "large language model right so like you",
      "offset": 1445.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "know pretend you're generating your",
      "offset": 1448,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "model is like already 1.4 trillion",
      "offset": 1449.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "parameters what are you going to do make",
      "offset": 1451.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "another 1.4 four trillion parameter",
      "offset": 1452.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "model for the value model. So we just",
      "offset": 1454.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "get rid of it, delete it. Um and that is",
      "offset": 1456.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "GPO. That's the only difference between",
      "offset": 1458.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Okay. Well, there's other differences,",
      "offset": 1459.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "but the biggest difference is we get rid",
      "offset": 1461.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "of the value model. Any questions?",
      "offset": 1463.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 1467.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Um so you talked about negative reward.",
      "offset": 1468.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 1472.08,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "It's confusing because in pre-training",
      "offset": 1472.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "isn't",
      "offset": 1474.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "probabilities.",
      "offset": 1476.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Well, it's like negative rewards, right?",
      "offset": 1478,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "And then I guess",
      "offset": 1480.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the phrase reward when it can be",
      "offset": 1482.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "positive or negative versus comparing it",
      "offset": 1484.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to pre-training where it's always",
      "offset": 1487.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "negative.",
      "offset": 1489.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Do you mean pre-training as in like the",
      "offset": 1491.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "negative log likelihood or like some",
      "offset": 1493.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "prob? So the when during pre-training",
      "offset": 1495.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the goal is to maximize probability.",
      "offset": 1496.96,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1498.96,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "So like you know you output a some",
      "offset": 1499.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "number from 0 to one the probability of",
      "offset": 1501.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the next word and you want to maximize",
      "offset": 1503.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "that for RL you want to maximize reward.",
      "offset": 1505.279,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "So if it's a negative reward, you still",
      "offset": 1508.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "want to maximize that. So if it's",
      "offset": 1511.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "negative 1, you just want to make this",
      "offset": 1513.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "negative one go away and you want to be",
      "offset": 1515.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "positive in the positive range. But also",
      "offset": 1516.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "rewards can actually be just negative,",
      "offset": 1518.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "right? So for example, if your your",
      "offset": 1520.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reward function can be -10 and negative",
      "offset": 1522.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "1. The good one is negative 1, the bad",
      "offset": 1524.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "one is -10. Your goal is to move towards",
      "offset": 1526.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "negative 1 as much as possible because",
      "offset": 1529.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "your goal is to maximize it. So I would",
      "offset": 1531.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "say the reward is a misnomer. You could",
      "offset": 1533.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "just add 10 to everything and then you",
      "offset": 1535.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "know the sc it scales the numbers away.",
      "offset": 1537.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um does that kind of make sense or",
      "offset": 1539.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "it does it question of nomenclature? I",
      "offset": 1541.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "think yeah most people I would say like",
      "offset": 1543.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to be honest they don't like negative",
      "offset": 1546.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "rewards actually in the RS space people",
      "offset": 1548.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "just like to do positive reward. I don't",
      "offset": 1549.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "know I like negative rewards. Um I I",
      "offset": 1551.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "feel like it's more for me it's more",
      "offset": 1553.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "intuitive. Um yeah any other questions?",
      "offset": 1555.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah.",
      "offset": 1559.12,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "So you've got your language model and",
      "offset": 1559.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "then you've got generating policy and",
      "offset": 1561.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "reference policy right. What what models",
      "offset": 1563.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "are being used there? Is it the same as",
      "offset": 1565.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "your language model or is that another",
      "offset": 1566.88,
      "duration": 1.679
    },
    {
      "lang": "en",
      "text": "model",
      "offset": 1568.159,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "trick? Yes, very good question. There",
      "offset": 1568.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "are some tricks that you can employ.",
      "offset": 1570.559,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Most people just make them at the same",
      "offset": 1571.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "model. The reference model is like the",
      "offset": 1573.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "beginning of the model. The generating",
      "offset": 1575.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "model is the model that you're updating.",
      "offset": 1576.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "So like essentially the reference model",
      "offset": 1578.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is like the base model. Okay, that's",
      "offset": 1579.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "probably not a good Okay, fine. Just",
      "offset": 1582,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "keep it as the base model. The base",
      "offset": 1583.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "model, the generating policy is actually",
      "offset": 1584.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "the model where you update it. So like",
      "offset": 1586.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the model that you update the the the",
      "offset": 1588.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "base models update. So every single time",
      "offset": 1590.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you get a base model, you update one,",
      "offset": 1592.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "update two, update three, that is the",
      "offset": 1593.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "generating policy, but we will talk",
      "offset": 1596.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "about this. So it's essentially the same",
      "offset": 1598.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model, but there is updates to the",
      "offset": 1600.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "model. So the reference policy is the",
      "offset": 1602.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "model that is not updated. The",
      "offset": 1605.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "generating policy is the model that is",
      "offset": 1606.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "actually updated, and they're both the",
      "offset": 1608.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "same model. So the language model, one",
      "offset": 1609.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "of them's updated, the other one's not",
      "offset": 1611.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "updated. But we will talk about that.",
      "offset": 1612.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um, yes.",
      "offset": 1614.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "So the actions, is it typically one",
      "offset": 1616.559,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "token or is it more tokens",
      "offset": 1619.279,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "full",
      "offset": 1623.039,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "that's a good question in the Pac-Man",
      "offset": 1627.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "case the action will be a string of",
      "offset": 1629.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "actions right so like you can go up then",
      "offset": 1631.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "down then left or right you know some",
      "offset": 1633.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "sort of like long history for in the",
      "offset": 1634.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "language model space we just generally",
      "offset": 1636.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "this is called single turn and",
      "offset": 1639.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "multi-turn generally speaking currently",
      "offset": 1640.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "single turn is what most people do it's",
      "offset": 1642.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "just one action so the action will be um",
      "offset": 1644.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "the action essentially is saying what is",
      "offset": 1648.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "2 plus two and the answer is four and",
      "offset": 1650.159,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that is the action the action is",
      "offset": 1651.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "actually the inference space so like",
      "offset": 1653.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "what is the actual chain of thought that",
      "offset": 1655.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "is the action um and so like it's just",
      "offset": 1656.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "one though but it is the total sum of",
      "offset": 1659.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the chain of thought so like if you have",
      "offset": 1661.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like what is 2 plus two I think the",
      "offset": 1663.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "answer is four you know let me do some",
      "offset": 1665.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "working out blah blah blah blah blah",
      "offset": 1667.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "blah blah that is the entire thing if",
      "offset": 1668.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "does that kind of make sense",
      "offset": 1670.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "okay yes",
      "offset": 1672.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "there was a clawed conversation around",
      "offset": 1675.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "how like to finish a poem's next They",
      "offset": 1677.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "have to like think ahead to the last",
      "offset": 1680.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "letter to the last word to match the",
      "offset": 1682.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "previous one. Do you think like reward",
      "offset": 1684.48,
      "duration": 9.079
    },
    {
      "lang": "en",
      "text": "focus next allows you to do that?",
      "offset": 1687.44,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "you could",
      "offset": 1695.919,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "understand",
      "offset": 1699.76,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I think for pre-training specifically",
      "offset": 1703.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "there are research p papers which show",
      "offset": 1705.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that actually pre-training doesn't just",
      "offset": 1707.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "predict the next word it does try to",
      "offset": 1708.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "predict many words ahead and so like yes",
      "offset": 1710.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "maybe reward model in reinforcement",
      "offset": 1713.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "learning you can see you can accent it",
      "offset": 1715.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "essentially accentuates a pre-training",
      "offset": 1718,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "behavior so maybe these behavior already",
      "offset": 1719.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "exists in the model we just see it more",
      "offset": 1721.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "often So maybe I would say that",
      "offset": 1723.6,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "I would say that the model itself it",
      "offset": 1727.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "already has this capability. We just",
      "offset": 1729.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "want to make it more obvious. And so",
      "offset": 1731.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "maybe the model already knows how to do",
      "offset": 1733.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that. It already knows it predicts 10",
      "offset": 1734.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "words ahead or 20 words ahead. It",
      "offset": 1736.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "already knows how to do that. But we",
      "offset": 1738.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just want to make them more obvious. I'm",
      "offset": 1739.679,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "not sure that answers. Would it be safe",
      "offset": 1741.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to say like so if it's a generation you",
      "offset": 1744.399,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "want like every last you want almost the",
      "offset": 1747.2,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "circuit for the last word?",
      "offset": 1750.48,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 1754.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I think for reinforcement learning",
      "offset": 1767.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "specifically yeah I guess yes. Um, I",
      "offset": 1768.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "think it essentially your goal is to",
      "offset": 1771.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "maximize a reward. And so like however",
      "offset": 1773.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "whatever way you try to get there, it's",
      "offset": 1776.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "different from generally pre-training.",
      "offset": 1778.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Pre-training is just maximizing next the",
      "offset": 1779.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "probability of the next word. But",
      "offset": 1781.52,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "reinforcement learning is you're trying",
      "offset": 1783.039,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "to maximize reward. The question is how",
      "offset": 1784.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "do you actually maximize a reward? Do",
      "offset": 1786.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you like do chain of thought? Do you do",
      "offset": 1787.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "what you describe like thinking about",
      "offset": 1789.679,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the next you know in the future or",
      "offset": 1790.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "something? I don't know. Like I mean the",
      "offset": 1792.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "question is like what is the reward",
      "offset": 1795.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "function actually doing? I don't know.",
      "offset": 1797.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "What is the language model actually",
      "offset": 1798.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "doing? I don't know. Um I don't know if",
      "offset": 1799.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that answers your question like it's to",
      "offset": 1802.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "be Yeah.",
      "offset": 1803.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Yes. Yeah.",
      "offset": 1805.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "I was I was curious about when you were",
      "offset": 1807.12,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "talking about you know the arithmetic",
      "offset": 1808.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "whether five is a better answer than say",
      "offset": 1809.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "yes or something",
      "offset": 1812.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and like given that there are these like",
      "offset": 1814.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "closed circuits between all these",
      "offset": 1816.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "different related you know mathematical",
      "offset": 1818.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "functions you can do on numbers in space",
      "offset": 1820.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like whether it is like in the",
      "offset": 1821.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "literature or in like the current state",
      "offset": 1824.88,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "of the art",
      "offset": 1826.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "better to train it so that a closer pred",
      "offset": 1827.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is more accurate or whether just saying",
      "offset": 1829.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "like the right answer is right and",
      "offset": 1831.36,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "everything else is wrong which in some",
      "offset": 1832.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "logical sense is true",
      "offset": 1833.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like tends to produce more performance",
      "offset": 1835.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in that space.",
      "offset": 1839.039,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "Yes, you're correct. You should have",
      "offset": 1840,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "data which is like getting more accurate",
      "offset": 1841.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "data. Is that what you're trying to say?",
      "offset": 1842.96,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "Like you should have data which is like",
      "offset": 1844.32,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "for example what is two plus two? You",
      "offset": 1845.679,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "should get more data which is like four.",
      "offset": 1846.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "It shouldn't be like five or 10 or minus",
      "offset": 1848.32,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "100.",
      "offset": 1850.159,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "Well like saying that like the practice",
      "offset": 1850.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of saying that five is better than 10.",
      "offset": 1852.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Yes. like in general is that what people",
      "offset": 1855.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "do in produce tend to say like when",
      "offset": 1857.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "there is like exactly one correct answer",
      "offset": 1860.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and everything else in a mathematical",
      "offset": 1862.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "sense is equally wrong because it's not",
      "offset": 1863.919,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "that answer",
      "offset": 1865.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "that is a good question I don't know um",
      "offset": 1866,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "I think large model labs won't tell you",
      "offset": 1868.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "exactly what they do in our experiments",
      "offset": 1870.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "when you use our notebooks we actually",
      "offset": 1872.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "show that if you do distance- based so",
      "offset": 1873.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the closer your number is to the actual",
      "offset": 1875.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "number you will get better results but",
      "offset": 1877.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "generally speaking it's easier to just",
      "offset": 1879.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "say five is wrong just give it zero",
      "offset": 1881.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "reward everything is zero and then The",
      "offset": 1883.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "good one is like one. It's actually much",
      "offset": 1885.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "easier to do. Um, for example, if you",
      "offset": 1887.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "want to do execution of code, how do you",
      "offset": 1889.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "actually do distance based scoring,",
      "offset": 1891.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "right? So like if you ask it to create a",
      "offset": 1893.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Flappy Bird game, you just have the",
      "offset": 1894.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "final output, but you don't actually",
      "offset": 1896.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "know how to verify like, you know, oh,",
      "offset": 1898.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is this Flappy Bird game game better",
      "offset": 1900.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "than the previous Flappy Bird game? It's",
      "offset": 1901.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "only in a mathematical sense you can",
      "offset": 1903.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "like do distance based scoring. Um, I'm",
      "offset": 1905.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "assuming large model labs, they probably",
      "offset": 1908.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "do the 01 better. Like the majority of",
      "offset": 1909.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "them just do like yes or no, yes or no,",
      "offset": 1912,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "binary. Um but in our experiments for",
      "offset": 1913.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "math specifically you should do distance",
      "offset": 1916.399,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "based scoring. Um it makes the model",
      "offset": 1918,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "learn faster.",
      "offset": 1919.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1921.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "For verifiable domain like math is it",
      "offset": 1922.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "actuallyable because 2 plus 2 makes",
      "offset": 1926,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sense but it's not going to scale for",
      "offset": 1928.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like really large numbers or large",
      "offset": 1930.32,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "multiplications.",
      "offset": 1931.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "So are we going to end up is it endame",
      "offset": 1932.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "using tool use to calculate that or",
      "offset": 1935.679,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "model could potentially be changed to",
      "offset": 1938.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that is a good question. In the olden",
      "offset": 1941.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "days before this paradigm came along, we",
      "offset": 1943.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "would think that you can just use a tool",
      "offset": 1945.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like a calculator to you should actually",
      "offset": 1946.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "I would say you should still use a",
      "offset": 1949.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "calculator to calculate 2 plus2 right",
      "offset": 1950.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you should not use a language model but",
      "offset": 1952,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "with RL you know VR the trick is we",
      "offset": 1954,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually found that actually wait a",
      "offset": 1956.559,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "second if you just do 2 plus2 or you do",
      "offset": 1957.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "another question like 10 * 10 or you do",
      "offset": 1959.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "some sort of complicated mathematical",
      "offset": 1961.519,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "expression you know like the derivative",
      "offset": 1963.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "of x^2 or something I don't know like",
      "offset": 1964.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "some random mathematical equation it",
      "offset": 1966.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "randomly learns to actually solve that",
      "offset": 1968.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "equation without actually doing",
      "offset": 1971.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "overfitting. And so like I would say",
      "offset": 1972.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that with RL you can actually make the",
      "offset": 1974.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "model actually learn how to do",
      "offset": 1976.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "multiplication, how to do addition. So",
      "offset": 1977.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "it's actually in the model. Um does that",
      "offset": 1980,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "kind of make sense?",
      "offset": 1982.399,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "Yeah. Would we use this in production or",
      "offset": 1983.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Oh yes. Yeah. People use that in",
      "offset": 1985.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "production. I Okay, maybe don't use it",
      "offset": 1986.399,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "in production. You know, you're not sure",
      "offset": 1988.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "if the answer is correct. Maybe it will",
      "offset": 1989.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "say 3 + 3 is 7. Okay. I don't know. It's",
      "offset": 1991.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "possible. So like essentially, but it's",
      "offset": 1994.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "getting better. Um maybe in the future",
      "offset": 1997.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "all of mathematical equations can just",
      "offset": 2000.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "be done by a model. Um I think in the",
      "offset": 2002,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "maybe a few months ago, maybe like seep",
      "offset": 2004.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "you know before 01 got released actually",
      "offset": 2006,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "not even 01 a few months ago people",
      "offset": 2007.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "would still say use a calculator you",
      "offset": 2010.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know like some sort of tool calling. Yes",
      "offset": 2012.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you should probably still do it. Um but",
      "offset": 2014.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "imagine you know as time goes on as",
      "offset": 2016.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "models get better and better and better",
      "offset": 2019.039,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "in terms of like training data just just",
      "offset": 2020.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "for the max equation you know 2 plus",
      "offset": 2021.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "two. Um, imagine in the limit as we get",
      "offset": 2023.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "all of the world's data for just this",
      "offset": 2025.919,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "maths question, right? 2 plus 2, 4 plus",
      "offset": 2027.519,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "4, 10 x 10 or whatever, it should in",
      "offset": 2029.519,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "theory solve them all in theory. Um,",
      "offset": 2031.919,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "yes, the it's always in theory. Um, but",
      "offset": 2035.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "yes, you don't need a tool calling. It's",
      "offset": 2037.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "not necessary. Um, yeah. Yeah. Yes.",
      "offset": 2039.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "My question is about the reward model.",
      "offset": 2042.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "In practice, are people using large",
      "offset": 2044.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "language models as a reward model?",
      "offset": 2046.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Good question.",
      "offset": 2048,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Or is it? Yes, I actually",
      "offset": 2050.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Good question. Oh, I was going to go in",
      "offset": 2053.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the next next slides. We'll be talking",
      "offset": 2055.28,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "about that. Um, yes.",
      "offset": 2057.28,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "Multi-turn",
      "offset": 2062.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "does this change with multi-turn? I",
      "offset": 2064.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "mean, you showed a single",
      "offset": 2066,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "multi.",
      "offset": 2069.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yes, you could do multi-turn. It's a bit",
      "offset": 2072.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "more complicated. Um, you just imagine.",
      "offset": 2074.639,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "There are tricks you can do. You imagine",
      "offset": 2077.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that your current step is good. Imagine",
      "offset": 2079.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "it and then you just continue doing",
      "offset": 2082.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "inference. You you append like your next",
      "offset": 2084.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "question. Like for example, how am I",
      "offset": 2086.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "going to what is 2 plus 2? You say okay",
      "offset": 2088.399,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "let me think about this question. What",
      "offset": 2091.76,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "is 2 plus2? Blah blah blah blah blah",
      "offset": 2093.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "blah blah blah. The answer is four. And",
      "offset": 2094.639,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "then the question is what is your next",
      "offset": 2096.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "question? Maybe the user interacts with",
      "offset": 2097.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "it and says okay I I don't think your",
      "offset": 2099.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "question is correct. Oh, I don't think",
      "offset": 2101.76,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "your answer is correct. And then the",
      "offset": 2102.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "model says oh okay let me rethink about",
      "offset": 2104.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "this blah blah blah blah blah blah blah.",
      "offset": 2105.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I still think the answer is four. Um, so",
      "offset": 2107.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you could chain this all together and",
      "offset": 2109.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "shove it into the, you know, the whole",
      "offset": 2110.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "RL step. You could do that. Um, it's a",
      "offset": 2112.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bit more complicated. I think the",
      "offset": 2114.64,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "diagram will be a little bit more",
      "offset": 2115.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "different. Um,",
      "offset": 2117.04,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "the follow that to that would be",
      "offset": 2119.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "you assume like a loop is a single turn",
      "offset": 2123.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "or a loop is a lot of turns and then you",
      "offset": 2125.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "only give a reward at the end or you",
      "offset": 2128.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "give like subrewards.",
      "offset": 2130.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Very good question. So there is in the",
      "offset": 2131.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "deepse R1 paper you could do subrewards",
      "offset": 2133.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "or you could just do the reward at the",
      "offset": 2136.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "very end. I think subrewards might",
      "offset": 2137.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "actually do better in general but the",
      "offset": 2139.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "question is subrewards is very hard to",
      "offset": 2141.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "calculate. You would rather just wait",
      "offset": 2143.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you know all until the very very end and",
      "offset": 2145.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "just give a reward. That's probably the",
      "offset": 2146.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "easiest. So it's more about efficiency.",
      "offset": 2148.32,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "It's all to be honest all of AI is about",
      "offset": 2151.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "efficiency. What is more efficient? What",
      "offset": 2153.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "is more it's all optimization. Um so the",
      "offset": 2154.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "answer is like I would suggest people",
      "offset": 2157.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "just to like shove a reward at the very",
      "offset": 2159.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "end. Um yes",
      "offset": 2160.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "once you get your reward signal is it",
      "offset": 2162.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "just the reinforce algorithm with the",
      "offset": 2164.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "gradient to go back",
      "offset": 2166.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "we will talk about that yes",
      "offset": 2168.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "yes yes most yes correct",
      "offset": 2169.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "yes so we will talk about reinforce",
      "offset": 2172.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "we'll talk about po and stuff like that",
      "offset": 2173.52,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "um yes",
      "offset": 2175.44,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "this one",
      "offset": 2188.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the problem is if you skip from",
      "offset": 2190.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "pre-training to the RLVA st RLVR stage",
      "offset": 2192,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it's relatively hard because your model",
      "offset": 2195.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "doesn't actually know how to do",
      "offset": 2197.52,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "instructions right so like you have this",
      "offset": 2198.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "base model you ask the question to the",
      "offset": 2200.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "base model what is 2 plus2 it's not",
      "offset": 2202.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "going to say I think the answer is four",
      "offset": 2204.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it might you might be lucky somewhere in",
      "offset": 2206.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "your pre-training data somewhere on the",
      "offset": 2209.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "web someone asked this question what is",
      "offset": 2211.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "2 plus two and then the you know the the",
      "offset": 2212.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "answer was like oh the answer is four",
      "offset": 2214.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "but you have to be lucky um so like so",
      "offset": 2216.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the problem of this is the whole trick",
      "offset": 2219.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "of SFT is you want to force the model to",
      "offset": 2221.599,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "answer what is 2 plus2 instruction way",
      "offset": 2225.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "right so it will tell the model what is",
      "offset": 2228.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "2 plus2 you want it to say the answer is",
      "offset": 2230,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "four you don't want it to like blabber",
      "offset": 2232.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "on and like j like get some Wikipedia",
      "offset": 2234.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "article and shove it as the output so",
      "offset": 2236.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the whole point of SFT preference",
      "offset": 2238.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "fine-tuning and stuff like that is to",
      "offset": 2240.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "make the model forced to make it more",
      "offset": 2242.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "optimal to like output conversation",
      "offset": 2244.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "style um if you want to skip it's also",
      "offset": 2246.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "fine",
      "offset": 2248.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "It's just not efficient. Um because like",
      "offset": 2250.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you you could do this. Um I'm assuming",
      "offset": 2252.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "large model labs are trying to do this.",
      "offset": 2255.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Um so it's not like a you should or you",
      "offset": 2256.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "shouldn't. They are trying. Um does that",
      "offset": 2258.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 2261.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "there? Yes. Question",
      "offset": 2262.8,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "couple of questions. One question is",
      "offset": 2266.079,
      "duration": 8.801
    },
    {
      "lang": "en",
      "text": "it policy optimizer",
      "offset": 2270.079,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "model after",
      "offset": 2274.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the reference model does not change. So",
      "offset": 2278,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the reference model is just a model that",
      "offset": 2279.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you didn't train. Um it's like the it's",
      "offset": 2281.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like the it's like the base model or",
      "offset": 2283.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "like the SFT whatever checkpoint you",
      "offset": 2284.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "started with. It doesn't change. You",
      "offset": 2286.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "could change it. I think that would be",
      "offset": 2288.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "too expensive though. I think if you",
      "offset": 2290.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "change it that'll be more complicated.",
      "offset": 2292.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Remember all of AI is about optimization",
      "offset": 2293.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and efficiency. So I feel like you don't",
      "offset": 2296.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "have to. You could I I don't know if",
      "offset": 2298.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "there are papers talking about it",
      "offset": 2301.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "though. U maybe open AI does it. I don't",
      "offset": 2302.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "know. Um",
      "offset": 2304.16,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "other question is that do we need less",
      "offset": 2305.76,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 2314.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So the trick of RL is you just need a",
      "offset": 2324.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "reward function. You need to make that",
      "offset": 2326.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and you don't need data. You don't need",
      "offset": 2329.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the answer of the data. Oh actually you",
      "offset": 2331.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "do need the answer. You don't need the",
      "offset": 2333.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "chain of thought. You just need lots of",
      "offset": 2334.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "questions like what is 2 plus2? What is",
      "offset": 2336.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "4 plus 4? Remember you can actually",
      "offset": 2338.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "automatically generate this. Right? So",
      "offset": 2340.079,
      "duration": 1.841
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 2341.44,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "number of samples do you need less",
      "offset": 2341.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "number of samples compared to",
      "offset": 2343.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you should do as much as possible. you",
      "offset": 2345.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "most large language models I think for",
      "offset": 2347.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "like you know 03 or 01 I don't know what",
      "offset": 2349.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "is a percentage of compute maybe they",
      "offset": 2351.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "spend like 5% or less but the goal is",
      "offset": 2352.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what happens if you spend double the",
      "offset": 2355.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "compute just on RL right so like",
      "offset": 2357.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "previously if you do 14 trillion tokens",
      "offset": 2359.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "on pre-training make RL 14 trillion",
      "offset": 2361.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "tokens and then the goal of large large",
      "offset": 2364.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "labs is to just do that so currently",
      "offset": 2366.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it's very less but over time it will",
      "offset": 2370.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "increase",
      "offset": 2372.4,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "but compared Let's say the number of",
      "offset": 2374.24,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "samples will be much less",
      "offset": 2378.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "currently. Yes.",
      "offset": 2380.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Because it's expensive like you need",
      "offset": 2382.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "models.",
      "offset": 2385.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Correct. It's expensive but over time I",
      "offset": 2385.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "think like maybe by next year or like",
      "offset": 2388.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this year large model labs their goal is",
      "offset": 2390.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "to do this phase the most. That's their",
      "offset": 2392.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "goal because remember you can",
      "offset": 2395.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "automatically generate questions now.",
      "offset": 2396.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "What is 2 plus 2? What is two 2 * 2?",
      "offset": 2398,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "What is 10 divided by 10? I don't know.",
      "offset": 2400.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Generate as many math questions as you",
      "offset": 2402.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like. But remember you can also generate",
      "offset": 2403.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you know like coding questions. You can",
      "offset": 2406,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "generate any questions that you like or",
      "offset": 2408,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you can use the supervised fine tetuning",
      "offset": 2409.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "data itself for the RL step. You can do",
      "offset": 2411.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that as well. Um does that kind of make",
      "offset": 2413.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "sense?",
      "offset": 2416.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Okay. Any Okay.",
      "offset": 2417.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "How do you protect the SF from being",
      "offset": 2419.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "like screwed up by",
      "offset": 2422.56,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Oh, good. Yes. We won't talk about it.",
      "offset": 2423.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Like you have 2 plus 2",
      "offset": 2425.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is also a good answer. 2 plus 2.",
      "offset": 2427.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Correct.",
      "offset": 2429.76,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "But I don't want more equations. I want",
      "offset": 2430.4,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "the answer.",
      "offset": 2432.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "Very good. So is there like techniques",
      "offset": 2432.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to make sure that we're not violating",
      "offset": 2434.88,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "our",
      "offset": 2436.8,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "instruction? Yes. We will also talk",
      "offset": 2436.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "about that clipping and stuff like that.",
      "offset": 2438.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 2440.48,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 2441.839,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "Is there any research done",
      "offset": 2442.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "in the model? So for example, you know,",
      "offset": 2450.48,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "there's a circuit that says",
      "offset": 2454.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "yes.",
      "offset": 2457.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "But can you incentivizing",
      "offset": 2458,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the model?",
      "offset": 2462.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "addition the concept in general",
      "offset": 2463.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you that is a very good question I don't",
      "offset": 2466.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "know I think that's like the during the",
      "offset": 2468.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "pre-training phase essentially somewhere",
      "offset": 2470.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "somewhere in the internet someone wrote",
      "offset": 2472.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "what is 2 plus2 somewhere and then",
      "offset": 2475.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "somehow maybe someone did a formulation",
      "offset": 2477.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you know some sort of derivation of like",
      "offset": 2479.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "what is 2 plus2 okay I don't think",
      "offset": 2481.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "anyone has done pretend there is some",
      "offset": 2482.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "derivation of like some complicated",
      "offset": 2484.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "maths equation and so the model somehow",
      "offset": 2486.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "learned to predict all of that entire",
      "offset": 2488.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "trace and if it keeps seeing this it",
      "offset": 2491.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "would like accentuate the fact oh okay",
      "offset": 2493.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I've seen this before let's make this",
      "offset": 2494.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "even more um more prevalent in the model",
      "offset": 2496.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so somewhere in the model it has learned",
      "offset": 2499.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "2 plus 2 is equal to four somewhere",
      "offset": 2501.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "yes I guess what I'm saying is so we can",
      "offset": 2503.119,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "use like the stepific",
      "offset": 2506.319,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "or super good reward you can",
      "offset": 2513.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "you're waiting it",
      "offset": 2516.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "correct so there was actually two",
      "offset": 2519.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "schools of thought. The first one is the",
      "offset": 2522,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "model already has this knowledge, right?",
      "offset": 2524.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "It already knows what is 2 plus2 and",
      "offset": 2526.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you're just RL just tries to like",
      "offset": 2528.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "maximize",
      "offset": 2530.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "what if it sees 2 plus 2 is equal to",
      "offset": 2532.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "four. It tries to weight this factor",
      "offset": 2534,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "more weight this circuit in the model",
      "offset": 2536.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "more. So the model already learns learns",
      "offset": 2538,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but then the second school of thought is",
      "offset": 2540.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "like okay maybe the model doesn't know",
      "offset": 2541.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "actually an arro actually learns a new",
      "offset": 2542.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "thing. Um I'm more in the first one. The",
      "offset": 2545.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "model probably already learns. It",
      "offset": 2547.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "already knows it and we're just",
      "offset": 2549.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "maximizing the you're just trying to",
      "offset": 2550.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "make it more accentuated. Um",
      "offset": 2552.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "exactly. So the extent of my question is",
      "offset": 2554.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "basically I want I'm wondering if you",
      "offset": 2556.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "can steer maybe there's like you can",
      "offset": 2559.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "steer the model away from the circuit to",
      "offset": 2562.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "the always do the",
      "offset": 2564.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "addition circuit if that makes sense.",
      "offset": 2567.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "You mean like just do addition?",
      "offset": 2570.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Yes. You could I guess what you could do",
      "offset": 2572.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is like get the language model see which",
      "offset": 2574.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "weights are changed during the RL phase",
      "offset": 2576.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "which weights are changed and you just",
      "offset": 2579.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "give it what is 2 plus2 what is 2 plus2",
      "offset": 2580.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "what is 2 plus2 you just keep doing this",
      "offset": 2582.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "question and you can see which of the",
      "offset": 2583.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "weights are changing and essentially you",
      "offset": 2585.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "can extract this from the model you",
      "offset": 2586.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "could I don't know if there's research",
      "offset": 2588.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "about this but I'm just making I'm just",
      "offset": 2590.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "making stuff up on the spot you could do",
      "offset": 2591.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that if that makes sense oh maybe that's",
      "offset": 2593.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a research question someone should do",
      "offset": 2596.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "research paper",
      "offset": 2597.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "any other yes",
      "offset": 2599.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "go back",
      "offset": 2601.599,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "Yes, we will also talk about that. Yes.",
      "offset": 2622.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "Yes. Yes. Okay.",
      "offset": 2624.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Okay. Yeah. Sorry.",
      "offset": 2628.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Do we expect usually",
      "offset": 2629.359,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "changes all the parameters in the model",
      "offset": 2632.079,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "set",
      "offset": 2636,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "as a subset",
      "offset": 2642.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "you you could there's like two large",
      "offset": 2643.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model labs will most likely change all",
      "offset": 2645.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "of the parameters every single parameter",
      "offset": 2647.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "is changed um but there are papers which",
      "offset": 2649.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "show that actually not all of the",
      "offset": 2651.359,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "parameters are actually changing that",
      "offset": 2652.56,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "much some of them are changed by like",
      "offset": 2653.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "zero like the majority of updates to the",
      "offset": 2655.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "model is like zero and only some very",
      "offset": 2657.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "small updates to the model are seen and",
      "offset": 2659.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so that's kind of like a circuit idea",
      "offset": 2661.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "where like the model already knows how",
      "offset": 2662.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "to do whatever question you give it you",
      "offset": 2664.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "just most of the updates are like zero",
      "offset": 2667.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "um any say which one is better is it",
      "offset": 2669.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "better to aim for changing all the",
      "offset": 2672.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "parameters",
      "offset": 2674.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you can also do like lower you know",
      "offset": 2675.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "parameter efficient fine tuning you can",
      "offset": 2677.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "do other things you don't have to",
      "offset": 2679.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "fine-tune every single thing um but I",
      "offset": 2680.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "think majority of large language model",
      "offset": 2682.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "labs they just do everything um",
      "offset": 2684.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "otherwise this again becomes a",
      "offset": 2687.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "optimization problem you know what do",
      "offset": 2688.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "which layer do we select and stuff like",
      "offset": 2690.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that. So it gets more complicated but",
      "offset": 2692.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "yes you could do parameter efficient",
      "offset": 2694.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "fight actually we're going to show a",
      "offset": 2696.16,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "notebook for that so you can actually do",
      "offset": 2697.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "it on your own computer um yeah was yes",
      "offset": 2698.56,
      "duration": 8.6
    },
    {
      "lang": "en",
      "text": "one more question one more yes",
      "offset": 2702.48,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "oh Okay.",
      "offset": 2710.48,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "Capabil",
      "offset": 2720.319,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Good question for The K divergence term.",
      "offset": 2758.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Do you mean like removing it? Would that",
      "offset": 2761.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "make it better?",
      "offset": 2763.28,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "I was saying",
      "offset": 2764.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 2768,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "because the whole point of the K",
      "offset": 2773.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "divergence term is to like not make it",
      "offset": 2774.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "too stray away from the supervised",
      "offset": 2776.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "model.",
      "offset": 2778.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Okay, maybe I'm not kept up to date with",
      "offset": 2779.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "with research papers, but anyways, I So",
      "offset": 2782,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "your your point was like if we remove",
      "offset": 2784.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the KL term, it will be better. It",
      "offset": 2786.24,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "learns new capabilities.",
      "offset": 2788.8,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "I wanted to understand.",
      "offset": 2805.2,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Do you mean like do you want to have",
      "offset": 2822.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "more capabilities into the model?",
      "offset": 2824.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Are there strategies for",
      "offset": 2828.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh strategies?",
      "offset": 2830,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Hard to say. I'm assuming the large",
      "offset": 2833.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "model labs will probably know",
      "offset": 2835.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "strategies. We will show like I I will",
      "offset": 2836.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "show examples of how to like make RL",
      "offset": 2838.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "better like how to like reach higher",
      "offset": 2840.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "reward faster. I'm not sure about new",
      "offset": 2843.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "capabilities. It's actually very hard.",
      "offset": 2846.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "It's actually very very very very hard",
      "offset": 2847.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to is elicit new capabilities in the",
      "offset": 2849.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "model. Um the question is like is this",
      "offset": 2851.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "new or not new? I think that's a",
      "offset": 2854.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "question like is this actually part of",
      "offset": 2855.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the model or not part of the model? Um",
      "offset": 2857.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "and most research papers are like hand",
      "offset": 2860.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "wavy. They say oh most updates are",
      "offset": 2862.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "sparse you know like so most likely it's",
      "offset": 2864.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "not you know new capabilities but what",
      "offset": 2867.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "happens if you know one year later all",
      "offset": 2870.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of the model updates are like not",
      "offset": 2872.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "sparse. Is this considered new",
      "offset": 2873.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "capability? I don't know like you know",
      "offset": 2875.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "those are the questions. It's more like",
      "offset": 2877.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "I don't know if that answers your",
      "offset": 2879.44,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "question. I probably didn't answer your",
      "offset": 2880.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "question but",
      "offset": 2881.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "maybe maybe the other parts of the talk",
      "offset": 2884.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "maybe might answer some part. Um yeah",
      "offset": 2886.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "okay I will keep going on more questions",
      "offset": 2888,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "later. Um okay so like the reward model",
      "offset": 2890.319,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "right was actually a language model that",
      "offset": 2894.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like some sort of model some neuronet",
      "offset": 2897.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "network some AI model that predicts the",
      "offset": 2898.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "reward in RLVR we delete this entirely",
      "offset": 2900.48,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "and we just call it the reward function.",
      "offset": 2904,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "So like the ground truth reward you know",
      "offset": 2906.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "if it's correct you plus one if it's bad",
      "offset": 2908.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's just zero right so you essentially",
      "offset": 2910.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "delete another part gpo essentially",
      "offset": 2912.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "deletes another part right so like you",
      "offset": 2914.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "remove remember gpio you delete the",
      "offset": 2916.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "value model get totally remove it and",
      "offset": 2917.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "then you delete the reward model and",
      "offset": 2920,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's just a reward function",
      "offset": 2921.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and yes as a reward model you could use",
      "offset": 2924.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "lm as a judge so you could ask a",
      "offset": 2926.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "language model itself to say is the",
      "offset": 2928.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "answer good or bad you could do that you",
      "offset": 2931.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "could do regular expression check you",
      "offset": 2933.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "know like is the formatting of the",
      "offset": 2935.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "answer good or bad. Is the maths",
      "offset": 2936.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "equation good or bad? You know, is the",
      "offset": 2940.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "final output good or bad? You can do",
      "offset": 2942,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "distance scoring and stuff like that.",
      "offset": 2943.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "You can also execute the Python code and",
      "offset": 2944.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "then you can see if it actually",
      "offset": 2946.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "executed, right? So like is there like",
      "offset": 2948.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "import errors or like format errors or",
      "offset": 2950.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like some sort of Python error and you",
      "offset": 2952.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "can use this as a reward. And so this",
      "offset": 2954.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "blue box, the reward can be anything",
      "offset": 2957.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that you like. It just needs to output a",
      "offset": 2959.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "number, you know, minus one, plus one, I",
      "offset": 2962.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "don't know. It just has to be a number.",
      "offset": 2965.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "In fact, you can make a dumb reward just",
      "offset": 2966.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "everything. It just does random, you",
      "offset": 2968,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "know, plus one 50% of the time, minus",
      "offset": 2969.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "one 50% of the time, I don't know. And",
      "offset": 2972.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "confusingly, a paper recently showed",
      "offset": 2974.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "that actually random rewards works. Um,",
      "offset": 2976.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so like yes, go ahead. You can try it.",
      "offset": 2980.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 2982.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "but also why did someone say why? Um,",
      "offset": 2984.079,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "probably read the paper, but but what to",
      "offset": 2988.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "be honest actually I think the paper",
      "offset": 2991.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "might be a bit",
      "offset": 2992.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I don't actually believe there was yes",
      "offset": 2994.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "there was an update showing that",
      "offset": 2996.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "actually this was wrong um that actually",
      "offset": 2997.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "it's because the model they don't the",
      "offset": 2999.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "benchmarks are incorrect. So when you",
      "offset": 3002.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "say that you actually increase accuracy",
      "offset": 3004.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like from 20% to 50%. But actually the",
      "offset": 3006.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "model itself was already 50%. They just",
      "offset": 3009.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "didn't check the accuracy of the correct",
      "offset": 3011.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "model before. Um so there was a recent",
      "offset": 3013.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "rebuttal to those types of papers. Um",
      "offset": 3016.079,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "but you know interesting results. Um,",
      "offset": 3018.88,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "yeah, I don't know.",
      "offset": 3022.559,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "Okay, so remember in RL the goal is you",
      "offset": 3027.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "don't know the best action to take in",
      "offset": 3030.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the space, right? When you're doing",
      "offset": 3033.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "Pac-Man, I don't know if going left,",
      "offset": 3034.559,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "right, up or down is the best. I don't",
      "offset": 3036.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "know. But at the very very very end, you",
      "offset": 3037.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "will either, you know, win or like, you",
      "offset": 3040.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "know, get some reward or you will die.",
      "offset": 3042.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Yes. Um, but the goal of RL is to",
      "offset": 3045.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "maximize the best action you can ever",
      "offset": 3048.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "take, right? So like what is a better",
      "offset": 3050.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "action than all of the other bad",
      "offset": 3051.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actions. So RL just tries to maximize",
      "offset": 3053.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the best well not the best action the",
      "offset": 3055.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "better action",
      "offset": 3057.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "normal pre-training you already know",
      "offset": 3059.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "what is the best answer. So like you",
      "offset": 3061.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "already know what is the next word right",
      "offset": 3062.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you if you want to predict you know",
      "offset": 3064.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "hello my name is Daniel you already know",
      "offset": 3065.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the next word is going to be Daniel",
      "offset": 3067.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "right so you already know it but RL you",
      "offset": 3069.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "don't know in advance what is the actual",
      "offset": 3071.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "correct reward. Um so you the only thing",
      "offset": 3073.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you can do in RL is to maximize the you",
      "offset": 3075.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "know one of the better options",
      "offset": 3078.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and so yes okay now more maths um the",
      "offset": 3081.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "goal is to maximize this equation um",
      "offset": 3084.319,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "that's the goal of RL",
      "offset": 3087.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "so what is this equation the J is like",
      "offset": 3091.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the total gradient um well actually it's",
      "offset": 3094.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it's more like the it's more like we",
      "offset": 3097.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "want to maximize this it's not actually",
      "offset": 3099.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "the total gradient okay maybe I misro",
      "offset": 3100.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that anyways didn't write that. Um the",
      "offset": 3102.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "we want to calculate the gradient with",
      "offset": 3106.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "respect to the policy language model and",
      "offset": 3108.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the action is given a state and the r is",
      "offset": 3110.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "a reward. If you want to write this down",
      "offset": 3113.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "in like English, it's like we want to",
      "offset": 3116.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "take the derivative of the log",
      "offset": 3118.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "probability of the action given the",
      "offset": 3120.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "state times the reward. Now I don't know",
      "offset": 3122.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "if you guys understand what that means",
      "offset": 3124.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but I did like a example um Pac-Man",
      "offset": 3125.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "case. Okay, so you are Pac-Man. The red",
      "offset": 3129.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is your enemy. You don't want to go",
      "offset": 3132,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "there, right? So like you definitely",
      "offset": 3133.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "don't want to go to the red thing,",
      "offset": 3134.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "right? But you want to eat the two gray",
      "offset": 3136.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "dots, right? That's remember you can",
      "offset": 3138.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "only go up, down, left, or right, right?",
      "offset": 3140.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "You only have four actions. Remember the",
      "offset": 3142.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "action space is just up, down, left, or",
      "offset": 3144.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "right. So if you do rewards, I just",
      "offset": 3146.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "randomly made some rewards up. If you go",
      "offset": 3149.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to the red thing, you will get minus 10",
      "offset": 3151.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "reward. Or actually, it should be minus",
      "offset": 3152.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "infinity. You die. But anyways, minus",
      "offset": 3154.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "10. If you eat the gray dots, you get",
      "offset": 3156.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "plus one or plus one. And if you go up,",
      "offset": 3158.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it's just zero reward. There's nothing",
      "offset": 3160.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "there.",
      "offset": 3161.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Now when you get this language model or",
      "offset": 3163.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like some sort of model it has to tell",
      "offset": 3166.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "you what is the next action right it",
      "offset": 3168.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "tells you what to do to the next action",
      "offset": 3170,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for now we'll just assign every single",
      "offset": 3172.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "action up down left or right as one",
      "offset": 3174.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "quarter probability right so like you go",
      "offset": 3176.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "up 25% of the time left 25% of the time",
      "offset": 3178.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and so on so these are your numbers",
      "offset": 3181.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "right this is the entire state",
      "offset": 3184.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "so the goal of RL is you want to do that",
      "offset": 3186.8,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "red going towards um the right you want",
      "offset": 3190.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "to do this you want to go towards the",
      "offset": 3192.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "right less you want to do a much less",
      "offset": 3194.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "right so like you want to push the",
      "offset": 3196.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "probability of the 0.25 25 of the right",
      "offset": 3197.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "much less and you want to go bot like",
      "offset": 3200.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "you know down and left much more right",
      "offset": 3203.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so you want to push the probabilities",
      "offset": 3206.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "much more and the top are not really",
      "offset": 3208.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that important and so RL essentially you",
      "offset": 3210.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "your goal is to avoid doing the bad",
      "offset": 3212.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "thing and you want to do the good thing",
      "offset": 3215.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "much more that's kind of RL if you",
      "offset": 3217.119,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "convert this into a table right you have",
      "offset": 3221.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the probability of the action given the",
      "offset": 3224.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "state right remember up down left or",
      "offset": 3225.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "right we just assigned 25% % chance,",
      "offset": 3227.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "right? Just just pretend 25% chance. The",
      "offset": 3229.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "reward, which we can calculate, right?",
      "offset": 3232.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "We calculated this. We calculated the",
      "offset": 3234.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "reward. We just made some numbers up,",
      "offset": 3236.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "right? As 0 1 1 - 10.",
      "offset": 3238.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "The probability times the reward, we get",
      "offset": 3241.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "some numbers, right? So like 0 0.25, 0.5",
      "offset": 3243.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "minus 2.5. And then if you take the log",
      "offset": 3246.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of the probability times the reward, you",
      "offset": 3248.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "get some number, right? So like 0 minus",
      "offset": 3250.88,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "0.6 minus 0.6 and 6.02.",
      "offset": 3253.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "So from this table, does anyone know",
      "offset": 3257.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which row do we want to maximize? What",
      "offset": 3259.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "is the goal? Like what do we want to",
      "offset": 3262,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "maximize?",
      "offset": 3263.359,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "Which row?",
      "offset": 3265.839,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "You want to maximize the bottom row.",
      "offset": 3270.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "What is the reward of the bottom row?",
      "offset": 3272.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Correct. You want to minimize the bottom",
      "offset": 3276.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "row. Remember the reward is minus 10. We",
      "offset": 3277.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "do not want to maximize the last row",
      "offset": 3279.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "because the last row is the worst. And",
      "offset": 3280.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "so that means the 6.02. We want to",
      "offset": 3282.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually decrease this number",
      "offset": 3284.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "dramatically. Right? That's way too",
      "offset": 3285.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "large. We want to decrease it. The other",
      "offset": 3287.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "rows we want to maximize. And so the",
      "offset": 3290.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "goal is okay, we just take the sum of",
      "offset": 3292.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "all of that, right? We take the sum of",
      "offset": 3293.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the four numbers and it's 4.8.",
      "offset": 3295.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "And so remember, okay, let's try, right?",
      "offset": 3298.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So like by hand, by hand, we shall",
      "offset": 3300.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "remember the right remember all the",
      "offset": 3304.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "probabilities are one quarter 0.25. By",
      "offset": 3305.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "hand we shall do the bad action even",
      "offset": 3308.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "more, right? We actually do the worst",
      "offset": 3311.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "thing. What happens to the what happens?",
      "offset": 3313.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Right? So like the reward the",
      "offset": 3315.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "probability times the reward is now",
      "offset": 3317.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "minus4. It used to be minus 2.5. And so",
      "offset": 3318.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the reward the log probabilities times",
      "offset": 3322,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the reward the sum actually decreased",
      "offset": 3324.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "right it decreased to 2.58. Before it",
      "offset": 3326.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "was 4.81.",
      "offset": 3328.96,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "Is 2.58 smaller or bigger than 4.81?",
      "offset": 3331.44,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "Obviously smaller. So actually this is",
      "offset": 3336.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "worse. You should not do this. But this",
      "offset": 3337.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "is actually bad. Remember the goal is to",
      "offset": 3340,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "maximize",
      "offset": 3343.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "maximize this equation. Maximize it,",
      "offset": 3344.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? Maximize.",
      "offset": 3346.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "And so 4.81 is actually better. The",
      "offset": 3348.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "original state is actually better than",
      "offset": 3351.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the 2.58. So this the thing that we just",
      "offset": 3353.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "did is worse. So do not do this.",
      "offset": 3356.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "However, let's do the right thing less.",
      "offset": 3358.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Right? Let's not go to the right and",
      "offset": 3361.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "actually maximize the rest.",
      "offset": 3363.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "You shall see that if you do the log",
      "offset": 3365.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "probability times reward sum of them all",
      "offset": 3367.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "you will get 8.9 which is a larger",
      "offset": 3369.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "number and so the goal is to maximize",
      "offset": 3371.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "this as much as possible. You could say",
      "offset": 3373.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "wait we know the answer right you should",
      "offset": 3376.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "go towards the right just make this 100%",
      "offset": 3378.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "probability let's just you know and",
      "offset": 3381.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you'll have an infinite reward okay okay",
      "offset": 3382.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "not infinite but you will get maximum",
      "offset": 3384.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "reward right why don't we just do that",
      "offset": 3386.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "but you should not do that because",
      "offset": 3389.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you're actually forced if you do this",
      "offset": 3391.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "your model will be like learn oh okay",
      "offset": 3393.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "let's just keep going right let's keep",
      "offset": 3395.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going right and they just get stuck and",
      "offset": 3396.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it just becomes very bad for",
      "offset": 3399.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "optimization so definitely don't do that",
      "offset": 3400.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "now there are someone who's who talked",
      "offset": 3403.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "reinforce um we don't just multiply the",
      "offset": 3405.04,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "reward but remember this equation we did",
      "offset": 3408.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "where is it the probability of the",
      "offset": 3411.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "action given the state times the reward",
      "offset": 3413.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we don't actually multiply the reward we",
      "offset": 3415.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "should not do that um you actually",
      "offset": 3417.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "multiply by something called the",
      "offset": 3419.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "advantage um and what is the advantage",
      "offset": 3420.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the advantage is a reward minus the",
      "offset": 3423.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "average reward the base reward so you",
      "offset": 3426.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "shouldn't actually just see you want to",
      "offset": 3429.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "maximize reward you want to actually",
      "offset": 3431.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "maximize a reward",
      "offset": 3433.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "but also looking at the average reward",
      "offset": 3435.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "across the entire model. So it's called",
      "offset": 3437.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the baseline",
      "offset": 3438.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and so this B this baseline model is",
      "offset": 3441.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what is the value function the value",
      "offset": 3444.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "model remember GPO deletes the value",
      "offset": 3445.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "model this was the value model and this",
      "offset": 3447.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "value model essentially estimates what",
      "offset": 3450,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is the average reward if we just see the",
      "offset": 3451.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "current state it does not take it does",
      "offset": 3454.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "not look at like you know what is the",
      "offset": 3456.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "next uh next step it doesn't look at you",
      "offset": 3457.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "know what is the next action it just",
      "offset": 3460.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "takes a snapshot of what you currently",
      "offset": 3462.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so essentially it looks at this it looks",
      "offset": 3463.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "at this and just guesses what is a",
      "offset": 3466.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "reward, right? It doesn't you you're not",
      "offset": 3469.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "supposed to give it the rewards. You're",
      "offset": 3471.04,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "not supposed to give a minus 10 + 1 + 1",
      "offset": 3472.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "or zero. It just looks at the current",
      "offset": 3474.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "state and produces a number. And this",
      "offset": 3476.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "number is called the average reward.",
      "offset": 3478.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "And so the goal is now we don't actually",
      "offset": 3482.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "want to maximize this, you know, just a",
      "offset": 3485.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "reward. We want to maximize the",
      "offset": 3487.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "advantage as well. Um so like we",
      "offset": 3488.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "multiply all this together and the goal",
      "offset": 3490.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "is we want to maximize this new",
      "offset": 3492.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "equation. Does anyone have any",
      "offset": 3493.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "questions? There's lots of baths but",
      "offset": 3496.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "questions. Yeah. Yes.",
      "offset": 3498.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "So in terms of probability, how do you",
      "offset": 3500.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "is the large language modeling an",
      "offset": 3502.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "estimated probability or this is a known",
      "offset": 3504.799,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "probability of all the possible states.",
      "offset": 3507.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "But how do you get that in practice",
      "offset": 3509.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "world?",
      "offset": 3512,
      "duration": 1.599
    },
    {
      "lang": "en",
      "text": "So large",
      "offset": 3512.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "so a model a large language model",
      "offset": 3513.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "predicts the next word. So for example",
      "offset": 3515.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you take the entire Wikipedia and then",
      "offset": 3517.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you like chunk it into small little",
      "offset": 3519.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "tokens and then the output is just what",
      "offset": 3521.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is the next word. So for example, my",
      "offset": 3523.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "name is Daniel. But it could also be my",
      "offset": 3525.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "name is Michael, my name is Bob, my name",
      "offset": 3527.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "is whatever, whatever. Right? You have",
      "offset": 3528.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "all of these probabilities for every",
      "offset": 3530.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "single word in the entire language",
      "offset": 3531.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "possible like 128,000 words. You assign",
      "offset": 3532.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "a probability for every single one",
      "offset": 3535.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "probability based on the token.",
      "offset": 3538.88,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "Yes, correct.",
      "offset": 3540.079,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "But there'll be",
      "offset": 3540.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the trick of this for language models is",
      "offset": 3542.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you can utilize the probabilities",
      "offset": 3544.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "directly. That's the trick. And so like",
      "offset": 3546.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "that essentially makes everything",
      "offset": 3548.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "easier.",
      "offset": 3549.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Any other question?",
      "offset": 3551.839,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 3553.76,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yes. Yes. Yes. Correct. Correct.",
      "offset": 3561.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 3566.319,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "What about multimodal?",
      "offset": 3566.72,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "Oh,",
      "offset": 3569.119,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "multimodal models. Do you mean like",
      "offset": 3575.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "doing RL multimodal models? Oh, that is",
      "offset": 3576.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "more harder. I would say you you could",
      "offset": 3579.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "you could look at the Sudoku puzzle just",
      "offset": 3581.599,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "convert the text model into a vision.",
      "offset": 3585.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Just cheat. I guess you could do that.",
      "offset": 3588.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "You could like say, \"Oh, you know, I",
      "offset": 3590.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "guess you could give it the Pac-Man, you",
      "offset": 3592.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "know, give it give it the Pac-Man thing",
      "offset": 3594.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and tell the model what should I do",
      "offset": 3596.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "next?\" You you could I mean vision is",
      "offset": 3598.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "it's kind of the same thing, but it's",
      "offset": 3601.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "more",
      "offset": 3602.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Does 03 do vision plus reinforcement",
      "offset": 3604.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "learn? I I think it does. Um yes, you",
      "offset": 3606.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "could. I think for open source I don't",
      "offset": 3608.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "think I've seen Yeah, I don't think",
      "offset": 3610.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah, I don't think I've seen open",
      "offset": 3612.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "source models do that very well. Um, it",
      "offset": 3613.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "is still very hard. Um, yeah. Any other",
      "offset": 3615.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "questions?",
      "offset": 3619.04,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "No. No. Okay.",
      "offset": 3621.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Did someone did ask a question? Oh. Oh,",
      "offset": 3624.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3627.2,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "yes. So, sorry.",
      "offset": 3628.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Oh, what is the B? What is the base",
      "offset": 3636.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "model? average but like what is average",
      "offset": 3638.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "reference model for what?",
      "offset": 3640.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "So we just like your goal is to your",
      "offset": 3643.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "goal is you see this current state of",
      "offset": 3646.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the model like whatever the environment",
      "offset": 3648,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "currently looks like and you just want",
      "offset": 3649.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to produce a number that approximates",
      "offset": 3651.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "what is the total average reward. Okay,",
      "offset": 3653.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "I'll give you an example. Pretend you're",
      "offset": 3655.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "playing chess or like go or I remember",
      "offset": 3657.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "alpha go. You look at the board the",
      "offset": 3659.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "current state of the board and just say",
      "offset": 3661.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "what is the probability of the white",
      "offset": 3663.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "player winning? What is it? You're not",
      "offset": 3665.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "supposed to do any prediction. You just",
      "offset": 3668.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "have to predict what is the probability",
      "offset": 3670.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of the white player winning by just",
      "offset": 3671.839,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "looking at the board. That's kind of the",
      "offset": 3673.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "average reward.",
      "offset": 3674.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "Always low.",
      "offset": 3676.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "It's always low. Yes. Yes. Correct. But",
      "offset": 3678,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "remember at the very very very end",
      "offset": 3680.96,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "phases like you know you might get",
      "offset": 3682.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "higher reward but that's the goal. You",
      "offset": 3683.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "want to you essentially want to predict",
      "offset": 3685.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what is the probability always. You know",
      "offset": 3686.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "for example in chess I'm sure there are",
      "offset": 3689.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like some steps you can take to make the",
      "offset": 3691.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "reward higher. The question is when the",
      "offset": 3693.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model sees this, you need to essentially",
      "offset": 3695.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "you need to say is this board better",
      "offset": 3698.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "than the previous boards and so this",
      "offset": 3701.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "model you have to train as well. You",
      "offset": 3703.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "have to train this model. It needs to",
      "offset": 3705.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "output a probability of it winning.",
      "offset": 3706.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "That's for the chess example. Does that",
      "offset": 3708.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "kind of make sense or",
      "offset": 3710.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "no?",
      "offset": 3712.96,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 3715.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "The value No, no, no. The value model is",
      "offset": 3720.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "totally different. It's a there's three",
      "offset": 3722,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "models. There is a value model which",
      "offset": 3724,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "predicts the average reward of the",
      "offset": 3725.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "state. The reference model is just the",
      "offset": 3727.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "model that you started with and then the",
      "offset": 3729.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "policy or the actual model that you're",
      "offset": 3731.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "changing is the out the final result of",
      "offset": 3732.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "your model like the actual chat model.",
      "offset": 3735.04,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "So there's actually three models.",
      "offset": 3737.599,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "This one the B",
      "offset": 3744.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you will see the current state. You will",
      "offset": 3746.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "see the current state and then you will",
      "offset": 3748.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "see okay what is the actual I think you",
      "offset": 3749.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do use the policy no you no you just",
      "offset": 3751.839,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "look at the current state you look at",
      "offset": 3753.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "the current state and then you output a",
      "offset": 3754.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "probability of whether this chess board",
      "offset": 3756.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "is good or bad",
      "offset": 3758.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of some like 0.8% 8% you're going to",
      "offset": 3759.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "win. Um, okay. Yeah, something like",
      "offset": 3762,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "that. Yes.",
      "offset": 3764.079,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "Oh, yes. Yes. Yeah, we'll talk about",
      "offset": 3775.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that. This is just a general simpler",
      "offset": 3776.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "formula. Yes, we'll talk about that.",
      "offset": 3779.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I guess my question.",
      "offset": 3781.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "Oh, you can ask. Yeah, go.",
      "offset": 3784.96,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "I guess because the policy is predicting",
      "offset": 3786.96,
      "duration": 8.52
    },
    {
      "lang": "en",
      "text": "That's the probability.",
      "offset": 3792.079,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 3796.079,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "That is a good question and that is an",
      "offset": 3807.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "active era of research because you could",
      "offset": 3808.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "either normalize by all of tokens or the",
      "offset": 3810.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "entire just one turn. Remains to be seen",
      "offset": 3813.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "which one's better. Um it's actually",
      "offset": 3815.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "still people talking about that",
      "offset": 3817.28,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "because",
      "offset": 3818.72,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "correct. Yes. So generally speaking",
      "offset": 3829.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "normally people just assume this assume",
      "offset": 3831.599,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this roll out is correct assume this",
      "offset": 3834.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "chain of thought is correct and they",
      "offset": 3836.799,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "just do the very end. But then you do",
      "offset": 3838,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "have to multiply probab I don't yeah you",
      "offset": 3839.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "do have to multiply probabilities. So",
      "offset": 3841.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "there is a multiplication somewhere you",
      "offset": 3843.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "will get very small. Yes. Um but you",
      "offset": 3844.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "know you get very small but the numbers",
      "offset": 3847.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "are relative right? So everything is",
      "offset": 3849.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "very small but then the smaller one the",
      "offset": 3851.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "bigger ones are still very small but",
      "offset": 3853.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it's still better. So they're all",
      "offset": 3854.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "relative.",
      "offset": 3856.079,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "Any was there one more? Yes.",
      "offset": 3858.72,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 3864.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Oh no no it's very old. Yes. Very very",
      "offset": 3872,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "old.",
      "offset": 3874.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 3876.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I wonder if you can give some like",
      "offset": 3878.079,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "advice on like how to think about this",
      "offset": 3881.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "training or abstract level about like",
      "offset": 3883.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "error propagation between like if you",
      "offset": 3884.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "have a trained model which does the",
      "offset": 3887.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "scoring or does the value function or",
      "offset": 3890.319,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "whatever",
      "offset": 3891.839,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "that itself is trained from data. It has",
      "offset": 3892.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "some like error margin and that",
      "offset": 3894,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "you know you have some soft max function",
      "offset": 3897.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for example that like only one in a 100",
      "offset": 3899.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "times will produce the wrong that",
      "offset": 3902.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "probability.",
      "offset": 3904.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Yes. like how do we think about the like",
      "offset": 3905.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "development over time of these models",
      "offset": 3909.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and like to what extent that error",
      "offset": 3910.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "propagation is something that you can",
      "offset": 3912.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "observe and measure like systematize and",
      "offset": 3913.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "engineer around",
      "offset": 3916.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like I don't really understand like what",
      "offset": 3918.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the mindset is in this process right now",
      "offset": 3920.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that",
      "offset": 3923.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "in my view I think all of these formulas",
      "offset": 3924.799,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "are just",
      "offset": 3928.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "made up and so like the goal is to",
      "offset": 3930.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "maximize reward",
      "offset": 3932.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "but the question is you need to like you",
      "offset": 3933.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "can't just maximize reward because",
      "offset": 3934.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "otherwise you might make the model",
      "offset": 3936.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "really silly. Like you might say, okay,",
      "offset": 3938.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "what is 2 plus two? It just says four.",
      "offset": 3940.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Pretend your data set was just what is 2",
      "offset": 3942.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "plus2, right? So like you literally just",
      "offset": 3944.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "cheat. What is 2 plus2? What is 2 plus2?",
      "offset": 3945.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "What is 2 plus2? Just make the model",
      "offset": 3947.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "just say four four. Just four forever.",
      "offset": 3949.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Do you want this as a model? Definitely",
      "offset": 3951.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "not. So like we want it needs to learn.",
      "offset": 3953.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Okay, if I give it the next question,",
      "offset": 3956.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what is 8 plus 8? It should not just say",
      "offset": 3957.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "four or what is 2 minus 2? It shouldn't",
      "offset": 3960.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "say four. And so the goal of all these",
      "offset": 3963.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "algorithms is to somehow force the",
      "offset": 3965.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "models not to like overfit to your",
      "offset": 3967.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "question. And so like these formulations",
      "offset": 3969.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "are trying to do these things to like",
      "offset": 3972.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "not overfit.",
      "offset": 3974.559,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Yeah. Well, I'm thinking about like the",
      "offset": 3975.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "chess you were saying which scores the",
      "offset": 3977.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "board and produces this like",
      "offset": 3979.839,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "number",
      "offset": 3981.68,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "a number which is like this good or bad.",
      "offset": 3982.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Sometimes these well trained models have",
      "offset": 3984.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these novelties that you know where they",
      "offset": 3986.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "say like make this move. It's like not",
      "offset": 3988.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the new state is not obviously good or",
      "offset": 3990.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "whatever but they somehow like figured",
      "offset": 3992.559,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "out this.",
      "offset": 3994.319,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 3995.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "And suppose that your training mechanism",
      "offset": 3996.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "for the value function model you know",
      "offset": 3998.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "hasn't picked up on something like that.",
      "offset": 4001.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "In fact there's like some error in the",
      "offset": 4003.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "tendency of the value model.",
      "offset": 4006,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Okay. like it's probability of producing",
      "offset": 4008.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 4010.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "perfect scoring of the board.",
      "offset": 4012.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 4016.4,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "You know is not always exactly right.",
      "offset": 4016.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Yes. Always not into your training",
      "offset": 4018.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "process.",
      "offset": 4021.2,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Correct.",
      "offset": 4022.4,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 4023.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "How do you think about that? Like how do",
      "offset": 4024.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you like what is the",
      "offset": 4026.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "the value model? You have to train it",
      "offset": 4027.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "together. So it's like a combination of",
      "offset": 4029.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the entire algorithm. So the value model",
      "offset": 4031.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "predicts what is the probability but you",
      "offset": 4033.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "actually have to train this as well. And",
      "offset": 4035.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so that is actually the problem. Some",
      "offset": 4036.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "people you could train this you could",
      "offset": 4039.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "train this separately. You know you can",
      "offset": 4041.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like get all the chess possibilities and",
      "offset": 4042.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "then output what is the final number. I",
      "offset": 4044.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "think that's what some people do. You",
      "offset": 4046.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "could train this in tandem with the",
      "offset": 4047.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "model actually. I think that's actually",
      "offset": 4048.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "more harder. I don't know if that you so",
      "offset": 4050.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "there is always error in the value model",
      "offset": 4053.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "always. But you have to train this model",
      "offset": 4055.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "as well. So you will reduce the error",
      "offset": 4056.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "but there's always error. So I think",
      "offset": 4058.64,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "there's like some numbers you can like",
      "offset": 4059.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "force the value model to be like less",
      "offset": 4061.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "less prominent. like don't forcibly",
      "offset": 4062.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "utilize the reward uh the value function",
      "offset": 4064.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but in Japia we just get rid of the",
      "offset": 4067.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "reward model uh the value model anyways",
      "offset": 4069.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "um so totally gone um no so you don't",
      "offset": 4071.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "need to worry about that anymore um",
      "offset": 4073.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "okay I will keep going on let me just",
      "offset": 4075.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "check time actually",
      "offset": 4079.44,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 4082,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "so remember the goal is the advantage we",
      "offset": 4085.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "want to maximize advantage not reward",
      "offset": 4087.92,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "anymore advantage is the reward minus",
      "offset": 4089.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "minus the average reward or the base the",
      "offset": 4093.039,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "base reward right if the advantage is",
      "offset": 4095.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "less than zero it means that it is worse",
      "offset": 4097.759,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "than average is if the advantage is more",
      "offset": 4100.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "than zero it means it is better than",
      "offset": 4102.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "average and so the goal is we want to do",
      "offset": 4104.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the action more if it's better than",
      "offset": 4106.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "average on general",
      "offset": 4108.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "now to PO right so like I don't know if",
      "offset": 4111.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you guys have seen the PO formula it is",
      "offset": 4113.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "ugly but this is the PO formula right so",
      "offset": 4115.679,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "like it looks it's more confusing",
      "offset": 4118.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "because there's like a clip and then",
      "offset": 4121.359,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "there's epsilon and blah blah blah",
      "offset": 4122.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "whatever.",
      "offset": 4124.159,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "But we could just strip everything away.",
      "offset": 4125.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "It's just it's just the probability of",
      "offset": 4127.759,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "the action given the state times",
      "offset": 4131.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "advantage, right? We literally just",
      "offset": 4132.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "discussed about this. Okay, minus a log.",
      "offset": 4134.4,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Okay, the log's gone. But anyways, it's",
      "offset": 4137.679,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "just that and then the rest is the rest",
      "offset": 4139.759,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "is trying to reduce overfitting.",
      "offset": 4143.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And so remember, so essentially this",
      "offset": 4146.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "there's a thing called the division of",
      "offset": 4148.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the old model. And essentially it's the",
      "offset": 4150.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "model that created the action. And the",
      "offset": 4154.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "goal is we now want to maximize this",
      "offset": 4156.159,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "likelihood ratio. We don't just want to",
      "offset": 4157.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "maximize the probability of the model.",
      "offset": 4159.759,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "We don't want to maximize the",
      "offset": 4161.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "probability of the action given the",
      "offset": 4162.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "highest reward. We actually want to",
      "offset": 4164.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "maximize the likelihood instead. But",
      "offset": 4166,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "what is this likelihood?",
      "offset": 4168.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "So I did some numbers. I just made some",
      "offset": 4171.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "numbers up. So this is the pack. So",
      "offset": 4173.279,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "pretend the the top the um numerator is",
      "offset": 4175.199,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "0.01 and the denominator is 0.01.",
      "offset": 4178.4,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "0.01 divided by 0.01 is 1. If the if the",
      "offset": 4182.56,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "top is 0.01 and the bottom is 0.99.",
      "offset": 4187.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Remember these are all probabilities.",
      "offset": 4190.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "You divide the top from the bottom",
      "offset": 4192.719,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "you'll get 0.01. Again if the top is",
      "offset": 4194.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "0.99 and the bottom is 0.01 you'll get",
      "offset": 4196.64,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "99 and so on. Right? The last one's one.",
      "offset": 4199.28,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "And so the goal is 0.01",
      "offset": 4202.719,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "divided by 0.99 is 0.01.",
      "offset": 4206.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "This means that the action that you do",
      "offset": 4210.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "is actually very likely, right? Because",
      "offset": 4212.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the bottom the bottom thing is 0.99. But",
      "offset": 4214.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "we actually don't like this, right?",
      "offset": 4216.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Remember the top is 0.01. We do not like",
      "offset": 4218.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "this. So the ratio is 0.01.",
      "offset": 4220.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And then the bottom is this. This action",
      "offset": 4224,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the bottom the denominator is 0.01. It",
      "offset": 4226.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is actually not likely. But we actually",
      "offset": 4229.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like this because the top number is",
      "offset": 4231.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "0.99. And so when you multi when you do",
      "offset": 4233.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the division you get 99. So this is",
      "offset": 4235.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "actually good. And so actually we're not",
      "offset": 4237.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "actually trying to maximize the",
      "offset": 4239.6,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "probability. We're actually trying to",
      "offset": 4240.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "maximize the likelihood now.",
      "offset": 4241.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "And so the question is why don't we just",
      "offset": 4245.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "maximize the probability right? The",
      "offset": 4247.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "first equation. Why do we need to do",
      "offset": 4249.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this division thing? Because if we",
      "offset": 4250.719,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "maximize just the top you will have",
      "offset": 4253.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "reward hacking. What is 2 plus two? It",
      "offset": 4254.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "might say to solve this question we need",
      "offset": 4257.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to do blah blah blah blah blah blah blah",
      "offset": 4259.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "blah blah and suddenly it says hello",
      "offset": 4260.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "hello hello hello hello hello hello",
      "offset": 4262.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and then it says four. Is this good? I",
      "offset": 4264.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "don't think so. It's very good. We don't",
      "offset": 4267.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "want it to say hello hello hello",
      "offset": 4268.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "something or like some weird trace in",
      "offset": 4270.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the reasoning model. It does something",
      "offset": 4272.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "weird. We don't want this to happen. And",
      "offset": 4273.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so actually this this you know hello",
      "offset": 4275.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "hello hello hello hello is actually very",
      "offset": 4278.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "not likely. And so the goal of the",
      "offset": 4279.679,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "division is to reduce these issues.",
      "offset": 4282,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "The epsilon part is called the trust",
      "offset": 4288.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "region. Essentially we don't want to",
      "offset": 4290.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "make we don't want to do large steps for",
      "offset": 4292.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "um po right. We don't want to do large",
      "offset": 4294.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "steps and the the trick is we want to",
      "offset": 4297.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "restrict them right. So like you don't",
      "offset": 4299.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "want to overfit the model. So now we",
      "offset": 4300.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "restrict the model and so epsilon could",
      "offset": 4302.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "be like 0.2.1",
      "offset": 4304.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and the 1 minus epsilon is 0.8 8 1 +",
      "offset": 4307.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "epsilon is 1.2. And the trick is we just",
      "offset": 4310.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "want to not move the direction of the",
      "offset": 4313.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "gradient that much, right? We don't",
      "offset": 4315.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "trust the model that much. We don't",
      "offset": 4317.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "trust the algorithm that much. So, we",
      "offset": 4319.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "want to constrain it.",
      "offset": 4321.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "And then also the PO there's also a K",
      "offset": 4325.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "term. Um there's another term. Um and",
      "offset": 4327.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "essentially what this does is we want",
      "offset": 4330.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the model to be as close to the",
      "offset": 4331.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "supervised fine-tune model as much as",
      "offset": 4333.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "possible. We want it to be we don't want",
      "offset": 4334.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it to go so far away from the base model",
      "offset": 4337.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "or the pre-tra or the supervised",
      "offset": 4340.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "finetuning model. So essentially if it",
      "offset": 4341.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "deviates too much we want to tax the uh",
      "offset": 4343.679,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "we want to tax it and so this beta is",
      "offset": 4346.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "like 0.05 and the ko divergence is the",
      "offset": 4349.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "dis okay it's not a distance it's like",
      "offset": 4352.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the distance between the current model",
      "offset": 4354.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and the pre-trained model and",
      "offset": 4356.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "essentially we want to also shove this",
      "offset": 4358.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "into the equation. Um, so you can see",
      "offset": 4360.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with PPOs, there's many moving parts.",
      "offset": 4362.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "It's who cares about the equation? It's",
      "offset": 4364.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not that complicated. The point is all",
      "offset": 4367.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of these extra add-ons are just to",
      "offset": 4368.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "reduce overfitting and not to make the",
      "offset": 4370.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "model like randomly go to some weird",
      "offset": 4372.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "state um that like you know overfits to",
      "offset": 4374.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "like your questions. And so the trick of",
      "offset": 4377.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "PO is they just added all these terms in",
      "offset": 4379.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to make training more stable.",
      "offset": 4381.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And so the final equation is like this.",
      "offset": 4384.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "This again um hopefully you will to be",
      "offset": 4386.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "honest no one even counts the formula.",
      "offset": 4389.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "It's not that important. Um, but I just",
      "offset": 4390.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "try to like break it down into pieces.",
      "offset": 4392.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "And the goal, remember the goal is to",
      "offset": 4394.719,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "maximize this equation, right? We want",
      "offset": 4396.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to maximize it. And normally I just like",
      "offset": 4397.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to think about",
      "offset": 4400.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "this one, right? You just need to learn",
      "offset": 4402.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this one, right? You want to maximize",
      "offset": 4404.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the probab. So it's just this equation.",
      "offset": 4406.8,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Um, remember we did the table. Just this",
      "offset": 4409.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "is enough. You don't need to learn the",
      "offset": 4411.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "rest of the formulas. It's not very",
      "offset": 4413.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "interesting. Um,",
      "offset": 4415.44,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "yeah. Any questions? Yes.",
      "offset": 4417.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Yes, correct.",
      "offset": 4428.64,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "So the biggest problem is pretend you're",
      "offset": 4437.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like pretend you just started RL like",
      "offset": 4439.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "you you have that pre you have like the",
      "offset": 4441.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "base model you have like a supervised",
      "offset": 4443.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "fine tuned model and then you do RL the",
      "offset": 4445.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "gradient updates at the very beginning",
      "offset": 4448.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "are going to be gigantic right you're",
      "offset": 4449.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like what is 2 plus two it says four but",
      "offset": 4451.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "if it says five you want to like",
      "offset": 4455.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "penalize it dramatically and so the",
      "offset": 4456.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "problem is you don't actually want to do",
      "offset": 4458,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "large steps. And so the goal is you want",
      "offset": 4459.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "to constrain it. And so the constraint",
      "offset": 4461.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "factor is like you know if the if the if",
      "offset": 4463.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the num if the gradient update is",
      "offset": 4465.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "extremely large you just want to like",
      "offset": 4467.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "constrain all the numbers if that makes",
      "offset": 4469.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "sense. The goal is just to constrain the",
      "offset": 4471.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "update not to make it too large.",
      "offset": 4473.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "What about the ratio?",
      "offset": 4477.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Oh the the ratio it's the KI divergence.",
      "offset": 4478.88,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Oh sorry not the K the likelihood. To be",
      "offset": 4482.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "honest I think I need to do more",
      "offset": 4485.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "research. I would ask Gemini exactly",
      "offset": 4486.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "what it is.",
      "offset": 4489.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "That's my answer. I'm probably not the",
      "offset": 4491.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "best person to answer every single",
      "offset": 4493.679,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "question. Yes. Any other questions? Yes.",
      "offset": 4494.88,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 4512.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "It's the model that actually created the",
      "offset": 4518,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "action. And so the top one is all of the",
      "offset": 4519.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "numbers that actually like how do I",
      "offset": 4522.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "explain this? The bottom one is the",
      "offset": 4525.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "model that created the action. So for",
      "offset": 4527.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "example, you the model says you want to",
      "offset": 4528.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "go up, down, left or right.",
      "offset": 4530.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Is there a correct or a",
      "offset": 4532.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Oh, we just created the action. Like it",
      "offset": 4535.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "could be anything.",
      "offset": 4536.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "It could be the So it's the m it's the",
      "offset": 4538.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "maximum. It's whatever of action the",
      "offset": 4540.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "model says currently.",
      "offset": 4541.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "It might be wrong. It might be good. It",
      "offset": 4543.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "might be bad. It's just any action.",
      "offset": 4544.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Any other questions?",
      "offset": 4548.719,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 4551.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 4556.719,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "space.",
      "offset": 4569.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I don't think so can answer that",
      "offset": 4576.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "question. I don't know. That's why I",
      "offset": 4577.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "don't know. Maybe research papers show",
      "offset": 4579.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it. I I'm not sure. Ed.",
      "offset": 4580.64,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "Okay. Okay. Well, okay. So, GRPO the",
      "offset": 4584.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "trick from PO is we remember remove the",
      "offset": 4588.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "value model. We get rid of it entirely.",
      "offset": 4590.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "We do not want to estimate the average",
      "offset": 4592.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "reward. It's totally removed. Um and the",
      "offset": 4594.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "reward model is now removed as well for",
      "offset": 4597.199,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "a reward function.",
      "offset": 4599.6,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "So, we get yeah we remove it right",
      "offset": 4603.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "remember the value model is removed. B",
      "offset": 4605.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is a reward value model. We get rid of",
      "offset": 4607.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it entirely. But what do we replace? So,",
      "offset": 4609.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the trick of GPO is we do roll out or",
      "offset": 4612.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "inference sampling.",
      "offset": 4614.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "we get the answer what is 2 plus two you",
      "offset": 4616,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "literally make four inferences you just",
      "offset": 4618.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "literally call the model four times it",
      "offset": 4620.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "could say the answer is zero the answer",
      "offset": 4622.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is one the answer is two or the answer",
      "offset": 4624.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "is four you can do you do like you just",
      "offset": 4626.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "literally call the model four times",
      "offset": 4628.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "and you take the reward right zero the",
      "offset": 4631.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "what is 2 plus 2 the correct answer is",
      "offset": 4634.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "four so you want the last number to be",
      "offset": 4636.56,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "one but the rest is all zero",
      "offset": 4638.96,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "and the trick is you literally just take",
      "offset": 4643.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "the statistics of your current roll out.",
      "offset": 4646.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "You take the statistics of all of this.",
      "offset": 4649.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "You literally take the reward minus the",
      "offset": 4651.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "mean divided by the standard deviation.",
      "offset": 4653.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "You get the zed score. And this is your",
      "offset": 4654.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this is your base model. This is your",
      "offset": 4657.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "value model, right? This is there's no",
      "offset": 4659.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "more value model anymore. It's just a",
      "offset": 4661.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "number.",
      "offset": 4663.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "And so I did this on a table as well,",
      "offset": 4664.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "right? What is 2 plus two? If you think",
      "offset": 4666.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it's zero, remember the prediction could",
      "offset": 4669.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "be zero, one, two, or four. And your",
      "offset": 4671.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "reward could be 000 or one. And if you",
      "offset": 4674.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "take the mean or the average of all the",
      "offset": 4677.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "rewards, you get 0.375. If you take the",
      "offset": 4678.719,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "standard deviation, you take 0.43301.",
      "offset": 4681.199,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "And then you do the reward minus the",
      "offset": 4684.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "mean divided by the standard dev",
      "offset": 4686.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "deviation, you get some numbers.",
      "offset": 4687.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Remember the number four is correct.",
      "offset": 4690,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "That is why you know reward minus the",
      "offset": 4693.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "mean divided by the standard deviation",
      "offset": 4695.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is 1.44. It's the largest number. And so",
      "offset": 4696.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that is why we need to like max we need",
      "offset": 4700,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to essentially maximize that good answer",
      "offset": 4701.679,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "and we want to reduce the bad answers.",
      "offset": 4703.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "But why is it called group relative in",
      "offset": 4707.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "GPO? Because it's not just one question.",
      "offset": 4709.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "It's many questions. It could be what is",
      "offset": 4712.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "2 plus 2? What is 4 plus 4? Okay, well",
      "offset": 4714.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "my graphs are all the same. My plots are",
      "offset": 4716.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "all the same. But anyways, imagine",
      "offset": 4718.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's like four different tables. What",
      "offset": 4719.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "is 2 plus2? What is 4 plus 4? How do I",
      "offset": 4721.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "create this Python function? You know,",
      "offset": 4724.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "whatever. And there will be four tables.",
      "offset": 4726.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And so the goal group relative just",
      "offset": 4728.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "means you for each question we take the",
      "offset": 4730.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "statistics within each group.",
      "offset": 4732.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "For example, what is 2 plus2? You create",
      "offset": 4736.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "four. You literally call the model four",
      "offset": 4738.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "times and you get some, you know,",
      "offset": 4740.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "answers. What is 4 plus4? You call it",
      "offset": 4742.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "four times, right? Create Python code,",
      "offset": 4744.08,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "you call it four times.",
      "offset": 4746,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "Yes, there are other factors of G. So",
      "offset": 4750.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "essentially we already explained what",
      "offset": 4752.719,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "GPO is, right? Everything you need to",
      "offset": 4753.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know about GRP we already explained in",
      "offset": 4755.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in the total mathematical formula looks",
      "offset": 4757.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "kind of like this. Um",
      "offset": 4758.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there's some rearrangement. For example,",
      "offset": 4761.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the minus beta the KR divergence is just",
      "offset": 4763.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "taken out of the reward function. That's",
      "offset": 4766,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the only other difference. Um hopefully",
      "offset": 4767.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it makes more sense about the parts of",
      "offset": 4771.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the GPO formula and stuff like that.",
      "offset": 4772.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "It's actually not that complicated to",
      "offset": 4774.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "understand. The majority is just trying",
      "offset": 4775.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to reduce overfitting, right? That's the",
      "offset": 4777.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "whole goal, right? minus beta times a k",
      "offset": 4779.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "divergence is to reduce overfitting. One",
      "offset": 4782.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "minus epsilon 1 plus epsilon is to",
      "offset": 4784.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "reduce overfitting. The division reduce",
      "offset": 4786.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "overfitting. Everything's reducing",
      "offset": 4788.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "overfitting. Right? So that's all of",
      "offset": 4789.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "machine learning and AI. It's just to",
      "offset": 4791.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "make the training more stable and to",
      "offset": 4794.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "reduce overfitting.",
      "offset": 4795.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I would highly suggest there is these",
      "offset": 4798.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "are the two things that I really highly",
      "offset": 4800.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "suggest. Um Nathan Lambert's policy",
      "offset": 4802.32,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "gradients um book. It's online though.",
      "offset": 4804.88,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "Very very very very helpful. Um and um",
      "offset": 4808.239,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Yanick's video on GPO and stuff very",
      "offset": 4811.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "very very helpful as well. Um yeah and",
      "offset": 4814.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "now I will go into a collab",
      "offset": 4817.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "demonstration of GPO. Um before that",
      "offset": 4819.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like does anyone have any questions? Let",
      "offset": 4821.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "me just check time",
      "offset": 4823.52,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "questions. Yes.",
      "offset": 4825.6,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Because maybe the answer is there",
      "offset": 4847.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 4850.32,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "to me",
      "offset": 4852.88,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "whatever information.",
      "offset": 4857.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 4860.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Memorization. You make more",
      "offset": 4861.92,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "memorization.",
      "offset": 4865.199,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "To answer your question another way, I",
      "offset": 4870,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think it's actually because GRP itself",
      "offset": 4871.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "is a problem. Remember the goal of all",
      "offset": 4873.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "these algorithms is to force the model",
      "offset": 4875.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "not to detract too much from the",
      "offset": 4877.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "original model, right? With this like",
      "offset": 4879.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "minus beta K divergence, you know, one",
      "offset": 4881.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "minus epsilon, all of this is trying to",
      "offset": 4883.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "make the model not go towards too far",
      "offset": 4884.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "away from the original model. And I",
      "offset": 4887.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "think that's the problem because you're",
      "offset": 4889.199,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "essentially forcing the model not to go",
      "offset": 4890.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "too far. And so maybe there might be",
      "offset": 4892.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "some new algorithm I don't know",
      "offset": 4894.719,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "something some other formulation which",
      "offset": 4897.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you know you want to go very far away",
      "offset": 4900.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you could do that um I I don't know if",
      "offset": 4903.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there are any I don't know if there's",
      "offset": 4905.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "any research papers about that I I don't",
      "offset": 4907.6,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "know",
      "offset": 4909.12,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "but you could yes you could do that",
      "offset": 4909.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "yes any other questions yeah yeah yeah",
      "offset": 4912.239,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "you probably saying",
      "offset": 4915.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 4919.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that's why",
      "offset": 4920.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yes. Yes.",
      "offset": 4922.8,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "Energy based models.",
      "offset": 4923.44,
      "duration": 1.44
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 4924.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "So what do you think about that?",
      "offset": 4924.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "What do I think about it? I can't really",
      "offset": 4926.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "comment. I mean he definitely you should",
      "offset": 4928.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "listen to what he says. Um",
      "offset": 4930.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "but you don't see anything like",
      "offset": 4933.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "movements in open source.",
      "offset": 4934.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "I don't think so. I think open source we",
      "offset": 4936.719,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "kind of got captivated by RL GPO. I",
      "offset": 4938.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "don't think so open source people are",
      "offset": 4942.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "doing whatever he's talking about.",
      "offset": 4943.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Unfortunately",
      "offset": 4946.719,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "I think he needs to talk about it more.",
      "offset": 4947.76,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "Yeah. Japa. Yeah. And energy based",
      "offset": 4949.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "models. I unfortunately I don't think",
      "offset": 4950.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "so. Open source. Yeah, maybe we should",
      "offset": 4952.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "talk about it more, but yeah, I I don't",
      "offset": 4954.56,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "think so. Um Yeah. Yeah. Yes.",
      "offset": 4956.08,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 4966.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Yes. You got rid of it.",
      "offset": 4967.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Yes, correct.",
      "offset": 4981.199,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 4990.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "group.",
      "offset": 5004.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So this is more about an optimization",
      "offset": 5020.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "question. So in theory you should make",
      "offset": 5022.639,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "the you should make for example I just",
      "offset": 5025.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "selected four right what is 2 plus two",
      "offset": 5027.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "create four examples you should do you",
      "offset": 5030.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "know as many as you like you know 3,000",
      "offset": 5033.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "whatever number you like you should do",
      "offset": 5036.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "as much as possible but remember",
      "offset": 5038.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "AI is about optimization this is going",
      "offset": 5041.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to take forever you know it's all about",
      "offset": 5043.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "efficiency so probably don't do as many",
      "offset": 5044.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "as you like um but you should in the",
      "offset": 5046.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "limit you should do that but you know",
      "offset": 5048.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "everyone can't just wait there waiting",
      "offset": 5050.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for the computer to spin. Um so yes, you",
      "offset": 5052.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "you should do as much as you like. Um",
      "offset": 5054.239,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "it's but yes also for like",
      "offset": 5057.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "recommendations",
      "offset": 5059.6,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "when you do inference sampling you",
      "offset": 5061.199,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "should set set temperaturees like you",
      "offset": 5062.639,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "know 1.2 1.5 set m to be 0.1 you know",
      "offset": 5063.84,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "something like that. If you set",
      "offset": 5068.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "temperature to be zero you'll have the",
      "offset": 5070.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "same answer every single time. So",
      "offset": 5072.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "definitely don't do that. But you should",
      "offset": 5073.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "have high temperature numbers to make",
      "offset": 5074.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the model you know produce new output as",
      "offset": 5076.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "much as possible. maximize variability.",
      "offset": 5080.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Yes,",
      "offset": 5082.56,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "distribution.",
      "offset": 5083.679,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "You should try your best to maximize",
      "offset": 5084.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "variability. Your outputs should not be",
      "offset": 5086.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "all the same. Um, if it's all the same,",
      "offset": 5088.159,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "I don't think so is going to learn. So,",
      "offset": 5089.84,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you should make it as different as",
      "offset": 5091.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "possible. So, that's why you should set",
      "offset": 5092.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "temperature to be 1.2, 1.5, whatever,",
      "offset": 5094.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "some large number. Don't do too large",
      "offset": 5097.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "though. Um,",
      "offset": 5098.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "any other questions?",
      "offset": 5101.04,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 5103.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Yeah. I'm like a situation where like",
      "offset": 5103.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "all",
      "offset": 5107.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "are not",
      "offset": 5109.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Yes, correct.",
      "offset": 5113.6,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "All the first steps are all the rewards",
      "offset": 5130.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "are basically giving no signal at all of",
      "offset": 5133.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the underlying",
      "offset": 5136.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "train.",
      "offset": 5138.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "How do you deal with that? I have faced",
      "offset": 5140.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that a lot of times with",
      "offset": 5142.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "moving to a larger base model helps",
      "offset": 5145.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "sometimes",
      "offset": 5147.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "knowledge training data like",
      "offset": 5150,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Yes, that's a good question. So",
      "offset": 5153.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "essentially you're saying the model if",
      "offset": 5154.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the model starts off with like no reward",
      "offset": 5156.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "like every single update is like 000000",
      "offset": 5158.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "it's not going to do anything. Yes, that",
      "offset": 5163.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "happens all the time. But by chance,",
      "offset": 5165.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "just by chance, you know, you have like",
      "offset": 5168.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "some small little little probability",
      "offset": 5170.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "just by chance, you will have some",
      "offset": 5172.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reward. So that's the trick. You will",
      "offset": 5174.159,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "see this after 10,000 inferences. What",
      "offset": 5176.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is 2 plus 2? Suddenly the model says",
      "offset": 5180.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "four suddenly. Okay, just suddenly just",
      "offset": 5182,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "by random probability. Let's make this",
      "offset": 5184.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "small. That's all that's all of GPO.",
      "offset": 5186.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Like this is addition simple task if it",
      "offset": 5189.6,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "was like a proof of concept. It may",
      "offset": 5192.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "never come with",
      "offset": 5195.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "yes may never. But remember you're not",
      "offset": 5197.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "doing this one question. You're also",
      "offset": 5199.6,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "shoving this together with other",
      "offset": 5200.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "questions. What is 2 plus 2? What is 4",
      "offset": 5202.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "plus 4? What is you know derive the",
      "offset": 5203.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "derivative of blah do this Python",
      "offset": 5206.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "function.",
      "offset": 5209.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "This step is very large. You essentially",
      "offset": 5211.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "shove this all together. And the trick",
      "offset": 5212.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "is in general it works. in general maybe",
      "offset": 5215.36,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "by bad luck it might not work but I feel",
      "offset": 5219.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "like the bad luck won't last forever",
      "offset": 5222.239,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "because remember you're changing the",
      "offset": 5223.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "samples right so like the question what",
      "offset": 5224.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is 2 plus2 you're changing that the next",
      "offset": 5226.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "phase will be some other question and so",
      "offset": 5228.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the trick is just by chance you will",
      "offset": 5230.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "have a good reward just by chance and we",
      "offset": 5232.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "just force that to be more",
      "offset": 5235.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "does that kind of make sense so it's to",
      "offset": 5238.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "be honest it's all luck yes it's all",
      "offset": 5240.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "luck we're just guessing oh you know",
      "offset": 5242.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we're praying that there's going to be",
      "offset": 5245.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "some positive reward somewhere in the",
      "offset": 5247.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "model. There will be negative reward,",
      "offset": 5249.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right? So like if your model is really",
      "offset": 5251.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "really bad, you you can do negative",
      "offset": 5252.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "reward. And so you just don't want to do",
      "offset": 5254.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the negative one. You just want to do",
      "offset": 5256.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the negative one less.",
      "offset": 5257.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And by miraculous probability, you know,",
      "offset": 5260,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "just rely on probabilities, you will get",
      "offset": 5263.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "a good reward somewhere just by chance.",
      "offset": 5265.199,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "That does that kind of make sense? I",
      "offset": 5268.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "mean, all of the large model labs are",
      "offset": 5271.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "just literally relying on the fact that",
      "offset": 5272.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that's what they're doing. They're just",
      "offset": 5274.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "guessing. We're just praying for the",
      "offset": 5275.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "GPUs to work and then suddenly the",
      "offset": 5277.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "reward comes out. I'm being serious.",
      "offset": 5279.84,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "That's exactly what they do. They're",
      "offset": 5281.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "just waiting for the algorithm to work",
      "offset": 5282.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "and if suddenly oh okay that's why",
      "offset": 5284.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "people do random seeds as well. So for",
      "offset": 5287.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "example the initialization of the model",
      "offset": 5288.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "might not be good. So you just kill the",
      "offset": 5290.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "training run. You do like 500 training",
      "offset": 5292.48,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "runs. Oh no 499 of them are like zero",
      "offset": 5295.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "reward. Oh just kill them all. Don't",
      "offset": 5298.719,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "release them.",
      "offset": 5300.32,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "I have seen on my training.",
      "offset": 5305.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "Yes. Very common.",
      "offset": 5309.52,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "As small as step.",
      "offset": 5312.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Yes, that's the Yes, I was going to show",
      "offset": 5317.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you guys that.",
      "offset": 5318.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Exactly. So like you could force the",
      "offset": 5321.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "model to answer some question like for",
      "offset": 5323.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "example you ask a question what is 2",
      "offset": 5325.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "plus two? It's very easy. It's four. You",
      "offset": 5326.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "just force it to learn oh okay it should",
      "offset": 5329.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "be four first. And then you do other",
      "offset": 5331.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "steps that is actually why remember?",
      "offset": 5334,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Okay, I have to go back to all the",
      "offset": 5336,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "slides. I don't remember. Okay, where is",
      "offset": 5337.199,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "it? How do I Okay, I'll exit. Uh, it's",
      "offset": 5340.48,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "the same as this problem. Where is it?",
      "offset": 5344.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "This one, right? Someone asked about why",
      "offset": 5346.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "don't you just start from the blue, you",
      "offset": 5348.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "know, the pre-trained model to go to the",
      "offset": 5349.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "green one. It's the same thing.",
      "offset": 5351.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Essentially, the trick is we want to do",
      "offset": 5353.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "some supervised fine tuning to make it",
      "offset": 5354.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "know some instructions. So it knows",
      "offset": 5356.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "something and then you want to go to the",
      "offset": 5359.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reinforcement learning phase. But if you",
      "offset": 5361.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "want to start from nothing like just the",
      "offset": 5363.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "pre-training phase that's the hard part,",
      "offset": 5365.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right? Your reward might be 00 Z00 Z00",
      "offset": 5366.96,
      "duration": 1.52
    },
    {
      "lang": "en",
      "text": "Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00",
      "offset": 5368.32,
      "duration": 0.399
    },
    {
      "lang": "en",
      "text": "Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00",
      "offset": 5368.48,
      "duration": 0.4
    },
    {
      "lang": "en",
      "text": "Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00",
      "offset": 5368.719,
      "duration": 0.721
    },
    {
      "lang": "en",
      "text": "Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z like",
      "offset": 5368.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "zero forever and then suddenly one you",
      "offset": 5369.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "know suddenly",
      "offset": 5371.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "oh if you suff",
      "offset": 5375.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that's just unlucky then I I think like",
      "offset": 5377.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "if you if you see zero rewards most",
      "offset": 5379.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "likely either one your reward function",
      "offset": 5381.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is not that good",
      "offset": 5383.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "yes doing priming or like you know make",
      "offset": 5385.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the model learn a little bit about your",
      "offset": 5387.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "data is actually does work does help um",
      "offset": 5389.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so there are tricks to make it work but",
      "offset": 5391.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "generally I would just say it's bad luck",
      "offset": 5393.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just bad luck um And unfortunately, you",
      "offset": 5395.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "can't do anything. It's not your fault.",
      "offset": 5397.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "It's Yeah. Just unfortunate. Yeah. Yes.",
      "offset": 5399.28,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "How should you think about like when or",
      "offset": 5403.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "like what to expect from",
      "offset": 5406.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "is it going to be just that this is the",
      "offset": 5408.8,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "way open source catches up with clos",
      "offset": 5410.48,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "your competent ML engineer to be able to",
      "offset": 5417.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like specialize a model for like I just",
      "offset": 5419.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "don't is there consensus on like where",
      "offset": 5422.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "what this is going to bring us to smart",
      "offset": 5426.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "people like you going to give us a",
      "offset": 5428,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "really good open source model or is it",
      "offset": 5429.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that you know we should This is a new",
      "offset": 5431.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tool for specialization.",
      "offset": 5432.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "The algorithm is not special. The hard",
      "offset": 5434.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "part is actually the reward functions",
      "offset": 5438.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "itself and the data that you're going to",
      "offset": 5439.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "shove into the model. That's the hard",
      "offset": 5441.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "part. So like I I think there is a",
      "offset": 5443.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "misconception like you know the",
      "offset": 5445.44,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "algorithm is important. No science it's",
      "offset": 5446.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "useless. Who cares about the algorithm?",
      "offset": 5447.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "You can literally just use you know the",
      "offset": 5449.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "general the function which I gave you",
      "offset": 5450.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "can just use any algorithm that you",
      "offset": 5452.639,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "like. But the problem is actually the",
      "offset": 5453.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "reward function itself. I gave you some",
      "offset": 5455.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "examples you know like what is 2 plus",
      "offset": 5457.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "two? The answer is four. Yes, you can do",
      "offset": 5459.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "distance- based, but that's just one",
      "offset": 5461.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "example. Can you like can someone make a",
      "offset": 5464,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "function? Can someone make a reward",
      "offset": 5467.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "function for like trading stocks? That's",
      "offset": 5468.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you do that. Do that, right? And then",
      "offset": 5471.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "there you have like a model for trading.",
      "offset": 5472.96,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "Go ahead.",
      "offset": 5474.239,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "So So you think it's going to be more",
      "offset": 5474.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that because people are able to create",
      "offset": 5476.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "reward functions like it's a little bit",
      "offset": 5478.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "more easy like similar to like you could",
      "offset": 5480,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "create a prompt. It's an easier thing",
      "offset": 5481.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for most people to like iterate on. Um",
      "offset": 5483.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's actually quite hard",
      "offset": 5486.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "coming up but it's easier than coming up",
      "offset": 5487.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "with the algorithm itself or whatever",
      "offset": 5489.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually I think collecting so in the",
      "offset": 5491.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "olden days large model labs will ask",
      "offset": 5493.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like you know large data providers like",
      "offset": 5496,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "scale or whatever to create data like",
      "offset": 5497.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "what is 2 plus two you literally have",
      "offset": 5500.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "someone sit there and write okay the",
      "offset": 5502.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "answer is four but then you also have to",
      "offset": 5504.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "do the chain of thought like oh I think",
      "offset": 5505.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the answer is four because of blah blah",
      "offset": 5508,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "blah blah blah blah blah or like you",
      "offset": 5509.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "know this is my working out you",
      "offset": 5510.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "literally have to ask someone sitting",
      "offset": 5512.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there to make the data",
      "offset": 5514,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "The trick is get no more. You don't need",
      "offset": 5515.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the data labeling step anymore. It's",
      "offset": 5518.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "totally gone. You have the answer and",
      "offset": 5519.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you have the question. The middle step",
      "offset": 5522.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "is totally removed. But you still need",
      "offset": 5524.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to make the reward function. You need to",
      "offset": 5526.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "ver you need to say is the answer for",
      "offset": 5528.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "good or bad. For maths, it's very easy",
      "offset": 5531.44,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "for code. For code, it's somewhat easy.",
      "offset": 5534.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "You know, you can check, oh, did you",
      "offset": 5538.159,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "import the correct function? Did you",
      "offset": 5539.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "import the library? Did did your code",
      "offset": 5540.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "execute, you know, some other reward",
      "offset": 5543.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "functions? But it's still hard to verify",
      "offset": 5545.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "if your actual function is correct. For",
      "offset": 5547.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "example, let's say your question, let's",
      "offset": 5548.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "say the task was create the Flappy Bird",
      "offset": 5550.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "game. How do you actually know that the",
      "offset": 5552.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "output's good? How do you actually know?",
      "offset": 5555.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "We don't you could again ask a human to",
      "offset": 5557.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "verify or you know, let's test a Flappy",
      "offset": 5560.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Bird game",
      "offset": 5563.199,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "and then give it a good reward, a bad",
      "offset": 5564,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "reward. Or the trick is did the game",
      "offset": 5565.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "actually run? If it ran, plus one. Did",
      "offset": 5568.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "you see the word Flappy Bird in the, you",
      "offset": 5571.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "know, Flappy Bird inside of the",
      "offset": 5574.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "functions? If yes, plus one. Did you see",
      "offset": 5575.52,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the, you know, the image of the Flappy",
      "offset": 5578.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Bird sprite be used? If yes, plus one",
      "offset": 5580.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "there. Something like that. So, like you",
      "offset": 5583.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "don't, it's still I would say the",
      "offset": 5585.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "hardest part is writing the reward",
      "offset": 5587.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "functions. And for open source",
      "offset": 5589.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "specifically,",
      "offset": 5591.679,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "if you know the whole open source",
      "offset": 5593.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "community starts writing reward",
      "offset": 5594.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "functions, we can probably beat 03 01",
      "offset": 5595.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like you know. Okay. plus compute. You",
      "offset": 5598.159,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "still need compute. That's the problem.",
      "offset": 5599.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "You still need compute. But if you write",
      "offset": 5601.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "good reward functions, you'll probably",
      "offset": 5602.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "catch up in no time.",
      "offset": 5605.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "So the end state here is that you want",
      "offset": 5606.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "an open version of the closed model.",
      "offset": 5608.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "It's not I guess my original question is",
      "offset": 5611.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like is this a thing that people are",
      "offset": 5613.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "going to use like prompts have",
      "offset": 5615.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "specialized models separately or is it",
      "offset": 5617.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "more that we want",
      "offset": 5619.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "good open?",
      "offset": 5621.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "It okay that is a good question. It",
      "offset": 5624.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "depends on which school of thought",
      "offset": 5626.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you're in. If you're in the thought that",
      "offset": 5628.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "large language models already have the",
      "offset": 5630.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "capability and you're just trying to",
      "offset": 5631.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "accentuate it, then there will be just",
      "offset": 5633.12,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "one model. Yes, but this model can only",
      "offset": 5636.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "learn some facts because otherwise",
      "offset": 5639.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you're like overfitting and like it's",
      "offset": 5641.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "not but if you're in the second camp",
      "offset": 5642.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that the model actually learns something",
      "offset": 5645.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "new. Oh wait, did I say right? I think",
      "offset": 5646.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it's the opposite way around. Um the",
      "offset": 5648.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "first one is you have many models. I saw",
      "offset": 5650.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "sorry I said it wrong. The first one is",
      "offset": 5651.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "you have many models because a model",
      "offset": 5653.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "doesn't actually learn that much. But if",
      "offset": 5654.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "the second one is a model actually",
      "offset": 5656.239,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "learns everything then you'll have this",
      "offset": 5657.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "one gigantic model. I think open air",
      "offset": 5658.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "probably subscribes to that point. You",
      "offset": 5660.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "know, like most large model labs think",
      "offset": 5662.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that actually RL can get you to AGI,",
      "offset": 5663.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "right? It will know everything about",
      "offset": 5666,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "everything. Any single question you ask,",
      "offset": 5667.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it already knows. And so like that's I",
      "offset": 5669.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "think that's where they're trying to go",
      "offset": 5671.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "for. For open source, it's more harder.",
      "offset": 5672.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I think the open source community",
      "offset": 5675.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "consensus for now for now is the model",
      "offset": 5676.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "it already knows your questions. You're",
      "offset": 5680.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "just trying to accentuate it. And by",
      "offset": 5682.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "doing reward functions, you're trying to",
      "offset": 5684.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "weight the circuits more. you're trying",
      "offset": 5686.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "to wait the model to know how to do",
      "offset": 5688.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "these equations and stuff like that. So",
      "offset": 5689.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I think like",
      "offset": 5691.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the goal of open source is you know if",
      "offset": 5693.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the entire community comes up with good",
      "offset": 5695.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "reward functions write them all then the",
      "offset": 5697.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "problem is we need compute that's the",
      "offset": 5700.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "second problem right if you shove both",
      "offset": 5702,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of them together you will get 0 o 10 or",
      "offset": 5703.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "something I don't know um right imagine",
      "offset": 5705.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "if every single person writes a reward",
      "offset": 5707.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "function once per day okay that's",
      "offset": 5709.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "probably too hard once per day will have",
      "offset": 5710.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like you know seven billion reward",
      "offset": 5712.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "functions more than open air can ever",
      "offset": 5713.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "come up with and they will defeat open",
      "offset": 5715.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "air",
      "offset": 5717.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "but you know you need the computer part",
      "offset": 5718.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that's the only Um,",
      "offset": 5720.239,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "okay. Any other questions? Yes.",
      "offset": 5723.44,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 5730.56,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Correct.",
      "offset": 5733.76,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 5735.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "How do you feel like saving those traces",
      "offset": 5736.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of those?",
      "offset": 5740.08,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Very smart. That's what Yes. Yes. Yes. I",
      "offset": 5740.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "don't know if large model labs do that.",
      "offset": 5742.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "You could do that. Yes.",
      "offset": 5743.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "I feel like Yeah. We're just mining for",
      "offset": 5745.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "those",
      "offset": 5747.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "good examples.",
      "offset": 5748.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "The only problem I would say is pretend",
      "offset": 5750.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the question was what is 2 plus two",
      "offset": 5753.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "right and then the model says okay",
      "offset": 5755.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "pretend the model just says this okay it",
      "offset": 5758.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "says oh let me work out what is 2 plus",
      "offset": 5759.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "two I think it is oh you know the number",
      "offset": 5761.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "two means two apples and I want to add",
      "offset": 5764.639,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "two more apples I think the answer might",
      "offset": 5767.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "be three h but let me rethink about it",
      "offset": 5769.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "wait a second it's like four is that a",
      "offset": 5772.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "should you fine tune on that I mean you",
      "offset": 5774.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "could but maybe it's like cheating Maybe",
      "offset": 5777.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "it just says four by chance. Maybe like",
      "offset": 5779.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Okay, I'll give you an example. Let's",
      "offset": 5782.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "say that by chance. Okay, the question",
      "offset": 5784.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "was what is 2 plus two. It says",
      "offset": 5786,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "gibberish like um I like to go to Paris",
      "offset": 5788.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "for fun or whatever. I don't know. I",
      "offset": 5791.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like to go to the you know this event",
      "offset": 5793.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "blah blah blah blah blah blah blah blah",
      "offset": 5796.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "blah and then suddenly says four",
      "offset": 5797.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "just by chance. Remember we're still",
      "offset": 5799.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "rewarding this. We're literally",
      "offset": 5802,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "rewarding this as good. But this is not",
      "offset": 5803.28,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "good.",
      "offset": 5805.679,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "No, so we we reward it at the very end.",
      "offset": 5809.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Remember we see the number four. It is",
      "offset": 5810.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "good. The question was what is 2 plus",
      "offset": 5812.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "two? The model can generate anything it",
      "offset": 5814.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "likes.",
      "offset": 5817.28,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "It could",
      "offset": 5818.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "you could but that gets harder. So the",
      "offset": 5820.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "trick is people don't actually reward",
      "offset": 5822.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "the steps in between. They just do the",
      "offset": 5823.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "final step because otherwise it gets too",
      "offset": 5825.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "complicated, right? So like what is",
      "offset": 5827.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "intermediate steps reward? It's way too",
      "offset": 5828.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "complicated. So what you do is you just",
      "offset": 5830.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "reward the final step. So if if you see",
      "offset": 5832.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "the number four, it's good.",
      "offset": 5835.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "But we don't know how we got there. So",
      "offset": 5837.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you should So yes, you could maybe at",
      "offset": 5839.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "the very very very end step of RL you",
      "offset": 5842.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "can then use them data to do do um",
      "offset": 5845.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "fine-tuning. Yes, you could. But I think",
      "offset": 5848.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like in general it's because we don't",
      "offset": 5850.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "know what the process is in between.",
      "offset": 5852,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "You say we don't train on",
      "offset": 5853.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Oh no, we don't train on the thought.",
      "offset": 5857.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Yes, we don't. But",
      "offset": 5858.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "problem is",
      "offset": 5860.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we don't train on the intermediate step",
      "offset": 5862.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in between. You don't have to.",
      "offset": 5864.48,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "You you could you could but remember we",
      "offset": 5867.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "don't know if the traces are good or",
      "offset": 5870.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "bad. We don't know. So you can't just",
      "offset": 5871.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "take this trace and then do supervised",
      "offset": 5874.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "fine tuning because pretend the answer",
      "offset": 5876.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "was four is good but we don't know the",
      "offset": 5878.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "intermediate steps unless if you read",
      "offset": 5881.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the data right you could ask you know",
      "offset": 5882.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "some human labelist to like oh you know",
      "offset": 5884.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "please verify if this trace is good. You",
      "offset": 5886.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you could but then that kind of defeats",
      "offset": 5889.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the whole purpose of RL. So like you",
      "offset": 5890.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "don't you don't want to do this. Um does",
      "offset": 5892.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "that kind of make sense or not really?",
      "offset": 5894.239,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Okay. Yes.",
      "offset": 5897.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "What? Sorry.",
      "offset": 5900.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "Oh, yeah. Yeah. Yeah. We Yes. We will do",
      "offset": 5901.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that. Yes. Yes. Yes.",
      "offset": 5903.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "If we have multiple categories",
      "offset": 5905.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "for math.",
      "offset": 5908.08,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 5909.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "What's the normalization among the",
      "offset": 5909.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "best practices?",
      "offset": 5913.04,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "What do you mean by normalization",
      "offset": 5914.239,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "amongst rewards?",
      "offset": 5915.36,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "We have four plus four",
      "offset": 5916.32,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "gets zero. Then you have one python.",
      "offset": 5923.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "just be a compile binary",
      "offset": 5925.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "function.",
      "offset": 5929.28,
      "duration": 0.959
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 5929.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "If we're running this in one big",
      "offset": 5930.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Yes. One batch.",
      "offset": 5932.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "How do we normalize the fact that",
      "offset": 5933.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "good question?",
      "offset": 5935.84,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "Correct. It could be like minus 10.",
      "offset": 5937.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Very good question. That's your choice.",
      "offset": 5943.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Unfortunately, that's the problem of RL.",
      "offset": 5945.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "It's all about human choice. Like you",
      "offset": 5947.6,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "you will have to decide, you know, is",
      "offset": 5949.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the Python one more important than your",
      "offset": 5951.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "um than your maths? what what is 2 plus2",
      "offset": 5953.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then you can weight it more for example",
      "offset": 5955.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "your make 2 plus two the reward as minus",
      "offset": 5957.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "one and one and the Python function as",
      "offset": 5960.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like 1,00 and zero right you you have to",
      "offset": 5962.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "decide on the waiting functions that is",
      "offset": 5965.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that is your choice um unfortunately it",
      "offset": 5966.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "is a",
      "offset": 5970,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's kind of like an art you could like",
      "offset": 5971.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "dumbly you could just do everything is",
      "offset": 5974.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the same scale I think that's what most",
      "offset": 5976.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "large I think that's what the large",
      "offset": 5978,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "model labs probably do is like",
      "offset": 5979.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "everything all the reward functions have",
      "offset": 5980.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the same scale you know plus one minus",
      "offset": 5982.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "plus oneus one notus",
      "offset": 5984.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "100.",
      "offset": 5987.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So it's up to you.",
      "offset": 5989.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 5991.52,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "What about stuff like you know is this a",
      "offset": 5992.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "good summary? Is this a how we how we",
      "offset": 5993.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "creating",
      "offset": 5996.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that is a question. So like now you want",
      "offset": 5997.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to like analyze okay that's where the",
      "offset": 6000.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "LLM as a judge comes in. So like there",
      "offset": 6001.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "is a school of thought that you can use",
      "offset": 6003.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a language model itself to to make a",
      "offset": 6004.8,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "number. You can ask chbt is this a good",
      "offset": 6007.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "summary or is this a bad summary? Please",
      "offset": 6011.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "give me a score from minus 10 to 10. Ask",
      "offset": 6013.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "LGBT. You could do that. That's called",
      "offset": 6015.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the LLM as a judge thing. Um there is a",
      "offset": 6017.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "paper which shows that you can do this",
      "offset": 6020.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for some time but then it breaks down.",
      "offset": 6022.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "So you can't just keep calling the",
      "offset": 6025.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "language model you know like it's kind",
      "offset": 6026.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of like cheating if I would say like",
      "offset": 6028.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you're trying to call chatbt to train",
      "offset": 6029.76,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "chatbt like it will work for some time",
      "offset": 6033.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "but then it will break down. There was a",
      "offset": 6036.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "paper I need to find the paper but the",
      "offset": 6038.08,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "paper showed that if you keep doing this",
      "offset": 6039.76,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "your actual reward actually goes",
      "offset": 6041.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "backwards. Um so like you will get more",
      "offset": 6042.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "more more reward and then suddenly it",
      "offset": 6044.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just I don't know by bad luck again it's",
      "offset": 6046.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "always about bad luck the reward just",
      "offset": 6048.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "goes back. Um so yes I mean serious like",
      "offset": 6050.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "all of AI is about bad luck and good",
      "offset": 6053.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "luck um and you know optimization trying",
      "offset": 6054.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to do efficiency um that's what everyone",
      "offset": 6057.6,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Yeah. Um so yes you can use LLM as a",
      "offset": 6060.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "judge. Um is that kind of",
      "offset": 6063.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "if I use as a judge doesn't it just end",
      "offset": 6066.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "up being a teacher and student model or",
      "offset": 6068.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "yes that's why like that's why sometimes",
      "offset": 6071.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "essentially the problem is I I need to",
      "offset": 6074.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "find the paper if you keep doing this it",
      "offset": 6076.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "will actually do bad I mean intuitively",
      "offset": 6078.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it kind of works but then at some point",
      "offset": 6080.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "there is actually another way you could",
      "offset": 6083.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "ask a language model to generate reward",
      "offset": 6084.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "functions that is actually another",
      "offset": 6086.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "school of thought you can actually ask a",
      "offset": 6088,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "language model to generate 10 seven",
      "offset": 6089.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "billion reward functions but the",
      "offset": 6091.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "question is is the reward functions good",
      "offset": 6092.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "or bad I don't know um so like now you",
      "offset": 6094.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "need to like rely on the fact that the",
      "offset": 6096.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "models are good or bad. You could then",
      "offset": 6097.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "ask another language model to verify the",
      "offset": 6098.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "reward functions. So yes, you could do",
      "offset": 6100.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this. Maybe that's what OpenAI is doing.",
      "offset": 6102.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I don't know. Maybe OpenAI's goal this",
      "offset": 6104.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "whole time is like, oh, let's generate",
      "offset": 6106.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "all these reward functions, verify each",
      "offset": 6107.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of them, and then shove it into the",
      "offset": 6109.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "function. Let's see what happens. Maybe",
      "offset": 6110.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "that's what they're doing. I I don't",
      "offset": 6112.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know. Um but yes, it is a student",
      "offset": 6113.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "teacher. Um yes.",
      "offset": 6116,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "What's your opinion on how to make",
      "offset": 6117.6,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "models",
      "offset": 6119.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "inverse?",
      "offset": 6124.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Is it something else?",
      "offset": 6126.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Do you mean like how to make reward",
      "offset": 6128.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "functions more efficient in general?",
      "offset": 6130.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "No, I mean scalable in the sense of",
      "offset": 6131.92,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "many different",
      "offset": 6134.96,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "Yes, the majority of reward functions",
      "offset": 6139.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "currently that's why it's called",
      "offset": 6141.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "verifiable rewards is like maths and",
      "offset": 6142.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "coding to be honest. I think coding is",
      "offset": 6143.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "also hard. I I don't know why people",
      "offset": 6146.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "lump. So coding you can't actually",
      "offset": 6148.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "verify technically it's correct. You can",
      "offset": 6150.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "just say it ran or the output is most",
      "offset": 6152.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "likely correct, right? For some",
      "offset": 6155.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "functions. But for example, the Flappy",
      "offset": 6157.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Bird game, tell it to create a Flappy",
      "offset": 6158.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Bird game. How do you actually verify if",
      "offset": 6159.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it even is the Flappy Bird game? I don't",
      "offset": 6161.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "know. But you could, right? That's the",
      "offset": 6163.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "whole point of the LM as a judge. You",
      "offset": 6166.08,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "could take the output of the Flappy Bird",
      "offset": 6167.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "game. Ask the language model, does this",
      "offset": 6169.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "look like the Flappy Bird game? And if",
      "offset": 6171.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it's yes, I get plus one. If no, minus",
      "offset": 6173.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "one. You could do that. Um, but you can",
      "offset": 6175.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "only go so far. If there's Does that",
      "offset": 6178.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "kind of",
      "offset": 6180.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "what your opinion is the scaling",
      "offset": 6182.639,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "oh",
      "offset": 6184.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "part to say I think most I I think large",
      "offset": 6187.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "model labs they're currently just trying",
      "offset": 6190.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to use their own model to to to like",
      "offset": 6191.679,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "literally reward reward it like as I",
      "offset": 6194.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like described you know ask it oh does",
      "offset": 6198.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "this look like the flappy bird game if",
      "offset": 6200,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "yes plus one if no minus one and I think",
      "offset": 6201.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "maybe that's what I think large model",
      "offset": 6204.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "labs their view is if you keep doing",
      "offset": 6205.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "this you'll get to AGI that's their view",
      "offset": 6207.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I mean if you think about You could",
      "offset": 6210.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "maybe, but then I always go fall back",
      "offset": 6212.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to, oh, but you might be bad luck. It's",
      "offset": 6214.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "not going to work. Um, so like I think",
      "offset": 6216.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "in general I think bad luck will just it",
      "offset": 6217.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "won't work. Um, you will only get so far",
      "offset": 6219.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "and then suddenly it just doesn't work.",
      "offset": 6221.36,
      "duration": 8.759
    },
    {
      "lang": "en",
      "text": "Does that Okay. Any other? Yes.",
      "offset": 6224.639,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "That is your choice again. So like if",
      "offset": 6245.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you want to specify for example you just",
      "offset": 6246.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "want to make a legal bot you given some",
      "offset": 6248.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "sort of court case and if it's like you",
      "offset": 6250.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "know the plaintiff wins or the defendant",
      "offset": 6252.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "wins I don't know you could just do law.",
      "offset": 6255.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Yes you could you could do that but in",
      "offset": 6257.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "my view you should combine it with other",
      "offset": 6259.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "sources. You should combine it with some",
      "offset": 6261.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "maths. should combine it with some",
      "offset": 6263.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "programming because the point is you",
      "offset": 6264.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "don't want the model just to know like",
      "offset": 6266.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you just you don't want the model just",
      "offset": 6268.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to like overfit just to just law maybe",
      "offset": 6269.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "maths might be helpful just by you know",
      "offset": 6272.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "by chance again maybe you know maybe",
      "offset": 6275.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "coding might be helpful probably not but",
      "offset": 6278.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "like you know in general um so like you",
      "offset": 6280.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "should combine other source um other",
      "offset": 6282.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "domains together um I feel like you know",
      "offset": 6284.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "all the large model apps their goal is",
      "offset": 6286.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to do every single domain possible right",
      "offset": 6288.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like mine every single reward function",
      "offset": 6290.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in the whole world take all the robot",
      "offset": 6292.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "functions, shove it into the model and",
      "offset": 6294.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "just learns. Um so like yes you should",
      "offset": 6295.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do more domains",
      "offset": 6297.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "if yeah",
      "offset": 6299.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that's another yeah",
      "offset": 6302.56,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "yeah",
      "offset": 6303.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "latest",
      "offset": 6308.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "model",
      "offset": 6319.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "is that this particular",
      "offset": 6324.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So the notebook I will share will",
      "offset": 6330,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "showcase you should probably do some",
      "offset": 6331.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "supervised finetuning fast. It's called",
      "offset": 6333.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the priming stage. Um otherwise you're",
      "offset": 6335.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "remember the plot over here the the the",
      "offset": 6338.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "one right you don't want to be in the",
      "offset": 6341.119,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "situation where like you're starting",
      "offset": 6342.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "from like some bad you know pre-trained",
      "offset": 6344.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "state and you're trying to go to the RL",
      "offset": 6346.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "stage very not efficient but remember AI",
      "offset": 6348.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is all about efficiency. You don't want",
      "offset": 6351.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "to do this step. So we do have to do",
      "offset": 6352.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "some priming you know the SFT stage and",
      "offset": 6354,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the other stages if that's your question",
      "offset": 6357.04,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "or",
      "offset": 6359.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "oh if you want to yes the bigger the",
      "offset": 6363.6,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "model the better. Yes",
      "offset": 6365.44,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "that's the trick. So essentially the",
      "offset": 6376.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "research papers show that small models",
      "offset": 6378.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "actually do work confusingly enough",
      "offset": 6381.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "because essentially these small models",
      "offset": 6384,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it just does longer thinking. It does",
      "offset": 6386,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "longer reasoning traces if the model's",
      "offset": 6387.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "smaller and if it's a larger model maybe",
      "offset": 6389.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the reasoning traces are like smaller in",
      "offset": 6391.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "general. So like I I feel like the small",
      "offset": 6393.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "models actually do work. They do break",
      "offset": 6395.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "down though. If you want to do like very",
      "offset": 6397.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "complicated reasoning traces then maybe",
      "offset": 6398.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the small models might not work because",
      "offset": 6400.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you know there is only seven billion",
      "offset": 6402.56,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "parameters. There's no not that much",
      "offset": 6403.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "space you can move. Um, and so the large",
      "offset": 6405.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "models you just have more space to move",
      "offset": 6407.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "around. And so that's why large models",
      "offset": 6408.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "are better. If I don't know if that",
      "offset": 6410.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "maybe kind of I don't know if that",
      "offset": 6413.199,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "answer your question, but",
      "offset": 6414.56,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "model.",
      "offset": 6436.08,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yes, correct. Exactly. Yes, that's what",
      "offset": 6439.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "you should do. Yes, you can take a",
      "offset": 6441.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "distilled model like I already reasoning",
      "offset": 6443.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "model and then further fine tune it.",
      "offset": 6445.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Yes, you could. Um, I would say it's a",
      "offset": 6446.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "bit more complicated because you could",
      "offset": 6448.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "do that, but remember the reasoning",
      "offset": 6450.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "model itself is already a reasoning",
      "offset": 6453.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "model and you're trying to fine-tune it",
      "offset": 6455.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "to become other re like you're trying to",
      "offset": 6456.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "do some other domain. It might be",
      "offset": 6457.84,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "easier, it might be harder. It's all",
      "offset": 6460.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "about luck again. I don't know. So, you",
      "offset": 6463.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "have to try. It's all trial and error.",
      "offset": 6465.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 6467.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "yeah. Yeah. Yes.",
      "offset": 6468.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Two questions for you. one like",
      "offset": 6470.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it's pretty empirical just like try and",
      "offset": 6473.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "see what works and what doesn't and see",
      "offset": 6476.08,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "what",
      "offset": 6477.44,
      "duration": 1.279
    },
    {
      "lang": "en",
      "text": "yes correct",
      "offset": 6478,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "okay and then the other side how you",
      "offset": 6478.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "keeping up with all the papers and all",
      "offset": 6480.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the content that's been good at I'm sure",
      "offset": 6482.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "it's a lot how you learn",
      "offset": 6484.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "to follow",
      "offset": 6488.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to be honest I don't you don't need to",
      "offset": 6490.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "follow that's my view don't try to",
      "offset": 6492.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "follow the latest research because",
      "offset": 6494.32,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "sometimes I may like the next day it's",
      "offset": 6495.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "like rebuttal of the previous paper and",
      "offset": 6496.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "then the next paper says oh it's a",
      "offset": 6498.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "rebuttal of the rebuttal I don't know.",
      "offset": 6499.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "So I would not try to keep too much up",
      "offset": 6501.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "to date with the latest research. I",
      "offset": 6503.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "think the field has kind of matured and",
      "offset": 6505.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "it is mostly stable now. You might have",
      "offset": 6507.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "like some algorithm increasing accuracy",
      "offset": 6510.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "by 1% or 2% or some efficient. Remember",
      "offset": 6512.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "all of the papers are about efficiency.",
      "offset": 6515.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "It's always about efficiency. Making the",
      "offset": 6516.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "training more stable, reducing",
      "offset": 6518.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "overfitting. It's always these similar",
      "offset": 6520.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "similar papers. Um so I would say don't",
      "offset": 6523.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "you can keep up to date with papers.",
      "offset": 6526.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Twitter is very good as a resource.",
      "offset": 6528.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Sometimes I tweet about papers, although",
      "offset": 6530.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I I don't suggest the Nathan Lambert",
      "offset": 6532.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "paper. The Nathan Lambert um book is",
      "offset": 6535.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "very good. He keeps updating it. Where",
      "offset": 6537.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is it? Where did I put it? Um this one,",
      "offset": 6539.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the RHF book. That is very good. So",
      "offset": 6542.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "definitely read that. Um he updates it",
      "offset": 6545.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "all the time. And so maybe follow Nathan",
      "offset": 6547.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Lambert. He's actually a very good",
      "offset": 6549.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Twitter on like the latest research. Um",
      "offset": 6551.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "so he's very useful. In general, there's",
      "offset": 6553.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "a lot of noise in the RS space as well.",
      "offset": 6556.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "You don't know if the research is good",
      "offset": 6558.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "or bad, like you know, rebuttals on top",
      "offset": 6560.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of rebuttals. So, I would suggest people",
      "offset": 6562.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "just to like try. It's just trial and",
      "offset": 6564.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "error, right? Try to see if your reward",
      "offset": 6567.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "function is good or bad, you know, is",
      "offset": 6569.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the loss not, you know, is the reward",
      "offset": 6571.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just 000? Like, unfortunately,",
      "offset": 6573.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "something's wrong or you just it's bad",
      "offset": 6575.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "luck. Try again. Um, so it's just",
      "offset": 6577.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "empirical. Yes. Um,",
      "offset": 6580,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "these slides. Sorry.",
      "offset": 6582.88,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Are you put",
      "offset": 6584.639,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "Oh, yeah. Yeah. Yeah. Yeah. Yeah. Um,",
      "offset": 6585.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "yes. These slides should be up. I was",
      "offset": 6586.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "supposed to make a bitly link. Um, I'll",
      "offset": 6588.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "probably do that later, but I will share",
      "offset": 6591.119,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "the slides. Yes.",
      "offset": 6592.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Slacking.",
      "offset": 6594.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Oh, yeah. Okay. I'll do that then. Okay.",
      "offset": 6595.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Any uh Yes.",
      "offset": 6598.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6599.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 6605.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "You are creating",
      "offset": 6613.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "in the old PO sense you are the value",
      "offset": 6617.04,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "model is a new model. The reward model",
      "offset": 6619.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "is a new model. Yes, but remember in",
      "offset": 6622.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "GRPO we delete the value model. The",
      "offset": 6624.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "value model is totally gone. We we",
      "offset": 6626.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "create the value from just statistics",
      "offset": 6628.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "from the distribution. We essentially",
      "offset": 6630.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "just create you know four what is 2 plus",
      "offset": 6632.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "two? Just create four examples, four",
      "offset": 6634.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "trials and then find the mean, find the",
      "offset": 6635.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "standard deviation and then that is your",
      "offset": 6638.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "that is your value model. It's not even",
      "offset": 6639.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "a model anymore. And then the reward",
      "offset": 6641.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "model is no more as well. It's just",
      "offset": 6643.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "reward functions. And that is why we",
      "offset": 6646.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "call it reinforcement learning with",
      "offset": 6648.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "verifiable rewards. It's not normal RL",
      "offset": 6650.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "anymore. It's like you replace a reward",
      "offset": 6652.719,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "model as well. If does that kind of make",
      "offset": 6655.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "sense?",
      "offset": 6657.119,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "Okay. My follow question.",
      "offset": 6658.159,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Yes. So the again there is two schools",
      "offset": 6677.199,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "of thought. The first one is like",
      "offset": 6680.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the the question what is 2 plus two?",
      "offset": 6683.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "somewhere in the model somewhere I don't",
      "offset": 6685.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "know where right this high dimensional",
      "offset": 6688,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "1.4 trillion parameter space somewhere",
      "offset": 6689.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it knows to calculate it as four right",
      "offset": 6692.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "there is some sort of circuit inside the",
      "offset": 6694.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "model and then the goal of RL is just",
      "offset": 6696,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just to maximize this circuit somehow",
      "offset": 6698.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "right via these formulas and stuff",
      "offset": 6700.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you're just trying to maximize it but",
      "offset": 6702.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "that's one school of thought the other",
      "offset": 6704.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "school of thought is oh you know RL is",
      "offset": 6705.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "actually learning something you know",
      "offset": 6707.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like it's actually learning how to do 2",
      "offset": 6708.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "plus 2 is equal to four and it's not",
      "offset": 6711.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "actually in the model",
      "offset": 6712.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Okay. Any Yeah. Yes.",
      "offset": 6715.199,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "When you say capabilities of of a model",
      "offset": 6717.199,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that already has them inside.",
      "offset": 6720.719,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 6722.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "You mean knowing actually the the answer",
      "offset": 6723.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "to a question or knowing how to reason",
      "offset": 6726.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to get a question?",
      "offset": 6729.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "That is a good question. Maybe both. I I",
      "offset": 6731.52,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "think it depends. It probably knows how",
      "offset": 6734.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "to do the re.",
      "offset": 6736.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "It might",
      "offset": 6738.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for example a contrived example. You get",
      "offset": 6741.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "all of the entire world's data, right?",
      "offset": 6743.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Like, you know, 30 trillion tokens. Get",
      "offset": 6744.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "all of the data and you just make a",
      "offset": 6746.639,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "question that is not part of the data,",
      "offset": 6748.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "right? You you could do that, right?",
      "offset": 6749.679,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "What is 10 billion, you know, some",
      "offset": 6751.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "random number times some random, you can",
      "offset": 6753.119,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "make a math equation which is not in the",
      "offset": 6754.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "data. But somehow the model has leared",
      "offset": 6756.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to do multiplication, has leared to do",
      "offset": 6759.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "addition somewhere. And so maybe this",
      "offset": 6762.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "circuit for addition for multiplication",
      "offset": 6765.679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you know for whatever some you know many",
      "offset": 6768.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "many many circuits of these like",
      "offset": 6770.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "functions we just want to accentuate",
      "offset": 6772.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "them all and that is what kind of RL is",
      "offset": 6774.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "trying to do okay and I mean yes there's",
      "offset": 6776.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a reasoning circuit so like somehow the",
      "offset": 6779.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "model also learns how to do reasoning",
      "offset": 6781.76,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "and so we also want to make that more",
      "offset": 6783.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "important and so you know 2 plus two is",
      "offset": 6784.719,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "very important addition is very",
      "offset": 6786.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "important multiplication is very",
      "offset": 6787.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "important and so on we're just trying to",
      "offset": 6788.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like make all of these circuits more",
      "offset": 6790.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "prevalent",
      "offset": 6792.32,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "But that's only one half of the AI",
      "offset": 6793.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "community, right? That's only one half",
      "offset": 6794.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "thinks like that. The other half is",
      "offset": 6796.159,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "like, \"Oh, but the model's actually",
      "offset": 6797.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "learning.\" You know, we're actually",
      "offset": 6798.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "training the model to learn and the base",
      "offset": 6800.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "model actually doesn't know how to do",
      "offset": 6802.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "reasoning. Does that kind of",
      "offset": 6803.36,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "Okay. Yes. Okay. Hopefully. Okay. Any",
      "offset": 6806.32,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "other questions? Yes.",
      "offset": 6810.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "Yes,",
      "offset": 6824.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there is val there is TRL um unsoft like",
      "offset": 6825.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "we we also make a it's not called a",
      "offset": 6828.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "framework that's more like we showcase",
      "offset": 6830.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that you can do uh gpo and reinforcement",
      "offset": 6832.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "learning with very low resources so we",
      "offset": 6835.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "are the only package which allows you to",
      "offset": 6837.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "do gpo on like a free collab and so like",
      "offset": 6839.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that's the only between us and everyone",
      "offset": 6841.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "else. So for ver",
      "offset": 6843.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "was very good for large training runs.",
      "offset": 6845.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "But for now onslaught like if you want",
      "offset": 6848.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "to do like small experimentation, you",
      "offset": 6850.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "want to try stuff out, you don't know",
      "offset": 6852.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "what reinforcement learning is, you",
      "offset": 6853.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "don't know how to make reward functions,",
      "offset": 6855.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "you don't even know what reward function",
      "offset": 6856.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to do, you should utilize our notebooks.",
      "offset": 6858.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "And that's what I was going to demo. Um",
      "offset": 6860.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "yeah. Yes.",
      "offset": 6863.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "So one question is",
      "offset": 6865.199,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "like let's say you don't think you just",
      "offset": 6867.76,
      "duration": 7.879
    },
    {
      "lang": "en",
      "text": "task right",
      "offset": 6872.32,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "of that using to just improve like let's",
      "offset": 6880.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "say use for your",
      "offset": 6883.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "yes yes you can do that exactly it",
      "offset": 6885.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "should increase accuracy by quite a bit",
      "offset": 6887.599,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "if your accuracy if your accuracy before",
      "offset": 6889.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with tool use was not very good IRL",
      "offset": 6891.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "should definitely help and I feel like",
      "offset": 6893.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the trick of RL is it reduces",
      "offset": 6894.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "overfitting I think that's the trick",
      "offset": 6896.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "because you do you do multiple",
      "offset": 6898.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "inferences you don't know which one's",
      "offset": 6900.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "correct, but you're trying to like",
      "offset": 6902.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "maximize some good ones, right? Some",
      "offset": 6903.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "good ones. The problem with general",
      "offset": 6905.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "fine-tuning is you're kind of like",
      "offset": 6907.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "overfitting the model. And so the trick",
      "offset": 6909.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of the trick of reinforcement",
      "offset": 6911.36,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "fine-tuning is you can essentially",
      "offset": 6912.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reduce overfitting. So the model",
      "offset": 6915.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "actually learns how to do tool hauling.",
      "offset": 6916.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Not just, oh, you know, I see someone's",
      "offset": 6919.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "trying to do a restaurant order, I just",
      "offset": 6921.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "want to call Door Dash to do order or",
      "offset": 6922.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "something, right? But it actually",
      "offset": 6925.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "learns, oh okay, because the person",
      "offset": 6926.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "wants to order food, I should order Door",
      "offset": 6928.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Dash. It's like reverse thinking. Um,",
      "offset": 6930.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so we should definitely help.",
      "offset": 6933.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Yeah, some of my experiments like unless",
      "offset": 6934.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "I tried not explicitly asking the model",
      "offset": 6937.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "instruction",
      "offset": 6939.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "like just do some task, right? And it",
      "offset": 6942.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "does not unatically",
      "offset": 6945.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "start the thinking process unless you're",
      "offset": 6948.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "explicitly prompted. Okay, first think",
      "offset": 6950.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "and then answer. I was trying to check",
      "offset": 6953.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "if like just without explicitly asking",
      "offset": 6956.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "it to start theing process just to",
      "offset": 6959.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "improve the tool accuracy itself does it",
      "offset": 6962.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "start and that I think that did not",
      "offset": 6964.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "happen because it",
      "offset": 6967.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you don't need to do so GPO generally",
      "offset": 6969.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "people utilize GPO and like",
      "offset": 6971.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "reinforcement learning algorithms to",
      "offset": 6972.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "create the thinking process",
      "offset": 6974.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that's because it's like an artifact of",
      "offset": 6976.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "GPO just by chance they see the",
      "offset": 6977.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reasoning process you don't need it so",
      "offset": 6979.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "maybe by chance by luck somehow how it",
      "offset": 6981.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "learns how to do tool calling and it's",
      "offset": 6985.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "not some thinking process it could be",
      "offset": 6987.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "some weird symbols maybe you know I",
      "offset": 6989.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "don't know it could be like using some",
      "offset": 6991.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "other you know like sometimes models",
      "offset": 6992.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "have like different languages suddenly",
      "offset": 6994,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "it could be like that you know randomly",
      "offset": 6995.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it learns how to do to calling it made",
      "offset": 6997.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "some new programming language internally",
      "offset": 7000.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I don't know but it could have done that",
      "offset": 7001.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "in this case there was no thinking",
      "offset": 7004.08,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "process",
      "offset": 7005.52,
      "duration": 1.04
    },
    {
      "lang": "en",
      "text": "yes",
      "offset": 7006.159,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "it just directly gave the output because",
      "offset": 7006.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like let's say you sample like 10",
      "offset": 7008.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "trajectories and none of them have a",
      "offset": 7009.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "thinking process then it's never it",
      "offset": 7011.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "never explores those",
      "offset": 7013.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "For now it will never but remember it's",
      "offset": 7014.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "all about luck over time you will have a",
      "offset": 7016.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "thinking process just by miraculous",
      "offset": 7019.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "chance there is a thinking process",
      "offset": 7021.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "somewhere and then oh you should do this",
      "offset": 7023.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "more and it would just do this more",
      "offset": 7025.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "but to make that more probable like you",
      "offset": 7026.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "should prompt it",
      "offset": 7028.48,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "you can prompt it so essentially in the",
      "offset": 7029.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "system prompt you can say please put",
      "offset": 7031.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "your working out between this box you",
      "offset": 7033.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "could force the model to create the",
      "offset": 7036.32,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "working out you could do that",
      "offset": 7038.4,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "you could Um, but is this the most",
      "offset": 7042.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "efficient? I don't know. You could you",
      "offset": 7045.199,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "could say, \"Oh, please create a new",
      "offset": 7047.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "language that I don't understand which",
      "offset": 7049.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "does tool calling and then it does some",
      "offset": 7052.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "weird symbols and then oh, it does tool",
      "offset": 7054.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "calling.\" I don't know. But yes, you you",
      "offset": 7056.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "should prompt it. It should make it more",
      "offset": 7058.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "effective.",
      "offset": 7059.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Okay. Any other Yes.",
      "offset": 7061.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "What's the secret sauce",
      "offset": 7063.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 7066.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Oh, we we utilize Triton kernels. We do",
      "offset": 7068.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like kernel optimizations. We reduce",
      "offset": 7070.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "memory usage by 70%. Um there are lots",
      "offset": 7072.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of optimizations that we do to make",
      "offset": 7075.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "training faster and more memory",
      "offset": 7076.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "efficient.",
      "offset": 7078.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Yes, later. Yes, it's not the main",
      "offset": 7079.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "focus, but yes, we will talk about that.",
      "offset": 7082.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "For VLM specifically, we use Okay,",
      "offset": 7084.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "that's actually at the notebook. Oh,",
      "offset": 7086.56,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "before that, do we have any other",
      "offset": 7087.76,
      "duration": 8.76
    },
    {
      "lang": "en",
      "text": "questions? Yes. focus on these days.",
      "offset": 7088.96,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "The actually the DC paper talks about",
      "offset": 7107.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this. There is pass at K and majority at",
      "offset": 7109.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "K. I think they said that if you do test",
      "offset": 7111.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "test time scaling, I think it improves",
      "offset": 7114.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "majority. I think that was correct. You",
      "offset": 7116.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "need to read the I'll have to rever",
      "offset": 7118.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "revert back to the DC paper. But they",
      "offset": 7119.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "did say remember test time scaling is",
      "offset": 7121.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "different from reinforcement learning.",
      "offset": 7123.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "There are different methodologies. Test",
      "offset": 7125.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "time scaling is calling the model 10,000",
      "offset": 7127.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "times and then you know then you just",
      "offset": 7129.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "check you know by average what okay you",
      "offset": 7132.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "for example you ask chbt what is 2 plus",
      "offset": 7135.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "two? It might say four you know it says",
      "offset": 7137.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "four four and suddenly it says five. I",
      "offset": 7140.32,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "don't know by chance it says five it",
      "offset": 7142.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "says zero and you just take the most",
      "offset": 7143.599,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "likely answer. That's called test time",
      "offset": 7145.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "test time scaling. And then",
      "offset": 7147.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "reinforcement learning is kind of",
      "offset": 7149.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "different. It's more like oh we want to",
      "offset": 7151.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like make we want to actually train the",
      "offset": 7152.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "model to actually do the whole trace.",
      "offset": 7154.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "And you do you don't need to do you",
      "offset": 7156.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "don't need to output 10,000 examples and",
      "offset": 7157.84,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "get the best answer. You just do one.",
      "offset": 7160.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Yes. Correct.",
      "offset": 7173.92,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "Yes. anything.",
      "offset": 7174.8,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "Yeah. So the trick is you do the RL step",
      "offset": 7181.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "and then you do test time compute it",
      "offset": 7184.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "will actually make the accuracy much",
      "offset": 7186.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "higher.",
      "offset": 7187.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "You that's a good question you could",
      "offset": 7189.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "kind of GPU is kind of like that right",
      "offset": 7192.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so GPU you do test time scaling in in",
      "offset": 7194.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the actual reward function. You",
      "offset": 7196.719,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "literally call the model what is 2 plus",
      "offset": 7198.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do you do test time scaling and then you",
      "offset": 7199.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "aggregate the results. So it's like GRPU",
      "offset": 7202.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "itself is doing test time scaling",
      "offset": 7204.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "internally. You could",
      "offset": 7205.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "that sounds like a new research paper.",
      "offset": 7209.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "You could do that I guess. Yes.",
      "offset": 7210.56,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Okay. I will have to go to the notebook",
      "offset": 7213.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "now. Um so in order to access the",
      "offset": 7216.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "notebook you can go to our GitHub page.",
      "offset": 7219.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 7221.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "which until my internet actually loads.",
      "offset": 7223.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Um so if you go to unsloaf right the",
      "offset": 7225.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "github page unsoft there is a button",
      "offset": 7226.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "called quen 3 gpo right and you can",
      "offset": 7229.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "click start for free that's how you get",
      "offset": 7232.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "the notebook or you can go to our docs",
      "offset": 7234.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "um which have the notebook um so",
      "offset": 7236.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "remember go to the github page and then",
      "offset": 7238.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "click start for free um yes for quen",
      "offset": 7240.56,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "3gio and then you will get this notebook",
      "offset": 7243.04,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "um generally it's dark mode but I know",
      "offset": 7246.719,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "in presentations you know presentations",
      "offset": 7249.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I don't think so people can see that so",
      "offset": 7252.239,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "I will change this to light mode. Um,",
      "offset": 7253.679,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "so we utilize VLM behind the hood. So",
      "offset": 7260,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "VLM does does anyone not know VLM? I",
      "offset": 7262.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "think that's a good question. Who does",
      "offset": 7265.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "not know VLM?",
      "offset": 7267.119,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Okay, 100% you must use VLM, right? So",
      "offset": 7269.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "like for all open source, how do you",
      "offset": 7273.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "serve a large language model? Please use",
      "offset": 7275.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "VLM or SG lang um or you know I think",
      "offset": 7277.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hugging face has one as well. So like",
      "offset": 7280.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "these are the best open source libraries",
      "offset": 7282.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "to serve open source models. Um you know",
      "offset": 7283.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like you have a GPU how do we actually",
      "offset": 7286.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "serve Llama 3 you know how do we serve",
      "offset": 7287.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Llama 4 you use VLM to serve it. The",
      "offset": 7290.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "trick of unsloaf is so unsoft is a",
      "offset": 7293.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "package for fine-tuning, for GRPO, for",
      "offset": 7295.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "reinforcement learning, for whatever you",
      "offset": 7297.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like. Continue pre-training, whatever.",
      "offset": 7298.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And the trick is we just optimize it. We",
      "offset": 7300.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "make it much faster. You know, use two,",
      "offset": 7302.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "use, um, 70% less memory. Um, make it",
      "offset": 7304.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "fit on a free collab. Um, remember,",
      "offset": 7307.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "please use free collab resources. You",
      "offset": 7309.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "know, Kaggle has 30 hours for I already",
      "offset": 7311.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "said this again, Kaggle has 30 hours of",
      "offset": 7313.199,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "for free per week of GPUs. Please",
      "offset": 7316.239,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "utilize them. Um, they won't be unhappy,",
      "offset": 7319.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you know. please utilize them. Um and",
      "offset": 7322.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "yeah, so you install unsoft in VLM. Um",
      "offset": 7324.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "and we have this thing called the fast",
      "offset": 7328.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "language model class which essentially",
      "offset": 7329.599,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you can call a model. Um for example, we",
      "offset": 7331.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "we will now utilize the quen 3 base",
      "offset": 7334.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "model, right? So like remember I told",
      "offset": 7336.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "you not to do this, right? But we are",
      "offset": 7338.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "anyways we're going to do it. Um this",
      "offset": 7340.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "plot where is the plot? Um,",
      "offset": 7342.48,
      "duration": 8.639
    },
    {
      "lang": "en",
      "text": "we are going to do the dark blue dot to",
      "offset": 7346.4,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "to the um the dark green. Um, yeah,",
      "offset": 7351.119,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that's what we're going to do. We're",
      "offset": 7354.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "actually not going to do what I",
      "offset": 7356.239,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "suggested not to do. Um, but anyways,",
      "offset": 7357.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "um,",
      "offset": 7359.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "we're going to do that. Um, you also",
      "offset": 7361.119,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have to set set a max sequence length.",
      "offset": 7362.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "So, for example, if you want to make it",
      "offset": 7364.639,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "longer, you can set it for longer,",
      "offset": 7366.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "right? If you want longer reasoning",
      "offset": 7367.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "traces, you can increase the maximum",
      "offset": 7368.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "sequence length. We set it as 248. um if",
      "offset": 7370.48,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "you set up a larger that free GPU will",
      "offset": 7374,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "run out. Um so that's the problem. You",
      "offset": 7376.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "can also load in four bits. So if you",
      "offset": 7379.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "want to do four bit quantization, you",
      "offset": 7381.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "can make the model go to four bits. You",
      "offset": 7383.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "can reduce memory usage by quite a bit.",
      "offset": 7385.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "So you can do that as well. Um and",
      "offset": 7387.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "remember we are utilizing Laura which is",
      "offset": 7389.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "a parameter efficient fine-tuning",
      "offset": 7391.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "method. Um you don't need to fine-tune",
      "offset": 7392.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "every single weight inside the entire",
      "offset": 7394.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "model. Um this will be very very very",
      "offset": 7396.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "costly. Instead we add small weights to",
      "offset": 7398.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the model to fine-tune it. Um and so",
      "offset": 7401.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that's a trick that we do",
      "offset": 7403.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and because we utilize VLM directly we",
      "offset": 7405.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "do a trick we actually reduce memory",
      "offset": 7408.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "usage by 50%. The trick is we share",
      "offset": 7410.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "VLM's weights directly. Um other",
      "offset": 7412.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "training frameworks what they do is they",
      "offset": 7415.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have to copy VLM's weights um because",
      "offset": 7416.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "you have to you have the model for",
      "offset": 7419.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "fine-tuning and you have the VLM",
      "offset": 7420.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "weights. The trick that we do is we",
      "offset": 7422.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "actually share the VLM weights directly.",
      "offset": 7424,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "So you can reduce memory usage by a",
      "offset": 7425.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "further 50%.",
      "offset": 7427.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "We use something called the unsoft",
      "offset": 7430.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "grading checkpointing which reduces",
      "offset": 7431.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "memory. Okay, essentially everything in",
      "offset": 7433.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "AI is about reducing memory usage more",
      "offset": 7435.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "efficiency. You know everything that we",
      "offset": 7437.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "do is just efficiency. Um so everything",
      "offset": 7438.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that we set is for efficiency purposes.",
      "offset": 7440.08,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "Um your Laura rank for if you do Laura",
      "offset": 7442.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "please set it to be the alpha to be two",
      "offset": 7445.599,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "times the Laura rank. Um it speeds up",
      "offset": 7447.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "training dramatically. So please do",
      "offset": 7449.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that.",
      "offset": 7450.639,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Some lots of stuff right compiling. We",
      "offset": 7452.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "do like automatic compiling and stuff",
      "offset": 7455.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "like that. You don't need to read all of",
      "offset": 7457.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this. Um and here is the bulk. This is",
      "offset": 7458.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the most important part. Someone was",
      "offset": 7460.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "asking about you know about a prompt.",
      "offset": 7463.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "You make a system prompt, right? You are",
      "offset": 7465.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "given a problem. Think about the problem",
      "offset": 7467.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and provide your working out. Place it",
      "offset": 7469.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "between",
      "offset": 7472.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "reasoning start and reasoning end.",
      "offset": 7473.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Right? So like the reasoning start is",
      "offset": 7475.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "start working out and end working out.",
      "offset": 7477.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So it should look something like this.",
      "offset": 7479.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Um if you you know does uh start working",
      "offset": 7481.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "out and end working out,",
      "offset": 7484.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "right? You were given a problem. Think",
      "offset": 7487.28,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "about the problem. Provide you're",
      "offset": 7488.719,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "working out. Place it between start",
      "offset": 7489.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "working out and end working out. Then",
      "offset": 7490.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "provide your solution between solution",
      "offset": 7492.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "start and solution end. So it should",
      "offset": 7494.639,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "look something like this. Um",
      "offset": 7496.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and this is the system prompt that we're",
      "offset": 7500.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "going to use for reinforcement learning.",
      "offset": 7502.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Remember you can customize this to",
      "offset": 7504.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "however you like. Right? You don't have",
      "offset": 7505.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "to say you are given a problem. You are",
      "offset": 7507.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "given a legal case. Right? think about",
      "offset": 7509.199,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "the case and provide provide your I",
      "offset": 7512.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "don't know legal thinking I don't know",
      "offset": 7516.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "I'm just making stuff up I don't know",
      "offset": 7517.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "whatever provide place it between",
      "offset": 7519.599,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I don't know it can be literally",
      "offset": 7523.599,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "anything",
      "offset": 7524.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "um thinking I don't know I don't you can",
      "offset": 7526.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "even make spelling mistakes it doesn't",
      "offset": 7529.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "really matter right end thinking",
      "offset": 7530.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the whole goal of RL is you can design",
      "offset": 7533.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "your reward you can design the system",
      "offset": 7535.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "prompt to whatever you like right I",
      "offset": 7536.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "think that's the main problem is like",
      "offset": 7538.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "people think oh you must Follow deep",
      "offset": 7539.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "seeks think right think right you need",
      "offset": 7542,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "to people see in deepseek think right",
      "offset": 7544.239,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "and close think right",
      "offset": 7546.719,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "you do not need to follow you do not",
      "offset": 7550.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "need to follow this right you do not",
      "offset": 7552.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "need to follow this at all you can make",
      "offset": 7554.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "it up entirely right this is",
      "offset": 7556.639,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "customizable to whatever you like",
      "offset": 7558.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the hard part is because we are using a",
      "offset": 7562.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "base model remember this is a base model",
      "offset": 7564.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you have to make a chat template as well",
      "offset": 7567.36,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "um this is The more annoying part, you",
      "offset": 7569.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "can just copy and paste this chat",
      "offset": 7573.119,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "template. You do not need to do anything",
      "offset": 7574.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "else. Um, just, you know, literally copy",
      "offset": 7575.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and paste it. Um, the base model does",
      "offset": 7577.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "not have a chat template, right? A base",
      "offset": 7580,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "model when you call it, you can't",
      "offset": 7581.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "actually call it for conversation. You",
      "offset": 7583.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "can't, it's not GPT, right? It's just a",
      "offset": 7585.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "base model. It doesn't do anything. So,",
      "offset": 7587.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you need to specify a template for it to",
      "offset": 7590,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "understand how to do conversations. And",
      "offset": 7592.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "so, this is kind of like a template that",
      "offset": 7594.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "we did. It's a very it's very generic.",
      "offset": 7596.159,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "You can just copy and paste this. It",
      "offset": 7598.159,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "should be the same for anything.",
      "offset": 7599.599,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "And then we show so after you do the",
      "offset": 7604.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "chat template, we show an example of how",
      "offset": 7605.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "to actually utilize the chat template",
      "offset": 7608.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "and the tokenizer. Right? So for",
      "offset": 7609.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "example, if you ask what is oneplus 1,",
      "offset": 7610.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you do the reasoning process, it will",
      "offset": 7613.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "say you are given a problem. Think about",
      "offset": 7614.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the problem and provide you're working",
      "offset": 7616.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "out blah blah blah blah blah. And in the",
      "offset": 7617.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "question, this is a question. Your",
      "offset": 7619.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "question is what is 2 plus two? Remember",
      "offset": 7621.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the answer is four. And so start working",
      "offset": 7623.119,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "out. This is what you give to the model.",
      "offset": 7626.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "You give all of this to the entire",
      "offset": 7629.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "model, right? All of this. Um, okay, I",
      "offset": 7630.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "can't really highlight. All of this you",
      "offset": 7633.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "give into the model. And the goal of RL",
      "offset": 7635.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is you want to create the working out",
      "offset": 7637.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "process automatically, right? The RL",
      "offset": 7639.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "algorithm will automatically create the,",
      "offset": 7641.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you know, the working the working out or",
      "offset": 7643.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the thinking process. And then finally,",
      "offset": 7644.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it will say four. Well, hopefully it",
      "offset": 7647.119,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "will say four. And the goal is if you",
      "offset": 7648.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "see the if you see four, you want to",
      "offset": 7650.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "make the reward higher just for that.",
      "offset": 7652.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Now someone was talking about um you",
      "offset": 7655.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "know fine-tuning with the um you know",
      "offset": 7657.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "instruct fine-tuning first again",
      "offset": 7659.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "remember we go back to this diagram we",
      "offset": 7661.679,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "wanted to start from the blue dot to go",
      "offset": 7664.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to the green dot but we found it doesn't",
      "offset": 7666.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "actually work so don't actually do this",
      "offset": 7669.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "trick is we go back to this diagram we",
      "offset": 7673.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "actually want to take the pre-trained",
      "offset": 7675.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "model do some fine-tuning do some",
      "offset": 7677.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "supervised finetuning and then go to the",
      "offset": 7679.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "green dot",
      "offset": 7681.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and so this part we show that you should",
      "offset": 7683.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "actually do some supervised finetuning.",
      "offset": 7686.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "You need to do some fine-tuning to prime",
      "offset": 7688,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the model. Right? The goal is you want",
      "offset": 7690,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "to learn you make the you want to make",
      "offset": 7691.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the model not just output zero board",
      "offset": 7693.199,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "forever. And so this data set allows you",
      "offset": 7694.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "to prime the model to do supervised fine",
      "offset": 7697.599,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tuning.",
      "offset": 7699.599,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So for example the problem is you know",
      "offset": 7702.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "what is the sum of all the real numbers",
      "offset": 7705.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "blah blah blah blah blah. And then you",
      "offset": 7706.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "use deepseeek R1. So this is a trick",
      "offset": 7708.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "this is a hack. You use DeepSseek R1 to",
      "offset": 7710.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "create some examples and then you shove",
      "offset": 7712.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "this during the finetuning step and",
      "offset": 7714.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "essentially the model already learns how",
      "offset": 7716.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "to do some reasoning and that's the",
      "offset": 7718.079,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "trick. This data set is very small. It's",
      "offset": 7719.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "only 7,000 rows, right? You don't need",
      "offset": 7722.159,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to have that much data for just this",
      "offset": 7724,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "first step. You can have like I think I",
      "offset": 7725.599,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "only use 600 rows. Very very very less",
      "offset": 7727.04,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "data.",
      "offset": 7729.199,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So this is just data preparation step.",
      "offset": 7732.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Not that important. Um I need to skip to",
      "offset": 7734,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the reward functions. This is the most",
      "offset": 7735.84,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "important part of the model. So this is",
      "offset": 7737.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the supervised finetuning step. So all",
      "offset": 7739.119,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of this is a supervis supervised fine",
      "offset": 7740.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tuning step. So this this part this part",
      "offset": 7742.719,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "of the model um that's this part.",
      "offset": 7744.96,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "So not that important. Um okay we skip",
      "offset": 7748.719,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "all of that. This is the most important",
      "offset": 7752.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "part. The reward function creation is",
      "offset": 7754.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the most important part. And I feel like",
      "offset": 7756.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you know the majority of people like",
      "offset": 7758.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "neglect this part. It is the most",
      "offset": 7759.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "hardest part to do. Um",
      "offset": 7761.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "okay let's see where is the reward",
      "offset": 7765.36,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "function. Oh here it is. Okay,",
      "offset": 7766.56,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "for example, okay, no, no, this one is a",
      "offset": 7769.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "regular expression to match if your",
      "offset": 7772.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "format is correct. For example, remember",
      "offset": 7774,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "we have we ask the question, we ask the",
      "offset": 7776.719,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "model to say please put your working out",
      "offset": 7779.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "between start working out and end",
      "offset": 7782.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "working out. This regular expression",
      "offset": 7784.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "essentially um essentially rewards the",
      "offset": 7787.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "model to see start working out and end",
      "offset": 7789.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "working out. If it doesn't have this,",
      "offset": 7792.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you will actually penalize it. And so",
      "offset": 7793.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this is one reward function that I",
      "offset": 7795.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "created.",
      "offset": 7796.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "For example, I give it an example. If",
      "offset": 7799.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "you say let me think end working out,",
      "offset": 7801.92,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "it extracts two. Yes, that's good.",
      "offset": 7805.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Right? Remember we force the model to",
      "offset": 7808.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "say we force a model you must generate",
      "offset": 7809.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the answer between this and this. And it",
      "offset": 7811.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "successfully extracted two. So that's",
      "offset": 7814.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "good. But you know also sometimes a",
      "offset": 7816.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "model might generate some random spaces.",
      "offset": 7819.119,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "It's possible. you know the model might",
      "offset": 7820.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "not actually the model might not follow",
      "offset": 7822.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "your exact format. We still try to match",
      "offset": 7823.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it, right? Even if it generates extra",
      "offset": 7826.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "spaces, we still successfully match the",
      "offset": 7827.679,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "number two. So that's good.",
      "offset": 7829.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "This is a reward function, right? So",
      "offset": 7833.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "essentially what we see is if it matches",
      "offset": 7835.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the format exactly, we add the score by",
      "offset": 7836.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "three.",
      "offset": 7839.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "If not, we just put zero. And remember",
      "offset": 7841.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "this match format essentially matches",
      "offset": 7844.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "regular expression. We had to create it",
      "offset": 7846.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "by hand for matching the format.",
      "offset": 7847.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "This number does not have to be plus",
      "offset": 7850.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "three. It can be plus 300. Whatever you",
      "offset": 7852.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like. It can be plus one. I don't know.",
      "offset": 7854.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "It can be anything that you like. But I",
      "offset": 7856.639,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "just found plus three to work fine. Um",
      "offset": 7858.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so you can do whatever you like. Um",
      "offset": 7860.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "yeah, anything. And remember the score",
      "offset": 7862.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is zero. If you don't see it, you can",
      "offset": 7864.639,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "also do minus one. For example, if it's",
      "offset": 7866.159,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "else, right? If it's not good, you can",
      "offset": 7868.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "also do score minus, right? Minus three.",
      "offset": 7870.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "You can minus three points from it. So",
      "offset": 7872.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "up to you. You can design your reward",
      "offset": 7874.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "function as whatever you like.",
      "offset": 7875.44,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "But remember, if the model if the model",
      "offset": 7878.8,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "output is not exactly following your",
      "offset": 7881.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "format, we should still at least reward",
      "offset": 7883.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "it a little bit, right? Otherwise, the",
      "offset": 7886.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "reward will just be 0000.",
      "offset": 7888,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So the trick is if we see a keyword, we",
      "offset": 7890.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "plus one. And if we don't, oh, sorry,",
      "offset": 7893.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "plus 0.5. If you see the keyword and if",
      "offset": 7896,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you don't see the keyword, right, if you",
      "offset": 7898.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "see the keyword this, if you see this in",
      "offset": 7900.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "the output, you should at least plus",
      "offset": 7902.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "0.5.",
      "offset": 7904,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "But if you don't see it, then you're",
      "offset": 7905.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "minus one. And so this essentially",
      "offset": 7907.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "allows you to partially reward the",
      "offset": 7908.96,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "model.",
      "offset": 7911.679,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "More reward functions now gets more",
      "offset": 7915.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "complicated. This this large reward",
      "offset": 7916.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "function essentially allows you to",
      "offset": 7919.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "calculate the distancebased scoring. For",
      "offset": 7921.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "example, remember we said um over here",
      "offset": 7923.28,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "um where is it? Um this one, right? What",
      "offset": 7925.84,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "is 2 plus two? Four is is correct, but",
      "offset": 7929.36,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "three is also a better answer than D.",
      "offset": 7933.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Right? If you if you output D, it's",
      "offset": 7935.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "definitely wrong, right? If you output",
      "offset": 7937.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "five, it's okay, but it's wrong. And so",
      "offset": 7939.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "this function essentially allows you to",
      "offset": 7942.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "take the good answer. So, sorry, this is",
      "offset": 7944.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the guess divided by your true answer.",
      "offset": 7947.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And it's like a ratio. And this ratio",
      "offset": 7949.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "essentially allows you to reward if your",
      "offset": 7952.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "number is close to the actual answer,",
      "offset": 7954.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "you give it higher reward. And if your",
      "offset": 7956.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "if your answer is very very very far",
      "offset": 7959.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "off, then you penalize it by minusing",
      "offset": 7961.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "reward. And so this essentially allows",
      "offset": 7964.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you to do that.",
      "offset": 7966.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "If it's exactly correct, you also add",
      "offset": 7968.8,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "five points.",
      "offset": 7971.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "And so this this is probably the this is",
      "offset": 7973.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "probably the most important reward",
      "offset": 7976.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "function. Um but this is only for maths.",
      "offset": 7977.199,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Um for other like code and stuff like",
      "offset": 7979.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "that, you have to create more reward",
      "offset": 7981.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "functions.",
      "offset": 7982.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Now we test if our reward functions",
      "offset": 7984.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "actually work. Um and yes, it extracts",
      "offset": 7986.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the numbers. Um this is just format",
      "offset": 7989.599,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "reward. Sorry, this is just extracting",
      "offset": 7991.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the solution. And you can see that it",
      "offset": 7993.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "extracted 0.34. It extracted this",
      "offset": 7994.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "number. It extracted this and extracted",
      "offset": 7997.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "this. Um, if your reward function is not",
      "offset": 7999.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "working very well, you probably did",
      "offset": 8001.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "something wrong in the regular",
      "offset": 8003.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "expression. So, please like edit that.",
      "offset": 8004.32,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "And then this is helper functions. Oh,",
      "offset": 8008.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this is another another reward function.",
      "offset": 8011.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "If you see if you see the number 1 2 3,",
      "offset": 8013.52,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "45, we want to remove the comma because",
      "offset": 8016.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "you can't actually convert this into",
      "offset": 8019.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "Python. So, you want to remove the",
      "offset": 8020.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "comma. Um, and then if it's equal to the",
      "offset": 8022.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "true answer, you're plus 3.5 reward. And",
      "offset": 8024.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "if it's not, then you're minus 1.5",
      "offset": 8027.84,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "reward.",
      "offset": 8029.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "More data set preparation functions not",
      "offset": 8033.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that important. Um, and here is the meat",
      "offset": 8035.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of the code for training, right? We call",
      "offset": 8038.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "VLM",
      "offset": 8040.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "top P is 1.0. Um, you don't it's not",
      "offset": 8042,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "that you can probably it's probably not",
      "offset": 8044.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "a good 1.0 just means you're sampling",
      "offset": 8046.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the entire space. Um, so that's good.",
      "offset": 8048.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "You can set this to like 0.8 eight or",
      "offset": 8050.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "something else up to you but I generally",
      "offset": 8052.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "set it to be 1.0 to be like full",
      "offset": 8054.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "sampling of the entire space min is 0.1",
      "offset": 8056.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "I suggest people to use this because",
      "offset": 8059.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "otherwise the model might like go into",
      "offset": 8061.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "like it might do inference of like",
      "offset": 8062.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "random outputs so you know use 0.1",
      "offset": 8064.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "and temperature I did suggest people to",
      "offset": 8069.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "increase temperature to 0.1.2 two,",
      "offset": 8071.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "right? Or you can do one. Um try to",
      "offset": 8073.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "increase your temperature as much as",
      "offset": 8076.48,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "possible. The more temperature you",
      "offset": 8077.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "increase, the model becomes very very um",
      "offset": 8079.119,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "creative, right? It like creates random",
      "offset": 8082,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "outputs. If you increase the temperature",
      "offset": 8084.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "too much, like you know two, your model",
      "offset": 8086.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "will be like gibberish. So probably",
      "offset": 8088.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "don't do too large numbers. So I",
      "offset": 8090.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "normally suggest 1.0 1.1 or 1.2 or",
      "offset": 8092.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "somewhere around there. You should",
      "offset": 8095.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "utilize minp together, right? you should",
      "offset": 8097.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "utilize minp together with high",
      "offset": 8100.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "temperature numbers. Um there is a paper",
      "offset": 8102.96,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "about using temperature 1.5 and 0.1 min.",
      "offset": 8104.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "You should utilize that. There are some",
      "offset": 8108.719,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "some other things that we utilize. Um",
      "offset": 8111.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "num generations is very important. I set",
      "offset": 8115.599,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "this to before this number is is this",
      "offset": 8117.92,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "thing. Um where is it?",
      "offset": 8122.239,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "This number is this. How many how many",
      "offset": 8126.239,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "like uh how many rollouts do you want to",
      "offset": 8130.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "do? How many inference steps do you want",
      "offset": 8132.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to do for GPU? Right? We chose four. And",
      "offset": 8133.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so four will just means what is 2 plus",
      "offset": 8136.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "two? It will create four four options.",
      "offset": 8138.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "And so that is this number. If you",
      "offset": 8140.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "increase this number too much, you will",
      "offset": 8142.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "use much more memory. Um you should",
      "offset": 8143.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "increase this as much as possible if you",
      "offset": 8146.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "can. Um and there's like gradient uh",
      "offset": 8148.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "there's like batch size. We set this to",
      "offset": 8151.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "be one. Um the trick is the batch size",
      "offset": 8153.36,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "times the gradient accumulation is",
      "offset": 8156.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "equivalent most of the time. Um GPO is",
      "offset": 8158.719,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "not but essentially what this does is if",
      "offset": 8161.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you do one it just means we're doing",
      "offset": 8164.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "one. What is one? What is 2 plus two? If",
      "offset": 8166.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you set batch size to be three then we",
      "offset": 8169.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "shove all of these three examples",
      "offset": 8172.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "together into one.",
      "offset": 8174,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Generally you should set batch size to",
      "offset": 8176.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be much larger. Um the problem is if you",
      "offset": 8177.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "set batch size to be too big you're",
      "offset": 8180.159,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "going to use more memory. So the trick",
      "offset": 8181.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "is instead you do gradient accumulation.",
      "offset": 8183.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "You set this to be 16. That's a trick.",
      "offset": 8184.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um gradient accumulation. It essentially",
      "offset": 8186.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "allows you to do addition of gradients",
      "offset": 8188.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "over time and you can skip using too",
      "offset": 8190.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "much memory.",
      "offset": 8193.12,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "And then there's like evaluation. If you",
      "offset": 8196,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "want to do evaluation, there's some",
      "offset": 8197.359,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "functions for that. Um and then we shall",
      "offset": 8198.479,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "see the training. Um",
      "offset": 8201.439,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "you will get a large table of numbers",
      "offset": 8204.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "during the training process. This took 2",
      "offset": 8206.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "hours and 54 minutes on a free collab.",
      "offset": 8208.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Um look at the reward column right the",
      "offset": 8210.88,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "reward column minus 7.5 - 5.5 - 5.5 all",
      "offset": 8213.599,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "very bad oh and then suddenly plus 13",
      "offset": 8218.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "just by chance suddenly it's plus 13",
      "offset": 8221.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "remember GPO the trick is if you see",
      "offset": 8223.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this + 13 let's maximize this even more",
      "offset": 8225.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and then it's oh but then it didn't",
      "offset": 8229.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "really work so it goes back to - 7.5",
      "offset": 8230.639,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "minus 5.5 minus 7.5 and so on and then",
      "offset": 8233.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "plus 11 you see another good reward we",
      "offset": 8236.719,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "want to maximize ize this as well and so",
      "offset": 8239.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "so on so on so on right that's that's",
      "offset": 8242.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the trick of GPO by luck by chance",
      "offset": 8243.84,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "literally by luck you will have good",
      "offset": 8247.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "answers right good answers you want to",
      "offset": 8250.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "maximize this and if you keep looking",
      "offset": 8251.92,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "down right if you keep scrolling down in",
      "offset": 8253.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "the end okay I need to make a plot but",
      "offset": 8256.479,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in general your reward will increase",
      "offset": 8258.8,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "over time right look these are all",
      "offset": 8260.319,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "positive numbers now right these are all",
      "offset": 8261.599,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "positive numbers your minus numbers are",
      "offset": 8263.519,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "getting less and less and less and less",
      "offset": 8265.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "um I think if I can I don't know if I",
      "offset": 8266.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "can plot this But okay, I'll plot this",
      "offset": 8268.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "later. But essentially, if you plot this",
      "offset": 8271.76,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "over time, the reward will actually",
      "offset": 8273.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "increase over time. There is also other",
      "offset": 8274.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "numbers like completion length. So",
      "offset": 8277.04,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "essentially, remember when you use a",
      "offset": 8278.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "reasoning model, the reasoning trace can",
      "offset": 8280.559,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "be extremely long. So this column just",
      "offset": 8282.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "tracks how long the reasoning process",
      "offset": 8284.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is. Um it's over time in general, the",
      "offset": 8286.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "reasoning length should get longer and",
      "offset": 8289.519,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "longer. Um but sometimes not always the",
      "offset": 8290.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "case. Um yeah, not always the case.",
      "offset": 8292.719,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "There is also another column called KR",
      "offset": 8295.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "divergence. This essentially tells you",
      "offset": 8297.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "how far the model is the you know the",
      "offset": 8299.679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "final model from the original model and",
      "offset": 8302.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the larger the number it means it's",
      "offset": 8304.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "getting very very very far away from the",
      "offset": 8306.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "original model. Um in general this",
      "offset": 8307.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "number should get bigger over time in",
      "offset": 8310.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "general. Um sometimes it doesn't move",
      "offset": 8311.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "but you should make this number go as",
      "offset": 8313.519,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "much much higher as possible. Um and",
      "offset": 8315.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "then we also we made separate reward",
      "offset": 8317.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "functions. Each of those reward",
      "offset": 8320.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "functions also has their own reward. um",
      "offset": 8322,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the most important one is the last",
      "offset": 8325.2,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "column, right? Or the second last",
      "offset": 8327.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "column. These are the two numbers. So in",
      "offset": 8329.439,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "in RL there is a problem. Most RL most",
      "offset": 8331.599,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "RL training runs just follow the format",
      "offset": 8335.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and it doesn't actually learn. So the",
      "offset": 8338.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "format columns are not important. Do not",
      "offset": 8340.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "look at the format columns. These are",
      "offset": 8343.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "useless. You need to look at the last",
      "offset": 8344.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "two columns. And if you look at the last",
      "offset": 8346.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "two columns, right, rewards, check check",
      "offset": 8348.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "numbers. This essentially checks if the",
      "offset": 8350.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "output is good or bad. You see that it's",
      "offset": 8351.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "minus 2.5 minus 2.5 not very good and",
      "offset": 8354.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "then suddenly 3.5 right 3.5 is good we",
      "offset": 8356.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "want to maximize this if you keep",
      "offset": 8359.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "looking",
      "offset": 8361.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "over time in general if you take the if",
      "offset": 8363.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you take like a rolling average in",
      "offset": 8367.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "general the model gets better and better",
      "offset": 8368.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "and better obviously we only train this",
      "offset": 8370.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "for two hours and 50 minutes you know if",
      "offset": 8372.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you train it for 20 days it might",
      "offset": 8374.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually do very well um but remember",
      "offset": 8376.559,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this is a free collab GPU um so in",
      "offset": 8378.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "general remember so like the goal is the",
      "offset": 8380.639,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "goal of GRPO is suddenly we see a good",
      "offset": 8383.76,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "answer with a good reward we want to",
      "offset": 8387.519,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "maximize that and that's the whole point",
      "offset": 8390.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of GPU it it's it's nothing fancy it's",
      "offset": 8392.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "just like by luck we see it and we just",
      "offset": 8395.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "want to maximize it",
      "offset": 8396.96,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "we can also see some output from the",
      "offset": 8399.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model right at the very beginning right",
      "offset": 8401.439,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "at the very beginning of the model let's",
      "offset": 8403.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "see um where is it an example",
      "offset": 8404.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "compute the number of positive integers",
      "offset": 8408.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that divide at least to a blah blah blah",
      "offset": 8410.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "some question and then it does some",
      "offset": 8412.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reasoning trace. Remember, we already",
      "offset": 8414.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "fine-tuned it a little bit. So, it does",
      "offset": 8416.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "something, right? It does something, but",
      "offset": 8418,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the answer, it just goes on and blah",
      "offset": 8420.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "blah blah. It just blah blah blah. It",
      "offset": 8421.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "just keeps going on blabbering on. Um,",
      "offset": 8423.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "but then if you look at the actual",
      "offset": 8425.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "answer, um, where is it? If you keep",
      "offset": 8426.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Okay, there's a lot. Um, we print out",
      "offset": 8430,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "every single We print out a lot. Okay,",
      "offset": 8431.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it just keeps Okay, whatever. It keeps",
      "offset": 8433.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "going on and on and on. Um, this is the",
      "offset": 8435.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "output of the GPO algorithm. And you",
      "offset": 8438.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "will see over time if you inspect this",
      "offset": 8440.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you will see that the model actually",
      "offset": 8442.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "gets better and better and better. Um",
      "offset": 8443.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for just an example right let's say we",
      "offset": 8445.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "ask the model what is the square root of",
      "offset": 8447.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "101 right it's not we don't just say",
      "offset": 8449.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "what is the square root of 100 that's",
      "offset": 8452,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "just 10 we say what is the square root",
      "offset": 8453.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of 101",
      "offset": 8454.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "if you do not train the model this is",
      "offset": 8458.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "what you get it will say answers",
      "offset": 8459.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "education math and arithmetic what is",
      "offset": 8462.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the square root of 101 wiki user oh wiki",
      "offset": 8464.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "user this is what it actually will say",
      "offset": 8467.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "right that's actually what it will say",
      "offset": 8468.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "does do where do you think this data",
      "offset": 8470.399,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "comes from does anyone know can take a",
      "offset": 8471.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "guess Where do you think this data comes",
      "offset": 8474.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "from? Probably Wikipedia, right? So if",
      "offset": 8475.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you ask the question, what is the square",
      "offset": 8478.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "root of 101? It doesn't do anything.",
      "offset": 8479.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Right? Remember this is the base model.",
      "offset": 8482,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "The base model is useless, right? You're",
      "offset": 8483.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "not going to get it's not going to",
      "offset": 8485.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "answer the question.",
      "offset": 8486.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "But after we do GPO,",
      "offset": 8490,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "right? What is the square root of 101?",
      "offset": 8493.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "We ask the question again. It says,",
      "offset": 8495.28,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "okay, so I need to find the square root",
      "offset": 8496.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "of 101. Hm, let me think. I remember",
      "offset": 8498.479,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "that the square root of numbers between",
      "offset": 8500.479,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the perfect numbers are rational. blah",
      "offset": 8501.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "blah blah blah blah blah blah right it",
      "offset": 8503.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "just says blah blah whatever some",
      "offset": 8504.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "whatever and it says solution 10 point",
      "offset": 8506.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "right here",
      "offset": 8509.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "solution 10.049875 049875 I think that's",
      "offset": 8512,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "correct. I don't know if that's correct",
      "offset": 8515.28,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "but probably it's very close. And so the",
      "offset": 8516.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "goal of so the whole point is GPO allows",
      "offset": 8518.399,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "you the GPO algorithm produced all of",
      "offset": 8520.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "this right this reasoning trace in the",
      "offset": 8524.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "olden days you actually have to have a",
      "offset": 8526.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "human write all of this and then you",
      "offset": 8529.439,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "have to fine-tune the model GPO you skip",
      "offset": 8532.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you don't need to make this anymore it's",
      "offset": 8535.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "automatic right that's the trick of GPO",
      "offset": 8537.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and reinforcement learning all of this",
      "offset": 8539.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "reasoning phase is automatic totally",
      "offset": 8541.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "produced from nothing and in the end it",
      "offset": 8544.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "gets a solution",
      "offset": 8546.399,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "Yes,",
      "offset": 8548.399,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "you had that.",
      "offset": 8548.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Yes, that's trick. If you do the base",
      "offset": 8551.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model, the trick of doing this is if you",
      "offset": 8553.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "just do the base model, go into the",
      "offset": 8555.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "space, you'll still get this, but it'll",
      "offset": 8557.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "be too long.",
      "offset": 8558.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Otherwise, you'll wait there for like 20",
      "offset": 8560.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "days and for demonstration purposes in a",
      "offset": 8562.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "collab, you'll have to do the supervised",
      "offset": 8564.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "fine tuning step. That's trick.",
      "offset": 8566.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Yeah. Yes.",
      "offset": 8568.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "What is the advantage of doing this",
      "offset": 8569.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "7,000 examples versus using model out of",
      "offset": 8571.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the box? You can use an instru we",
      "offset": 8574.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "actually have notebooks for that. So if",
      "offset": 8577.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "you go to GPU in general um we have",
      "offset": 8579.92,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "notebooks for using instruct model. Okay",
      "offset": 8583.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the internet's very slow. Um GP we",
      "offset": 8585.439,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "actually have other notebooks. For",
      "offset": 8588.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "example if you use llama 3.2 3 billion",
      "offset": 8589.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that is using instruct model. You don't",
      "offset": 8591.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "need to use a base model but we showed",
      "offset": 8593.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that you can use a base model. Um yeah",
      "offset": 8595.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you can use I suggest people to use",
      "offset": 8598.24,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "instruct. Um you probably shouldn't use",
      "offset": 8599.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "base. It's all about efficiency as well.",
      "offset": 8601.439,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 8603.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "yes.",
      "offset": 8604.399,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "Sorry. What? What?",
      "offset": 8610.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Yes. Yes.",
      "offset": 8613.76,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "Yes. Correct.",
      "offset": 8615.92,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "Yes. So the goal of Ko divergence is you",
      "offset": 8624.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "want the model not to stray too much",
      "offset": 8626.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "away from the original model. Right. KO",
      "offset": 8628.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "divergence is I shouldn't say distance",
      "offset": 8631.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but K cho divergence is like a distance",
      "offset": 8633.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "between the the current model the",
      "offset": 8635.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "current model that you're training and",
      "offset": 8637.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the previous very very very beginning of",
      "offset": 8639.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the model right and so essentially if",
      "offset": 8641.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the model is too far away your ko",
      "offset": 8643.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "divergence will be very large right if",
      "offset": 8645.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "you look at the plot uh where is the",
      "offset": 8646.8,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "table uh I'll scroll up a bit um",
      "offset": 8648.319,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "right which one is the k diver oh here",
      "offset": 8653.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this is the column right this column is",
      "offset": 8655.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "a ko divergence um column over time it",
      "offset": 8657.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "should get larger and larger and larger",
      "offset": 8660.479,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "over time, right? The number should get",
      "offset": 8661.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "larger and larger and larger because the",
      "offset": 8663.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "model is straying away from the fine uh",
      "offset": 8664.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the original model. If you set the beta",
      "offset": 8667.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to be zero, then you remove this term.",
      "offset": 8670,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Maybe this might make the capabilities",
      "offset": 8673.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of the model more maybe because you're",
      "offset": 8675.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "essentially not forcing the model to be",
      "offset": 8677.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "as close as possible to the base model.",
      "offset": 8678.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Active error of research. So yeah, some",
      "offset": 8681.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "people might set it to zero, some people",
      "offset": 8684.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "might not set it to zero. You know, I",
      "offset": 8686.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think 0.0 I think the default is 0.05 5",
      "offset": 8687.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "or 0.03",
      "offset": 8689.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "if that.",
      "offset": 8692.24,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "Okay. Any other questions for Yes.",
      "offset": 8693.28,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "So for fine-tuning actually fine-tuning",
      "offset": 8712,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is actually very helpful already. Um",
      "offset": 8714,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "where is the loss? Where is the loss?",
      "offset": 8715.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "The base model. So the base model",
      "offset": 8717.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "already is very bad. Um if you do",
      "offset": 8718.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh here there's a loss. There's a loss.",
      "offset": 8722.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "This is using the finetuning step the",
      "offset": 8724.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "pre the priming stage right you use like",
      "offset": 8726.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a data set to firstly prime the data to",
      "offset": 8728.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "prime the model. The loss does decrease.",
      "offset": 8730.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Remember if you see a loss of 0.64",
      "offset": 8733.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that's good. Um if you see a loss higher",
      "offset": 8734.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "than 30 definitely something's wrong. Um",
      "offset": 8736.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "higher than three is very bad. Um you",
      "offset": 8738.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "can see the loss definitely decreases",
      "offset": 8740.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "over time. So yes doing the fine-tuning",
      "offset": 8742.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "stage does teach the model a little bit",
      "offset": 8745.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "to do reasoning and it learns how to do",
      "offset": 8748.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "some stuff. Also a good a very",
      "offset": 8752.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "interesting um fact is we used deepse R1",
      "offset": 8754.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "some of the reasoning process to do the",
      "offset": 8757.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "finetuning step. And interestingly, if",
      "offset": 8760.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you just call the model without doing",
      "offset": 8763.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "GRPO, it kind of does reasoning already",
      "offset": 8764.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "by doing 7,000 examples, right? It",
      "offset": 8768.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "already says, remember the question was,",
      "offset": 8770.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "um, what is 2 plus? Oh, wait, this is",
      "offset": 8772.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "just a general question, right? It kind",
      "offset": 8774.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "of learns how to do reasoning somewhat,",
      "offset": 8776.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "but it's not perfect. And so the goal of",
      "offset": 8779.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "GPO is to forcibly make it perfect.",
      "offset": 8781.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Okay, not perfect, but as much as",
      "offset": 8784.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "possible. Um, so actually the finetuning",
      "offset": 8785.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "step already kind of learns a little",
      "offset": 8788.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "bit.",
      "offset": 8789.68,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "No, we No, actually, you don't need to",
      "offset": 8797.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "use 7,000. I think I only used",
      "offset": 8799.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I think I used 118.",
      "offset": 8803.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "It's so it's uh uh two training epochs.",
      "offset": 8805.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I think it's Yeah, it's 118. I only use",
      "offset": 8809.2,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "118 rows. You don't need to You can use",
      "offset": 8812,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "10 rows. You can use Yeah, use as less.",
      "offset": 8814.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "You must use more than three rows",
      "offset": 8817.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "though. Um because when you do Laura the",
      "offset": 8819.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "gradients become are zero. So you must",
      "offset": 8821.2,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "use more than three but but anything",
      "offset": 8822.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "more than three is fine.",
      "offset": 8824.479,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Yeah. So even if you use 118 it does",
      "offset": 8826.64,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "fine. Um yeah.",
      "offset": 8828.64,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 8832.399,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Do you mean like a small model versus a",
      "offset": 8845.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "big model? What's the difference? Um,",
      "offset": 8847.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "can you tell any tricks if you want to",
      "offset": 8849.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "do this training on a bigger?",
      "offset": 8852,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "Oh, yeah. Go ahead. You can you can take",
      "offset": 8853.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the notebook. You will need a better GPU",
      "offset": 8855.439,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "though. Take the notebook. Edit",
      "offset": 8857.2,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "edit this here. Not four. You can do 14",
      "offset": 8861.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "billion. Wait, I think there's a 14",
      "offset": 8864.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "billion, I think. Or is it 12 billion? I",
      "offset": 8865.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "can't remember. You can do whatever you",
      "offset": 8867.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like. You could even do, you know, llama",
      "offset": 8868.16,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "3.370 billion. I don't know. Up to you.",
      "offset": 8871.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Um, do whatever you like. And but the",
      "offset": 8874.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "goal is for a collab demonstration",
      "offset": 8876.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "because it's a small GPU. I use like a",
      "offset": 8878.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "small model. Um we actually have",
      "offset": 8880.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "notebooks for free collab which fits 14",
      "offset": 8882.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "billion. Um so fe 14 billion actually",
      "offset": 8885.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "fits in a free collab. Um so you can do",
      "offset": 8888,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "big models in a free collab. Um Kaggle",
      "offset": 8890.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "again I said Kaggle use Kaggle free",
      "offset": 8892.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "GPUs. Um there are like no so",
      "offset": 8894.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "essentially this whole page has like no",
      "offset": 8897.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "books. Uh where's Kaggle? Kaggle. Kaggle",
      "offset": 8898.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "has like notebooks for GPO as well. Um",
      "offset": 8900.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "so you can do whatever you like for",
      "offset": 8903.28,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "large models. Um,",
      "offset": 8904.479,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "oh, do you mean like for VLM rollouts",
      "offset": 8911.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like Oh, so the trick what we do is you",
      "offset": 8912.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we colllocate so you use the same",
      "offset": 8915.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "machine for inference and fine-tuning",
      "offset": 8917.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and the trick is you can reduce memory",
      "offset": 8919.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "usage because you're sharing the VLM",
      "offset": 8920.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "weights. So some other trainers like",
      "offset": 8922.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Verl and TRL you do have to put the",
      "offset": 8924.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "inference on another server and then",
      "offset": 8926.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "your training is like a separate server",
      "offset": 8928.24,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and they have to do communication. We",
      "offset": 8929.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "don't there is no communication for us.",
      "offset": 8931.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "There is none. So we do it's very close",
      "offset": 8932.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to asynchronous training nearly no delay",
      "offset": 8935.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in training but yes we we don't support",
      "offset": 8936.96,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "it yet but we do plan to support like",
      "offset": 8940.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you know larger training runs um yeah",
      "offset": 8942.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "yes",
      "offset": 8946.16,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "question what's",
      "offset": 8947.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "yes people have asked that no road map I",
      "offset": 8954,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "don't know if we're going to support it",
      "offset": 8956.96,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "it's a bit more complicated you could",
      "offset": 8958,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "use I think XLA like they they do have",
      "offset": 8959.28,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "PyTorch converted down to TPU So maybe",
      "offset": 8961.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it might work. I don't know if it works.",
      "offset": 8963.439,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "I've never tried it. Um",
      "offset": 8965.359,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "maybe later. Um yeah, maybe later.",
      "offset": 8969.2,
      "duration": 8.119
    },
    {
      "lang": "en",
      "text": "Okay. Yeah. Yes.",
      "offset": 8973.2,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "Oh, you don't have to. You you the the",
      "offset": 8979.12,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "whole point then like this thing um",
      "offset": 8982.399,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "uh here, right? We chose a base model",
      "offset": 8986.479,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "because you can show that you can do a",
      "offset": 8990.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "base model going to the green dot. But",
      "offset": 8991.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "then unfortunately in the collab we do",
      "offset": 8993.92,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "have to do some supervised fine tuning",
      "offset": 8996,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "otherwise you'll wait there forever. The",
      "offset": 8997.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reward again will be 000000. We just",
      "offset": 8998.88,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "want to remember all of AI is about",
      "offset": 9001.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "efficiency and speed. So we just want to",
      "offset": 9003.439,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "showcase okay you do need to do the",
      "offset": 9005.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "light blue step. You still need to do",
      "offset": 9007.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the supervised fine tuning step. Um yeah",
      "offset": 9009.68,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "yeah",
      "offset": 9012.16,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "oh no you don't need to. You can take",
      "offset": 9016.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the instructor. So the notebooks over",
      "offset": 9017.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "here. So for example, if you go to the",
      "offset": 9018.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 9020.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the llama 3.23 billion notebook, we",
      "offset": 9022.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "don't do any fine tuning step at all.",
      "offset": 9025.439,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "You skip directly because it's an",
      "offset": 9027.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "instructor model already. It already",
      "offset": 9028.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "learns how to do chat. It already learns",
      "offset": 9029.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "how to answer some questions. You can",
      "offset": 9032,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "skip directly to GPU. Um if it loads, um",
      "offset": 9033.68,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "but yes, the notebooks, okay, you'll",
      "offset": 9038,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "have to wait for it to load. Um",
      "offset": 9040.72,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "whatever. The internet's very bad. Um it",
      "offset": 9044.24,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "is loading. Um yes. Any other questions?",
      "offset": 9047.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 9051.68,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Over in the reinforce algorithm you had",
      "offset": 9052.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the log probability of a state of an",
      "offset": 9054.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "act.",
      "offset": 9056.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Is that happening inside the sampling",
      "offset": 9058.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "model? Where is that in the notebook?",
      "offset": 9060.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Oh the the the algorithm itself of GPO.",
      "offset": 9062.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Oh it's like behind the scenes like",
      "offset": 9065.359,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "yeah is that happening over in the GPO",
      "offset": 9066.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "trainer?",
      "offset": 9069.6,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "Yes it's inside the trainer itself like",
      "offset": 9070.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "somewhere in the code somewhere it does",
      "offset": 9071.84,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "that.",
      "offset": 9073.6,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "It's figuring out like what is the",
      "offset": 9074.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "probability of token versus all the",
      "offset": 9075.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "tokens.",
      "offset": 9077.84,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "Oh, the calculation is inside the",
      "offset": 9078.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "trainer. So like somewhere, you know, on",
      "offset": 9080.479,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the GPU, you're doing this calculation,",
      "offset": 9081.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "but you do get the probabilities.",
      "offset": 9083.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "Remember the language model, you get the",
      "offset": 9085.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "probabilities already. You just get the",
      "offset": 9086.56,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "reward function and you just want to",
      "offset": 9088.16,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "maximize it.",
      "offset": 9089.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So do you take the log that come out of",
      "offset": 9090.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the large language model and turn them",
      "offset": 9093.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "into pseudo probabilities and then just",
      "offset": 9094.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "assume that's",
      "offset": 9096.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I think so. Yes, that's correct. I think",
      "offset": 9098.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it's exponential of Yes, I think that's",
      "offset": 9100.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "correct. There is if you go to like the",
      "offset": 9102.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "code there is like some derivation for",
      "offset": 9104.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "it but yes you're correct any oh okay",
      "offset": 9105.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the notebook loaded but yes there is",
      "offset": 9108.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "another notebook which does the instruct",
      "offset": 9110.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "here right there is instruct model and",
      "offset": 9111.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "there is no fine-tuning step at all it",
      "offset": 9115.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "just does the reward function um and",
      "offset": 9118,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "stuff like that um wheels notebook for",
      "offset": 9119.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "example is also very good so like if",
      "offset": 9121.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "anyone wants to check other notebooks",
      "offset": 9123.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "out wheels notebook um also utilizes I",
      "offset": 9125.04,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "think also the instruct model and then",
      "offset": 9127.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "does gio um",
      "offset": 9129.439,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Okay, the GR Okay, kind of time is",
      "offset": 9132.319,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "running out, but okay, technically the",
      "offset": 9134.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "GP4 portion is done.",
      "offset": 9136.399,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Oh, there's actually more portions. Um,",
      "offset": 9140.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I will have to breeze through them.",
      "offset": 9142.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "There's only 10 minutes left. Um,",
      "offset": 9143.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Whoops. Um, any other I will take",
      "offset": 9146.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "questions at the very end. Anyways, I'm",
      "offset": 9149.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "going to stay here anyways afterwards.",
      "offset": 9150.479,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 9151.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "quantization. We'll now shift over to",
      "offset": 9153.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "quantization. Um, so I don't know if you",
      "offset": 9155.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "guys know about the deepse R1 1.5 bit",
      "offset": 9157.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "quance that we did. Um but you can",
      "offset": 9160.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "essentially download these models.",
      "offset": 9162.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Deepseek R1 is 730 I think 730GB.",
      "offset": 9163.92,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "You can quantize them down to 140GB um",
      "offset": 9168.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "without that much loss in accuracy.",
      "offset": 9171.439,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Okay, there's obviously loss in",
      "offset": 9173.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "accuracy. But the trick is you can",
      "offset": 9174.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "quantize them down to be very small and",
      "offset": 9176.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "miraculously they work.",
      "offset": 9179.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Um llama for scout for example, right?",
      "offset": 9182.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "You can't really see the accuracy plot.",
      "offset": 9184.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Um the smallest number is 80% accuracy",
      "offset": 9186.64,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "on MLU5 shot 80%. And the highest",
      "offset": 9189.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "accuracy is 81 point something right so",
      "offset": 9193.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it's actually only 1% difference and the",
      "offset": 9196,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "one on the left is a one bit quant it's",
      "offset": 9198.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "very small it's tiny in comparison to",
      "offset": 9201.84,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the full precision like float 8. And so",
      "offset": 9204.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "essentially you can make the model eight",
      "offset": 9208.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "times smaller and you only decrease",
      "offset": 9209.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "accuracy by 1%. Um so that's very",
      "offset": 9212.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "interesting. And so essentially we",
      "offset": 9215.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "showcase that you can actually quantize",
      "offset": 9216.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "layers the mixture of experts layers",
      "offset": 9218.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "very heavily but you must leave the",
      "offset": 9220.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "attention layers the shared experts and",
      "offset": 9222.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "other layers in higher precision and",
      "offset": 9225.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that's called that's what we call the",
      "offset": 9227.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "dynamic quantization methodology.",
      "offset": 9228.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Um if you see there was like a benchmark",
      "offset": 9231.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of llama 4 scout for example um if you",
      "offset": 9233.6,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "use a two bit a twobit quant it actually",
      "offset": 9236.319,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "gets high accuracy than other other full",
      "offset": 9239.359,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "precision um providers which is very",
      "offset": 9243.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "interesting right so like for example",
      "offset": 9246.399,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "the two-bit quant gets um 73% accuracy",
      "offset": 9247.84,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and then other inference providers get",
      "offset": 9252.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "65% accuracy 67 right there is a very",
      "offset": 9254.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "large difference and so okay there is",
      "offset": 9257.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like some bugs in the models and there's",
      "offset": 9259.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Maybe they quantize it incorrectly, but",
      "offset": 9261.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the goal is to show that if you quantize",
      "offset": 9262.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "a model down to be very small bits, it",
      "offset": 9265.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "still works.",
      "offset": 9267.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "We showcase this with an example. For",
      "offset": 9270.319,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "example, if you take a vision model like",
      "offset": 9271.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "um Quen 7 bill uh Quen 2 billion, um if",
      "offset": 9273.439,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "you naively quantize all the layers to",
      "offset": 9276.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "be four bit, right? You ask the model",
      "offset": 9278.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "what does this image show? It will say",
      "offset": 9281.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "the image depicts a vibrant and colorful",
      "offset": 9284.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "scene of a coastal area, which is",
      "offset": 9286.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "totally wrong, right? The answer should",
      "offset": 9288.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "be the image shows a train traveling on",
      "offset": 9290.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "tracks or something like that, right? If",
      "offset": 9292,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "you quantize everything to 4bit, it's",
      "offset": 9294.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "1.36GB,",
      "offset": 9297.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "but it's definitely bad. So, the trick",
      "offset": 9299.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is you must quantize some layers to be",
      "offset": 9301.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you must leave some layers to higher",
      "offset": 9304.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "precision and you only need to increase",
      "offset": 9306.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "it by 500 dB or so to 1.8 bit and it",
      "offset": 9308.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "works. The image shows a train traveling",
      "offset": 9311.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "on tracks. It suddenly works.",
      "offset": 9313.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "But the question is which layers do you",
      "offset": 9316.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "not quantize? That's the question,",
      "offset": 9318.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "right? Which layers? You could do an",
      "offset": 9320.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "exhaustive search, right? You can check,",
      "offset": 9322,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "oh, let's not quantize layer zero. Let's",
      "offset": 9323.68,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "check layer 1, layer two, you know,",
      "offset": 9325.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "check every single one, but it'll take",
      "offset": 9327.439,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "forever. So, definitely don't do that,",
      "offset": 9328.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "right? You have 70 choose one or",
      "offset": 9330.479,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "something. 70 choose one plus 70 choose",
      "offset": 9332.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "two. Horrible, right? And remember, all",
      "offset": 9334.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "of AI has a bad efficiency. So, don't do",
      "offset": 9336.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that. The trick is you can check the",
      "offset": 9338.399,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "activation quantization error and the",
      "offset": 9340.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "weight quantization error and you will",
      "offset": 9342.479,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "see these large outliers. For example,",
      "offset": 9344.56,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "for Quen, if you quantize the first few",
      "offset": 9347.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "layers, it's extremely bad. So, you must",
      "offset": 9350.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "leave the first few layers not",
      "offset": 9353.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "quantized. And also, this gigantic jump",
      "offset": 9355.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for the weight quantization error. This",
      "offset": 9358,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "means you probably shouldn't quantize",
      "offset": 9359.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that layer as well. There are some other",
      "offset": 9361.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "plots that we show. For example, for",
      "offset": 9363.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Llama 3.2,",
      "offset": 9364.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "it's interesting. All of these graphs",
      "offset": 9366.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "are very different from each model. Um,",
      "offset": 9368,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you will notice Llama 3.2 has these",
      "offset": 9369.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "weird, you know, continuous spikes. Um",
      "offset": 9371.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it's because they use attention and then",
      "offset": 9374.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "they put the attention back to the",
      "offset": 9376.64,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "vision module. I think every single",
      "offset": 9377.84,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "three I think it was every single three",
      "offset": 9379.2,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "layers. So every single three layers it",
      "offset": 9380.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "has these big jumps. Um this means you",
      "offset": 9382.479,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "should not quantize those layers.",
      "offset": 9384.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Pixrol for example is also a difference",
      "offset": 9388.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "graph again. Um pixrol seems like you",
      "offset": 9390.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "can't quantize many layers",
      "offset": 9392.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "unfortunately. Um and so like the whole",
      "offset": 9394.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "vision module must not be quantized.",
      "offset": 9397.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "There is a very there's a very important",
      "offset": 9400.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "paper talking about like you know why",
      "offset": 9402.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "which layers you should quantize and",
      "offset": 9404.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "which layers you should not quantize.",
      "offset": 9406.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "It's called the super weights paper. Um",
      "offset": 9407.52,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "you should you guys definitely should",
      "offset": 9409.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "read that. Essentially it says that in",
      "offset": 9410.479,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "all language models the first few layers",
      "offset": 9412.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "of the down projection there is a very",
      "offset": 9415.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "very very important number in one of the",
      "offset": 9417.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "numbers. One of the numbers in the",
      "offset": 9420.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "models very very very important and you",
      "offset": 9421.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "should never quantize it ever.",
      "offset": 9423.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "But the trick is that the interesting",
      "offset": 9426.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "finding is it's not actually a very",
      "offset": 9429.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "large number. So there is a trend in the",
      "offset": 9432,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "there is a trend in um language model",
      "offset": 9434.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "space where people think that you should",
      "offset": 9436.479,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "not quantize outliers. The problem is",
      "offset": 9438,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you know these models have these big",
      "offset": 9440.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "outliers like suddenly in the model",
      "offset": 9441.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "there's like this big number like 3,000",
      "offset": 9443.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and if you quantize it it essentially",
      "offset": 9445.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "ruins the model but actually this paper",
      "offset": 9447.84,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "shows that it's not actually the",
      "offset": 9449.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "outliers that are the problem. These",
      "offset": 9451.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "numbers could be very small and if you",
      "offset": 9453.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "look at the plots if you remove if you",
      "offset": 9456.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "select these numbers and if you make",
      "offset": 9458,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "them zero",
      "offset": 9459.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the accuracy",
      "offset": 9461.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "uh the accuracy decreases dramatically.",
      "offset": 9463.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Um and so like if you see like for",
      "offset": 9465.52,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "example if you remove one of the numbers",
      "offset": 9467.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "um you know the activation value totally",
      "offset": 9469.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "decreases very bad they have very large",
      "offset": 9471.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "activation values and then if you remove",
      "offset": 9473.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "them it's very very very very bad. Um,",
      "offset": 9476.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "there is another trick that you can do.",
      "offset": 9478.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "If you have a model that has seven",
      "offset": 9481.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "billion parameters, make every single",
      "offset": 9482.479,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "number go to zero. The first parameter,",
      "offset": 9484.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "make it go to zero, check accuracy. The",
      "offset": 9487.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "second number, make it go to zero. Check",
      "offset": 9489.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "accuracy. The third number go to zero.",
      "offset": 9491.439,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Check accuracy. You can do this seven",
      "offset": 9493.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "billion times and you can see which",
      "offset": 9494.479,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "number is the most important. You could",
      "offset": 9495.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "do that as well. Um, but remember AI",
      "offset": 9497.2,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "efficiency very not a good idea. um",
      "offset": 9499.6,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "more later research. For example, the",
      "offset": 9504.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "new Blackwell chips um instead of doing",
      "offset": 9506.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "a quantization to like one bit, two bit,",
      "offset": 9509.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "three bit, 4bit, Nvidia chips also have",
      "offset": 9511.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this new architecture, this new format",
      "offset": 9513.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "called FP4 or MXFP4. Um and essentially",
      "offset": 9516.56,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "this is float 4. Um and float 4 is very",
      "offset": 9520.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it's it's most likely going to be very",
      "offset": 9524.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "used a lot in the future. Um and then",
      "offset": 9525.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's these like new formats for",
      "offset": 9527.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "quantization as well which which",
      "offset": 9529.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "essentially allows you to train models",
      "offset": 9531.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "in very low precision.",
      "offset": 9533.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "I also made this plot um going from",
      "offset": 9535.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "float 32. So the question people always",
      "offset": 9538.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "ask is why is GPUs getting faster and",
      "offset": 9541.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "faster and faster? My take is actually",
      "offset": 9542.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this year is probably the last year",
      "offset": 9545.84,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "you're going to get GPUs that is",
      "offset": 9547.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "actually faster. There is no more faster",
      "offset": 9548.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "GPUs. Why? Because the majority of GPU",
      "offset": 9550,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is getting faster is because of",
      "offset": 9552.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "numerical precision. From float 32 to",
      "offset": 9553.68,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "float 16, you get five times faster.",
      "offset": 9556.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Why? Why is it five times faster?",
      "offset": 9559.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Because because um the calculation of",
      "offset": 9561.2,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "faster is when you use transistors, it's",
      "offset": 9563.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "the exponent plus the mantissa squared,",
      "offset": 9566.479,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "right? And float 32, you have to use 23",
      "offset": 9569.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "numbers for the mantissa and 23 squared",
      "offset": 9571.76,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "is very large. Float 16, you reduce the",
      "offset": 9574.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "mantissa to 10. And that is why you get",
      "offset": 9577.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "five times speed up from float 32 to",
      "offset": 9580,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "float 16. It's because the number itself",
      "offset": 9581.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is getting smaller. The representation",
      "offset": 9584.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "inside the models for each of the",
      "offset": 9585.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "weights is getting smaller. And then we",
      "offset": 9587.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "moved from float 16 to be B float 16. It",
      "offset": 9589.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is again maybe around two times faster",
      "offset": 9592.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "than float 16.",
      "offset": 9594,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "We then have float 8. Um float 8 is even",
      "offset": 9596.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "more faster. Um you know uses even less",
      "offset": 9599.68,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "space. Um but then there is a problem.",
      "offset": 9602.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "Float 4. We get to float four and it's",
      "offset": 9606.479,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "around two times faster than float 8",
      "offset": 9608.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "around. Um the problem is what's next?",
      "offset": 9609.92,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Do we go to float two, float three,",
      "offset": 9614.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "float one? You know I there's you can't",
      "offset": 9616.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "push anymore in terms of numerical",
      "offset": 9620.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "precision. There is not much more to go",
      "offset": 9622,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "in terms of that space. And so like you",
      "offset": 9623.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "can only get maybe 180 times faster than",
      "offset": 9625.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "float 32, maybe 200 times faster, but",
      "offset": 9628.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "essentially my take is float 4 might be",
      "offset": 9631.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the final flop that's getting faster.",
      "offset": 9633.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "You know, the final um precision",
      "offset": 9637.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "numerical precision and in the future",
      "offset": 9638.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "GPUs are not going to get faster. Um so",
      "offset": 9641.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "maybe if you know people want to buy",
      "offset": 9642.88,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "blackwell GPUs, that's probably you",
      "offset": 9644,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "should probably buy them. It's most",
      "offset": 9645.439,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "likely not going to get faster anymore.",
      "offset": 9646.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Um that's kind of my take.",
      "offset": 9648.479,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "And also for Okay, I was going to talk",
      "offset": 9651.359,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "about kernels and stuff. I don't think I",
      "offset": 9653.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "have enough time. You must use",
      "offset": 9654.479,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "torch.compile. You know, every single",
      "offset": 9656.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "function that you see, wrap it in torso",
      "offset": 9658.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "compile. Try it out. You know, like I I",
      "offset": 9660.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "always tell the PyTorch team, please",
      "offset": 9663.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "make it by default. You know, definitely",
      "offset": 9664.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "use torso compile. Um why? Because it",
      "offset": 9666.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "makes your training faster sometimes,",
      "offset": 9669.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "only sometimes. Um not all the time. It",
      "offset": 9671.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "reduces memory usage most of the time.",
      "offset": 9674.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "If you see bugs, they'll probably fix",
      "offset": 9676.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it. Um,",
      "offset": 9679.04,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "but remember torch.compile is not as",
      "offset": 9681.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "easy as you think. You don't just do",
      "offset": 9683.439,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "torch.compile the model. There is",
      "offset": 9684.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "actually many options you can tune,",
      "offset": 9686.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "right? I just listed a few options. Um,",
      "offset": 9688.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "this is this is literally just a few.",
      "offset": 9692.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "There is like 10 more pages of options.",
      "offset": 9694.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I'm being serious. 10 more pages you can",
      "offset": 9696.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tune. Imagine if you can like use",
      "offset": 9698.479,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "torch.compile and tune every single one.",
      "offset": 9700.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And that's why I highly suggest people",
      "offset": 9703.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "to use torture compile more effectively.",
      "offset": 9706,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "um it's probably the biggest thing that",
      "offset": 9708.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "can change your entire, you know,",
      "offset": 9710.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "training run. Um make a more more memory",
      "offset": 9711.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "efficient, make it faster. So definitely",
      "offset": 9714.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "look through this. Um",
      "offset": 9716.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "okay, so in general, yes, thank you.",
      "offset": 9718.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Definitely start us on GitHub. Um join",
      "offset": 9720.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "our discord if you want to have any",
      "offset": 9722.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "questions on RL and stuff. Um we have a",
      "offset": 9724,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "website as well. Um and finally, we have",
      "offset": 9726.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "stickers. Um yes, there are some limited",
      "offset": 9729.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "time stickers as well that we have",
      "offset": 9732.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "somewhere, I think over there. Um, and",
      "offset": 9734.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "remember if you have any questions, I'm",
      "offset": 9736.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "still going to stay around and ask. Um,",
      "offset": 9737.359,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "yeah. Thanks a lot.",
      "offset": 9739.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 9746.65,
      "duration": 2.239
    }
  ],
  "cleanText": "[Music]\nHello guys.\nHello.\nHello.\nHello.\nYes.\nSorry for being a bit late.\nThere's a lot of traffic.\nBut um, hello.\nUm, welcome to AI Engineer World's Fair.\nThanks for coming to my session.\nUm, and yes, today we're going to talk about the deep dive into Reinforcement Learning, Kernels, Agents, and Quantization.\nUm, you might know me or you might not know me, but I'm Daniel Han.\nUh, my brother is somewhere.\nUh, yeah, somewhere, but yes.\nUm, thanks for coming again.\nOh, we also have stickers and other like random stuff later.\nUm, that's after the talk.\nUm, so maybe you might know me, maybe you might not.\nUm, so we, um, on Twitter, we tweet a lot.\nUm, we did like a gradient accumulation bug fix last year.\nUm, we introduced something called async offloaded gradient checkpointing.\nUm, we also work with the Hugging Face, Google, Meta, Mistral teams to like fix bugs and like their open source models like Gemma, Llama, Mistral, Phi, and more.\nUm, yeah, so like we, if you want to follow on the latest stuff of AI, better follow us.\nUm, we tweet about random stuff.\nUm, you might even know when next models might be released.\nYou know, we sometimes tell people approximately.\nUm, so that might be very interesting.\nUm, we also do like open source contributions to the entire open source ecosystem.\nFor example, we contribute sometimes to Llama CBP.\nUm, we work with the Quen team and Mistral on their releases.\nUm, we also do like, you know, F4 bug fixes, Llama 4 bug fixes which increase accuracy by a bit.\nUm, so definitely, you know, utilize some of the new newer uploads that we do, um, which fix bugs all the time.\nWe just surpassed 10 million monthly downloads and Hugging Face.\nUm, and yeah, we also have a GitHub package um with 40,000 GitHub stars.\nUm, and we do like essentially we make fine-tuning faster and reduce memory usage.\nAnd yeah, that's the GitHub package.\nDefinitely check that out.\nUm, there's like free collab notebooks.\nI'm not sure if many people know, but you have like free GPUs that Google offers.\nYou can just use them.\nUm, please use them more.\nUm, and Kaggle, if not many people know, have 30 hours for free of GPUs per week, right?\nAnd there's no restrictions on it.\nYou know, please utilize the free resources as much as possible.\nUm, yes, they won't be unhappy.\nJust use them all.\nSo, yes, we have notebooks.\nSo, if you scroll down a bit on our GitHub page, there's like free notebooks um for Collab, Kaggle.\nWe do Reasoning, um, continue pre-training, supervised fine-tuning, and other stuff.\nWe also upload models to our Hugging Face page.\nUm, for example, DeepSeek-R1 released a few days ago.\nWe upload 1.58 bit quants which are like very small.\nThey retain most of the accuracy.\nUm, so like these can run on your local device.\nUm, even if you have like very low VRAM or like not a very good GPU, um, it will still work and we will constantly upload models.\nSo like sometimes people complain to us, you know, please stop uploading fixed models.\nYou know, it's kind of annoying.\nUm, but too bad, you know, unfortunately models have bugs.\nSo we do have to fix them immediately.\nYou will see sometimes, for example, the accuracy can increase by 10%.\nUm, sometimes, you know, the large model providers won't tell you that they uploaded a fix.\nThey're not going to tell you.\nBut, you know, be sure to download the latest models.\nYou will get all the fixes.\nNow, today, let's start off from history, right?\nDoes everyone remember Llama?\nUm, although finally it got leaked, right?\nIt was just a research paper, you know, Meta saying, \"Oh, we trained Llama.\nWhere's the weights?\"\nYou know, it's only research access.\nAnd then suddenly it got leaked.\nAnd then you kind of just kind of spawn the entire open source movement.\nUm, you know, some of the people who are the who are the authors, you know, are not part of Meta anymore, but Llama is extremely important for the entire ecosystem and it was like the beginning of open source kind of um for large language models.\nThe most famous plot from the paper is this, right?\nSo like, you know, if you keep making the model train more, it just the loss just keeps going down.\nUm, well the question is, will the loss keep going down?\nThat's the question, right?\nSo Llama 1 was only trained at 1.4 trillion tokens, right?\nSo now 1.4 trillion tokens is actually very less.\nMost models are trained 10 times more.\nUm, and you can also see from the trend that the bigger the model, the lower the loss, right?\nSo 7 billion is the blue line.\nUm, and then 65 billion was like the red line.\nAnd you can see that in general as a model gets bigger, it it gets smarter.\nUm, the training loss, the numbers are correct.\nSo you should see generally these numbers from around maybe a bit higher than one.\nUm, if you see training losses when you do fine tuning of like 8, 13, definitely something's wrong.\nUm, so you should get at least losses around two-ish, three-ish.\nAnd so now, oh, now Google, um, you know, Google's new Gemma 3 models are trained on 14 trillion tokens, right?\nWhich is much more.\nUm, Llama 4 is trained on 30 trillion tokens, right?\nSo like literally like at least 10 times more.\nUm, so Gemma is 10 times more.\nLlama is like, you know, 30 times more.\nOh yes, I forgot.\nYou can also access the slides by the QR code if you want.\nIt's also on the docs as well.\nUm, so if you go to the docs, there will be a link to all the slides.\nUm, I will probably post the slides anyways on Twitter and elsewhere so you can access them as well.\nAlso if there are questions for people, like, you know, raise your hand, ask, I will essentially do intermission between like some parts and I will ask people if you have questions, please ask.\nUm, last time the talk many people asked questions, I will answer every single one, even if it's stupid, I don't care, please ask.\nUm, sometimes I get stuff wrong.\nSo, yes, just ask questions.\nUm, are there any questions?\nI'm I'm assuming no.\nOkay.\nOkay.\nLet's go to the Okay, I will.\nOkay.\nSo, I I don't know if people have seen this plot, very famous from Maxim.\nUm, he shows the open source versus closed source performance of popular benchmarks.\nI think this is MLU5 shot.\nUm, you can see the green line is open source models.\nUm, and then the red line or the orange line is like, you know, closed source models.\nUm, and you can see that in general the slope of the open source models is more dramatic than the closed source models.\nAnd I would say in general that, you know, the open source models and closed source models in terms of MLOU, they've kind of like reached the same accuracy, right?\nSo like you can see that, okay, this is already outdated, but in general you see like Llama 3.1 405 billion kind of reached, you know, GP40 level, um, so open source models definitely have caught up to closed source models.\nHowever, there was a however, um, recent, you know, like since September 2024, I would call this something called the open source drought.\nUm, no, no one wants to talk about it, but I will, right?\nSo, like September 2024, 01 got released, 01 preview.\nAnd to be honest, the open source community was shocked, right?\nSo, like suddenly the capabilities diverged, right?\nSo, there's something called the MLU plateau where most models, the open source models and the closed source models, they kind of converged.\nSo, the open source models was equivalent to the closed source models.\nBut suddenly in 2024 September, you know, OpenAI released 01 preview and it kind of shocked the entire community because the capability or intelligence kind of skyrocketed, right?\nSo like with Reasoning, long Reasoning traces, it just was a total change of mindset.\nUm, and for four months the open source community kind of died internally because there was nothing, we can't replicate it, we don't know what to do, you know, do we do this, do we do that, I don't know, like, and so like, but then suddenly in 2025 January, DeepSeek-R1 came along and they released R1 and that's when the entire world kind of changed their view, right?\nSo like you can in fact train open source models to be as powerful as 01 or 03 or whatever, right?\nSo like that's that was what I call the open source drought.\nHowever, there was a previous drought even before that, remember when CHBT got released in 2022 December, right?\nSo like before even CHBTT, most models were base models, um, they were not really instruct fine-tuned that well and so most large pre-trained models were actually useless, but they were terri.\nBut then suddenly CHP came along and they did better, you know, Reinforcement Learning from Human Feedback, better instruction fine-tuning, better instruction following and it really changed the world, right?\nSo like I think large lang large large language models were already here before 2022, right?\nIt they were already there, but it was just chipd which showed that if you have good data, right, good instructions, good answers, good supervised fine-tuning, um, and good Reinforcement Learning, you can actually make the model very useful.\nUm, and yes, again, open source had a delay, a very long delay, right?\nUntil like Llama one, I guess.\nUm, and so always I would say open source always tries to catch up to the closed source models.\nUm, the next question is what is after Reasoning?\nUm, is there going to be something else?\nUm, I think that's a very good question.\nMy personal take is it's going to be very hard.\nUm, I think Reasoning was like the last most the DeepSeek-R1 paper said that most likely the model already has these Reasoning capabilities and we just need to accentuate them.\nUm, and so like I'm not sure if there's going to be some new, you know, like step function where we'll get to like the next capability, um, but in my view, I think like every single time the closed source models will always do like a step function, um, but who knows, maybe now it will plateau forever, I don't know.\nSo that's like you have like, you know, you have like long discussions about, you know, if AGI is going to come or not, like, but who knows.\nUm, the talk is not going to be about that, but um, yes, next.\nUm, so I call the first jump the SFT or um, RHF jump, right?\nSo like that's essentially if you do good supervised fine tuning, you get this large jump in performance and then the second jump is called the RL jump, right?\nSo like this essentially can increase performance dramatically if you employ second methodologies like RL, right?\nSo like, but the question is like what's the next jump?\nUm, I don't know.\nSo I'm not sure if you guys saw this picture before, um, it's very widely known in the community about, you know, by Yan Lakun, he essentially showed this cake, um, and essentially unsupervised learning or like just pre-training in general, um, you know, it's just a cake, not that good, um, and then supervised fine-tuning is kind of the icing on on top of the cake.\nSo like it's a bit better.\nUm, and then the Reinforcement Learning is the cherry, right?\nSo like I'm not sure people like the cherry, but like some people like the cherry.\nUm, and so like the goal is how can we get the cherry?\nUm, but the problem is there's so less data about this, right?\nReinforcement Learning is like very, very, very less data.\nAnd so the problem is most large model labs will train these large pre-trained models and then they will iteratively refine it to make the model better through supervised learning, through Reinforcement Learning.\nInterestingly enough, this slide was actually shared last year.\nVery popular.\nBut actually, this was from 2016 September.\nUm, so I had to dig this up on YouTube.\nAnd so Yan Lakun actually talked about this back in 2016.\nSo literally nearly 10 years ago.\nUm, I was like, how wait a second, that's 10 years ago.\nVery long.\nUm, so this slide was actually very popular on Twitter.\nI I think it was in November last year.\nPeople kept tweeting about it.\nI don't know.\nI saw this, I was like shocked.\nUm, but yes, so this encapsulates like the current AI um, boom.\nAnd so like firstly like when we talked about these large models, remember they started from a base model.\nUm, and so we call these training stages, right?\nSo when you have a base model, um, you then convert it to a chat model, right?\nSo like for example, chat GBT is not a base model, it is a instruct fine-tune model or like some sort of fine-tuned model from a base model.\nSo actually OpenAI does have a base model somewhere sitting in their server somewhere.\nUm, they're probably not going to serve it ever, but it is somewhere on the computers and they essentially fine-tuned it to make Chai GP4.\nUm, Claude 4 has most likely a base model and then they fine-tune it to become Opus, right?\nSo like Gemini also has a base model and they convert it to Gemini 2.5 Pro.\nSo this this phase when you convert a base model to a chat model is the finetuning phase.\nUm, and then the question is like, you know, what do we do in the arrow, right?\nLike, you know, is it Reinforcement Learning, is it supervised fine tuning, is it like some other special source, I don't know, but like we essentially we'll discuss about these um topics.\nUm, any questions first?\nOkay.\nSo for example, in open source models, you might have seen Gemma 3 pt, um, Gemma 3, it, Llama 4, Llama 4 instruct, Quen 3 base, Quen 3, um, M small base, M small instruct, Llama 2, Llama 2 chat, right?\nThese like terminologies, to be honest, I think the open source community should standardize a method like terminologies like instruct or chat or, you know, no, not even a word like, you know, it and PT, maybe they should like standardize it a bit, um, but in general, if you see it, it means instruct instruction tuned, PT means pre-trained, um, instruct just means, you know, instruction fine-tuned, Quen 3 just removed it entirely, it's just called Quen 3, um, and then the base model is called with a base, um, and so like essentially these naming methodologies, um, if you see on Hugging Face, um, hopefully you will now recognize these different types of um, models.\nAnd so generally what we say for like, you know, Reinforcement Learning and finetuning is I would say it's called re fine tuning is everywhere.\nUm, you start off with pre-training, you then convert it into a supervised fine-tuning model via supervised fine tetuning, SFT, it's called SFT, you also might hear like IF, which is instruction fine tuning, they're the same thing, um, and then we call something post training, the post-training phase.\nUm, but actually recently it actually kind of changed.\nUm, so I don't know if you guys have been keeping up with the latest stuff, terminology.\nUm, I actually don't really like terminology anymore, but like we have something called pre-training, which is you take like all of Wikipedia, all of the web, you know, everything, all of the data you can ever\n\n\nSee, shove it into the model, predict the next word. That's called the pre-training, uh, pre-training stage. We then have something called the mid-training stage, um, which essentially gives you higher quality data. Like, for example, you can weight Wikipedia more because it's higher quality. Um, you can essentially do long context extension as well. You shove this during the mid, mid pre-tra, uh, mid-training stage. So if your context of your model is very short, you want to extend it to very long context. You shove this during the mid-training stage. And then the second stage is the supervised fine-tuning stage where you want to convert the model to a chat model. And then we have the post-training phase, which is like pre-preference fine-tuning, DPO, RHF, and stuff like that. And then we have this new thing called reinforcement fine-tuning or RLVR. Uh, if no one knows what RLVR stands for, it stands for reinforcement learning with verifiable rewards. And this is like a new paradigm, um, not the same as preference fine-tuning or DPO, where we consider reward functions to make models much better. Um, and so this is how I would envision, like, you know, the whole training phases of models.\n\nAnother way to put it is we have some random initialization of the model, like some random weights of the model, right? So, like, seven billion parameters, literally random numbers, right? Like GPT4, GP4, I don't know, 1.4 trillion parameters, like just random numbers, and then somehow we move in the space, like the black line, right? So pretend this is like some high-dimensional 1.4 trillion dimensional space, and then we somehow move in this space, and then we get the final model. Right? That's a green job. The question is, how do we move in this space to get to the final model? That's the question.\n\nMost people, what they do is, firstly, you start from a random initialization. You do the pre-training phase, which is very long. You get to this dark blue dot, right? That's called the pre-trained model. And then you do some supervised fine-tuning, instruction fine-tuning to get the blue dot. And notice the line for the light blue line is very short because it is very short, right? There is not that much data for supervised fine-tuning. And then somehow we get the blue dot, and then we keep doing more iteration to get to the purple dot, which is through preference fine-tuning. And then finally, we get the, the green dot, which is reinforcement learning via, you know, verifiable rewards, like, you know, 03 or 01. And so, like, the goal is somehow we have to move from the black dot to the green dot. And essentially, all of large language models, all of AI is just an optimization problem, right? Like, how do we make this easier to get to the green dot? You could, you know, you could kind of theoretically guess, you could just, why don't you just go from the green dot, black dot to the green dot, like skipping all of the dumb phases, you know, just skip it entirely. Yes, you could do that, but it's not going to be very efficient. You're going to be waiting there for, like, you know, I don't know, millennia. Your loss is not going to go down. So, the tricks that we found in Reinforcement Learning, um, AI is, like, you have to do these phases to get to your final green dot. Um, there is, like, a new methodology where you can actually bypass this supervised fine-tuning stage and the preference fine-tuning stage and directly go to the green dot. There is a way, and that's the dark red line. Um, I think DeepSeek, uh, DeepSeek-R1 kind of, like, showed that you can use a pre-trained model, a base model, and directly do some reinforcement learning with verifiable rewards and just skip it entirely. Um, so that's, like, a new paradigm that people want to focus on. In my view, I think you should still do the light blue, the purple, and then the green. I don't think so you should, like, directly skip over to the green. If you want to waste resources, you can skip to the green. Um, but I don't think so large model labs want to waste resources. Um, hopefully not.\n\nSo, I, I don't know if people have seen this diagram. Agents in the old sense, like everyone keeps connecting agents with reinforcement learning. Okay, but, like, why? Um, so, in general, an agent is, you have some sort of environment. You have, like, the agent doing something in the environment. You get, like, an action, you do the action, and then you get some, some sort of reward. Um, and the reward is R. S is the state. So, the current, what the environment currently looks like. Um, and then essentially, RL tries to optimize this loop. You're trying to maximize a reward, um, giving some sort of action. Um, and that's why, like, you know, that's why RL and agents are kind of connected. Assume the agent is the language model, right? So, assume the agent is, in fact, the language model. And so, this, the environment is kind of fishy, like, you know, it's, it's hard to say what exactly is the environment, it's more like the language model's inference space, that's the environment, kind of, um, but, like, pretend this was a game, right? So, like, the agent was the computer, the environment is, like, Mario, for example, right? And so, like, you're playing the Mario game automatically, and your goal is to win the game. And so, like, the whole goal of RL is to maximize reward.\n\nAnother one is, like, Pac-Man, right? So, like, you have the yellow Pac-Man. And, um, you can either go up, down, left, or right. Right. So, like, up, down, left, or right. That is the action. That's the a, the yellow, the, I think, I don't know, orange or something. The orange little things are, like, rewards, right? So, like, if you eat, if you eat, you know, a yellow, a red, an orange dot, you will get positive reward for. So, R plus, right? So, like, you have R pluses. If you eat a very big one, you'll get, like, very large reward. But if you encounter one of the enemies, you will get minus reward, right? All right. So, like, the qu, the, the question is, how do we maximize the reward based on this environment?\n\nFor language models, there is a trick. The trick is this loop kind of changes because we don't actually have a continuous loop, um, the state does not actually change over time, right? So, like, for example, in a game, in a game, if you do an action, the whole state changes, right? The environment totally changes, and so you have to, like, continuously keep a history of the past steps. But in language models, there is no history, right? So, like, if you do a prompt, what is 2 plus 2? If you ask another question, what is 4 plus 4? It's, like, totally not relevant to your previous prompt. Okay, well, fine, it is kind of relevant, but you, like, it's not directly correlated. And so, like, you can actually delete one of the lines. Um, that's the next prompt. You can delete it entirely. So, for example, what is 2 plus 2? Right? So, like, essentially, you have all of these options. It could be zero, it could be one, it could be two, it could be infinity, it could be B, it could be D. I don't know. It could be anything that you like, a symbol. And so, the, what is 2 plus 2 is the state. So, like, what is the question, that's the question, right? Like, the question, the reward, for example, if you choose the one, if you choose four, your reward is plus one. If you choose anything else, your reward might be negative infinity, zero, whatever number you like. You can come up with any number you like for reward. It doesn't have to be plus one. It can be plus 10. It can be plus 100, and you can do anything that you like. Um, you can also do distance-based scoring. For example, is, you know, is choosing the number five better than zero? So, that's a question. What, what do you guys think? Is choosing the number five better than zero, or is it worse?\n\nI guess better.\n\nYes. Okay. Better. So, what would you do for the reward then? Pretend the model outputs five for what is 2 plus 2?\n\nZero.\n\nZero. Someone said zero.\n\nOkay. Zero is fine because it's wrong.\n\nYes. Like, if you, the answer five is wrong. So, you should probably give a reward of zero. But is there a better answer?\n\nLess than one.\n\nOkay. Yes. Less than one. So, like, some sort of, like, maybe 0.8. I don't know. Right. So, correct. So, like, you could do, like, the answer divided by the correct answer, right? So, like, if it's five, you divide it by four. You could do some sort of Okay, no, that's wrong. It's five minus four divided by four. Um, so, like, something, some reward like that. Um, pretend if your, pretend if the model says a, what is a reward?\n\nMinus one. Okay. Or could be minus 10 because that's very bad, right? You, you should not output a letter. It should be some sort of number. So, that's how you design reward functions, right? You, you, we just design a reward function, shove this into, you know, take your reward function, it's, like, just if statements, shove this into a language model fine-tuning phase, and there we go, you have 03. Okay, well, you won't have 03, but, you know what I mean. Um, essentially, 03 is a collection of all these reward functions, right? So, like, for example, this, what is 2 plus 2 is one question. Remember, it doesn't have to be what is 2 plus 2, it is a general maths question, what is 10 + 20, what is 10 * 200 / 10, you know, whatever maths equation you ever want. And this function can take your question and convert it into a number. And 03 is just a collection of all of these reward functions.\n\nAnd so, the goal of RL is to make the good ones more good. You want the good rewards increase in value. So, for example, the four, you want the four to appear more, but you want the three to decrease. You know, you don't want the three to keep appearing in your answer, but you want the D and the B to be very, very, very heavily penalized. That's the goal of RL, right? So, like, we don't actually have the answer, right? Okay, this, this question is very easy. What is 2 plus 2, obviously, is four? Yes. Okay, that's very easy. But pretend you have some sort of complicated question, like, for example, how do I win the stock market? For example, let's dumb, dumb thing. Okay, how do I win the stock market? You don't know what actions you're going to take, right? Like, but the point is, you have the result, right? You have the result, like, profit or loss. But the question is, we don't know how do we get to the good profit. And so, the question is, how do we maximize good actions as much as possible and decrease bad actions as much as possible, and that is RL.\n\nAnd so, OpenAI released something called RHF in, uh, ChatGPT, and they, oh, actually, I think it was Instruct, but anyways, for ChatGPT, they showed that you just need some training data, you need some data, you interact with the agent, which is the language model, you then get some actions, which is your answer to the language model, you feed this into reward model, and then you get some reward, and then you keep it, you know, iteratively doing this step, and you'll finally get ChatGPT. So, remember the, the base model you start with, so, ChatGPT base, you convert this into ChatGPT4 via this method.\n\nTo expand on a bit, if you guys have heard of PPO, right? What is PPO? Um, essentially, PPO is just, you just expand the box for the agent, right? So, the language model is, like, an agent, you expand it, and there's just three models inside of it. Um, there is a generating policy, the reference policy, and then there's a value model. Um, and that's all, not that special. Um, we will talk about each of these things separately. Um, but PPO is just a optimization algorithm to make RAHF work better.\n\nGRPO, which is the algorithm behind DeepSeek-R1, smartly deletes one of the fact, uh, deletes one of the things, the value model. It just gets rid of it. Um, now, why would you delete it? We will talk about this, but the trick is, if you delete a model, the value model, you just save parameters, you save compute, and it's much more efficient, right? Remember, each of these models is kind of like a large language model, right? So, like, you know, pretend you're generating, your model is, like, already 1.4 trillion parameters, what are you going to do, make another 1.4 four trillion parameter model for the value model? So, we just get rid of it, delete it. Um, and that is GPO. That's the only difference between Okay. Well, there's other differences, but the biggest difference is we get rid of the value model. Any questions?\n\nYes.\n\nUm, so, you talked about negative reward.\n\nYes.\n\nIt's confusing because in pre-training, isn't probabilities?\n\nWell, it's, like, negative rewards, right? And then, I guess, the phrase reward when it can be positive or negative versus comparing it to pre-training where it's always negative.\n\nDo you mean pre-training as in, like, the negative log likelihood or, like, some prob? So, the, when during pre-training, the goal is to maximize probability.\n\nYeah.\n\nSo, like, you know, you output a some number from 0 to one, the probability of the next word, and you want to maximize that. For RL, you want to maximize reward. So, if it's a negative reward, you still want to maximize that. So, if it's negative 1, you just want to make this negative one go away, and you want to be positive in the positive range. But also, rewards can actually be just negative, right? So, for example, if your, your reward function can be -10 and negative 1. The good one is negative 1, the bad one is -10. Your goal is to move towards negative 1 as much as possible because your goal is to maximize it. So, I would say the reward is a misnomer. You could just add 10 to everything, and then, you know, the sc, it scales the numbers away. Um, does that kind of make sense, or is it a question of nomenclature? I think, yeah, most people, I would say, like, to be honest, they don't like negative rewards actually in the RL space, people just like to do positive reward. I don't know, I like negative rewards. Um, I, I feel like it's more, for me, it's more intuitive. Um, yeah, any other questions?\n\nYeah. Yeah.\n\nSo, you've got your language model, and then you've got generating policy and reference policy, right? What, what models are being used there? Is it the same as your language model, or is that another model trick?\n\nYes, very good question. There are some tricks that you can employ. Most people just make them at the same model. The reference model is, like, the beginning of the model. The generating model is the model that you're updating. So, like, essentially, the reference model is, like, the base model. Okay, that's probably not a good Okay, fine. Just keep it as the base model. The base model, the generating policy is actually the model where you update it. So, like, the model that you update, the, the, the base models update. So, every single time you get a base model, you update one, update two, update three, that is the\n\n\nGenerating policy, but we will talk about this.\nSo it's essentially the same model, but there is updates to the model.\nSo the reference policy is the model that is not updated.\nThe generating policy is the model that is actually updated, and they're both the same model.\nSo the language model, one of them's updated, the other one's not updated.\nBut we will talk about that.\nUm, yes.\n\nSo the actions, is it typically one token or is it more tokens?\nFull?\nThat's a good question.\nIn the Pac-Man case, the action will be a string of actions, right?\nSo like you can go up, then down, then left or right, you know, some sort of like long history.\nFor in the language model space, we just generally, this is called single turn and multi-turn.\nGenerally speaking, currently, single turn is what most people do.\nIt's just one action.\nSo the action will be, um, the action essentially is saying what is 2 plus two, and the answer is four, and that is the action.\nThe action is actually the inference space.\nSo like what is the actual chain of thought that is the action?\nUm, and so like it's just one though, but it is the total sum of the chain of thought.\nSo like if you have like what is 2 plus two, I think the answer is four, you know, let me do some working out, blah, blah, blah, blah, blah, blah, that is the entire thing.\nIf does that kind of make sense?\nOkay, yes.\n\nThere was a clawed conversation around how like to finish a poem's next.\nThey have to like think ahead to the last letter, to the last word, to match the previous one.\nDo you think like reward focus next allows you to do that?\nYou could understand.\nI think for pre-training specifically, there are research papers which show that actually pre-training doesn't just predict the next word, it does try to predict many words ahead.\nAnd so like yes, maybe reward model in Reinforcement Learning, you can see you can accent it, essentially accentuates a pre-training behavior.\nSo maybe these behavior already exists in the model, we just see it more often.\nSo maybe I would say that, I would say that the model itself, it already has this capability.\nWe just want to make it more obvious.\nAnd so maybe the model already knows how to do that.\nIt already knows it predicts 10 words ahead or 20 words ahead.\nIt already knows how to do that, but we just want to make them more obvious.\nI'm not sure that answers.\nWould it be safe to say like, so if it's a generation, you want like every last, you want almost the circuit for the last word?\nOkay.\nI think for Reinforcement Learning specifically, yeah, I guess yes.\nUm, I think it essentially your goal is to maximize a reward.\nAnd so like however, whatever way you try to get there, it's different from generally pre-training.\nPre-training is just maximizing next the probability of the next word.\nBut Reinforcement Learning is you're trying to maximize reward.\nThe question is how do you actually maximize a reward?\nDo you like do chain of thought?\nDo you do what you describe like thinking about the next, you know, in the future or something?\nI don't know.\nLike I mean the question is like what is the reward function actually doing?\nI don't know.\nWhat is the language model actually doing?\nI don't know.\nUm, I don't know if that answers your question, like it's to be.\nYeah.\nYes.\nYeah.\nI was, I was curious about when you were talking about, you know, the arithmetic, whether five is a better answer than say yes or something, and like given that there are these like closed circuits between all these different related, you know, mathematical functions you can do on numbers in space, like whether it is like in the literature or in like the current state of the art, better to train it so that a closer pred is more accurate or whether just saying like the right answer is right and everything else is wrong, which in some logical sense is true, like tends to produce more performance in that space.\nYes, you're correct.\nYou should have data which is like getting more accurate data.\nIs that what you're trying to say?\nLike you should have data which is like, for example, what is two plus two?\nYou should get more data which is like four.\nIt shouldn't be like five or 10 or minus 100.\nWell like saying that like the practice of saying that five is better than 10.\nYes.\nLike in general, is that what people do in produce tend to say like when there is like exactly one correct answer and everything else in a mathematical sense is equally wrong because it's not that answer?\nThat is a good question.\nI don't know.\nUm, I think large model labs won't tell you exactly what they do.\nIn our experiments, when you use our notebooks, we actually show that if you do distance-based, so the closer your number is to the actual number, you will get better results, but generally speaking, it's easier to just say five is wrong, just give it zero reward, everything is zero, and then the good one is like one.\nIt's actually much easier to do.\nUm, for example, if you want to do execution of code, how do you actually do distance-based scoring, right?\nSo like if you ask it to create a Flappy Bird game, you just have the final output, but you don't actually know how to verify like, you know, oh, is this Flappy Bird game game better than the previous Flappy Bird game?\nIt's only in a mathematical sense you can like do distance-based scoring.\nUm, I'm assuming large model labs, they probably do the 01 better.\nLike the majority of them just do like yes or no, yes or no, binary.\nUm, but in our experiments for math specifically, you should do distance-based scoring.\nUm, it makes the model learn faster.\nYeah.\nFor verifiable domain like math, is it actuallyable because 2 plus 2 makes sense, but it's not going to scale for like really large numbers or large multiplications?\nSo are we going to end up, is it endame using tool use to calculate that or model could potentially be changed to that?\nThat is a good question.\nIn the olden days, before this paradigm came along, we would think that you can just use a tool like a calculator to, you should actually, I would say you should still use a calculator to calculate 2 plus 2, right?\nYou should not use a language model, but with RL, you know, VR, the trick is we actually found that actually, wait a second, if you just do 2 plus 2 or you do another question like 10 * 10 or you do some sort of complicated mathematical expression, you know, like the derivative of x^2 or something, I don't know, like some random mathematical equation, it randomly learns to actually solve that equation without actually doing overfitting.\nAnd so like I would say that with RL, you can actually make the model actually learn how to do multiplication, how to do addition.\nSo it's actually in the model.\nUm, does that kind of make sense?\nYeah.\nWould we use this in production or?\nOh yes.\nYeah.\nPeople use that in production.\nI Okay, maybe don't use it in production.\nYou know, you're not sure if the answer is correct.\nMaybe it will say 3 + 3 is 7.\nOkay.\nI don't know.\nIt's possible.\nSo like essentially, but it's getting better.\nUm, maybe in the future, all of mathematical equations can just be done by a model.\nUm, I think in the maybe a few months ago, maybe like DeepSeek-R1, you know, before 01 got released, actually not even 01, a few months ago, people would still say use a calculator, you know, like some sort of tool calling.\nYes, you should probably still do it.\nUm, but imagine, you know, as time goes on, as models get better and better and better in terms of like training data, just just for the max equation, you know, 2 plus two.\nUm, imagine in the limit as we get all of the world's data for just this maths question, right?\n2 plus 2, 4 plus 4, 10 x 10 or whatever, it should in theory solve them all in theory.\nUm, yes, the it's always in theory.\nUm, but yes, you don't need a tool calling.\nIt's not necessary.\nUm, yeah.\nYeah.\nYes.\nMy question is about the reward model.\nIn practice, are people using large language models as a reward model?\nGood question.\nOr is it?\nYes, I actually.\nGood question.\nOh, I was going to go in the next next slides.\nWe'll be talking about that.\nUm, yes.\nMulti-turn?\nDoes this change with multi-turn?\nI mean, you showed a single multi.\nYes, you could do multi-turn.\nIt's a bit more complicated.\nUm, you just imagine.\nThere are tricks you can do.\nYou imagine that your current step is good.\nImagine it and then you just continue doing inference.\nYou you append like your next question.\nLike for example, how am I going to, what is 2 plus 2?\nYou say okay, let me think about this question.\nWhat is 2 plus 2?\nBlah, blah, blah, blah, blah, blah, blah.\nThe answer is four.\nAnd then the question is what is your next question?\nMaybe the user interacts with it and says okay, I I don't think your question is correct.\nOh, I don't think your answer is correct.\nAnd then the model says oh okay, let me rethink about this, blah, blah, blah, blah, blah, blah.\nI still think the answer is four.\nUm, so you could chain this all together and shove it into the, you know, the whole RL step.\nYou could do that.\nUm, it's a bit more complicated.\nI think the diagram will be a little bit more different.\nUm, the follow that to that would be, you assume like a loop is a single turn or a loop is a lot of turns and then you only give a reward at the end or you give like subrewards.\nVery good question.\nSo there is in the DeepSeek-R1 paper, you could do subrewards or you could just do the reward at the very end.\nI think subrewards might actually do better in general, but the question is subrewards is very hard to calculate.\nYou would rather just wait, you know, all until the very, very end and just give a reward.\nThat's probably the easiest.\nSo it's more about efficiency.\nIt's all to be honest, all of AI is about efficiency.\nWhat is more efficient?\nWhat is more, it's all optimization.\nUm, so the answer is like I would suggest people just to like shove a reward at the very end.\nUm, yes.\nOnce you get your reward signal, is it just the reinforce algorithm with the gradient to go back?\nWe will talk about that, yes.\nYes, yes, most yes, correct.\nYes, so we will talk about reinforce, we'll talk about PPO and stuff like that.\nUm, yes.\nThis one.\nThe problem is if you skip from pre-training to the RLVA st RLVR stage, it's relatively hard because your model doesn't actually know how to do instructions, right?\nSo like you have this base model, you ask the question to the base model, what is 2 plus 2?\nIt's not going to say I think the answer is four.\nIt might, you might be lucky somewhere in your pre-training data, somewhere on the web, someone asked this question, what is 2 plus two, and then the, you know, the the answer was like, oh, the answer is four, but you have to be lucky.\nUm, so like, so the problem of this is the whole trick of SFT is you want to force the model to answer what is 2 plus 2 instruction way, right?\nSo it will tell the model what is 2 plus 2, you want it to say the answer is four, you don't want it to like blabber on and like j like get some Wikipedia article and shove it as the output.\nSo the whole point of SFT, preference fine-tuning and stuff like that is to make the model forced to make it more optimal to like output conversation style.\nUm, if you want to skip, it's also fine.\nIt's just not efficient.\nUm, because like you, you could do this.\nUm, I'm assuming large model labs are trying to do this.\nUm, so it's not like a you should or you shouldn't.\nThey are trying.\nUm, does that okay there?\nYes.\nQuestion.\nCouple of questions.\nOne question is it policy optimizer?\nModel after?\nThe reference model does not change.\nSo the reference model is just a model that you didn't train.\nUm, it's like the, it's like the, it's like the base model or like the SFT, whatever checkpoint you started with.\nIt doesn't change.\nYou could change it.\nI think that would be too expensive though.\nI think if you change it, that'll be more complicated.\nRemember all of AI is about optimization and efficiency.\nSo I feel like you don't have to.\nYou could, I, I don't know if there are papers talking about it though.\nU maybe OpenAI does it.\nI don't know.\nUm, other question is that do we need less of?\nSo the trick of RL is you just need a reward function.\nYou need to make that and you don't need data.\nYou don't need the answer of the data.\nOh, actually you do need the answer.\nYou don't need the chain of thought.\nYou just need lots of questions like what is 2 plus 2?\nWhat is 4 plus 4?\nRemember you can actually automatically generate this.\nRight?\nSo like, number of samples do you need less number of samples compared to?\nYou should do as much as possible.\nYou most large language models, I think for like, you know, 03 or 01, I don't know what is a percentage of compute, maybe they spend like 5% or less, but the goal is what happens if you spend double the compute just on RL, right?\nSo like previously if you do 14 trillion tokens on pre-training, make RL 14 trillion tokens, and then the goal of large large labs is to just do that.\nSo currently it's very less, but over time it will increase.\nBut compared, let's say the number of samples will be much less currently.\nYes.\nBecause it's expensive, like you need models.\nCorrect.\nIt's expensive, but over time I think like maybe by next year or like this year, large model labs, their goal is to do this phase the most.\nThat's their goal because remember you can automatically generate questions now.\nWhat is 2 plus 2?\nWhat is two 2 * 2?\nWhat is 10 divided by 10?\nI don't know.\nGenerate as many math questions as you like.\nBut remember you can also generate, you know, like coding questions.\nYou can generate any questions that you like or you can use the supervised fine-tuning data itself for the RL step.\nYou can do that as well.\nUm, does that kind of make sense?\nOkay.\nAny Okay.\nHow do you protect the SF from being like screwed up by?\nOh, good.\nYes.\nWe won't talk about it.\nLike you have 2 plus 2 is also a good answer.\n2 plus 2.\nCorrect.\nBut I don't want more equations.\nI want the answer.\nVery good.\nSo is there like techniques to make sure that we're not violating our instruction?\nYes.\nWe will also talk about that, clipping and stuff like that.\nYes.\nYes.\nIs there any research done in the model?\nSo for example, you know, there's a circuit that says yes.\nBut can you incentivizing the model?\nAddition the concept in general?\nYou that is a very good question.\nI don't know.\nI think that's like the during the pre-training phase, essentially somewhere, somewhere in the internet, someone wrote what is 2 plus 2 somewhere, and then somehow maybe someone did a formulation, you know, some sort of derivation of like what is 2 plus 2.\nOkay, I don't think anyone has done pretend there is some derivation of like some complicated maths equation and so the model somehow learn.\n\n\nEd to predict all of that entire trace, and if it keeps seeing this, it would like accentuate the fact, \"Oh, okay, I've seen this before. Let's make this even more, um, more prevalent in the model.\" So somewhere in the model, it has learned 2 + 2 is equal to four somewhere. Yes, I guess what I'm saying is, so we can use like the stepific or super good reward, you can you're weighting it.\n\nCorrect. So there were actually two schools of thought. The first one is the model already has this knowledge, right? It already knows what is 2 + 2, and you're just Reinforcement Learning just tries to like maximize what if it sees 2 + 2 is equal to four. It tries to weight this factor more, weight this circuit in the model more. So the model already learns, learns, but then the second school of thought is like, \"Okay, maybe the model doesn't know, actually, it actually learns a new thing.\" Um, I'm more in the first one. The model probably already learns. It already knows it, and we're just maximizing the, you're just trying to make it more accentuated. Um, exactly. So the extent of my question is, basically, I want, I'm wondering if you can steer, maybe there's like, you can steer the model away from the circuit to the always do the addition circuit, if that makes sense.\n\nYou mean like just do addition?\n\nYes. You could, I guess what you could do is like get the language model, see which weights are changed during the Reinforcement Learning phase, which weights are changed, and you just give it what is 2 + 2, what is 2 + 2, what is 2 + 2. You just keep doing this question, and you can see which of the weights are changing, and essentially, you can extract this from the model. You could, I don't know if there's research about this, but I'm just making, I'm just making stuff up on the spot. You could do that, if that makes sense. Oh, maybe that's a research question someone should do a research paper.\n\nAny other? Yes.\n\nGo back.\n\nYes, we will also talk about that. Yes. Yes. Yes. Okay. Okay. Yeah. Sorry.\n\nDo we expect usually changes all the parameters in the model set as a subset?\n\nYou, you could. There's like two large model labs will most likely change all of the parameters. Every single parameter is changed. Um, but there are papers which show that actually not all of the parameters are actually changing that much. Some of them are changed by like zero. Like the majority of updates to the model is like zero, and only some very small updates to the model are seen. And so that's kind of like a circuit idea, where like the model already knows how to do whatever question you give it. You just most of the updates are like zero. Um, any say which one is better? Is it better to aim for changing all the parameters?\n\nYou can also do like lower, you know, parameter efficient fine-tuning. You can do other things. You don't have to fine-tune every single thing. Um, but I think the majority of large language model labs, they just do everything. Um, otherwise, this again becomes an optimization problem, you know, what do, which layer do we select and stuff like that. So it gets more complicated, but yes, you could do parameter efficient fight. Actually, we're going to show a notebook for that, so you can actually do it on your own computer. Um, yeah, was yes, one more question, one more. Yes.\n\nOh, okay.\n\nCapabil...\n\nGood question for the K divergence term. Do you mean like removing it? Would that make it better?\n\nI was saying...\n\nIt... because the whole point of the K divergence term is to like not make it too stray away from the supervised model.\n\nOkay, maybe I'm not kept up to date with with research papers, but anyways, I... So your, your point was like, if we remove the KL term, it will be better. It learns new capabilities.\n\nI wanted to understand.\n\nDo you mean like, do you want to have more capabilities into the model?\n\nAre there strategies for, uh, strategies?\n\nHard to say. I'm assuming the large model labs will probably know strategies. We will show, like, I, I will show examples of how to like make Reinforcement Learning better, like how to like reach higher reward faster. I'm not sure about new capabilities. It's actually very hard. It's actually very, very, very, very hard to elicit new capabilities in the model. Um, the question is like, is this new or not new? I think that's a question, like, is this actually part of the model or not part of the model? Um, and most research papers are like hand wavy. They say, \"Oh, most updates are sparse,\" you know, like, so most likely it's not, you know, new capabilities, but what happens if, you know, one year later, all of the model updates are like not sparse? Is this considered new capability? I don't know, like, you know, those are the questions. It's more like, I don't know if that answers your question. I probably didn't answer your question, but maybe, maybe the other parts of the talk maybe might answer some part. Um, yeah, okay, I will keep going on more questions later. Um, okay, so like the reward model, right, was actually a language model that, like, some sort of model, some neuronet network, some AI model that predicts the reward. In RLVR, we delete this entirely, and we just call it the reward function. So like the ground truth reward, you know, if it's correct, you plus one, if it's bad, it's just zero, right? So you essentially delete another part. GPO essentially deletes another part, right? So like you remove, remember GPO, you delete the value model, get totally remove it, and then you delete the reward model, and it's just a reward function.\n\nAnd yes, as a reward model, you could use an LLM as a judge. So you could ask a language model itself to say, \"Is the answer good or bad?\" You could do that. You could do regular expression check, you know, like, \"Is the formatting of the answer good or bad? Is the maths equation good or bad? You know, is the final output good or bad?\" You can do distance scoring and stuff like that. You can also execute the Python code, and then you can see if it actually executed, right? So like, is there like import errors or like format errors or like some sort of Python error, and you can use this as a reward. And so this blue box, the reward can be anything that you like. It just needs to output a number, you know, minus one, plus one, I don't know. It just has to be a number. In fact, you can make a dumb reward, just everything. It just does random, you know, plus one 50% of the time, minus one 50% of the time, I don't know. And confusingly, a paper recently showed that actually random rewards works. Um, so like, yes, go ahead. You can try it. Um, but also, why did someone say why? Um, probably read the paper, but, but to be honest, actually, I think the paper might be a bit... I don't actually believe there was... yes, there was an update showing that actually this was wrong. Um, that actually it's because the model, they don't... the benchmarks are incorrect. So when you say that you actually increase accuracy, like from 20% to 50%, but actually the model itself was already 50%. They just didn't check the accuracy of the correct model before. Um, so there was a recent rebuttal to those types of papers. Um, but, you know, interesting results. Um, yeah, I don't know.\n\nOkay, so remember in Reinforcement Learning, the goal is you don't know the best action to take in the space, right? When you're doing Pac-Man, I don't know if going left, right, up, or down is the best. I don't know. But at the very, very, very end, you will either, you know, win or like, you know, get some reward or you will die. Yes. Um, but the goal of Reinforcement Learning is to maximize the best action you can ever take, right? So like, what is a better action than all of the other bad actions? So Reinforcement Learning just tries to maximize the best, well, not the best action, the better action.\n\nNormal pre-training, you already know what is the best answer. So like, you already know what is the next word, right? You, if you want to predict, you know, \"Hello, my name is Daniel,\" you already know the next word is going to be Daniel, right? So you already know it, but Reinforcement Learning, you don't know in advance what is the actual correct reward. Um, so you, the only thing you can do in Reinforcement Learning is to maximize the, you know, one of the better options.\n\nAnd so, yes, okay, now more maths. Um, the goal is to maximize this equation. Um, that's the goal of Reinforcement Learning. So what is this equation? The J is like the total gradient. Um, well, actually, it's, it's more like the, it's more like we want to maximize this. It's not actually the total gradient. Okay, maybe I miswrote that. Anyways, didn't write that. Um, the, we want to calculate the gradient with respect to the policy language model, and the action is given a state, and the r is a reward. If you want to write this down in like English, it's like, we want to take the derivative of the log probability of the action given the state times the reward. Now, I don't know if you guys understand what that means, but I did like an example, um, Pac-Man case. Okay, so you are Pac-Man. The red is your enemy. You don't want to go there, right? So like, you definitely don't want to go to the red thing, right? But you want to eat the two gray dots, right? That's, remember, you can only go up, down, left, or right, right? You only have four actions. Remember the action space is just up, down, left, or right. So if you do rewards, I just randomly made some rewards up. If you go to the red thing, you will get minus 10 reward. Or actually, it should be minus infinity. You die. But anyways, minus 10. If you eat the gray dots, you get plus one or plus one. And if you go up, it's just zero reward. There's nothing there.\n\nNow, when you get this language model or like some sort of model, it has to tell you what is the next action, right? It tells you what to do to the next action. For now, we'll just assign every single action, up, down, left, or right, as one quarter probability, right? So like, you go up 25% of the time, left 25% of the time, and so on. So these are your numbers, right? This is the entire state.\n\nSo the goal of Reinforcement Learning is you want to do that red going towards, um, the right. You want to do this. You want to go towards the right less. You want to do a much less right. So like, you want to push the probability of the 0.25, 25 of the right much less, and you want to go bot, like, you know, down and left much more, right? So you want to push the probabilities much more, and the top are not really that important. And so Reinforcement Learning, essentially, you, your goal is to avoid doing the bad thing, and you want to do the good thing much more. That's kind of Reinforcement Learning. If you convert this into a table, right? You have the probability of the action given the state, right? Remember, up, down, left, or right, we just assigned 25% % chance, right? Just, just pretend 25% chance. The reward, which we can calculate, right? We calculated this. We calculated the reward. We just made some numbers up, right? As 0, 1, 1, -10.\n\nThe probability times the reward, we get some numbers, right? So like 0, 0.25, 0.5, minus 2.5. And then if you take the log of the probability times the reward, you get some number, right? So like 0, minus 0.6, minus 0.6, and 6.02.\n\nSo from this table, does anyone know which row do we want to maximize? What is the goal? Like, what do we want to maximize?\n\nWhich row?\n\nYou want to maximize the bottom row.\n\nWhat is the reward of the bottom row?\n\nCorrect. You want to minimize the bottom row. Remember the reward is minus 10. We do not want to maximize the last row because the last row is the worst. And so that means the 6.02. We want to actually decrease this number dramatically, right? That's way too large. We want to decrease it. The other rows we want to maximize. And so the goal is, okay, we just take the sum of all of that, right? We take the sum of the four numbers, and it's 4.8.\n\nAnd so remember, okay, let's try, right? So like, by hand, by hand, we shall remember the right, remember all the probabilities are one quarter, 0.25. By hand, we shall do the bad action even more, right? We actually do the worst thing. What happens to the, what happens? Right? So like the reward, the probability times the reward is now minus 4. It used to be minus 2.5. And so the reward, the log probabilities times the reward, the sum actually decreased, right? It decreased to 2.58. Before it was 4.81.\n\nIs 2.58 smaller or bigger than 4.81?\n\nObviously smaller. So actually, this is worse. You should not do this. But this is actually bad. Remember the goal is to maximize, maximize this equation. Maximize it, right? Maximize. And so 4.81 is actually better. The original state is actually better than the 2.58. So this, the thing that we just did is worse. So do not do this. However, let's do the right thing less. Right? Let's not go to the right and actually maximize the rest.\n\nYou shall see that if you do the log probability times reward, sum of them all, you will get 8.9, which is a larger number. And so the goal is to maximize this as much as possible. You could say, \"Wait, we know the answer, right? You should go towards the right, just make this 100% probability, let's just, you know, and you'll have an infinite reward.\" Okay, okay, not infinite, but you will get maximum reward, right? Why don't we just do that? But you should not do that because you're actually forced, if you do this, your model will be like, \"Learn, oh, okay, let's just keep going right, let's keep going right,\" and they just get stuck, and it just becomes very bad for optimization. So definitely don't do that.\n\nNow, there are someone who's who talked reinforce. Um, we don't just multiply the reward, but remember this equation we did, where is it? The probability of the action given the state times the reward, we don't actually multiply the reward. We should not do that. Um, you actually multiply by something called the advantage. Um, and what is the advantage? The advantage is a reward minus the average reward, the base reward. So you shouldn't actually just see you want to maximize reward, you want to actually maximize a reward, but also looking at the average reward across the entire model. So it's called the baseline.\n\nAnd so this B, this baseline model is what is the value function, the value model. Remember GPO deletes the value model. This was the value model, and this value model essentially estimates what is the average reward if we just see the current state. It does not take, it does not look at, like, you know, what is the next, uh, next step. It doesn't look at, you know, what is the next action. It just takes a snapshot of what you currently... so essentially, it looks at this, it looks at this and just guesses what is a reward, right? It doesn't, you, you're not supposed to give it the rewards. You're not supposed to give a minus 10, +1, +1, or zero. It just looks at the current state and produces a number. And this number is called the average reward.\n\nAnd so the...\n\n\nThe goal is, now we don't actually want to maximize this, you know, just a reward. We want to maximize the advantage as well. Um, so like we multiply all this together, and the goal is we want to maximize this new equation. Does anyone have any questions? There's lots of baths, but questions. Yeah. Yes.\n\nSo in terms of probability, how do you... is the large language modeling an estimated probability, or this is a known probability of all the possible states? But how do you get that in the practice world?\n\nSo large... so a model, a large language model predicts the next word. So for example, you take the entire Wikipedia and then you like chunk it into small little tokens, and then the output is just what is the next word. So for example, \"My name is Daniel.\" But it could also be \"My name is Michael,\" \"My name is Bob,\" \"My name is whatever, whatever.\" Right? You have all of these probabilities for every single word in the entire language possible, like 128,000 words. You assign a probability for every single one, probability based on the token.\n\nYes, correct.\n\nBut there'll be... the trick of this for language models is you can utilize the probabilities directly. That's the trick. And so like that essentially makes everything easier.\n\nAny other question?\n\nYes. Yes. Yes. Yes. Correct. Correct. Yes.\n\nWhat about multimodal?\n\nOh, multimodal models. Do you mean like doing Reinforcement Learning (RL) multimodal models? Oh, that is more harder. I would say you... you could... you could look at the Sudoku puzzle, just convert the text model into a vision. Just cheat. I guess you could do that. You could like say, \"Oh, you know, I guess you could give it the Pac-Man, you know, give it the Pac-Man thing and tell the model what should I do next?\" You... you could... I mean, vision is... it's kind of the same thing, but it's more...\n\nDoes 03 do vision plus Reinforcement Learning? I... I think it does. Um, yes, you could. I think for open source, I don't think I've seen... Yeah, I don't think... Yeah, I don't think I've seen open source models do that very well. Um, it is still very hard. Um, yeah. Any other questions?\n\nNo. No. Okay.\n\nDid someone... did ask a question? Oh. Oh, so... yes. So, sorry.\n\nOh, what is the B? What is the base model? Average, but like, what is the average reference model for what?\n\nSo we just... like your goal is to... your goal is you see this current state of the model, like whatever the environment currently looks like, and you just want to produce a number that approximates what is the total average reward. Okay, I'll give you an example. Pretend you're playing chess or like go, or I remember AlphaGo. You look at the board, the current state of the board, and just say, \"What is the probability of the white player winning? What is it?\" You're not supposed to do any prediction. You just have to predict what is the probability of the white player winning by just looking at the board. That's kind of the average reward.\n\nAlways low.\n\nIt's always low. Yes. Yes. Correct. But remember at the very, very, very end phases, like, you know, you might get higher reward, but that's the goal. You want to... you essentially want to predict what is the probability always. You know, for example, in chess, I'm sure there are like some steps you can take to make the reward higher. The question is, when the model sees this, you need to essentially... you need to say, \"Is this board better than the previous boards?\" And so this model, you have to train as well. You have to train this model. It needs to output a probability of it winning. That's for the chess example. Does that kind of make sense or no?\n\nYes.\n\nThe value... No, no, no. The value model is totally different. It's a... there's three models. There is a value model which predicts the average reward of the state. The reference model is just the model that you started with, and then the policy or the actual model that you're changing is the out... the final result of your model, like the actual chat model. So there's actually three models.\n\nThis one, the B... you will see the current state. You will see the current state, and then you will see, \"Okay, what is the actual...\" I think you do use the policy... no, you... no, you just look at the current state, you look at the current state, and then you output a probability of whether this chess board is good or bad... of some like 0.8%, 8% you're going to win. Um, okay. Yeah, something like that. Yes.\n\nOh, yes. Yes. Yeah, we'll talk about that. This is just a general, simpler formula. Yes, we'll talk about that.\n\nI guess my question...\n\nOh, you can ask. Yeah, go.\n\nI guess because the policy is predicting...\n\nThat's the probability.\n\nYes.\n\nThat is a good question, and that is an active era of research because you could either normalize by all of tokens or the entire just one turn. Remains to be seen which one's better. Um, it's actually still people talking about that because... correct. Yes. So generally speaking, normally people just assume this, assume this roll out is correct, assume this chain of thought is correct, and they just do the very end. But then you do have to multiply probab... I don't... yeah, you do have to multiply probabilities. So there is a multiplication somewhere, you will get very small. Yes. Um, but you know, you get very small, but the numbers are relative, right? So everything is very small, but then the smaller one, the bigger ones are still very small, but it's still better. So they're all relative.\n\nAny... was there one more? Yes.\n\nYes.\n\nOh, no, no, it's very old. Yes. Very, very old. Yes.\n\nI wonder if you can give some like advice on like how to think about this training or abstract level about like error propagation between like if you have a trained model which does the scoring or does the value function or whatever... that itself is trained from data. It has some like error margin, and that, you know, you have some soft max function, for example, that like only one in a 100 times will produce the wrong that probability.\n\nYes. Like how do we think about the like development over time of these models and like to what extent that error propagation is something that you can observe and measure, like systematize and engineer around? Like I don't really understand like what the mindset is in this process right now that...\n\nIn my view, I think all of these formulas are just made up, and so like the goal is to maximize reward, but the question is, you need to like... you can't just maximize reward because otherwise you might make the model really silly. Like you might say, \"Okay, what is 2 plus two?\" It just says four. Pretend your data set was just \"What is 2 plus 2?\" right? So like you literally just cheat. \"What is 2 plus 2? What is 2 plus 2? What is 2 plus 2?\" Just make the model just say four, four. Just four forever. Do you want this as a model? Definitely not. So like we want it needs to learn. Okay, if I give it the next question, \"What is 8 plus 8?\" It should not just say four, or \"What is 2 minus 2?\" It shouldn't say four. And so the goal of all these algorithms is to somehow force the models not to like overfit to your question. And so like these formulations are trying to do these things to like not overfit.\n\nYeah. Well, I'm thinking about like the chess you were saying, which scores the board and produces this like number... a number which is like this good or bad. Sometimes these well-trained models have these novelties that, you know, where they say, like, \"Make this move.\" It's like, not the new state is not obviously good or whatever, but they somehow like figured out this.\n\nYes.\n\nAnd suppose that your training mechanism for the value function model, you know, hasn't picked up on something like that. In fact, there's like some error in the tendency of the value model.\n\nOkay.\n\nLike it's probability of producing like... perfect scoring of the board.\n\nYes.\n\nYou know, is not always exactly right.\n\nYes. Always not into your training process.\n\nCorrect.\n\nYes.\n\nHow do you think about that? Like how do you... like what is the... the value model? You have to train it together. So it's like a combination of the entire algorithm. So the value model predicts what is the probability, but you actually have to train this as well. And so that is actually the problem. Some people... you could train this... you could train this separately. You know, you can like get all the chess possibilities and then output what is the final number. I think that's what some people do. You could train this in tandem with the model actually. I think that's actually more harder. I don't know if that... you... so there is always error in the value model, always. But you have to train this model as well. So you will reduce the error, but there's always error. So I think there's like some numbers you can like force the value model to be like less, less prominent. Like don't forcibly utilize the reward, uh, the value function, but in Japia, we just get rid of the reward model, uh, the value model anyways. Um, so totally gone. Um, no, so you don't need to worry about that anymore. Um, okay, I will keep going on. Let me just check time actually. Okay.\n\nSo remember the goal is the advantage. We want to maximize advantage, not reward anymore. Advantage is the reward minus... minus the average reward or the base... the base reward, right? If the advantage is less than zero, it means that it is worse than average. If the advantage is more than zero, it means it is better than average. And so the goal is we want to do the action more if it's better than average on general.\n\nNow to Proximal Policy Optimization (PPO), right? So like, I don't know if you guys have seen the PPO formula, it is ugly, but this is the PPO formula, right? So like it looks... it's more confusing because there's like a clip and then there's epsilon and blah, blah, blah, whatever. But we could just strip everything away. It's just... it's just the probability of the action given the state times advantage, right? We literally just discussed about this. Okay, minus a log. Okay, the log's gone. But anyways, it's just that, and then the rest is... the rest is trying to reduce overfitting.\n\nAnd so remember, so essentially this... there's a thing called the division of the old model. And essentially it's the model that created the action. And the goal is we now want to maximize this likelihood ratio. We don't just want to maximize the probability of the model. We don't want to maximize the probability of the action given the highest reward. We actually want to maximize the likelihood instead. But what is this likelihood?\n\nSo I did some numbers. I just made some numbers up. So this is the pack. So pretend the... the top... the um, numerator is 0.01 and the denominator is 0.01. 0.01 divided by 0.01 is 1. If the... if the top is 0.01 and the bottom is 0.99. Remember these are all probabilities. You divide the top from the bottom, you'll get 0.01. Again, if the top is 0.99 and the bottom is 0.01, you'll get 99 and so on. Right? The last one's one. And so the goal is 0.01 divided by 0.99 is 0.01. This means that the action that you do is actually very likely, right? Because the bottom... the bottom thing is 0.99. But we actually don't like this, right? Remember the top is 0.01. We do not like this. So the ratio is 0.01. And then the bottom is this. This action... the bottom, the denominator is 0.01. It is actually not likely. But we actually like this because the top number is 0.99. And so when you multi... when you do the division, you get 99. So this is actually good. And so actually we're not actually trying to maximize the probability. We're actually trying to maximize the likelihood now.\n\nAnd so the question is, why don't we just maximize the probability, right? The first equation. Why do we need to do this division thing? Because if we maximize just the top, you will have reward hacking. \"What is 2 plus two?\" It might say, \"To solve this question, we need to do blah, blah, blah, blah, blah, blah, and suddenly it says hello, hello, hello, hello, hello, hello, hello,\" and then it says four. Is this good? I don't think so. It's very good. We don't want it to say hello, hello, hello, something, or like some weird trace in the Reasoning model. It does something weird. We don't want this to happen. And so actually this... this, you know, hello, hello, hello, hello, is actually very not likely. And so the goal of the division is to reduce these issues.\n\nThe epsilon part is called the trust region. Essentially, we don't want to make... we don't want to do large steps for um, PPO, right? We don't want to do large steps, and the... the trick is we want to restrict them, right? So like you don't want to overfit the model. So now we restrict the model, and so epsilon could be like 0.2, 0.1, and the 1 minus epsilon is 0.8, 8, 1 + epsilon is 1.2. And the trick is we just want to not move the direction of the gradient that much, right? We don't trust the model that much. We don't trust the algorithm that much. So, we want to constrain it.\n\nAnd then also the PPO, there's also a K term. Um, there's another term. Um, and essentially what this does is we want the model to be as close to the supervised fine-tune model as much as possible. We want it to be... we don't want it to go so far away from the base model or the pre-tra... or the supervised fine-tuning model. So essentially, if it deviates too much, we want to tax the, uh, we want to tax it, and so this beta is like 0.05, and the Kullbackâ€“Leibler divergence is the dis... okay, it's not a distance, it's like the distance between the current model and the pre-trained model, and essentially we want to also shove this into the equation. Um, so you can see with PPOs, there's many moving parts. It's who cares about the equation? It's not that complicated. The point is all of these extra add-ons are just to reduce overfitting and not to make the model like randomly go to some weird state, um, that like, you know, overfits to like your questions. And so the trick of PPO is they just added all these terms in to make training more stable.\n\nAnd so the final equation is like this. This again, um, hopefully you will... to be honest, no one even counts the formula. It's not that important. Um, but I just try to like break it down into pieces. And the goal, remember the goal is to maximize this equation, right? We want to maximize it. And normally I just like to think about this one, right? You just need to learn this one, right? You want to maximize the probab... So it's just this equation. Um.\n\n\nRemember we did the table. Just this is enough. You don't need to learn the rest of the formulas. It's not very interesting. Um, yeah. Any questions? Yes. Yes, correct.\n\nSo the biggest problem is, pretend you're like, pretend you just started RL, like you, you have that pre, you have like the base model, you have like a supervised fine-tuned model, and then you do RL. The gradient updates at the very beginning are going to be gigantic, right? You're like, \"What is 2 plus two?\" It says four, but if it says five, you want to like penalize it dramatically. And so the problem is, you don't actually want to do large steps. And so the goal is, you want to constrain it. And so the constraint factor is, like, you know, if the, if the, if the num, if the gradient update is extremely large, you just want to like constrain all the numbers, if that makes sense. The goal is just to constrain the update, not to make it too large.\n\nWhat about the ratio?\n\nOh, the, the ratio, it's the KL divergence. Oh, sorry, not the K, the likelihood. To be honest, I think I need to do more research. I would ask Gemini exactly what it is. That's my answer. I'm probably not the best person to answer every single question. Yes. Any other questions? Yes. Yes.\n\nIt's the model that actually created the action. And so the top one is all of the numbers that actually, like, how do I explain this? The bottom one is the model that created the action. So, for example, you, the model says you want to go up, down, left, or right.\n\nIs there a correct or a...\n\nOh, we just created the action. Like, it could be anything. It could be the... So it's the m, it's the maximum. It's whatever action the model says currently. It might be wrong. It might be good. It might be bad. It's just any action.\n\nAny other questions? Okay. Yes. space.\n\nI don't think so can answer that question. I don't know. That's why I don't know. Maybe research papers show it. I, I'm not sure. Ed.\n\nOkay. Okay. Well, okay. So, GPO, the trick from PO is, we remember, remove the value model. We get rid of it entirely. We do not want to estimate the average reward. It's totally removed. Um, and the reward model is now removed as well for a reward function. So, we get, yeah, we remove it, right? Remember the value model is removed. B is a reward value model. We get rid of it entirely. But what do we replace? So, the trick of GPO is, we do roll out or inference sampling. We get the answer, \"What is 2 plus two?\" You literally make four inferences. You just literally call the model four times. It could say the answer is zero, the answer is one, the answer is two, or the answer is four. You can do, you do like, you just literally call the model four times, and you take the reward, right? Zero, the, \"What is 2 plus 2?\" The correct answer is four, so you want the last number to be one, but the rest is all zero. And the trick is, you literally just take the statistics of your current roll out. You take the statistics of all of this. You literally take the reward minus the mean divided by the standard deviation. You get the zed score. And this is your, this is your base model. This is your value model, right? This is, there's no more value model anymore. It's just a number.\n\nAnd so I did this on a table as well, right? \"What is 2 plus two?\" If you think it's zero, remember the prediction could be zero, one, two, or four. And your reward could be 000 or one. And if you take the mean or the average of all the rewards, you get 0.375. If you take the standard deviation, you take 0.43301. And then you do the reward minus the mean divided by the standard deviation, you get some numbers. Remember the number four is correct. That is why you know reward minus the mean divided by the standard deviation is 1.44. It's the largest number. And so that is why we need to like max, we need to essentially maximize that good answer, and we want to reduce the bad answers.\n\nBut why is it called group relative in GPO? Because it's not just one question. It's many questions. It could be, \"What is 2 plus 2?\" \"What is 4 plus 4?\" Okay, well, my graphs are all the same. My plots are all the same. But anyways, imagine there's like four different tables. \"What is 2 plus 2?\" \"What is 4 plus 4?\" \"How do I create this Python function?\" You know, whatever. And there will be four tables. And so the goal, group relative, just means you, for each question, we take the statistics within each group. For example, \"What is 2 plus 2?\" You create four. You literally call the model four times, and you get some, you know, answers. \"What is 4 plus 4?\" You call it four times, right? Create Python code, you call it four times.\n\nYes, there are other factors of G. So essentially, we already explained what GPO is, right? Everything you need to know about GRP, we already explained. In the total mathematical formula, it looks kind of like this. Um, there's some rearrangement. For example, the minus beta the KR divergence is just taken out of the reward function. That's the only other difference. Um, hopefully it makes more sense about the parts of the GPO formula and stuff like that. It's actually not that complicated to understand. The majority is just trying to reduce overfitting, right? That's the whole goal, right? Minus beta times a K divergence is to reduce overfitting. One minus epsilon, 1 plus epsilon is to reduce overfitting. The division reduces overfitting. Everything's reducing overfitting, right? So that's all of machine learning and AI. It's just to make the training more stable and to reduce overfitting.\n\nI would highly suggest, there is, these are the two things that I really highly suggest. Um, Nathan Lambert's policy gradients, um, book. It's online though. Very, very, very, very helpful. Um, and um, Yanick's video on GPO and stuff, very, very, very helpful as well. Um, yeah, and now I will go into a collab demonstration of GPO. Um, before that, like, does anyone have any questions? Let me just check time.\n\nQuestions. Yes. Because maybe the answer is there is to me whatever information. Yes. Memorization. You make more memorization.\n\nTo answer your question another way, I think it's actually because GRP itself is a problem. Remember the goal of all these algorithms is to force the model not to detract too much from the original model, right? With this like minus beta K divergence, you know, one minus epsilon, all of this is trying to make the model not go towards too far away from the original model. And I think that's the problem because you're essentially forcing the model not to go too far. And so maybe there might be some new algorithm, I don't know, something, some other formulation which, you know, you want to go very far away, you could do that. Um, I, I don't know if there are any, I don't know if there's any research papers about that, I, I don't know. But you could, yes, you could do that. Yes, any other questions? Yeah, yeah, yeah, you're probably saying...\n\nOkay. That's why. Yes. Yes. Energy-based models. Yes.\n\nSo what do you think about that?\n\nWhat do I think about it? I can't really comment. I mean, he definitely, you should listen to what he says. Um, but you don't see anything like movements in open source.\n\nI don't think so. I think open source, we kind of got captivated by RL, GPO. I don't think so open source people are doing whatever he's talking about. Unfortunately. I think he needs to talk about it more. Yeah. Japa. Yeah. And energy-based models. I, unfortunately, I don't think so. Open source. Yeah, maybe we should talk about it more, but yeah, I, I don't think so. Um, yeah. Yeah. Yes. Yes. Yes. You got rid of it. Yes, correct. Yes. group.\n\nSo this is more about an optimization question. So in theory, you should make the, you should make, for example, I just selected four, right? \"What is 2 plus two?\" Create four examples. You should do, you know, as many as you like, you know, 3,000, whatever number you like, you should do as much as possible, but remember, AI is about optimization. This is going to take forever, you know, it's all about efficiency, so probably don't do as many as you like. Um, but you should, in the limit, you should do that, but you know, everyone can't just wait there waiting for the computer to spin. Um, so yes, you, you should do as much as you like. Um, it's, but yes, also for like recommendations, when you do inference sampling, you should set, set temperatures, like, you know, 1.2, 1.5, set m to be 0.1, you know, something like that. If you set temperature to be zero, you'll have the same answer every single time. So definitely don't do that. But you should have high temperature numbers to make the model, you know, produce new output as much as possible, maximize variability.\n\nYes, distribution. You should try your best to maximize variability. Your outputs should not be all the same. Um, if it's all the same, I don't think so is going to learn. So, you should make it as different as possible. So, that's why you should set temperature to be 1.2, 1.5, whatever, some large number. Don't do too large though. Um, any other questions? Yes.\n\nYeah. I'm like a situation where like all are not...\n\nYes, correct. All the first steps are all the rewards are basically giving no signal at all of the underlying train.\n\nHow do you deal with that? I have faced that a lot of times with...\n\nMoving to a larger base model helps sometimes. Knowledge training data, like...\n\nYes, that's a good question. So essentially, you're saying the model, if the model starts off with like no reward, like every single update is like 000000, it's not going to do anything. Yes, that happens all the time. But by chance, just by chance, you know, you have like some small little, little probability, just by chance, you will have some reward. So that's the trick. You will see this after 10,000 inferences. \"What is 2 plus 2?\" Suddenly the model says four, suddenly. Okay, just suddenly, just by random probability. Let's make this small. That's all, that's all of GPO. Like this is addition, simple task, if it was like a proof of concept. It may never come with...\n\nYes, may never. But remember, you're not doing this one question. You're also shoving this together with other questions. \"What is 2 plus 2?\" \"What is 4 plus 4?\" \"What is, you know, derive the derivative of blah, do this Python function.\" This step is very large. You essentially shove this all together. And the trick is, in general, it works. In general, maybe by bad luck, it might not work, but I feel like the bad luck won't last forever because remember, you're changing the samples, right? So like the question, \"What is 2 plus 2?\" You're changing that, the next phase will be some other question. And so the trick is, just by chance, you will have a good reward, just by chance, and we just force that to be more...\n\nDoes that kind of make sense? So it's, to be honest, it's all luck. Yes, it's all luck. We're just guessing, \"Oh, you know, we're praying that there's going to be some positive reward somewhere in the model.\" There will be negative reward, right? So like, if your model is really, really bad, you, you can do negative reward. And so you just don't want to do the negative one. You just want to do the negative one less. And by miraculous probability, you know, just rely on probabilities, you will get a good reward somewhere, just by chance. Does that kind of make sense? I mean, all of the large model labs are just literally relying on the fact that that's what they're doing. They're just guessing. We're just praying for the GPUs to work, and then suddenly the reward comes out. I'm being serious. That's exactly what they do. They're just waiting for the algorithm to work, and if suddenly, \"Oh, okay.\" That's why people do random seeds as well. So, for example, the initialization of the model might not be good. So you just kill the training run. You do like 500 training runs. \"Oh no, 499 of them are like zero reward. Oh, just kill them all. Don't release them.\"\n\nI have seen on my training.\n\nYes. Very common.\n\nAs small as step.\n\nYes, that's the... Yes, I was going to show you guys that. Exactly. So like, you could force the model to answer some question, like, for example, you ask a question, \"What is 2 plus two?\" It's very easy. It's four. You just force it to learn, \"Oh, okay, it should be four first.\" And then you do other steps. That is actually why, remember? Okay, I have to go back to all the slides. I don't remember. Okay, where is it? How do I... Okay, I'll exit. Uh, it's the same as this problem. Where is it? This one, right? Someone asked about why don't you just start from the blue, you know, the pre-trained model to go to the green one. It's the same thing. Essentially, the trick is, we want to do some supervised fine-tuning to make it know some instructions. So it knows something, and then you want to go to the reinforcement learning phase. But if you want to start from nothing, like just the pre-training phase, that's the hard part, right? Your reward might be 00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00 Z00  \n\nSo, I think there is a misconception. Like, you know, the algorithm is important. No science, it's useless. Who cares about the algorithm? You can literally just use, you know, the general, the function, which I gave you, can just use any algorithm that you like. But the problem is actually the reward function itself. I gave you some examples, you know, like, what is 2 plus two? The answer is four. Yes, you can do distance-based, but that's just one example. Can you, like, can someone make a function? Can someone make a reward function for, like, trading stocks? That's you do that. Do that, right? And then there you have, like, a model for trading. Go ahead.\n\nSo, so you think it's going to be more that because people are able to create reward functions, like, it's a little bit more easy, like, similar to, like, you could create a prompt. It's an easier thing for most people to, like, iterate on. Um, it's actually quite hard coming up, but it's easier than coming up with the algorithm itself or whatever.\n\nActually, I think collecting, so in the olden days, large model labs will ask, like, you know, large data providers, like Scale or whatever, to create data, like, what is 2 plus two? You literally have someone sit there and write, okay, the answer is four. But then you also have to do the chain of thought, like, oh, I think the answer is four because of blah, blah, blah, blah, blah, blah, or like, you know, this is my working out. You literally have to ask someone sitting there to make the data.\n\nThe trick is, get no more. You don't need the data labeling step anymore. It's totally gone. You have the answer and you have the question. The middle step is totally removed. But you still need to make the reward function. You need to verify, you need to say, is the answer for good or bad? For maths, it's very easy. For code, for code, it's somewhat easy. You know, you can check, oh, did you import the correct function? Did you import the library? Did your code execute, you know, some other reward functions? But it's still hard to verify if your actual function is correct. For example, let's say your question, let's say the task was create the Flappy Bird game. How do you actually know that the output's good? How do you actually know? We don't. You could again ask a human to verify, or, you know, let's test a Flappy Bird game and then give it a good reward, a bad reward. Or the trick is, did the game actually run? If it ran, plus one. Did you see the word Flappy Bird in the, you know, Flappy Bird inside of the functions? If yes, plus one. Did you see the, you know, the image of the Flappy Bird sprite be used? If yes, plus one there. Something like that. So, like, you don't, it's still, I would say, the hardest part is writing the reward functions. And for open source specifically, if you know the whole open source community starts writing reward functions, we can probably beat 03, 01, like, you know. Okay. Plus compute. You still need compute. That's the problem. You still need compute. But if you write good reward functions, you'll probably catch up in no time.\n\nSo, the end state here is that you want an open version of the closed model. It's not, I guess my original question is, like, is this a thing that people are going to use, like, prompts have specialized models separately, or is it more that we want good open?\n\nIt, okay, that is a good question. It depends on which school of thought you're in. If you're in the thought that large language models already have the capability and you're just trying to accentuate it, then there will be just one model. Yes, but this model can only learn some facts because otherwise you're like overfitting and like it's not. But if you're in the second camp that the model actually learns something new. Oh, wait, did I say right? I think it's the opposite way around. Um, the first one is you have many models. I saw, sorry, I said it wrong. The first one is you have many models because a model doesn't actually learn that much. But if the second one is a model actually learns everything, then you'll have this one gigantic model. I think OpenAI probably subscribes to that point. You know, like most large model labs think that actually Reinforcement Learning can get you to AGI, right? It will know everything about everything. Any single question you ask, it already knows. And so, like, that's, I think that's where they're trying to go for. For open source, it's more harder. I think the open source community consensus for now, for now, is the model, it already knows your questions. You're just trying to accentuate it. And by doing reward functions, you're trying to weight the circuits more. You're trying to wait the model to know how to do these equations and stuff like that. So, I think, like, the goal of open source is, you know, if the entire community comes up with good reward functions, write them all, then the problem is we need compute. That's the second problem, right? If you shove both of them together, you will get 0, 0, 10, or something, I don't know. Um, right, imagine if every single person writes a reward function once per day, okay, that's probably too hard. Once per day will have, like, you know, seven billion reward functions, more than OpenAI can ever come up with, and they will defeat OpenAI. But, you know, you need the computer part. That's the only. Um, okay. Any other questions? Yes.\n\nYeah.\n\nCorrect.\n\nYes.\n\nHow do you feel like saving those traces of those?\n\nVery smart. That's what. Yes. Yes. Yes. I don't know if large model labs do that. You could do that. Yes.\n\nI feel like, yeah, we're just mining for those good examples.\n\nThe only problem I would say is, pretend the question was, what is 2 plus two, right? And then the model says, okay, pretend the model just says this, okay, it says, oh, let me work out what is 2 plus two. I think it is, oh, you know, the number two means two apples, and I want to add two more apples. I think the answer might be three. Uh, but let me rethink about it. Wait a second, it's like four. Is that a, should you fine-tune on that? I mean, you could, but maybe it's like cheating. Maybe it just says four by chance. Maybe, like, okay, I'll give you an example. Let's say that by chance, okay, the question was, what is 2 plus two? It says gibberish, like, um, I like to go to Paris for fun or whatever. I don't know. I like to go to the, you know, this event, blah, blah, blah, blah, blah, blah, blah, and then suddenly says four, just by chance. Remember, we're still rewarding this. We're literally rewarding this as good. But this is not good.\n\nNo, so we, we reward it at the very end. Remember, we see the number four. It is good. The question was, what is 2 plus two? The model can generate anything it likes.\n\nIt could.\n\nYou could, but that gets harder. So the trick is, people don't actually reward the steps in between. They just do the final step because otherwise it gets too complicated, right? So, like, what is intermediate steps reward? It's way too complicated. So what you do is you just reward the final step. So if, if you see the number four, it's good.\n\nBut we don't know how we got there. So you should. So yes, you could maybe at the very, very, very end step of Reinforcement Learning, you can then use them data to do, do, um, fine-tuning. Yes, you could. But I think, like, in general, it's because we don't know what the process is in between.\n\nYou say we don't train on.\n\nOh, no, we don't train on the thought. Yes, we don't. But, the problem is, we don't train on the intermediate step in between. You don't have to.\n\nYou, you could, you could, but remember, we don't know if the traces are good or bad. We don't know. So you can't just take this trace and then do supervised fine-tuning because pretend the answer was four is good, but we don't know the intermediate steps unless if you read the data, right? You could ask, you know, some human labelist to, like, oh, you know, please verify if this trace is good. You, you could, but then that kind of defeats the whole purpose of Reinforcement Learning. So, like, you don't, you don't want to do this. Um, does that kind of make sense or not really? Okay. Yes.\n\nWhat? Sorry.\n\nOh, yeah. Yeah. Yeah. We, yes, we will do that. Yes. Yes. Yes.\n\nIf we have multiple categories for math.\n\nYes.\n\nWhat's the normalization amongst the best practices?\n\nWhat do you mean by normalization amongst rewards?\n\nWe have four plus four gets zero. Then you have one Python. Just be a compile binary function.\n\nYes.\n\nIf we're running this in one big.\n\nYes. One batch.\n\nHow do we normalize the fact that?\n\nGood question.\n\nCorrect. It could be like minus 10.\n\nVery good question. That's your choice. Unfortunately, that's the problem of Reinforcement Learning. It's all about human choice. Like, you, you will have to decide, you know, is the Python one more important than your, um, than your maths? What, what is 2 plus 2, then you can weight it more, for example, your make 2 plus 2, the reward as minus one and one, and the Python function as, like, 1,000 and zero, right? You, you have to decide on the waiting functions. That is, that is your choice. Um, unfortunately, it is a, it's kind of like an art. You could, like, dumbly, you could just do everything is the same scale. I think that's what most, I think that's what the large model labs probably do is, like, everything, all the reward functions have the same scale, you know, plus one, minus, plus one, minus one, not minus 100.\n\nSo it's up to you.\n\nYes.\n\nWhat about stuff like, you know, is this a good summary? Is this a, how we, how we creating?\n\nThat is a question. So, like, now you want to, like, analyze, okay, that's where the LLM as a judge comes in. So, like, there is a school of thought that you can use a language model itself to, to make a number. You can ask ChatGPT, is this a good summary or is this a bad summary? Please give me a score from minus 10 to 10. Ask ChatGPT. You could do that. That's called the LLM as a judge thing. Um, there is a paper which shows that you can do this for some time, but then it breaks down. So you can't just keep calling the language model, you know, like, it's kind of like cheating, if I would say, like, you're trying to call ChatGPT to train ChatGPT, like, it will work for some time, but then it will break down. There was a paper, I need to find the paper, but the paper showed that if you keep doing this, your actual reward actually goes backwards. Um, so, like, you will get more, more, more reward, and then suddenly it just, I don't know, by bad luck, again, it's always about bad luck, the reward just goes back. Um, so yes, I mean, serious, like, all of AI is about bad luck and good luck, um, and, you know, optimization, trying to do efficiency, um, that's what everyone. Yeah. Um, so yes, you can use LLM as a judge. Um, is that kind of.\n\nIf I use as a judge, doesn't it just end up being a teacher and student model or?\n\nYes, that's why, like, that's why sometimes, essentially, the problem is, I, I need to find the paper, if you keep doing this, it will actually do bad. I mean, intuitively, it kind of works, but then at some point. There is actually another way you could ask a language model to generate reward functions. That is actually another school of thought. You can actually ask a language model to generate 10, seven billion reward functions, but the question is, is the reward functions good or bad? I don't know. Um, so, like, now you need to, like, rely on the fact that the models are good or bad. You could then ask another language model to verify the reward functions. So yes, you could do this. Maybe that's what OpenAI is doing. I don't know. Maybe OpenAI's goal this whole time is, like, oh, let's generate all these reward functions, verify each of them, and then shove it into the function. Let's see what happens. Maybe that's what they're doing. I, I don't know. Um, but yes, it is a student teacher. Um, yes.\n\nWhat's your opinion on how to make models inverse?\n\nIs it something else?\n\nDo you mean, like, how to make reward functions more efficient in general?\n\nNo, I mean, scalable in the sense of many different.\n\nYes, the majority of reward functions currently, that's why it's called verifiable rewards, is, like, maths and coding, to be honest. I think coding is also hard. I, I don't know why people lump. So coding, you can't actually verify technically it's correct. You can just say it ran or the output is most likely correct, right? For some functions. But for example, the Flappy Bird game, tell it to create a Flappy Bird game. How do you actually verify if it even is the Flappy Bird game? I don't know. But you could, right? That's the whole point of the LLM as a judge. You could take the output of the Flappy Bird game. Ask the language model, does this look like the Flappy Bird game? And if it's yes, I get plus one. If no, minus one. You could do that. Um, but you can only go so far. If there's. Does that kind of.\n\nWhat your opinion is the scaling.\n\nOh.\n\nPart to say. I think most, I, I think large model labs, they're currently just trying to use their own model to, to, to, like, literally reward, reward it, like, as I, like, described, you know, ask it, oh, does this look like the Flappy Bird game? If yes, plus one, if no, minus one. And I think maybe that's what, I think large model labs, their view is, if you keep doing this, you'll get to AGI. That's their view. I mean, if you think about. You could maybe, but then I always go fall back to, oh, but you might be bad luck. It's not going to work. Um, so, like, I think in general, I think bad luck will just, it won't work. Um, you will only get so far, and then suddenly it just doesn't work. Does that. Okay. Any other? Yes.\n\nThat is your choice again. So, like, if you want to specify, for example, you just want to make a legal bot, you're given some sort of court case, and if it's, like, you know, the plaintiff wins or the defendant wins, I don't know, you could just do law. Yes, you could, you could do that, but in my view, you should combine it with other sources. You should combine it with some maths. You should combine it with some programming because the point is, you don't want the model just to know, like, you just, you don't want the model just to, like, overfit just to just law. Maybe maths might be helpful just by, you know, by chance again. Maybe, you know, maybe coding might be helpful, probably not, but, like, you know, in general, um, so, like, you should combine other source, um, other domains together. Um, I feel like, you know, all the large model apps, their goal is to do every single domain possible, right? Like, mine every single reward function in the whole world, take.\n\n\nAll the robot functions, shove it into the model and just learns. Um, so like, yes, you should do more domains. If, yeah, that's another, yeah, yeah.\n\nLatest model is that this particular...\n\nSo, the notebook I will share will showcase. You should probably do some supervised finetuning fast. It's called the priming stage. Um, otherwise, you're... remember the plot over here, the... the one, right? You don't want to be in the situation where, like, you're starting from, like, some bad, you know, pre-trained state, and you're trying to go to the RL stage. Very not efficient, but remember, AI is all about efficiency. You don't want to do this step. So, we do have to do some priming, you know, the SFT stage and the other stages, if that's your question, or...\n\nOh, if you want to, yes, the bigger the model, the better. Yes, that's the trick. So, essentially, the research papers show that small models actually do work, confusingly enough, because essentially, these small models, it just does longer thinking. It does longer reasoning traces if the model's smaller, and if it's a larger model, maybe the reasoning traces are, like, smaller in general. So, like, I... I feel like the small models actually do work. They do break down, though. If you want to do, like, very complicated reasoning traces, then maybe the small models might not work because, you know, there is only seven billion parameters. There's no... not that much space you can move. Um, and so the large models, you just have more space to move around. And so that's why large models are better. If... I don't know if that maybe kind of... I don't know if that answers your question, but...\n\nModel.\n\nYes, correct. Exactly. Yes, that's what you should do. Yes, you can take a distilled model, like, I already reasoning model, and then further fine-tune it. Yes, you could. Um, I would say it's a bit more complicated because you could do that, but remember, the reasoning model itself is already a reasoning model, and you're trying to fine-tune it to become other... re... like, you're trying to do some other domain. It might be easier, it might be harder. It's all about luck again. I don't know. So, you have to try. It's all trial and error. Um, yeah. Yeah. Yes.\n\nTwo questions for you. One, like, it's pretty empirical, just like, try and see what works and what doesn't, and see what...\n\nYes, correct.\n\nOkay. And then the other side, how are you keeping up with all the papers and all the content that's been good at? I'm sure it's a lot. How do you learn?\n\nTo follow?\n\nTo be honest, I don't... you don't need to follow. That's my view. Don't try to follow the latest research because sometimes I may, like, the next day, it's like, rebuttal of the previous paper, and then the next paper says, \"Oh, it's a rebuttal of the rebuttal.\" I don't know. So, I would not try to keep too much up to date with the latest research. I think the field has kind of matured, and it is mostly stable now. You might have, like, some algorithm increasing accuracy by 1% or 2%, or some efficient. Remember, all of the papers are about efficiency. It's always about efficiency. Making the training more stable, reducing overfitting. It's always these similar, similar papers. Um, so I would say, don't... you can keep up to date with papers. Twitter is very good as a resource. Sometimes I tweet about papers, although I... I don't suggest the Nathan Lambert paper. The Nathan Lambert, um, book is very good. He keeps updating it. Where is it? Where did I put it? Um, this one, the RHF book. That is very good. So, definitely read that. Um, he updates it all the time. And so, maybe follow Nathan Lambert. He's actually a very good Twitter on, like, the latest research. Um, so he's very useful. In general, there's a lot of noise in the RL space as well. You don't know if the research is good or bad, like, you know, rebuttals on top of rebuttals. So, I would suggest people just to, like, try. It's just trial and error, right? Try to see if your reward function is good or bad, you know, is the loss not, you know, is the reward just 000? Like, unfortunately, something's wrong, or you just... it's bad luck. Try again. Um, so it's just empirical. Yes. Um,\n\nThese slides. Sorry.\n\nAre you put...\n\nOh, yeah. Yeah. Yeah. Yeah. Yeah. Um, yes. These slides should be up. I was supposed to make a Bitly link. Um, I'll probably do that later, but I will share the slides. Yes.\n\nSlacking.\n\nOh, yeah. Okay. I'll do that then. Okay. Any, uh... Yes.\n\nYeah.\n\nYes.\n\nYou are creating...\n\nIn the old PO sense, you are the value model is a new model. The reward model is a new model. Yes, but remember, in GRPO, we delete the value model. The value model is totally gone. We... we create the value from just statistics from the distribution. We essentially just create, you know, four... what is 2 plus two? Just create four examples, four trials, and then find the mean, find the standard deviation, and then that is your... that is your value model. It's not even a model anymore. And then the reward model is no more as well. It's just reward functions. And that is why we call it Reinforcement Learning with Verifiable Rewards. It's not normal RL anymore. It's like you replace a reward model as well. If... does that kind of make sense?\n\nOkay. My follow question.\n\nYes. So, the... again, there is two schools of thought. The first one is, like, the... the question, \"What is 2 plus two?\" somewhere in the model, somewhere, I don't know where, right? This high dimensional 1.4 trillion parameter space, somewhere it knows to calculate it as four, right? There is some sort of circuit inside the model, and then the goal of RL is just... just to maximize this circuit somehow, right? Via these formulas and stuff, you're just trying to maximize it. But that's one school of thought. The other school of thought is, \"Oh, you know, RL is actually learning something, you know, like, it's actually learning how to do 2 plus 2 is equal to four,\" and it's not actually in the model.\n\nOkay. Any... Yeah. Yes.\n\nWhen you say capabilities of a model that already has them inside.\n\nYes.\n\nYou mean knowing actually the... the answer to a question or knowing how to reason to get a question?\n\nThat is a good question. Maybe both. I... I think it depends. It probably knows how to do the re...\n\nIt might...\n\nFor example, a contrived example. You get all of the entire world's data, right? Like, you know, 30 trillion tokens. Get all of the data, and you just make a question that is not part of the data, right? You... you could do that, right? What is 10 billion, you know, some random number times some random... you can make a math equation which is not in the data. But somehow the model has learned to do multiplication, has learned to do addition somewhere. And so, maybe this circuit for addition, for multiplication, you know, for whatever, some, you know, many, many, many circuits of these, like, functions, we just want to accentuate them all, and that is what kind of RL is trying to do. Okay. And I mean, yes, there's a reasoning circuit, so, like, somehow the model also learns how to do reasoning, and so we also want to make that more important. And so, you know, 2 plus two is very important, addition is very important, multiplication is very important, and so on. We're just trying to, like, make all of these circuits more prevalent.\n\nBut that's only one half of the AI community, right? That's only one half thinks like that. The other half is like, \"Oh, but the model's actually learning.\" You know, we're actually training the model to learn, and the base model actually doesn't know how to do reasoning. Does that kind of...\n\nOkay. Yes. Okay. Hopefully. Okay. Any other questions? Yes.\n\nYes, there is... there is TRL, um, Unsloth, like, we... we also make a... it's not called a framework, that's more like we showcase that you can do, uh, GPO and Reinforcement Learning with very low resources. So, we are the only package which allows you to do GPO on, like, a free collab. And so, like, that's the only between us and everyone else. So, for ver...\n\nWas very good for large training runs. But for now, Unsloth, like, if you want to do, like, small experimentation, you want to try stuff out, you don't know what Reinforcement Learning is, you don't know how to make reward functions, you don't even know what reward function to do, you should utilize our notebooks. And that's what I was going to demo. Um, yeah. Yes.\n\nSo, one question is, like, let's say you don't think you just task, right?\n\nOf that using to just improve, like, let's say, use for your...\n\nYes, yes, you can do that, exactly. It should increase accuracy by quite a bit. If your accuracy... if your accuracy before with tool use was not very good, IRL should definitely help. And I feel like the trick of RL is it reduces overfitting. I think that's the trick, because you do... you do multiple inferences, you don't know which one's correct, but you're trying to, like, maximize some good ones, right? Some good ones. The problem with general fine-tuning is you're kind of, like, overfitting the model. And so, the trick of... the trick of reinforcement fine-tuning is you can essentially reduce overfitting. So, the model actually learns how to do tool calling. Not just, \"Oh, you know, I see someone's trying to do a restaurant order, I just want to call DoorDash to do order,\" or something, right? But it actually learns, \"Oh, okay, because the person wants to order food, I should order DoorDash.\" It's like reverse thinking. Um, so we should definitely help.\n\nYeah, some of my experiments, like, unless I tried not explicitly asking the model instruction, like, just do some task, right? And it does not automatically start the thinking process unless you're explicitly prompted. Okay, first think and then answer. I was trying to check if, like, just without explicitly asking it to start the thinking process, just to improve the tool accuracy itself, does it start? And that, I think, that did not happen because it...\n\nYou don't need to do so. GPO generally, people utilize GPO and, like, Reinforcement Learning algorithms to create the thinking process. That's because it's like an artifact of GPO. Just by chance, they see the reasoning process. You don't need it. So, maybe by chance, by luck, somehow it learns how to do tool calling, and it's not some thinking process. It could be some weird symbols, maybe, you know, I don't know. It could be, like, using some other, you know, like, sometimes models have, like, different languages, suddenly. It could be like that, you know, randomly it learns how to do tool calling. It made some new programming language internally, I don't know, but it could have done that. In this case, there was no thinking process.\n\nYes.\n\nIt just directly gave the output because, like, let's say you sample, like, 10 trajectories, and none of them have a thinking process, then it's never... it never explores those.\n\nFor now, it will never, but remember, it's all about luck. Over time, you will have a thinking process just by miraculous chance. There is a thinking process somewhere, and then, \"Oh, you should do this more,\" and it would just do this more. But to make that more probable, like, you should prompt it. You can prompt it. So, essentially, in the system prompt, you can say, \"Please put your working out between this box.\" You could force the model to create the working out. You could do that. You could... Um, but is this the most efficient? I don't know. You could... you could say, \"Oh, please create a new language that I don't understand which does tool calling, and then it does some weird symbols, and then, oh, it does tool calling.\" I don't know. But yes, you... you should prompt it. It should make it more effective.\n\nOkay. Any other... Yes.\n\nWhat's the secret sauce?\n\nAnd...\n\nOh, we... we utilize Triton Kernels. We do, like, kernel optimizations. We reduce memory usage by 70%. Um, there are lots of optimizations that we do to make training faster and more memory efficient.\n\nYes, later. Yes, it's not the main focus, but yes, we will talk about that. For VLM specifically, we use... Okay, that's actually at the notebook. Oh, before that, do we have any other questions? Yes.\n\nFocus on these days.\n\nThe... actually, the DC paper talks about this. There is pass at K and majority at K. I think they said that if you do test time scaling, I think it improves majority. I think that was correct. You need to read the... I'll have to rever... revert back to the DC paper. But they did say, remember, test time scaling is different from Reinforcement Learning. There are different methodologies. Test time scaling is calling the model 10,000 times, and then, you know, then you just check, you know, by average, what... okay, you... for example, you ask ChatGPT, \"What is 2 plus two?\" It might say four, you know, it says four, four, and suddenly it says five. I don't know, by chance, it says five, it says zero, and you just take the most likely answer. That's called test time... test time scaling. And then Reinforcement Learning is kind of different. It's more like, \"Oh, we want to, like, make... we want to actually train the model to actually do the whole trace.\" And you do... you don't need to do... you don't need to output 10,000 examples and get the best answer. You just do one.\n\nYes. Correct.\n\nYes. Anything.\n\nYeah. So, the trick is you do the RL step, and then you do test time, compute it will actually make the accuracy much higher.\n\nYou... that's a good question. You could kind of... GPU is kind of like that, right? So, GPU, you do test time scaling in... in the actual reward function. You literally call the model, \"What is 2 plus...\" do you do test time scaling, and then you aggregate the results. So, it's like GRPU itself is doing test time scaling internally. You could...\n\nThat sounds like a new research paper. You could do that, I guess. Yes.\n\nOkay. I will have to go to the notebook now. Um, so in order to access the notebook, you can go to our GitHub page. Um... which, until my internet actually loads. Um, so if you go to Unsloth, right? The GitHub page, Unsloth, there is a button called \"Quen 3 GPO,\" right? And you can click \"Start for free.\" That's how you get the notebook, or you can go to our docs, um, which have the notebook. Um, so remember, go to the GitHub page and then click \"Start for free,\" um, yes, for Quen 3 GIO, and then you will get this notebook. Um, generally, it's dark mode, but I know in presentations, you know, presentations, I don't think so people can see that, so I will change this to light mode. Um, so we utilize VLM behind the hood. So, VLM... does anyone not know VLM? I think that's a good question. Who does not know VLM?\n\nOkay, 100% you must use VLM, right? So, like, for all open source, how do you serve a large language model? Please use VLM or SG Lang, um, or, you know, I think Hugging Face has one as well. So, like, these are the best open source libraries to serve open source models. Um, you know, like you have a GPU, how...\n\n\nDo we actually serve Llama 3? You know, how do we serve Llama 4? You use VLM to serve it. The trick of Unsloth is, so Unsloth is a package for fine-tuning, for GRPO, for reinforcement learning, for whatever you like. Continue pre-training, whatever. And the trick is, we just optimize it. We make it much faster. You know, use two, use, um, 70% less memory. Um, make it fit on a free collab. Um, remember, please use free collab resources. You know, Kaggle has 30 hours for, I already said this again, Kaggle has 30 hours of free per week of GPUs. Please utilize them. Um, they won't be unhappy, you know. Please utilize them. Um, and yeah, so you install Unsloth in VLM. Um, and we have this thing called the fast language model class, which essentially you can call a model. Um, for example, we will now utilize the quen 3 base model, right? So, like, remember I told you not to do this, right? But we are, anyways, we're going to do it. Um, this plot, where is the plot? Um, we are going to do the dark blue dot to the, um, the dark green. Um, yeah, that's what we're going to do. We're actually not going to do what I suggested not to do. Um, but anyways, um, we're going to do that. Um, you also have to set a max sequence length. So, for example, if you want to make it longer, you can set it for longer, right? If you want longer reasoning traces, you can increase the maximum sequence length. We set it as 248. Um, if you set up a larger that free GPU will run out. Um, so that's the problem. You can also load in four bits. So, if you want to do four-bit quantization, you can make the model go to four bits. You can reduce memory usage by quite a bit. So, you can do that as well. Um, and remember, we are utilizing Laura, which is a parameter-efficient fine-tuning method. Um, you don't need to fine-tune every single weight inside the entire model. Um, this will be very, very, very costly. Instead, we add small weights to the model to fine-tune it. Um, and so that's a trick that we do.\n\nAnd because we utilize VLM directly, we do a trick. We actually reduce memory usage by 50%. The trick is, we share VLM's weights directly. Um, other training frameworks, what they do is, they have to copy VLM's weights, um, because you have to, you have the model for fine-tuning and you have the VLM weights. The trick that we do is, we actually share the VLM weights directly. So you can reduce memory usage by a further 50%.\n\nWe use something called the Unsloth grading checkpointing, which reduces memory. Okay, essentially everything in AI is about reducing memory usage, more efficiency. You know, everything that we do is just efficiency. Um, so everything that we set is for efficiency purposes. Um, your Laura rank, for if you do Laura, please set it to be the alpha to be two times the Laura rank. Um, it speeds up training dramatically. So please do that.\n\nSome lots of stuff, right? Compiling. We do like automatic compiling and stuff like that. You don't need to read all of this. Um, and here is the bulk. This is the most important part. Someone was asking about, you know, about a prompt. You make a system prompt, right? You are given a problem. Think about the problem and provide your working out. Place it between reasoning start and reasoning end. Right? So, like, the reasoning start is start working out and end working out. So it should look something like this. Um, if you, you know, does, uh, start working out and end working out, right? You were given a problem. Think about the problem. Provide your working out. Place it between start working out and end working out. Then provide your solution between solution start and solution end. So it should look something like this. Um, and this is the system prompt that we're going to use for reinforcement learning. Remember, you can customize this to however you like. Right? You don't have to say you are given a problem. You are given a legal case. Right? Think about the case and provide, provide your, I don't know, legal thinking, I don't know, I'm just making stuff up, I don't know, whatever, provide, place it between, I don't know, it can be literally anything, um, thinking, I don't know, I don't, you can even make spelling mistakes, it doesn't really matter, right? End thinking.\n\nThe whole goal of RL is, you can design your reward, you can design the system prompt to whatever you like, right? I think that's the main problem is, like, people think, oh, you must follow DeepSeek's think, right? Think right, you need to, people see in DeepSeek think right and close think right. You do not need to follow, you do not need to follow this, right? You do not need to follow this at all. You can make it up entirely, right? This is customizable to whatever you like.\n\nThe hard part is, because we are using a base model, remember, this is a base model, you have to make a chat template as well. Um, this is the more annoying part. You can just copy and paste this chat template. You do not need to do anything else. Um, just, you know, literally copy and paste it. Um, the base model does not have a chat template, right? A base model, when you call it, you can't actually call it for conversation. You can't, it's not GPT, right? It's just a base model. It doesn't do anything. So, you need to specify a template for it to understand how to do conversations. And so, this is kind of like a template that we did. It's a very, it's very generic. You can just copy and paste this. It should be the same for anything.\n\nAnd then we show, so after you do the chat template, we show an example of how to actually utilize the chat template and the tokenizer. Right? So, for example, if you ask what is one plus one, you do the reasoning process, it will say, you are given a problem. Think about the problem and provide you're working out, blah, blah, blah, blah, blah. And in the question, this is a question. Your question is, what is 2 plus two? Remember, the answer is four. And so, start working out. This is what you give to the model. You give all of this to the entire model, right? All of this. Um, okay, I can't really highlight. All of this you give into the model. And the goal of RL is, you want to create the working out process automatically, right? The RL algorithm will automatically create the, you know, the working, the working out or the thinking process. And then finally, it will say four. Well, hopefully it will say four. And the goal is, if you see the, if you see four, you want to make the reward higher just for that.\n\nNow someone was talking about, um, you know, fine-tuning with the, um, you know, instruct fine-tuning first. Again, remember, we go back to this diagram, we wanted to start from the blue dot to go to the green dot, but we found it doesn't actually work. So, don't actually do this. The trick is, we go back to this diagram, we actually want to take the pre-trained model, do some fine-tuning, do some supervised fine-tuning, and then go to the green dot.\n\nAnd so, this part, we show that you should actually do some supervised fine-tuning. You need to do some fine-tuning to prime the model. Right? The goal is, you want to learn, you make the, you want to make the model not just output zero board forever. And so, this data set allows you to prime the model to do supervised fine-tuning.\n\nSo, for example, the problem is, you know, what is the sum of all the real numbers, blah, blah, blah, blah, blah. And then you use DeepSeek-R1. So, this is a trick, this is a hack. You use DeepSeek-R1 to create some examples, and then you shove this during the fine-tuning step, and essentially the model already learns how to do some reasoning, and that's the trick. This data set is very small. It's only 7,000 rows, right? You don't need to have that much data for just this first step. You can have, like, I think I only use 600 rows. Very, very, very less data.\n\nSo, this is just data preparation step. Not that important. Um, I need to skip to the reward functions. This is the most important part of the model. So, this is the supervised fine-tuning step. So, all of this is a supervised fine-tuning step. So, this, this part, this part of the model, um, that's this part.\n\nSo, not that important. Um, okay, we skip all of that. This is the most important part. The reward function creation is the most important part. And I feel like, you know, the majority of people, like, neglect this part. It is the most hardest part to do. Um, okay, let's see, where is the reward function? Oh, here it is. Okay, for example, okay, no, no, this one is a regular expression to match if your format is correct. For example, remember we have, we ask the question, we ask the model to say, please put your working out between start working out and end working out. This regular expression essentially, um, essentially rewards the model to see start working out and end working out. If it doesn't have this, you will actually penalize it. And so, this is one reward function that I created.\n\nFor example, I give it an example. If you say, let me think end working out, it extracts two. Yes, that's good. Right? Remember, we force the model to say, we force a model, you must generate the answer between this and this. And it successfully extracted two. So, that's good. But, you know, also sometimes a model might generate some random spaces. It's possible. You know, the model might not actually, the model might not follow your exact format. We still try to match it, right? Even if it generates extra spaces, we still successfully match the number two. So, that's good.\n\nThis is a reward function, right? So, essentially what we see is, if it matches the format exactly, we add the score by three. If not, we just put zero. And remember, this match format essentially matches regular expression. We had to create it by hand for matching the format. This number does not have to be plus three. It can be plus 300. Whatever you like. It can be plus one. I don't know. It can be anything that you like. But I just found plus three to work fine. Um, so you can do whatever you like. Um, yeah, anything. And remember, the score is zero. If you don't see it, you can also do minus one. For example, if it's else, right? If it's not good, you can also do score minus, right? Minus three. You can minus three points from it. So, up to you. You can design your reward function as whatever you like.\n\nBut remember, if the model, if the model output is not exactly following your format, we should still at least reward it a little bit, right? Otherwise, the reward will just be 0000. So, the trick is, if we see a keyword, we plus one. And if we don't, oh, sorry, plus 0.5. If you see the keyword and if you don't see the keyword, right, if you see the keyword this, if you see this in the output, you should at least plus 0.5. But if you don't see it, then you're minus one. And so, this essentially allows you to partially reward the model.\n\nMore reward functions now gets more complicated. This, this large reward function essentially allows you to calculate the distance-based scoring. For example, remember we said, um, over here, um, where is it? Um, this one, right? What is 2 plus two? Four is is correct, but three is also a better answer than D. Right? If you, if you output D, it's definitely wrong, right? If you output five, it's okay, but it's wrong. And so, this function essentially allows you to take the good answer. So, sorry, this is the guess divided by your true answer. And it's like a ratio. And this ratio essentially allows you to reward if your number is close to the actual answer, you give it higher reward. And if your, if your answer is very, very, very far off, then you penalize it by minusing reward. And so, this essentially allows you to do that. If it's exactly correct, you also add five points. And so, this, this is probably the, this is probably the most important reward function. Um, but this is only for maths. Um, for other, like, code and stuff like that, you have to create more reward functions.\n\nNow we test if our reward functions actually work. Um, and yes, it extracts the numbers. Um, this is just format reward. Sorry, this is just extracting the solution. And you can see that it extracted 0.34. It extracted this number. It extracted this and extracted this. Um, if your reward function is not working very well, you probably did something wrong in the regular expression. So, please, like, edit that. And then this is helper functions. Oh, this is another, another reward function. If you see, if you see the number 1, 2, 3, 45, we want to remove the comma because you can't actually convert this into Python. So, you want to remove the comma. Um, and then if it's equal to the true answer, you're plus 3.5 reward. And if it's not, then you're minus 1.5 reward.\n\nMore data set preparation functions, not that important. Um, and here is the meat of the code for training, right? We call VLM. Top P is 1.0. Um, you don't, it's not that, you can probably, it's probably not a good 1.0, just means you're sampling the entire space. Um, so that's good. You can set this to, like, 0.8, eight or something else, up to you, but I generally set it to be 1.0 to be like full sampling of the entire space. Min P is 0.1. I suggest people to use this because otherwise the model might, like, go into, like, it might do inference of, like, random outputs, so, you know, use 0.1. And temperature, I did suggest people to increase temperature to 0.1, 0.2, two, right? Or you can do one. Um, try to increase your temperature as much as possible. The more temperature you increase, the model becomes very, very, um, creative, right? It, like, creates random outputs. If you increase the temperature too much, like, you know, two, your model will be, like, gibberish. So, probably don't do too large numbers. So, I normally suggest 1.0, 1.1 or 1.2 or somewhere around there. You should utilize min P together, right? You should utilize min P together with high temperature numbers. Um, there is a paper about using temperature 1.5 and 0.1 min. You should utilize that. There are some, some other things that we utilize. Um, num generations is very important. I set this to, before this number is, is this thing. Um, where is it? This number is this. How many, how many, like, uh, how many rollouts do you want to do? How many inference steps do you want to do for GPU?\n\n\nRight? We chose four. And so four will just means what is 2 plus two? It will create four four options. And so that is this number. If you increase this number too much, you will use much more memory. Um, you should increase this as much as possible if you can. Um, and there's like gradient, uh, there's like batch size. We set this to be one. Um, the trick is the batch size times the gradient accumulation is equivalent most of the time. Um, GPO is not, but essentially what this does is if you do one, it just means we're doing one. What is one? What is 2 plus two? If you set batch size to be three, then we shove all of these three examples together into one.\n\nGenerally, you should set batch size to be much larger. Um, the problem is if you set batch size to be too big, you're going to use more memory. So the trick is instead, you do gradient accumulation. You set this to be 16. That's a trick. Um, gradient accumulation. It essentially allows you to do addition of gradients over time, and you can skip using too much memory.\n\nAnd then there's like evaluation. If you want to do evaluation, there's some functions for that. Um, and then we shall see the training. Um, you will get a large table of numbers during the training process. This took 2 hours and 54 minutes on a free collab. Um, look at the reward column, right? The reward column, minus 7.5, -5.5, -5.5, all very bad. Oh, and then suddenly plus 13. Just by chance, suddenly it's plus 13. Remember GPO, the trick is if you see this +13, let's maximize this even more. And then it's, oh, but then it didn't really work, so it goes back to -7.5, minus 5.5, minus 7.5, and so on. And then plus 11, you see another good reward. We want to maximize this as well, and so, so on, so on, so on, right? That's that's the trick of GPO. By luck, by chance, literally by luck, you will have good answers, right? Good answers, you want to maximize this. And if you keep looking down, right? If you keep scrolling down, in the end, okay, I need to make a plot, but in general, your reward will increase over time, right? Look, these are all positive numbers now, right? These are all positive numbers. Your minus numbers are getting less and less and less and less. Um, I think if I can, I don't know if I can plot this. But okay, I'll plot this later. But essentially, if you plot this over time, the reward will actually increase over time. There is also other numbers like completion length. So essentially, remember when you use a reasoning model, the reasoning trace can be extremely long. So this column just tracks how long the reasoning process is. Um, it's over time. In general, the reasoning length should get longer and longer. Um, but sometimes not always the case. Um, yeah, not always the case.\n\nThere is also another column called KR divergence. This essentially tells you how far the model is, the, you know, the final model from the original model. And the larger the number, it means it's getting very, very, very far away from the original model. Um, in general, this number should get bigger over time in general. Um, sometimes it doesn't move, but you should make this number go as much, much higher as possible. Um, and then we also, we made separate reward functions. Each of those reward functions also has their own reward. Um, the most important one is the last column, right? Or the second last column. These are the two numbers. So in RL, there is a problem. Most RL, most RL training runs just follow the format, and it doesn't actually learn. So the format columns are not important. Do not look at the format columns. These are useless. You need to look at the last two columns. And if you look at the last two columns, right, rewards, check, check numbers. This essentially checks if the output is good or bad. You see that it's minus 2.5, minus 2.5, not very good, and then suddenly 3.5, right? 3.5 is good. We want to maximize this. If you keep looking over time, in general, if you take the, if you take like a rolling average, in general, the model gets better and better and better. Obviously, we only train this for two hours and 50 minutes. You know, if you train it for 20 days, it might actually do very well. Um, but remember, this is a free collab GPU. Um, so in general, remember, so like the goal is, the goal of GRPO is suddenly we see a good answer with a good reward, we want to maximize that, and that's the whole point of GPU. It, it's, it's nothing fancy. It's just like by luck, we see it, and we just want to maximize it.\n\nWe can also see some output from the model, right? At the very beginning, right? At the very beginning of the model, let's see, um, where is it? An example. Compute the number of positive integers that divide at least to a blah blah blah, some question, and then it does some reasoning trace. Remember, we already fine-tuned it a little bit. So, it does something, right? It does something, but the answer, it just goes on and blah blah blah. It just blah blah blah. It just keeps going on blabbering on. Um, but then if you look at the actual answer, um, where is it? If you keep, okay, there's a lot. Um, we print out every single, we print out a lot. Okay, it just keeps, okay, whatever. It keeps going on and on and on. Um, this is the output of the GPO algorithm. And you will see over time, if you inspect this, you will see that the model actually gets better and better and better. Um, for just an example, right? Let's say we ask the model what is the square root of 101, right? It's not, we don't just say what is the square root of 100, that's just 10. We say what is the square root of 101.\n\nIf you do not train the model, this is what you get. It will say answers, education, math, and arithmetic, what is the square root of 101, wiki user. Oh, wiki user, this is what it actually will say, right? That's actually what it will say. Does, do, where do you think this data comes from? Does anyone know? Can take a guess? Where do you think this data comes from? Probably Wikipedia, right? So if you ask the question, what is the square root of 101? It doesn't do anything. Right? Remember this is the base model. The base model is useless, right? You're not going to get, it's not going to answer the question.\n\nBut after we do GPO, right? What is the square root of 101? We ask the question again. It says, okay, so I need to find the square root of 101. Hm, let me think. I remember that the square root of numbers between the perfect numbers are rational. Blah blah blah blah blah blah blah, right? It just says blah blah, whatever, some whatever, and it says solution 10 point, right here, solution 10.049875049875. I think that's correct. I don't know if that's correct, but probably it's very close. And so the goal of, so the whole point is GPO allows you, the GPO algorithm produced all of this, right? This reasoning trace. In the olden days, you actually have to have a human write all of this, and then you have to fine-tune the model. GPO, you skip, you don't need to make this anymore. It's automatic, right? That's the trick of GPO and reinforcement learning. All of this reasoning phase is automatic, totally produced from nothing, and in the end, it gets a solution.\n\nYes, you had that. Yes, that's trick. If you do the base model, the trick of doing this is if you just do the base model, go into the space, you'll still get this, but it'll be too long. Otherwise, you'll wait there for like 20 days, and for demonstration purposes in a collab, you'll have to do the supervised fine tuning step. That's trick.\n\nYeah. Yes.\n\nWhat is the advantage of doing this 7,000 examples versus using model out of the box? You can use an instru, we actually have notebooks for that. So if you go to GPU, in general, um, we have notebooks for using instruct model. Okay, the internet's very slow. Um, GP, we actually have other notebooks. For example, if you use Llama 3.2 3 billion, that is using instruct model. You don't need to use a base model, but we showed that you can use a base model. Um, yeah, you can use, I suggest people to use instruct. Um, you probably shouldn't use base. It's all about efficiency as well. Um,\n\nyes.\n\nSorry. What? What?\n\nYes. Yes.\n\nYes. Correct.\n\nYes. So the goal of Ko divergence is you want the model not to stray too much away from the original model, right? KO divergence is, I shouldn't say distance, but K cho divergence is like a distance between the, the current model, the current model that you're training and the previous, very, very, very beginning of the model, right? And so essentially, if the model is too far away, your ko divergence will be very large, right? If you look at the plot, uh, where is the table? Uh, I'll scroll up a bit. Um, right, which one is the k diver, oh, here, this is the column, right? This column is a ko divergence, um, column. Over time, it should get larger and larger and larger over time, right? The number should get larger and larger and larger because the model is straying away from the fine, uh, the original model. If you set the beta to be zero, then you remove this term. Maybe this might make the capabilities of the model more, maybe because you're essentially not forcing the model to be as close as possible to the base model. Active error of research. So yeah, some people might set it to zero, some people might not set it to zero. You know, I think 0.0, I think the default is 0.05, 5 or 0.03 if that.\n\nOkay. Any other questions for? Yes.\n\nSo for fine-tuning, actually, fine-tuning is actually very helpful already. Um, where is the loss? Where is the loss? The base model. So the base model already is very bad. Um, if you do, uh, here, there's a loss. There's a loss. This is using the finetuning step, the pre, the priming stage, right? You use like a data set to firstly prime the data, to prime the model. The loss does decrease. Remember, if you see a loss of 0.64, that's good. Um, if you see a loss higher than 30, definitely something's wrong. Um, higher than three is very bad. Um, you can see the loss definitely decreases over time. So yes, doing the fine-tuning stage does teach the model a little bit to do reasoning, and it learns how to do some stuff. Also a good, a very interesting, um, fact is we used DeepSeek-R1, some of the reasoning process to do the finetuning step. And interestingly, if you just call the model without doing GRPO, it kind of does reasoning already by doing 7,000 examples, right? It already says, remember the question was, um, what is 2 plus? Oh, wait, this is just a general question, right? It kind of learns how to do reasoning somewhat, but it's not perfect. And so the goal of GPO is to forcibly make it perfect. Okay, not perfect, but as much as possible. Um, so actually the finetuning step already kind of learns a little bit.\n\nNo, we, no, actually, you don't need to use 7,000. I think I only used, I think I used 118. It's so it's, uh, uh, two training epochs. I think it's, yeah, it's 118. I only use 118 rows. You don't need to, you can use 10 rows. You can use, yeah, use as less. You must use more than three rows though. Um, because when you do Laura, the gradients become are zero. So you must use more than three, but, but anything more than three is fine.\n\nYeah. So even if you use 118, it does fine. Um, yeah.\n\nYes.\n\nDo you mean like a small model versus a big model? What's the difference? Um, can you tell any tricks if you want to do this training on a bigger?\n\nOh, yeah. Go ahead. You can, you can take the notebook. You will need a better GPU though. Take the notebook. Edit, edit this here. Not four. You can do 14 billion. Wait, I think there's a 14 billion, I think. Or is it 12 billion? I can't remember. You can do whatever you like. You could even do, you know, Llama 3.370 billion. I don't know. Up to you. Um, do whatever you like. And but the goal is for a collab demonstration because it's a small GPU. I use like a small model. Um, we actually have notebooks for free collab which fits 14 billion. Um, so fe 14 billion actually fits in a free collab. Um, so you can do big models in a free collab. Um, Kaggle, again, I said Kaggle, use Kaggle free GPUs. Um, there are like no, so essentially this whole page has like no books. Uh, where's Kaggle? Kaggle. Kaggle has like notebooks for GPO as well. Um, so you can do whatever you like for large models. Um,\n\noh, do you mean like for VLM rollouts like? Oh, so the trick what we do is you, we colllocate, so you use the same machine for inference and fine-tuning, and the trick is you can reduce memory usage because you're sharing the VLM weights. So some other trainers like Verl and TRL, you do have to put the inference on another server, and then your training is like a separate server, and they have to do communication. We don't, there is no communication for us. There is none. So we do, it's very close to asynchronous training, nearly no delay in training, but yes, we, we don't support it yet, but we do plan to support like, you know, larger training runs. Um, yeah, yes.\n\nQuestion, what's?\n\nYes, people have asked that, no road map. I don't know if we're going to support it. It's a bit more complicated. You could use, I think XLA, like they, they do have PyTorch converted down to TPU. So maybe it might work. I don't know if it works. I've never tried it. Um, maybe later. Um, yeah, maybe later.\n\nOkay. Yeah. Yes.\n\nOh, you don't have to. You, you, the, the whole point then, like this thing, um, uh, here, right? We chose a base model because you can show that you can do a base model going to the green dot. But then unfortunately, in the collab, we do have to do some supervised fine tuning, otherwise you'll wait there forever. The reward again will be 000000. We just want to remember all of AI is about efficiency and speed. So we just want to showcase, okay, you do need to do the light blue step. You still need to do the supervised fine tuning step. Um, yeah, yeah.\n\nOh, no, you don't need to. You can take the instructor. So the notebooks over here. So for example, if you go to the, uh, the Llama 3.2 3 billion notebook, we don't do any fine tuning step at all. You skip directly because it's an instructor model already. It already learns how to do chat. It already learns how to answer some questions. You can skip directly to GPU. Um, if it loads, um, but yes, the notebooks, okay, you'll have to wait for\n\n\nIt to load. Um, whatever. The internet's very bad. Um, it is loading. Um, yes. Any other questions? Yes.\n\nOver in the reinforce algorithm, you had the log probability of a state of an act. Is that happening inside the sampling model? Where is that in the notebook?\n\nOh, the the the algorithm itself of GPO. Oh, it's like behind the scenes, like, yeah, is that happening over in the GPO trainer?\n\nYes, it's inside the trainer itself, like somewhere in the code, somewhere it does that. It's figuring out like what is the probability of token versus all the tokens.\n\nOh, the calculation is inside the trainer. So, like somewhere, you know, on the GPU, you're doing this calculation, but you do get the probabilities. Remember the language model, you get the probabilities already. You just get the reward function and you just want to maximize it.\n\nSo, do you take the log that come out of the large language model and turn them into pseudo probabilities and then just assume that's...\n\nI think so. Yes, that's correct. I think it's exponential of... Yes, I think that's correct. There is, if you go to like the code, there is like some derivation for it, but yes, you're correct. Any... Oh, okay, the notebook loaded, but yes, there is another notebook which does the instruct here, right? There is instruct model and there is no fine-tuning step at all. It just does the reward function, um, and stuff like that. Um, wheels notebook, for example, is also very good. So, like, if anyone wants to check other notebooks out, wheels notebook, um, also utilizes, I think, also the instruct model and then does GIO. Um...\n\nOkay, the GR... Okay, kind of time is running out, but okay, technically the GP4 portion is done. Oh, there's actually more portions. Um, I will have to breeze through them. There's only 10 minutes left. Um, whoops. Um, any other... I will take questions at the very end. Anyways, I'm going to stay here anyways afterwards. Um,\n\nquantization. We'll now shift over to quantization. Um, so I don't know if you guys know about the DeepSeek-R1 1.5 bit quance that we did. Um, but you can essentially download these models. DeepSeek-R1 is 730, I think, 730GB. You can quantize them down to 140GB, um, without that much loss in accuracy. Okay, there's obviously loss in accuracy. But the trick is you can quantize them down to be very small and miraculously they work.\n\nUm, Llama for scout, for example, right? You can't really see the accuracy plot. Um, the smallest number is 80% accuracy on MLU5 shot, 80%. And the highest accuracy is 81 point something, right? So, it's actually only 1% difference and the one on the left is a one bit quant. It's very small, it's tiny in comparison to the full precision, like float 8. And so, essentially, you can make the model eight times smaller and you only decrease accuracy by 1%. Um, so that's very interesting. And so, essentially, we showcase that you can actually quantize layers, the mixture of experts layers, very heavily, but you must leave the attention layers, the shared experts, and other layers in higher precision, and that's called, that's what we call the dynamic quantization methodology.\n\nUm, if you see there was like a benchmark of Llama 4 scout, for example, um, if you use a two bit, a two-bit quant, it actually gets high accuracy than other other full precision, um, providers, which is very interesting, right? So, like, for example, the two-bit quant gets, um, 73% accuracy and then other inference providers get 65% accuracy, 67, right? There is a very large difference and so, okay, there is like some bugs in the models and there's... Maybe they quantize it incorrectly, but the goal is to show that if you quantize a model down to be very small bits, it still works.\n\nWe showcase this with an example. For example, if you take a vision model like, um, Quen 7 bill, uh, Quen 2 billion, um, if you naively quantize all the layers to be four bit, right? You ask the model, what does this image show? It will say, the image depicts a vibrant and colorful scene of a coastal area, which is totally wrong, right? The answer should be the image shows a train traveling on tracks or something like that, right? If you quantize everything to 4bit, it's 1.36GB, but it's definitely bad. So, the trick is you must quantize some layers to be... you must leave some layers to higher precision and you only need to increase it by 500 dB or so to 1.8 bit and it works. The image shows a train traveling on tracks. It suddenly works.\n\nBut the question is, which layers do you not quantize? That's the question, right? Which layers? You could do an exhaustive search, right? You can check, oh, let's not quantize layer zero. Let's check layer 1, layer two, you know, check every single one, but it'll take forever. So, definitely don't do that, right? You have 70 choose one or something. 70 choose one plus 70 choose two. Horrible, right? And remember, all of AI has a bad efficiency. So, don't do that. The trick is you can check the activation quantization error and the weight quantization error and you will see these large outliers. For example, for Quen, if you quantize the first few layers, it's extremely bad. So, you must leave the first few layers not quantized. And also, this gigantic jump for the weight quantization error. This means you probably shouldn't quantize that layer as well. There are some other plots that we show. For example, for Llama 3.2, it's interesting. All of these graphs are very different from each model. Um, you will notice Llama 3.2 has these weird, you know, continuous spikes. Um, it's because they use attention and then they put the attention back to the vision module. I think every single three... I think it was every single three layers. So, every single three layers it has these big jumps. Um, this means you should not quantize those layers.\n\nPixrol, for example, is also a difference graph again. Um, pixrol seems like you can't quantize many layers, unfortunately. Um, and so, like, the whole vision module must not be quantized. There is a very... there's a very important paper talking about like, you know, why which layers you should quantize and which layers you should not quantize. It's called the super weights paper. Um, you should... you guys definitely should read that. Essentially, it says that in all language models, the first few layers of the down projection, there is a very, very, very important number in one of the numbers. One of the numbers in the models, very, very, very important and you should never quantize it ever.\n\nBut the trick is that the interesting finding is it's not actually a very large number. So, there is a trend in the... there is a trend in, um, language model space where people think that you should not quantize outliers. The problem is, you know, these models have these big outliers, like suddenly in the model, there's like this big number, like 3,000, and if you quantize it, it essentially ruins the model, but actually this paper shows that it's not actually the outliers that are the problem. These numbers could be very small and if you look at the plots, if you remove... if you select these numbers and if you make them zero, the accuracy... uh, the accuracy decreases dramatically. Um, and so, like, if you see, like, for example, if you remove one of the numbers, um, you know, the activation value totally decreases, very bad, they have very large activation values and then if you remove them, it's very, very, very, very bad. Um, there is another trick that you can do. If you have a model that has seven billion parameters, make every single number go to zero. The first parameter, make it go to zero, check accuracy. The second number, make it go to zero. Check accuracy. The third number go to zero. Check accuracy. You can do this seven billion times and you can see which number is the most important. You could do that as well. Um, but remember, AI efficiency, very not a good idea. um, more later research. For example, the new Blackwell chips, um, instead of doing a quantization to like one bit, two bit, three bit, 4bit, NVIDIA chips also have this new architecture, this new format called FP4 or MXFP4. Um, and essentially, this is float 4. Um, and float 4 is very... it's... it's most likely going to be very used a lot in the future. Um, and then there's these like new formats for quantization as well, which which essentially allows you to train models in very low precision.\n\nI also made this plot, um, going from float 32. So, the question people always ask is why is GPUs getting faster and faster and faster? My take is actually this year is probably the last year you're going to get GPUs that is actually faster. There is no more faster GPUs. Why? Because the majority of GPU is getting faster is because of numerical precision. From float 32 to float 16, you get five times faster. Why? Why is it five times faster? Because because, um, the calculation of faster is when you use transistors, it's the exponent plus the mantissa squared, right? And float 32, you have to use 23 numbers for the mantissa and 23 squared is very large. Float 16, you reduce the mantissa to 10. And that is why you get five times speed up from float 32 to float 16. It's because the number itself is getting smaller. The representation inside the models for each of the weights is getting smaller. And then we moved from float 16 to be B float 16. It is again maybe around two times faster than float 16. We then have float 8. Um, float 8 is even more faster. Um, you know, uses even less space. Um, but then there is a problem. Float 4. We get to float four and it's around two times faster than float 8 around. Um, the problem is what's next? Do we go to float two, float three, float one? You know, I... there's... you can't push anymore in terms of numerical precision. There is not much more to go in terms of that space. And so, like, you can only get maybe 180 times faster than float 32, maybe 200 times faster, but essentially my take is float 4 might be the final flop that's getting faster. You know, the final, um, precision numerical precision and in the future GPUs are not going to get faster. Um, so maybe if you know people want to buy blackwell GPUs, that's probably you should probably buy them. It's most likely not going to get faster anymore. Um, that's kind of my take.\n\nAnd also for... Okay, I was going to talk about kernels and stuff. I don't think I have enough time. You must use torch.compile. You know, every single function that you see, wrap it in torso compile. Try it out. You know, like I... I always tell the PyTorch team, please make it by default. You know, definitely use torso compile. Um, why? Because it makes your training faster sometimes, only sometimes. Um, not all the time. It reduces memory usage most of the time. If you see bugs, they'll probably fix it. Um, but remember, torch.compile is not as easy as you think. You don't just do torch.compile the model. There is actually many options you can tune, right? I just listed a few options. Um, this is... this is literally just a few. There is like 10 more pages of options. I'm being serious. 10 more pages you can tune. Imagine if you can like use torch.compile and tune every single one. And that's why I highly suggest people to use torture compile more effectively. um, it's probably the biggest thing that can change your entire, you know, training run. Um, make a more more memory efficient, make it faster. So, definitely look through this. Um...\n\nokay, so in general, yes, thank you. Definitely start us on GitHub. Um, join our discord if you want to have any questions on RL and stuff. Um, we have a website as well. Um, and finally, we have stickers. Um, yes, there are some limited time stickers as well that we have somewhere, I think over there. Um, and remember, if you have any questions, I'm still going to stay around and ask. Um, yeah. Thanks a lot.\n\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.301Z"
}