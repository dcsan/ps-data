{
  "episodeId": "AbZ4IYGbfpQ",
  "channelSlug": "@aidotengineer",
  "title": "Netflix's Big Bet: One model to rule recommendations: Yesu Feng, Netflix",
  "publishedAt": "2025-07-16T18:00:36.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 3.54,
      "duration": 5.11
    },
    {
      "lang": "en",
      "text": "Uh good afternoon. Uh thank you uh",
      "offset": 15.12,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "Eugene for the introduction. Uh so today",
      "offset": 17.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh I'm going to share our big bet and",
      "offset": 21.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Netflix uh on personalization namely to",
      "offset": 23.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "use one foundation model to cover all",
      "offset": 26.64,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "the recommendation use cases.",
      "offset": 28.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Uh at Netflix we have diverse",
      "offset": 32.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "recommendation needs. Uh this is example",
      "offset": 34.32,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "homepage of one of profile on Netflix.",
      "offset": 37.04,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "Uh it's a 2D layout rows and items.",
      "offset": 42,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Diversity comes at uh at least three",
      "offset": 45.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "levels. Uh there is first level about",
      "offset": 47.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "row. We have diverse rows. We have",
      "offset": 51.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "genres for example rows on comedies,",
      "offset": 52.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "roles on action movies. We have rows",
      "offset": 55.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "about uh new trending uh just the",
      "offset": 58.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "release titles. We also have rows about",
      "offset": 61.12,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "for example titles only available on",
      "offset": 63.199,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Netflix. Um so that's the first",
      "offset": 65.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "dimension. Second dimension is of course",
      "offset": 68.799,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "of the the items or entities. uh in",
      "offset": 70.72,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "addition to traditionally movie and TV",
      "offset": 74.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "shows now we have games we have live",
      "offset": 76.479,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "streaming and we are going to add more",
      "offset": 79.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "so our content space is expanding to uh",
      "offset": 81.759,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "very heterogeneous content types",
      "offset": 86,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "the third level is page right so we have",
      "offset": 89.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "homepage we have search page we have a",
      "offset": 93.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "kids homepage which is tailored very",
      "offset": 95.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "differently toward kids interest uh",
      "offset": 98,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "mobile feed is a linear page page is not",
      "offset": 101.28,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "a 2D layout. Uh so on and so forth. So",
      "offset": 103.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "page different pages are also very",
      "offset": 106.399,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "diverse. Uh what happened traditionally",
      "offset": 108.32,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "was that these lead to naturally uh many",
      "offset": 111.84,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "specialized models that got developed",
      "offset": 116.24,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "over the years. Uh some models rank",
      "offset": 119.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "videos, some rank rows, some focus on",
      "offset": 122.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "for example shows user have not watched",
      "offset": 125.28,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "yet. Some uh focus on shows what user",
      "offset": 127.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "are already engaging.",
      "offset": 130.959,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "uh and many of those models are were",
      "offset": 133.2,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "built independently over the years. They",
      "offset": 135.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "may have different objectives uh but",
      "offset": 138.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "have a lot of overlaps as well.",
      "offset": 140.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Uh so naturally this lead to",
      "offset": 144,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "duplications",
      "offset": 146.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "uh uh duplications in our label",
      "offset": 147.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "engineering as well as feature",
      "offset": 151.12,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "engineering. Take the uh feature",
      "offset": 152.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "engineering as example. Uh we have this",
      "offset": 154.879,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "very commonly used factual data about",
      "offset": 157.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "user interaction history. uh the the",
      "offset": 159.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "factual data is the same but over the",
      "offset": 162.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "years many features are developed",
      "offset": 164.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "derived out of the same fact data like",
      "offset": 167.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "counts of different actions kinds of",
      "offset": 170,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "actions within various time window or",
      "offset": 171.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "other kind of uh slice and dice",
      "offset": 173.519,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "dimensions similarity between the users",
      "offset": 176,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "history titles against the target titles",
      "offset": 180,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "unique lastly like uh just a sequence of",
      "offset": 182.959,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "unique show ids uh to be used as a",
      "offset": 187.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "sequence feature into the model so This",
      "offset": 189.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "list can go on and on and a lot of those",
      "offset": 191.519,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "features uh because they are developed",
      "offset": 194.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "independently into each model they have",
      "offset": 197.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "slight variations but become very but",
      "offset": 199.599,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "largely uh very similar so become very",
      "offset": 202.8,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "hard to uh maintain. So the challenge",
      "offset": 205.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the challenge uh back then was uh is",
      "offset": 208.879,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "this scalable? Uh obviously not. If we",
      "offset": 211.36,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "keep expanding our landscape of content",
      "offset": 214.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "type or business use cases, it's not",
      "offset": 216.959,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "manageable to spin up new models for",
      "offset": 220.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "each uh individual use cases. Uh there's",
      "offset": 222.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "not much leverage. uh there's some",
      "offset": 226.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "shared components on building the",
      "offset": 228.799,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "feature label but still by and large",
      "offset": 230.72,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "each model uh basically uh spinned up",
      "offset": 232.959,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "independently and that also impact our",
      "offset": 236.239,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "uh innovation velocity in in the terms",
      "offset": 239.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that you don't reuse as much as you can",
      "offset": 241.519,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "instead you just spin up new models uh",
      "offset": 244,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "pretty much from scratch so this was the",
      "offset": 247.2,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "situation about four years ago uh at the",
      "offset": 250.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "beginning or middle of the pandemic so",
      "offset": 253.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the question we asked at that time was",
      "offset": 256.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "uh can we centralize the learning of",
      "offset": 258.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "user representation",
      "offset": 260.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "in one place.",
      "offset": 262.639,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "So the answer is yes and we had this key",
      "offset": 265.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "hypothesis that about foundation model",
      "offset": 268.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "based on transformer architecture. Uh",
      "offset": 270.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "concretely two hypothesis here. One",
      "offset": 273.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "hypothesis is that through scale up",
      "offset": 275.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "semi-supervised learning personalization",
      "offset": 277.28,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "can be improved. Uh the scaling law also",
      "offset": 279.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "applies to recommendation system as it",
      "offset": 282.479,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "applies to LLM. Uh second is that by",
      "offset": 285.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "integrating the foundation model into",
      "offset": 288.4,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "all systems we can create high leverage.",
      "offset": 290,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we can simultaneously improve all the",
      "offset": 291.759,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "downstream canvas facing models at the",
      "offset": 293.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "same time. So we'll see in the following",
      "offset": 296.8,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "slides how we validate those hypothesis.",
      "offset": 299.6,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Uh I I'll break up the overview into two",
      "offset": 303.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "sub sessions. First about dating data",
      "offset": 306.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and training and later uh second about",
      "offset": 308.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "application and serving.",
      "offset": 311.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "So um about data and training. So",
      "offset": 314.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "starting from data a very interesting",
      "offset": 316.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "aspect of building such foundation model",
      "offset": 318.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "auto reggressive transformer is that",
      "offset": 320.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there's a lot of analogy but also",
      "offset": 323.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "differences sometimes uh between this",
      "offset": 325.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and LLM. So we can transfer a lot of",
      "offset": 328.16,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "learnings inspirations from LLM uh uh",
      "offset": 331.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "development. Uh if we start from the",
      "offset": 335.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "very bottom layer which is basically",
      "offset": 337.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "data cleaning and tokenization.",
      "offset": 339.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "uh people work with LLM understand",
      "offset": 342,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "tokenization decisions have profound",
      "offset": 344.08,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "impact in your model quality. So uh",
      "offset": 346.479,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "although it's the bottom layer the",
      "offset": 350.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "decision you made there can percolate",
      "offset": 352.32,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "through all the downstream layers and",
      "offset": 354.16,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "manifest as either your model quality",
      "offset": 355.759,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "problem or model quality plus. So uh",
      "offset": 357.759,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "this applies to recommendation uh",
      "offset": 361.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "foundation model as well. uh instead of",
      "offset": 364.319,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh there are some differences very",
      "offset": 367.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "importantly instead of language tokens",
      "offset": 368.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "which is just the one ID here for uh if",
      "offset": 370.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we want to translate the user",
      "offset": 374,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "interaction history or sequence each of",
      "offset": 376.16,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "the token is a event interaction event",
      "offset": 379.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "from the user right but that event has",
      "offset": 381.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "many facets or many fields so it's not",
      "offset": 384.16,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "just one ID you can represent there are",
      "offset": 386.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a lot of rich information about the",
      "offset": 387.759,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "event so how you all of those fields can",
      "offset": 389.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "play a role in making the decision of",
      "offset": 393.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "tokenization. Uh I think that's what we",
      "offset": 395.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "need to consider very carefully. Um what",
      "offset": 398.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "is the granularity of tokenization and",
      "offset": 401.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "trade off that versus the context window",
      "offset": 403.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for example. Um and through many",
      "offset": 406,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "iterations we reach the right I think",
      "offset": 408.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "reach the right abstraction and",
      "offset": 410.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "interfaces that we can use to uh adjust",
      "offset": 411.919,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "our tokenization to different use cases.",
      "offset": 415.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "For example, you can imagine we have a",
      "offset": 417.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "tokeniz one version of tokenization used",
      "offset": 419.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "for pre-training for fine-tuning against",
      "offset": 421.039,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a specific application. We apply",
      "offset": 423.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "slightly different tokenization.",
      "offset": 425.599,
      "duration": 8.241
    },
    {
      "lang": "en",
      "text": "Um so moving up from the tokenization",
      "offset": 428.88,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "layer uh then becomes the model layers.",
      "offset": 433.84,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "uh at high level uh from bottom to top",
      "offset": 437.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "we go through the uh event",
      "offset": 440.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "representation",
      "offset": 443.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "uh embedding layer transformer layer and",
      "offset": 445.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the objective layer. So event",
      "offset": 447.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "representation as we just briefly",
      "offset": 450.24,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "touched upon uh many information in the",
      "offset": 452.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "event but at high level you can break it",
      "offset": 455.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "down by when where and what when that",
      "offset": 457.599,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "event happened that's about time",
      "offset": 460.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "encoding and where it happened it's",
      "offset": 462.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "about the physical location your local",
      "offset": 464.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "country so forth but also about device",
      "offset": 466.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "about the uh canvas or which row which",
      "offset": 469.28,
      "duration": 7.759
    },
    {
      "lang": "en",
      "text": "page this action happened uh and then uh",
      "offset": 473.28,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "what basically is about the target",
      "offset": 477.039,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "entity or the title which title you",
      "offset": 479.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "interacted with what is the interaction",
      "offset": 481.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "how long and uh any that kind of",
      "offset": 483.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "information associated with the action.",
      "offset": 487.36,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "So um that's where the we need to decide",
      "offset": 490,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "what information we need to keep what we",
      "offset": 493.599,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "should drop so forth. uh moving one",
      "offset": 495.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "layer above uh the embedding feature",
      "offset": 498.879,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "transformation layer. Uh one thing that",
      "offset": 501.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "needs to be pointed out is that for",
      "offset": 504.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "recommendation we need to combine ID",
      "offset": 506,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "embedding learning with other semantic",
      "offset": 508.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "content information. Um if you only have",
      "offset": 511.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "ID embedding learn from scratch in the",
      "offset": 514.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "model then you have problem with co-star",
      "offset": 516,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "meaning that titles the model hasn't",
      "offset": 517.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "seen during training it doesn't know how",
      "offset": 520.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to deal with it at inference time. So we",
      "offset": 522.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "need to have uh semantic content",
      "offset": 524.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "information to be uh uh comp",
      "offset": 526.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "complementaryary to those ID embeddings.",
      "offset": 529.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Uh this is not a problem for LLM but",
      "offset": 532.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "very commonly encountered the co-star",
      "offset": 534.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "problem for re recommendation system. Uh",
      "offset": 536.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "uh transformer layer I think there's no",
      "offset": 539.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "need to talk too much into this in terms",
      "offset": 541.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of architecture choices optimization so",
      "offset": 543.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "on and so forth. The only thing that I",
      "offset": 545.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "want to point out is that uh we are",
      "offset": 546.959,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "using the hidden state output from this",
      "offset": 550.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "layer as our user representation which",
      "offset": 552.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is one of the primary goal of the",
      "offset": 554.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "foundation model is to learn a good",
      "offset": 556.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "long-term user representation. Then uh",
      "offset": 557.839,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "we need to put this into context. Then",
      "offset": 561.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "things to consider are for example how",
      "offset": 563.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "stable is our user user representation",
      "offset": 565.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "given our user profile user interaction",
      "offset": 568,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "history keep changing. How do we",
      "offset": 570.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "guarantee or maintain the stability of",
      "offset": 572.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that representation and what kind of",
      "offset": 574.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "aggregation we should use? You can think",
      "offset": 576.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of broadly aggregate across the time",
      "offset": 578.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "dimension in terms of sequence dimension",
      "offset": 581.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "or aggregate uh across the layers. You",
      "offset": 583.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "have multiple self attention layer. How",
      "offset": 587.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "do you aggregate that? Um and then",
      "offset": 588.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "lastly, do we need to do explicit",
      "offset": 591.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "adaptation of the representation based",
      "offset": 593.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "on our downstream objective to fine-tune",
      "offset": 596,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it?",
      "offset": 598.08,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "Um so then we move to last uh the very",
      "offset": 600.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "top layer objective loss function. This",
      "offset": 603.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is also very interesting in the sense",
      "offset": 605.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that it's much richer than LLM because",
      "offset": 607.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "you can see first we use uh instead of",
      "offset": 610.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "one sequence but multiple sequence to",
      "offset": 612.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "represent the output because you can",
      "offset": 614.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "have a sequence of entity ids that's",
      "offset": 616.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "your like uh next token prediction",
      "offset": 619.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "softmax or sample softmax but then we",
      "offset": 622.56,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "have many other facets of field of each",
      "offset": 625.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "event that can be also used as a target",
      "offset": 628.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "right so it could be for things like uh",
      "offset": 631.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "action type It could be some aspect of",
      "offset": 634.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the entity's metadata like entity type,",
      "offset": 636.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "yarn, language, so on so forth and also",
      "offset": 638.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "about your action like prediction of the",
      "offset": 641.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "duration or uh the device where the",
      "offset": 644.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "action happen or the time when the next",
      "offset": 646.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "uh user play will happen. So those are",
      "offset": 650.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "all legitimate uh targets or labels",
      "offset": 653.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "depends on your use case you can use",
      "offset": 656.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "them to do the finetuning. Now instead",
      "offset": 657.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of so you can cast the problem as a",
      "offset": 659.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "multitask learning problem multi head or",
      "offset": 661.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "hierarchical prediction but you can also",
      "offset": 664.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "use them just as your weights your",
      "offset": 666.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "rewards or your mask on the loss",
      "offset": 669.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "function. So in terms of to adapt the",
      "offset": 671.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "model to zooming into one subcategory of",
      "offset": 673.279,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh user behavior you want to you want",
      "offset": 677.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the model to learn. Okay. So that's",
      "offset": 679.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "about the model architecture that I want",
      "offset": 681.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to talk about.",
      "offset": 684.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Um so does it scale? The first question",
      "offset": 686.32,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "part of the first hypothesis we want to",
      "offset": 690.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "answer is does a scaling law apply and I",
      "offset": 692.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "think the answer is yes. So this is over",
      "offset": 695.36,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the uh roughly two two to two and a half",
      "offset": 697.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "years we were scaling up and we",
      "offset": 701.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "constantly still see the gain uh from",
      "offset": 704.32,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "only on the order of 10 million profile",
      "offset": 708.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "or a few million profile to now on the",
      "offset": 711.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "order of 1 billion uh model parameters.",
      "offset": 713.2,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "We scale up the data accordingly. Um now",
      "offset": 716.56,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "we stop here because we can still keep",
      "offset": 721.279,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "going but uh as you may realize that",
      "offset": 724.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "recommendation system usually have much",
      "offset": 727.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "stringent latency cost requirement. So",
      "offset": 729.36,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "scaling up scaling up more require us to",
      "offset": 733.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "also distill back. Yeah. But certainly I",
      "offset": 735.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "think this is not the end of the scaling",
      "offset": 738.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "law.",
      "offset": 740,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Uh before we wrapping up the data and",
      "offset": 741.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "training discussion I would like to",
      "offset": 743.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "highlight some of the learnings I think",
      "offset": 745.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "quite interesting we borrow from LLM.",
      "offset": 746.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "This is not exhaustive list but uh uh I",
      "offset": 748.56,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "think very interesting to me uh the top",
      "offset": 752,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "three one is top multi-token prediction.",
      "offset": 755.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "You may have seen this in the deepseek",
      "offset": 758.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "paper so on and so forth. So you can",
      "offset": 760.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "implementation wise you can use multi",
      "offset": 762.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "head multi- label so and different",
      "offset": 763.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "implementation flavor but the goal is",
      "offset": 766.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "really to force the model to be less",
      "offset": 768.639,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "myopic more robust to serving time shift",
      "offset": 770.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "because you have a time gap between your",
      "offset": 774.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "training and serving and also force the",
      "offset": 775.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "model to targets long-term user",
      "offset": 779.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "satisfaction and long-term user behavior",
      "offset": 780.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "instead of just focus on next action. Uh",
      "offset": 782.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I we have observed a very notable uh",
      "offset": 786,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "metrics improvement by doing that.",
      "offset": 788.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "uh the second is multi-layer",
      "offset": 791.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "representation which uh I touched upon",
      "offset": 793.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "on the profile representation. So this",
      "offset": 795.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is also translated from LLM side of",
      "offset": 797.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "techniques of layer wise supervision,",
      "offset": 800.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "self-distillation or multi-layer output",
      "offset": 801.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "aggregation. The goal here is really to",
      "offset": 804.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "make a better and more stable user",
      "offset": 806.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "representation.",
      "offset": 808.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Lastly, u this is also should be no",
      "offset": 810.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "surprise, long context window handling",
      "offset": 813.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "from truncated sliding window to sparse",
      "offset": 815.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "attention to progressively training uh",
      "offset": 818.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "longer and longer sequences uh to",
      "offset": 820.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "eventually all of the parallelism",
      "offset": 823.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "strategies. So this is about more",
      "offset": 825.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "efficient training and maximize the",
      "offset": 827.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "learning.",
      "offset": 829.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Okay. So uh shift gear to talk about the",
      "offset": 831.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "serving and applications.",
      "offset": 834.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "uh before the foundation model FM uh",
      "offset": 836.639,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "this is roughly the algo stack we have",
      "offset": 839.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "for personalization many data many",
      "offset": 842.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "features many models independently",
      "offset": 844.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "developed each serving multiple or one",
      "offset": 846.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "canvases or applications we call",
      "offset": 849.44,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "now with the foundation model we",
      "offset": 852.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "consolidate largely the data and",
      "offset": 854.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "representation layer especially the user",
      "offset": 857.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "representation as well as content",
      "offset": 859.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "representation in the personalization",
      "offset": 861.519,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "domain uh model layer as well because",
      "offset": 864.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "model now each application model now are",
      "offset": 867.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "built on top of FM so become a thinner",
      "offset": 869.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "layer instead of a very standalone",
      "offset": 872.079,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "full-fledged model train from scratch.",
      "offset": 874.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "So how do the various models utilize a",
      "offset": 877.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "foundation model? Um there are three",
      "offset": 879.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "main approaches uh or consumption",
      "offset": 882,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "patterns. Okay, the first is foundation",
      "offset": 884.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "model can be integrated as a subgraph",
      "offset": 887.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "within the downstream model. uh",
      "offset": 889.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "additionally the content embeddings",
      "offset": 891.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "learned from the foundation model can be",
      "offset": 893.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "integrated as the embedding lookup",
      "offset": 894.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "layers. So downstream model is a neuron",
      "offset": 896.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "network uh may already have initially",
      "offset": 898.48,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "some of the sequence transformer uh",
      "offset": 902.079,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "tower or graph and then using a",
      "offset": 905.519,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "pre-trained foundation model subgraph to",
      "offset": 909.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "directly replace that.",
      "offset": 912.079,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "Uh second is that uh we can push out",
      "offset": 914.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "embeddings. This is no surprise from",
      "offset": 917.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "both content side and entity embedding",
      "offset": 919.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "as well as member embeddings. Uh the",
      "offset": 921.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "only the main concern here of course is",
      "offset": 923.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "how we want to re how frequently we want",
      "offset": 926.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to refresh the member embeddings and how",
      "offset": 928.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "we make sure they are stable. Uh and",
      "offset": 931.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "push them to the centralized embedding",
      "offset": 934.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "store. And this of course allow far more",
      "offset": 936.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "uh wider use cases than just the",
      "offset": 939.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "personalization because people analytics",
      "offset": 941.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "data scientists can also just fetch",
      "offset": 944.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "those embeddings directly to do the",
      "offset": 946.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "things that they want.",
      "offset": 948.72,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "Finally user can u extract the models",
      "offset": 951.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and fine-tune it for specific",
      "offset": 954.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "applications either fine-tune or they",
      "offset": 956.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "need to do distillation to meet the uh",
      "offset": 958.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "online serving uh requirement.",
      "offset": 960.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "um especially for those with a very",
      "offset": 963.839,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "strict latency requirement.",
      "offset": 965.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "To wrap up uh I want to show at high",
      "offset": 969.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "level the wings we accumulated over the",
      "offset": 972.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "last one year and a half uh by",
      "offset": 974.32,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "incorporating FM into various places. So",
      "offset": 977.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the blue bar represent how many",
      "offset": 981.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "applications have FM incorporated. The",
      "offset": 983.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "green bar represent the AB test swings",
      "offset": 986,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "because in any application we may have",
      "offset": 988.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "multiple AB tests going on there to have",
      "offset": 991.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "wings. So we see we indeed see high",
      "offset": 993.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "leverage of FM to bring about both AB",
      "offset": 996.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "test wings as well as infrastructure",
      "offset": 999.12,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "consolidation.",
      "offset": 1001.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Uh so I think the big back uh big bets",
      "offset": 1004.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "are validated. Uh it is a scalable",
      "offset": 1007.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "solution uh in terms of both both in",
      "offset": 1009.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "terms of a scalable scale up the model",
      "offset": 1012.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "with improved quality as well as make",
      "offset": 1015.04,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the whole infra consolidated and scale",
      "offset": 1017.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh to new applications to be much",
      "offset": 1020.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "easier. high leverage because it's a",
      "offset": 1022.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "centralized learning. Innovation",
      "offset": 1024.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "velocity also is faster because we allow",
      "offset": 1026.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "a lot of newly uh launched applications",
      "offset": 1029.36,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "to directly fine-tune the foundation",
      "offset": 1032.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "model to uh launch the first experience.",
      "offset": 1034.799,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "Uh so the current directions um one is",
      "offset": 1038.64,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "that um we want to have a universal",
      "offset": 1042.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "representation for heterogeneous",
      "offset": 1045.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "entities. This is uh as you can guess",
      "offset": 1046.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "the semantic ID and along those lines",
      "offset": 1049.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "because we want to cover that uh as",
      "offset": 1051.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "Netflix is expanding to very different",
      "offset": 1054.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "very heterogeneous content types. Uh",
      "offset": 1056.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "second is generative retrieval for",
      "offset": 1059.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "collection recommendation right so",
      "offset": 1061.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "instead of just recommend a single video",
      "offset": 1062.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "be generative at inference time and",
      "offset": 1064.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "serving time because you have a",
      "offset": 1066.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "multi-step decoding a lot of the",
      "offset": 1068.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "consideration about business business",
      "offset": 1070.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "rules or diversity for example can be",
      "offset": 1072.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "naturally handled in the decoding",
      "offset": 1074.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "process lastly faster adaptation through",
      "offset": 1076.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "prompt tuning so this is also borrowed",
      "offset": 1079.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "from LLM can we just train some soft",
      "offset": 1081.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "tokens so that at inference time we can",
      "offset": 1084.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "directly swap in and out the soft tokens",
      "offset": 1087.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "to prompt the FM to behave differently.",
      "offset": 1089.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So that is also a very promising",
      "offset": 1093.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "direction that we are getting into. All",
      "offset": 1094.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "right, that concludes my talk. Thank you",
      "offset": 1097.36,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "for your attention and questions.",
      "offset": 1099.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Thank you. Um if you have any questions,",
      "offset": 1107.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "may I invite you to come to the mics in",
      "offset": 1108.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "front um while we get our next speakers",
      "offset": 1110.48,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "from Mr. K.",
      "offset": 1112.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Uh hi, thank you for the talk. Uh since",
      "offset": 1117.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "you get billions of users, so except the",
      "offset": 1120.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "recommendation system, you maybe it can",
      "offset": 1124.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "do much more, right? So what's your",
      "offset": 1126.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "thought on that? Since I can just ask it",
      "offset": 1129.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to to predict who's the next president",
      "offset": 1131.84,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "in the United States. Thank you. Um so I",
      "offset": 1134.08,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "actually don't uh could you explain a",
      "offset": 1138.559,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "little bit what do you mean by beyond",
      "offset": 1140.559,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "recommendation? Do you mean the other",
      "offset": 1141.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "personalization or other things? Yeah.",
      "offset": 1143.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Um yeah, since you get kind of beating",
      "offset": 1146.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "users preference. So actually that that",
      "offset": 1149.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that preference is also been leaning to",
      "offset": 1152.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "what things they buy or who they will",
      "offset": 1155.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "vote for the next president. So do you",
      "offset": 1157.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "think your foundation model has that",
      "offset": 1160.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "capability to to expand not only",
      "offset": 1162.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "recommendation what videos they want to",
      "offset": 1164.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "look what others they like or what's",
      "offset": 1165.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "their opinions on anything else? Thank",
      "offset": 1168.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you. Yes. So I think we are expanding to",
      "offset": 1170.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "different uh I think entity type and",
      "offset": 1173.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "also capture",
      "offset": 1176,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "uh users taste from both on and off our",
      "offset": 1178.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "platform. I think that's a general trend",
      "offset": 1181.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that we're going to.",
      "offset": 1183.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Yes. Okay.",
      "offset": 1185.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Great. Thank you. This was really",
      "offset": 1188.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "helpful. Um question on and you might",
      "offset": 1190.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "not be able to share it um for IP",
      "offset": 1192.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "reasons but whatever you can. Uh",
      "offset": 1194.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "thoughts on graph models. Didn't I",
      "offset": 1196.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "didn't hear a lot of that in your talks.",
      "offset": 1199.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "graphs and uh reinforcement learning any",
      "offset": 1201.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "utilization there any benefits you saw",
      "offset": 1204.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "any boost in in performance and accuracy",
      "offset": 1206.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "yeah that's a very good question I think",
      "offset": 1208.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "we have actually uh a dedicated team sub",
      "offset": 1210.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "team doing graph model uh especially",
      "offset": 1214.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "around our knowledge graph to cover the",
      "offset": 1216.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "content space both on and off our",
      "offset": 1219.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "platform in the whole entire",
      "offset": 1221.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "entertainment ecosystem. So we use",
      "offset": 1223.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "actually a lot of embeddings for example",
      "offset": 1226.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "from the graph model to co-start that's",
      "offset": 1229.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "where I see I show those semantic",
      "offset": 1232.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "embeddings that's where it comes from in",
      "offset": 1234.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "terms of reinforcement learning yes as",
      "offset": 1236.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "well especially where we consider sparse",
      "offset": 1237.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "reward that we have on users from users",
      "offset": 1241.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "action are pretty much sparse but we",
      "offset": 1244.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "want to use them to guide how for",
      "offset": 1246.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "example we generate the whole collection",
      "offset": 1249.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that's where we need to consider how to",
      "offset": 1252.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "use those reward to guide those uh",
      "offset": 1254.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "process.",
      "offset": 1256.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1257.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Can I ask you two part questions? Sure.",
      "offset": 1260.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "I will be here and so we can also",
      "offset": 1263.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "follow. Yeah.",
      "offset": 1264.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Do you also use these unified",
      "offset": 1267.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "representations as embedding features to",
      "offset": 1268.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "downstream models?",
      "offset": 1271.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "You had a slide how you use the unified",
      "offset": 1273.2,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "model. Yeah. So uh the",
      "offset": 1275.2,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "we so for the embeddings learning within",
      "offset": 1280.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "our model we also expose to downstream",
      "offset": 1282.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to direct consume them. Uh we also have",
      "offset": 1284.64,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "a to train our unified embedding we also",
      "offset": 1287.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "have some upstream like just for example",
      "offset": 1291.039,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the GN embeddings that those are also",
      "offset": 1293.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "consumed to to do that.",
      "offset": 1296.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Last one is it fast?",
      "offset": 1299.12,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "Uh hello. Uh in your embeddings, are you",
      "offset": 1302.159,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "just using when someone does an action",
      "offset": 1306.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "or sorry for the in these embeddings are",
      "offset": 1308.64,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "you just using metadata over the video",
      "offset": 1312.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to understand what they like or are you",
      "offset": 1313.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "actually using like frame by frame of",
      "offset": 1315.919,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "the video or second clips? Uh not yet.",
      "offset": 1318.08,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "We do have that from some other content",
      "offset": 1322.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "group of our organization but I think",
      "offset": 1325.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the trend will go there. So we are not",
      "offset": 1328.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "yet uh into very granular sub like clips",
      "offset": 1330.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "level or view level. We have those",
      "offset": 1334.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "embeddings but not quite yet to",
      "offset": 1335.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "incorporate. Yep. Thank you. Thank you.",
      "offset": 1337.52,
      "duration": 5.09
    },
    {
      "lang": "en",
      "text": "Uh please another round of applause for",
      "offset": 1340.08,
      "duration": 9.62
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1342.61,
      "duration": 7.09
    }
  ],
  "cleanText": "[Music]\nUh, good afternoon.\nUh, thank you, Eugene, for the introduction.\nUh, so today, uh, I'm going to share our big bet at Netflix on personalization, namely, to use one foundation model to cover all the recommendation use cases.\nUh, at Netflix, we have diverse recommendation needs.\nUh, this is an example homepage of one profile on Netflix.\nUh, it's a 2D layout: rows and items.\nDiversity comes at, uh, at least three levels.\nUh, there is a first level about rows.\nWe have diverse rows.\nWe have genres, for example, rows on comedies, rows on action movies.\nWe have rows about, uh, new trending, uh, just the release titles.\nWe also have rows about, uh, for example, titles only available on Netflix.\nUm, so that's the first dimension.\nThe second dimension is, of course, of the items or entities.\nUh, in addition to traditionally movie and TV shows, now we have games, we have live streaming, and we are going to add more, so our content space is expanding to, uh, very heterogeneous content types.\nThe third level is page, right?\nSo we have homepage, we have search page, we have a kids homepage, which is tailored very differently toward kids' interests.\nUh, mobile feed is a linear page; page is not a 2D layout.\nUh, so on and so forth.\nSo, page, different pages are also very diverse.\nUh, what happened traditionally was that these led to naturally, uh, many specialized models that got developed over the years.\nUh, some models rank videos, some rank rows, some focus on, for example, shows users have not watched yet.\nSome, uh, focus on shows what users are already engaging.\nUh, and many of those models were built independently over the years.\nThey may have different objectives, uh, but have a lot of overlaps as well.\nUh, so naturally, this led to duplications, uh, duplications in our label engineering as well as feature engineering.\nTake the, uh, feature engineering as an example.\nUh, we have this very commonly used factual data about user interaction history.\nUh, the factual data is the same, but over the years, many features are developed, derived out of the same fact data, like counts of different actions, kinds of actions within various time windows, or other kind of, uh, slice and dice dimensions, similarity between the users' history titles against the target titles, unique, lastly, like, uh, just a sequence of unique show IDs, uh, to be used as a sequence feature into the model.\nSo this list can go on and on, and a lot of those features, uh, because they are developed independently into each model, they have slight variations, but become very, but largely, uh, very similar, so become very hard to, uh, maintain.\nSo the challenge, the challenge, uh, back then was, uh, is this scalable?\nUh, obviously not.\nIf we keep expanding our landscape of content type or business use cases, it's not manageable to spin up new models for each, uh, individual use cases.\nUh, there's not much leverage.\nUh, there's some shared components on building the feature label, but still, by and large, each model, uh, basically, uh, spinned up independently, and that also impacts our, uh, innovation velocity in the terms that you don't reuse as much as you can; instead, you just spin up new models, uh, pretty much from scratch.\nSo this was the situation about four years ago, uh, at the beginning or middle of the pandemic.\nSo the question we asked at that time was, uh, can we centralize the learning of user representation in one place?\nSo the answer is yes, and we had this key hypothesis that about foundation model based on transformer architecture.\nUh, concretely, two hypotheses here.\nOne hypothesis is that through scale-up semi-supervised learning, personalization can be improved.\nUh, the scaling law also applies to recommendation systems as it applies to LLMs.\nUh, second is that by integrating the foundation model into all systems, we can create high leverage.\nWe can simultaneously improve all the downstream canvas-facing models at the same time.\nSo we'll see in the following slides how we validate those hypotheses.\nUh, I, I'll break up the overview into two sub-sessions.\nFirst about dating data and training, and later, uh, second about application and serving.\nSo, um, about data and training.\nSo, starting from data, a very interesting aspect of building such foundation model, auto-regressive transformer, is that there's a lot of analogy, but also differences sometimes, uh, between this and LLMs.\nSo we can transfer a lot of learnings, inspirations from LLM, uh, uh, development.\nUh, if we start from the very bottom layer, which is basically data cleaning and tokenization, uh, people work with LLMs understand tokenization decisions have a profound impact in your model quality.\nSo, uh, although it's the bottom layer, the decision you made there can percolate through all the downstream layers and manifest as either your model quality problem or model quality plus.\nSo, uh, this applies to recommendation, uh, foundation model as well.\nUh, instead of, uh, there are some differences, very importantly, instead of language tokens, which is just the one ID here for, uh, if we want to translate the user interaction history or sequence, each of the token is an event interaction event from the user, right?\nBut that event has many facets or many fields, so it's not just one ID you can represent; there are a lot of rich information about the event, so how you all of those fields can play a role in making the decision of tokenization.\nUh, I think that's what we need to consider very carefully.\nUm, what is the granularity of tokenization and trade off that versus the context window, for example?\nUm, and through many iterations, we reach the right, I think, reach the right abstraction and interfaces that we can use to, uh, adjust our tokenization to different use cases.\nFor example, you can imagine we have a tokeniz one version of tokenization used for pre-training, for fine-tuning against a specific application.\nWe apply slightly different tokenization.\nUm, so moving up from the tokenization layer, uh, then becomes the model layers.\nUh, at high level, uh, from bottom to top, we go through the, uh, event representation, uh, embedding layer, transformer layer, and the objective layer.\nSo event representation, as we just briefly touched upon, uh, many information in the event, but at high level, you can break it down by when, where, and what.\nWhen that event happened, that's about time encoding, and where it happened, it's about the physical location, your local country, so forth, but also about device, about the, uh, canvas or which row, which page this action happened, uh, and then, uh, what basically is about the target entity or the title, which title you interacted with, what is the interaction, how long, and, uh, any that kind of information associated with the action.\nSo, um, that's where the we need to decide what information we need to keep, what we should drop, so forth.\nUh, moving one layer above, uh, the embedding feature transformation layer.\nUh, one thing that needs to be pointed out is that for recommendation, we need to combine ID embedding learning with other semantic content information.\nUm, if you only have ID embedding learn from scratch in the model, then you have a problem with co-star, meaning that titles the model hasn't seen during training, it doesn't know how to deal with it at inference time.\nSo we need to have, uh, semantic content information to be, uh, uh, complementary to those ID embeddings.\nUh, this is not a problem for LLMs, but very commonly encountered the co-star problem for recommendation systems.\nUh, uh, transformer layer, I think there's no need to talk too much into this in terms of architecture choices, optimization, so on and so forth.\nThe only thing that I want to point out is that, uh, we are using the hidden state output from this layer as our user representation, which is one of the primary goals of the foundation model is to learn a good long-term user representation.\nThen, uh, we need to put this into context.\nThen things to consider are, for example, how stable is our user user representation, given our user profile, user interaction history keep changing.\nHow do we guarantee or maintain the stability of that representation, and what kind of aggregation we should use?\nYou can think of broadly aggregate across the time dimension in terms of sequence dimension or aggregate, uh, across the layers.\nYou have multiple self-attention layers.\nHow do you aggregate that?\nUm, and then lastly, do we need to do explicit adaptation of the representation based on our downstream objective to fine-tune it?\nUm, so then we move to last, uh, the very top layer, objective loss function.\nThis is also very interesting in the sense that it's much richer than LLMs because you can see first, we use, uh, instead of one sequence, but multiple sequences to represent the output because you can have a sequence of entity IDs, that's your like, uh, next token prediction, softmax or sample softmax, but then we have many other facets of field of each event that can be also used as a target, right?\nSo it could be for things like, uh, action type.\nIt could be some aspect of the entity's metadata, like entity type, yarn, language, so on and so forth, and also about your action, like prediction of the duration or, uh, the device where the action happened or the time when the next, uh, user play will happen.\nSo those are all legitimate, uh, targets or labels, depends on your use case, you can use them to do the finetuning.\nNow, instead of, so you can cast the problem as a multitask learning problem, multi-head or hierarchical prediction, but you can also use them just as your weights, your rewards, or your mask on the loss function.\nSo in terms of to adapt the model to zooming into one subcategory of, uh, user behavior you want to, you want the model to learn.\nOkay.\nSo that's about the model architecture that I want to talk about.\nUm, so does it scale?\nThe first question, part of the first hypothesis we want to answer is, does a scaling law apply?\nAnd I think the answer is yes.\nSo this is over the, uh, roughly two, two to two and a half years we were scaling up, and we constantly still see the gain, uh, from only on the order of 10 million profile or a few million profile to now on the order of 1 billion, uh, model parameters.\nWe scale up the data accordingly.\nUm, now we stop here because we can still keep going, but, uh, as you may realize that recommendation systems usually have much stringent latency cost requirement.\nSo scaling up, scaling up more require us to also distill back.\nYeah.\nBut certainly, I think this is not the end of the scaling law.\nUh, before we wrapping up the data and training discussion, I would like to highlight some of the learnings I think quite interesting we borrow from LLMs.\nThis is not an exhaustive list, but, uh, uh, I think very interesting to me, uh, the top three.\nOne is top multi-token prediction.\nYou may have seen this in the DeepSeek paper, so on and so forth.\nSo you can implementation wise, you can use multi-head, multi-label, so and different implementation flavor, but the goal is really to force the model to be less myopic, more robust to serving time shift because you have a time gap between your training and serving, and also force the model to targets long-term user satisfaction and long-term user behavior instead of just focus on next action.\nUh, I, we have observed a very notable, uh, metrics improvement by doing that.\nUh, the second is multi-layer representation, which, uh, I touched upon on the profile representation.\nSo this is also translated from LLM side of techniques of layer-wise supervision, self-distillation, or multi-layer output aggregation.\nThe goal here is really to make a better and more stable user representation.\nLastly, u, this is also should be no surprise, long context window handling from truncated sliding window to sparse attention to progressively training, uh, longer and longer sequences, uh, to eventually all of the parallelism strategies.\nSo this is about more efficient training and maximize the learning.\nOkay.\nSo, uh, shift gear to talk about the serving and applications.\nUh, before the foundation model, FM, uh, this is roughly the algo stack we have for personalization: many data, many features, many models independently developed, each serving multiple or one canvases or applications, we call.\nNow, with the foundation model, we consolidate largely the data and representation layer, especially the user representation as well as content representation in the personalization domain, uh, model layer as well because model now, each application model now are built on top of FM, so become a thinner layer instead of a very standalone, full-fledged model train from scratch.\nSo how do the various models utilize a foundation model?\nUm, there are three main approaches, uh, or consumption patterns.\nOkay, the first is foundation model can be integrated as a subgraph within the downstream model.\nUh, additionally, the content embeddings learned from the foundation model can be integrated as the embedding lookup layers.\nSo downstream model is a neuron network, uh, may already have initially some of the sequence transformer, uh, tower or graph, and then using a pre-trained foundation model subgraph to directly replace that.\nUh, second is that, uh, we can push out embeddings.\nThis is no surprise from both content side and entity embedding as well as member embeddings.\nUh, the only the main concern here, of course, is how we want to re, how frequently we want to refresh the member embeddings and how we make sure they are stable.\nUh, and push them to the centralized embedding store.\nAnd this, of course, allow far more, uh, wider use cases than just the personalization because people analytics, data scientists can also just fetch those embeddings directly to do the things that they want.\nFinally, users can, u, extract the models and fine-tune it for specific applications, either fine-tune or they need to do distillation to meet the, uh, online serving, uh, requirement, um, especially for those with a very strict latency requirement.\nTo wrap up, uh, I want to show at high level the wings we accumulated over the last one year and a half, uh, by incorporating FM into various places.\nSo the blue bar represent how many applications have FM incorporated.\nThe green bar represent the AB test swings because in any application, we may have multiple AB tests going on there to have wings.\nSo we see we indeed see high leverage of FM to bring about both AB test wings as well as infrastructure consolidation.\nUh, so I think the big back, uh, big bets are validated.\nUh, it is a scalable solution, uh, in terms of both, both in terms of a scalable scale up the model with improved quality as well as make the whole infra consolidated and scale, uh, to new applications to be much easier.\nHigh leverage because it's a centralized learning.\nInnovation velocity also is faster because we allow a lot of newly, uh, launched applications to directly fine-tune the foundation model to, uh, launch the first experience.\nUh, so the current directions, um, one is that, um, we want to have a universal representation for heterogeneous entities.\nThis is, uh, as you can guess, the semantic ID, and along those lines, because we want to cover that, uh, as Netflix is expanding to very different, very heterogeneous content types.\nUh, second is generative retrieval for collection recommendation, right?\nSo instead of just recommend a single video, be generative at inference time and serving time because you have a multi-step decoding, a lot of the consideration about business, business rules or diversity, for example, can be naturally handled in the decoding process.\nLastly, faster adaptation through prompt tuning, so this is also borrowed from LLMs.\nCan we just train some soft tokens so that at inference time, we can directly swap in and out the soft tokens to prompt the FM to behave differently?\nSo that is also a very promising direction that we are getting into.\nAll right, that concludes my talk.\nThank you for your attention and questions.\nThank you.\nUm, if you have any questions, may I invite you to come to the mics in front, um, while we get our next speakers from Mr. K.\nUh, hi, thank you for the talk.\nUh, since\n\n\nYou get billions of users, so except the recommendation system, you maybe it can do much more, right?\nSo what's your thought on that?\nSince I can just ask it to predict who's the next president in the United States.\nThank you.\nUm, so I actually don't.\nUh, could you explain a little bit what do you mean by beyond recommendation?\nDo you mean the other personalization or other things?\nYeah.\nUm, yeah, since you get kind of beating users preference.\nSo actually that that preference is also been leaning to what things they buy or who they will vote for the next president.\nSo do you think your foundation model has that capability to expand not only recommendation what videos they want to look, what others they like, or what's their opinions on anything else?\nThank you.\nYes.\nSo I think we are expanding to different, uh, I think entity type and also capture, uh, users taste from both on and off our platform.\nI think that's a general trend that we're going to.\nYes.\nOkay.\nGreat.\nThank you.\nThis was really helpful.\nUm, question on, and you might not be able to share it, um, for IP reasons, but whatever you can.\nUh, thoughts on graph models.\nDidn't I didn't hear a lot of that in your talks.\nGraphs and, uh, reinforcement learning, any utilization there, any benefits you saw, any boost in in performance and accuracy?\nYeah, that's a very good question.\nI think we have actually, uh, a dedicated team, sub team doing graph model, uh, especially around our knowledge graph to cover the content space both on and off our platform in the whole entire entertainment ecosystem.\nSo we use actually a lot of embeddings, for example, from the graph model to co-start, that's where I see, I show those semantic embeddings, that's where it comes from.\nIn terms of reinforcement learning, yes, as well, especially where we consider sparse reward that we have on users from users action are pretty much sparse, but we want to use them to guide how, for example, we generate the whole collection, that's where we need to consider how to use those reward to guide those, uh, process.\nYeah.\nCan I ask you two part questions?\nSure.\nI will be here and so we can also follow.\nYeah.\nDo you also use these unified representations as embedding features to downstream models?\nYou had a slide how you use the unified model.\nYeah.\nSo, uh, the, we, so for the embeddings learning within our model, we also expose to downstream to direct consume them.\nUh, we also have a to train our unified embedding, we also have some upstream like just for example, the GN embeddings that those are also consumed to to do that.\nLast one is it fast?\nUh, hello.\nUh, in your embeddings, are you just using when someone does an action or sorry for the in these embeddings, are you just using metadata over the video to understand what they like, or are you actually using like frame by frame of the video or second clips?\nUh, not yet.\nWe do have that from some other content group of our organization, but I think the trend will go there.\nSo we are not yet, uh, into very granular sub like clips level or view level.\nWe have those embeddings, but not quite yet to incorporate.\nYep.\nThank you.\nThank you.\nUh, please another round of applause for [Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.861Z"
}