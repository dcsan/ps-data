{
  "episodeId": "iXhba366fQc",
  "channelSlug": "@aidotengineer",
  "title": "Building voice agents with OpenAI â€” Dominik Kundel, OpenAI",
  "publishedAt": "2025-06-29T22:30:05.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Awesome.",
      "offset": 0,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Well, hi everyone. My name is Dominic. I",
      "offset": 16.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "work on developer experience at OpenAI",
      "offset": 19.84,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and I'm excited to spend the next two",
      "offset": 21.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "hours to talk to you all about voice",
      "offset": 23.519,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "agents. Um, the QR code was already on",
      "offset": 25.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the slides. If you just entered the",
      "offset": 28.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "room, you want to try to download the",
      "offset": 30.4,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "dependencies as soon as possible. So,",
      "offset": 34,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "head over to that QR code or to that",
      "offset": 37.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "starter repository and follow the",
      "offset": 38.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "instructions to install. That might take",
      "offset": 40.559,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "a while with the internet right now. So,",
      "offset": 42.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "please do that as soon as possible. You",
      "offset": 44.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have like 15 minutes of me rambling",
      "offset": 46.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about stuff before we get started with",
      "offset": 48.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "uh with actually coding.",
      "offset": 50.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Um, so I said we're going to talk about",
      "offset": 52.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "voice agents. I want to first put",
      "offset": 54.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "everyone on the same page because I know",
      "offset": 56.16,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "we all have different uh definitions of",
      "offset": 57.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "agents and there's going to be a lot of",
      "offset": 60.239,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "definitions flying around at this",
      "offset": 61.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "conference naturally. Uh so when we're",
      "offset": 63.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "going to talk about agents, we're",
      "offset": 65.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "talking about systems that are going to",
      "offset": 66.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "accomplish tasks independently on behalf",
      "offset": 68.08,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "of users and most importantly um they're",
      "offset": 70.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "going to be essentially a combination of",
      "offset": 73.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "a model that is equipped with some set",
      "offset": 74.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of instructions that then has access to",
      "offset": 76.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "tools that can both be used to work on",
      "offset": 79.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that goal. And then all of that is",
      "offset": 82.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "encapsulated into a runtime to manage",
      "offset": 84.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "that life cycle. And that's an important",
      "offset": 86.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "definition because uh today we launched",
      "offset": 89.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the OpenAI agents SDK for TypeScript. If",
      "offset": 91.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you've heard of the one for Python,",
      "offset": 94.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "today we basically released the",
      "offset": 95.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "TypeScript equivalent. Um and so we're",
      "offset": 97.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "going to use that and it maps those",
      "offset": 101.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "exact patterns. So, if you're unfamiliar",
      "offset": 103.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "with the agents SDK, it's basically a",
      "offset": 105.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "SDK that provides you with an",
      "offset": 108.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "abstraction based on the best press",
      "offset": 110.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "practices that we learned at OpenAI to",
      "offset": 113.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "build agents. Um, and they it comes with",
      "offset": 115.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "a couple of different base foundational",
      "offset": 118.079,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "features including things like handoffs,",
      "offset": 120.399,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "guard rails, streaming input and output",
      "offset": 122.159,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "tools, MCP support, built-in tracing, so",
      "offset": 124.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "you can see actually what your agents",
      "offset": 127.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "did and how they interacted with each",
      "offset": 129.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "other. And then additionally to those",
      "offset": 131.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "features that are coming from the Python",
      "offset": 133.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "SDK, the SDK we launched today in",
      "offset": 134.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Typescript also includes human in the",
      "offset": 137.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "loop support with resumability so that",
      "offset": 139.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "if you need to wait for human approval",
      "offset": 141.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "for a while, you can deal with that. And",
      "offset": 144.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "most importantly, native voice agent",
      "offset": 146.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "support. What that means in practice is",
      "offset": 148.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you can use that same those same",
      "offset": 150.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "primitives that we already have in the",
      "offset": 152.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "agents SDK and you can build voice",
      "offset": 154,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "agents with that that handle handoffs",
      "offset": 156.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "have output guard rails to make sure",
      "offset": 157.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that the agent is not saying things it's",
      "offset": 159.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "not supposed to. uh tool calling,",
      "offset": 161.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "context management, meaning keeping",
      "offset": 163.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "track of the conversation history so you",
      "offset": 165.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "can use it in other applications and",
      "offset": 167.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "built-in tracer support um so that you",
      "offset": 170.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "can actually replay conversations,",
      "offset": 173.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "listen to the audio of the user and",
      "offset": 175.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "properly debug what happened. Plus",
      "offset": 177.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "native interruption support. If you've",
      "offset": 179.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tried to build interruptions, you might",
      "offset": 181.36,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "know how hard this is. If you haven't,",
      "offset": 183.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "be glad you don't have to. um both",
      "offset": 185.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "WebRTC and websocket support meaning",
      "offset": 188.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it's actually can communicate both on",
      "offset": 190.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the uh on the server for things like",
      "offset": 193.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Twilio communicate like phone call voice",
      "offset": 195.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "agents or directly in the client in the",
      "offset": 198.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "browser that's what we're going to use",
      "offset": 201.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "today using WebRTC",
      "offset": 203.519,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "but first why why would we be interested",
      "offset": 206.72,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "in voice agents in the first place um",
      "offset": 209.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "one of the things that I'm most excited",
      "offset": 213.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "about is it makes technology much more",
      "offset": 215.12,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "accessible to people. There's something",
      "offset": 217.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "magical about being able to talk to a",
      "offset": 218.879,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "voice uh to like a voice agent and just",
      "offset": 220.959,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "have it like see it do things. It's also",
      "offset": 223.76,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "um much more information dense. I can",
      "offset": 227.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "convey information much faster, but also",
      "offset": 230.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it can contain a lot of information",
      "offset": 233.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "through the type of tone and voice that",
      "offset": 235.36,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "I'm using, the emotions. So, it's much",
      "offset": 238,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "more information dense than sort of just",
      "offset": 240.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "based basic text is. One of the cool",
      "offset": 242.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "things is also it can act as like an API",
      "offset": 245.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "to the real world. You can have a voice",
      "offset": 247.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "agent go and like call a business for",
      "offset": 250.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "you and like have a conversation with",
      "offset": 252.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "them where maybe there isn't a there",
      "offset": 254.4,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "isn't an API for that business.",
      "offset": 256.639,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And so when we talk about building voice",
      "offset": 260.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "agents, there's essentially two types of",
      "offset": 262.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "architectures that have emerged on when",
      "offset": 264.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "building these. The first one is based",
      "offset": 266.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "on your traditional textbased agent and",
      "offset": 269.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "just sort of wrapping it into a chained",
      "offset": 272.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "approach where we have a speech to text",
      "offset": 274.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model that is taking the audio and then",
      "offset": 276.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "turning it into text so that we can run",
      "offset": 278.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "our basic textbased agent on it and then",
      "offset": 281.36,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "we take that and we uh run that agent",
      "offset": 284.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "take the text output and run it through",
      "offset": 288.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a texttospech model to generate audio",
      "offset": 290.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that we can play. Again",
      "offset": 292.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this has a lot of strengths. One of the",
      "offset": 295.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "most common reasons why we raise this is",
      "offset": 297.28,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "it's much easier to get started with if",
      "offset": 300.639,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "you already have a textbased agent. You",
      "offset": 303.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "can take that wrap some audio around it",
      "offset": 306.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and you have something that you can you",
      "offset": 308.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "can interact with. But uh the other",
      "offset": 310.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "aspect is that you have full access to",
      "offset": 313.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "any model. Text is the main modality",
      "offset": 315.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that any LLM has and so you can use",
      "offset": 318.16,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "really any of the cutting edge models.",
      "offset": 321.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "also gives you much more control and",
      "offset": 323.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "visibility of what the model did by",
      "offset": 325.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "being able to actually look into the",
      "offset": 327.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "exact text that went in and out of the",
      "offset": 330.479,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "model. But it also comes with some",
      "offset": 332.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "challenges. Turn detection is one of the",
      "offset": 334.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "one of the big ones where you need to",
      "offset": 336.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "now take into consideration what did the",
      "offset": 338.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "user hear by the time that they",
      "offset": 341.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "interrupted some uh interrupted the",
      "offset": 344.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "voice agent. Then translate that part",
      "offset": 346.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "back into text. make sure that like your",
      "offset": 348.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "transcript is appropriately adapted so",
      "offset": 351.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that the model doesn't think it told the",
      "offset": 354.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "user something that it didn't. Um,",
      "offset": 356.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "chaining all of these models together",
      "offset": 359.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "adds latency on every possible level.",
      "offset": 361.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "And so that's another big challenge. And",
      "offset": 364.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "then you're losing some of that audio",
      "offset": 366.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "context, right? You're transcribing the",
      "offset": 367.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "audio. And if you've ever tried to",
      "offset": 370.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "convey a complicated topic over a text,",
      "offset": 372.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you know it's a bit harder than dealing",
      "offset": 375.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the deal dealing with the same thing",
      "offset": 376.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "using using your own voice.",
      "offset": 378.16,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "And so an alternative to that chained",
      "offset": 381.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "approach is a speechtoech approach where",
      "offset": 383.759,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "we have a model that has been trained on",
      "offset": 386.88,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "audio and then takes that audio uh to",
      "offset": 389.039,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "directly interact on the conversation",
      "offset": 392.479,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "and make tool calls. Meaning there's no",
      "offset": 394.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "transcribing in the process. the model",
      "offset": 398.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "can just natively deal with that audio",
      "offset": 400.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and that translates into much lower",
      "offset": 403.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "latency because we're now skipping the",
      "offset": 406.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "speech to text text to speech processes.",
      "offset": 408,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "We can also now have much more",
      "offset": 410.479,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "contextual understanding of the audio um",
      "offset": 412.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "including things like tone and voice and",
      "offset": 415.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "all of that leads to a much more natural",
      "offset": 418.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "fluid uh level of conversation.",
      "offset": 420.319,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "But there are some challenges with this.",
      "offset": 423.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "One of the most common ones is reusing",
      "offset": 425.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "your existing capabilities. Everything",
      "offset": 427.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is built around text. So if you already",
      "offset": 428.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have some of those existing capabilities",
      "offset": 430.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "or a very specialized agent for a",
      "offset": 432.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "certain task, uh it's harder to reuse",
      "offset": 435.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "those. Also, dealing with like complex",
      "offset": 437.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "states and complex decision making. It's",
      "offset": 440,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "a bit harder with these models since",
      "offset": 442.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they've been really focused on improving",
      "offset": 443.759,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "on the audio conversational tone, less",
      "offset": 446.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so on being very complic complex",
      "offset": 449.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "decision makers.",
      "offset": 451.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "But there is a solution that we can get",
      "offset": 453.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "around with this. Um again taking",
      "offset": 455.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "inspiration from what we do with",
      "offset": 458,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "textbased agents, we can actually create",
      "offset": 459.28,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "a um delegation approach using tools",
      "offset": 462.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "where we have a frontline agent that is",
      "offset": 465.919,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "talking continuously to the user and",
      "offset": 468,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "then that one uses tool calls to",
      "offset": 470.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "interact with much smarter reasoning",
      "offset": 472.479,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "models like 04 mini or 03.",
      "offset": 475.199,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "Uh actually let me at this point give",
      "offset": 478.879,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "you a quick demo and see how the",
      "offset": 481.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "internet goes here. Um, so I have a",
      "offset": 483.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "real-time agent here that we build with",
      "offset": 485.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the agent SDK. It's going to be very",
      "offset": 487.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "similar to what you're going to build",
      "offset": 489.599,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "later on. Um, but when I start talking",
      "offset": 490.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "to this,",
      "offset": 493.759,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "hello there.",
      "offset": 496.639,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "Hello there. How is the engineer going?",
      "offset": 499.68,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "So we can now give it t like give a task",
      "offset": 503.44,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "to like call tools that I gave it like",
      "offset": 507.599,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "hey what's the weather today?",
      "offset": 510.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Let me check the weather for you. One",
      "offset": 514.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "moment, please. Transferring you to the",
      "offset": 516.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "weather expert now. Actually, I can",
      "offset": 518.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "directly help you with the weather",
      "offset": 521.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "information. Could you please specify",
      "offset": 523.039,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the location you're interested in?\n Oh,",
      "offset": 525.279,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "yeah. What's the weather in San",
      "offset": 526.959,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Francisco?",
      "offset": 527.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "So, you can see here it's actually",
      "offset": 530.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "dealing with the interruption.",
      "offset": 531.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Enjoy the bright and pleasant day. Is",
      "offset": 534.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "there anything else I can?\n No, that's",
      "offset": 537.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it. Thanks.",
      "offset": 539.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "You're welcome. If you need anything",
      "offset": 542.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "else, feel free to ask. Have a great",
      "offset": 544.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "day.\n And so in a similar way, we can",
      "offset": 546.64,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "actually trigger the more complicated",
      "offset": 548.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "backend agents as well. So I have a tool",
      "offset": 550.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "for this to handle refunds that will",
      "offset": 553.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "call out to 04 mini and evaluate the",
      "offset": 555.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "refund. So hey there. Um I have one more",
      "offset": 557.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "thing. So I recently ordered this",
      "offset": 560.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "skateboard that I tried to use and it",
      "offset": 563.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "seems like I'm really bad at",
      "offset": 565.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "skateboarding. So I want to return it.",
      "offset": 567.04,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "Um, it is slightly a scratch though.",
      "offset": 569.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "I'm here to assist, but it sounds like",
      "offset": 574.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "you need customer service for that. I",
      "offset": 576.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "recommend contacting the company where",
      "offset": 579.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you bought the skateboard. They can",
      "offset": 581.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "provide you with the\n Looks like I didn't",
      "offset": 583.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "add the tool. Maybe I did.",
      "offset": 586.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Oh, I asked for I didn't ask for a",
      "offset": 589.2,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "refund. Um, let's try this once more.",
      "offset": 591.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Hey there. Um, I bought a skateboard",
      "offset": 595.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "recently that I tried and apparently I'm",
      "offset": 597.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "really bad at using it, so I wanted to",
      "offset": 600.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "return it. It is slightly scratched, so",
      "offset": 602.56,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "can you give me a refund?",
      "offset": 604.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Hello there. How is the",
      "offset": 607.519,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "Hello there? How is the AI engine?\n The",
      "offset": 610.88,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "joys of internet. Um,",
      "offset": 613.6,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "hey. Um, I recently ordered a skateboard",
      "offset": 617.839,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "from you and it uh",
      "offset": 621.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "failed. Like I I can't use it. I'm",
      "offset": 624.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "struggling to use it. It's slightly",
      "offset": 627.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "scratched. Can you give me a refund,",
      "offset": 628.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "please?",
      "offset": 630.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "Hello there. How is the",
      "offset": 632.399,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "I'm going to assess your request for a",
      "offset": 635.839,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "refund.\n There we go.\n It is slightly",
      "offset": 638.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "struggling with this like weird echo",
      "offset": 640.959,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that we're having here. your skateboard",
      "offset": 642.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "arrived damaged and you're eligible for",
      "offset": 645.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "a full refund, we'll process that for",
      "offset": 647.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you.\n All right. Um, but you can see here",
      "offset": 650.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that uh it was able to call that more",
      "offset": 652.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "advanced tool and um actually process",
      "offset": 655.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that request. And one of the nice things",
      "offset": 658.399,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is that like while time to first token",
      "offset": 661.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is often a really important thing, the",
      "offset": 664.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "longer a conversation goes, like your",
      "offset": 666.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "model is always going to be faster than",
      "offset": 669.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the audio that has to be read out. And",
      "offset": 670.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "so this this is like a really helpful",
      "offset": 673.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "thing where by the time that the model",
      "offset": 675.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "was able to say like, hey, I'm going to",
      "offset": 677.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "check on this for you, it already had",
      "offset": 680.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "completed that LM call to the 04 mini",
      "offset": 682.72,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "model to get the response there.",
      "offset": 686.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "All right. Um,",
      "offset": 689.519,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "let me Oh, one more thing. Since we",
      "offset": 692.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "talked about traces, one of the nice",
      "offset": 694.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "things now is we can actually go back",
      "offset": 696.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "here into our traces UI. And this launch",
      "offset": 698.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "today, um, you'll be able to actually",
      "offset": 701.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "look for any of your real-time API",
      "offset": 703.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "cases, look at all the audio that it",
      "offset": 705.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "dealt with, um, and all the tool calls.",
      "offset": 708,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "So, we can actually see here that the",
      "offset": 710,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "tool call was triggered, what the input",
      "offset": 712.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "was, the output. We can listen to some",
      "offset": 715.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of the audio again",
      "offset": 716.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "um, to understand what happened. And",
      "offset": 719.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "then because both this and the backend",
      "offset": 722.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "agent used the agents SDK, we can go",
      "offset": 725.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "into the other agent as well, which was",
      "offset": 727.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "the O4 mini one, which we can see here.",
      "offset": 729.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And we can see that it received the",
      "offset": 732.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "context of the full past conversation,",
      "offset": 734.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the full transcript, um, as well as",
      "offset": 736.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "additional information about the request",
      "offset": 738.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and then generated the response here. So",
      "offset": 741.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this allows us to then get a full",
      "offset": 744,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "complete picture of like what happened",
      "offset": 745.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "both on the front end and the back end.",
      "offset": 747.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Let's jump back into the slides and",
      "offset": 750.56,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "cover a couple of more things before we",
      "offset": 751.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "get coding. Um, and that's about best",
      "offset": 753.279,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "practices. So, I would group when you're",
      "offset": 756,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "group the best practices of like",
      "offset": 760.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "building a voice agent into three main",
      "offset": 761.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "things to keep in mind. Uh, the first",
      "offset": 763.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "one is to start with a small and clear",
      "offset": 766.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "goal. This is super important because",
      "offset": 768.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "measuring the performance of a textbased",
      "offset": 771.36,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "agent, you will hear a lot about evalu.",
      "offset": 773.44,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "But with voice voice agents, it's going",
      "offset": 778.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "to be even harder. So you want to make",
      "offset": 780.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "sure that you're very focused on like",
      "offset": 782.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "what is the first problem you want to",
      "offset": 783.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "solve and keep it focused on that and",
      "offset": 785.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "give it a like limited number of tools",
      "offset": 787.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "so that you're fully centered on this.",
      "offset": 790.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "The agents SDK makes this really easy",
      "offset": 792.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "because you can then later on add",
      "offset": 794,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "additional tools to additional agents",
      "offset": 795.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and deal with like handoffs between",
      "offset": 798.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "them. Uh but this way you can kind of",
      "offset": 800.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "really stay focused and make sure that",
      "offset": 803.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "one of your use cases is great and then",
      "offset": 805.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "hand off other ones to human agents for",
      "offset": 807.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "example.",
      "offset": 809.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "The second one is what I elaborated on",
      "offset": 811.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "which is building evals and guard rails",
      "offset": 814.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "very early on. Uh so that you can feel",
      "offset": 816.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "both confident in what you're building",
      "offset": 818.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "but also confident in um that is",
      "offset": 820.399,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "actually working so that you can then",
      "offset": 823.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "continue to iterate on it and know when",
      "offset": 826.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it's time for you to like grow the",
      "offset": 828.079,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "complexity of your voice agent. Uh as of",
      "offset": 829.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "today you can use the traces API the",
      "offset": 833.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "trace dashboard for that. Uh but",
      "offset": 836.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "alternatively, some of our customers",
      "offset": 838.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "have even built their own dashboards",
      "offset": 840.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like Lemonade to really get an",
      "offset": 841.839,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "end-to-end idea of the customer",
      "offset": 843.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "experience and then even replay some of",
      "offset": 845.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "these conversations with their agent as",
      "offset": 847.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "they're iterating on it.",
      "offset": 849.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "The other thing that I'm personally",
      "offset": 852.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "super excited about with these models is",
      "offset": 854.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "uh both our speech to speech model and",
      "offset": 856.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "our text to speech model are generative",
      "offset": 858.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "models meaning you can prompt them the",
      "offset": 860,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "same way that you can prompt a LLM",
      "offset": 862.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "around tone and voice and you can give",
      "offset": 865.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "it emotions roles personality. Uh we",
      "offset": 867.519,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "built this little micro site called",
      "offset": 871.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "openai.fm. It's a really fun website to",
      "offset": 873.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "play around with where we have a lot of",
      "offset": 876,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "examples of different personalities and",
      "offset": 877.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "how that uh that that style of prompt",
      "offset": 880.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "can then change what is being read out",
      "offset": 882.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "by our text to speech model. And so",
      "offset": 884.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that's a great way for you to not just",
      "offset": 886.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "limit one second uh limit the experience",
      "offset": 889.6,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "of your model",
      "offset": 892.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "um or like the personality of your model",
      "offset": 895.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "by the voice that you picked but also by",
      "offset": 897.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the prompt and instructions that you're",
      "offset": 899.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "giving it. There was a question there.",
      "offset": 902.079,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Would you mind using the mic that is",
      "offset": 903.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "right behind you just so that it's on",
      "offset": 905.279,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "the recording?",
      "offset": 906.88,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "Hello sir. So my question regards to the",
      "offset": 911.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "previous slides on Lemonade. So you're",
      "offset": 914.079,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "displaying how they have this dashboard",
      "offset": 916.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "where they can show all of this. Is this",
      "offset": 919.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a dashboard that OpenAI provides and",
      "offset": 922,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Lemonade just integrates as like an",
      "offset": 924.16,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "iframe or something?\n No. So in this case",
      "offset": 927.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "they built their own um solution for it.",
      "offset": 929.199,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Okay. And does OpenAI then provides all",
      "offset": 932.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "the JSON or the data structure that we",
      "offset": 934.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "can just plug into the\n so the the way",
      "offset": 936.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the real time API under the hood works",
      "offset": 938.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "is that you get all the audio data and",
      "offset": 940.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you can do whatever you want with that.",
      "offset": 943.279,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Basically, you're getting all the",
      "offset": 945.519,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "necessary audio events. So you can use",
      "offset": 946.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "those data structures. So we're not",
      "offset": 948.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "storing them by default. You can use the",
      "offset": 950.079,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "traces AP uh the traces dashboard. We",
      "offset": 952.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "don't have an API for it yet, but you",
      "offset": 955.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "can use the trace traces dashboard to um",
      "offset": 956.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "get a a basic look of of that, but it's",
      "offset": 959.759,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "not ifraable.\n But you mentioned it's",
      "offset": 962.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "only audio data. This shows not just",
      "offset": 965.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "audio, but also transcription and all of",
      "offset": 967.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that as well. Right. So, the the traces",
      "offset": 969.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "dashboard, if we go back to it, um does",
      "offset": 971.519,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "show",
      "offset": 974,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 975.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "all of the transcripts and stuff as",
      "offset": 977.839,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "well, as long as you have um",
      "offset": 979.68,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "transcription turned on, which I don't",
      "offset": 983.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "seem to have turned on for this",
      "offset": 985.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "particular one. Um but it should like",
      "offset": 986.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you can turn on transcription and you",
      "offset": 989.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "should be able to see the transcripts as",
      "offset": 991.279,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "well.\n Okay. Thank you.\n You're welcome.",
      "offset": 992.88,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "All right. Um let's go back to this. Um",
      "offset": 998.079,
      "duration": 8.481
    },
    {
      "lang": "en",
      "text": "the other part with it is as I said you",
      "offset": 1004.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "can prompt both the personality you can",
      "offset": 1006.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "also be very descriptive with the",
      "offset": 1008.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "conversation flows. One of our",
      "offset": 1010.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "colleagues um found that giving it",
      "offset": 1011.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "conversation states and so this JSON",
      "offset": 1015.279,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "structure is a great way to help the",
      "offset": 1017.839,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "model think through sort of what",
      "offset": 1019.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "processes and what steps it should go",
      "offset": 1021.759,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "through the same way that you would give",
      "offset": 1023.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "a human agent a script to operate on.",
      "offset": 1024.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "If you're struggling to write those",
      "offset": 1028.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "scripts though, uh we also have a custom",
      "offset": 1030.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "GPT that you can use to access that. And",
      "offset": 1032.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I'll share all of those links and a copy",
      "offset": 1034.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of the slide deck later on in the Slack",
      "offset": 1036.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "channel. So if you're in that, uh you",
      "offset": 1038.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "should be able to access those.",
      "offset": 1040.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "But with that, um that's primarily what",
      "offset": 1043.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "I wanted to talk through from a from a",
      "offset": 1046.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "slides perspective. So from here on,",
      "offset": 1049.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "what I want to do is um build with you a",
      "offset": 1052.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "voice agent. We'll see how that goes",
      "offset": 1055.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "with the internet. Um, also, if you have",
      "offset": 1056.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "headphones, now is a great time to bring",
      "offset": 1059.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "them out. It's going to be really weird",
      "offset": 1061.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "when we're all gonna talk to our own",
      "offset": 1062.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "agent. Um, but we're going to try this",
      "offset": 1064.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and see how that goes. Um, so if you",
      "offset": 1066.32,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "came in later, please scan the QR code,",
      "offset": 1069.76,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "go to that uh GitHub repository, and set",
      "offset": 1072.559,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "that up. Install the instructions.",
      "offset": 1076.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "There's no code in it yet other than",
      "offset": 1078.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like a boilerplate Nex.js JS app and a",
      "offset": 1079.76,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "um empty like package JSON that install",
      "offset": 1082.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "just like the dependencies that we",
      "offset": 1086.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "needed so that we are not all trying to",
      "offset": 1088.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "run npm install at the same time.",
      "offset": 1090.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "But what I want to do is build a first",
      "offset": 1093.76,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "agent. Um, so if you want, you can just",
      "offset": 1097.28,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "straight up copy the code that is on",
      "offset": 1100.72,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "here, but I'm gonna um actually go and",
      "offset": 1103.84,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "type it along with you all so that you",
      "offset": 1107.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "get a feeling for what's happening and",
      "offset": 1110.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "we have a good idea of like timing. So",
      "offset": 1112.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if you want to take a picture now and",
      "offset": 1115.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just code ahead, do that. And otherwise,",
      "offset": 1117.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I'm going to switch over to my code",
      "offset": 1119.6,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "editor and we're going to do this",
      "offset": 1120.88,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "together.",
      "offset": 1121.919,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Oh, and if you're running into trouble,",
      "offset": 1124.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "the Slack is a great way to post",
      "offset": 1127.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "questions that are technical questions",
      "offset": 1130.799,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "and a noop who's over there um is going",
      "offset": 1132.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to try to help you. Um, alternatively,",
      "offset": 1136.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "raise your hand, but it's a bit easier",
      "offset": 1138.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "if you're just slacking the messages",
      "offset": 1140.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "there and we can kind of multi-thread",
      "offset": 1142.559,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the problem. Um, all right, let's go and",
      "offset": 1145.28,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "build an agent. So if you clone the",
      "offset": 1149.039,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "project, you should see an index",
      "offset": 1152.72,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "uh.ts file. Go and open that and you",
      "offset": 1155.679,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "should be able to import the agent class",
      "offset": 1159.6,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "from the OpenAI/ aents package. That's",
      "offset": 1163.039,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "what we're going to use to create the",
      "offset": 1166.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "first agent. Yeah.",
      "offset": 1168.88,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "Oh yeah, good call. Um",
      "offset": 1171.919,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "Is that better?",
      "offset": 1184,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Cool.",
      "offset": 1186,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "That seems a bit seems worse on my side",
      "offset": 1188,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "than yours, but I think as long as you",
      "offset": 1191.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "all can read that, I'll be fine. Um, all",
      "offset": 1193.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right. So, what you what I want you to",
      "offset": 1195.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "do is go and import an agent. And we're",
      "offset": 1197.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "going to define our first agent. And as",
      "offset": 1199.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I mentioned, um, primarily the an agent",
      "offset": 1201.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "has a like a few centerpieces. The first",
      "offset": 1204.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "one being a set of instructions on what",
      "offset": 1207.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "to do. So we can give it instructions.",
      "offset": 1210,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "I'm going to say you're a helpful",
      "offset": 1212.559,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "assistant. That's sort of the most",
      "offset": 1213.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "boilerplate thing you can do. We do need",
      "offset": 1215.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to also give it a name and that's uh so",
      "offset": 1217.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that we can actually keep track of them",
      "offset": 1219.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in our traces dashboard.",
      "offset": 1221.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I'm going to say my agent here. This can",
      "offset": 1224.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "be anything that helps you identify it.",
      "offset": 1226.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And then we need to actually execute",
      "offset": 1229.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this agent. So we can import a run",
      "offset": 1231.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "function here.",
      "offset": 1234.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And then",
      "offset": 1236.48,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "we can await the run here. I'm going to",
      "offset": 1239.28,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "run this agent with just hello, how are",
      "offset": 1244.559,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "you? And then log out the results. And",
      "offset": 1247.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "with the results we get a lot of",
      "offset": 1250.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "different information because uh",
      "offset": 1252,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "essentially when we run an agent it's",
      "offset": 1254.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "going to do a lot of different tasks",
      "offset": 1255.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "from executing all necessary tool calls",
      "offset": 1258.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "if there are any to validating output",
      "offset": 1260.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "guard rails etc. But one of the most",
      "offset": 1262.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "common things that you just want to log",
      "offset": 1265.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "out is the final output. That's whatever",
      "offset": 1266.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the last agent in an execution said. Um,",
      "offset": 1269.12,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "so in this case it's going to be uh a",
      "offset": 1273.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "set of text and then you should be able",
      "offset": 1276.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to run npm start",
      "offset": 1278.4,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "um oh npm run start",
      "offset": 1281.44,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "01",
      "offset": 1286.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and that should execute it and then you",
      "offset": 1288.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "should see something like this um",
      "offset": 1290.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "depending on what your model decides to",
      "offset": 1293.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "generate and by default this is going to",
      "offset": 1296.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "run gbt 4.1",
      "offset": 1298,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "um as the model but if you want",
      "offset": 1300.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "experiment with this. You can set the",
      "offset": 1302.48,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "model property here and we can set it to",
      "offset": 1304.48,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "for mini for example and then rerun the",
      "offset": 1309.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "same thing. So this is the most basic",
      "offset": 1312.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "agent that you can build. But one of the",
      "offset": 1315.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "things that really makes something an",
      "offset": 1318.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "agent is if it can execute tools. So we",
      "offset": 1319.76,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "can import a tool here.",
      "offset": 1322.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "We go and we can define a get weather",
      "offset": 1327.28,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "tool. Um,",
      "offset": 1330.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "one of the things here is you have to",
      "offset": 1334.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "specify what arguments the model is",
      "offset": 1336.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "going to receive. And one of the ways",
      "offset": 1340.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that you can do this is through a tool",
      "offset": 1342.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "called ZOD. If you've never heard of it,",
      "offset": 1344.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it's essentially a way to define",
      "offset": 1346.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "schemas. Um, and what we'll do is we'll",
      "offset": 1347.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "both use that ZOD schema to inform the",
      "offset": 1350.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "model on what the parameters for this",
      "offset": 1354.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "function call are, but we're also going",
      "offset": 1356.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "to use it to validate then what are the",
      "offset": 1357.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "actual arguments that the model tried to",
      "offset": 1361.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "pass in and do they fit to that schema.",
      "offset": 1362.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "So we get full type safety here if",
      "offset": 1364.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you're a TypeScript developer and you",
      "offset": 1367.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "care about that. So in this case we have",
      "offset": 1368.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "um a get weather tool and then we can",
      "offset": 1371.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "give that tool to the agent and we can",
      "offset": 1373.679,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "change this to what is the weather in",
      "offset": 1376.96,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Tokyo uh is what cursor wants to check.",
      "offset": 1380.4,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "So if I run this again,",
      "offset": 1383.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "let me move this slightly. We can see",
      "offset": 1386.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "it's going to take a bit longer now and",
      "offset": 1389.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that's because it ran some tools. Um and",
      "offset": 1390.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "now it's telling me the weather in Tokyo",
      "offset": 1393.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is sunny. And if you're wondering, well,",
      "offset": 1395.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "did it actually run a tool? We can go",
      "offset": 1397.6,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "into",
      "offset": 1401.52,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "our traces dashboard here",
      "offset": 1405.919,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "and look at the trace. We have a my",
      "offset": 1410.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "agent here.",
      "offset": 1412.64,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "And in there we can see it ran",
      "offset": 1415.36,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "tried to call the tool executed the tool",
      "offset": 1420.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and got the weather in Tokyo sunny back",
      "offset": 1422.799,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "and then took the response to generate",
      "offset": 1426.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the final response. So the traces",
      "offset": 1428.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "dashboard is a great way for you to see",
      "offset": 1431.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "what actually happened behind the",
      "offset": 1432.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "scenes.",
      "offset": 1434.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "How we feeling? Can I get a quick",
      "offset": 1437.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "temperature check? Are able are people",
      "offset": 1438.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "able to follow along? See Zeke is giving",
      "offset": 1440.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "a thumbs up there. So um all right so",
      "offset": 1443.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "this is a textbased agent. I wanted to",
      "offset": 1447.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "show you this just to get a bit familiar",
      "offset": 1449.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "with the um overall agents SDK so that",
      "offset": 1451.039,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "we can jump into building voice agents.",
      "offset": 1454.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Uh the first one we're going to build or",
      "offset": 1457.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like the first thing we need to",
      "offset": 1459.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "understand about a voice agent is the",
      "offset": 1462.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "slight differences between a voice agent",
      "offset": 1464.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and a and a what we call a real-time",
      "offset": 1466.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "agent. Essentially, a real-time agent is",
      "offset": 1469.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "just a specialized version of an agent",
      "offset": 1471.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "configuration. There's just a few fields",
      "offset": 1473.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "you can't pass in, but they can be used",
      "offset": 1475.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in what is a what's called a real-time",
      "offset": 1477.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "session because with voice agents,",
      "offset": 1479.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "there's a lot more things to deal with",
      "offset": 1481.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "than just executing tools in a loop. Um,",
      "offset": 1483.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "one of the most important things is you",
      "offset": 1486.159,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "need to deal with both the audio that's",
      "offset": 1487.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "coming in, uh, process that and then",
      "offset": 1489.279,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "like run the run the model with that and",
      "offset": 1491.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "then gener deal with the audio that's",
      "offset": 1494.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "coming out. But you also need to think",
      "offset": 1496.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "about things like guard rails, handoffs,",
      "offset": 1498.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "other life cycle things. And so the",
      "offset": 1501.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "real-time sessions are really dealing",
      "offset": 1503.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "with all of that. So let me show you how",
      "offset": 1504.799,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that works. Um for this what we're going",
      "offset": 1508.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "to do is we're going to go in the same",
      "offset": 1511.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "project. There's a 02",
      "offset": 1513.2,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "and it has a page txt tsx in there. Um,",
      "offset": 1515.919,
      "duration": 8.401
    },
    {
      "lang": "en",
      "text": "this is a Nex.js app that really I just",
      "offset": 1520.4,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "gutted to have like the bare minimum in",
      "offset": 1524.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "there, but this is a great way for us to",
      "offset": 1526.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "just uh build both the front end and the",
      "offset": 1528.72,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "back end part of the um voice experience",
      "offset": 1530.96,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "because this voice agent that we're",
      "offset": 1535.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "going to build is going to run in the",
      "offset": 1537.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "browser. In order to make sure we're not",
      "offset": 1538.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "leaking your API credentials, one of the",
      "offset": 1541.039,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "important things is you need to use an",
      "offset": 1543.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "ephemeral key. That is a key that is",
      "offset": 1545.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "shortlived and is going to be generated",
      "offset": 1547.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "by your server and handed off to your",
      "offset": 1549.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "client so that they can use that to",
      "offset": 1552.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "interact with the real-time API over a",
      "offset": 1553.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "protocol called WebRTC.",
      "offset": 1556.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Um, for that you should see a token.ts",
      "offset": 1558.64,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "file in your repository that just calls",
      "offset": 1562.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "out to the real-time API to generate a",
      "offset": 1566,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "session and then return a client secret",
      "offset": 1568.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "which is that ephemeral key that we can",
      "offset": 1571.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "then use to authenticate with the SDK.",
      "offset": 1572.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "You do not have to do this if you're",
      "offset": 1574.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "building a real-time agent that is",
      "offset": 1576.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "running on your server. For example, in",
      "offset": 1579.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the case of a Twilio app or something",
      "offset": 1581.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "else where you can just directly",
      "offset": 1584,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "interact with the",
      "offset": 1586.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "uh open AI API key. But if you're",
      "offset": 1589.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "running anything in the browser, then",
      "offset": 1592.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you actually need to generate this",
      "offset": 1593.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "client key just so that you're not, you",
      "offset": 1595.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "know, giving your API key to the world.",
      "offset": 1597.679,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "So with that in here, we can actually go",
      "offset": 1600.48,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "and build our first real-time agent. So",
      "offset": 1604.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "similar to previously, we're going to",
      "offset": 1606.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "import a an agent class here, but in",
      "offset": 1608.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this case, it's going to be a real-time",
      "offset": 1612.24,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "agent.",
      "offset": 1613.6,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "And we're going to import it from the",
      "offset": 1617.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "real time package",
      "offset": 1618.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "uh which is just a subpath in the same",
      "offset": 1620.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "package. So you don't need to install a",
      "offset": 1622.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "different package here. But now we can",
      "offset": 1624.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "define a real-time agent that works the",
      "offset": 1626.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "same way. We have a name, we give it",
      "offset": 1629.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "instructions.",
      "offset": 1631.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Um, just sort of going with a default",
      "offset": 1632.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "suggestion here. And now we actually",
      "offset": 1636.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "need to connect that agent to a",
      "offset": 1638.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "real-time session. So I have a connect",
      "offset": 1640.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "button here for running this example.",
      "offset": 1642.08,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "Let me start it up here with npm run",
      "offset": 1645.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "start02. That command should be in your",
      "offset": 1649.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "readme as well.",
      "offset": 1651.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Uh, it's going to start a development",
      "offset": 1653.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "server. And we can go over here, reload",
      "offset": 1655.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this, and you can actually see",
      "offset": 1658.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it just has like a little connect button",
      "offset": 1661.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "that right now doesn't do anything.",
      "offset": 1663.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "So, let's connect that up. I don't need",
      "offset": 1666.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this anymore. Um, so let me just move",
      "offset": 1669.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "this to the side.",
      "offset": 1672.559,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Um, so in this on connect function that",
      "offset": 1675.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "gets triggered whenever we press the",
      "offset": 1679.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "button, we want to deal with that",
      "offset": 1680.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "connected state. So",
      "offset": 1682.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we're what we're going to do here is",
      "offset": 1685.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we're first going to fetch that token.",
      "offset": 1687.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And this code what this basically does",
      "offset": 1689.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "is it's going to import that server",
      "offset": 1691.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "action which is an next.js concept that",
      "offset": 1694.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "just makes sure that like this code is",
      "offset": 1696.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going to run on your back end. If you're",
      "offset": 1698.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "using a different framework, you you",
      "offset": 1700.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "should be able to just go and fetch this",
      "offset": 1702.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "key from your backend server. Um, and",
      "offset": 1704.799,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "then once we have that token, we can go",
      "offset": 1708.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and create a new real time session. So,",
      "offset": 1711.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "what we're doing here is we're going to",
      "offset": 1714.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "give it the first agent that should",
      "offset": 1715.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "start the conversation up. Uh, I'm going",
      "offset": 1717.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "to specify the latest model that we",
      "offset": 1720.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "released today along with the agents",
      "offset": 1722.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "SDK. This model is a if you've used the",
      "offset": 1724.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "real-time API before, it's an",
      "offset": 1727.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "improvement especially around tool",
      "offset": 1729.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "calling. It's much better on that front.",
      "offset": 1730.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Um, we have a couple of different",
      "offset": 1734,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "customer stories on our Twitter if you",
      "offset": 1735.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "want to check that out. Um, and then I'm",
      "offset": 1737.2,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "going to give it not there. I don't know",
      "offset": 1740.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "why cursor insists on that. Um, the last",
      "offset": 1743.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "step that we need to do is we need to",
      "offset": 1745.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "connect to that session.",
      "offset": 1747.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "So, this is where we're going to give it",
      "offset": 1749.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "that API key so that the mo that we can",
      "offset": 1751.36,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "connect to the real-time session under",
      "offset": 1755.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the hood",
      "offset": 1757.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "just so that it's easier for us to deal",
      "offset": 1759.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "with all of this. I'm also going to",
      "offset": 1760.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "close the session. But I've got one",
      "offset": 1762.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "thing here that is an oddity of",
      "offset": 1765.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "uh React. We do not want to generate",
      "offset": 1768.48,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "that session every time. So, I'm going",
      "offset": 1771.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to on every rerender. So, I'm going to",
      "offset": 1774.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "create a what is called a ref here.",
      "offset": 1777.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Again, if you're new to React. Um, this",
      "offset": 1779.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "basically is just a variable that's",
      "offset": 1783.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "going to persist through rerenders. So,",
      "offset": 1784.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we need to slightly change this here",
      "offset": 1786.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "where we're going to assign that to",
      "offset": 1788.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "session.curren",
      "offset": 1789.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so we can maintain that. And then that",
      "offset": 1791.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "also allows us to say if there is a",
      "offset": 1794.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "session.curren set, uh, we want to",
      "offset": 1796.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "actually close that connection when we",
      "offset": 1799.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "press the disconnect button. That just",
      "offset": 1800.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "makes sure that we're disconnected from",
      "offset": 1802.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "the audio again.",
      "offset": 1804.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "So, I'm going to leave that on the",
      "offset": 1807.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "screen for a second and then we can test",
      "offset": 1809.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this out. But if you already typed this,",
      "offset": 1811.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "go into your browser, refresh, press",
      "offset": 1813.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "connect, and you should be able to talk",
      "offset": 1816,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "to your agent.",
      "offset": 1817.44,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "All right, let's try mine. Let me move",
      "offset": 1829.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "this to the other side so it's not",
      "offset": 1832.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "blocking your cut.",
      "offset": 1833.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "Hello.",
      "offset": 1838.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "Hi there. How can I assist you today?",
      "offset": 1840.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "All right. So, you can see um it's just",
      "offset": 1843.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "a few lines of code. We didn't have to",
      "offset": 1846.559,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "deal with things like figuring out how",
      "offset": 1848.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to set up the microphone, how to set up",
      "offset": 1849.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the speakers. By default, if it's",
      "offset": 1851.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "running in the browser, it will do deal",
      "offset": 1853.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "with all of that automatically. If you",
      "offset": 1855.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "do want to pass in your own microphone",
      "offset": 1857.44,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "source or other things like that, you",
      "offset": 1858.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "can do that as well. Um, if this is",
      "offset": 1860.399,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "running on a server, you have a um",
      "offset": 1862.72,
      "duration": 8.6
    },
    {
      "lang": "en",
      "text": "both a",
      "offset": 1867.919,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "send audio function that allows you to",
      "offset": 1871.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "send an audio buffer in or you can",
      "offset": 1873.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "listen to the audio event which is going",
      "offset": 1876.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to emit all of the audio buffers that",
      "offset": 1879.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "are coming back from the model so that",
      "offset": 1881.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you can pass it to whatever your source",
      "offset": 1885.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is.",
      "offset": 1887.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 1888.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "so that's our first basic agent. Any any",
      "offset": 1891.12,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "questions so far?",
      "offset": 1894.159,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "H,\n can you send that code to the red?",
      "offset": 1900.559,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "Sorry,\n we want you to send the code.\n Can",
      "offset": 1904.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "you push it? Can you push it up?\n I can",
      "offset": 1907.919,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "push it. Yeah.",
      "offset": 1910.72,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Good call. Thank you. Um,",
      "offset": 1924.72,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "all right. So, now that we have that,",
      "offset": 1928.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "let's go and actually give it a tool.",
      "offset": 1932.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "So, this is really where the benefit of",
      "offset": 1934.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the um agents SDK comes in. We can",
      "offset": 1936.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "actually use that same tool definition",
      "offset": 1939.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that we did earlier. Um, so I'm just",
      "offset": 1942.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "going to follow the autocomplete here.",
      "offset": 1944.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "We should be able to just give that tool",
      "offset": 1946.48,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "now to our agent",
      "offset": 1949.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "and save.",
      "offset": 1953.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I need to import zod",
      "offset": 1955.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "again to do that schema validation. This",
      "offset": 1958.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is especially important on the real time",
      "offset": 1960.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "side because the real- time model",
      "offset": 1962.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "currently do not does not support strict",
      "offset": 1964.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "mode. So the JSON might not fully comply",
      "offset": 1966.559,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "with your um",
      "offset": 1969.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "um schema unless you're running you're",
      "offset": 1973.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "giving us a zod schema and we'll go and",
      "offset": 1976.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "validate that this actually fits that",
      "offset": 1978.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "schema. So that makes your code a bit",
      "offset": 1979.84,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "easier. So with that we can go back.",
      "offset": 1981.919,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "Hey, what's the weather in San",
      "offset": 1987.279,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "Francisco?",
      "offset": 1989.12,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "The weather in San Francisco.",
      "offset": 1992.24,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "We can disconnect it here. Um, also this",
      "offset": 1995.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "does now deal with interruptions. So,",
      "offset": 1998.799,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "hey, can you slowly count to like 20?",
      "offset": 2002,
      "duration": 11.6
    },
    {
      "lang": "en",
      "text": "Sure, I'll count to 20 for you. 1 2 3 4",
      "offset": 2006.399,
      "duration": 10.801
    },
    {
      "lang": "en",
      "text": "5\n Okay, stop.",
      "offset": 2013.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Sure, I'll stop counting.\n How far did",
      "offset": 2017.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you count?",
      "offset": 2019.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "I counted up to six.",
      "offset": 2021.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "It's pretty pretty close. It's always",
      "offset": 2024.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "hard with the timing to like perfectly",
      "offset": 2026.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "get it, but uh normally that's enough to",
      "offset": 2028.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "um deal with the context, but it is",
      "offset": 2031.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "super crucial to have that interruption",
      "offset": 2033.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "timing so that like your model doesn't",
      "offset": 2035.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "think it read out like the full customer",
      "offset": 2037.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "policy um but the customer interrupted",
      "offset": 2039.919,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "it halfway through, for example. Um all",
      "offset": 2042.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "right,",
      "offset": 2045.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "question. You don't have to um manage",
      "offset": 2047.12,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "all the events to actually do that",
      "offset": 2049.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "anymore?\n No. Um, so the the real- time",
      "offset": 2051.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "session will handle all of those events.",
      "offset": 2054.639,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "That is a great proxy to if you are",
      "offset": 2057.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "curious about these events um that",
      "offset": 2060.24,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "you're alluding to. What we can do is",
      "offset": 2063.28,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "listen to the",
      "offset": 2066.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "transport event",
      "offset": 2071.04,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "and let's do this later. Uh this will",
      "offset": 2073.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "log out all of the events that are",
      "offset": 2076.879,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "happening under the hood. So, if we open",
      "offset": 2078.8,
      "duration": 9.96
    },
    {
      "lang": "en",
      "text": "the dev tools here and rerun this.",
      "offset": 2082.639,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "Hey,\n hey there. How can I help you",
      "offset": 2089.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "today?\n So, you can see all of the events",
      "offset": 2092.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that normally you would have to deal",
      "offset": 2095.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "with are being dealt with, you still",
      "offset": 2096.639,
      "duration": 4.001
    },
    {
      "lang": "en",
      "text": "have full access to them. So, you can",
      "offset": 2098.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "both read them, but you can also send",
      "offset": 2100.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "events yourself. So, it's going to",
      "offset": 2102.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "handle all of the things but continue to",
      "offset": 2104.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "par pass them on to you if you want to",
      "offset": 2107.28,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "do your own logic. On top of that,",
      "offset": 2109.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "I'm going to push that code for you so",
      "offset": 2114.4,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "you can pull it.",
      "offset": 2115.92,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Cool. Um,",
      "offset": 2128.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "all right. Since we already have this",
      "offset": 2131.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "commented out code, the other part of",
      "offset": 2133.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this that typically is a request that",
      "offset": 2135.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you want to deal with is I want to show",
      "offset": 2137.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "like the transcript. I want to see what",
      "offset": 2140.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sort of is being transcribed. And the",
      "offset": 2142.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "important thing here is I'm using the",
      "offset": 2144.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "word transcribe because even though the",
      "offset": 2145.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "speechtoech model is dealing with the",
      "offset": 2147.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "audio directly, um, and there is no",
      "offset": 2150.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "transcription step in between, by",
      "offset": 2152.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "default, we're going to transcribe all",
      "offset": 2155.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of the conversation at the same time.",
      "offset": 2157.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Um, you can turn that off if you want",
      "offset": 2160.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to. Um, if you're using the API",
      "offset": 2162.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "directly, you have to actually turn it",
      "offset": 2164.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "on. Um, in the agents SDK, it's turned",
      "offset": 2166.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "on by default because it's such a common",
      "offset": 2169.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "request and it enables us to do a couple",
      "offset": 2171.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of additional features that we'll cover",
      "offset": 2173.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "later on. But this is going to give us",
      "offset": 2175.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that whole history every time. So, I'm",
      "offset": 2178.32,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "just going to um log that history here.",
      "offset": 2180.8,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "Or rather, I'm gonna There we go. import",
      "offset": 2186.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that. I'm going to",
      "offset": 2189.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "set that as a variable. And then because",
      "offset": 2192.56,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "it's React, we can let's create a list",
      "offset": 2194.88,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "here. We're going to go over all of",
      "offset": 2199.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "this. I need to filter because it has",
      "offset": 2202.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "both tool calls and messages. And I only",
      "offset": 2204.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "want to show the messages for this. Um,",
      "offset": 2207.2,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "so should be able to does why I want",
      "offset": 2209.92,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "that. Um, let's see.",
      "offset": 2214.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Close this. Refresh.",
      "offset": 2218.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Hey.",
      "offset": 2220.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Hello. How can I assist you today?",
      "offset": 2222.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "How's the weather today in San",
      "offset": 2226,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Francisco?",
      "offset": 2227.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "The weather in San Francisco today is",
      "offset": 2229.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "sunny. Anything else you'd like to know?",
      "offset": 2232.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So, you're automatically getting that",
      "offset": 2235.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "conversation. If you are interrupting",
      "offset": 2236.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the model, one of the things that",
      "offset": 2239.359,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "happens is the transcript is going to",
      "offset": 2240.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "disappear. And that's because uh the the",
      "offset": 2242.32,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "model currently does not adjust that",
      "offset": 2246.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "transcript and instead it's going to be",
      "offset": 2248.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "removed and we're going to remove it",
      "offset": 2251.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "from that object as well just so that",
      "offset": 2252.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you get the most accurate representation",
      "offset": 2254.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and you're not thinking that like the",
      "offset": 2256.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "model read out a certain piece of text.",
      "offset": 2258.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And again with everything that we're",
      "offset": 2261.599,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "doing here can actually go back into",
      "offset": 2264.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "traces and we can see that same",
      "offset": 2266.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "representation here with the weather",
      "offset": 2268.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "call and everything. So again helps with",
      "offset": 2270.32,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "the debugging.",
      "offset": 2272.88,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "Go briefly back to the slides. So we",
      "offset": 2277.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "covered we set up our first agent.",
      "offset": 2278.8,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2282.56,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "The question was how do you store the",
      "offset": 2287.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "conversation history? Um so it's",
      "offset": 2288.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "currently fully stored in memory. Um, so",
      "offset": 2291.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "basically there's going to be a bunch of",
      "offset": 2294.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "events that that I logged out that are",
      "offset": 2296,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "being emitted by the real-time API. All",
      "offset": 2298.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of those are going to be sent over to",
      "offset": 2301.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "the client and then store it in memory",
      "offset": 2303.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in a conversation like in just like an",
      "offset": 2306.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "array essentially. So you can do",
      "offset": 2308.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "whatever you want with that by listening",
      "offset": 2310.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to that history updated event. So if you",
      "offset": 2311.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do want to store it somewhere, you can",
      "offset": 2314.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "store it. Um the other part is the",
      "offset": 2315.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "traces part is automatically going to be",
      "offset": 2319.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "stored on the open AAI uh platform as",
      "offset": 2321.119,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "long as you both enable that tracing you",
      "offset": 2324.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "can disable it um by default in the",
      "offset": 2327.359,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "agent SDK is enabled and then um the",
      "offset": 2330.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "other aspect of that is if you are a ZDR",
      "offset": 2333.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "customer so a zero data retention",
      "offset": 2336.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "customer of open AAI you don't have",
      "offset": 2338.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "access to that traces feature um\n about",
      "offset": 2340.4,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "the conversation",
      "offset": 2343.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Um, so the question was how much of the",
      "offset": 2357.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like voice context, how much of the",
      "offset": 2360.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "previous conversation is being used. Um,",
      "offset": 2362.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that's going to depend and sort of like",
      "offset": 2364.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "dealt with directly by the real-time",
      "offset": 2367.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "API. So like the real-time API when you",
      "offset": 2368.96,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "do start that session um that holds the",
      "offset": 2371.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "source of truth for that whole",
      "offset": 2375.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "conversation session. So what you're",
      "offset": 2377.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "receiving on the on the client side is",
      "offset": 2379.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "just a copy of whatever is happening at",
      "offset": 2382.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that point. It's not the source of truth",
      "offset": 2384.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of what we're going to adapt to pass",
      "offset": 2386.8,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "into the model.",
      "offset": 2389.2,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Um the question is how does it work with",
      "offset": 2411.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "like the inference cost and like whether",
      "offset": 2414.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you're passing like passing in that",
      "offset": 2416.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "whole conversation. Um, and Noob is",
      "offset": 2418.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "nodding. Um, he is the bigger expert",
      "offset": 2421.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "there. But yes, um, we're actually like",
      "offset": 2423.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "you can log the the like we're keeping",
      "offset": 2425.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "track of the usage. There's an event",
      "offset": 2429.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that you can like log out to see your",
      "offset": 2431.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "token cost. Um, so you have an idea of",
      "offset": 2432.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like what is being actually passed in.",
      "offset": 2435.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So like with every if we're going back",
      "offset": 2437.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "here to this example, you can see these",
      "offset": 2439.68,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "response done events. Um I don't know",
      "offset": 2442.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "where is the",
      "offset": 2446.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "um shouldn't it be on the response done?",
      "offset": 2448.8,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "Uh it is being sent over. I just do not",
      "offset": 2452.64,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "know right now why it's not showing. Oh",
      "offset": 2456.72,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "there. So you can see here um it outputs",
      "offset": 2460.16,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "the detailed information of your tok to",
      "offset": 2464.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "token usage at any point in time. So",
      "offset": 2466.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "while you don't have like access to like",
      "offset": 2468.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "what is exactly what was passed into the",
      "offset": 2470.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "next response generation um you can keep",
      "offset": 2473.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "track of the cost as it's happening.",
      "offset": 2476.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "You're welcome. Uh yes. Do you there's a",
      "offset": 2480.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "microphone right over there that might",
      "offset": 2483.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "be easier than you yelling across the",
      "offset": 2484.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "stage to me.\n I I see that the format",
      "offset": 2486.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that you're using is PCM16. Is there a",
      "offset": 2489.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "way in which we can modify the output",
      "offset": 2492,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "formats of the audio files so we can",
      "offset": 2494.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "save in memory? Um yeah, there are",
      "offset": 2496.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "different different audio modes that you",
      "offset": 2498.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "can use um including like for example u",
      "offset": 2501.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "law for that is like helpful for phone",
      "offset": 2504.079,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "calls for example.",
      "offset": 2506.48,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Sorry, another question on the usage. Um",
      "offset": 2510,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "does that like final assistant response",
      "offset": 2513.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "roll up all the tokens from like all the",
      "offset": 2516.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "intermittent tool calls as well? Does",
      "offset": 2518.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "that make sense? Like the agent needs to",
      "offset": 2521.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like kind of reason through and then",
      "offset": 2523.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "format tool calls. So I'm assuming it's",
      "offset": 2525.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "not just the output tokens for only the",
      "offset": 2527.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "assistant response, right?\n It like every",
      "offset": 2529.839,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "tool call is a response in general as",
      "offset": 2532.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "well. So like it it works the same way",
      "offset": 2536.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that like the responses API works for",
      "offset": 2539.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "example.\n Okay. So like each right",
      "offset": 2541.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "because we're using this and we have",
      "offset": 2544.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like tool calls and tool call outputs,",
      "offset": 2546.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "right? And I couldn't find the like",
      "offset": 2548,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "usage attribute on the tool call output.",
      "offset": 2549.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Is it somewhere in those like raw events",
      "offset": 2552.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that are outputed?",
      "offset": 2553.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Do you do you know a new\n Okay, no",
      "offset": 2555.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "worries. I I know it's like kind of",
      "offset": 2558.64,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "early on.\n We can we can follow up",
      "offset": 2559.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "behind.\n Thank you.\n You're welcome. Uh",
      "offset": 2560.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "yeah. Do you want to head over to that",
      "offset": 2562.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "microphone that is right behind you? It",
      "offset": 2564.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "just makes it a bit easier.",
      "offset": 2567.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Uh yeah, there's in the meantime\n just a",
      "offset": 2570.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "quick question. Can I go back to the",
      "offset": 2572.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "slides explaining the different modes of",
      "offset": 2574.24,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the uh the audio agents like the text in",
      "offset": 2576.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "text out that's the first one. Oh yeah,",
      "offset": 2580.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "text to speech that's the second one.",
      "offset": 2582.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Uh, I didn't get the third one. And uh",
      "offset": 2584.319,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "Oh, you mean this?",
      "offset": 2588.64,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 2592.319,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yes. It's uh when you just showed us the",
      "offset": 2595.68,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "GPT4 real time that one. Yeah. That",
      "offset": 2599.04,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "model is that uh this PPT this slide is",
      "offset": 2602.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "about.\n Yeah, exactly. So like where it's",
      "offset": 2605.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "like when we did the refund um it kind",
      "offset": 2608,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "of followed this pattern where it",
      "offset": 2611.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "performs a tool call like the like real",
      "offset": 2613.599,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "time API agent can perform tool calls.",
      "offset": 2616.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "It performed a tool call to trigger a",
      "offset": 2619.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "separate agent that was the refund agent",
      "offset": 2622.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "that in my case used 04 mini to execute",
      "offset": 2624.48,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "that task and then hand that back.",
      "offset": 2627.359,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "Okay, got it. Thanks.\n You're welcome.",
      "offset": 2630.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Yes. Um I'm currently using like a",
      "offset": 2633.52,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "regular uh OpenAI agents. So what will",
      "offset": 2636.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "be the challenge that I will face when I",
      "offset": 2640.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "would when I want to change my regular",
      "offset": 2642.16,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "regular agents to realtime agents?\n So",
      "offset": 2644.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "there's a couple of different challenges",
      "offset": 2648.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "like one is like anything that you're",
      "offset": 2650.4,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "doing around latency like like anything",
      "offset": 2654.4,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "you're doing around um voice latency is",
      "offset": 2656.8,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "always king. So like you want to figure",
      "offset": 2661.119,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "out what are the best ways to actually",
      "offset": 2663.92,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "have a slide around this. Um like when",
      "offset": 2666.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you when it comes to things like tool",
      "offset": 2670.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "calling, you want to find ways to do",
      "offset": 2671.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "things like buying yourself some time.",
      "offset": 2673.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So you will typically see some prompting",
      "offset": 2675.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "around like announce what you're about",
      "offset": 2677.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to do next um before you're doing it.",
      "offset": 2680.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "And that's to do that little trick",
      "offset": 2682.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "around uh while the while the previous",
      "offset": 2684.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "audio is still being read out, the agent",
      "offset": 2687.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "can already perform the tool call and",
      "offset": 2690.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "wait for the tool call to come back",
      "offset": 2693.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because similar to a textbased agent,",
      "offset": 2694.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the model can't do like can't receive",
      "offset": 2697.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "additional data as like like do another",
      "offset": 2699.68,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "thing. Um outside of like we can",
      "offset": 2703.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "interrupt the response but it can't",
      "offset": 2705.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "finish that response if that makes",
      "offset": 2707.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "sense. And so you want to do these sort",
      "offset": 2709.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "of like buying time. The other thing is",
      "offset": 2710.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "like if you're building a real-time",
      "offset": 2713.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "agent, the longer your prompt gets, at",
      "offset": 2715.119,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "one point it increases the likelihood",
      "offset": 2719.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that it gets confused. So you want to",
      "offset": 2721.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "make sure you're properly scoping those",
      "offset": 2722.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "use cases and like through what what we",
      "offset": 2724.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "call handoffs where you have different",
      "offset": 2728.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "agents that are more spec scope to",
      "offset": 2730.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "specific steps in your experience.",
      "offset": 2732.96,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "Thank you.\n You're welcome.",
      "offset": 2736,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 2740.4,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah. So you're uh",
      "offset": 2750.96,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "Yeah. So the question is about memory.",
      "offset": 2755.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Um basically",
      "offset": 2757.92,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "right now the the the",
      "offset": 2760,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "let me correct this. Um, when we go back",
      "offset": 2764.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to this demo, what you're seeing here is",
      "offset": 2767.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "essentially just like a copy of the",
      "offset": 2770.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "events that we're receiving back. So,",
      "offset": 2772.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this is like helpful as a visualization",
      "offset": 2774.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "of the history. Uh, that being said, the",
      "offset": 2776.56,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "actual like memory in the sense of like",
      "offset": 2779.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "an LM agent memory is the session",
      "offset": 2783.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "context that is happening on the",
      "offset": 2785.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "real-time API side. There are events",
      "offset": 2787.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "that you can use to update that. Um, we",
      "offset": 2789.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "actually have an update history event",
      "offset": 2792.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that you can pass in what you want the",
      "offset": 2794.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "history to be. But what that does is",
      "offset": 2796.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "essentially like fire off events to the",
      "offset": 2798.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "real-time API to say like delete this",
      "offset": 2801.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "item from the history or add this new",
      "offset": 2803.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "item. Um, and you can give it a previous",
      "offset": 2805.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "item ID. So like you can for example",
      "offset": 2808.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "like slot messages into a specific spot",
      "offset": 2810.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "if you wanted to. Um, does that make",
      "offset": 2813.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "sense? But there's like no no like",
      "offset": 2815.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "advanced like long-term memory solution",
      "offset": 2818.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "like like you were alluding to. Cool.",
      "offset": 2821.76,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Yes.\n Hi. Do you have tips for handling",
      "offset": 2825.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "input from low fluency users? Like say",
      "offset": 2828.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "someone who's just learning a language",
      "offset": 2831.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and they have like multilingual input",
      "offset": 2833.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and maybe broken grammar and their",
      "offset": 2835.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "pronunciation is not so good.\n Uh I don't",
      "offset": 2837.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "think I have any like best practices",
      "offset": 2839.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "right now that I could share. Can it",
      "offset": 2842.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "handle it just off the shelf or?\n Um, it",
      "offset": 2844.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "can handle like switching languages and",
      "offset": 2847.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and things like that.\n Okay. But it might",
      "offset": 2849.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "not be able to handle low fluency.",
      "offset": 2851.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "Um, I don't know if we have any use",
      "offset": 2854.88,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "cases.",
      "offset": 2857.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah, we have we have some customers",
      "offset": 2864.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "that are like language learning",
      "offset": 2865.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "companies. So there there is some that",
      "offset": 2867.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "are using it that way. But I don't think",
      "offset": 2869.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "I have any like best practices that I",
      "offset": 2870.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "can share. Okay. Thank you.\n You're",
      "offset": 2872.48,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "welcome.",
      "offset": 2874.079,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Sorry. Back in the code, is there a call",
      "offset": 2878.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "back for the interrupt and does it",
      "offset": 2881.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "include the last transcription?\n Um,",
      "offset": 2883.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "there is a call for the interrupt, but",
      "offset": 2886.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "uh there is no",
      "offset": 2889.359,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2891.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "there's no actual like event that",
      "offset": 2895.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "there's no parameter E that comes with",
      "offset": 2898.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it or anything like that. No, there's",
      "offset": 2900,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "currently no um transcript. So what you",
      "offset": 2901.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "can do is if you're getting this, you",
      "offset": 2904.16,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "can call\n get history or something.\n Uh",
      "offset": 2907.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "there's just history. Um so like this",
      "offset": 2910.319,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "always is up to date.\n Um so you can you",
      "offset": 2912.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "have access in that moment. The the",
      "offset": 2915.839,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "thing that we do have is um for tool",
      "offset": 2917.92,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "called specifically",
      "offset": 2923.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "um you're getting some additional",
      "offset": 2925.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "context and that context has a",
      "offset": 2927.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "um history parameter that you can like",
      "offset": 2931.599,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "push into.\n Okay.\n Um it's more documented",
      "offset": 2934,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "in the in the documentation\n the API.",
      "offset": 2937.119,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Okay. Great.\n Thank you.\n You're welcome.",
      "offset": 2939.839,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "Awesome. Um, let's move a bit on and",
      "offset": 2943.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "show a couple of other things. So, we",
      "offset": 2947.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "talked about tools. As I said, like one",
      "offset": 2949.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of the benefits is you can reuse the",
      "offset": 2951.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "same syntax that you're doing with",
      "offset": 2953.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "textbased ones. Um, it's also a good way",
      "offset": 2954.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "for you to then communicate with your",
      "offset": 2957.28,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "backend systems using HTTP. um follows",
      "offset": 2959.359,
      "duration": 8.801
    },
    {
      "lang": "en",
      "text": "sort of a um",
      "offset": 2964.319,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "general practice around like keeping",
      "offset": 2968.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "both the tool calls as low latency as",
      "offset": 2971.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "possible. like send out a tool like for",
      "offset": 2974.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "example if you know a task is going to",
      "offset": 2977.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "take longer, start the task, give it a",
      "offset": 2979.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "task ID and have the agent have a tool",
      "offset": 2982.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "to check on the status for example like",
      "offset": 2984.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "that helps getting back to it because",
      "offset": 2987.359,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "again while the tool call is going on",
      "offset": 2990.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the model is sort of stuck. So you want",
      "offset": 2993.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to you want to make sure to like get get",
      "offset": 2995.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "back to that as soon as possible. Um one",
      "offset": 2997.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of the other things that you can do is",
      "offset": 3000.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "human appro uh uh human approval. I can",
      "offset": 3001.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "show you that quickly. There's",
      "offset": 3004.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "essentially a use uh it needs approval",
      "offset": 3006.64,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "that um you can either specify as a",
      "offset": 3011.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "function that will be evaluated before",
      "offset": 3014.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "the tool ever gets triggered. This is a",
      "offset": 3016.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "great way if you have like a more",
      "offset": 3019.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "complex",
      "offset": 3020.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "logic on I need approval for this. You",
      "offset": 3021.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "can also give it just straight up I",
      "offset": 3024.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "always need approval at which point",
      "offset": 3027.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "there is a",
      "offset": 3029.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "another event here. um",
      "offset": 3032,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "tool approval requested and then that",
      "offset": 3035.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "gets a",
      "offset": 3038.079,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "um event here. So we can do things like",
      "offset": 3040.079,
      "duration": 8.441
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3045.52,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "good old prompt",
      "offset": 3051.44,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "um and then we can go and approve that",
      "offset": 3053.76,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "tool call again.",
      "offset": 3058.079,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "I don't know why the autocomplete is not",
      "offset": 3062.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "working.",
      "offset": 3064.4,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 3067.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "proof.",
      "offset": 3073.119,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "There we go. And",
      "offset": 3074.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "why is it",
      "offset": 3081.68,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Oh, this is wrong.",
      "offset": 3090.079,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "This is where I go into the docs because",
      "offset": 3093.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "I do not remember why this is",
      "offset": 3096.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "autocompleting the wrong way. But",
      "offset": 3099.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "everything I'm showing you is in the",
      "offset": 3101.44,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "docs. Um, so we can just",
      "offset": 3103.04,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "Oh, took the wrong thing. Right. The",
      "offset": 3107.599,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "first",
      "offset": 3110.88,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "There we go. Approval request.",
      "offset": 3115.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "And then we can do",
      "offset": 3120.8,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "approval request. Thank you. It's like",
      "offset": 3127.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the classic thing when you're on stage",
      "offset": 3129.92,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "and you can't really",
      "offset": 3131.68,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "There we go. So, in this case, I'm just",
      "offset": 3136.88,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "going to always approve. But if we now",
      "offset": 3140.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "go in,",
      "offset": 3144.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "hey, um, can you tell me the weather in",
      "offset": 3146.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Seattle?",
      "offset": 3149.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "So we can in that case approve it. It's",
      "offset": 3154.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "always going to approve right now",
      "offset": 3156.96,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "because I'm not actually checking the",
      "offset": 3158,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "status. But um that means you can build",
      "offset": 3159.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like a human in the loop approval",
      "offset": 3161.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "experience.",
      "offset": 3163.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "This is really convenient especially if",
      "offset": 3165.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you're running it in the browser and you",
      "offset": 3166.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "just want to have like a confirmation if",
      "offset": 3168.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like the tool is hallucinating things",
      "offset": 3169.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "before the customer actually submits it.",
      "offset": 3171.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And does it do it directly? kind of",
      "offset": 3174.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually say you okay if I do this or",
      "offset": 3176.559,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "not to do that and then do voice extra",
      "offset": 3178.88,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "voice\n so the the basically this is",
      "offset": 3182.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "happening so the question is does it",
      "offset": 3185.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "automatically do this like the what",
      "offset": 3187.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "we're doing and the reason why this is",
      "offset": 3190.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "separate is the model is asking for this",
      "offset": 3192.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tool to be executed but we're",
      "offset": 3195.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "intercepting this um basically before",
      "offset": 3197.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "we're ever generating or executing the",
      "offset": 3199.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "response this is intentional so that",
      "offset": 3202.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "like you don't have to deal with like we",
      "offset": 3204.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "want you to think through why should",
      "offset": 3207.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this tool need approval as opposed to",
      "offset": 3210,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "doing that somewhere halfway through",
      "offset": 3212.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "your tool execution and you have to like",
      "offset": 3214.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "deal with the consequence of rolling",
      "offset": 3216.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "back every decision that you've made for",
      "offset": 3218.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "example and so by default if this is",
      "offset": 3220.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "just needs true it cannot get past that",
      "offset": 3222.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "until the execution was approved at",
      "offset": 3225.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "which point it stores it in the context",
      "offset": 3228.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that is stored locally and then bypasses",
      "offset": 3230.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that security. So this is not happening",
      "offset": 3233.2,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "on the on the model level.",
      "offset": 3235.52,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Um I'm going to remove that again.",
      "offset": 3240.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Uh so the other thing we talked about",
      "offset": 3243.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "already but I want to show it in",
      "offset": 3245.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "practice is handoff. So a handoff is",
      "offset": 3247.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "essentially just a specialized tool uh",
      "offset": 3249.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "call that resets the configuration of",
      "offset": 3252.319,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "the agent in the session so that um we",
      "offset": 3255.119,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "can update the system instructions, we",
      "offset": 3259.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "can update the tools and make sure that",
      "offset": 3261.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we can nicely scope the task of what",
      "offset": 3263.92,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "we're trying to um solve. So what you",
      "offset": 3266.4,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "cannot do, I know people probably gonna",
      "offset": 3271.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "ask about this is you can't change the",
      "offset": 3273.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "voice of the agent mid session. You",
      "offset": 3276.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "could define different voices on",
      "offset": 3279.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "different agents, but the moment that",
      "offset": 3281.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you're like the first agent that starts",
      "offset": 3283.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "talking, that's the voice that we're",
      "offset": 3286.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "going to stick with throughout the",
      "offset": 3288.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "entire conversation. So that's a caveat",
      "offset": 3290.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "to just keep in mind. Um, but they're",
      "offset": 3292.4,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "still very helpful to let's say have a",
      "offset": 3295.44,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "weather agent here.",
      "offset": 3299.119,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "Sorry.",
      "offset": 3306,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Um, and what we're going to do in this",
      "offset": 3309.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "one is called weather agent.",
      "offset": 3312.319,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 3315.76,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3323.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "let's give we'll do this and then what",
      "offset": 3329.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "we can do is we can actually give it a",
      "offset": 3332.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "handoff description. So if you don't",
      "offset": 3334.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "want to have this in your system prompt,",
      "offset": 3335.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "but you just want to help the model",
      "offset": 3337.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "understand when to use this, you can say",
      "offset": 3339.119,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "like uh this agent is an expert in",
      "offset": 3341.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "weather. And then this one is going to",
      "offset": 3346,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "have that weather tool. We're going to",
      "offset": 3347.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "remove it from this one and we're going",
      "offset": 3349.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to give it a handoff instead to that",
      "offset": 3351.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "other weather agent.",
      "offset": 3353.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "So now if I'm going to restart this,",
      "offset": 3355.359,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "hey, can you tell me the weather in uh",
      "offset": 3358.96,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "New York?",
      "offset": 3362.079,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "The weather in New York is sunny, so you",
      "offset": 3365.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "might want to grab your sunglasses if",
      "offset": 3368.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you're heading outside. Enjoy the day.",
      "offset": 3370.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "All right, that's that's the model's",
      "offset": 3373.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "best attempt at New York accent. Uh,",
      "offset": 3375.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "we'll take it. Um, but you can see there",
      "offset": 3378.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that like it automatically handed off",
      "offset": 3381.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "from that first agent to that second one",
      "offset": 3382.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and let it handle it. You can through",
      "offset": 3385.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "prompting do things like do you want it",
      "offset": 3386.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to announce that it's about to hand off?",
      "offset": 3389.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Do you not want to do that? Sometimes",
      "offset": 3391.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it's a bit awkward if you're forcing it",
      "offset": 3393.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to always do it. So like I would not",
      "offset": 3395.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "necessarily try it, but maybe that's the",
      "offset": 3397.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "type of experience that you want to",
      "offset": 3399.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have. Um,",
      "offset": 3400.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "so that's handoffs. Let me do you a",
      "offset": 3403.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "favor and push that code.",
      "offset": 3405.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Baby push.",
      "offset": 3408.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "So",
      "offset": 3410.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the change voice, but it can change",
      "offset": 3412.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "accents.",
      "offset": 3415.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Yeah. So uh, that's a good question. and",
      "offset": 3416.96,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the the the agent can ch can't change",
      "offset": 3419.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "the voice but it can change the accent.",
      "offset": 3423.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Um that's because the",
      "offset": 3425.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "um like again this goes back to like the",
      "offset": 3428.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "model is a generative model. So you can",
      "offset": 3431.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "prompt it to have different like",
      "offset": 3434.559,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "pronunciations, tonality like voice in",
      "offset": 3437.599,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "that sense but it cannot change the",
      "offset": 3441.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "voice model that is actually being used",
      "offset": 3444.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to generate that output.",
      "offset": 3446.96,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "So maybe it",
      "offset": 3449.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "is it like the whole real time request",
      "offset": 3453.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "body that can't be changed or just the",
      "offset": 3456.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "voice. So can I like create a tool that",
      "offset": 3458.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "could adjust the speed if someone was",
      "offset": 3460.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "saying it's talking too fast for the",
      "offset": 3462.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "noise reduction?",
      "offset": 3465.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "You should be able to change I I have",
      "offset": 3467.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "not tried the speed because speed",
      "offset": 3469.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "parameter changing at mid session",
      "offset": 3471.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "because it literally came out today. Um",
      "offset": 3472.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "uh I don't know Anoop if you tried this.",
      "offset": 3476.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "No. Um but like you can like essentially",
      "offset": 3478.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a handoff does change the session",
      "offset": 3481.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "configuration. Like if we look back at",
      "offset": 3483.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "like um the",
      "offset": 3485.44,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "like one of the transcripts here like",
      "offset": 3490.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "now that we have a handoff",
      "offset": 3492.72,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "um let's go to this trace. So you can",
      "offset": 3495.04,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "see here that like it called the",
      "offset": 3498.799,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "transfer to weather agent. Um but then",
      "offset": 3500.559,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "like the this these instructions were",
      "offset": 3503.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "talk to talk talk with a New York",
      "offset": 3507.119,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "accent. Um so in this case it did change",
      "offset": 3509.04,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "the instructions midway through the",
      "offset": 3513.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "session. And the same way like when that",
      "offset": 3515.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "handoff happens we take away the tools",
      "offset": 3517.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "we give it new tools. So you can change",
      "offset": 3520,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "those tools. You could have a tool to",
      "offset": 3522.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "change the tool but my recommendation",
      "offset": 3525.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "would be like use a different agent for",
      "offset": 3527.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that. Um, but then like the speed",
      "offset": 3529.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "control like you could you should be",
      "offset": 3531.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "able to send off an event but I have not",
      "offset": 3533.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tried that.\n Yeah. Maybe like the",
      "offset": 3535.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "background basically like if you had",
      "offset": 3538.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "something and someone was like in a",
      "offset": 3540.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "noisy environment like hey you seem to",
      "offset": 3542.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "be getting interrupted could you adjust",
      "offset": 3545.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we catch in the background and start",
      "offset": 3548.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "adjusting that parameter so just the",
      "offset": 3549.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "voice.",
      "offset": 3552.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yeah. So the the question is like for",
      "offset": 3554.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "example if someone is like in a noisy",
      "offset": 3556.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "environment like could you have the",
      "offset": 3558.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "agent detect that and then use um like",
      "offset": 3561.04,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "adjust some of the session configuration",
      "offset": 3564.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "to deal with that and just the voice is",
      "offset": 3567.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "protected. I don't know honestly which",
      "offset": 3569.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "parameters are protected or not. The",
      "offset": 3572.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "good thing is the like the API will",
      "offset": 3574.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "throw an error if if a thing didn't",
      "offset": 3576,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "work. Um so it's a good way it's a good",
      "offset": 3578.24,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "thing to experiment with.\n Um Python",
      "offset": 3580.16,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "with the previous ones you could do that",
      "offset": 3585.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Python.\n Oh yeah. Um well with the Python",
      "offset": 3587.04,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "uh agents SDK we're doing the chained",
      "offset": 3590.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "approach. We don't have a real time",
      "offset": 3593.359,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "built in yet. So",
      "offset": 3595.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "just calling the old API Python.\n Oh all",
      "offset": 3599.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "right. Yeah. Yeah. Um then it should",
      "offset": 3602.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "work. Yeah. Uh if you can do it in",
      "offset": 3605.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Python like it should just work.",
      "offset": 3606.799,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "Cool. Um",
      "offset": 3610.079,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "so the other thing we talked about um is",
      "offset": 3613.44,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "this delegation part. So that's what I",
      "offset": 3616.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "like had mentioned earlier that was in",
      "offset": 3619.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "the diagram. So this is essentially",
      "offset": 3622.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "where you want to be able to have",
      "offset": 3624.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "certain complex tasks dealt with by a",
      "offset": 3627.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "more intelligent model. Um and the way",
      "offset": 3630.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we can do that is essentially just",
      "offset": 3632.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "creating another",
      "offset": 3634.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "um agent except on the back end. And",
      "offset": 3636.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "because it's because the TypeScript SDK",
      "offset": 3639.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "works both in the front end and back",
      "offset": 3641.76,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "end, we can do that through um",
      "offset": 3643.2,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "uh I think I have a let's see if we have",
      "offset": 3648.799,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "a file here or not. Uh we can do that",
      "offset": 3651.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "using the same SDK. So I'm going to",
      "offset": 3655.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "create on the in the server folder here",
      "offset": 3657.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "a new uh file. I'm going to call just",
      "offset": 3660,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "agent.",
      "offset": 3663.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And in here we can build our regular",
      "offset": 3665.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "textbased agent. So this is essentially",
      "offset": 3668.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the same code that we've done before. Um",
      "offset": 3670.24,
      "duration": 10.4
    },
    {
      "lang": "en",
      "text": "and we can say this is a I don't know um",
      "offset": 3673.68,
      "duration": 10.399
    },
    {
      "lang": "en",
      "text": "called the Riddler. Um",
      "offset": 3680.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "you are excellent",
      "offset": 3684.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "at creating",
      "offset": 3686.4,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "riddles based",
      "offset": 3688.64,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "on a target demographic.",
      "offset": 3691.839,
      "duration": 8.641
    },
    {
      "lang": "en",
      "text": "and topic. Um, and we'll just give it a",
      "offset": 3696.559,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "model of",
      "offset": 3700.48,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "O4 mini.",
      "offset": 3702.4,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "Also, a reminder for those if you are",
      "offset": 3707.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "trying to follow along um, and you run",
      "offset": 3709.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "into troubles, post in the Slack and the",
      "offset": 3711.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Noob can help you with that. Um, so we",
      "offset": 3714.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "have that new agent here. We're not",
      "offset": 3718,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "going to give it any tools or anything.",
      "offset": 3719.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Uh and then we can export a function",
      "offset": 3721.76,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 3725.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "that we just call run agent.",
      "offset": 3729.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And this is just going to take some",
      "offset": 3732.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "input and then return that output. And",
      "offset": 3735.599,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "we can go back into our front end code,",
      "offset": 3738.96,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "create a new tool here, create riddle.",
      "offset": 3742.319,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "Um, and this one we're just going to",
      "offset": 3747.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "have take like two parameters, the",
      "offset": 3750.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "demographic and the topic, and then call",
      "offset": 3752.64,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "out the run agent function that is going",
      "offset": 3756.559,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "to run on the server. We can pass in",
      "offset": 3761.2,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "realize I didn't specify this. Let's do",
      "offset": 3765.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "demographic and topic and then create an",
      "offset": 3768.559,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "input here of",
      "offset": 3771.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "this. The other thing you want to do",
      "offset": 3775.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "when you're",
      "offset": 3778,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "using server actions in uh next is put",
      "offset": 3780,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that use server at the top. That makes",
      "offset": 3783.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "sure that this file executes on the",
      "offset": 3786.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "server. And then we can pass in that",
      "offset": 3788.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "demographic. And again, if you're using",
      "offset": 3791.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "a different framework, this is just the",
      "offset": 3792.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "equivalent of a fancy fetch request. So",
      "offset": 3795.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "like if you want to do an HTTP request",
      "offset": 3797.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to your service, if you want to maintain",
      "offset": 3799.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "a websocket connection to your own",
      "offset": 3801.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "backend, you can do all of those things",
      "offset": 3804.079,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "to like talk back to other systems.",
      "offset": 3806,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "So with that, we can give that to our",
      "offset": 3810.799,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "main agent.",
      "offset": 3813.92,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Riddle",
      "offset": 3818.319,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "tools",
      "offset": 3822.4,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "And then what you want to do in these",
      "offset": 3827.2,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "cases is like um",
      "offset": 3829.52,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "you can tell it like announce when you",
      "offset": 3835.92,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "are about to do a task.",
      "offset": 3839.359,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "Don't say you are calling a tool.",
      "offset": 3843.68,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Things like that can be helpful to like",
      "offset": 3848.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "buy itself some time.",
      "offset": 3850.64,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "Hey there. Um, can you tell me a riddle",
      "offset": 3854.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "uh for like a five-year-old Star Wars",
      "offset": 3857.92,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "fan?",
      "offset": 3860.079,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Hey, are you there?",
      "offset": 3866.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "I'm still here. I'm working on creating",
      "offset": 3868.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "that Star Wars riddle for you. It should",
      "offset": 3871.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "be ready in just a moment. Here's a",
      "offset": 3874.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "riddle for your little Star Wars fan.",
      "offset": 3876.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "I'm not.\n So you can see that like",
      "offset": 3879.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "because it announced that like what it's",
      "offset": 3882.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "about to do, the tool call came back",
      "offset": 3884.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "before it even finished what it",
      "offset": 3888.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "previously said. And so like that's",
      "offset": 3890.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "again one of the benefits of like if you",
      "offset": 3892.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "can get your agent to um balance out",
      "offset": 3894.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that and like buy itself some time. This",
      "offset": 3897.68,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "is a good way to uh you know deal with",
      "offset": 3899.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "the more complex tasks. And like it also",
      "offset": 3903.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "means that you can like for example take",
      "offset": 3905.44,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "all of the like more reasoning heavy",
      "offset": 3908.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "workloads and take it out out of the",
      "offset": 3911.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "voice agent model.",
      "offset": 3913.119,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "Um for delegation. Yeah.\n Is it possible",
      "offset": 3915.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to delegate to more than one agent like",
      "offset": 3919.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "simultaneously or is it just one in the",
      "offset": 3921.52,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "current SDK? Um you can it's it's tool",
      "offset": 3923.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "called so like I think you like you",
      "offset": 3928.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "would have two options right like you",
      "offset": 3929.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "could do par like parallel tool calling",
      "offset": 3931.44,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "or you could um like have one tool that",
      "offset": 3934.079,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "then triggers running multiple agents",
      "offset": 3939.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "right so like my recommendation would be",
      "offset": 3942,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that part potentially",
      "offset": 3944.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uh so that you're not relying on the",
      "offset": 3946.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "model making the right decision of",
      "offset": 3948.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "calling multiple tools at the same time",
      "offset": 3950.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like you want to make the decision",
      "offset": 3952.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "making for the voice agent always as",
      "offset": 3954.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "easy as possible.\n Thanks.\n Yeah,",
      "offset": 3956.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "previously when you did you go back to",
      "offset": 3960,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "previous page.\n Yeah.\n Oh, no. Sorry. Your",
      "offset": 3962,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "example.",
      "offset": 3964.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Which example?\n The one you're running",
      "offset": 3966.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "where it had the output.\n Oh, yeah.\n So on",
      "offset": 3969.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "line three there where it said I'm still",
      "offset": 3972.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "here.\n Yeah.\n Is that coming from your",
      "offset": 3974.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "SDK? Do I have to use the SDK to do that",
      "offset": 3977.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "or is the real time API?\n No, it's the",
      "offset": 3979.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "real time API. It responded because I",
      "offset": 3981.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "asked like, &quot;Hey, are you there?&quot;\n Um, so",
      "offset": 3983.68,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "it it it tri it realized that like it",
      "offset": 3987.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "didn't start anything and like",
      "offset": 3991.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interrupt. I was like, &quot;Hey,\n going.&quot;",
      "offset": 3992.72,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 3995.119,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 3998.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "yeah. So, the thing is that like I",
      "offset": 4000.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "didn't render out the tool calls in",
      "offset": 4002.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "here, right? So like what basically",
      "offset": 4005.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "happened between this and this was like",
      "offset": 4006.96,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "it started off a tool call um and then",
      "offset": 4010.319,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "that tool call because I interrupted it",
      "offset": 4014.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "then it stopped that tool call because I",
      "offset": 4016.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "interrupted it. It stopped the the",
      "offset": 4020.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "generation it also reset that like",
      "offset": 4022.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "transcript here. it's a good indicator",
      "offset": 4025.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that like the interruption happened and",
      "offset": 4027.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "so when I said this it remembered it was",
      "offset": 4029.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "trying to call a tool did that tool call",
      "offset": 4031.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "um and then gave you back the response",
      "offset": 4034.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "so that's all the just the the regular",
      "offset": 4037.359,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "realtime API",
      "offset": 4040,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "any other questions around this",
      "offset": 4044,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "what's the cost per minute\n what's the",
      "offset": 4049.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "cost per minute we charge per token",
      "offset": 4051.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "there's some translations I I don't",
      "offset": 4053.599,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "know. Nuke, do you have the",
      "offset": 4056.319,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "right? So it it's more expensive than",
      "offset": 4070,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "TTS and uh uh and like speech to text",
      "offset": 4072.799,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "chained up with a model some like in",
      "offset": 4077.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "most cases, but it depends on the use",
      "offset": 4080,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "case um and sort of like your model",
      "offset": 4081.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "choices and stuff.",
      "offset": 4084,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "So, a bit harder to say like what the",
      "offset": 4086.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "per minute pricing is because again it's",
      "offset": 4088.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "by tokens and it also depends on like if",
      "offset": 4089.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you have like transcription turned on",
      "offset": 4092.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and like how many function calls you",
      "offset": 4094.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "have and things like that because it's a",
      "offset": 4096.48,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "mix between audio and text tokens.",
      "offset": 4098.08,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "Um so one of the interesting things and",
      "offset": 4102.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "this is not a thing in the in the",
      "offset": 4105.759,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "regular API this is a agent agents SDK",
      "offset": 4108.319,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "specific thing is um guard rail. So like",
      "offset": 4111.12,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "the agents SDK both in Python and",
      "offset": 4114.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "TypeScript has this concept of guard",
      "offset": 4116.799,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "rails that can either protect your input",
      "offset": 4118.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "or your output to make sure that like",
      "offset": 4120.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the agent is not being meddled with or",
      "offset": 4122.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "does things that are against policy. Um",
      "offset": 4125.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we took that same pattern and moved it",
      "offset": 4127.679,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "over to the real time side um where",
      "offset": 4129.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "essentially we're running these guard",
      "offset": 4133.279,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "rails that you can define in parallel on",
      "offset": 4134.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "on top of the transcription at all",
      "offset": 4138.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "times. You can kind of specify, you can",
      "offset": 4140.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "see it at the bottom here, like how",
      "offset": 4142.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "often you want to run them or if you",
      "offset": 4144.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "only want to run them when the full",
      "offset": 4145.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "transcript is available. But this is a",
      "offset": 4147.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "great way for you to like make sure that",
      "offset": 4149.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the model for doesn't violate certain",
      "offset": 4151.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "policies. You want to make sure that",
      "offset": 4154.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "these run as efficiently as possible",
      "offset": 4156.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because they're still running in the",
      "offset": 4158.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "client. Um, but this is a good way to",
      "offset": 4159.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "still fix cert like stick to certain",
      "offset": 4162.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "policies and if it violates those it",
      "offset": 4164.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "will interrupt it. Now there is sort of",
      "offset": 4167.44,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the bit of the cav a bit of a caveat",
      "offset": 4169.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "where because we're running this on a",
      "offset": 4171.759,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "transcript",
      "offset": 4173.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "um it",
      "offset": 4174.799,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "results in a bit of a timing aspect",
      "offset": 4178,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "where if it if it would violate your",
      "offset": 4180.88,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "guard rail in the first couple of words,",
      "offset": 4184.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "chances are it will say those first",
      "offset": 4187.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "couple of words and then get",
      "offset": 4189.12,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "interrupted. If it is happening at a",
      "offset": 4190.96,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "later point in time um then",
      "offset": 4194.719,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "the transcript will be completed or like",
      "offset": 4199.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the text output is not really a",
      "offset": 4201.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "transcript the text output will be done",
      "offset": 4203.36,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "before the audio is done speaking every",
      "offset": 4207.04,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "like is is done saying everything and so",
      "offset": 4210.159,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "in that case it will just correct",
      "offset": 4215.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "itself. So in that ca like to give you",
      "offset": 4216.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "an example of like this is a guardrail",
      "offset": 4218.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that just checks like is there the word",
      "offset": 4220.48,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "dom in the output in this case like if I",
      "offset": 4222.96,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "would ask it like hey please call me Dom",
      "offset": 4227.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "chances are it will call me dom and then",
      "offset": 4230.4,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "selfcorrect. If I tell it to tell me a",
      "offset": 4232.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "story and only introduce DOM in the",
      "offset": 4236.239,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "second act, then it will um catch that",
      "offset": 4238.56,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "it was trying to do that in a at a much",
      "offset": 4243.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "earlier point because that transcript is",
      "offset": 4246.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "going to be done before the audio is",
      "offset": 4248.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "being complet uh before the audio is",
      "offset": 4251.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "being read out to the user. So the user",
      "offset": 4253.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "will never hear DOM, but instead the",
      "offset": 4255.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "model is going to be like, okay, I'm",
      "offset": 4257.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "sorry I I couldn't help you with that.",
      "offset": 4258.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Like, let's do something else instead.",
      "offset": 4260.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And you can give it um",
      "offset": 4262.48,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "policy hints essentially on like why it",
      "offset": 4266.159,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "violated this policy or what it should",
      "offset": 4269.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "do instead. So you can give it the this",
      "offset": 4272.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like output info where you can inform",
      "offset": 4274.719,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "the model like why this happened.",
      "offset": 4276.719,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "No. Um you can choose what uh so the",
      "offset": 4282.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "question was is the transcript still",
      "offset": 4286.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "done with whisper? Uh you can you can",
      "offset": 4287.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "switch the transcript model. So we have",
      "offset": 4289.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "we released in March uh two models. One",
      "offset": 4292.08,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "is GPT4 mini transcribe",
      "offset": 4295.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 4298.239,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "we have two right? Yeah. And GPT4 u GPT4",
      "offset": 4300.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "transcribe. I was trying to remember",
      "offset": 4304.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like we have only one text to speech",
      "offset": 4306.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "model but we have two transcribe models",
      "offset": 4308.64,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "for transcribe models. Awesome.",
      "offset": 4311.28,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Um, this is the main part of what I",
      "offset": 4316.239,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "wanted to walk with you all through. So,",
      "offset": 4319.12,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "um, one, I'm going to, um,",
      "offset": 4321.84,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "post all of the links and the slides in",
      "offset": 4326.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the Slack channel, which let me go back",
      "offset": 4328.56,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "to",
      "offset": 4332.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "where was that slide?",
      "offset": 4335.12,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "There we go. Um, so in that Slack I'm",
      "offset": 4337.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "going to post all of the resources. So",
      "offset": 4340.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "if you want to check them out",
      "offset": 4343.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "afterwards, I'll also like I already put",
      "offset": 4344.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a bunch of the resources that I talked",
      "offset": 4347.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "about into the bottom of that starter",
      "offset": 4349.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "repository. So you should have access",
      "offset": 4352.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "there as well. And uh I'm happy to hang",
      "offset": 4354.08,
      "duration": 8.68
    },
    {
      "lang": "en",
      "text": "around answer any questions. Yes.",
      "offset": 4357.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "My understand",
      "offset": 4367.36,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "Yeah. Um",
      "offset": 4392.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "in the API.",
      "offset": 4395.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Yeah, the the question was around how",
      "offset": 4398.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "prom caching works um and sort of",
      "offset": 4400,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "whether we like for prompt caching that",
      "offset": 4402.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "like we guide the requests to the same",
      "offset": 4405.679,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "um same system again to run and whether",
      "offset": 4408.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "like there's any control with that with",
      "offset": 4412.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "real time because latency obviously",
      "offset": 4414.239,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "matters. Um I don't think there is any",
      "offset": 4416.64,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "controls about that. Um,",
      "offset": 4419.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "no. Um, I'm getting a no from there. So,",
      "offset": 4423.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like I don't think there's any any",
      "offset": 4425.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "controls around that right now.",
      "offset": 4427.12,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "Yes.\n Hey, so we all know that having",
      "offset": 4430.32,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "natural conversations is involves more",
      "offset": 4434.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "than just a spoken words. It involves",
      "offset": 4437.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "detecting emotion and adjusting tone.",
      "offset": 4440,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "Um, it also involves a cadence and",
      "offset": 4442.96,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "even humming to let the other person",
      "offset": 4448.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "know that you are listening.\n Mhm.\n I",
      "offset": 4450.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "wonder if the current speechtoech model",
      "offset": 4452.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is capable of having that kind of",
      "offset": 4454.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "natural conversation.",
      "offset": 4457.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Um, part of this is like a a prompting",
      "offset": 4459.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "challenge. So like it it definitely can",
      "offset": 4462.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "have pretty natural sounding",
      "offset": 4465.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "conversations. And like I think this is",
      "offset": 4467.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "sort of the the part where I highly",
      "offset": 4469.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "recommend to check out the openai.fm",
      "offset": 4472.48,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "page. Um because this is like it's it's",
      "offset": 4475.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "pretty interesting to see sort of uh if",
      "offset": 4480.08,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "we go to um let's see if we find the",
      "offset": 4482.32,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4487.6,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "actually I didn't show one neat feature",
      "offset": 4489.28,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "that I normally call out on the",
      "offset": 4494.159,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "playground. If you're just getting",
      "offset": 4498.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "started with real time and you don't",
      "offset": 4499.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "even want to write any line of code, um",
      "offset": 4501.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "this is a great way to just have",
      "offset": 4504.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "conversations and try things out. But",
      "offset": 4505.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "one of the things it has is this. Um it",
      "offset": 4507.6,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "has a couple of system prompts. One of",
      "offset": 4510.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "my favorite ones to show sort of this is",
      "offset": 4512.719,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "the board teenager. Um so if we start",
      "offset": 4515.679,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 4518.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Hey there. Um, so I'm at AI Engineer",
      "offset": 4522.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "World's Fair and everyone is super",
      "offset": 4525.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "stoked about voice agents. Can you show",
      "offset": 4527.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "me some excitement of this whole thing",
      "offset": 4529.52,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "launching today?",
      "offset": 4531.84,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "See,",
      "offset": 4536.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "I guess it's cool. Whatever. There's",
      "offset": 4538.48,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "always new stuff launching. People get",
      "offset": 4542.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "excited, but you know, it's just voice",
      "offset": 4544.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "agents. Not really my thing to get all",
      "offset": 4547.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "hyped up about it.",
      "offset": 4550.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So you can see in this case like it it",
      "offset": 4553.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "put its own pauses in there and stuff",
      "offset": 4555.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "like this wasn't a pause because the",
      "offset": 4558.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "model was waiting, right? Like um it can",
      "offset": 4559.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "deal with a lot of that sort of",
      "offset": 4562.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "adjusting tone and voice and it can do",
      "offset": 4564.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "similar things like reacting to like",
      "offset": 4567.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "someone talking in and stuff.\n Okay,",
      "offset": 4570,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "thanks. You're welcome.\n Yes.\n So again on",
      "offset": 4572.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "the new API that's been released, what",
      "offset": 4576.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "else has changed? Semantic has improved.",
      "offset": 4579.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Is there anything improve in terms of",
      "offset": 4581.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "tone detection or voice.\n Um we have not",
      "offset": 4582.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "released any new VAT models. Um we",
      "offset": 4585.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "primarily released like a new base model",
      "offset": 4588,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "for or like a new model for the um GPT4",
      "offset": 4590.64,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "realtime model that is just better at",
      "offset": 4595.199,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "function calling and has been overall",
      "offset": 4597.92,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "well received from our alpha testers.",
      "offset": 4601.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Yes. Can you inject uh can you inject",
      "offset": 4604.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "audio as as a background audio like",
      "offset": 4607.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "ambience audio like piping audio and so",
      "offset": 4611.04,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "on?\n Um you like",
      "offset": 4614.08,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "basically\n like you're in a real office,",
      "offset": 4618.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "right? Yeah. Yeah. Um you can just",
      "offset": 4620.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "intercept the audio that is coming back",
      "offset": 4622.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "from the model and then like overlay",
      "offset": 4624.4,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "your own audio.",
      "offset": 4626.239,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Oh yeah. Hi. Um, I was wondering what",
      "offset": 4630.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "your support is for like multiple",
      "offset": 4633.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "speakers. If there's more than one",
      "offset": 4634.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "person in a conversation, can it detect",
      "offset": 4636.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "who's talking and and do pauses that",
      "offset": 4638.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "way?\n There's no no current like speaker",
      "offset": 4640.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "detection in the model.\n Um, so it might",
      "offset": 4642.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "struggle with that.",
      "offset": 4645.12,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 4647.199,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Um, I just wanted to ask um about like",
      "offset": 4652.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "custom voices. Is it limited to the",
      "offset": 4655.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "preset voices you have or can I upload",
      "offset": 4657.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "my voice as a sample for instance?\n It's",
      "offset": 4659.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "currently limited to the voices that we",
      "offset": 4662.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "have. We keep adding new voices though.",
      "offset": 4664.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Is there going to be support to add",
      "offset": 4667.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "custom voices anytime in the future?\n Um",
      "offset": 4668.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "at any time in the future?\n Not not like",
      "offset": 4671.6,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "not like years.\n Uh what I mean what I",
      "offset": 4674.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "can say is like we're trying to make",
      "offset": 4678.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "sure we're finding like the safest",
      "offset": 4680.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "approach. Like we have an article on",
      "offset": 4682.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "online that talks about sort of like the",
      "offset": 4683.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "responsible take we're trying to take on",
      "offset": 4686.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this on making sure that like the vo the",
      "offset": 4688.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "voice like if we're providing custom",
      "offset": 4691.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "voices that it comes with the right",
      "offset": 4693.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "guard rails in place and stuff to avoid",
      "offset": 4695.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "abuse.\n All right. Thank you. You're",
      "offset": 4697.76,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "welcome.\n Yes.",
      "offset": 4699.679,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "I don't think that has changed. Um to my",
      "offset": 4709.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "knowledge,",
      "offset": 4712.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "my personal recommendation would be that",
      "offset": 4714.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like one of the things that you can do",
      "offset": 4716.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and this goes back to like for example",
      "offset": 4718.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "in the demo that I was showing um if",
      "offset": 4721.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "you're keeping track of the transcript",
      "offset": 4725.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and stuff you can re like when you're",
      "offset": 4727.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "starting a new session you can populate",
      "offset": 4731.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that context by creating new items using",
      "offset": 4733.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the API. So like one of the things that",
      "offset": 4736.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you could do is starting a new session",
      "offset": 4739.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "if you know what the previous context",
      "offset": 4741.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "was because you kept track of it you can",
      "offset": 4742.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "then um basically inject that as",
      "offset": 4744.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "additional context.",
      "offset": 4746.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Um what type of event are you looking",
      "offset": 4751.84,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "for?\n Oh if there's a timeout event um I",
      "offset": 4753.679,
      "duration": 8.401
    },
    {
      "lang": "en",
      "text": "do not know right now but all of our",
      "offset": 4758.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "events are documented in the API",
      "offset": 4762.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "reference.",
      "offset": 4764,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "Yeah. So when you see",
      "offset": 4766.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "file,",
      "offset": 4774.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "can you sorry can you repeat that",
      "offset": 4778.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "question once more?\n Oh, I was wondering",
      "offset": 4780.56,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "API function",
      "offset": 4783.04,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "functions such as system file.",
      "offset": 4786.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "Oh uh whether the function calls can do",
      "offset": 4789.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "things like system file reading and",
      "offset": 4791.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "stuff. I would say it depends on where",
      "offset": 4793.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you're running that a where you're",
      "offset": 4795.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "running that um real-time session. So if",
      "offset": 4797.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you're running it on the server, you can",
      "offset": 4800.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "do anything you can do on the server. If",
      "offset": 4801.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it's if it's running in the browser,",
      "offset": 4803.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "then you're limited to whatever things",
      "offset": 4805.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "are available in the browser.",
      "offset": 4807.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Um, no, you should be able to like you",
      "offset": 4813.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "could create a like web socket based",
      "offset": 4816.159,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "like um voice agent that runs on on your",
      "offset": 4818.96,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "device. I mean like it's going to use",
      "offset": 4824.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the real-time API for the model, but",
      "offset": 4825.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "then like because the actual tool calls",
      "offset": 4827.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "will get executed on your system, you",
      "offset": 4829.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "should have access to whatever aspect of",
      "offset": 4832.08,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "your system your program has access to.",
      "offset": 4834.32,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "Cool.\n Yes.\n So even before we get to",
      "offset": 4839.199,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "voice agents, we all need bigger,",
      "offset": 4842.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "better, and more diverse evaluation",
      "offset": 4845.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "sets, especially for anything around",
      "offset": 4846.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "function calling and parameterization.",
      "offset": 4848.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Uh do you have any best practices or",
      "offset": 4851.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "suggestions for how we now take",
      "offset": 4852.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "evaluation into the voice world? Should",
      "offset": 4855.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we keep things in text and then turn",
      "offset": 4856.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "them, you know, use text to speech to",
      "offset": 4859.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have voice versions of it? just if you",
      "offset": 4861.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "have any suggestions for how we evaluate",
      "offset": 4863.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the the full range of inputs that we",
      "offset": 4865.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "would expect users to bring to this.\n I",
      "offset": 4867.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "mean one of my suggestions would be if",
      "offset": 4869.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you can go to the leadership track go to",
      "offset": 4870.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Anoop's talk um which is tomorrow I",
      "offset": 4872.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "think. Yeah. Um he's going to talk a lot",
      "offset": 4875.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "more about like additional best",
      "offset": 4877.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "practices of what we've learned um in uh",
      "offset": 4879.199,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "building voice agents. I would say like",
      "offset": 4882.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "if you can hold on to the audio like it",
      "offset": 4885.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "it's helpful. Um, obviously",
      "offset": 4887.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "transcriptions definitely, but like it",
      "offset": 4891.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "it's sort of like the audio is still the",
      "offset": 4893.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "thing that is like the most powerful",
      "offset": 4895.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thing, especially for speechto speech",
      "offset": 4898,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "models where you have the model act on",
      "offset": 4899.6,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "the speech, not on the text, right? And",
      "offset": 4902.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like this is one of the one of the few",
      "offset": 4906.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "things where like the the chained",
      "offset": 4907.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "approach obviously makes some of this",
      "offset": 4910.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "much more approachable because if you",
      "offset": 4912.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "have if if you're if your agent is",
      "offset": 4914.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "running on text anyways and you can just",
      "offset": 4917.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "store the text and rerun it that makes",
      "offset": 4919.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that part of eval.",
      "offset": 4922.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "That makes sense. And then also for",
      "offset": 4924.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "those of us who might be thinking about",
      "offset": 4926.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "launching a new voice agent, how would",
      "offset": 4928.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you suggest evaluating it before we get",
      "offset": 4930.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to that stage that we'd have uh customer",
      "offset": 4932.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "interactions to work with?\n Um I would",
      "offset": 4934.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "start with like having like I think it",
      "offset": 4938,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "goes back to like um",
      "offset": 4940.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "like some of this is like",
      "offset": 4945.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "uh where is it?",
      "offset": 4947.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "there. Um like human review is an",
      "offset": 4950.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "excellent solution for this, right? So",
      "offset": 4953.28,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "like have it like have a system like",
      "offset": 4955.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like one of the big things with like",
      "offset": 4959.84,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "things like lemonade and stuff is like",
      "offset": 4961.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "they're able to go through all of these",
      "offset": 4962.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "calls and like get an idea, but they",
      "offset": 4963.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "also have their own like predetermined",
      "offset": 4965.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "set of examples that they might want to",
      "offset": 4968.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "test as they're developing the agent. So",
      "offset": 4971.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like that's a great first way. I would",
      "offset": 4973.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "scope clearly the problem you're trying",
      "offset": 4975.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to solve as well. Like if you're if",
      "offset": 4977.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you're trying to sort of boil the ocean,",
      "offset": 4979.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it makes a lot of this significantly",
      "offset": 4981.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "harder as opposed to like well scoping",
      "offset": 4983.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "what the agent should be able to do and",
      "offset": 4986.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what it shouldn't be able to do.\n Makes",
      "offset": 4987.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sense. Thank you.\n You're welcome.\n Get",
      "offset": 4990.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "getting on the same path of our friend",
      "offset": 4992.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "over there. Uh actually when I'm testing",
      "offset": 4994.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "my agents, my text agents",
      "offset": 4997.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "conversational, uh I use promptful.\n It's",
      "offset": 4999.6,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "a platform for do all the proper",
      "offset": 5002.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "testing. Could I put another agent to",
      "offset": 5005.199,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "talk with this this agent to do all the",
      "offset": 5009.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "evaluation?",
      "offset": 5011.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "I think you can try it. Um like I don't",
      "offset": 5013.92,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "think it shouldn't work. Um",
      "offset": 5017.92,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "um like\n so I put another voice agent",
      "offset": 5021.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "talking to that agent to try to execute",
      "offset": 5024.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "all the prompets and then I could get",
      "offset": 5027.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "like the transcription",
      "offset": 5029.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "community. I mean I know we have we have",
      "offset": 5031.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "use cases where customers also use our",
      "offset": 5034.08,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "models to like prompt humans right so",
      "offset": 5036.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "like um where it's like for for like",
      "offset": 5040.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "training use cases or other things for",
      "offset": 5042.639,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "example\n um so she'll work out but I",
      "offset": 5044.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "don't know if anyone uses that kind of",
      "offset": 5048.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "approach and lemonade does",
      "offset": 5050.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "oh cool so the second the second picture",
      "offset": 5053.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "is is exactly that\n awesome thank you",
      "offset": 5056,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "welcome Any",
      "offset": 5060.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "any other questions?",
      "offset": 5063.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Yeah, go ahead.\n Slightly related uh do",
      "offset": 5065.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you have something around wake word",
      "offset": 5068.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "detection on the real time API road map",
      "offset": 5069.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "or patterns for wake words?\n Oh reports.",
      "offset": 5072.239,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "Um\n no wake words. So that's activating",
      "offset": 5075.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the case.\n Um no we don't have any wake",
      "offset": 5078.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "words built in or anything.",
      "offset": 5081.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "No patterns either\n like any patterns to",
      "offset": 5084,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "avoid costs. Um, no.\n On device, I guess.",
      "offset": 5086.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "No. Okay.\n You could basically like what",
      "offset": 5090.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you can do is you can build your like",
      "offset": 5093.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you can turn off our uh voice activity",
      "offset": 5095.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "detection and then build your own um and",
      "offset": 5097.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "then basically use that. So like you",
      "offset": 5100.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "could use a model that has like a like a",
      "offset": 5102.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "VAD voice activity detection model that",
      "offset": 5105.679,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "has wake words in it and then like do it",
      "offset": 5108.32,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "that way and basically commit all of",
      "offset": 5111.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that audio to our API and then send like",
      "offset": 5114.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "a commit like a commit event.",
      "offset": 5116.8,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "Cool.\n Thanks.",
      "offset": 5119.76,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "Well, awesome. Thank you so much for",
      "offset": 5123.84,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "taking the time and spending the",
      "offset": 5125.52,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "afternoon with me.",
      "offset": 5126.56,
      "duration": 3.48
    }
  ],
  "cleanText": "Awesome.\n\nWell, hi everyone. My name is Dominik. I work on developer experience at OpenAI, and I'm excited to spend the next two hours to talk to you all about voice agents. Um, the QR code was already on the slides. If you just entered the room, you want to try to download the dependencies as soon as possible. So, head over to that QR code or to that starter repository and follow the instructions to install. That might take a while with the internet right now. So, please do that as soon as possible. You have like 15 minutes of me rambling about stuff before we get started with uh with actually coding.\n\nUm, so I said we're going to talk about voice agents. I want to first put everyone on the same page because I know we all have different uh definitions of agents, and there's going to be a lot of definitions flying around at this conference naturally. Uh so when we're going to talk about agents, we're talking about systems that are going to accomplish tasks independently on behalf of users and most importantly um they're going to be essentially a combination of a model that is equipped with some set of instructions that then has access to tools that can both be used to work on that goal. And then all of that is encapsulated into a runtime to manage that life cycle. And that's an important definition because uh today we launched the OpenAI Agents SDK for TypeScript. If you've heard of the one for Python, today we basically released the TypeScript equivalent. Um and so we're going to use that and it maps those exact patterns. So, if you're unfamiliar with the agents SDK, it's basically a SDK that provides you with an abstraction based on the best press practices that we learned at OpenAI to build agents. Um, and they it comes with a couple of different base foundational features including things like handoffs, guard rails, streaming input and output tools, MCP support, built-in tracing, so you can see actually what your agents did and how they interacted with each other. And then additionally to those features that are coming from the Python SDK, the SDK we launched today in Typescript also includes human in the loop support with resumability so that if you need to wait for human approval for a while, you can deal with that. And most importantly, native voice agent support. What that means in practice is you can use that same those same primitives that we already have in the agents SDK and you can build voice agents with that that handle handoffs, have output guard rails to make sure that the agent is not saying things it's not supposed to. uh tool calling, context management, meaning keeping track of the conversation history so you can use it in other applications and built-in tracer support um so that you can actually replay conversations, listen to the audio of the user and properly debug what happened. Plus native interruption support. If you've tried to build interruptions, you might know how hard this is. If you haven't, be glad you don't have to. um both WebRTC and websocket support meaning it's actually can communicate both on the uh on the server for things like Twilio, communicate like phone call voice agents or directly in the client in the browser, that's what we're going to use today using WebRTC.\n\nBut first, why would we be interested in voice agents in the first place? Um, one of the things that I'm most excited about is it makes technology much more accessible to people. There's something magical about being able to talk to a voice uh to like a voice agent and just have it like see it do things. It's also um much more information dense. I can convey information much faster, but also it can contain a lot of information through the type of tone and voice that I'm using, the emotions. So, it's much more information dense than sort of just based basic text is. One of the cool things is also it can act as like an API to the real world. You can have a voice agent go and like call a business for you and like have a conversation with them where maybe there isn't an API for that business.\n\nAnd so when we talk about building voice agents, there's essentially two types of architectures that have emerged on when building these. The first one is based on your traditional text-based agent and just sort of wrapping it into a chained approach where we have a speech to text model that is taking the audio and then turning it into text so that we can run our basic text-based agent on it and then we take that and we uh run that agent, take the text output and run it through a text-to-speech model to generate audio that we can play. Again, this has a lot of strengths. One of the most common reasons why we raise this is it's much easier to get started with if you already have a text-based agent. You can take that, wrap some audio around it, and you have something that you can you can interact with. But uh the other aspect is that you have full access to any model. Text is the main modality that any LLM has and so you can use really any of the cutting edge models. also gives you much more control and visibility of what the model did by being able to actually look into the exact text that went in and out of the model. But it also comes with some challenges. Turn detection is one of the one of the big ones where you need to now take into consideration what did the user hear by the time that they interrupted some uh interrupted the voice agent. Then translate that part back into text. make sure that like your transcript is appropriately adapted so that the model doesn't think it told the user something that it didn't. Um, chaining all of these models together adds latency on every possible level. And so that's another big challenge. And then you're losing some of that audio context, right? You're transcribing the audio. And if you've ever tried to convey a complicated topic over a text, you know it's a bit harder than dealing the deal dealing with the same thing using using your own voice.\n\nAnd so an alternative to that chained approach is a Speech-to-Speech Approach where we have a model that has been trained on audio and then takes that audio uh to directly interact on the conversation and make tool calls. Meaning there's no transcribing in the process. The model can just natively deal with that audio and that translates into much lower latency because we're now skipping the speech to text text to speech processes. We can also now have much more contextual understanding of the audio um including things like tone and voice and all of that leads to a much more natural fluid uh level of conversation.\n\nBut there are some challenges with this. One of the most common ones is reusing your existing capabilities. Everything is built around text. So if you already have some of those existing capabilities or a very specialized agent for a certain task, uh it's harder to reuse those. Also, dealing with like complex states and complex decision making. It's a bit harder with these models since they've been really focused on improving on the audio conversational tone, less so on being very complic complex decision makers.\n\nBut there is a solution that we can get around with this. Um again taking inspiration from what we do with text-based agents, we can actually create a um delegation approach using tools where we have a frontline agent that is talking continuously to the user and then that one uses tool calls to interact with much smarter reasoning models like GPT-4 mini or GPT-3.\n\nUh actually let me at this point give you a quick demo and see how the internet goes here. Um, so I have a real-time agent here that we build with the agent SDK. It's going to be very similar to what you're going to build later on. Um, but when I start talking to this,\n\nHello there.\n\nHello there. How is the engineer going?\n\nSo we can now give it t like give a task to like call tools that I gave it like hey what's the weather today?\n\nLet me check the weather for you. One moment, please. Transferring you to the weather expert now. Actually, I can directly help you with the weather information. Could you please specify the location you're interested in?\n\n Oh, yeah. What's the weather in San Francisco?\n\nSo, you can see here it's actually dealing with the interruption.\n\nEnjoy the bright and pleasant day. Is there anything else I can?\n\n No, that's it. Thanks.\n\nYou're welcome. If you need anything else, feel free to ask. Have a great day.\n\n And so in a similar way, we can actually trigger the more complicated backend agents as well. So I have a tool for this to handle refunds that will call out to GPT-4 mini and evaluate the refund. So hey there. Um I have one more thing. So I recently ordered this skateboard that I tried to use and it seems like I'm really bad at skateboarding. So I want to return it. Um, it is slightly a scratch though.\n\nI'm here to assist, but it sounds like you need customer service for that. I recommend contacting the company where you bought the skateboard. They can provide you with the\n\n Looks like I didn't add the tool. Maybe I did.\n\nOh, I asked for I didn't ask for a refund. Um, let's try this once more.\n\nHey there. Um, I bought a skateboard recently that I tried and apparently I'm really bad at using it, so I wanted to return it. It is slightly scratched, so can you give me a refund?\n\nHello there. How is the\n\nHello there? How is the AI engine?\n\n The joys of internet. Um,\n\nhey. Um, I recently ordered a skateboard from you and it uh failed. Like I I can't use it. I'm struggling to use it. It's slightly scratched. Can you give me a refund, please?\n\nHello there. How is the\n\nI'm going to assess your request for a refund.\n\n There we go.\n\n It is slightly struggling with this like weird echo that we're having here. Your skateboard arrived damaged and you're eligible for a full refund, we'll process that for you.\n\n All right. Um, but you can see here that uh it was able to call that more advanced tool and um actually process that request. And one of the nice things is that like while time to first token is often a really important thing, the longer a conversation goes, like your model is always going to be faster than the audio that has to be read out. And so this this is like a really helpful thing where by the time that the model was able to say like, hey, I'm going to check on this for you, it already had completed that LLM call to the GPT-4 mini model to get the response there.\n\nAll right. Um, let me Oh, one more thing. Since we talked about traces, one of the nice things now is we can actually go back here into our traces UI. And this launch today, um, you'll be able to actually look for any of your real-time API cases, look at all the audio that it dealt with, um, and all the tool calls. So, we can actually see here that the tool call was triggered, what the input was, the output. We can listen to some of the audio again um, to understand what happened. And then because both this and the backend agent used the agents SDK, we can go into the other agent as well, which was the GPT-4 mini one, which we can see here. And we can see that it received the context of the full past conversation, the full transcript, um, as well as additional information about the request and then generated the response here. So this allows us to then get a full complete picture of like what happened both on the front end and the back end.\n\nLet's jump back into the slides and cover a couple of more things before we get coding. Um, and that's about best practices. So, I would group when you're group the best practices of like building a voice agent into three main things to keep in mind. Uh, the first one is to start with a small and clear goal. This is super important because measuring the performance of a text-based agent, you will hear a lot about evalu. But with voice voice agents, it's going to be even harder. So you want to make sure that you're very focused on like what is the first problem you want to solve and keep it focused on that and give it a like limited number of tools so that you're fully centered on this. The agents SDK makes this really easy because you can then later on add additional tools to additional agents and deal with like handoffs between them. Uh but this way you can kind of really stay focused and make sure that one of your use cases is great and then hand off other ones to human agents for example.\n\nThe second one is what I elaborated on which is building evals and guard rails very early on. Uh so that you can feel both confident in what you're building but also confident in um that is actually working so that you can then continue to iterate on it and know when it's time for you to like grow the complexity of your voice agent. Uh as of today you can use the traces API the trace dashboard for that. Uh but alternatively, some of our customers have even built their own dashboards like Lemonade to really get an end-to-end idea of the customer experience and then even replay some of these conversations with their agent as they're iterating on it.\n\nThe other thing that I'm personally super excited about with these models is uh both our Speech-to-Speech model and our text-to-speech model are generative models meaning you can prompt them the same way that you can prompt a LLM around tone and voice and you can give it emotions roles personality. Uh we built this little micro site called openai.fm. It's a really fun website to play around with where we have a lot of examples of different personalities and how that uh that that style of prompt can then change what is being read out by our text to speech model. And so that's a great way for you to not just limit one second uh limit the experience of your model um or like the personality of your model by the voice that you picked but also by the prompt and instructions that you're giving it. There was a question there.\n\nWould you mind using the mic that is right behind you just so that it's on the recording?\n\nHello sir. So my question regards to the previous slides on Lemonade. So you're displaying how they have this dashboard where they can show all of this. Is this a dashboard that OpenAI provides and Lemonade just integrates as like an iframe or something?\n\n No. So in this case they built their own um solution for it.\n\nOkay. And does OpenAI then provides all the JSON or the data structure that we can just plug into the\n\n so the the way the real time API under the hood works is that you get all the audio data and you can do whatever you want with that. Basically, you're getting all the necessary audio events. So you can use those data structures. So we're not storing them by default. You can use the traces AP uh the traces dashboard. We don't have an API for it yet, but you can use the trace traces dashboard to um get a a basic look of of that, but it's not ifraable.\n\n But you mentioned it's only audio data. This shows not just audio, but also transcription and all of that as\n\n\nWell. Right. So, the traces dashboard, if we go back to it, um does show um all of the transcripts and stuff as well, as long as you have um transcription turned on, which I don't seem to have turned on for this particular one. Um but it should like you can turn on transcription and you should be able to see the transcripts as well.\nOkay. Thank you.\nYou're welcome.\nAll right. Um let's go back to this. Um the other part with it is as I said you can prompt both the personality, you can also be very descriptive with the conversation flows. One of our colleagues um found that giving it conversation states and so this JSON structure is a great way to help the model think through sort of what processes and what steps it should go through the same way that you would give a human agent a script to operate on.\nIf you're struggling to write those scripts though, uh we also have a custom GPT that you can use to access that. And I'll share all of those links and a copy of the slide deck later on in the Slack channel. So if you're in that, uh you should be able to access those.\nBut with that, um that's primarily what I wanted to talk through from a slides perspective. So from here on, what I want to do is um build with you a voice agent. We'll see how that goes with the internet. Um, also, if you have headphones, now is a great time to bring them out. It's going to be really weird when we're all gonna talk to our own agent. Um, but we're going to try this and see how that goes. Um, so if you came in later, please scan the QR code, go to that GitHub repository, and set that up. Install the instructions. There's no code in it yet other than like a boilerplate Next.js app and a um empty like package JSON that install just like the dependencies that we needed so that we are not all trying to run npm install at the same time.\nBut what I want to do is build a first agent. Um, so if you want, you can just straight up copy the code that is on here, but I'm gonna um actually go and type it along with you all so that you get a feeling for what's happening and we have a good idea of like timing. So if you want to take a picture now and just code ahead, do that. And otherwise, I'm going to switch over to my code editor and we're going to do this together.\nOh, and if you're running into trouble, the Slack is a great way to post questions that are technical questions and a noop who's over there um is going to try to help you. Um, alternatively, raise your hand, but it's a bit easier if you're just slacking the messages there and we can kind of multi-thread the problem. Um, all right, let's go and build an agent. So if you clone the project, you should see an index.ts file. Go and open that and you should be able to import the agent class from the OpenAI Agents SDK package. That's what we're going to use to create the first agent. Yeah.\nOh yeah, good call. Um\nIs that better?\nCool.\nThat seems a bit seems worse on my side than yours, but I think as long as you all can read that, I'll be fine. Um, all right. So, what you what I want you to do is go and import an agent. And we're going to define our first agent. And as I mentioned, um, primarily the an agent has a like a few centerpieces. The first one being a set of instructions on what to do. So we can give it instructions. I'm going to say you're a helpful assistant. That's sort of the most boilerplate thing you can do. We do need to also give it a name and that's uh so that we can actually keep track of them in our traces dashboard.\nI'm going to say my agent here. This can be anything that helps you identify it. And then we need to actually execute this agent. So we can import a run function here.\nAnd then we can await the run here. I'm going to run this agent with just hello, how are you? And then log out the results. And with the results we get a lot of different information because uh essentially when we run an agent it's going to do a lot of different tasks from executing all necessary tool calls if there are any to validating output guard rails etc. But one of the most common things that you just want to log out is the final output. That's whatever the last agent in an execution said. Um, so in this case it's going to be uh a set of text and then you should be able to run npm start\num oh npm run start\n01\nand that should execute it and then you should see something like this um depending on what your model decides to generate and by default this is going to run GPT-4.1\num as the model but if you want experiment with this. You can set the model property here and we can set it to for mini for example and then rerun the same thing. So this is the most basic agent that you can build. But one of the things that really makes something an agent is if it can execute tools. So we can import a tool here.\nWe go and we can define a get weather tool. Um,\none of the things here is you have to specify what arguments the model is going to receive. And one of the ways that you can do this is through a tool called ZOD. If you've never heard of it, it's essentially a way to define schemas. Um, and what we'll do is we'll both use that ZOD schema to inform the model on what the parameters for this function call are, but we're also going to use it to validate then what are the actual arguments that the model tried to pass in and do they fit to that schema. So we get full type safety here if you're a TypeScript developer and you care about that. So in this case we have um a get weather tool and then we can give that tool to the agent and we can change this to what is the weather in Tokyo uh is what cursor wants to check.\nSo if I run this again,\nlet me move this slightly. We can see it's going to take a bit longer now and that's because it ran some tools. Um and now it's telling me the weather in Tokyo is sunny. And if you're wondering, well, did it actually run a tool? We can go into\nour traces dashboard here\nand look at the trace. We have a my agent here.\nAnd in there we can see it ran tried to call the tool, executed the tool and got the weather in Tokyo sunny back and then took the response to generate the final response. So the traces dashboard is a great way for you to see what actually happened behind the scenes.\nHow we feeling? Can I get a quick temperature check? Are able are people able to follow along? See Zeke is giving a thumbs up there. So um all right so this is a text-based agent. I wanted to show you this just to get a bit familiar with the um overall agents SDK so that we can jump into building voice agents.\nUh the first one we're going to build or like the first thing we need to understand about a voice agent is the slight differences between a voice agent and a and a what we call a real-time agent. Essentially, a real-time agent is just a specialized version of an agent configuration. There's just a few fields you can't pass in, but they can be used in what is a what's called a real-time session because with voice agents, there's a lot more things to deal with than just executing tools in a loop. Um, one of the most important things is you need to deal with both the audio that's coming in, uh, process that and then like run the run the model with that and then gener deal with the audio that's coming out. But you also need to think about things like guard rails, handoffs, other life cycle things. And so the real-time sessions are really dealing with all of that. So let me show you how that works. Um for this what we're going to do is we're going to go in the same project. There's a 02\nand it has a page.tsx in there. Um, this is a Next.js app that really I just gutted to have like the bare minimum in there, but this is a great way for us to just uh build both the front end and the back end part of the um voice experience because this voice agent that we're going to build is going to run in the browser. In order to make sure we're not leaking your API credentials, one of the important things is you need to use an ephemeral key. That is a key that is short-lived and is going to be generated by your server and handed off to your client so that they can use that to interact with the real-time API over a protocol called WebRTC.\nUm, for that you should see a token.ts file in your repository that just calls out to the real-time API to generate a session and then return a client secret which is that ephemeral key that we can then use to authenticate with the SDK. You do not have to do this if you're building a real-time agent that is running on your server. For example, in the case of a Twilio app or something else where you can just directly interact with the\nuh OpenAI API key. But if you're running anything in the browser, then you actually need to generate this client key just so that you're not, you know, giving your API key to the world.\nSo with that in here, we can actually go and build our first real-time agent. So similar to previously, we're going to import a an agent class here, but in this case, it's going to be a real-time agent.\nAnd we're going to import it from the real time package\nuh which is just a subpath in the same package. So you don't need to install a different package here. But now we can define a real-time agent that works the same way. We have a name, we give it instructions.\nUm, just sort of going with a default suggestion here. And now we actually need to connect that agent to a real-time session. So I have a connect button here for running this example.\nLet me start it up here with npm run start02. That command should be in your readme as well.\nUh, it's going to start a development server. And we can go over here, reload this, and you can actually see\nit just has like a little connect button that right now doesn't do anything.\nSo, let's connect that up. I don't need this anymore. Um, so let me just move this to the side.\nUm, so in this on connect function that gets triggered whenever we press the button, we want to deal with that connected state. So\nwe're what we're going to do here is we're first going to fetch that token.\nAnd this code what this basically does is it's going to import that server action which is an next.js concept that just makes sure that like this code is going to run on your back end. If you're using a different framework, you you should be able to just go and fetch this key from your backend server. Um, and then once we have that token, we can go and create a new real time session. So, what we're doing here is we're going to give it the first agent that should start the conversation up. Uh, I'm going to specify the latest model that we released today along with the agents SDK. This model is a if you've used the real-time API before, it's an improvement especially around tool calling. It's much better on that front.\nUm, we have a couple of different customer stories on our Twitter if you want to check that out. Um, and then I'm going to give it not there. I don't know why cursor insists on that. Um, the last step that we need to do is we need to connect to that session.\nSo, this is where we're going to give it that API key so that the mo that we can connect to the real-time session under the hood\njust so that it's easier for us to deal with all of this. I'm also going to close the session. But I've got one thing here that is an oddity of\nuh React. We do not want to generate that session every time. So, I'm going to on every rerender. So, I'm going to create a what is called a ref here.\nAgain, if you're new to React. Um, this basically is just a variable that's going to persist through rerenders. So, we need to slightly change this here where we're going to assign that to session.curren\nso we can maintain that. And then that also allows us to say if there is a session.curren set, uh, we want to actually close that connection when we press the disconnect button. That just makes sure that we're disconnected from the audio again.\nSo, I'm going to leave that on the screen for a second and then we can test this out. But if you already typed this, go into your browser, refresh, press connect, and you should be able to talk to your agent.\nAll right, let's try mine. Let me move this to the other side so it's not blocking your cut.\nHello.\nHi there. How can I assist you today?\nAll right. So, you can see um it's just a few lines of code. We didn't have to deal with things like figuring out how to set up the microphone, how to set up the speakers. By default, if it's running in the browser, it will do deal with all of that automatically. If you do want to pass in your own microphone source or other things like that, you can do that as well. Um, if this is running on a server, you have a um both a\nsend audio function that allows you to send an audio buffer in or you can listen to the audio event which is going to emit all of the audio buffers that are coming back from the model so that you can pass it to whatever your source is.\nUm,\nso that's our first basic agent. Any any questions so far?\nH,\ncan you send that code to the red?\nSorry,\nwe want you to send the code.\nCan\nyou push it? Can you push it up?\nI can\npush it. Yeah.\nGood call. Thank you. Um,\nall right. So, now that we have that, let's go and actually give it a tool.\nSo, this is really where the benefit of the um agents SDK comes in. We can actually use that same tool definition that we did earlier. Um, so I'm just going to follow the autocomplete here.\nWe should be able to just give that tool now to our agent\nand save.\nI need to import zod\nagain to do that schema validation. This is especially important on the real time side because the real-time model currently do not does not support strict mode. So the JSON might not fully comply with your um\num schema unless you're running you're giving us a zod schema and we'll go and validate that this actually fits that schema. So that makes your code a bit easier. So with that we\n\n\nCan go back.\nHey, what's the weather in San Francisco?\nThe weather in San Francisco.\nWe can disconnect it here.\nUm, also this does now deal with interruptions.\nSo, hey, can you slowly count to like 20?\nSure, I'll count to 20 for you.\n1 2 3 4 5.\nOkay, stop.\nSure, I'll stop counting.\nHow far did you count?\nI counted up to six.\nIt's pretty, pretty close.\nIt's always hard with the timing to like perfectly get it, but uh normally that's enough to um deal with the context, but it is super crucial to have that interruption timing so that like your model doesn't think it read out like the full customer policy um but the customer interrupted it halfway through, for example.\nUm, all right, question.\nYou don't have to um manage all the events to actually do that anymore?\nNo.\nUm, so the the real-time session will handle all of those events.\nThat is a great proxy to if you are curious about these events um that you're alluding to.\nWhat we can do is listen to the transport event and let's do this later.\nUh, this will log out all of the events that are happening under the hood.\nSo, if we open the dev tools here and rerun this.\nHey, hey there.\nHow can I help you today?\nSo, you can see all of the events that normally you would have to deal with are being dealt with, you still have full access to them.\nSo, you can both read them, but you can also send events yourself.\nSo, it's going to handle all of the things but continue to pass them on to you if you want to do your own logic.\nOn top of that, I'm going to push that code for you so you can pull it.\nCool.\nUm, all right.\nSince we already have this commented out code, the other part of this that typically is a request that you want to deal with is I want to show like the transcript.\nI want to see what sort of is being transcribed.\nAnd the important thing here is I'm using the word transcribe because even though the Speech-to-Speech model is dealing with the audio directly, um, and there is no transcription step in between, by default, we're going to transcribe all of the conversation at the same time.\nUm, you can turn that off if you want to.\nUm, if you're using the API directly, you have to actually turn it on.\nUm, in the agents SDK, it's turned on by default because it's such a common request and it enables us to do a couple of additional features that we'll cover later on.\nBut this is going to give us that whole history every time.\nSo, I'm just going to um log that history here.\nOr rather, I'm gonna There we go.\nImport that.\nI'm going to set that as a variable.\nAnd then because it's React, we can let's create a list here.\nWe're going to go over all of this.\nI need to filter because it has both tool calls and messages.\nAnd I only want to show the messages for this.\nUm, so should be able to does why I want that.\nUm, let's see.\nClose this.\nRefresh.\nHey.\nHello.\nHow can I assist you today?\nHow's the weather today in San Francisco?\nThe weather in San Francisco today is sunny.\nAnything else you'd like to know?\nSo, you're automatically getting that conversation.\nIf you are interrupting the model, one of the things that happens is the transcript is going to disappear.\nAnd that's because uh the the model currently does not adjust that transcript and instead it's going to be removed and we're going to remove it from that object as well just so that you get the most accurate representation and you're not thinking that like the model read out a certain piece of text.\nAnd again with everything that we're doing here can actually go back into traces and we can see that same representation here with the weather call and everything.\nSo again helps with the debugging.\nGo briefly back to the slides.\nSo we covered we set up our first agent.\nYeah.\nThe question was how do you store the conversation history?\nUm, so it's currently fully stored in memory.\nUm, so basically there's going to be a bunch of events that that I logged out that are being emitted by the real-time API.\nAll of those are going to be sent over to the client and then store it in memory in a conversation like in just like an array essentially.\nSo you can do whatever you want with that by listening to that history updated event.\nSo if you do want to store it somewhere, you can store it.\nUm, the other part is the traces part is automatically going to be stored on the OpenAI platform as long as you both enable that tracing you can disable it um by default in the agent SDK is enabled and then um the other aspect of that is if you are a ZDR customer so a zero data retention customer of OpenAI you don't have access to that traces feature.\nUm, about the conversation.\nUm, so the question was how much of the like voice context, how much of the previous conversation is being used.\nUm, that's going to depend and sort of like dealt with directly by the real-time API.\nSo like the real-time API when you do start that session um that holds the source of truth for that whole conversation session.\nSo what you're receiving on the on the client side is just a copy of whatever is happening at that point.\nIt's not the source of truth of what we're going to adapt to pass into the model.\nUm, the question is how does it work with like the inference cost and like whether you're passing like passing in that whole conversation.\nUm, and Noob is nodding.\nUm, he is the bigger expert there.\nBut yes, um, we're actually like you can log the the like we're keeping track of the usage.\nThere's an event that you can like log out to see your token cost.\nUm, so you have an idea of like what is being actually passed in.\nSo like with every if we're going back here to this example, you can see these response done events.\nUm, I don't know where is the um shouldn't it be on the response done?\nUh, it is being sent over.\nI just do not know right now why it's not showing.\nOh there.\nSo you can see here um it outputs the detailed information of your tok to token usage at any point in time.\nSo while you don't have like access to like what is exactly what was passed into the next response generation um you can keep track of the cost as it's happening.\nYou're welcome.\nUh, yes.\nDo you there's a microphone right over there that might be easier than you yelling across the stage to me.\nI I see that the format that you're using is PCM16.\nIs there a way in which we can modify the output formats of the audio files so we can save in memory?\nUm, yeah, there are different different audio modes that you can use um including like for example u law for that is like helpful for phone calls for example.\nSorry, another question on the usage.\nUm, does that like final assistant response roll up all the tokens from like all the intermittent tool calls as well?\nDoes that make sense?\nLike the agent needs to like kind of reason through and then format tool calls.\nSo I'm assuming it's not just the output tokens for only the assistant response, right?\nIt like every tool call is a response in general as well.\nSo like it it works the same way that like the responses API works for example.\nOkay.\nSo like each right because we're using this and we have like tool calls and tool call outputs, right?\nAnd I couldn't find the like usage attribute on the tool call output.\nIs it somewhere in those like raw events that are outputed?\nDo you do you know a new?\nOkay, no worries.\nI I know it's like kind of early on.\nWe can we can follow up behind.\nThank you.\nYou're welcome.\nUh, yeah.\nDo you want to head over to that microphone that is right behind you?\nIt just makes it a bit easier.\nUh yeah, there's in the meantime just a quick question.\nCan I go back to the slides explaining the different modes of the uh the audio agents like the text in text out that's the first one.\nOh yeah, text to speech that's the second one.\nUh, I didn't get the third one.\nAnd uh Oh, you mean this?\nYes.\nYes.\nIt's uh when you just showed us the GPT-4 real time that one.\nYeah.\nThat model is that uh this PPT this slide is about.\nYeah, exactly.\nSo like where it's like when we did the refund um it kind of followed this pattern where it performs a tool call like the like real time API agent can perform tool calls.\nIt performed a tool call to trigger a separate agent that was the refund agent that in my case used 04 mini to execute that task and then hand that back.\nOkay, got it.\nThanks.\nYou're welcome.\nYes.\nUm, I'm currently using like a regular uh OpenAI agents.\nSo what will be the challenge that I will face when I would when I want to change my regular regular agents to realtime agents?\nSo there's a couple of different challenges like one is like anything that you're doing around latency like like anything you're doing around um voice latency is always king.\nSo like you want to figure out what are the best ways to actually have a slide around this.\nUm like when you when it comes to things like tool calling, you want to find ways to do things like buying yourself some time.\nSo you will typically see some prompting around like announce what you're about to do next um before you're doing it.\nAnd that's to do that little trick around uh while the while the previous audio is still being read out, the agent can already perform the tool call and wait for the tool call to come back because similar to a textbased agent, the model can't do like can't receive additional data as like like do another thing.\nUm outside of like we can interrupt the response but it can't finish that response if that makes sense.\nAnd so you want to do these sort of like buying time.\nThe other thing is like if you're building a real-time agent, the longer your prompt gets, at one point it increases the likelihood that it gets confused.\nSo you want to make sure you're properly scoping those use cases and like through what what we call handoffs where you have different agents that are more spec scope to specific steps in your experience.\nThank you.\nYou're welcome.\nYes.\nYeah.\nSo you're uh Yeah.\nSo the question is about memory.\nUm basically right now the the the let me correct this.\nUm, when we go back to this demo, what you're seeing here is essentially just like a copy of the events that we're receiving back.\nSo, this is like helpful as a visualization of the history.\nUh, that being said, the actual like memory in the sense of like an LM agent memory is the session context that is happening on the real-time API side.\nThere are events that you can use to update that.\nUm, we actually have an update history event that you can pass in what you want the history to be.\nBut what that does is essentially like fire off events to the real-time API to say like delete this item from the history or add this new item.\nUm, and you can give it a previous item ID.\nSo like you can for example like slot messages into a specific spot if you wanted to.\nUm, does that make sense?\nBut there's like no no like advanced like long-term memory solution like like you were alluding to.\nCool.\nYes.\nHi.\nDo you have tips for handling input from low fluency users?\nLike say someone who's just learning a language and they have like multilingual input and maybe broken grammar and their pronunciation is not so good.\nUh, I don't think I have any like best practices right now that I could share.\nCan it handle it just off the shelf or?\nUm, it can handle like switching languages and and things like that.\nOkay.\nBut it might not be able to handle low fluency.\nUm, I don't know if we have any use cases.\nYeah, we have we have some customers that are like language learning companies.\nSo there there is some that are using it that way.\nBut I don't think I have any like best practices that I can share.\nOkay.\nThank you.\nYou're welcome.\nSorry.\nBack in the code, is there a call back for the interrupt and does it include the last transcription?\nUm, there is a call for the interrupt, but uh there is no um there's no actual like event that there's no parameter E that comes with it or anything like that.\nNo, there's currently no um transcript.\nSo what you can do is if you're getting this, you can call get history or something.\nUh there's just history.\nUm so like this always is up to date.\nUm so you can you have access in that moment.\nThe the thing that we do have is um for tool called specifically um you're getting some additional context and that context has a um history parameter that you can like push into.\nOkay.\nUm it's more documented in the in the documentation the API.\nOkay.\nGreat.\nThank you.\nYou're welcome.\nAwesome.\nUm, let's move a bit on and show a couple of other things.\nSo, we talked about tools.\nAs I said, like one of the benefits is you can reuse the same syntax that you're doing with textbased ones.\nUm, it's also a good way for you to then communicate with your backend systems using HTTP.\nUm follows sort of a um general practice around like keeping both the tool calls as low latency as possible.\nLike send out a tool like for example if you know a task is going to take longer, start the task, give it a task ID and have the agent have a tool to check on the status for example like that helps getting back to it because again while the tool call is going on the model is sort of stuck.\nSo you want to you want to make sure to like get get back to that as soon as possible.\nUm one of the other things that you can do is human appro uh uh human approval.\nI can show you that quickly.\nThere's essentially a use uh it needs approval that um you can either specify as a function that will be evaluated before the tool ever gets triggered.\nThis is a great way if you have like a more complex logic on I need approval for this.\nYou can also give it just straight up I always need approval at which point there is a another event here.\nUm tool approval requested and then that gets a um event here.\nSo we can do things like um good old prompt um and then we can go and approve that tool call again.\nI don't know why the autocomplete is not working.\nUm, proof.\nThere we go.\nAnd why is it Oh, this is wrong.\nThis is where I go into the docs because I do not remember why this is autocompleting the wrong way.\nBut everything I'm showing you is in the docs.\nUm, so we can just Oh, took the wrong thing.\nRight.\nThe first There we go.\nApproval request.\nAnd then\n\n\nWe can do approval request.\nThank you.\nIt's like the classic thing when you're on stage and you can't really...\nThere we go.\nSo, in this case, I'm just going to always approve.\nBut if we now go in, hey, um, can you tell me the weather in Seattle?\nSo we can in that case approve it.\nIt's always going to approve right now because I'm not actually checking the status.\nBut um that means you can build like a human in the loop approval experience.\nThis is really convenient especially if you're running it in the browser and you just want to have like a confirmation if like the tool is hallucinating things before the customer actually submits it.\nAnd does it do it directly?\nKind of actually say you okay if I do this or not to do that and then do voice extra voice.\nSo the the basically this is happening.\nSo the question is, does it automatically do this like the what we're doing?\nAnd the reason why this is separate is the model is asking for this tool to be executed, but we're intercepting this um basically before we're ever generating or executing the response.\nThis is intentional so that like you don't have to deal with like we want you to think through why should this tool need approval as opposed to doing that somewhere halfway through your tool execution and you have to like deal with the consequence of rolling back every decision that you've made, for example.\nAnd so by default if this is just needs true, it cannot get past that until the execution was approved, at which point it stores it in the context that is stored locally and then bypasses that security.\nSo this is not happening on the on the model level.\nUm, I'm going to remove that again.\nUh, so the other thing we talked about already, but I want to show it in practice is handoff.\nSo a handoff is essentially just a specialized tool uh call that resets the configuration of the agent in the session so that um we can update the system instructions, we can update the tools and make sure that we can nicely scope the task of what we're trying to um solve.\nSo what you cannot do, I know people probably gonna ask about this, is you can't change the voice of the agent mid session.\nYou could define different voices on different agents, but the moment that you're like the first agent that starts talking, that's the voice that we're going to stick with throughout the entire conversation.\nSo that's a caveat to just keep in mind.\nUm, but they're still very helpful to let's say have a weather agent here.\nSorry.\nUm, and what we're going to do in this one is called weather agent.\nUm, um, let's give...\nWe'll do this and then what we can do is we can actually give it a handoff description.\nSo if you don't want to have this in your system prompt, but you just want to help the model understand when to use this, you can say like uh this agent is an expert in weather.\nAnd then this one is going to have that weather tool.\nWe're going to remove it from this one and we're going to give it a handoff instead to that other weather agent.\nSo now if I'm going to restart this, hey, can you tell me the weather in uh New York?\nThe weather in New York is sunny, so you might want to grab your sunglasses if you're heading outside.\nEnjoy the day.\nAll right, that's that's the model's best attempt at New York accent.\nUh, we'll take it.\nUm, but you can see there that like it automatically handed off from that first agent to that second one and let it handle it.\nYou can through prompting do things like do you want it to announce that it's about to hand off?\nDo you not want to do that?\nSometimes it's a bit awkward if you're forcing it to always do it.\nSo like I would not necessarily try it, but maybe that's the type of experience that you want to have.\nUm, so that's handoffs.\nLet me do you a favor and push that code.\nBaby push.\nSo, the change voice, but it can change accents.\nYeah.\nSo uh, that's a good question.\nAnd the the the agent can't change the voice, but it can change the accent.\nUm that's because the um like again this goes back to like the model is a generative model.\nSo you can prompt it to have different like pronunciations, tonality like voice in that sense, but it cannot change the voice model that is actually being used to generate that output.\nSo maybe it...\nIs it like the whole real time request body that can't be changed or just the voice?\nSo can I like create a tool that could adjust the speed if someone was saying it's talking too fast for the noise reduction?\nYou should be able to change...\nI have not tried the speed because speed parameter changing at mid session because it literally came out today.\nUm uh I don't know Anoop if you tried this.\nNo.\nUm but like you can like essentially a handoff does change the session configuration.\nLike if we look back at like um the...\nLike one of the transcripts here, like now that we have a handoff, um let's go to this trace.\nSo you can see here that like it called the transfer to weather agent.\nUm but then like the this these instructions were talk to talk talk with a New York accent.\nUm so in this case it did change the instructions midway through the session.\nAnd the same way like when that handoff happens we take away the tools, we give it new tools.\nSo you can change those tools.\nYou could have a tool to change the tool, but my recommendation would be like use a different agent for that.\nUm, but then like the speed control, like you could, you should be able to send off an event, but I have not tried that.\nYeah.\nMaybe like the background, basically like if you had something and someone was like in a noisy environment, like hey, you seem to be getting interrupted, could you adjust...\nWe catch in the background and start adjusting that parameter so just the voice.\nYeah.\nSo the the question is like, for example, if someone is like in a noisy environment, like could you have the agent detect that and then use um like adjust some of the session configuration to deal with that and just the voice is protected?\nI don't know honestly which parameters are protected or not.\nThe good thing is the like the API will throw an error if if a thing didn't work.\nUm so it's a good way, it's a good thing to experiment with.\nUm Python...\nWith the previous ones you could do that Python.\nOh yeah.\nUm well with the Python uh agents SDK we're doing the chained approach.\nWe don't have a real time built in yet.\nSo just calling the old API Python.\nOh all right.\nYeah.\nYeah.\nUm then it should work.\nYeah.\nUh if you can do it in Python, like it should just work.\nCool.\nUm, so the other thing we talked about um is this delegation part.\nSo that's what I like had mentioned earlier that was in the diagram.\nSo this is essentially where you want to be able to have certain complex tasks dealt with by a more intelligent model.\nUm and the way we can do that is essentially just creating another um agent except on the back end.\nAnd because it's because the TypeScript SDK works both in the front end and back end, we can do that through um uh I think I have a...\nLet's see if we have a file here or not.\nUh we can do that using the same SDK.\nSo I'm going to create on the in the server folder here a new uh file.\nI'm going to call just agent.\nAnd in here we can build our regular textbased agent.\nSo this is essentially the same code that we've done before.\nUm and we can say this is a I don't know um called the Riddler.\nUm, you are excellent at creating riddles based on a target demographic and topic.\nUm, and we'll just give it a model of O4 mini.\nAlso, a reminder for those if you are trying to follow along um, and you run into troubles, post in the Slack and the Noob can help you with that.\nUm, so we have that new agent here.\nWe're not going to give it any tools or anything.\nUh and then we can export a function here that we just call run agent.\nAnd this is just going to take some input and then return that output.\nAnd we can go back into our front end code, create a new tool here, create riddle.\nUm, and this one we're just going to have take like two parameters, the demographic and the topic, and then call out the run agent function that is going to run on the server.\nWe can pass in...\nRealize I didn't specify this.\nLet's do demographic and topic and then create an input here of this.\nThe other thing you want to do when you're using server actions in uh Next.js is put that use server at the top.\nThat makes sure that this file executes on the server.\nAnd then we can pass in that demographic.\nAnd again, if you're using a different framework, this is just the equivalent of a fancy fetch request.\nSo like if you want to do an HTTP request to your service, if you want to maintain a websocket connection to your own backend, you can do all of those things to like talk back to other systems.\nSo with that, we can give that to our main agent.\nRiddle tools.\nAnd then what you want to do in these cases is like um, you can tell it like announce when you are about to do a task.\nDon't say you are calling a tool.\nThings like that can be helpful to like buy itself some time.\nHey there.\nUm, can you tell me a riddle uh for like a five-year-old Star Wars fan?\nHey, are you there?\nI'm still here.\nI'm working on creating that Star Wars riddle for you.\nIt should be ready in just a moment.\nHere's a riddle for your little Star Wars fan.\nI'm not.\nSo you can see that like because it announced that like what it's about to do, the tool call came back before it even finished what it previously said.\nAnd so like that's again one of the benefits of like if you can get your agent to um balance out that and like buy itself some time.\nThis is a good way to uh you know deal with the more complex tasks.\nAnd like it also means that you can like for example take all of the like more reasoning heavy workloads and take it out out of the voice agent model.\nUm for delegation.\nYeah.\nIs it possible to delegate to more than one agent like simultaneously or is it just one in the current SDK?\nUm you can, it's it's tool called, so like I think you like you would have two options, right?\nLike you could do par like parallel tool calling or you could um like have one tool that then triggers running multiple agents, right?\nSo like my recommendation would be that part potentially uh so that you're not relying on the model making the right decision of calling multiple tools at the same time, like you want to make the decision making for the voice agent always as easy as possible.\nThanks.\nYeah, previously when you did...\nDid you go back to previous page?\nYeah.\nOh, no.\nSorry.\nYour example.\nWhich example?\nThe one you're running where it had the output.\nOh, yeah.\nSo on line three there where it said I'm still here.\nYeah.\nIs that coming from your SDK?\nDo I have to use the SDK to do that or is the real time API?\nNo, it's the real time API.\nIt responded because I asked like, \"Hey, are you there?\"\nUm, so it it it tri...\nIt realized that like it didn't start anything and like interrupt.\nI was like, \"Hey, going.\"\nYeah.\nUm, yeah.\nSo, the thing is that like I didn't render out the tool calls in here, right?\nSo like what basically happened between this and this was like it started off a tool call um and then that tool call because I interrupted it, then it stopped that tool call because I interrupted it.\nIt stopped the the generation, it also reset that like transcript here.\nIt's a good indicator that like the interruption happened and so when I said this, it remembered it was trying to call a tool, did that tool call um and then gave you back the response.\nSo that's all the just the the regular realtime API.\nAny other questions around this?\nWhat's the cost per minute?\nWhat's the cost per minute?\nWe charge per token.\nThere's some translations.\nI I don't know.\nNuke, do you have the right?\nSo it it's more expensive than TTS and uh uh and like speech to text chained up with a model some like in most cases, but it depends on the use case um and sort of like your model choices and stuff.\nSo, a bit harder to say like what the per minute pricing is because again it's by tokens and it also depends on like if you have like transcription turned on and like how many function calls you have and things like that because it's a mix between audio and text tokens.\nUm so one of the interesting things and this is not a thing in the in the regular API, this is a agent agents SDK specific thing is um guard rail.\nSo like the agents SDK both in Python and TypeScript has this concept of guard rails that can either protect your input or your output to make sure that like the agent is not being meddled with or does things that are against policy.\nUm we took that same pattern and moved it over to the real time side um where essentially we're running these guard rails that you can define in parallel on on top of the transcription at all times.\nYou can kind of specify, you can see it at the bottom here, like how often you want to run them or if you only want to run them when the full transcript is available.\nBut this is a great way for you to like make sure that the model for doesn't violate certain policies.\nYou want to make sure that these run as efficiently as possible because they're still running in the client.\nUm, but this is a good way to still fix cert like stick to certain policies and if it violates those, it will interrupt it.\nNow there is sort of the bit of the cav, a bit of a caveat where because we're running this on a transcript, um it results in a bit of a timing aspect where if it if it would violate your guard rail in the first couple of words, chances are it will say those first couple of words and then get interrupted.\nIf it is happening at a later point in time, um then the transcript will be completed or like the text output is not really a transcript, the text output will be done before the audio is done speaking every like is is done saying everything and so in that case it will just correct itself.\nSo in that ca, like to give you an example of like this is a guardrail that just checks like is there the word Dom in the output, in this case, like if I would ask it like, hey, please call me Dom, chances are it will call me Dom and then self-correct.\nIf I tell it to tell me a story and only introduce Dom in the second act, then it will um catch that it was trying to do that in a at a much earlier point because that transcript is going to be done before the audio is being complet uh before the audio is being read out to the user.\n\n\nSo the user will never hear DOM, but instead the model is going to be like, okay, I'm sorry I couldn't help you with that. Like, let's do something else instead. And you can give it policy hints essentially on like why it violated this policy or what it should do instead. So you can give it this like output info where you can inform the model like why this happened.\n\nNo. Um you can choose what uh so the question was, is the transcript still done with whisper? Uh you can you can switch the transcript model. So we have, we released in March uh two models. One is GPT4 mini transcribe and we have two, right? Yeah. And GPT4 u GPT4 transcribe. I was trying to remember, like we have only one text to speech model, but we have two transcribe models for transcribe models. Awesome.\n\nUm, this is the main part of what I wanted to walk with you all through. So, um, one, I'm going to, um, post all of the links and the slides in the Slack channel, which let me go back to where was that slide? There we go. Um, so in that Slack, I'm going to post all of the resources. So if you want to check them out afterwards, I'll also like I already put a bunch of the resources that I talked about into the bottom of that starter repository. So you should have access there as well. And uh I'm happy to hang around, answer any questions. Yes.\n\nMy understand. Yeah. Um in the API.\n\nYeah, the the question was around how prompt caching works um and sort of whether we like for prompt caching that like we guide the requests to the same um same system again to run and whether like there's any control with that with real time because latency obviously matters. Um I don't think there is any controls about that. Um, no. Um, I'm getting a no from there. So, like I don't think there's any any controls around that right now.\n\nYes. Hey, so we all know that having natural conversations involves more than just spoken words. It involves detecting emotion and adjusting tone. Um, it also involves a cadence and even humming to let the other person know that you are listening.\n\nMhm. I wonder if the current speech-to-speech model is capable of having that kind of natural conversation.\n\nUm, part of this is like a a prompting challenge. So like it it definitely can have pretty natural sounding conversations. And like I think this is sort of the the part where I highly recommend to check out the openai.fm page. Um because this is like it's it's pretty interesting to see sort of uh if we go to um let's see if we find the um actually I didn't show one neat feature that I normally call out on the playground. If you're just getting started with real time and you don't even want to write any line of code, um this is a great way to just have conversations and try things out. But one of the things it has is this. Um it has a couple of system prompts. One of my favorite ones to show sort of this is the board teenager. Um so if we start this.\n\nHey there. Um, so I'm at AI Engineer World's Fair and everyone is super stoked about voice agents. Can you show me some excitement of this whole thing launching today?\n\nSee, I guess it's cool. Whatever. There's always new stuff launching. People get excited, but you know, it's just voice agents. Not really my thing to get all hyped up about it.\n\nSo you can see in this case, like it it put its own pauses in there and stuff like this wasn't a pause because the model was waiting, right? Like um it can deal with a lot of that sort of adjusting tone and voice and it can do similar things like reacting to like someone talking in and stuff.\n\nOkay, thanks. You're welcome.\n\nYes. So again on the new API that's been released, what else has changed? Semantic has improved. Is there anything improve in terms of tone detection or voice.\n\nUm we have not released any new VAT models. Um we primarily released like a new base model for or like a new model for the um GPT4 real-time model that is just better at function calling and has been overall well received from our alpha testers.\n\nYes. Can you inject uh can you inject audio as as a background audio like ambience audio like piping audio and so on?\n\nUm you like basically like you're in a real office, right? Yeah. Yeah. Um you can just intercept the audio that is coming back from the model and then like overlay your own audio.\n\nOh yeah. Hi. Um, I was wondering what your support is for like multiple speakers. If there's more than one person in a conversation, can it detect who's talking and and do pauses that way?\n\nThere's no no current like speaker detection in the model. Um, so it might struggle with that.\n\nYeah. Um, I just wanted to ask um about like custom voices. Is it limited to the preset voices you have or can I upload my voice as a sample for instance?\n\nIt's currently limited to the voices that we have. We keep adding new voices though.\n\nIs there going to be support to add custom voices anytime in the future?\n\nUm at any time in the future? Not not like not like years. Uh what I mean what I can say is like we're trying to make sure we're finding like the safest approach. Like we have an article on online that talks about sort of like the responsible take we're trying to take on this on making sure that like the vo the voice like if we're providing custom voices that it comes with the right guard rails in place and stuff to avoid abuse.\n\nAll right. Thank you. You're welcome.\n\nYes. I don't think that has changed. Um to my knowledge, my personal recommendation would be that like one of the things that you can do and this goes back to like for example in the demo that I was showing um if you're keeping track of the transcript and stuff you can re like when you're starting a new session you can populate that context by creating new items using the API. So like one of the things that you could do is starting a new session if you know what the previous context was because you kept track of it you can then um basically inject that as additional context.\n\nUm what type of event are you looking for?\n\nOh if there's a timeout event um I do not know right now, but all of our events are documented in the API reference.\n\nYeah. So when you see file, can you sorry, can you repeat that question once more?\n\nOh, I was wondering API function functions such as system file.\n\nOh uh whether the function calls can do things like system file reading and stuff. I would say it depends on where you're running that a where you're running that um real-time session. So if you're running it on the server, you can do anything you can do on the server. If it's if it's running in the browser, then you're limited to whatever things are available in the browser.\n\nUm, no, you should be able to like you could create a like web socket based like um voice agent that runs on on your device. I mean like it's going to use the real-time API for the model, but then like because the actual tool calls will get executed on your system, you should have access to whatever aspect of your system your program has access to.\n\nCool. Yes. So even before we get to voice agents, we all need bigger, better, and more diverse evaluation sets, especially for anything around function calling and parameterization. Uh do you have any best practices or suggestions for how we now take evaluation into the voice world? Should we keep things in text and then turn them, you know, use text to speech to have voice versions of it? Just if you have any suggestions for how we evaluate the the full range of inputs that we would expect users to bring to this.\n\nI mean one of my suggestions would be if you can go to the leadership track, go to Anoop's talk um which is tomorrow I think. Yeah. Um he's going to talk a lot more about like additional best practices of what we've learned um in uh building voice agents. I would say like if you can hold on to the audio, like it it's helpful. Um, obviously transcriptions definitely, but like it it's sort of like the audio is still the thing that is like the most powerful thing, especially for speech-to-speech models where you have the model act on the speech, not on the text, right? And like this is one of the one of the few things where like the the chained approach obviously makes some of this much more approachable because if you have if if you're if your agent is running on text anyways and you can just store the text and rerun it that makes that part of eval.\n\nThat makes sense. And then also for those of us who might be thinking about launching a new voice agent, how would you suggest evaluating it before we get to that stage that we'd have uh customer interactions to work with?\n\nUm I would start with like having like I think it goes back to like um like some of this is like uh where is it? there. Um like human review is an excellent solution for this, right? So like have it like have a system like like one of the big things with like things like lemonade and stuff is like they're able to go through all of these calls and like get an idea, but they also have their own like predetermined set of examples that they might want to test as they're developing the agent. So like that's a great first way. I would scope clearly the problem you're trying to solve as well. Like if you're if you're trying to sort of boil the ocean, it makes a lot of this significantly harder as opposed to like well scoping what the agent should be able to do and what it shouldn't be able to do.\n\nMakes sense. Thank you. You're welcome.\n\nGet getting on the same path of our friend over there. Uh actually when I'm testing my agents, my text agents conversational, uh I use promptful. It's a platform for do all the proper testing. Could I put another agent to talk with this this agent to do all the evaluation?\n\nI think you can try it. Um like I don't think it shouldn't work. Um um like so I put another voice agent talking to that agent to try to execute all the prompets and then I could get like the transcription community. I mean I know we have we have use cases where customers also use our models to like prompt humans right so like um where it's like for for like training use cases or other things for example um so she'll work out, but I don't know if anyone uses that kind of approach and lemonade does.\n\nOh cool. So the second the second picture is is exactly that.\n\nAwesome. Thank you. Welcome. Any any other questions?\n\nYeah, go ahead. Slightly related uh do you have something around wake word detection on the real time API road map or patterns for wake words?\n\nOh reports. Um no wake words. So that's activating the case. Um no, we don't have any wake words built in or anything.\n\nNo patterns either like any patterns to avoid costs. Um, no.\n\nOn device, I guess. No. Okay.\n\nYou could basically like what you can do is you can build your like you can turn off our uh voice activity detection and then build your own um and then basically use that. So like you could use a model that has like a like a VAD voice activity detection model that has wake words in it and then like do it that way and basically commit all of that audio to our API and then send like a commit like a commit event.\n\nCool. Thanks.\n\nWell, awesome. Thank you so much for taking the time and spending the afternoon with me.\n",
  "dumpedAt": "2025-07-21T18:43:24.572Z"
}