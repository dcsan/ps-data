{
  "episodeId": "DNw_Vrkoohc",
  "channelSlug": "@aidotengineer",
  "title": "Large Scale AI on Apple Silicon (as mentioned by @AndrejKarpathy ) â€” Alex Cheema, EXO Labs",
  "publishedAt": "2025-06-20T22:52:47.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1.23,
      "duration": 6.63
    },
    {
      "lang": "en",
      "text": "Yeah, thank you all for coming. Um, I'm",
      "offset": 15.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "sure you're wondering what this has to",
      "offset": 17.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "do with AI. Uh, but we'll get there. Um,",
      "offset": 18.8,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "so let me set the stage. Uh, at the turn",
      "offset": 22.72,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "of the 20th century, physics had a big",
      "offset": 25.279,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "problem. Um so the problem was that",
      "offset": 27.439,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "um the current theory said that there",
      "offset": 31.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "would be an infinite amount of energy in",
      "offset": 33.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the universe which was very strange",
      "offset": 35.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "because uh clearly that is it would",
      "offset": 36.96,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "violate a lot of physical principles. Um",
      "offset": 39.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 42.719,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "what did uh what did physicists do about",
      "offset": 45.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this? Well, this guy called Max Plank",
      "offset": 47.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "came along and he was like, um, well, if",
      "offset": 49.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "we just assume that all the energy is",
      "offset": 53.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "quantized in some way, then we can solve",
      "offset": 55.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this problem. And mathematically, it",
      "offset": 57.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "works out fine. But he introduced this",
      "offset": 58.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "new thing called H, which is like this",
      "offset": 60.8,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "constant that comes out of nowhere. And",
      "offset": 63.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "um, you know, they didn't really know at",
      "offset": 65.439,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "the time like how to measure that. Um so",
      "offset": 67.76,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "this another chap uh in 1909 called",
      "offset": 72.08,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "Milik came up with an experiment to sort",
      "offset": 76.159,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "of measure this value of H. So he did it",
      "offset": 79.119,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "indirectly by measuring the charge of an",
      "offset": 81.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "electron. And the way he did it is he",
      "offset": 83.759,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "put this sort of um charged oil spray",
      "offset": 86.32,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "into some water with some charged plates",
      "offset": 90.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and then looked at how fast this charge",
      "offset": 92.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "moves. the details aren't so important,",
      "offset": 95.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "but he got a result and you know it it",
      "offset": 97.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "was well it was like big news in the",
      "offset": 101.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "scientific community and um everyone was",
      "offset": 103.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "like oh man like now we know what this",
      "offset": 106.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this charge is. We know what this H is",
      "offset": 108.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "and all was good and then you know sort",
      "offset": 110.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "of he he he he had this result and and",
      "offset": 113.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "everyone was using it for their for",
      "offset": 116.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "their calculations and stuff and you",
      "offset": 117.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know many years went by and all the",
      "offset": 119.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "experiments seemed to agree with with",
      "offset": 121.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "his reading. Um",
      "offset": 124.079,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "now there was one problem which was it",
      "offset": 127.84,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "was wrong. Um and",
      "offset": 131.2,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "you know it took until just after that",
      "offset": 135.04,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "last data point like 1929",
      "offset": 139.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "for the sort of um people to realize",
      "offset": 141.84,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "that okay well you know this isn't the",
      "offset": 145.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "actual value. Um, and I think what's",
      "offset": 148.879,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interesting is when you look at the",
      "offset": 151.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "history, you've got this like period of",
      "offset": 152.879,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "uh 15 years or so where everyone thought",
      "offset": 155.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that this was the sort of the right",
      "offset": 160.08,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "value. And um, you know, you look back",
      "offset": 162,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "and you think, okay, why why did they",
      "offset": 166.4,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "think this? Um, and it turns out that,",
      "offset": 168.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you know, people were doing the right",
      "offset": 172.239,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "experiments. Um, they were doing a lot",
      "offset": 173.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of experiments and there's a lot of data",
      "offset": 175.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "points that would be in between here as",
      "offset": 177.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "well. it's just, you know, been lost in",
      "offset": 178.319,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "in in in in the history books. But, um,",
      "offset": 180.56,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you know, and part of that is because of",
      "offset": 184.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "embarrassment. This is like a very",
      "offset": 186.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "embarrassing thing for the scientific",
      "offset": 188,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "community. So,",
      "offset": 189.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "what happened? Well, basically these",
      "offset": 191.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "scientists, they were running their",
      "offset": 195.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "experiments, they got a result, and then",
      "offset": 196.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "they they saw, oh, wait, this great",
      "offset": 198.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Milican guy that had this result in",
      "offset": 201.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "1909, it doesn't agree with him, so I",
      "offset": 203.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "must be wrong. Um and then they sort of",
      "offset": 207.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "like fudged you know the experiments to",
      "offset": 210,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you know make it work such that they get",
      "offset": 213.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the same value as him and that went on",
      "offset": 215.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "for a long time and this is like this is",
      "offset": 218.159,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "not a trivial thing. This is quite an",
      "offset": 219.84,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "important thing to to science in",
      "offset": 221.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "general. Um",
      "offset": 222.48,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "so this kind of gets me up to the point",
      "offset": 225.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of like you know sort of scientific",
      "offset": 226.879,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "rigor and you know it it's it's actually",
      "offset": 229.04,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "very tricky to do science properly. Um",
      "offset": 232.64,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "and uh you know the fact that you know",
      "offset": 236.08,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "these subsequent experiments basically",
      "offset": 240.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "they all agreed on the wrong thing it",
      "offset": 242.799,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "says something about you know sort of",
      "offset": 245.28,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the way in which scientific progress",
      "offset": 248.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "happens. So you know there's a lot of",
      "offset": 250.879,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "inertia behind the current way of doing",
      "offset": 253.439,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "things and you know there there's",
      "offset": 254.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "another example which is sort of about",
      "offset": 257.68,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "questioning assumptions and you know the",
      "offset": 259.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there's another guy who who was looking",
      "offset": 261.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "at something completely different. He",
      "offset": 264.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "was looking at experiments with rats um",
      "offset": 265.52,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "and he wanted to basically he had an",
      "offset": 269.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "hypothesis. He wanted to test like some",
      "offset": 271.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "weird esoteric thing. He wanted to test",
      "offset": 274,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that like you know that rats he could",
      "offset": 276,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "basically get rats to um navigate a maze",
      "offset": 278.8,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "in a specific way. He wanted them to go",
      "offset": 282.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "through this corridor of doors, right?",
      "offset": 285.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And they would go through a random door",
      "offset": 287.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and they wanted them he wanted the rats",
      "offset": 289.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to come out of a door that was three",
      "offset": 291.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "doors along from from the one that they",
      "offset": 293.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "went in. It doesn't matter which one",
      "offset": 295.84,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "they go in. He always wanted it to be",
      "offset": 297.04,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "three. So he wanted to show that like",
      "offset": 298.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "they could actually think and you know",
      "offset": 299.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "be able to consistently go like three",
      "offset": 302.8,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "doors along. Um and",
      "offset": 304.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "basically he tried like a bunch of",
      "offset": 308.479,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "stuff. Um and what kept happening was",
      "offset": 310.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "whatever door like he basically put like",
      "offset": 314.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a a piece of food um on the door that",
      "offset": 316.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "was three doors along to make them go",
      "offset": 319.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "through that one. But what ended up",
      "offset": 321.12,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "happening is they just always went to",
      "offset": 322.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the the previous door. So I if if it was",
      "offset": 323.759,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "like you know door one that they went",
      "offset": 327.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "through and they want he wanted it to",
      "offset": 329.199,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "come out of door four and then he tested",
      "offset": 330.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it again where they go through door two",
      "offset": 333.039,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and they should go out of door five they",
      "offset": 334.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "would just still go out of door four and",
      "offset": 336.479,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "um he he tried a bunch of stuff. So like",
      "offset": 338.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "he was like okay why is this happening",
      "offset": 342.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "like um how do the rats know like to go",
      "offset": 343.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "back to that same door? So he's like he",
      "offset": 346.639,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "very meticulously went through and he",
      "offset": 348.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "made sure that there was no pattern or",
      "offset": 352.479,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "anything that they could distinguish on",
      "offset": 354,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the doors. He painted them all the same",
      "offset": 355.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "way. He made sure the patterns were all",
      "offset": 357.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the same, but still didn't work. Um he",
      "offset": 358.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "thought it might be the smell. So maybe",
      "offset": 362.479,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "there was like a smell that came from",
      "offset": 364.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the food. Um he tried basically putting",
      "offset": 365.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "chemicals in so that they couldn't smell",
      "offset": 368.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the the food and that didn't work",
      "offset": 370,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "either. Um, and then he thought, okay,",
      "offset": 371.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "maybe it's something to do with the",
      "offset": 374.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "lighting, you know, like a human could",
      "offset": 375.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "do this like through common sense, just",
      "offset": 378,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "sort of see like, okay, the lighting is",
      "offset": 379.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is in such and such a way and see the",
      "offset": 381.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "pattern. And so he covered up the",
      "offset": 382.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "corridors and stuff and made sure that",
      "offset": 384.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that couldn't be a thing. And still, you",
      "offset": 385.919,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "know, the same thing happened. Um, and",
      "offset": 387.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "eventually what he found out was that",
      "offset": 390.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "the reason they could consistently go to",
      "offset": 392.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that same door was because they",
      "offset": 396.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "remembered the sounds. So as they walked",
      "offset": 397.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "along uh they remembered the pattern of",
      "offset": 399.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the sounds um in this in this corridor.",
      "offset": 402,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So what he did was he put some sand um",
      "offset": 404.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "in in there so that they couldn't",
      "offset": 407.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "distinguish the sounds basically. Um now",
      "offset": 409.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "from a scientific perspective this is",
      "offset": 412.8,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "like S tier science in terms of you know",
      "offset": 414.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "really clearly looking at like what are",
      "offset": 418.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "all the assumptions I'm making and just",
      "offset": 420.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like systematically",
      "offset": 422.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "um eliminating them. Um, and you know,",
      "offset": 424.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "this is great great science, but you",
      "offset": 427.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "know, the the problem was that the",
      "offset": 430,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "scientific community didn't agree. Um,",
      "offset": 431.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so the the the people that were",
      "offset": 433.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "conducting experiments at the time, they",
      "offset": 435.52,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "they made a lot of these assumptions and",
      "offset": 436.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "they were kind of stuck in their ways.",
      "offset": 438.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "So they, you know, they discarded this.",
      "offset": 440.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "So, you know, none of the it wasn't",
      "offset": 442.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "cited. You know, this was this was",
      "offset": 444.479,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "basically just forgotten. Um and so I",
      "offset": 446.479,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "think you know there's",
      "offset": 450.8,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "sort of this um tendency to stick to the",
      "offset": 453.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "current way that things are done and",
      "offset": 457.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "even the methodology if it's spot on um",
      "offset": 458.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "if there's a certain way of doing things",
      "offset": 461.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "then that kind of you know has a lot of",
      "offset": 463.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "inertia behind it. Um and you know",
      "offset": 464.8,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Fineman talked about this in one of his",
      "offset": 467.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "talks called cargo cult science um in",
      "offset": 469.759,
      "duration": 8.401
    },
    {
      "lang": "en",
      "text": "like the 70s uh and he had this like you",
      "offset": 473.44,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "know quote the first principle is that",
      "offset": 478.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you must not fool yourself and you are",
      "offset": 480.639,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the easiest person to fool um and uh I",
      "offset": 482.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "think you know there's this tendency to",
      "offset": 486.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "sort of",
      "offset": 488.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "make oversimplify the science you know",
      "offset": 491.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "it is hard to get it right and if you're",
      "offset": 493.52,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "interested Gwen wrote like a a blog post",
      "offset": 495.599,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "all about this and there's some",
      "offset": 498.639,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "controversy about like who this guy was",
      "offset": 499.759,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "and stuff. Um but getting to AI uh so",
      "offset": 501.68,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "like you know there's a very similar",
      "offset": 506.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "thing happened in AI um you know sort of",
      "offset": 509.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "about questioning assumptions right and",
      "offset": 511.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you know just having a good idea out",
      "offset": 514,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "there is not enough. So you know in in",
      "offset": 516,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "1963 uh back propagation was um",
      "offset": 518.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "introduced in this paper and then it was",
      "offset": 522.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you know reinvented in 1976 in this",
      "offset": 524.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "paper and reinvented again in 1988 and",
      "offset": 526.959,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "then you know sort of deep convolutional",
      "offset": 530.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "neural networks uh were introduced here",
      "offset": 534.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and then it was only in 1989 that these",
      "offset": 536.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "two things were combined. So you had",
      "offset": 538.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "like uh deep CN ads and uh and back",
      "offset": 540.16,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "propagation. Um and then it was only",
      "offset": 543.839,
      "duration": 8.321
    },
    {
      "lang": "en",
      "text": "like three decades later that you know",
      "offset": 547.2,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "CNN's that were widely accepted. There",
      "offset": 552.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "was still like this massive skepticism",
      "offset": 554.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "even though the ideas were out there. Um",
      "offset": 556.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and why was that? I think like a big",
      "offset": 558.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "part of it is sort of again being stuck",
      "offset": 561.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "in in the way of doing things and",
      "offset": 563.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "looking at the existing hardware. So if",
      "offset": 565.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you look at obviously CPUs they have",
      "offset": 567.6,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "this vonoyman bottleneck uh you know",
      "offset": 570.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "they can they have really good single",
      "offset": 572.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "core performance but you know if if",
      "offset": 574.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you're sort of having to read uh memory",
      "offset": 576.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "often then it's usually bottlenecked by",
      "offset": 579.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "that um and you know you can sort of",
      "offset": 581.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "look at like at a systems level why did",
      "offset": 584.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "GPUs fix that um and it sort of changed",
      "offset": 586.959,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "this like ratio of how many you know how",
      "offset": 590,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "many bytes you have to load to how many",
      "offset": 593.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "flops you can execute Um, and you know,",
      "offset": 594.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "it's kind of striking, you know, like",
      "offset": 598.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "when you look at the the the history of",
      "offset": 600.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this, this was like a groundbreaking",
      "offset": 602.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "paper and a very famous paper like where",
      "offset": 604.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "they trained a network on a thousand",
      "offset": 606.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "machines uh 16,000 CPU cores and it took",
      "offset": 609.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "three days and then um like less than a",
      "offset": 612.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "year later there was this other paper",
      "offset": 615.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that did the got the exact same results",
      "offset": 616.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "but it took like you know three machines",
      "offset": 618.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "in a couple of days and this was using",
      "offset": 621.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "um you know uh hardware acceleration",
      "offset": 624.079,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "like using GPUs. So, um yeah, the the",
      "offset": 626.24,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "this gets me to the hardware lottery,",
      "offset": 631.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "which is essentially this idea",
      "offset": 633.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "introduced by Sarah Hooker in 2020, um",
      "offset": 634.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "which says that, you know, the best",
      "offset": 638.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "research ideas don't necessarily win.",
      "offset": 640.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "There's a lot of factors that sort of",
      "offset": 642.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "make it so that a great idea can be out",
      "offset": 644.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "there, great science can be being done,",
      "offset": 646.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but you know, it doesn't necessarily get",
      "offset": 648.32,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "adopted um and accepted. Um",
      "offset": 650.16,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "there's a recent example of this which I",
      "offset": 655.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "think is quite interesting that I um you",
      "offset": 657.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "know preparing for this I I I had the",
      "offset": 659.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "realization that you know LLMs are kind",
      "offset": 661.92,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "of a forcing like they're kind of",
      "offset": 665.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "creating inertia as well because the the",
      "offset": 667.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "things that they're good at are the",
      "offset": 670.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "things that people will work on. So if",
      "offset": 671.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "they're good at generating Python code,",
      "offset": 673.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the more people will write Python code",
      "offset": 674.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and it's this feedback loop of you know",
      "offset": 676.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "more people use it and then LMS get",
      "offset": 679.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "better at that thing and you know now if",
      "offset": 681.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "you wanted to come out with a new",
      "offset": 683.04,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "programming language maybe it is a",
      "offset": 684,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "really good idea. Um you wouldn't it's",
      "offset": 685.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "much harder to get adoption if the",
      "offset": 688.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "tooling is like way worse if the LMS",
      "offset": 689.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "don't have good support for it. And",
      "offset": 692.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "there's this paper that you know this",
      "offset": 693.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "this result like basically um they made",
      "offset": 695.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "a table of like all the uh tasks that",
      "offset": 698.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like different languages are best at and",
      "offset": 701.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "basically everything was Python. Um and",
      "offset": 703.2,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "yeah like uh what it 90 to 97% of all",
      "offset": 706.72,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "problems uh yeah Python was the best",
      "offset": 710.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "thing. Um",
      "offset": 713.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so what if we did question our",
      "offset": 716.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "assumptions? Um, so that brings me to",
      "offset": 719.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "sort of what I'm working on with Exo. Um",
      "offset": 722,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "so what we're doing is we're building a",
      "offset": 725.12,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "orchestration layer um for",
      "offset": 729.04,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "uh you know AI that runs on different",
      "offset": 733.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "hardware uh targets and it's sort of",
      "offset": 735.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "we're sitting at this layer that I",
      "offset": 738.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "haven't seen much um right now and it's",
      "offset": 739.839,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "kind of a pain point um in terms of just",
      "offset": 742.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "having this like reliable thing that can",
      "offset": 745.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "orchestrate a lot of um different kinds",
      "offset": 747.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of devices with different con",
      "offset": 750.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "connections in like this ad hoc kind of",
      "offset": 752.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "mesh network. Um, and just to give you",
      "offset": 754.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like some idea of like kind of the kind",
      "offset": 756.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of things that we're we're we're doing",
      "offset": 759.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and the kind of um, you know, the",
      "offset": 761.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "solution space that we sit in like you",
      "offset": 764.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "know everything in in Exo is modeled as",
      "offset": 766.079,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "as um a causely consistent set of",
      "offset": 768.72,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "events. So there's um essentially like",
      "offset": 772.56,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "an ordering on everything that happens",
      "offset": 775.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "across the whole distributed system. if",
      "offset": 777.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "there's all these things going on, it's",
      "offset": 779.519,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "really hard to reason about like sort of",
      "offset": 780.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "if you want to move a KV cache around",
      "offset": 782.56,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "like how do you know that it happens",
      "offset": 784.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "successfully? How do you know then if",
      "offset": 785.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "something depends on that? Um so you",
      "offset": 787.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "build this sort of causal graph and you",
      "offset": 789.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "know you can then reason about the",
      "offset": 791.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "system and get some guarantees about you",
      "offset": 793.519,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "know sort of uh where the data is and",
      "offset": 796.48,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "you know what's going on. So, just to",
      "offset": 800.639,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "give you a quick example to put this",
      "offset": 802.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "into like um you know more practical",
      "offset": 803.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "terms like what does this enable, Spark",
      "offset": 806.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "is coming out soon. Uh I'm still waiting",
      "offset": 809.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh but it's been delayed a few times I",
      "offset": 812.079,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "think but hopefully soon. Um and this is",
      "offset": 813.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Nvidia's new like consumer thing if you",
      "offset": 817.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "haven't seen that. And it's like has a",
      "offset": 818.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "lot of flops. It's pretty good for the",
      "offset": 820.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the cost. Um but the memory bandwidth is",
      "offset": 821.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "kind of lacking. It doesn't have that",
      "offset": 825.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "much memory. Like Studio has a lot more",
      "offset": 826.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "memory bandwidth but a lot less flops.",
      "offset": 828.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So if you look at sort of just if you",
      "offset": 830.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "wanted to generate uh with an LLM like",
      "offset": 832.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "there's two phases right there's the",
      "offset": 835.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "prefill phase which is computebound and",
      "offset": 837.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the generation phase which is memory",
      "offset": 839.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "bandwidth bound. Um as far as I I I know",
      "offset": 840.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "there isn't really anything that can",
      "offset": 844.16,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "nicely do this reliably where you'd have",
      "offset": 845.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "these like different sets of devices and",
      "offset": 847.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "then figure out the best way to like run",
      "offset": 849.6,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "this whole workload um across you know",
      "offset": 851.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "all the devices that you have available.",
      "offset": 854.959,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Um but this is possible now with exo. Um",
      "offset": 856.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "and just another example, this is some",
      "offset": 860.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "research that we're we're working on. Um",
      "offset": 862.399,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "it's more on the training side. Uh so",
      "offset": 864.8,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "um this this will be out pretty soon. It",
      "offset": 869.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "be it'll be made public. Um essentially",
      "offset": 872.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "we've we're kind of questioning all",
      "offset": 875.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "these assumptions about like you know",
      "offset": 877.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "what hardware is best to run on. If you",
      "offset": 878.639,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "look at Apple silicon, it has a lot more",
      "offset": 880.24,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "memory obviously, but it's a lot more",
      "offset": 881.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "expensive per flop. Um but what if you",
      "offset": 883.519,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "could use that memory for something",
      "offset": 887.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "useful to make training more efficient?",
      "offset": 888.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Um and there's been this whole area of",
      "offset": 890.959,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "research of like sort of second order",
      "offset": 893.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "methods and you know different ways of",
      "offset": 894.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "um making training more efficient but",
      "offset": 897.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that a lot of them have been discarded",
      "offset": 899.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "because of the memory requirements. Um,",
      "offset": 900.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and so what we've done is we've come out",
      "offset": 903.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "with the we're going to come out with",
      "offset": 905.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this like new optimizer um, which is",
      "offset": 906.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "essentially like two times more",
      "offset": 909.36,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "efficient per uh, per flop um, than",
      "offset": 911.199,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Adam, but it uses a lot more memory. So",
      "offset": 915.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "if you look at the sort of ratio of like",
      "offset": 918.079,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "memory to flops of Apple silicon, it's",
      "offset": 919.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "probably like 20x um, it's around 20x",
      "offset": 921.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "depending on which one you get uh,",
      "offset": 924.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Nvidia. So you've got all that spare",
      "offset": 925.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "memory that you can, you know, think",
      "offset": 927.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "about, okay, what are like what does a",
      "offset": 929.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "solution space look like if we make use",
      "offset": 930.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "of that memory. Um, and yeah, we we we",
      "offset": 932.639,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "we're buying a lot of mags. We're like",
      "offset": 936.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "trying a lot of stuff. Uh, this is the",
      "offset": 937.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "first batch, but there's going to be",
      "offset": 940.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "another batch. Um, and um, yeah, we're",
      "offset": 942.079,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "just running a lot of experiments at",
      "offset": 945.839,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "scale. Like there's a lot of stuff in",
      "offset": 947.6,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "that paper that's really interesting in",
      "offset": 948.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "terms of like not many people have tried",
      "offset": 950.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "doing um, large training runs on Apple",
      "offset": 952.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "silicon. I mean nobody nobody has uh",
      "offset": 954.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "even I mean talking to people at Apple",
      "offset": 956.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like they're also surprised at these",
      "offset": 958.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "results like they're not really aware of",
      "offset": 960.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "what the cap uh the hardware is capable",
      "offset": 962.959,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "of. Um I mean just a final thing as well",
      "offset": 964.88,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "around sort of you know going back to",
      "offset": 969.12,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "the um this the process of doing science",
      "offset": 971.759,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "right like if if if anything like",
      "offset": 976.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "publishing kind of um like publishing",
      "offset": 978.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "results that are maybe not the best I",
      "offset": 982,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "think is is something that we need to",
      "offset": 983.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like more normalize and you know we're",
      "offset": 985.44,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "sort of we're doing a lot to",
      "offset": 987.68,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "make all the data accessible whether",
      "offset": 992.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it's good or bad. And like, you know,",
      "offset": 994.8,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "right now you can go to",
      "offset": 996.32,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "benchmarks.xlabs.net",
      "offset": 997.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "and there's a lot of configurations",
      "offset": 998.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "there that are just really bad. Um, but",
      "offset": 1000.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "you know, I think having that data out",
      "offset": 1003.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "there and publishing stuff that isn't",
      "offset": 1006.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "necessarily like the best thing is uh",
      "offset": 1008.88,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "super important. So, we've got this um",
      "offset": 1012.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "this these benchmarks. Part of that is",
      "offset": 1016.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "also running like these Macs uh",
      "offset": 1018.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "continuously in CI and different",
      "offset": 1020.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "devices. we'll probably end up with like",
      "offset": 1022.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "at least one of pretty much like any",
      "offset": 1024.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "device that you can run reasonably run",
      "offset": 1026.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "um an AI workload on and just have those",
      "offset": 1029.28,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "continuously pushing out to the",
      "offset": 1032.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "benchmarks. Um",
      "offset": 1033.439,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "yeah, so uh we're coming out with a big",
      "offset": 1036.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "release in",
      "offset": 1038.799,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "uh the end of this week. So um that will",
      "offset": 1040.959,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "sort of you know be uh this this",
      "offset": 1045.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "orchestration",
      "offset": 1049.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "um this orchestration layer uh that I",
      "offset": 1050.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "talked about and with that will come a",
      "offset": 1053.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "lot of this sort of tooling around it.",
      "offset": 1055.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So the benchmarks website um a lot of",
      "offset": 1057.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "stuff around you know uh being able to",
      "offset": 1060.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "test different algorithms that run on",
      "offset": 1063.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "different uh devices. Um, so we're",
      "offset": 1065.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "coming out of Exo Gym for example, which",
      "offset": 1068.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "is a way to run experiments on uh, you",
      "offset": 1070.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "know, if you don't have 16 Macs like",
      "offset": 1074.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "this, then you know, being able to",
      "offset": 1077.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "actually just run them locally uh,",
      "offset": 1079.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "quickly and test different distributed",
      "offset": 1082,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "algorithms. Uh, that would be part of",
      "offset": 1083.6,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "those releases.",
      "offset": 1086.32,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "Yep, that's it.",
      "offset": 1091.919,
      "duration": 9.33
    },
    {
      "lang": "en",
      "text": "[Applause]",
      "offset": 1094.19,
      "duration": 7.059
    },
    {
      "lang": "en",
      "text": "We've actually got like four minutes, so",
      "offset": 1102.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "we're ahead of schedule. Anyone have",
      "offset": 1104.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "questions? I'll don't get mad if I don't",
      "offset": 1105.679,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "pick you, but I'll say we'll do these",
      "offset": 1108.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "these three right here.",
      "offset": 1110.16,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Uh right now like right now it's a bit",
      "offset": 1120.799,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "of a like ideally not like ideally what",
      "offset": 1123.679,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "we we want to sit at like this um like",
      "offset": 1127.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "higher up in the stack and and just",
      "offset": 1130.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "focus on this orchestration piece",
      "offset": 1132.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "because I think that's where there's",
      "offset": 1133.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like not much um and you know there's",
      "offset": 1134.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the MLX team for example have done a lot",
      "offset": 1138,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of work on like DI MLX distributed which",
      "offset": 1139.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is really good uh it's just it's really",
      "offset": 1141.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "fast but it's like kind of brittle So if",
      "offset": 1143.84,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "you lose a connection, it would just",
      "offset": 1145.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "break completely like you just you get",
      "offset": 1146.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "errors everywhere. Um and it's super",
      "offset": 1148.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "specialized obviously to their",
      "offset": 1150.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "configuration. So um you know our hope",
      "offset": 1152.16,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "is that there will be a lot of work on",
      "offset": 1156.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that layer that is just done by all",
      "offset": 1157.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "these frameworks like MLX and VLM um and",
      "offset": 1160.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then and then we can sit on top. But",
      "offset": 1163.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "right now we're doing a lot of sort of",
      "offset": 1164.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "just work with like the MLX team for",
      "offset": 1166,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "example and just you know um building",
      "offset": 1167.919,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "out those primitives.",
      "offset": 1170.72,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Yeah, it's it's orange one.",
      "offset": 1182.16,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Yeah. So, a lot of this stuff is the",
      "offset": 1199.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "absolute numbers don't matter too much.",
      "offset": 1201.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "It's more about the ratios. So, AMD has",
      "offset": 1203.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "way more flops than Apple Silicon would",
      "offset": 1205.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "have. So, the ratio is probably going to",
      "offset": 1207.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "be, you know, still really high. Um, and",
      "offset": 1208.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "then yeah, it just it's just sort of,",
      "offset": 1212.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you know, what do you end up being",
      "offset": 1214.32,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "bottlenecked by? Obviously, they have a",
      "offset": 1215.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "lot more network bandwidth, but again,",
      "offset": 1216.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "but it's it's relative to the flops,",
      "offset": 1218,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? So if you look at the ratio of",
      "offset": 1219.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "network bandwidth to flops of Apple",
      "offset": 1221.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "silicon is actually better than AMD. Um",
      "offset": 1223.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "so I'm not sure like I would need to",
      "offset": 1226.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "look at the specific uh device that",
      "offset": 1228.08,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "you're talking about but maybe",
      "offset": 1230.32,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "yeah we we're we're not working on that.",
      "offset": 1245.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "I think there's other uh projects that",
      "offset": 1248.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "might be working on that kind of thing",
      "offset": 1251.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like maybe prime intellect um",
      "offset": 1252.96,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "uh hyperbolic perhaps as well. Um",
      "offset": 1256.96,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "a few of them in this room um but maybe",
      "offset": 1261.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "there's some synergy there. I don't know",
      "offset": 1264.799,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "like uh for now at least there just",
      "offset": 1266.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "seems to be a lot of work to be done on",
      "offset": 1267.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "like you know these these private",
      "offset": 1268.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "clusters where you have a fully trusted",
      "offset": 1270.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "setup and um you know you don't really",
      "offset": 1272.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "care about uh all the hard problems that",
      "offset": 1275.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "come with uh you know untrusted public",
      "offset": 1278.32,
      "duration": 4.41
    },
    {
      "lang": "en",
      "text": "networks.",
      "offset": 1281.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1282.73,
      "duration": 3.23
    }
  ],
  "cleanText": "[Music]\nYeah, thank you all for coming. Um, I'm sure you're wondering what this has to do with AI, but we'll get there. Um, so let me set the stage. Uh, at the turn of the 20th century, physics had a big problem. Um, so the problem was that the current theory said that there would be an infinite amount of energy in the universe, which was very strange because, uh, clearly that is, it would violate a lot of physical principles. Um, so what did, uh, what did physicists do about this? Well, this guy called Max Plank came along and he was like, um, well, if we just assume that all the energy is quantized in some way, then we can solve this problem. And mathematically, it works out fine. But he introduced this new thing called H, which is like this constant that comes out of nowhere. And, um, you know, they didn't really know at the time like how to measure that. Um, so this another chap, uh, in 1909 called Milikan came up with an experiment to sort of measure this value of H. So he did it indirectly by measuring the charge of an electron. And the way he did it is he put this sort of, um, charged oil spray into some water with some charged plates and then looked at how fast this charge moves. The details aren't so important, but he got a result and, you know, it, it was, well, it was like big news in the scientific community and, um, everyone was like, oh man, like now we know what this, this charge is. We know what this H is, and all was good. And then, you know, sort of he, he, he, he had this result and, and everyone was using it for their, for their calculations and stuff, and, you know, many years went by and all the experiments seemed to agree with, with his reading. Um, now there was one problem, which was it was wrong. Um, and, you know, it took until just after that last data point, like 1929, for the sort of, um, people to realize that, okay, well, you know, this isn't the actual value. Um, and I think what's interesting is when you look at the history, you've got this like period of, uh, 15 years or so where everyone thought that this was the sort of the right value. And, um, you know, you look back and you think, okay, why, why did they think this? Um, and it turns out that, you know, people were doing the right experiments. Um, they were doing a lot of experiments, and there's a lot of data points that would be in between here as well. It's just, you know, been lost in, in, in, in the history books. But, um, you know, and part of that is because of embarrassment. This is like a very embarrassing thing for the scientific community. So, what happened? Well, basically these scientists, they were running their experiments, they got a result, and then they, they saw, oh, wait, this great Milikan guy that had this result in 1909, it doesn't agree with him, so I must be wrong. Um, and then they sort of like fudged, you know, the experiments to, you know, make it work such that they get the same value as him, and that went on for a long time. And this is like, this is not a trivial thing. This is quite an important thing to science in general. Um, so this kind of gets me up to the point of like, you know, sort of scientific rigor, and you know, it, it's, it's actually very tricky to do science properly. Um, and, uh, you know, the fact that, you know, these subsequent experiments basically, they all agreed on the wrong thing, it says something about, you know, sort of the way in which scientific progress happens. So, you know, there's a lot of inertia behind the current way of doing things. And, you know, there, there's another example which is sort of about questioning assumptions, and you know, the, there's another guy who, who was looking at something completely different. He was looking at experiments with rats, um, and he wanted to basically, he had an hypothesis. He wanted to test like some weird esoteric thing. He wanted to test that like, you know, that rats, he could basically get rats to, um, navigate a maze in a specific way. He wanted them to go through this corridor of doors, right? And they would go through a random door, and they wanted them, he wanted the rats to come out of a door that was three doors along from, from the one that they went in. It doesn't matter which one they go in. He always wanted it to be three. So he wanted to show that like they could actually think and, you know, be able to consistently go like three doors along. Um, and basically he tried like a bunch of stuff. Um, and what kept happening was whatever door, like he basically put like a, a piece of food, um, on the door that was three doors along to make them go through that one. But what ended up happening is they just always went to the, the previous door. So if it was like, you know, door one that they went through and they want, he wanted it to come out of door four, and then he tested it again where they go through door two and they should go out of door five, they would just still go out of door four. And, um, he, he tried a bunch of stuff. So like he was like, okay, why is this happening? Like, um, how do the rats know like to go back to that same door? So he's like, he very meticulously went through and he made sure that there was no pattern or anything that they could distinguish on the doors. He painted them all the same way. He made sure the patterns were all the same, but still didn't work. Um, he thought it might be the smell. So maybe there was like a smell that came from the food. Um, he tried basically putting chemicals in so that they couldn't smell the, the food, and that didn't work either. Um, and then he thought, okay, maybe it's something to do with the lighting, you know, like a human could do this like through common sense, just sort of see like, okay, the lighting is, is in such and such a way and see the pattern. And so he covered up the corridors and stuff and made sure that that couldn't be a thing. And still, you know, the same thing happened. Um, and eventually what he found out was that the reason they could consistently go to that same door was because they remembered the sounds. So as they walked along, uh, they remembered the pattern of the sounds, um, in this in this corridor. So what he did was he put some sand, um, in in there so that they couldn't distinguish the sounds basically. Um, now from a scientific perspective, this is like S tier science in terms of, you know, really clearly looking at like what are all the assumptions I'm making and just like systematically, um, eliminating them. Um, and you know, this is great, great science, but you know, the, the problem was that the scientific community didn't agree. Um, so the, the, the people that were conducting experiments at the time, they, they made a lot of these assumptions and they were kind of stuck in their ways. So they, you know, they discarded this. So, you know, none of the, it wasn't cited. You know, this was, this was basically just forgotten. Um, and so I think, you know, there's sort of this, um, tendency to stick to the current way that things are done, and even the methodology, if it's spot on, um, if there's a certain way of doing things, then that kind of, you know, has a lot of inertia behind it. Um, and you know, Fineman talked about this in one of his talks called cargo cult science, um, in like the 70s, uh, and he had this like, you know, quote, the first principle is that you must not fool yourself, and you are the easiest person to fool. Um, and, uh, I think, you know, there's this tendency to sort of make oversimplify the science, you know, it is hard to get it right. And if you're interested, Gwen wrote like a blog post all about this, and there's some controversy about like who this guy was and stuff. Um, but getting to AI, uh, so like, you know, there's a very similar thing happened in AI, um, you know, sort of about questioning assumptions, right? And you know, just having a good idea out there is not enough. So, you know, in 1963, backpropagation was, um, introduced in this paper, and then it was, you know, reinvented in 1976 in this paper, and reinvented again in 1988, and then, you know, sort of deep convolutional neural networks, uh, were introduced here, and then it was only in 1989 that these two things were combined. So you had like, uh, deep CNNs and, uh, and backpropagation. Um, and then it was only like three decades later that, you know, CNNs that were widely accepted. There was still like this massive skepticism, even though the ideas were out there. Um, and why was that? I think like a big part of it is sort of again being stuck in, in the way of doing things and looking at the existing hardware. So if you look at obviously CPUs, they have this von Neumann bottleneck, uh, you know, they can, they have really good single core performance, but you know, if, if you're sort of having to read, uh, memory often, then it's usually bottlenecked by that. Um, and you know, you can sort of look at like at a systems level, why did GPUs fix that? Um, and it sort of changed this like ratio of how many, you know, how many bytes you have to load to how many flops you can execute. Um, and you know, it's kind of striking, you know, like when you look at the, the, the history of this, this was like a groundbreaking paper and a very famous paper, like where they trained a network on a thousand machines, uh, 16,000 CPU cores, and it took three days. And then, um, like less than a year later, there was this other paper that did the, got the exact same results, but it took like, you know, three machines in a couple of days, and this was using, um, you know, uh, hardware acceleration, like using GPUs. So, um, yeah, the, the, this gets me to the hardware lottery, which is essentially this idea introduced by Sarah Hooker in 2020, um, which says that, you know, the best research ideas don't necessarily win. There's a lot of factors that sort of make it so that a great idea can be out there, great science can be being done, but, you know, it doesn't necessarily get adopted, um, and accepted. Um, there's a recent example of this, which I think is quite interesting that I, um, you know, preparing for this, I, I, I had the realization that, you know, LLMs are kind of a forcing, like they're kind of creating inertia as well because the, the things that they're good at are the things that people will work on. So if they're good at generating Python code, the more people will write Python code, and it's this feedback loop of, you know, more people use it, and then LLMs get better at that thing. And, you know, now if you wanted to come out with a new programming language, maybe it is a really good idea. Um, you wouldn't, it's much harder to get adoption if the tooling is like way worse, if the LLMs don't have good support for it. And there's this paper that, you know, this, this result, like basically, um, they made a table of like all the, uh, tasks that like different languages are best at, and basically everything was Python. Um, and yeah, like, uh, what, it, 90 to 97% of all problems, uh, yeah, Python was the best thing. Um, so what if we did question our assumptions? Um, so that brings me to sort of what I'm working on with EXO Labs. Um, so what we're doing is we're building a orchestration layer, um, for, uh, you know, AI that runs on different hardware, uh, targets, and it's sort of, we're sitting at this layer that I haven't seen much, um, right now, and it's kind of a pain point, um, in terms of just having this like reliable thing that can orchestrate a lot of, um, different kinds of devices with different connections in like this ad hoc kind of mesh network. Um, and just to give you like some idea of like kind of the kind of things that we're, we're, we're doing and the kind of, um, you know, the solution space that we sit in, like, you know, everything in EXO is modeled as, as, um, a causally consistent set of events. So there's, um, essentially like an ordering on everything that happens across the whole distributed system. If there's all these things going on, it's really hard to reason about like sort of, if you want to move a KV cache around, like how do you know that it happens successfully? How do you know then if something depends on that? Um, so you build this sort of causal graph, and you know, you can then reason about the system and get some guarantees about, you know, sort of, uh, where the data is and, you know, what's going on. So, just to give you a quick example to put this into like, um, you know, more practical terms, like what does this enable? Spark is coming out soon. Uh, I'm still waiting, uh, but it's been delayed a few times, I think, but hopefully soon. Um, and this is Nvidia's new like consumer thing, if you haven't seen that. And it's like has a lot of flops. It's pretty good for the, the cost. Um, but the memory bandwidth is kind of lacking. It doesn't have that much memory. Like Studio has a lot more memory bandwidth, but a lot less flops. So if you look at sort of just if you wanted to generate, uh, with an LLM, like there's two phases, right? There's the prefill phase, which is compute bound, and the generation phase, which is memory bandwidth bound. Um, as far as I, I, I know, there isn't really anything that can nicely do this reliably where you'd have these like different sets of devices and then figure out the best way to like run this whole workload, um, across, you know, all the devices that you have available. Um, but this is possible now with EXO. Um, and just another example, this is some research that we're, we're working on. Um, it's more on the training side. Uh, so, um, this, this will be out pretty soon. It'll be, it'll be made public. Um, essentially, we've, we're kind of questioning all these assumptions about like, you know, what hardware is best to run on. If you look at Apple Silicon, it has a lot more memory, obviously, but it's a lot more expensive per flop. Um, but what if you could use that memory for something useful to make training more efficient? Um, and there's been this whole area of research of like sort of second order methods and, you know, different ways of, um, making training more efficient, but that a lot of them have been discarded because of the memory requirements. Um, and so what we've done is we've come out with the, we're going to come out with this like new optimizer, um, which is essentially like two times more efficient per, uh, per flop, um, than Adam, but it uses a lot more memory. So if you look at the sort of ratio of like memory to flops of Apple Silicon, it's probably like 20x, um, it's around 20x depending on which one you get, uh, Nvidia. So you've got all that spare memory that you can, you know, think about, okay, what are like, what does a solution space look like if we make use of that memory? Um, and yeah, we, we, we're buying a lot of Macs. We're like trying a lot of stuff. Uh, this is the first batch, but there's going to be another batch.\n\n\nUm, and um, yeah, we're just running a lot of experiments at scale.\nLike there's a lot of stuff in that paper that's really interesting in terms of like not many people have tried doing um, large training runs on Apple Silicon.\nI mean nobody nobody has uh even I mean talking to people at Apple, like they're also surprised at these results, like they're not really aware of what the cap uh the hardware is capable of.\nUm I mean just a final thing as well around sort of you know going back to the um this the process of doing science, right?\nLike if if if anything like publishing kind of um like publishing results that are maybe not the best, I think is is something that we need to like more normalize and you know we're sort of we're doing a lot to make all the data accessible whether it's good or bad.\nAnd like, you know, right now you can go to benchmarks.xlabs.net and there's a lot of configurations there that are just really bad.\nUm, but you know, I think having that data out there and publishing stuff that isn't necessarily like the best thing is uh super important.\nSo, we've got this um this these benchmarks.\nPart of that is also running like these Macs uh continuously in CI and different devices.\nWe'll probably end up with like at least one of pretty much like any device that you can run reasonably run um an AI workload on and just have those continuously pushing out to the benchmarks.\nUm yeah, so uh we're coming out with a big release in uh the end of this week.\nSo um that will sort of you know be uh this this orchestration um this orchestration layer uh that I talked about and with that will come a lot of this sort of tooling around it.\nSo the benchmarks website um a lot of stuff around you know uh being able to test different algorithms that run on different uh devices.\nUm, so we're coming out of Exo Gym for example, which is a way to run experiments on uh, you know, if you don't have 16 Macs like this, then you know, being able to actually just run them locally uh, quickly and test different distributed algorithms.\nUh, that would be part of those releases.\nYep, that's it.\n[Applause]\nWe've actually got like four minutes, so we're ahead of schedule.\nAnyone have questions?\nI'll don't get mad if I don't pick you, but I'll say we'll do these these three right here.\nUh right now like right now it's a bit of a like ideally not like ideally what we we want to sit at like this um like higher up in the stack and and just focus on this orchestration piece because I think that's where there's like not much um and you know there's the MLX team for example have done a lot of work on like DI MLX distributed which is really good uh it's just it's really fast but it's like kind of brittle.\nSo if you lose a connection, it would just break completely like you just you get errors everywhere.\nUm and it's super specialized obviously to their configuration.\nSo um you know our hope is that there will be a lot of work on that layer that is just done by all these frameworks like MLX and VLM um and then and then we can sit on top.\nBut right now we're doing a lot of sort of just work with like the MLX team for example and just you know um building out those primitives.\nYeah, it's it's orange one.\nYeah.\nSo, a lot of this stuff is the absolute numbers don't matter too much.\nIt's more about the ratios.\nSo, AMD has way more flops than Apple Silicon would have.\nSo, the ratio is probably going to be, you know, still really high.\nUm, and then yeah, it just it's just sort of, you know, what do you end up being bottlenecked by?\nObviously, they have a lot more network bandwidth, but again, but it's it's relative to the flops, right?\nSo if you look at the ratio of network bandwidth to flops of Apple silicon is actually better than AMD.\nUm so I'm not sure like I would need to look at the specific uh device that you're talking about but maybe yeah we we're we're not working on that.\nI think there's other uh projects that might be working on that kind of thing like maybe prime intellect um uh hyperbolic perhaps as well.\nUm a few of them in this room um but maybe there's some synergy there.\nI don't know like uh for now at least there just seems to be a lot of work to be done on like you know these these private clusters where you have a fully trusted setup and um you know you don't really care about uh all the hard problems that come with uh you know untrusted public networks.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.923Z"
}