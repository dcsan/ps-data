{
  "episodeId": "sn79oS4MZFI",
  "channelSlug": "@aidotengineer",
  "title": "Case Study + Deep Dive: Telemedicine Support Agents with LangGraph/MCP - Dan Mason",
  "publishedAt": "2025-06-22T16:00:06.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 0.33,
      "duration": 7.479
    },
    {
      "lang": "en",
      "text": "Okay. Um, hey everybody. Thank you so",
      "offset": 14.719,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "much for coming. Uh, really appreciate",
      "offset": 17.279,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "you being here. Um, this this is a great",
      "offset": 18.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "show. I love this show. Um, I was here",
      "offset": 20.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "last year as an attendee. Um, spoke in",
      "offset": 22.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "New York uh at the the New York Summit",
      "offset": 24.4,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "in February and I'm I'm really thrilled",
      "offset": 26.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "to be back. Um, so this is very much a",
      "offset": 27.439,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "showand tell. I I I said this in the",
      "offset": 31.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "Slack channel, so anybody's not in the",
      "offset": 32.88,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "Slack channel, feel free to join it.",
      "offset": 33.92,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "There's a couple links in there that",
      "offset": 35.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "might be helpful to you. Um, it is",
      "offset": 36.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "workshop Langraph MCP agents if anybody",
      "offset": 38.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "needs that. Um, but fundamentally, uh,",
      "offset": 41.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I'm I'm just here to walk through some",
      "offset": 44.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "some really interesting work, um, that",
      "offset": 46,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "my team's been doing around, um,",
      "offset": 47.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "building agent workflows for, uh, a",
      "offset": 49.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "healthcare use case. Um, and, uh, this",
      "offset": 51.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "is this is very much like the way we did",
      "offset": 54.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it. Um, I'll get into some details about",
      "offset": 56.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "that. It's not the only way to do it.",
      "offset": 57.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um, and I'm hopeful that somebody in",
      "offset": 59.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "this audience might look at this and be",
      "offset": 61.92,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "like, &quot;That's dumb. You should do that",
      "offset": 63.199,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "better.&quot; And, you know, please raise",
      "offset": 64.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "your hand and tell me. Um, but, uh, but",
      "offset": 65.68,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "it's been really fun to build. I'm",
      "offset": 68,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "really really happy with the results.",
      "offset": 69.439,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And, uh, really excited to show you guys",
      "offset": 71.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "um, what it's all about. Um, okay. So, I",
      "offset": 73.119,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "will go into presentation mode.",
      "offset": 75.28,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "All right. Here we go. Okay. Uh first",
      "offset": 81.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "just a very quick couple things about",
      "offset": 84.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Stride. Um that's me if anybody needs my",
      "offset": 85.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "LinkedIn but um there's a couple other",
      "offset": 87.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "places you can find that. Um we are a",
      "offset": 89.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "custom software consultancy. Um so what",
      "offset": 92.24,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "that means in practice is uh whatever",
      "offset": 94.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you need we'll build it. We have been",
      "offset": 97.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "doing a whole lot of AI stuff. Um this",
      "offset": 98.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "kind of falls into a few specific",
      "offset": 101.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "buckets. Um we use a lot of AI for code",
      "offset": 103.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "generation. Um we we have a couple of",
      "offset": 105.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "both products and services we've built",
      "offset": 107.68,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to do things like uh unit test creation",
      "offset": 109.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "and and maintenance. We've done a bunch",
      "offset": 111.759,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "of stuff around uh modernization of of",
      "offset": 113.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "super old dumb code bases. Um you know",
      "offset": 115.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "things like uh you know early 2000s",
      "offset": 118.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "era.net is one of the things we",
      "offset": 120.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "specialize in. Um but what I'm going to",
      "offset": 122,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "show you today is really uh what we do",
      "offset": 124.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "around agent workflows. So the the idea",
      "offset": 125.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "with you know this agent workflow stuff",
      "offset": 128,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "is really just that you know it's",
      "offset": 129.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "something that could be done with",
      "offset": 131.599,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "traditional software and and this thing",
      "offset": 132.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "I'm going to show you was done with",
      "offset": 134.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "traditional software in its first run.",
      "offset": 135.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Um, but we have rebuilt it with an LLM",
      "offset": 138.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "at the core to make it more flexible,",
      "offset": 140.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "more capable, and you know, ultimately,",
      "offset": 143.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "um, just a lot cooler. Um, so, uh,",
      "offset": 145.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "really excited to show you guys more",
      "offset": 147.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "about that. So, I'm going to start with",
      "offset": 149.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "a little bit of grounding. Um, I'm going",
      "offset": 151.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to do a case study, um, which is very",
      "offset": 153.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "brief, but I'll give you a sense of kind",
      "offset": 154.8,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "of the problem we were trying to solve",
      "offset": 156,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and and and how well I think we solved",
      "offset": 157.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it. Um, and then we'll go as as deep as",
      "offset": 158.879,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we all want to go in terms of of how it",
      "offset": 161.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "works. Um, let me ask this up front. If",
      "offset": 162.879,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "you have questions, please raise your",
      "offset": 165.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "hand. I will try to notice and I'll try",
      "offset": 167.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "to get to you. Um, there are some mics",
      "offset": 168.64,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "we could pass around. Um, but it",
      "offset": 170.08,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "probably is better for you just to shout",
      "offset": 171.519,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "it out and then I'll repeat it um into",
      "offset": 172.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "into my mic. Um, and and fundamentally",
      "offset": 174.319,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like I have no idea if this is two hours",
      "offset": 177.28,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "worth of material. It probably is, but",
      "offset": 178.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you know, please uh keep me honest. Um,",
      "offset": 180.879,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "I'll talk about anything that is",
      "offset": 182.56,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "relevant to this that you guys want to",
      "offset": 183.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "talk about. So, um, the client here uh",
      "offset": 184.879,
      "duration": 8.481
    },
    {
      "lang": "en",
      "text": "is Aila. So, Aila Science is uh a",
      "offset": 189.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "women's health um sort of institution",
      "offset": 193.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "which is trying to help with uh the",
      "offset": 195.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "treatment of early pregnancy loss um",
      "offset": 197.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "otherwise usually known as miscarriage.",
      "offset": 199.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Um what what this is though is",
      "offset": 201.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "specifically a treatment where what",
      "offset": 203.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "happens is that you know you experience",
      "offset": 205.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the event you end up at the hospital or",
      "offset": 206.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "at a clinic. They send you home with",
      "offset": 208.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "medicine, right? The medicine is",
      "offset": 209.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "something that then you have to",
      "offset": 211.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "administer yourself um at both a very",
      "offset": 212.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "traumatic time for you and your family",
      "offset": 215.2,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "at a time when you know you need to keep",
      "offset": 216.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "track of when you are supposed to do",
      "offset": 218.159,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "things. It can be really challenging. Um",
      "offset": 219.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's other use cases beyond this in",
      "offset": 221.599,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "terms of chemotherapy when people have",
      "offset": 223.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "trouble remembering what day it is, you",
      "offset": 225.12,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "know, let alone what they're supposed to",
      "offset": 226.4,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "be doing, right? You know, there there's",
      "offset": 227.44,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "a variety of treatments that this is",
      "offset": 228.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "relevant for. Um but AVA in particular",
      "offset": 229.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "has a system that they use to help",
      "offset": 232.56,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "people essentially administer these tele",
      "offset": 234.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "medicine regimes at home right and um",
      "offset": 236.879,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that system is text message based so",
      "offset": 239.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "everything here I'm going to show you is",
      "offset": 241.439,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "essentially a text messaging based uh",
      "offset": 242.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "engine with you know some some core",
      "offset": 245.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "business logic um that helps people stay",
      "offset": 247.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "on track it answers their questions. It",
      "offset": 249.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "checks in on them you know to make sure",
      "offset": 251.599,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that the treatment went well. They still",
      "offset": 252.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "have a doctor relationship. This isn't",
      "offset": 254.319,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "replacing the doctor. it is simply",
      "offset": 255.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "helping to get people through this",
      "offset": 257.759,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "treatment without a doctor's direct",
      "offset": 259.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "support at least a lot of the time. Um,",
      "offset": 260.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so a few disclaimers up front. First of",
      "offset": 263.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "all, I'm going to show you a whole bunch",
      "offset": 265.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of stuff here that is is the is the",
      "offset": 267.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "client's actual code. Um, thank you so",
      "offset": 268.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "much to my client. Thanks to Aila for",
      "offset": 270.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "being so open with this. It's really",
      "offset": 271.919,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "awesome. I'm really happy to be able to",
      "offset": 273.12,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "show you as much as I'm going to show",
      "offset": 274.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you. Um, I have redacted a few things.",
      "offset": 275.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Um, I think what's left is still, you",
      "offset": 277.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "know, very much going to give you the",
      "offset": 279.12,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "character of the whole thing and and an",
      "offset": 280.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "idea of how it works. Um, Stride, we are",
      "offset": 281.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "custom software people. So, we built",
      "offset": 285.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "custom software. I I don't want to hide",
      "offset": 286.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that part, right? It's possible to do a",
      "offset": 288.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "lot of this stuff with off-the-shelf",
      "offset": 290.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "tools. Um, but there were some specific",
      "offset": 291.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "requirements this client had that made",
      "offset": 293.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it better, frankly, to build a lot of it",
      "offset": 295.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "custom. Um, so we did, but we did the",
      "offset": 297.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "best we could to use, you know, big",
      "offset": 299.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "swads of off-the-shelf, right? So,",
      "offset": 301.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you'll see a lot of Langraph, Langchain,",
      "offset": 302.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Lang Smith, you know, a bunch of other",
      "offset": 304.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "things like that, you know, very much in",
      "offset": 306.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "here because we do believe that that",
      "offset": 307.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "adds value and that it fundamentally",
      "offset": 308.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "makes the system a lot more explainable.",
      "offset": 310.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Um and you know there are also some",
      "offset": 313.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "constraints in terms of how it's hosted.",
      "offset": 315.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "This is you know at least partially",
      "offset": 316.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "intersecting with with patient data and",
      "offset": 318,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "various things like HIPPA and other",
      "offset": 319.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "privacy requirements. Um the other thing",
      "offset": 321.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "as I I started with earlier there's no",
      "offset": 324.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "right way to do this but this one does",
      "offset": 326.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "work for us and and I think you'll see",
      "offset": 328,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "as we walk through it some of the",
      "offset": 329.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "choices we made. Um you know there there",
      "offset": 330.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "are definitely other ways we could have",
      "offset": 333.12,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "plugged the tools together. I think",
      "offset": 334.4,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "there's definitely other ways we could",
      "offset": 335.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "have done this workflow. Um but but we",
      "offset": 336.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like how this came out. it it preserved",
      "offset": 338.479,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "some of the things that we really knew",
      "offset": 340.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "were important to our client and that",
      "offset": 341.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "kind of reserve um you know a lot of",
      "offset": 342.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "human judgment you know as opposed to",
      "offset": 345.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "sort of taking the elements entirely at",
      "offset": 346.8,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "their word and this is very much a",
      "offset": 348.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "hybrid system with humans very much in",
      "offset": 349.759,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the loop um and again as I mentioned",
      "offset": 351.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "before um I would really love it if you",
      "offset": 355.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "guys looked at what we're doing here and",
      "offset": 357.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "said that's dumb that or or have you",
      "offset": 359.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "thought about this right because a you",
      "offset": 361.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "know this is a project which we've only",
      "offset": 364,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "been working on for a few months but you",
      "offset": 365.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "know things have already evolved that's",
      "offset": 366.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the way it is in in AI. So I am certain",
      "offset": 368.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and I know of a handful of things where",
      "offset": 370.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "you know we could replace some of the",
      "offset": 372.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "choices we made with newer more modern",
      "offset": 373.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "choices. Um and at the same time you",
      "offset": 375.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "know there may be cases where you know I",
      "offset": 377.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I'm genuinely not using line graph.",
      "offset": 378.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Right. I I would love if someone raises",
      "offset": 380.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "their hand and tells me that. So please",
      "offset": 382.72,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "do uh have that in the back of your",
      "offset": 384.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "brains.",
      "offset": 385.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Cool. Um really briefly on the stack",
      "offset": 387.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "that we used and and on the team that we",
      "offset": 390.16,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "built. So the first thing again there's",
      "offset": 391.52,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "a lot of lang chain in here. Um that is",
      "offset": 393.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "not because other frameworks can't do",
      "offset": 395.759,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this. It's not because we couldn't build",
      "offset": 397.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "our own. The number one reason we went",
      "offset": 398.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "with this is because of how easy it is",
      "offset": 400.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to explain the system to other people,",
      "offset": 402.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "right? You know, if if you look at and",
      "offset": 404,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "I'll show you the Langraph stuff in",
      "offset": 405.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "particular. It it was straightforward to",
      "offset": 406.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "go into our client, you know, on on a",
      "offset": 408.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "very early day in the project and say,",
      "offset": 410.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "&quot;Hey, this is how this thing works. You",
      "offset": 412.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "can see it goes from here to here.",
      "offset": 413.84,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "There's loops here. Like this is where",
      "offset": 415.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "we're doing our um you know, our our",
      "offset": 416.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "evaluation of the process and here's",
      "offset": 418.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "where humans come in.&quot; It was very",
      "offset": 419.84,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "straightforward to do that. Um, and I",
      "offset": 421.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think it would have been a lot harder",
      "offset": 423.759,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "with something that was less visual and",
      "offset": 424.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and frankly just less well orchestrated.",
      "offset": 426.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "So, so we're happy with this, right?",
      "offset": 428.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "There there are some trade-offs to the",
      "offset": 429.52,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the lang chain tools, but they're",
      "offset": 430.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they're mostly things we can live with.",
      "offset": 432.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Um, we are using uh Claude in the",
      "offset": 434.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "examples that I'm going to show you",
      "offset": 437.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "here. Um, but the the core code that we",
      "offset": 438.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "wrote works with Gemini, works with",
      "offset": 440.24,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "OpenAI. Um, there are a few reasons that",
      "offset": 441.599,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we think Claude is is better for this.",
      "offset": 443.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "I'll get into that as we go. Um, but you",
      "offset": 445.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "know, there's there's no model specific",
      "offset": 447.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "stuff really happening here. This is",
      "offset": 448.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "almost all just tool calling and MCP and",
      "offset": 450.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "you know other things that are are",
      "offset": 452.479,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "pretty portable across most of the",
      "offset": 453.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "models.",
      "offset": 454.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "Um the stack overall is not just the LM",
      "offset": 456.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "piece, right? So the LM piece is Python",
      "offset": 460.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and a line graph container. Um and then",
      "offset": 462,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the the other piece, right, the piece",
      "offset": 464.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that is a text message gateway and a",
      "offset": 465.599,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "database and a a dashboard, which I'm",
      "offset": 467.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going to show you pretty extensively is",
      "offset": 469.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Node and React and MongoDB and Twilio",
      "offset": 471.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and the whole thing is hosted in AWS.",
      "offset": 474.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "None none of that has to be that way.",
      "offset": 476.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "That's just what we picked. Um, you",
      "offset": 477.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "know, the main reason we picked AWS was",
      "offset": 479.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "for, you know, that this has to support",
      "offset": 480.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "multiple different regions. We had to be",
      "offset": 482.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "able to deploy stuff, you know, entirely",
      "offset": 484.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "in Europe in a couple of cases, right?",
      "offset": 485.84,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "And so, we needed to make sure that we",
      "offset": 487.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "had, you know, a decent set of of, you",
      "offset": 488.879,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "know, cloud connections that we could",
      "offset": 490.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "work with.",
      "offset": 492.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Uh, eval. So, I will show you the the",
      "offset": 494.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "eval system that we built. Um, we were",
      "offset": 496.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "not able to use or at least I shouldn't",
      "offset": 498.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "say not able. We chose not to use the",
      "offset": 501.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "stuff um entirely off the shelf from",
      "offset": 503.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Langmith. This is partly because I",
      "offset": 504.56,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "didn't really want to be fully locked",
      "offset": 506.72,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "into them. I wanted the data to live",
      "offset": 507.759,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "there. I wanted to be able to see, you",
      "offset": 508.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "know, the current system in Langmith,",
      "offset": 510.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "but I wanted to have something separate.",
      "offset": 512,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "And it turns out that some of what we",
      "offset": 513.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "had to do to make the eval um, you know,",
      "offset": 515.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "fundamentally functional required a lot",
      "offset": 517.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of pre-processing. So, we built an",
      "offset": 519.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "external harness that essentially pulls",
      "offset": 521.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "data out of Langmith, processes it, and",
      "offset": 523.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then runs things through PromptFu. Um,",
      "offset": 525.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and one of the reasons we picked Prompt",
      "offset": 527.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Fu, if anyone's ever worked with it, um,",
      "offset": 529.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "they have a very flexible, uh, they call",
      "offset": 530.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "it an LLM rubric. And, and so this is an",
      "offset": 532.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "LLM as a judge. you basically describe",
      "offset": 534.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "how you want the the eval to work. Um",
      "offset": 536.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you feed the data in and you know then",
      "offset": 538.88,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "it gives you a separate sort of",
      "offset": 540.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "visualization for that. So we we ended",
      "offset": 541.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "up very happy with it. It's not the only",
      "offset": 543.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "way to do it at all. It was definitely",
      "offset": 544.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you know the the thing that fit best for",
      "offset": 546.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "for us",
      "offset": 548,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "uh the team. So there were uh and and",
      "offset": 550.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "still are um two software engineers, one",
      "offset": 553.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "designer um and me and and I'm just I I",
      "offset": 555.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "would not call myself a software",
      "offset": 558.56,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "engineer. That's why I didn't include",
      "offset": 559.68,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "myself in that pool. You can imagine",
      "offset": 560.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "there there being two software engineers",
      "offset": 562.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "kind of maintaining the core system that",
      "offset": 563.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "has the the gateway and the dashboard",
      "offset": 565.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and the text message stuff, right? And",
      "offset": 568.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and the database. Um I maintained and",
      "offset": 570.32,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and built basically everything on the",
      "offset": 573.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the Langraph side, right? So imagine",
      "offset": 575.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "this as being two separate systems that",
      "offset": 577.68,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "talk to each other through a well-",
      "offset": 579.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "definfined contract. Um and that two",
      "offset": 580.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that those two software engineers",
      "offset": 582.24,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "understand roughly how my code works,",
      "offset": 583.68,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "but they really weren't maintaining it.",
      "offset": 585.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "You know, it was it was almost entirely",
      "offset": 586.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "me with uh AI friends. Um, and on that",
      "offset": 587.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "note, so everything I'm going to show",
      "offset": 591.839,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "you is the code that I wrote and and I",
      "offset": 593.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "want to be very clear. Um, I haven't",
      "offset": 595.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "been a real software engineer in a long",
      "offset": 597.279,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "time. I do have an engineering",
      "offset": 598.56,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "background. I spent seven years out of",
      "offset": 599.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "college, you know, hacking on mobile",
      "offset": 601.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "apps. Um, I took 15 years off and went",
      "offset": 602.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to be a product person and for about two",
      "offset": 605.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "years now I've been back. But what that",
      "offset": 607.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "really means is just that, you know,",
      "offset": 610,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "essentially the stuff you're seeing,",
      "offset": 611.68,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "right, or the stuff that I'm going to",
      "offset": 612.8,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "show you is mostly, you know, code that",
      "offset": 613.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "I wrote with Klein. That's that's my",
      "offset": 615.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "personal favorite. Um, and so there's a",
      "offset": 616.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "bunch of options here. I like client",
      "offset": 619.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "best of all these options. Um, you can",
      "offset": 621.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "use anything you want. The code isn't",
      "offset": 623.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "actually that complicated. Like I would",
      "offset": 625.12,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "estimate, and I I haven't actually",
      "offset": 626.399,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "counted, but there's probably a few",
      "offset": 627.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "thousand lines of Python and there's a",
      "offset": 628.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "few thousand lines of prompt. It's it's",
      "offset": 630.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "about equal, right? So I vibecoded the",
      "offset": 632.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Python and I mostly handcoded the",
      "offset": 635.12,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "prompt. Um, not 100%, right? But but",
      "offset": 637.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that's that's the way to think about the",
      "offset": 640.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "division of labor here. Um, and for that",
      "offset": 641.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "matters, I mean, any of these tools can",
      "offset": 643.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "be great. The main reason I picked Klein",
      "offset": 645.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "was just because, you know, we did not",
      "offset": 646.959,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "need u, you know, sort of a",
      "offset": 648.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "hyperoptimized, you know, um, like $20 a",
      "offset": 650.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "month flow. Like I've spent a lot more",
      "offset": 652.72,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "than $20 a month on tokens. That's",
      "offset": 654,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that's just the way it is. Um, you know,",
      "offset": 655.519,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "that it was worth spending the money to",
      "offset": 657.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "just have sort of the best available",
      "offset": 658.8,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "context of the model at any given point.",
      "offset": 659.92,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "Um, client is a very good way to do",
      "offset": 661.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that.",
      "offset": 662.959,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "Okay. And there's a little bit of of",
      "offset": 664.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "sample code. So I did mention, and this",
      "offset": 665.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "is in the Slack channel as well. If you",
      "offset": 667.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "wanted to follow along with any of this,",
      "offset": 668.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you could sort of do it by standing up",
      "offset": 670.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "your own little Langraph container with",
      "offset": 672.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "MCP. you're more than welcome to do",
      "offset": 674,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that. Um, everything I'm going to show",
      "offset": 675.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you though is proprietary client code,",
      "offset": 677.12,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "so I I obviously can't send you those",
      "offset": 678.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "links. So, if you'd like to, um, feel",
      "offset": 680.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "free to fire it up. Um, we have two",
      "offset": 682.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "hours, which is a really long period of",
      "offset": 683.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "time. If if you're interested in",
      "offset": 685.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "spending a little time at the end of",
      "offset": 687.44,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "this actually working with some of this",
      "offset": 688.48,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "real code, I'm I'm thrilled to do that.",
      "offset": 689.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Um, so feel free to get yourself ready",
      "offset": 691.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "in the meantime.",
      "offset": 693.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Okay, just a couple things up front just",
      "offset": 695.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to make sure we're level set in terms of",
      "offset": 698.079,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "of sort of the terms and kind of the way",
      "offset": 699.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that we're talking about this stuff. So,",
      "offset": 701.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "um, I do like the lang chain definition",
      "offset": 703.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "here of of agent. Um, basically just",
      "offset": 705.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "because, you know, you'll see what we're",
      "offset": 708.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "doing here is using an LLM to control",
      "offset": 709.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the to control the flow, right, of of",
      "offset": 711.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "this application. That is literally what",
      "offset": 714,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "what this is. Um, and I like this and I",
      "offset": 716.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "don't know if Chris is here. He he was",
      "offset": 719.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "at the last event in New York. I like",
      "offset": 721.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this as a way of of sort of justifying",
      "offset": 723.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the way that we tried to architect this",
      "offset": 725.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "system and why, right? So the idea of of",
      "offset": 727.36,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "agents in production, right? You have to",
      "offset": 730.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "know what they're doing. You have to",
      "offset": 733.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "know, you know, that they can do it, and",
      "offset": 735.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you have to be able to steer, right? If",
      "offset": 736.959,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "you only have a couple of these things,",
      "offset": 739.12,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "you end up with with bad outcomes,",
      "offset": 740.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "right? And so I I I just like this",
      "offset": 741.839,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "framing of if you're capable, but you",
      "offset": 743.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "can't tell what it's doing, it's",
      "offset": 744.959,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "dangerous. If you know exactly what it's",
      "offset": 745.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "doing, but you can't control it, it does",
      "offset": 747.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "weird stuff and you can't help. Uh,",
      "offset": 749.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "please go ahead. Bunch of us are looking",
      "offset": 751.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "for the Slack channel. Oh, uh, let me",
      "offset": 752.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "find that one more. It's right here",
      "offset": 754.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "actually. workshop langraph MCP agents.",
      "offset": 755.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Got it. Okay,",
      "offset": 759.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "no problem. Okay, but uh and so",
      "offset": 762.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "transparency with no control is",
      "offset": 763.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "frustrating and control with no",
      "offset": 765.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "capability is useless. I I just love",
      "offset": 766.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "this framing. I think this is exactly",
      "offset": 768.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "the thing that we were trying to solve",
      "offset": 769.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "for. We needed something that was able",
      "offset": 771.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to do the job, clear about what it was",
      "offset": 773.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "doing, and that was steerable by humans",
      "offset": 775.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in a really obvious way. So, um with",
      "offset": 776.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that said, I'm going to start with a",
      "offset": 779.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "case study, right? And this is going to",
      "offset": 781.6,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "be a little weird out of context, but",
      "offset": 782.639,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "hopefully this will give you a sense of",
      "offset": 783.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of what we were trying to solve for. So",
      "offset": 785.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the idea here was that there's there's",
      "offset": 788.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "an existing product, right? So there was",
      "offset": 790.16,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "a product out there that was essentially",
      "offset": 791.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "having uh you know, humans manually push",
      "offset": 793.519,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "buttons on a console that would enable",
      "offset": 796.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uh a text message to go out, right? So",
      "offset": 799.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "you would read what the patient had",
      "offset": 800.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "said. They could say, &quot;I took my",
      "offset": 802.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "medicine at 3 p.m.&quot; They could say, &quot;I'm",
      "offset": 803.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "bleeding and I don't know what's going",
      "offset": 806,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "on. Like, am I okay?&quot;",
      "offset": 807.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "um they could ask other sorts of",
      "offset": 808.959,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "questions about the treatment and a",
      "offset": 810.16,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "human would have to go into a piece of",
      "offset": 811.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "software and click you know a button",
      "offset": 813.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that accurately reflected sort of where",
      "offset": 815.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "in the workflow somebody was right",
      "offset": 817.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "because you know you can model a lot of",
      "offset": 819.279,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "this out you know imagine there being",
      "offset": 820.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "fantastically complicated flowcharts of",
      "offset": 822.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "all the things that can happen during a",
      "offset": 824.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "medical treatment um so the AIA team had",
      "offset": 825.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "built this right they realized though",
      "offset": 828.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that essentially to scale the human team",
      "offset": 830.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to be able to serve a lot more patients",
      "offset": 832.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "was prohibitive right they they needed",
      "offset": 833.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "too many people clicking too many",
      "offset": 835.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "buttons they also realized they couldn't",
      "offset": 836.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "really scale the system to new",
      "offset": 838.959,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "treatments, right, which was something",
      "offset": 840.16,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "they wanted to do. That this isn't the",
      "offset": 841.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "only regimen that you needed to support.",
      "offset": 842.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "They had other ones. Um, and so the idea",
      "offset": 844.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is that either they were going to",
      "offset": 847.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "rebuild the legacy software to be more",
      "offset": 848.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "flexible or they were going to",
      "offset": 850.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "essentially rebuild it to to to use a",
      "offset": 851.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "different kind of decisioning at the",
      "offset": 854.399,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "core. And and when they were looking at",
      "offset": 855.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "doing this, you know, LLM had had",
      "offset": 856.959,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "started, I think, become capable enough",
      "offset": 858.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "to to handle this kind of of work. Um,",
      "offset": 859.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so what we built, what we did is we",
      "offset": 862.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "built for them a workflow and",
      "offset": 863.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "essentially a piece of software that",
      "offset": 866.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "connects to it that enabled them to do",
      "offset": 867.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "new treatments flexibly, right? So this",
      "offset": 870.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "idea of essentially defining a blueprint",
      "offset": 872.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and a knowledge base is the way that we",
      "offset": 873.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we thought about this. Um, and and",
      "offset": 875.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "essentially medically approved language,",
      "offset": 877.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "right? So one of the reasons that you",
      "offset": 878.8,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "had humans pressing buttons instead of",
      "offset": 879.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "typing text messages is because this is",
      "offset": 881.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "medical advice, right? You know, you you",
      "offset": 883.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "are not um you should not at least be",
      "offset": 885.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "giving medical advice um that differs",
      "offset": 887.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "substantially from from this approved",
      "offset": 889.36,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "language, right? there's reasons that",
      "offset": 890.959,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "this stuff, you know, is said the way",
      "offset": 892.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "that it's said. Um, you know, and and",
      "offset": 893.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "doctors have, you know, similar",
      "offset": 894.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "limitations. Um, we also built a",
      "offset": 895.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "self-evaluation function, which I'll go",
      "offset": 899.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "into tremendously, um, in a second. Uh,",
      "offset": 900.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we wanted to make sure that we caught",
      "offset": 903.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "essentially situations that were",
      "offset": 905.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "complicated, um, and surfaced them for",
      "offset": 906.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "humans, right? Because we wanted to have",
      "offset": 908.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "a human in the loop, but we were trying",
      "offset": 909.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to raise up the existing folks who were",
      "offset": 911.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "really just operating the system and and",
      "offset": 913.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "clicking all those buttons to be",
      "offset": 915.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "supervisors of of agents that were doing",
      "offset": 916.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that instead, right? that that really",
      "offset": 918.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "was the the model that we were working",
      "offset": 920.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "with at its core. Saw a question over",
      "offset": 921.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "here. Yeah, you may have said it. Were",
      "offset": 923.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "these operators?",
      "offset": 925.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Uh so the question is are these",
      "offset": 927.279,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "operators medically trained? There is a",
      "offset": 928.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "physician's assistant who essentially",
      "offset": 930.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "leads the operations team. So the way",
      "offset": 932.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that you can think about it is that um",
      "offset": 934.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "she would be escalated to whenever",
      "offset": 936.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "something came up that was outside of",
      "offset": 938,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the blueprint, right? So if you had a",
      "offset": 939.279,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "situation where they're just like, I'm",
      "offset": 940.8,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "really not sure what to do here, a Slack",
      "offset": 941.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "goes out to that channel with a",
      "offset": 943.44,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "physician's assistant in it who would",
      "offset": 944.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "then give medical advice. So, you know,",
      "offset": 946.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "again, this is one of the reasons it was",
      "offset": 948.24,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "hard to scale, right? Because, you know,",
      "offset": 949.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you only had one of those people on this",
      "offset": 950.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "particular team. Sure. Um, and so, you",
      "offset": 952.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "know, to to sort of jump a little bit",
      "offset": 956,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "ahead, but hopefully you'll see why this",
      "offset": 958,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "is in a minute. This roughly and and",
      "offset": 959.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "again, we're we're still doing the",
      "offset": 962.079,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "measurement, right? We're still trying",
      "offset": 962.959,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "to figure out exactly what, you know,",
      "offset": 963.839,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "capacity has has gone up to. Um, we",
      "offset": 965.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "think it's something like 10x. We think",
      "offset": 967.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that they can surface roughly 10x more",
      "offset": 969.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "people with this new approach. Um, now",
      "offset": 971.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it's not free, right? We have to build",
      "offset": 974.399,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "the software. we have to pay for the",
      "offset": 975.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "tokens. Um tokens can get expensive. But",
      "offset": 976.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "if you think about, you know, just the",
      "offset": 978.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "scale issues involved in scaling up a",
      "offset": 980,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "team of people and again in building the",
      "offset": 981.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "software to be more flexible for more",
      "offset": 983.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "treatments, um we think this capacity",
      "offset": 984.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "increase is is very very much warranted",
      "offset": 986.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and very much the thing that that you",
      "offset": 988.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "know solves solves the problem. Um and",
      "offset": 990.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you can do new treatments in new",
      "offset": 992.959,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "workflows without writing more code.",
      "offset": 994.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Right? That was the single biggest thing",
      "offset": 995.92,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "about this. And and you'll see what",
      "offset": 997.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we're doing here is largely Google Docs,",
      "offset": 998.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "right? And you know, we have some more",
      "offset": 1000.24,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "advanced techniques to to manage those",
      "offset": 1001.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "things and inversion them over time, but",
      "offset": 1003.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "but we're talking about being able to",
      "offset": 1004.88,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "support whole new treatments and whole",
      "offset": 1006.079,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "new workflows without going back to the",
      "offset": 1007.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "code, right? That's hugely valuable to",
      "offset": 1008.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "these guys.",
      "offset": 1010.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Question",
      "offset": 1012.72,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "you mentioned velocity",
      "offset": 1014.32,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "measuring quality of care. So question",
      "offset": 1017.199,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "was uh velocity increases. Is there a",
      "offset": 1020.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "quality of care measure? Um short answer",
      "offset": 1021.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "is it's early, right? I mean this is",
      "offset": 1023.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "still a system that's you know in",
      "offset": 1025.52,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "progress. it is being used with real",
      "offset": 1026.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "people, but it's still very very much",
      "offset": 1028.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "early on that the way that I think we're",
      "offset": 1029.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "looking at it is that there would be",
      "offset": 1031.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "some combination of the operators being",
      "offset": 1032.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the ultimate arbiter, right? They're",
      "offset": 1035.199,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "going to be able to see these",
      "offset": 1036.72,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "conversations and determine as they",
      "offset": 1037.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "approve them, you know, as they review",
      "offset": 1039.039,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "them like, hey, is this mostly getting",
      "offset": 1040.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "it right? And then there there are sort",
      "offset": 1041.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "of existing kind of seesat, you know,",
      "offset": 1043.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "level measures that you can apply to the",
      "offset": 1045.039,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "people who are on the other end of the",
      "offset": 1046.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "treatment.",
      "offset": 1047.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "10x sounds a little low. Is that because",
      "offset": 1049.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "the operators don't approving everything",
      "offset": 1050.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that comes out right now? uh so they're",
      "offset": 1052.559,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "not approving everything that comes out",
      "offset": 1054.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and I agree the 10x is kind of it's an",
      "offset": 1055.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "order of magnitude not a precise measure",
      "offset": 1056.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "right but I think in this case you'll",
      "offset": 1059.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "see a couple of cases that require",
      "offset": 1061.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "approval right and sort of why but the",
      "offset": 1062.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "approval also is very quick right so I",
      "offset": 1064.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the argument is that you probably only",
      "offset": 1066.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "see one of every 10 exchanges and when",
      "offset": 1068.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you see it it takes you roughly as long",
      "offset": 1070.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "as it took the last time to just push",
      "offset": 1072.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "the button right which was the thing",
      "offset": 1073.6,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "they were already doing so that's kind",
      "offset": 1075.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of why we've benchmarked it there",
      "offset": 1076.16,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "all right um so let's get into it a",
      "offset": 1079.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "little bit So, uh, this is just a",
      "offset": 1082.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "snapshot of of what this looks like in",
      "offset": 1084.559,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Langraph. I'll show you the real thing",
      "offset": 1086,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "in just a minute, and it's actually",
      "offset": 1087.039,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "evolved a tiny bit since I took this",
      "offset": 1088,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "picture. Um, but really what we're",
      "offset": 1089.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "talking about here is, um, the the the",
      "offset": 1091.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "people who operate the system today, we",
      "offset": 1093.679,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "call them operations associates. So,",
      "offset": 1094.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "what this is really doing is introducing",
      "offset": 1096.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "a virtual operations associate. that",
      "offset": 1098.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "operations associate is going to assess",
      "offset": 1100.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the state of essentially a conversation",
      "offset": 1103.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "interaction with a patient. Um determine",
      "offset": 1105.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "what the best response is both in terms",
      "offset": 1108,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "of uh the text message you might send um",
      "offset": 1110.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the questions you might ask the actions",
      "offset": 1112.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "you might take because some of this is",
      "offset": 1114.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "about maintaining essentially a state",
      "offset": 1115.679,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "for that patient, right? you know, you",
      "offset": 1117.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "are you are at any given point trying to",
      "offset": 1119.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "figure out um when is this person taking",
      "offset": 1120.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "their medicine, when did they take their",
      "offset": 1123.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "medicine, um you know, what medicine do",
      "offset": 1124.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "they have? Um what time is it for them,",
      "offset": 1126.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "which is actually more important than",
      "offset": 1128.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "than you may think. Um all of this has",
      "offset": 1130,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to be maintained, right, by the system.",
      "offset": 1131.919,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "And so the virtual lawyer is doing all",
      "offset": 1133.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of that work and then it's passing",
      "offset": 1135.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "essentially its proposal, right? It it",
      "offset": 1137.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "basically comes up with I think this is",
      "offset": 1139.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "what we should do and it passes it to an",
      "offset": 1140.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "evaluator agent. There's a live LLM as a",
      "offset": 1142.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "judge process separate from the evals",
      "offset": 1145.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "which which we'll get to. But the live",
      "offset": 1147.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "LLM as a judge is essentially saying,",
      "offset": 1149.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "okay, given this thing that just",
      "offset": 1151.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "happened. Um here is our assessment of a",
      "offset": 1152.24,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "you know how right the LLM thinks it is.",
      "offset": 1155.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Um that's frankly very challenging. LLMs",
      "offset": 1157.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "are very hard to convince that they're",
      "offset": 1160,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "wrong about anything. But um it also is",
      "offset": 1161.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "looking at the complexity, right? So",
      "offset": 1163.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "even if the LM believes it's made all",
      "offset": 1165.12,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "the right decisions, you can have it",
      "offset": 1166.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "impartially say, &quot;Well, I changed this",
      "offset": 1167.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and I changed that and I'm scheduling a",
      "offset": 1169.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "bunch of messages. That's complicated.",
      "offset": 1171.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "maybe a human should look at this,",
      "offset": 1172.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right? So that's actually a lot easier",
      "offset": 1174.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "to implement. Um, and both of these",
      "offset": 1176.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "things are calling tools. The tools are",
      "offset": 1178.799,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "a mix of MCP. Um, and so there there's",
      "offset": 1181.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sort of two versions of MCP here. I'm",
      "offset": 1184.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "going to show you one which is basically",
      "offset": 1185.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "just looking at local files just so I",
      "offset": 1187.12,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "can show you all the stuff in my",
      "offset": 1189.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "environment. Um, but there's also MCP",
      "offset": 1189.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "going across the wire to the the larger",
      "offset": 1192.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "software system and keeping all this",
      "offset": 1194.08,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "stuff in a database, right? So there's",
      "offset": 1195.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "there's a mix of those two things. Um,",
      "offset": 1196.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and the rest of the tools are about",
      "offset": 1198.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "maintaining state because as a",
      "offset": 1200.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "conversation is happening, the the LLM",
      "offset": 1203.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "needs to know, you know, essentially,",
      "offset": 1205.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "well, I made this update and that update",
      "offset": 1206.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and here's the current state that I'm",
      "offset": 1208.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "working with and it has to be able to",
      "offset": 1209.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "sort of manipulate these things in real",
      "offset": 1211.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "time. That is not MCP. That's not going",
      "offset": 1212.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to a database anywhere. Like, this is",
      "offset": 1214.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "happening entirely in sort of the live",
      "offset": 1216.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "thread. And then once it finishes, then",
      "offset": 1217.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "it gets pushed out and and essentially",
      "offset": 1219.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "saved away.",
      "offset": 1221.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Okay. Um, again, we'll get into a lot",
      "offset": 1223.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "more of that. I did want to spend a",
      "offset": 1225.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "minute on the on the system",
      "offset": 1226.799,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "architecture, right? And so I realize",
      "offset": 1228.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "it's a little bit small. Go ahead. When",
      "offset": 1229.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you mention about the system state, I I",
      "offset": 1231.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "heard before the L has a context or some",
      "offset": 1234.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "state object.",
      "offset": 1237.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "You mention that you use tools. Are you",
      "offset": 1239.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "talking about separate things or uh",
      "offset": 1241.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "question was about how the state is",
      "offset": 1244.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "managed in Langraph. So um short answer",
      "offset": 1245.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "is this may be one of the things where",
      "offset": 1247.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "I'm not not doing it optimally by the",
      "offset": 1249.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "way but um with langraph there is a",
      "offset": 1251.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "state object that we load essentially",
      "offset": 1253.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "when the request comes in from a JSON",
      "offset": 1255.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "blob right we keep it alive inside the",
      "offset": 1258.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the graph run it is not directly",
      "offset": 1260.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "accessible to the model right the at",
      "offset": 1263.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "least not the way that we're doing it",
      "offset": 1265.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "right so you you'll see actually as we",
      "offset": 1266.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "get into this that you can see all the",
      "offset": 1268.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "state coming in in Lang Smith right I",
      "offset": 1269.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "can see like hey this is the whole thing",
      "offset": 1271.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that was was loaded I still have to",
      "offset": 1272.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "repeat that in my first message to",
      "offset": 1274.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "clawed right it doesn't actually show up",
      "offset": 1276.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "you know in the same place and then I",
      "offset": 1278.64,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "call the functions that state will",
      "offset": 1280.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "evolve in terms of what's inside the",
      "offset": 1281.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "graph run and then when it outputs it's",
      "offset": 1283.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the it's the Python code not the model",
      "offset": 1285.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "which essentially takes all that state",
      "offset": 1287.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then uh serializes it and sends it",
      "offset": 1288.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "out. Um so you you'll see how it works",
      "offset": 1291.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but like I that's generally one of the",
      "offset": 1293.44,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "things that I'm not sure I'm doing",
      "offset": 1294.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "right.",
      "offset": 1295.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Anything else? Yep. Yeah. Somewhat",
      "offset": 1297.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "related. You have one node for that",
      "offset": 1299.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "virtual going back and forth tools right",
      "offset": 1301.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "now. Yeah. I'm assuming the reason you",
      "offset": 1303.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "haven't forcoded out that business logic",
      "offset": 1306.159,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "more into separate nodes is you'll lose",
      "offset": 1309.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the workflow for the next time. Is that",
      "offset": 1312.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "sort of the notion there? Yeah. So the",
      "offset": 1316,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "question is why the essentially the",
      "offset": 1317.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "virtual A is one one agent not you know",
      "offset": 1319.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a sort of a a precoded sort of version",
      "offset": 1321.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of here's how I administer the specific",
      "offset": 1323.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "treatment. Yes. the the reason I think",
      "offset": 1325.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we kept it simple is because we did not",
      "offset": 1327.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "want to be super treatment specific in",
      "offset": 1329.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "how the architecture worked. But you",
      "offset": 1330.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "could imagine doing, you know, a set of",
      "offset": 1332.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "slightly smaller, you know, better tuned",
      "offset": 1334.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "agents that were, you know, kind of",
      "offset": 1336.64,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "taking care of elements of the task that",
      "offset": 1338.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "was still pretty generic. The main",
      "offset": 1339.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "reason I think it's not optimal to do",
      "offset": 1341.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "that is is caching. Um, and this is",
      "offset": 1344.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "another question where, um, you know, I",
      "offset": 1346.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "I think I'm doing this right, but there",
      "offset": 1347.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "are a lot of variations here. um caching",
      "offset": 1349.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the entire message stream is easier with",
      "offset": 1352,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "with either one agent or with sort of",
      "offset": 1354.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "one agent doing most of the work. Um",
      "offset": 1355.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we're using Claude. Claude has very",
      "offset": 1357.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "explicit caching mechanisms. Um and",
      "offset": 1359.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "every time I switch the system prompt, I",
      "offset": 1362.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "think the cache blows up. And so",
      "offset": 1363.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "fundamentally changing the agent",
      "offset": 1365.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "identity does that. So that was one",
      "offset": 1367.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that's one reason we chose that. It's",
      "offset": 1369.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "it's certainly not you know a hard and",
      "offset": 1370.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "fast forever choice.",
      "offset": 1372.4,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "What's the uh duration of the uh like we",
      "offset": 1375.28,
      "duration": 8.639
    },
    {
      "lang": "en",
      "text": "talking like months or so like how many",
      "offset": 1380.4,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "messages? Yeah. So uh this use case the",
      "offset": 1383.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "early pregnancy loss um it tends to be a",
      "offset": 1386.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "treatment which takes I think three days",
      "offset": 1389.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "end to end to administer most of the",
      "offset": 1391.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "time and then there's a check-in after",
      "offset": 1392.799,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "that right so imagine that probably",
      "offset": 1394.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "within a week the entire interaction",
      "offset": 1395.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "with that patient is done unless they",
      "offset": 1397.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "come back and just have questions later",
      "offset": 1399.039,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "on right you know there there are some",
      "offset": 1400.559,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "variants of this where you take a",
      "offset": 1401.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "pregnancy test after six weeks right and",
      "offset": 1403.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and so that's all fine um the message",
      "offset": 1405.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "history is preserved but the computation",
      "offset": 1408.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "that happens to generate each message is",
      "offset": 1410.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "not or at least not in not in sort of",
      "offset": 1412.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the state that we behave. So like, you",
      "offset": 1414.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "know, the most complicated conversation",
      "offset": 1416.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "I've seen was something like 150 texts.",
      "offset": 1418.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "It's a lot in terms of, you know, a",
      "offset": 1420.799,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "human keeping it in their brain. It's",
      "offset": 1422.559,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "not that bad for an LM, right? So, but",
      "offset": 1423.679,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "it's it's that level.",
      "offset": 1425.12,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "All right. Um, so again, just to point",
      "offset": 1429.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "out where the lines are here, right? So,",
      "offset": 1431.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "as I kind of got off on a tangent, the",
      "offset": 1433.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "top box is what we're going to be",
      "offset": 1435.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "looking at here today, right? It's",
      "offset": 1436.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "really a Python container with access",
      "offset": 1438.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "locally to these blueprints, this",
      "offset": 1440.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "knowledge base, right? We are also then",
      "offset": 1442.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "maintaining some stuff over across the",
      "offset": 1444.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "wire in this blue container. That's",
      "offset": 1446.159,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "really where the the dashboard I'm going",
      "offset": 1447.44,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "to show you is. It's where the text",
      "offset": 1448.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "message gateway is. Um and it is where",
      "offset": 1449.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we're going to be moving I think a lot",
      "offset": 1451.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of that context, right? The blueprints",
      "offset": 1452.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "like all that stuff really should live",
      "offset": 1454.64,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "kind of in the more durable software",
      "offset": 1456.08,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "container. Right now it lives, you know,",
      "offset": 1457.279,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "close to to the Python.",
      "offset": 1458.64,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Okay. Um so let's get into it. Um so the",
      "offset": 1460.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "first thing I'll do here is just to show",
      "offset": 1465.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you uh kind of at a high level what the",
      "offset": 1466.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "software looks like. So um this again is",
      "offset": 1468.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "uh the the the console the dashboard",
      "offset": 1471.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "right the thing that that the operations",
      "offset": 1474.159,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "associates the humans are going to be",
      "offset": 1475.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "looking at um and I'll show a couple",
      "offset": 1477.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "things here just to to give you the the",
      "offset": 1479.039,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "sort of baseline right so the first",
      "offset": 1480.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "thing here is this needs attention so",
      "offset": 1482.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the current system basically has this",
      "offset": 1483.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "needs attention flashing all the time",
      "offset": 1485.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "every time a text message comes in from",
      "offset": 1487.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "any patient this thing is going off",
      "offset": 1489.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right you know so there and there's you",
      "offset": 1491.2,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "know hundreds of patients thousands of",
      "offset": 1492.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "patients in the system at any time so",
      "offset": 1493.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you know this needs attention used to be",
      "offset": 1495.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "something that multiple people were",
      "offset": 1497.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "having to stare at constantly, right?",
      "offset": 1499.039,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "Just to make sure that they caught",
      "offset": 1500.64,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "everything so that they got out messages",
      "offset": 1501.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in a in a reasonable time. Now, needs",
      "offset": 1502.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "attention is really, you know, just sort",
      "offset": 1505.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "of one thing at a time, right? And if I",
      "offset": 1507.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "look here at the conversations, there we",
      "offset": 1508.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "go. Um, you can see that the top one",
      "offset": 1510.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "here actually needs a response. I'll get",
      "offset": 1512.159,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "to that in a minute. But at any given",
      "offset": 1513.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "point, right, this is my test",
      "offset": 1515.039,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "environment. You know, I've got a",
      "offset": 1516.08,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "handful of these conversations kind of",
      "offset": 1517.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "already already queued up. What I can",
      "offset": 1518.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "see here, if I click into these things,",
      "offset": 1520.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is essentially I'll just go back to the",
      "offset": 1522.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "beginning here for the the whole message",
      "offset": 1524.4,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "history, and I'm going to toggle this",
      "offset": 1525.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "rationale on. Um, what you're seeing is",
      "offset": 1526.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "the entire conversation. Is that",
      "offset": 1530.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "readable? So, I blow it up a little bit.",
      "offset": 1532.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Is that a little better? Okay. Um, so",
      "offset": 1534.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "the idea here is, uh, the agent is named",
      "offset": 1538.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Ava, right? That's the personality that",
      "offset": 1539.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "people are interacting with. Um, this",
      "offset": 1541.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "language is all coming out of these",
      "offset": 1544.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "blueprints, right, that I'll show you.",
      "offset": 1546.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "And so this first message is just an",
      "offset": 1547.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "initial message sent by the system",
      "offset": 1549.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "essentially just to kick things off. So",
      "offset": 1550.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "imagine someone is they have a package",
      "offset": 1552.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "of medicine in their hand. They scan a",
      "offset": 1554.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "QR code. They put in their phone number,",
      "offset": 1555.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "they get this text message, right? And",
      "offset": 1557.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then they start talking. Um, so you can",
      "offset": 1559.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "see here the kinds of things a patient",
      "offset": 1561.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "is going to say are, you know, free form",
      "offset": 1563.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "text, right? You know, this I mean they",
      "offset": 1565.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "could say yes in any number of ways. The",
      "offset": 1567.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "old system used to have literally",
      "offset": 1569.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "different buttons for yes, like yes, I",
      "offset": 1571.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "have the medicine. Yes, I heard you. I",
      "offset": 1574.159,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "mean, it's like there's all sorts of",
      "offset": 1576,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "variants, right? And because you you did",
      "offset": 1577.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "have to respond differently depending on",
      "offset": 1578.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "what those things were. What we're able",
      "offset": 1579.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to do here is really just take, you",
      "offset": 1581.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "know, these free form answers, interpret",
      "offset": 1583.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "them, and then essentially provide a",
      "offset": 1586.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "rationale for why you would say a given",
      "offset": 1588.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "thing at a given time, right? So, this",
      "offset": 1590.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "is equivalent to if you were doing this",
      "offset": 1591.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "with a human and you ask the human,",
      "offset": 1593.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "well, why did you say this? The LM can",
      "offset": 1594.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "provide this kind of of context. So,",
      "offset": 1596.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this is Claude looking at the history",
      "offset": 1598.159,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "here, and I I'll show you what this",
      "offset": 1600.08,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "looks like in Langmith, which will make",
      "offset": 1601.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "it a lot more obvious. And then saying,",
      "offset": 1602.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "okay, here here's the next thing that I",
      "offset": 1604.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "should say. And my confidence that I",
      "offset": 1606.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "should say it is 100%. Right? It's it's",
      "offset": 1607.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "usually very confident, right? But but",
      "offset": 1609.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the point is this this whole process is",
      "offset": 1612,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "largely going to go along in an",
      "offset": 1614.88,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "automated fashion, right? You don't",
      "offset": 1616.24,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "usually need humans involved because",
      "offset": 1617.279,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this is a very straightforward thing.",
      "offset": 1618.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "They have their medicine. The next thing",
      "offset": 1620.159,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "I need to know, and this is a very",
      "offset": 1622,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "interesting part of this treatment, I",
      "offset": 1623.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "need to know what time it is. These are",
      "offset": 1624.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "text messages. We don't know anything",
      "offset": 1626.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "about these people for a variety of",
      "offset": 1627.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "reasons. It's kind of good that we don't",
      "offset": 1629.279,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "know much about them, right? We don't",
      "offset": 1630.32,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "want to have to deal with all of the",
      "offset": 1631.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "stuff around provider confidentiality",
      "offset": 1632.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and and patient data, right? So, one of",
      "offset": 1634.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the things that we need if we're going",
      "offset": 1636.24,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "to go through this longitudinal",
      "offset": 1637.2,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "treatment is to figure out what time it",
      "offset": 1638.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is for them and then essentially pull",
      "offset": 1639.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "out that data and figure out what their",
      "offset": 1642,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "local time is. Right? So, in this case,",
      "offset": 1643.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "I was in Eastern time when I answered",
      "offset": 1645.6,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "these questions. This is all me doing",
      "offset": 1647.039,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "this, you know, from my laptop. Um, I",
      "offset": 1648.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "tell it what time it is. It calculates",
      "offset": 1650.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "an offset from UTC and says, &quot;Well, I",
      "offset": 1651.919,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "guess you're in Eastern time, right?&quot;",
      "offset": 1653.679,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "And then it sets this over here and it",
      "offset": 1654.88,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "says, &quot;All right, from now on, I know",
      "offset": 1656.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "that my patient is in Eastern time",
      "offset": 1657.6,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "unless they tell me otherwise.&quot; And they",
      "offset": 1658.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "could come back and tell you otherwise,",
      "offset": 1660.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "right? That's something the old system",
      "offset": 1662.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "really didn't have a good way to do. Um,",
      "offset": 1663.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "but if the patient comes back and says,",
      "offset": 1665.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "&quot;I'm on a plane. It's actually seven for",
      "offset": 1666.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "me.&quot; We just update the time zone and",
      "offset": 1668.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "move on. Right? This is a very flexible",
      "offset": 1669.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "system that way. Um, then we get into",
      "offset": 1671.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this over here and we say, &quot;Okay, now",
      "offset": 1674.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that I know what time it is, I'm going",
      "offset": 1676.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to ask them if they've started their",
      "offset": 1678.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "treatment, right?&quot; And, you know, there",
      "offset": 1679.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "is a blueprint, right, which we'll get",
      "offset": 1681.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to, you know, that essentially just has,",
      "offset": 1682.799,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "you know, the the medicine that they're",
      "offset": 1684.48,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "going to take in a very specific way to",
      "offset": 1685.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "take it, right? The the protocol. Um,",
      "offset": 1687.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the patient says, &quot;Well, no, I want to",
      "offset": 1689.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "take it soon.&quot; You know, the Ava says,",
      "offset": 1691.039,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "&quot;Cool. I'll text you when we're ready.&quot;",
      "offset": 1692.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "And then it gives you know a regimen",
      "offset": 1694.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "which in this case this is an SVG that",
      "offset": 1695.679,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we are stapling times and dates on top",
      "offset": 1698,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "of right so you know fairly",
      "offset": 1700.399,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "straightforward we're doing this in",
      "offset": 1701.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "software the LM's not doing it the LM is",
      "offset": 1702.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "actually just passing along the",
      "offset": 1704.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "instructions you know it says send the",
      "offset": 1706.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "step one image and provide you know like",
      "offset": 1708.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "this date and this time and and we",
      "offset": 1710.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "substitute the rest of it in and this",
      "offset": 1712.399,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "goes out as an MMS right so this is this",
      "offset": 1713.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "is a text message um and so we provide",
      "offset": 1715.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "this the patient you know says you know",
      "offset": 1718.559,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "in this case we're we're talking you",
      "offset": 1720.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "know again this is all the LM reasoning",
      "offset": 1721.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "through this, right? You know, I I I am",
      "offset": 1723.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "sending this immediately because it's",
      "offset": 1726.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "actually within sort of the 35 minute",
      "offset": 1727.679,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "window that you've told me that I have",
      "offset": 1729.279,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "to send these things. This is all",
      "offset": 1730.559,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "business logic that the LLM is is",
      "offset": 1731.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "interpreting pretty much on the fly. Um,",
      "offset": 1733.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and then I have these reminders, right?",
      "offset": 1736.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "I didn't get back to it. So, this is an",
      "offset": 1738.159,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "important part. It sent me this thing",
      "offset": 1739.679,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "and it thought that I was going to take",
      "offset": 1741.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "it at 5:45. I didn't text it back,",
      "offset": 1742.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "right? This is partly because I was m",
      "offset": 1744.72,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "maintaining the system myself and I",
      "offset": 1746.159,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "forgot. So, I had to come back in the",
      "offset": 1747.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "next day and catch up. Um, so it sent me",
      "offset": 1748.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "an automated reminder because it",
      "offset": 1750.64,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "scheduled one when it sent the first",
      "offset": 1751.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "message. So part of this is the LM only",
      "offset": 1753.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "gets called when the patient says",
      "offset": 1755.679,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "anything. So if they don't, you know,",
      "offset": 1757.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "you have to make sure that you stay",
      "offset": 1759.279,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "engaged, right? You don't do this overly",
      "offset": 1760.48,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "like we don't try to bother people",
      "offset": 1762,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "beyond one or two reminders. It's their",
      "offset": 1763.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "treatment. Um, but this bump sort of",
      "offset": 1764.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "functionality was really important to",
      "offset": 1767.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "the client, right? So we built it in.",
      "offset": 1768.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Um, so you can see here I came back the",
      "offset": 1770.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "next day and I said, &quot;Yep, sorry. I I",
      "offset": 1772.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "did take it.&quot; You know, Ava confirms",
      "offset": 1774.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that I completed step one. And what it",
      "offset": 1776.159,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "does is it sets this thing called an",
      "offset": 1777.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "anchor, right? And it says, okay, you",
      "offset": 1778.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "know, the patient was going to take it",
      "offset": 1781.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "at 5:45, they confirmed that they did.",
      "offset": 1782.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And so now, you know, I can refer back",
      "offset": 1784.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "to this. I know that this happened,",
      "offset": 1786.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "right? And if the patient had then said,",
      "offset": 1787.679,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "oh no, I screwed up. I actually haven't",
      "offset": 1789.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "taken. I'll take it today. We just",
      "offset": 1790.559,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "change the anchor. We update everything.",
      "offset": 1792.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "Right? So this is a system that humans",
      "offset": 1793.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "used to have to do. If a patient came",
      "offset": 1795.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "back and said, I didn't take my",
      "offset": 1797.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "medicine, you know, a human has to go in",
      "offset": 1798.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and manually update all the times and",
      "offset": 1800.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "all the scheduled messages. And it was",
      "offset": 1802.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it was a big pain in the butt. Um, yeah,",
      "offset": 1804.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "please.",
      "offset": 1806.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "implement this functionality where",
      "offset": 1807.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "patient report",
      "offset": 1809.12,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "state",
      "offset": 1815.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "not exactly um the way that we do state",
      "offset": 1817.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and I'll I'll spend a lot of time on",
      "offset": 1820,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "this but the way that we do state is",
      "offset": 1821.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "really just that with any given message",
      "offset": 1822.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "from the patient right this entire",
      "offset": 1824.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "system only kicks off when the patient",
      "offset": 1826.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "sends a message um what we do is we say",
      "offset": 1827.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "all right given this state what is the",
      "offset": 1831.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "best response and that response could be",
      "offset": 1833.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "I changed some of these anchors. I",
      "offset": 1835.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "update their treatment phase. I I",
      "offset": 1837.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "schedule a bunch of messages. All that",
      "offset": 1839.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "state is preserved so that the next time",
      "offset": 1841.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "they write in, then you know, we have",
      "offset": 1843.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that state to go on. Um, but again,",
      "offset": 1844.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we're not checking, right? There's no",
      "offset": 1846.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "polling going on in the system where",
      "offset": 1848.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we're saying after 3 hours, did the",
      "offset": 1849.6,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "patient text me back? We we don't do",
      "offset": 1851.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "that. We depend on the scheduled",
      "offset": 1852.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "messages essentially just to nudge the",
      "offset": 1854.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "patient. Um, if they choose to not say",
      "offset": 1856.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "anything for three days and they come",
      "offset": 1858.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "back after three days, we just pick up",
      "offset": 1859.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "where we left off. Um, again, this is a",
      "offset": 1861.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "choice. This is the way the client wants",
      "offset": 1863.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it. It's it's intended to be low enough",
      "offset": 1864.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "touch that it doesn't bother people, but",
      "offset": 1866.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "high enough touch that it doesn't lose",
      "offset": 1868.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "track.",
      "offset": 1869.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Sure. Um I'll pause here actually. Any",
      "offset": 1871.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "other questions so far? Um I I realize",
      "offset": 1873.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I'm going through a lot. Yes. Do your",
      "offset": 1875.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "anchors have to be sequential or can",
      "offset": 1877.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "your user come in at any point?",
      "offset": 1879.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "They can. Great question. So the",
      "offset": 1882.64,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "question was do the anchors have to be",
      "offset": 1884,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "sequential? Um or like do you have to go",
      "offset": 1885.2,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "through these one step at a time? So,",
      "offset": 1886.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "one of the great things, one of the best",
      "offset": 1888.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "things about this system is that I could",
      "offset": 1889.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have and and I'm happy to try this when",
      "offset": 1891.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we go a little bit later. I could have",
      "offset": 1893.52,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "basically said, &quot;Oh, yeah. I already",
      "offset": 1894.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "took the first pill and I'm like in the",
      "offset": 1896.08,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "middle of taking the second pill, you",
      "offset": 1897.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "know, as like the first thing I say to",
      "offset": 1898.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "to Ava and she would be like, &quot;Okay,",
      "offset": 1900,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "cool. There's an anchor. Here's the next",
      "offset": 1902.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "thing.&quot; It skips ahead and it doesn't",
      "offset": 1903.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "force you to go through this",
      "offset": 1905.2,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "prescriptive part of the blueprint,",
      "offset": 1906.159,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "whereas the old system, you know, at",
      "offset": 1907.519,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "least nominally did, right? Like you",
      "offset": 1908.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "could you could kind of skip ahead, but",
      "offset": 1910.159,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this automatically does it. You know,",
      "offset": 1911.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "part of the instructions are don't ask",
      "offset": 1913.039,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the patient a question they've already",
      "offset": 1914.799,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "answered. Like, period, right? But",
      "offset": 1915.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that's annoying. Don't do that. Um, so",
      "offset": 1917.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "so yes, that's that's very much in",
      "offset": 1919.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "there.",
      "offset": 1921.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Anything else? Yeah. Does the",
      "offset": 1922.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "concept of the internal state machine",
      "offset": 1925.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that is kind of determining all of this",
      "offset": 1927.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "or is that kind of outsourced to the",
      "offset": 1929.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "actual software?",
      "offset": 1932.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Yeah. So question is, does the LM have",
      "offset": 1935.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "an internal representation of the state?",
      "offset": 1937.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Um, kind of sort of. So you'll you'll",
      "offset": 1938.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "see um when we get into the the the",
      "offset": 1940.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "actual back and forth with Claude um in",
      "offset": 1942.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Langmith you as a human can see kind of",
      "offset": 1945.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "where it starts right so every thread is",
      "offset": 1947.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "going to show like all right here's the",
      "offset": 1949.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "incoming state we repeat it essentially",
      "offset": 1950.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to claude again that's just one dot I've",
      "offset": 1953.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "never managed to connect with lang graph",
      "offset": 1954.96,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "right so we basically have to serialize",
      "offset": 1956.399,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "the state and say this is your this is",
      "offset": 1957.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "your starting point but then the LM has",
      "offset": 1959.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "that in its window and then you know",
      "offset": 1961.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it's going to cause changes to the state",
      "offset": 1963.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it'll call functions that update the",
      "offset": 1965.519,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "state it can always ask again it can say",
      "offset": 1966.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "well what's the current state you I can",
      "offset": 1968.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "go back and and retrieve it. Um, but in",
      "offset": 1970,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the context of that one from when the",
      "offset": 1972,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "patient responded, you know, to when I",
      "offset": 1974.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "actually come up with my response to",
      "offset": 1976,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "them, that whole thing is going to be in",
      "offset": 1977.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "its memory at one moment.",
      "offset": 1978.88,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "Did you ever run into issues with state?",
      "offset": 1981.76,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Um, so the the short answer is the",
      "offset": 1985.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "question was um, do we ever run into the",
      "offset": 1988.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "state being too big? uh generally",
      "offset": 1990,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "speaking because of the way that we're",
      "offset": 1992.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "kind of compressing and serializing at",
      "offset": 1993.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the end of the conversations it doesn't",
      "offset": 1995.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "ever get so big that it can't finish its",
      "offset": 1997.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "job of responding to one situation right",
      "offset": 1999.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "you know like patient said this now I'm",
      "offset": 2001.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "going to do this we have considered",
      "offset": 2003.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "having longer running threads where you",
      "offset": 2006.399,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "kind of pick up in the middle and you've",
      "offset": 2007.76,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "already you can reload sort of the",
      "offset": 2008.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "entire previous conversation that does",
      "offset": 2010.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "get weird right especially with older",
      "offset": 2012.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "clouds you would get it forgetting to",
      "offset": 2013.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "sort of call tools the right way and it",
      "offset": 2015.84,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "have all sorts of JSON errors right we",
      "offset": 2017.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "have a bunch of retry logic in there to",
      "offset": 2018.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "kind of compensate for that. Um, so",
      "offset": 2020.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that's one reason we kept it short. We",
      "offset": 2021.919,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "make it so that we basically throw",
      "offset": 2023.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "everything out and restart when the",
      "offset": 2025.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "patient gets back to us. In part because",
      "offset": 2027.279,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "blueprints could change, right? You a",
      "offset": 2029.279,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "bunch of things could could change in",
      "offset": 2030.799,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "the meantime that might end up with with",
      "offset": 2031.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "weird states.",
      "offset": 2033.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So on the management of states and",
      "offset": 2035.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "taking decisions what to do next. Yeah.",
      "offset": 2037.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "So this is 100% LLM driven or there's",
      "offset": 2039.76,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "some softer like logic around it as",
      "offset": 2042.559,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "well. It's 100% LLM driven. Uh sorry the",
      "offset": 2046.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "question was uh is the is the steering",
      "offset": 2049.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "done by software right any any of that",
      "offset": 2051.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "steering the answer is really no it's",
      "offset": 2053.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "not um except for when it surfaces to a",
      "offset": 2054.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "human right and so when it goes to a",
      "offset": 2057.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "human for approval the human can use",
      "offset": 2058.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "English and basically say yeah change",
      "offset": 2061.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that word to that and that message",
      "offset": 2063.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "shouldn't go out and you know whatever",
      "offset": 2064.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so like we we actually as part of the",
      "offset": 2066.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "flexibility part we are not building any",
      "offset": 2068.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "software that manages the state we just",
      "offset": 2070.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "want you to talk to the LM to do it",
      "offset": 2073.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "right we think that's a better practice",
      "offset": 2075.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "right it means like you know you as a",
      "offset": 2076.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "human just have to talk to it and you",
      "offset": 2078,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "don't have to figure out how to flip all",
      "offset": 2079.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the bits on this new console.",
      "offset": 2080.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Do you have any rack system? And second,",
      "offset": 2084.079,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "if a patient going off journey, how do",
      "offset": 2087.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you detect that?",
      "offset": 2090.159,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I'm sorry, what was the first question?",
      "offset": 2092.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Do you have any rack system",
      "offset": 2093.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "or Rex? I'm sorry, I just understand.",
      "offset": 2096.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Retrieval. Oh. Oh, got it. Sorry. So,",
      "offset": 2099.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "um, question was, is there a rag? Uh,",
      "offset": 2102.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "no, there's not. And and it's actually",
      "offset": 2104.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just because what we really did is we",
      "offset": 2106.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "just came up with a structure for the",
      "offset": 2108.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "documents that was self-reerential. So",
      "offset": 2109.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you read a very small document which",
      "offset": 2112.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "says here's the treatment, right? If you",
      "offset": 2113.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "need to read for this phase, go to this",
      "offset": 2115.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "file, right? If you need for this phase,",
      "offset": 2116.72,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "go to this file. If you have a question",
      "offset": 2118,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "that doesn't fall underneath any of",
      "offset": 2119.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "those things, here's a CSV with a bunch",
      "offset": 2120.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of questions and answers. We didn't do",
      "offset": 2122.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it as rag in part because we didn't",
      "offset": 2124.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "believe that e either we could do a",
      "offset": 2127.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "really good job of getting all the right",
      "offset": 2129.359,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "information into the window. Like we",
      "offset": 2130.56,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "didn't think we'd be reliable enough",
      "offset": 2131.68,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "about that. We just want to give the",
      "offset": 2132.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "entire document. They're not that big.",
      "offset": 2133.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Um, and because these this is clawed,",
      "offset": 2136.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "right? It's it's got a big enough window",
      "offset": 2138.24,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "that we could just put the entire thing",
      "offset": 2139.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "in there, you know, for for most",
      "offset": 2140.72,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "treatments. So, we chose to do that.",
      "offset": 2141.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "What was your second question, though?",
      "offset": 2143.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Is the patient going off a typical",
      "offset": 2144.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "journey? Yeah. How do you detect and",
      "offset": 2146.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "intercept? Right. So, the question is if",
      "offset": 2149.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the patient goes off track, so we we",
      "offset": 2151.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "have this idea of a blueprint, but then",
      "offset": 2153.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "there are plenty of cases where the",
      "offset": 2155.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "blueprint may um you know, not fully",
      "offset": 2156.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "answer whatever the patient is is is",
      "offset": 2159.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "bringing up. Um like one example is the",
      "offset": 2160.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "blueprint is very much about asking",
      "offset": 2163.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "questions right so you will say have you",
      "offset": 2164.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "taken your medicine yet when do you plan",
      "offset": 2167.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to take your medicine the patient will",
      "offset": 2168.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "say my stomach hurts okay so yes your",
      "offset": 2170.56,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "stomach hurts you didn't answer the",
      "offset": 2174.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "question what we do is the patient",
      "offset": 2175.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "typically will get an answer to their",
      "offset": 2177.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "question so one of the principles is",
      "offset": 2179.359,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "always answer the patient's question",
      "offset": 2180.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "right we don't ever want to leave them",
      "offset": 2182.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "hanging but then ask yours again so the",
      "offset": 2183.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "idea is that at any given point we can",
      "offset": 2186.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "answer anything that they need and as",
      "offset": 2188,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "gently as we can we'll try to pull them",
      "offset": 2189.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "back onto the blueprint so that we",
      "offset": 2191.119,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "understand where they are in the",
      "offset": 2192.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "treatment. Um, it's in exact science.",
      "offset": 2193.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "But, uh, is there a way to detect if",
      "offset": 2196.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "someone",
      "offset": 2199.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Well, the LM does that effectively by",
      "offset": 2202.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "knowing that it's supposed to keep",
      "offset": 2204.72,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "people on the blueprint, but having an",
      "offset": 2205.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "escape hatch for the knowledge base,",
      "offset": 2207.28,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "essentially what we call it, right?",
      "offset": 2208.8,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Triage or knowledge base, you know,",
      "offset": 2209.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "whatever you want to call it. Um, so,",
      "offset": 2211.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you know, we we don't have an explicit",
      "offset": 2212.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "bit sort of flipped in the system that",
      "offset": 2215.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "will say this patient is off track. We",
      "offset": 2217.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "just kind of know roughly where they are",
      "offset": 2219.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "in the treatment and if they want to",
      "offset": 2220.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "answer if they want to ask a bunch of",
      "offset": 2222.32,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "questions we we'll just answer them",
      "offset": 2223.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "until they they are satisfied.",
      "offset": 2224.48,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "Okay. Uh yeah.",
      "offset": 2227.44,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2234.56,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2240,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So question is why did we choose",
      "offset": 2242.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "langchain and would we still um I I will",
      "offset": 2243.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "be very candid that the main reason that",
      "offset": 2246.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I chose lang chain is that I had",
      "offset": 2248.56,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "personally gotten pretty comfortable",
      "offset": 2250,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "with langraph as as a a demonstration of",
      "offset": 2251.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "these concepts right it's not that crew",
      "offset": 2253.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "I mean we did a lot of autogen work back",
      "offset": 2255.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "in the earlier days right you know I've",
      "offset": 2257.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "done a little bit with crew AI all of",
      "offset": 2258.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "those frameworks can functionally do",
      "offset": 2260.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "very similar things langraph was the",
      "offset": 2261.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "absolute best at explaining to people",
      "offset": 2264.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "who were not neck deep in this stuff how",
      "offset": 2266.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it worked um and because there was a",
      "offset": 2267.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "path to production From there, I didn't",
      "offset": 2270.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "feel a need to to replplatform and",
      "offset": 2271.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "change all of it. We certainly thought",
      "offset": 2273.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "about it, right? We considered, well,",
      "offset": 2274.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "what if we didn't do this in Langraph?",
      "offset": 2275.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "What would we gain? And but the answer",
      "offset": 2277.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "is you still have to implement",
      "offset": 2279.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "observability in certain ways. You know,",
      "offset": 2280.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you don't necessarily get, you know, the",
      "offset": 2282.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "support that you might get from",
      "offset": 2283.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Langchain if you end up in a place.",
      "offset": 2284.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Remember that we're also doing this for",
      "offset": 2286.48,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "clients. We're not going to be there",
      "offset": 2287.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "forever. Um, leaving them with something",
      "offset": 2288.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that they can call, you know, somebody",
      "offset": 2290.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to to support is also a helpful aspect.",
      "offset": 2291.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "So, I think I I I don't think I do it",
      "offset": 2294.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "differently. I think it's really just",
      "offset": 2297.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that you know ultimately you know we're",
      "offset": 2298.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "getting pushed all of us in the",
      "offset": 2301.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "direction of using the native model",
      "offset": 2303.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "tools for this right you know openai has",
      "offset": 2304.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the responses API which lets you define",
      "offset": 2307.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tools cloud has its new stuff right like",
      "offset": 2308.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "I don't really want to be locked in um I",
      "offset": 2311.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "I I am to some degree locked into lang",
      "offset": 2313.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "chain now but I I prefer that honestly",
      "offset": 2315.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to being locked into the models um these",
      "offset": 2317.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "are these are not performance intensive",
      "offset": 2319.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "things we're doing in terms of the",
      "offset": 2320.88,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "software right like you know I don't",
      "offset": 2322.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "care that lang chain is sometimes a",
      "offset": 2323.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "little slow um I would rather have the",
      "offset": 2325.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "option",
      "offset": 2327.04,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "So you said you're not using",
      "offset": 2330.4,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "the scale how",
      "offset": 2333.359,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "those",
      "offset": 2337.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "uh it is just that they have sorry the",
      "offset": 2340.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "question was about um if it's not rag",
      "offset": 2341.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "how do we fetch documents the documents",
      "offset": 2343.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "refer to each other so you'll see that",
      "offset": 2345.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we have an overview MD right this is all",
      "offset": 2347.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "in markdown um there's an overview md",
      "offset": 2349.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that tells you what other documents are",
      "offset": 2352,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "involved in the treatment right there's",
      "offset": 2354.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "some of the prompting which says you can",
      "offset": 2356,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "always request a triage overview, right,",
      "offset": 2358,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to to try to handle problems. Um, and",
      "offset": 2360.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it'll be there, right, regardless of",
      "offset": 2362.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "what the treatment is. So, it it is very",
      "offset": 2364,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "much just a document management thing.",
      "offset": 2366.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "Um, rag, the main issue is just that I I",
      "offset": 2367.599,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "don't think and and you know, this will",
      "offset": 2372.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "probably be more obvious as we get into",
      "offset": 2374.079,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "it, right? I don't think that you could",
      "offset": 2375.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "really design a rag which would pull",
      "offset": 2376.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "back snippets of everything in sort of",
      "offset": 2378.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "perfectly relevant relevant ways. You",
      "offset": 2380,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "really do kind of need to understand the",
      "offset": 2382.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "shape of the whole treatment, right? to",
      "offset": 2383.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "to make a good decision, right?",
      "offset": 2384.8,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "Otherwise, you're just going to pair it",
      "offset": 2386.32,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "whatever particular snippet the rag",
      "offset": 2387.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "happened to bring back and then the",
      "offset": 2389.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "logic all has to be in the rag. It makes",
      "offset": 2390.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "more sense and it's more transparent, I",
      "offset": 2392.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "think, to do it this way. Maybe you'll",
      "offset": 2393.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "get to this in the state management",
      "offset": 2395.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "later on. Are anchors predefined in the",
      "offset": 2397.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "blueprint or are they",
      "offset": 2399.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "they are mostly predefined by the",
      "offset": 2401.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "blueprint and that we say as part of the",
      "offset": 2403.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "overview, you know, the concept of an",
      "offset": 2405.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "anchor is that it is a thing that",
      "offset": 2406.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "happened or a thing that will happen and",
      "offset": 2408,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "here are the examples for this",
      "offset": 2409.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "treatment, right? This is the thing that",
      "offset": 2410.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "will happen or did happen in this",
      "offset": 2412.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "treatment. Sorry that was questions",
      "offset": 2413.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "about the anchors. Yeah. So when you",
      "offset": 2415.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "you mentioned that you conversation and",
      "offset": 2418.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you keep track",
      "offset": 2421.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "uh no so that the state is essentially",
      "offset": 2424.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "um you know we we call it for reasons",
      "offset": 2426.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that only an engineer could love. We",
      "offset": 2429.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "call it a schedule document. Right? The",
      "offset": 2430.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "idea is that for any given patient there",
      "offset": 2431.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is a schedule that they're on and the",
      "offset": 2433.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "document snapshots their current state",
      "offset": 2435.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "at a given point. Right? And it's a",
      "offset": 2438.16,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "version database. So we could go back in",
      "offset": 2439.28,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "time and we could see what their",
      "offset": 2440.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "document was 3 days ago. Um but it has",
      "offset": 2441.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "at any given point the messages that",
      "offset": 2444.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "have been exchanged, any unscent",
      "offset": 2445.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "messages that are scheduled and enough",
      "offset": 2447.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "state about their treatment to fill out",
      "offset": 2449.68,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "this view. Yeah. Uh yeah.",
      "offset": 2451.119,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "So in this case all of this stuff is",
      "offset": 2460.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "locked away, right? So I mean just to go",
      "offset": 2462.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "back to this diagram for a second um",
      "offset": 2464,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this entire thing is all behind you know",
      "offset": 2466.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "AWS's VPC right so like there is no",
      "offset": 2468.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "external access to the LLM period the",
      "offset": 2471.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "only things it can talk to are",
      "offset": 2473.359,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "essentially its own documents you know",
      "offset": 2474.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in in local files and to the the blue",
      "offset": 2476.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "box so you know there there certainly",
      "offset": 2478.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "are vectors but the vectors would be",
      "offset": 2480.8,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "through the text messages right not",
      "offset": 2482.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "really through anything else",
      "offset": 2483.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "uh yeah oh I'm sorry bunch of people you",
      "offset": 2486.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "first question regarding I guess it's",
      "offset": 2489.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "twofold",
      "offset": 2491.76,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "is like how are you assessing the",
      "offset": 2492.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "confidence rate from the model's",
      "offset": 2493.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "response and the second is how are you",
      "offset": 2495.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "safe to get injection for malicious",
      "offset": 2496.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "behavior yeah well so uh question was",
      "offset": 2500.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "about prompt injection and generally",
      "offset": 2502.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "sort of steering um I mean the the basic",
      "offset": 2503.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "answer is just that",
      "offset": 2506.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you could definitely try to trick the",
      "offset": 2509.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "model by sending weird texts right and",
      "offset": 2510.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and we do that as part of our you know",
      "offset": 2512.319,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "sort of internal red teaming like we",
      "offset": 2513.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "have the entire team of operations",
      "offset": 2515.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "associates who have been spending you",
      "offset": 2516.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "know weeks and months trying to trick",
      "offset": 2518.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "this thing um and granted they're not",
      "offset": 2519.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "trying to trick it from a reveal",
      "offset": 2521.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "proprietary personal medical data, you",
      "offset": 2523.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "know, I mean, there there's things like",
      "offset": 2525.28,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "that. We also obscure a lot of that",
      "offset": 2526.24,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "medical data. So, the things that get",
      "offset": 2527.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "get to the yellow box do not include",
      "offset": 2529.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "phone numbers. They do not include",
      "offset": 2530.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "anything other than the patient's",
      "offset": 2532.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "identified first name. Um, so there's a",
      "offset": 2533.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "lot of there's a lot of that data that's",
      "offset": 2535.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "kept only in the blue, which is a lot",
      "offset": 2537.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "easier to to defend against. Um, so",
      "offset": 2538.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "yeah, we we we very much do obscure the",
      "offset": 2541.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the the patient. We don't obscure the",
      "offset": 2543.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "treatment, right? The treatment is fully",
      "offset": 2545.92,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "visible to the LM. Yeah. Cool. Yep.",
      "offset": 2547.119,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "Yeah. Hold that thought. I will get to",
      "offset": 2558,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that very very shortly. Um, we're back",
      "offset": 2559.52,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "there.",
      "offset": 2561.68,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Uh, sorry. It's just a question about",
      "offset": 2569.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "unclear instructions. Um so uh when when",
      "offset": 2571.04,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "the situation is ambiguous the LLM is",
      "offset": 2575.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "told to look at the blueprint and pick",
      "offset": 2578.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the best possible answer. Now if you",
      "offset": 2580.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "don't believe the LMU if you don't",
      "offset": 2583.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "believe the the answer is perfect um you",
      "offset": 2585.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "should say so right in the rationale. So",
      "offset": 2587.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "if I go back over here this idea of the",
      "offset": 2589.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "rationale if there is uncertainty on the",
      "offset": 2591.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "model's you know point of view it can",
      "offset": 2592.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "say well I picked this blueprint",
      "offset": 2594.319,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "response but I'm not sure that it's",
      "offset": 2595.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right. In practice it's not great at",
      "offset": 2596.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "doing that right but that is the idea.",
      "offset": 2598.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "And then the evaluator is also going to",
      "offset": 2600.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "look at this and say, well, did you",
      "offset": 2602.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "actually pick either the exact blueprint",
      "offset": 2603.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "response word for word? Did you adapt",
      "offset": 2605.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it? You know, does this seem right to",
      "offset": 2607.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you? Like we're trying to at least give",
      "offset": 2609.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a little bit of a layer before we get to",
      "offset": 2610.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "humans. And then hopefully we we can",
      "offset": 2612.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "trap situations like that and say, well,",
      "offset": 2614.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "this is a complicated situation. A human",
      "offset": 2616.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "should should take a look. Um, it is not",
      "offset": 2617.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "an exact science though, like that's",
      "offset": 2619.92,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "generally just true with this stuff.",
      "offset": 2620.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "Sorry, you in the back.",
      "offset": 2622,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Yeah. Um so the question was just about",
      "offset": 2634.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "load and scale. So uh the look the",
      "offset": 2635.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "really short answer is that this this",
      "offset": 2637.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "system exists right there's an existing",
      "offset": 2638.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "version of it that is humans pushing",
      "offset": 2640,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "buttons. Um that scale is you know again",
      "offset": 2641.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "let's say thousands not millions of",
      "offset": 2644.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "patients. Um this opens up the",
      "offset": 2645.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "possibility of doing more treatments",
      "offset": 2648.64,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "right? That's how we would get sort of",
      "offset": 2650.16,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "additional patient scale. You can also",
      "offset": 2651.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "sell this to new hospitals, new clinics,",
      "offset": 2652.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "things like that. Um so part of this is",
      "offset": 2654,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to get the scale to be larger. Um we",
      "offset": 2656.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "have not run into scale issues with you",
      "offset": 2658.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "know just the the conversations with",
      "offset": 2660.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "claude. You know the software that we're",
      "offset": 2662.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "building would scale much much larger",
      "offset": 2663.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "than thousands of users right you know",
      "offset": 2665.119,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the the text message gateway might",
      "offset": 2666.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "actually be the the biggest bottleneck.",
      "offset": 2667.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "So it's it's honestly it's a problem we",
      "offset": 2669.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "want to have. Um go ahead. So um you",
      "offset": 2671.599,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "keep saying did you guys select thems",
      "offset": 2674.64,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "or was there like a specific reason why",
      "offset": 2679.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "you're going with 35 or whatever you",
      "offset": 2681.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah. Uh so questions of model selection",
      "offset": 2683.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "um when we started this right and and I",
      "offset": 2685.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "think you know let's assume that we",
      "offset": 2687.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "kicked this project off you know late",
      "offset": 2688.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "last year early this year right um we",
      "offset": 2690.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "had to make a choice and our main",
      "offset": 2693.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "criteria were it had to be a steerable",
      "offset": 2694.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "model that we felt pretty good about you",
      "offset": 2696.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know transparency wise um you know one",
      "offset": 2698.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "example just just to give you a specific",
      "offset": 2700.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "one mini is pretty good at this workflow",
      "offset": 2702.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but it won't show its reasoning um like",
      "offset": 2704.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I mean that's just one example and like",
      "offset": 2707.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "it's not a dealbreaker like we can still",
      "offset": 2708.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "see the rationale like there's some",
      "offset": 2709.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "pieces of it but I like being able to go",
      "offset": 2711.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "into lang seeing the whole conversation,",
      "offset": 2712.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "right? That that really helps me out.",
      "offset": 2714.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Um, we needed, you know, again, flexible",
      "offset": 2715.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "hosting, but I mean, all the clouds kind",
      "offset": 2717.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of do that. Frankly, we didn't want to",
      "offset": 2719.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "deal with Microsoft and we kind of",
      "offset": 2721.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "preferred AWS to Google. That that was",
      "offset": 2722.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "kind of how we got there. But, you know,",
      "offset": 2724.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "you can do this anywhere. It really was",
      "offset": 2726.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "just we had to pick a horse and we",
      "offset": 2728.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "largely have not regretted it and in",
      "offset": 2730.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "part because we built enough flexibility",
      "offset": 2731.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "where if I want to switch, I I still",
      "offset": 2733.44,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "can.",
      "offset": 2734.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2741.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Yeah. Uh so the question was about",
      "offset": 2758.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "sensitivity of data through the text",
      "offset": 2759.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "carriers and also about uh using the",
      "offset": 2761.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "data to learn. Um I'll do the learning",
      "offset": 2763.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "first. um we don't we we do not take any",
      "offset": 2765.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "of the responses and and do anything to",
      "offset": 2768.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the models other than when we see",
      "offset": 2769.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "situations that we as humans have",
      "offset": 2772,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "evaluated and found wanting um we can",
      "offset": 2773.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "tweak the prompting and the guidelines",
      "offset": 2775.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "right but we are not putting this in any",
      "offset": 2777.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "sort of durable form like ultimately you",
      "offset": 2778.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "know we believe the right model here is",
      "offset": 2781.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the provider interaction if there's a",
      "offset": 2783.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "provider involved that sticks around",
      "offset": 2784.8,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "right the provider knows that you",
      "offset": 2786.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "interact with the system they can have",
      "offset": 2787.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "you know whatever records they need um",
      "offset": 2788.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "otherwise you know we forget about you",
      "offset": 2790.88,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "when your treatment is done we think",
      "offset": 2792.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it's better that way um on the on the",
      "offset": 2793.359,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "the sensitivity question. Yes, there is",
      "offset": 2796,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "sensitivity involved and at the same",
      "offset": 2799.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "time again there's prior art with these",
      "offset": 2801.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "products, right? There are existing",
      "offset": 2802.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "systems which essentially take, you",
      "offset": 2804.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "know, text messages in and provide",
      "offset": 2805.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "medical advice. Um, we're just trying to",
      "offset": 2807.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "stay within the guidelines of that. And",
      "offset": 2809.44,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "again, that's one reason why we don't",
      "offset": 2810.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "want the LLM actually to have any data",
      "offset": 2812.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "that is not explicitly required just to",
      "offset": 2814.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "do decisioning, right? It doesn't need",
      "offset": 2816.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "anything beyond that to to make a good",
      "offset": 2818.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "decision.",
      "offset": 2820.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 2822.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2823.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "every response is 100% determining that",
      "offset": 2825.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "that's 100% what situations where that's",
      "offset": 2828.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "notified. Yep. Sorry. Hold that thought",
      "offset": 2831.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "too because I will get to that in just a",
      "offset": 2834.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "second. Um let me move on. Uh please",
      "offset": 2835.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like bring these questions back up. I",
      "offset": 2837.359,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "just want to get a little bit further so",
      "offset": 2838.56,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "we can see some other some other cool",
      "offset": 2839.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "things about this. Um I'm going to move",
      "offset": 2840.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "on from this flow just because you can",
      "offset": 2843.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "imagine that this is going over a period",
      "offset": 2844.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of days, right? There's another step",
      "offset": 2846.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "here, step two, where there's, you know,",
      "offset": 2847.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "more medicine being dispersed. Um and",
      "offset": 2849.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "then, you know, ultimately we're going",
      "offset": 2851.359,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "to get to the end, right? and you know",
      "offset": 2852.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "essentially did you complete this and",
      "offset": 2853.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "then okay great you know this is what's",
      "offset": 2855.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "going to happen to you you know you're",
      "offset": 2857.44,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "going to see some bleeding um and then",
      "offset": 2858.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we have this check-in right so imagine",
      "offset": 2860.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that this now is you know a full let's",
      "offset": 2862,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "say three or four days later right after",
      "offset": 2863.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the the treatment has begun um you know",
      "offset": 2865.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "we check in you know the patient gets",
      "offset": 2867.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "back to them or not right remember some",
      "offset": 2869.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "of these patients will just be like I'm",
      "offset": 2871.2,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "done I don't really need to talk to this",
      "offset": 2872.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "thing anymore but if they do right we",
      "offset": 2873.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "continue with the treatment we don't",
      "offset": 2875.68,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "bother them we just let them sort of",
      "offset": 2876.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "resume where they left off again we have",
      "offset": 2878.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "these rationes you know we have these",
      "offset": 2879.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "questions and then what I want to do",
      "offset": 2881.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "here is just to show you briefly um",
      "offset": 2882.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "sorry I got to zoom back out so I get",
      "offset": 2884.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the full phone number um what it would",
      "offset": 2886,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "look like to interact. So if I go here",
      "offset": 2888.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "into my sandbox um imagine that normally",
      "offset": 2890.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "this would be a text message um so you",
      "offset": 2893.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "know I would be doing this on my phone",
      "offset": 2895.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "um but here you know I can answer this",
      "offset": 2896.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "question if I had any pregnancy systems",
      "offset": 2898.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "before have they decreased it's like yes",
      "offset": 2899.599,
      "duration": 7.561
    },
    {
      "lang": "en",
      "text": "uh they have decreased",
      "offset": 2903.04,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "okay so I post this message now what's",
      "offset": 2908.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "going to happen from here is thinking so",
      "offset": 2910.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "none of this is instant and so now what",
      "offset": 2912.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "I want to show you is what this looks",
      "offset": 2914.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like in Langmith so um you can see here",
      "offset": 2915.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "a couple of things um one is that this",
      "offset": 2918.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "this is now spinning. Um, so this thing",
      "offset": 2919.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that I just asked it is now in active",
      "offset": 2921.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "processing. I'll show you what it looks",
      "offset": 2923.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like when we're done. Um, but I will",
      "offset": 2925.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "give you just a brief look at um I think",
      "offset": 2927.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "this is probably a useful one here.",
      "offset": 2929.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Um, what this actually looks like in",
      "offset": 2932.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "terms of processing the state. Um, so",
      "offset": 2934.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "I'll blow this up a little bit and make",
      "offset": 2935.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "it a bit bigger. So um, what you can",
      "offset": 2938.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "imagine this is using sonnet 4. Um, is",
      "offset": 2942.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that every time a message comes in from",
      "offset": 2944.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a patient, this is what I get. Okay, I",
      "offset": 2946.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "get this description of, you know,",
      "offset": 2949.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "everything that's going on here. I can",
      "offset": 2951.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "see this is an AVLA patient. I can see",
      "offset": 2952.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the thread that we're currently",
      "offset": 2955.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "executing, right? Because you may need",
      "offset": 2956.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "to resume these threads if you need to",
      "offset": 2957.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "give feedback. Um, I have this idea of",
      "offset": 2959.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "I'm in the 3-day check-in phase. So,",
      "offset": 2961.599,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that's the blueprint that I'm going to",
      "offset": 2963.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "read. Um, and then I have a couple of",
      "offset": 2964.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "things. I have these anchors, right,",
      "offset": 2966.559,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "which, you know, you could see. I think",
      "offset": 2967.839,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "this is exactly what you saw before. Um,",
      "offset": 2968.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you know, in that same patient. Um,",
      "offset": 2970.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "these are all defined as, you know,",
      "offset": 2972.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "actually a mix of UTC and and Eastern",
      "offset": 2974.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "time stamps. Um, that's one of the",
      "offset": 2976.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "problems that's hard to eradicate. Um,",
      "offset": 2978.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "getting LM to deal well with time is",
      "offset": 2979.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "really tough. Um, but then I have this",
      "offset": 2981.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "entire message queue, right? And this is",
      "offset": 2983.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the compressed state of the conversation",
      "offset": 2985.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "to date, right? This does not include",
      "offset": 2987.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "every message that Claude sent itself",
      "offset": 2989.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "while it was thinking, right? That part",
      "offset": 2991.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "is contained in these individual",
      "offset": 2993.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Langsmith threads. I could go back and I",
      "offset": 2994.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "could look at this if I needed it. Um,",
      "offset": 2995.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "but what I'm doing is I'm compressing",
      "offset": 2997.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and basically saying all I really care",
      "offset": 2998.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "about is the actual messages that went",
      "offset": 2999.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "back and forth. I want these rationale",
      "offset": 3001.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "because I want to be able to review",
      "offset": 3003.599,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "them, right? that helps me understand",
      "offset": 3004.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "the decisioning that's going on here.",
      "offset": 3005.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "Um, you know, I want these confidence",
      "offset": 3007.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "scores so I can go back and look, you",
      "offset": 3009.119,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "know, what did it think at any given",
      "offset": 3010.48,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "point? And again, I'll show you one",
      "offset": 3011.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "where the confidence was low. But these",
      "offset": 3012.8,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "things can go on a little ways, right?",
      "offset": 3014.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "This is probably, I don't know, 20 25",
      "offset": 3015.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "messages, right? All of this goes in as",
      "offset": 3017.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "initial context in the window, right?",
      "offset": 3020.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So, if you had 150 messages, all 150 of",
      "offset": 3022.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "them are going to potentially go in.",
      "offset": 3024.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Now, we do have a function where you can",
      "offset": 3026.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "optionally set it to compress and say,",
      "offset": 3028.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "well, just show me the last 50, right?",
      "offset": 3030,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "If I need to request more, I can do",
      "offset": 3031.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "that. There's a way to do it. Um but I",
      "offset": 3032.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "don't need to have the entire thing in",
      "offset": 3034.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the window. Um so I get down here. This",
      "offset": 3035.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "is the last message from the patient.",
      "offset": 3038.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "Right? So the question was did you",
      "offset": 3039.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "notice blood clots? I said yes a few.",
      "offset": 3041.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Right? You know that was that was what I",
      "offset": 3043.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "as a patient said. Claude is now going",
      "offset": 3044.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "to start processing this thing. Right?",
      "offset": 3047.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "So imagine you know this all being",
      "offset": 3049.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "basically pasted into you know a claude",
      "offset": 3050.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "window and then having it go through",
      "offset": 3052.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "this process and and call tools. So it",
      "offset": 3054.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "starts by looking at directories that",
      "offset": 3057.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "it's allowed to view. Again, this is a",
      "offset": 3058.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "version where it's got the blueprints",
      "offset": 3060.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "kind of all local and and it's it's",
      "offset": 3061.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "talking to them this way. We have",
      "offset": 3062.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "another version where it talks via MCP",
      "offset": 3064.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "over to the the blue box, right? The",
      "offset": 3065.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "larger system. Um, so it figures out",
      "offset": 3067.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what directory it has. It reads these",
      "offset": 3069.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "basic ones because these need to be read",
      "offset": 3071.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in all cases. So these guidelines,",
      "offset": 3073.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "right, the idea of how do you do your",
      "offset": 3075.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "job, right? The idea of what the",
      "offset": 3076.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "confidence framework looks like, the",
      "offset": 3078.24,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "overview of the treatment, right? You",
      "offset": 3079.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "know, those sorts of things. We read",
      "offset": 3080.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "those up front. None of these is very",
      "offset": 3082.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "large, right? And so you read all this",
      "offset": 3083.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "stuff, you know, it it comes into the",
      "offset": 3085.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the window. Um, and then you know",
      "offset": 3086.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "essentially it reads those descriptions",
      "offset": 3088.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and it says well I was told as part of",
      "offset": 3089.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "this that I have to read the current",
      "offset": 3092.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "blueprint for this current phase, right?",
      "offset": 3093.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "So I read that file individually. So a",
      "offset": 3094.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "bunch of these early calls are just",
      "offset": 3096.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "about setting up the context. This is",
      "offset": 3097.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "not the only way to do it, right? I I",
      "offset": 3099.839,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "mean this this is the way that we've",
      "offset": 3101.2,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "chosen to do it. Again, we chose not to",
      "offset": 3102.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "do rag for a couple of, you know,",
      "offset": 3103.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "reasons around we just did not think we",
      "offset": 3105.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "could get good enough results and",
      "offset": 3106.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "because this is honestly easier to",
      "offset": 3108.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "interpret, right? You can sort of tell",
      "offset": 3110.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "what it's doing. Um, I get to the",
      "offset": 3111.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "blueprint. the blueprint. And you we'll",
      "offset": 3113.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we'll see more of these examples in a",
      "offset": 3114.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "second, but the blueprint is basically",
      "offset": 3116.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "this kind of structured bulleted list,",
      "offset": 3117.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "right? Here's all the stuff that you",
      "offset": 3119.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "might need to say to somebody, right?",
      "offset": 3122,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And you know, here's what you do when",
      "offset": 3123.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you know the user says a certain thing.",
      "offset": 3125.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "This isn't actually that prescriptive.",
      "offset": 3127.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "It's just structured, right? This isn't",
      "offset": 3129.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "an if then statement, right? It it's",
      "offset": 3132.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "kind of like that, but it's not an",
      "offset": 3134,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "actual if then statement. So, like this",
      "offset": 3135.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "format, you know, is one that we",
      "offset": 3137.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "iterated on and got to a point where we",
      "offset": 3139.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "actually get really good results. Um,",
      "offset": 3140.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "but you know, it wasn't 100% obvious",
      "offset": 3142.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this is the way to do it up front. Um,",
      "offset": 3144,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you know, we started with charts. Um,",
      "offset": 3145.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and so now you get to this point where",
      "offset": 3147.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "now you can see, okay, now I got to look",
      "offset": 3149.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "at these, you know, uh, conversations. I",
      "offset": 3151.839,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "got to figure out what's been going on",
      "offset": 3153.68,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "here. And so you can see here, even",
      "offset": 3154.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "though I passed in the state, it has a",
      "offset": 3155.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "function to list messages. And so it",
      "offset": 3157.599,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "basically says, all right, well, now",
      "offset": 3159.119,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "that I sort of know what's going on, let",
      "offset": 3160.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "me see the last five messages, right?",
      "offset": 3161.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "And you can see here it's going to start",
      "offset": 3163.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "sending, you know, a bunch of these in.",
      "offset": 3164.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Um, and so it does that. It looks to see",
      "offset": 3166.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "if there's anything scheduled. There's",
      "offset": 3168.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "not, right? And so now it says, &quot;All",
      "offset": 3170.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "right, this is this is sort of the point",
      "offset": 3172.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "where Claude does its little explaining",
      "offset": 3173.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thing. I understand what's going on.",
      "offset": 3175.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Patient's in the three-day check-in",
      "offset": 3177.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "phase. I already asked about bleeding",
      "offset": 3179.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and cramping. I I asked about blood",
      "offset": 3180.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "clots, and the patient, you know,",
      "offset": 3182.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "basically just said yes, they have blood",
      "offset": 3183.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "clots, and so I'm I'm just going to keep",
      "offset": 3185.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "on going, right?&quot; And it goes to the",
      "offset": 3186.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "next question about pregnancy systems.",
      "offset": 3188.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "This message comes directly from the",
      "offset": 3190.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "blueprint. Okay? And and I'll show you",
      "offset": 3192.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "in a Google Doc form in a second what",
      "offset": 3194.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that looks like. Um, so it schedules it.",
      "offset": 3196.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that says you should send this message,",
      "offset": 3198.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you know, as as soon as you want to. And",
      "offset": 3200.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then we get over to this evaluator flow,",
      "offset": 3202.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "right? And the evaluator says, &quot;All",
      "offset": 3204.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "right, I'm going to look at this",
      "offset": 3205.839,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "situation. I'm going to look at",
      "offset": 3206.8,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "everything that requires confidence",
      "offset": 3207.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "scoring, right? That new message is the",
      "offset": 3209.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "only thing. It's it's the the only thing",
      "offset": 3210.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that just happened. Um, and I'm going to",
      "offset": 3212.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "send it immediately. This is just a a",
      "offset": 3213.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "time stamp for immediately. Um, I then",
      "offset": 3215.359,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "get this kind of report, right? And the",
      "offset": 3218.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "way that we set up our framework, um,",
      "offset": 3221.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and and I'll show it in code a little",
      "offset": 3223.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "bit clearer is, you know, do we know",
      "offset": 3224.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "what the user is saying? Do we know what",
      "offset": 3226.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to say and do we think that we did a",
      "offset": 3228.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "good job? Again, this is a tough one,",
      "offset": 3230.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "right? Um, generally speaking, the LLM,",
      "offset": 3232.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you know, says at all times, &quot;Yes, I",
      "offset": 3235.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "know what I'm doing.&quot; And, you know,",
      "offset": 3236.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like buzz off. Um, but what I can also",
      "offset": 3237.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "do is I can say, &quot;All right, then",
      "offset": 3240.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "there's a bunch of cases in which if I",
      "offset": 3242.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "set an anchor, if I updated the",
      "offset": 3244.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "patient's data, like maybe I changed",
      "offset": 3246.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "their time zone offset, maybe I changed",
      "offset": 3247.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "their name, right? That's a weird thing",
      "offset": 3249.28,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "that, you know, if it happened, you'd",
      "offset": 3250.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "probably want a human to look at. Um, do",
      "offset": 3251.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "I am I sending multiple messages? Do I",
      "offset": 3253.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "send a am I sending duplicate messages",
      "offset": 3255.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "accidentally? Do I have reminders for",
      "offset": 3256.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "things that have already happened? All",
      "offset": 3258.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of those things would deduct from the",
      "offset": 3260.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "score and cause a human to get involved.",
      "offset": 3262.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Right? That that's part of how we do",
      "offset": 3264.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this is to combine does the model think",
      "offset": 3266.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "it's okay? Right? That's this top part.",
      "offset": 3268.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "And then overall, is there a weird",
      "offset": 3270.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "circumstance that I should try to catch,",
      "offset": 3271.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "right? And that I should try to to to",
      "offset": 3273.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "show people uh to show a human for",
      "offset": 3274.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "review. Um in this case, nothing came",
      "offset": 3276.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "up. I update the confidence. It's",
      "offset": 3278.8,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "confidence of 100%. Um, and then",
      "offset": 3280.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "essentially the virtual OA, you know, as",
      "offset": 3282.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "as a final thing, it's very hard to get",
      "offset": 3284.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Claude not to summarize itself. It does.",
      "offset": 3285.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Um, it basically just says here's",
      "offset": 3288.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "everything I did. I'm good. And then if",
      "offset": 3289.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you go down here to the bottom, this is",
      "offset": 3291.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the output state. So this output state",
      "offset": 3293.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "says, well, I have 100% confidence,",
      "offset": 3295.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "again, its version of it, that I did the",
      "offset": 3297.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "right thing. I, you know, here's my",
      "offset": 3299.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "anchors, here's my messages, and here's",
      "offset": 3301.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "the unscent message that I'm I'm now",
      "offset": 3302.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "going to send. And because it's 100%",
      "offset": 3304.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "confidence, it just goes out, right? it",
      "offset": 3306.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "goes back to the text message gateway",
      "offset": 3309.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and it just goes out. Um, that is a",
      "offset": 3310.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "risk, right? You know, if you wanted to",
      "offset": 3312.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "be perfectly safe, you have a human",
      "offset": 3314.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "review all of these things. We don't",
      "offset": 3316,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "want to do that because we're trying to",
      "offset": 3317.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "scale, right? So, we are comfortable in",
      "offset": 3318.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "general with things that are are, you",
      "offset": 3320.559,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "know, coming back with 100% confidence",
      "offset": 3321.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that we just send those messages out.",
      "offset": 3323.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Uh, question back there. Yeah.",
      "offset": 3325.44,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 3329.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "having trouble",
      "offset": 3335.28,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "later on. Uh yeah, so the question is",
      "offset": 3339.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "just uh how do we determine sort of the",
      "offset": 3341.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "the the the situations that might have",
      "offset": 3342.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "confidence issues? Um it is very",
      "offset": 3344.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "handtuned and geared to this evaluation",
      "offset": 3347.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "team like basically the virtual OA team",
      "offset": 3349.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that exists now as as humans. Um we will",
      "offset": 3351.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "review you know in sort of spot checks",
      "offset": 3354.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you know a bunch of situations just to",
      "offset": 3356.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "kind of see like hey is does this seem",
      "offset": 3357.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like it's okay? um when the when a",
      "offset": 3359.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "patient writes back because there are",
      "offset": 3361.52,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "cases where a patient will write back",
      "offset": 3362.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and say you got that wrong like that's",
      "offset": 3363.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "not the time I said like you know I'm",
      "offset": 3365.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "actually taking it now um the confidence",
      "offset": 3367.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "system is pretty good at picking up that",
      "offset": 3369.92,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "that happened and basically saying all",
      "offset": 3371.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "right even if I think I'm confident",
      "offset": 3372.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "something's wrong right you know a human",
      "offset": 3374.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "should take a look at this um but I mean",
      "offset": 3375.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the the answer is it's it's more art",
      "offset": 3377.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "than science it's not something that we",
      "offset": 3379.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "are perfect at even now and because we",
      "offset": 3381.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "want to scale we've chosen to say look",
      "offset": 3383.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the the worst that happens is",
      "offset": 3386,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "essentially something weird happens and",
      "offset": 3387.359,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "a couple of text messages go back and",
      "offset": 3388.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "forth that are just wrong. Usually the",
      "offset": 3389.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "human will get involved and say that",
      "offset": 3392.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "doesn't sound right to me. Right. It's",
      "offset": 3393.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "not it's not a case where the patient is",
      "offset": 3395.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "in danger. Um you know if they say well",
      "offset": 3396.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "I'm having these symptoms you're not",
      "offset": 3398.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "helping me. Like a human will step in.",
      "offset": 3400.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "Like that's that's something we're",
      "offset": 3402,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "pretty good at flagging. Yeah.",
      "offset": 3402.88,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 3412.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. I mean so the the short",
      "offset": 3416.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "answer is um we can look at interactions",
      "offset": 3419.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that ultimately are scored as low",
      "offset": 3422.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "confidence and then we can trace back",
      "offset": 3423.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "from there right so a lot of what we're",
      "offset": 3425.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "doing is when something gets flagged and",
      "offset": 3427.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "a human is like well there's something",
      "offset": 3429.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "weird here um you know we share those",
      "offset": 3430.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "things internally right the the Slack",
      "offset": 3432.799,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "channel that I was talking about before",
      "offset": 3434.16,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "where they talk to the physicians",
      "offset": 3435.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "assistant that's largely been repurposed",
      "offset": 3436.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "to people saying hey this behavior is",
      "offset": 3438.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "off like can you go take a look and that",
      "offset": 3439.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "ends up essentially in my queue as you",
      "offset": 3441.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "know I got to go check my evals I got to",
      "offset": 3443.76,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "see if there's something I can do to",
      "offset": 3445.2,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "catch this and maybe it's a matter of",
      "offset": 3446.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "changing the behavior here. But so it's",
      "offset": 3447.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "usually it when we when we know there's",
      "offset": 3449.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "an issue, we can backtrack. That's the",
      "offset": 3451.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "short answer. Uh over here, I'm curious,",
      "offset": 3452.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "humans also make mistakes. Yep. Do you",
      "offset": 3455.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "have any data from",
      "offset": 3458.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "like%",
      "offset": 3461.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of human responses versus AI? Yep. The",
      "offset": 3463.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "question was about human versus AI error",
      "offset": 3466.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "response from prior data. So yeah, great",
      "offset": 3468.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "question and and yes, the answer is we",
      "offset": 3470,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "do have that data and that's one of the",
      "offset": 3472.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "reasons that the client is as",
      "offset": 3473.839,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "comfortable as they are with letting an",
      "offset": 3474.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "LLM kind of run a muck, right? Is the",
      "offset": 3476.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "idea that humans do make mistakes now",
      "offset": 3478.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and when they get escalated, you know,",
      "offset": 3480.48,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "it's something where you can look back",
      "offset": 3482.16,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "and be like, &quot;Oh yeah, that was a little",
      "offset": 3483.119,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "bit off.&quot; You you correct it and you",
      "offset": 3484.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "move on. Um this is kind of unique and",
      "offset": 3485.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that again it it needs to be, you know,",
      "offset": 3488.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "precisely worded like one of the biggest",
      "offset": 3490.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "risks is just that you give sort of off",
      "offset": 3491.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "label medical advice. But if the idea is",
      "offset": 3493.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that like, oh, you misunderstood and you",
      "offset": 3495.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "have to go back and correct yourself.",
      "offset": 3497.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "That's okay, right? It's it's that's not",
      "offset": 3498.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "a fatal error, right? So, a lot of it is",
      "offset": 3499.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that, you know, we think that we can get",
      "offset": 3501.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "better use out of our humans by",
      "offset": 3503.599,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "reviewing these situations, you know,",
      "offset": 3505.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "than we can out of just having them push",
      "offset": 3506.799,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "the buttons because they will",
      "offset": 3508.16,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "occasionally push buttons wrong, right?",
      "offset": 3508.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "Same thing happens as as with the",
      "offset": 3510.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "robots. So, um talking about mistakes",
      "offset": 3511.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and this has been running for a while.",
      "offset": 3514.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Yeah. Have you thought about fine-tuning",
      "offset": 3517.119,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "a model with deidentified messages like",
      "offset": 3520.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "running it back through? Yeah, I mean",
      "offset": 3524,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the the",
      "offset": 3525.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so the question was about um how we",
      "offset": 3528.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "thought about fine-tuning. Um we have",
      "offset": 3529.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "already seen two major model releases in",
      "offset": 3532.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the time we've been working on this. Um",
      "offset": 3534.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "we we generally don't think that",
      "offset": 3536.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "fine-tuning is a great use of our of our",
      "offset": 3537.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "dollars. Um it it obviously it could be",
      "offset": 3539.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "cheaper. We I mean one one example is um",
      "offset": 3541.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we tried you know at one point to use",
      "offset": 3544.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Haiku um and you know Haiku is not even",
      "offset": 3546,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that much cheaper. It's maybe a third",
      "offset": 3548.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the cost right um we we got to a point",
      "offset": 3549.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "where we made our blueprints better in",
      "offset": 3552.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "part because like we'd sort of had some",
      "offset": 3555.04,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "shortcuts where we just didn't have to",
      "offset": 3556.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "be as as precise with sonnet right you",
      "offset": 3557.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "know we had to be more precise with hiku",
      "offset": 3559.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "and then it worked. Haiku did not get",
      "offset": 3560.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the time stuff. Haiku was terrible at",
      "offset": 3562.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "figuring out what times it needed to",
      "offset": 3564.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sort of put on things. And so the the",
      "offset": 3566.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "kind of thing we would have to do there",
      "offset": 3568.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like it either just kind of requires a",
      "offset": 3569.92,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "smarter model and there were smarter",
      "offset": 3571.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "models from multiple people like 04 mini",
      "offset": 3572.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "really is both you know I mean it's c it",
      "offset": 3574.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "costs a little bit less than haiku I",
      "offset": 3576.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think right and it it was every bit as",
      "offset": 3578,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "smart as sonnet we chose not to go with",
      "offset": 3579.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it in part because it wasn't as",
      "offset": 3581.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "transparent um but so in in general we",
      "offset": 3582.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "don't believe that fine-tuning is",
      "offset": 3585.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "warranted because we think the models",
      "offset": 3586.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "are just going to keep getting better",
      "offset": 3588.72,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "and cheaper and that we you know we'll",
      "offset": 3589.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "be able to kind of switch wholesale as",
      "offset": 3591.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "opposed to having fine tuned something.",
      "offset": 3592.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "So when you're going through that sort",
      "offset": 3596.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "of you had this like chattiness with the",
      "offset": 3597.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "model where it was describing its",
      "offset": 3599.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "actions and then calling tools. Is that",
      "offset": 3600.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "like is that an intentional choice? I",
      "offset": 3602.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "feel like you could just skip that just",
      "offset": 3603.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "outputs it. Well so yes it was it was",
      "offset": 3606.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "kind of intentional choice right this is",
      "offset": 3609.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "partly that we we already get the the",
      "offset": 3611.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "rationale and sort of the general you",
      "offset": 3614.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "know explanation of its actions. Um but",
      "offset": 3615.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "there are times where you want to be",
      "offset": 3617.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like look why did it do this and you",
      "offset": 3618.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "know if it's thinking out loud it's a",
      "offset": 3620.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "lot easier to catch. Um, so yes, it's",
      "offset": 3622.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "possible that we could eradicate some of",
      "offset": 3624.799,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "that. We don't really think the juice is",
      "offset": 3626.079,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "worth the squeeze.",
      "offset": 3627.28,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "Most likely you're going to suffer a",
      "offset": 3631.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "secondary. How does the current",
      "offset": 3632.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "structure set up so that you have a new",
      "offset": 3635.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "anchor point to see this person?",
      "offset": 3637.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yep. Uh, great question. So, uh,",
      "offset": 3641.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "questions about essentially multiple",
      "offset": 3642.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "treatments or coming back again after,",
      "offset": 3643.92,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "you know, having gone through a",
      "offset": 3645.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "treatment. Um, there are a couple ways",
      "offset": 3646.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "to do that. So one is that um you know",
      "offset": 3648.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "again depending on how you get there if",
      "offset": 3650.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you scan a QR code that can start kind",
      "offset": 3652.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "of a new activation so we can know that",
      "offset": 3654.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you're coming in a second time. Um but",
      "offset": 3656.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "people will write back after you know",
      "offset": 3658.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "two months and say I have a question",
      "offset": 3660.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "right and and so we either can just",
      "offset": 3662.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "reactivate that conversation. The other",
      "offset": 3664.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "thing is different treatments would",
      "offset": 3666.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "usually come from different phone",
      "offset": 3668.079,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "numbers. So there's a few different ways",
      "offset": 3669.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "to kind of disambiguate you know what",
      "offset": 3670.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "somebody's actually up to. But that",
      "offset": 3671.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "notion of like you know the same thing",
      "offset": 3673.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "happened to me again. I'm starting the",
      "offset": 3674.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "regimen over again. Fundamentally, you",
      "offset": 3676.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "could just explain it. You just say",
      "offset": 3677.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "like, &quot;Hey, I had a miscarriage two",
      "offset": 3679.2,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "months ago. I had another one. Can you",
      "offset": 3680.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "help me?&quot; And it would reset itself,",
      "offset": 3681.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right? The LM is smart enough to do",
      "offset": 3683.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "that.",
      "offset": 3685.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Can I share some",
      "offset": 3688.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "uh please? Because there's plenty to",
      "offset": 3690.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "there's plenty to share.",
      "offset": 3692.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "I mean intentionally obviously the",
      "offset": 3694,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "intention of improving two",
      "offset": 3696.24,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "intent I guess I'm skeptical that",
      "offset": 3701.119,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "doubling the costs are yielding",
      "offset": 3704.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "better",
      "offset": 3708.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "question you have like a funnel of how",
      "offset": 3710.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "often the evaluator might be second",
      "offset": 3712.16,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "question",
      "offset": 3715.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "they're both right in this case they are",
      "offset": 3718.559,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "yes is there was there",
      "offset": 3721.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "an intentional decision to stick with",
      "offset": 3724.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "rather than switching model where in",
      "offset": 3726,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "theory hypothetically you got a",
      "offset": 3728.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "different brain looking at the other",
      "offset": 3730.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "thing. Yep. And last question, sorry.",
      "offset": 3732.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "No, no, please. Um, was the inclusion of",
      "offset": 3734.799,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "this eval?",
      "offset": 3737.28,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "So, were there other impacts besides",
      "offset": 3740.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "just like this is a performance thing",
      "offset": 3742.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that made it rigid? Yeah, so questions",
      "offset": 3743.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "are all about sort of the evaluator node",
      "offset": 3746.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and the and the processes. So, uh, the",
      "offset": 3748,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the shortest possible answer is, um,",
      "offset": 3749.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "yes, we're also skeptical about it. And",
      "offset": 3752.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "at the same time, we think that there's",
      "offset": 3753.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "still value in trying, you know,",
      "offset": 3756.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "essentially it's given getting a second",
      "offset": 3759.28,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "bite at the apple, right? We do think",
      "offset": 3760.319,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "that just having a different system",
      "offset": 3761.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "prompt in the same conversation does",
      "offset": 3762.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "occasionally deliver better results, but",
      "offset": 3764.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you could have the the virtual OA",
      "offset": 3766.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "evaluating the complexity of its own",
      "offset": 3767.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "situation. I don't think you could get",
      "offset": 3769.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "it to evaluate whether it was right or",
      "offset": 3770.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "not, just typically LM are terrible at",
      "offset": 3772.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that anyway. So I think the I think the",
      "offset": 3773.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "basic answer though is that we wanted",
      "offset": 3776.16,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "the flexibility in part so we could do",
      "offset": 3777.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "things like try a different model",
      "offset": 3778.799,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "entirely, right? Or you know have",
      "offset": 3780.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "something where maybe you maybe you did",
      "offset": 3781.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "fine-tune a model specifically to catch",
      "offset": 3783.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "these errors, right? Like that I think",
      "offset": 3785.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "wouldn't be crazy at all. Um so yeah, we",
      "offset": 3786.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "wanted kind of that optionality and at",
      "offset": 3789.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this point you know it's still early",
      "offset": 3790.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "enough right again it's running it's out",
      "offset": 3792.319,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "there like you know we're still tuning",
      "offset": 3793.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "it. Um, if we get to a point where we're",
      "offset": 3794.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "like, look, the only issue with this is",
      "offset": 3796.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "how much it costs or like specific",
      "offset": 3797.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "details about like how good it is at",
      "offset": 3799.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "catching errors, um, we'd we'd go harder",
      "offset": 3800.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "at that. But we're pretty we're pretty",
      "offset": 3802.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "happy with the balance of it usually",
      "offset": 3804.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "escalates situations that need review,",
      "offset": 3806.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right? It will sometimes screw up",
      "offset": 3809.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "something just because it thinks that it",
      "offset": 3810.96,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "was easy and it wasn't. That that does",
      "offset": 3812.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "happen. The same thing happens with",
      "offset": 3813.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "humans, right? So like we we sort of are",
      "offset": 3815.039,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "meeting the bar that we'd set for",
      "offset": 3816.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "ourselves in the first place. That's a",
      "offset": 3817.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "good distinction though. The evaluator",
      "offset": 3819.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "has a different task of sorts. It does.",
      "offset": 3820.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "It's not really the same thing two",
      "offset": 3823.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "times. Correct. The evaluator is looking",
      "offset": 3825.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "at it differently and it has this",
      "offset": 3826.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "explicit and so one thing actually",
      "offset": 3828.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "though is that the evaluator can see",
      "offset": 3830.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "what the VA is supposed to do, right? It",
      "offset": 3831.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "can see the guidelines. So it can it is",
      "offset": 3833.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "able to basically say you didn't do that",
      "offset": 3835.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "right because I know what you were told",
      "offset": 3836.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to do and you didn't do it. And likewise",
      "offset": 3837.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the the virtual OA can see the",
      "offset": 3839.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "evaluator's confidence framework and it",
      "offset": 3841.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "can say well I'm going to be scored",
      "offset": 3843.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "against these things. You know I better",
      "offset": 3844.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "get it right. Again this is very much",
      "offset": 3846,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "more art than science but but I mean",
      "offset": 3848.16,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "you're asking the right question about",
      "offset": 3849.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "like could we just have either a more",
      "offset": 3850.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "optimal or a cheaper way of doing it. I",
      "offset": 3852.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think the answer is yes. Okay, let me",
      "offset": 3853.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "keep going for a second. Please just",
      "offset": 3855.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "hold your thoughts. Um, so again, this",
      "offset": 3856.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "idea of like every interaction looks",
      "offset": 3858.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "like this. It is a starting state, a",
      "offset": 3860.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "conversation, an ending state, which",
      "offset": 3863.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "then goes back to the system. And so I",
      "offset": 3865.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "wanted to show you here was if I go back",
      "offset": 3866.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to a conversation, right? In fact, let",
      "offset": 3868.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "me just see what I got here. Oh, yeah.",
      "offset": 3870,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "In fact, this this answered. So I said",
      "offset": 3871.839,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the pregnancy symptoms have decreased.",
      "offset": 3873.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "The next question in the blueprint is do",
      "offset": 3874.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "you think you're done? Right? you know,",
      "offset": 3877.039,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "do you believe that, you know, the",
      "offset": 3878.48,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "miscarriage and sort of the the changes",
      "offset": 3879.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "that these medicines were supposed to",
      "offset": 3881.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "elicit have have completed, right? Um,",
      "offset": 3882.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and there's basically one more message",
      "offset": 3884.799,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "after this which kind of confirms and",
      "offset": 3886,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "says like, hey, let us know if you have",
      "offset": 3887.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "any questions. But that kind of",
      "offset": 3888.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "interaction, right, back and forth, back",
      "offset": 3890.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and forth, assessing the state as it",
      "offset": 3891.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "currently exists is what this is built",
      "offset": 3893.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to do. And we're compressing after every",
      "offset": 3895.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "one of these interactions into only the",
      "offset": 3897.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "changes that happen to the state in a",
      "offset": 3899.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "given time. Right? We're not saving, you",
      "offset": 3901.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "know, in Langmith, we're saving the",
      "offset": 3903.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "entire conversation, right? This data,",
      "offset": 3904.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "sorry, that's the wrong tab. this data,",
      "offset": 3906.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you know, about like what the virtual",
      "offset": 3908.319,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "lawyer and the evaluator said to each",
      "offset": 3909.599,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "other and what tools they called. This",
      "offset": 3910.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "is preserved in Langsmith. We don't get",
      "offset": 3912.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "rid of this, right? But we do not save",
      "offset": 3913.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this in the state on the blue box,",
      "offset": 3915.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right? We that's not part of the",
      "offset": 3917.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "patient's interactions with us and we",
      "offset": 3919.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "don't reload it every time you go back",
      "offset": 3921.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "with with a new message because that",
      "offset": 3923.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "would ultimately both confuse things and",
      "offset": 3924.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and blow up the context window. So",
      "offset": 3926.559,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that's the way we've uh we've chosen to",
      "offset": 3928.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "do it. Um so let me let me now show you",
      "offset": 3930.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this. Um, I have another conversation",
      "offset": 3932.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "here which actually needs response. So,",
      "offset": 3934.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "I'm going to grab this and put it in the",
      "offset": 3937.2,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "sandbox so you can see what this looks",
      "offset": 3938.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like. Sorry.",
      "offset": 3939.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I just got",
      "offset": 3941.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "persistence.",
      "offset": 3943.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Uh, sorry. Persistence if you only have",
      "offset": 3947.2,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "what?",
      "offset": 3948.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "You you just don't save the the process",
      "offset": 3953.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "of the model talking to itself, right?",
      "offset": 3956.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "You you have it. You can refer to it if",
      "offset": 3958.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you need to. It's a debugging tool. Yep.",
      "offset": 3959.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Yep. input and output is all that we",
      "offset": 3961.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "snapshot in the in the larger system.",
      "offset": 3963.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Okay. So now let's look at this. So I",
      "offset": 3965.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "think that's actually the wrong one. Let",
      "offset": 3967.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "me go",
      "offset": 3968.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "sorry. Find this again.",
      "offset": 3970.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "All right. Yep. So this this right here",
      "offset": 3973.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "actually, you know, I can I can I don't",
      "offset": 3975.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "have to go to the sandbox to look at",
      "offset": 3977.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "this. This is an example of what happens",
      "offset": 3978.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "when things are complicated enough that",
      "offset": 3980.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we're asking for human review. Okay. So",
      "offset": 3982.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "in this case, I've just started this",
      "offset": 3984.24,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "conversation. All right. And I said,",
      "offset": 3985.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "&quot;Yep, I got my medicine came from the",
      "offset": 3986.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "clinic. Here's my time.&quot; Now, this is a",
      "offset": 3988.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "moment where in the treatment a lot of",
      "offset": 3990.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "stuff is happening. I'm figuring out",
      "offset": 3992.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "what time zone they're in, right? And",
      "offset": 3994.079,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "I'm saving that as part of the patient",
      "offset": 3995.599,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "data, right? So, in this case, I said I",
      "offset": 3996.799,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "was on West Coast time. So, my time zone",
      "offset": 3998.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "offset is 420 minutes before UTC. Um, I",
      "offset": 3999.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "am going to a new phase of the",
      "offset": 4004.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "treatment. I have my medicine. You know,",
      "offset": 4005.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "now I'm not in onboarding anymore. I'm",
      "offset": 4007.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "actually taking the medicine and I'm",
      "offset": 4008.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sending multiple messages. So, in the",
      "offset": 4010.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "confidence framework, and I think I can",
      "offset": 4012.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "find this, but uh I won't dig into it",
      "offset": 4014.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "until we get there. Um, in the",
      "offset": 4016.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "confidence framework, we say when you",
      "offset": 4017.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "have all of these changes at once, you",
      "offset": 4019.839,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "should deduct from your confidence",
      "offset": 4021.52,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "score. So, you see up here, this",
      "offset": 4022.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "confidence of 70%. I have the threshold",
      "offset": 4023.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "set at 75. So, for anything that's below",
      "offset": 4025.839,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "75%. I stop and I ask a human to either",
      "offset": 4028.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "approve, right? So, if I were to approve",
      "offset": 4033.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "this, it would just say, &quot;All right,",
      "offset": 4034.72,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "these changes are fine.&quot; And in this",
      "offset": 4035.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "case, the changes are fine. Um, or I",
      "offset": 4036.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "could give feedback, right? I could say,",
      "offset": 4039.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and I'll try this now, and live demos be",
      "offset": 4041.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "damned. Um, let's say, you know, I want",
      "offset": 4043.76,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "to say, please mention the patient's",
      "offset": 4046.4,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "name in your",
      "offset": 4051.2,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "in your next me in in your messages",
      "offset": 4054.799,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "or in your message. So, I'll say submit",
      "offset": 4058.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "feedback. Okay. And I'm working through",
      "offset": 4060.319,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "about this because it's kind of an",
      "offset": 4061.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "operational detail. Um, this is now",
      "offset": 4062.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "going and thinking again. So, I'll have",
      "offset": 4064.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "to reload this in a minute and and see",
      "offset": 4065.92,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "what happened. But what's actually",
      "offset": 4067.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "happening here if I go over to Langsmith",
      "offset": 4068.559,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "again, which I should be able to do",
      "offset": 4070.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "is see that what's happening now is that",
      "offset": 4076.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it is restarting a thread that I already",
      "offset": 4078.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "started in progress. So the one",
      "offset": 4080.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "exception to us wiping out its brain and",
      "offset": 4082.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "reloading everything is when you come",
      "offset": 4084.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "back with this feedback, right? Because",
      "offset": 4086,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "you wanted to basically be able to pick",
      "offset": 4087.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "up right in thread and say, &quot;Hey, you",
      "offset": 4088.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "just did that wrong, but everything else",
      "offset": 4090.72,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "here, like you need to be able to see",
      "offset": 4092,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "how you got to that place, right?&quot; You",
      "offset": 4093.119,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "know, so make the right decision and and",
      "offset": 4094.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "finish it up. Um, and so I think",
      "offset": 4096.159,
      "duration": 7.481
    },
    {
      "lang": "en",
      "text": "let's find out here.",
      "offset": 4099.92,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "All right, still thinking. Um, oh, there",
      "offset": 4105.759,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "we go. So, you can see the only change",
      "offset": 4108.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "that happened here is that it mentioned",
      "offset": 4110.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "her name, right? Otherwise is the same",
      "offset": 4112.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "thing. Same time zone offset, same",
      "offset": 4114.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "treatment phase, same reminder. Um, you",
      "offset": 4116,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "can see the rationale here. Um, and and",
      "offset": 4118.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you can see here the rationale even",
      "offset": 4121.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "includes this. I changed it to update",
      "offset": 4122.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the name. Now, you could imagine doing a",
      "offset": 4124,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "version of this where I just had a",
      "offset": 4126.159,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "little edit box and I said, &quot;I'm going",
      "offset": 4127.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "to change this message.&quot; We chose not to",
      "offset": 4128.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "do that, right? We want the LM actually",
      "offset": 4130.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to drive these changes. We think that",
      "offset": 4132.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "it's better for humans to speak to them",
      "offset": 4133.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "as though they're talking to a person.",
      "offset": 4135.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Um, this is a debatable choice, but it",
      "offset": 4136.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is a choice that we made. Um, and part",
      "offset": 4139.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of that means we can be very very",
      "offset": 4141.12,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "flexible about the treatment, right? We",
      "offset": 4142.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "can just give feedback on the situation",
      "offset": 4143.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "rather than having to build some sort of",
      "offset": 4145.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "tools that are are are flexible enough",
      "offset": 4147.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "to deal with all different types of",
      "offset": 4148.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "treatments. Um, but so here I'm just",
      "offset": 4149.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "going to go ahead and say approve. And",
      "offset": 4151.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "now those messages go out and the",
      "offset": 4153.359,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "changes are made, right? I have, you",
      "offset": 4155.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "know, my patient local time set and I",
      "offset": 4156.799,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "know I'm in the next part of the",
      "offset": 4158.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "blueprint. Okay, I'm going to pause",
      "offset": 4159.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "here. I'm about to jump over to code. I",
      "offset": 4161.12,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "think we have something like 45 minutes",
      "offset": 4162.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "left. Um, any questions on any of this",
      "offset": 4163.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "so far that are not? I just want to see",
      "offset": 4165.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the code because I can do that part over",
      "offset": 4167.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "there. Have you heard any",
      "offset": 4169.839,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "customers?",
      "offset": 4172.88,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yes. Uh, question is about feedback from",
      "offset": 4178.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "patients and that it is emotional. So",
      "offset": 4180.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "yes, absolutely. So remember this is a",
      "offset": 4182,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "system the patient or the client is",
      "offset": 4183.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "already running, right? So fundamentally",
      "offset": 4184.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "they already believe that they're",
      "offset": 4186.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "talking to humans even when they're not",
      "offset": 4188.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "exactly right. Even the humans pushing",
      "offset": 4190.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the buttons are just calling up",
      "offset": 4192.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "essentially bot generated responses. Um",
      "offset": 4194.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "when things get emotional, um humans can",
      "offset": 4197.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "step in. You know, we we tend to steer",
      "offset": 4199.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "them towards kind of approved knowledge",
      "offset": 4201.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "based responses. Like you don't want",
      "offset": 4203.28,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "this to be something where it goes",
      "offset": 4204.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "completely free form. There's there's",
      "offset": 4206.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "legal and other reasons not to do that.",
      "offset": 4207.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "So by stepping in and having LM make the",
      "offset": 4209.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "decisions doesn't really change the kind",
      "offset": 4211.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "of current context of these treatments.",
      "offset": 4213.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "They're already getting you know",
      "offset": 4215.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "basically this sort of medically",
      "offset": 4217.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "approved feedback you know based on a",
      "offset": 4218.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "certain flowchart and if it goes",
      "offset": 4220.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "somewhere you know a little crazy the",
      "offset": 4222.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "escalation point is usually to call",
      "offset": 4224.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "someone right it's not you know we keep",
      "offset": 4225.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "on talking forever in text because",
      "offset": 4227.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that's messy. Um there are a bunch of",
      "offset": 4228.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "points which I'm not going to be able to",
      "offset": 4230.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "demo here which basically just say yeah",
      "offset": 4231.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I'm sorry I can't answer that question.",
      "offset": 4233.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "call 911, go to your doctor, whatever it",
      "offset": 4235.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is, right? But that that is usually",
      "offset": 4237.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "where it goes from there.",
      "offset": 4239.52,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "The blueprints look a lot like",
      "offset": 4242.08,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "that. Well, it's a great question. So,",
      "offset": 4247.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "honestly, part of it is just that we",
      "offset": 4250.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "needed to have something that the",
      "offset": 4252.8,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "patient or not the patients, the client",
      "offset": 4253.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "was actually comfortable maintaining,",
      "offset": 4255.36,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "right? Because remember, part of it is",
      "offset": 4256.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that we do not want this in code, right?",
      "offset": 4257.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "We don't want this to be something where",
      "offset": 4259.6,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "you can only maintain it if you have a",
      "offset": 4261.36,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "technical person. That's the problem",
      "offset": 4262.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "they had before, right? And so just to",
      "offset": 4263.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "jump over for a second, I'll show you",
      "offset": 4265.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "what this was kind of looks like. So",
      "offset": 4266.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this is essentially the thing that the",
      "offset": 4268.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "client is maintaining. And I'll blow",
      "offset": 4270.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this up a little bit. I realize that is",
      "offset": 4272.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "small. Um, but the idea here is that",
      "offset": 4274,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "we're using terms and and you know, we",
      "offset": 4277.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "we'll see a bit more of this in the",
      "offset": 4279.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "code. We're using terms that are defined",
      "offset": 4280.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in the framework. A trigger is you know,",
      "offset": 4282.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "something that happens, you know,",
      "offset": 4285.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "essentially after an event, right? Um,",
      "offset": 4286.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "you know, we have the the conversation",
      "offset": 4288.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of these messages. We always tell the LM",
      "offset": 4289.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "why this is important. If we just had",
      "offset": 4291.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "this this detail and we just said this",
      "offset": 4293.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "is the message you send, I don't think",
      "offset": 4295.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "it would perform as well. It's much more",
      "offset": 4296.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "helpful to actually give the LLM",
      "offset": 4298.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "justification for why it would say",
      "offset": 4299.679,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "something because then it makes better",
      "offset": 4300.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "decisions. Um, one of the many quirks.",
      "offset": 4302.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Um, go ahead. on that. I'm not sure if",
      "offset": 4304.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "this is code or not for the next part,",
      "offset": 4306.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "but how how complicated how simple",
      "offset": 4308.08,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "statements",
      "offset": 4312.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "or if you have any any actually got lost",
      "offset": 4316.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in the Oh, sure. Yep. So, so the",
      "offset": 4318.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "question was just about you know",
      "offset": 4321.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "essentially why why do we have this",
      "offset": 4322.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "framework and and why is it maybe not",
      "offset": 4324,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "more declarative, right? In terms of",
      "offset": 4325.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like specifically if then and that sort",
      "offset": 4326.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "of thing, right? Actually I I use",
      "offset": 4328.64,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "similar with with a different index and",
      "offset": 4330.719,
      "duration": 8.681
    },
    {
      "lang": "en",
      "text": "like I got indus",
      "offset": 4335.6,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "so I couldn't go like very complicated",
      "offset": 4340.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "not a lot of nested got to be like one",
      "offset": 4342.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "two levels",
      "offset": 4344.719,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "right my question",
      "offset": 4346.64,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "so I I the so the answer is just about",
      "offset": 4352.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "again how do you how do you define these",
      "offset": 4354.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "things as clearly but you know maybe not",
      "offset": 4356.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "complexely as as possible. Right? So, um",
      "offset": 4358.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "this this framework tends to work where",
      "offset": 4361.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you're really just saying, look, I'm",
      "offset": 4364.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "giving you this approved language and",
      "offset": 4366,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "I'm trying to give you in the bold",
      "offset": 4368.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "statements here, right? Primarily, I'm",
      "offset": 4369.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "trying to give you a sense of, you know,",
      "offset": 4371.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "what what the conditioning really is.",
      "offset": 4372.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "But part of the reason that we did it",
      "offset": 4374.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "this way is because, you know, if the",
      "offset": 4375.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "patient writes back after this thing and",
      "offset": 4377.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "he says, you know, yes, I have the",
      "offset": 4378.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "medication and I took the pills and my",
      "offset": 4380.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "stomach hurts and I'm confused, right? I",
      "offset": 4382.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "mean, it could be all these things. We",
      "offset": 4384.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "wouldn't want to represent something",
      "offset": 4386.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like that in a flowchart. What we really",
      "offset": 4387.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "want to do is just say, &quot;Look, this is",
      "offset": 4389.44,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "the outline of the thing. You can see",
      "offset": 4390.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it. You know, if you need to jump ahead,",
      "offset": 4391.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "jump ahead and don't ask the patient",
      "offset": 4393.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "questions they've already answered.&quot; It",
      "offset": 4394.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "just turns out that this this framework",
      "offset": 4396.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "really does work pretty well for letting",
      "offset": 4397.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the LM do that sort of thing. It's I",
      "offset": 4399.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "know that's kind of a magic answer, but",
      "offset": 4400.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "pretty good at it.",
      "offset": 4402.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Uh yeah. No, I mean Claude Yeah, Claude",
      "offset": 4404.719,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "mostly nails it. Most of them do. Yes.",
      "offset": 4406.48,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "Does including the instructions",
      "offset": 4410.56,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "response quality? Uh, sorry. What do you",
      "offset": 4414.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "mean by including the instructions?",
      "offset": 4417.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "Including the reasoning. Oh, the",
      "offset": 4419.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "reasoning. I I So, question was do does",
      "offset": 4420.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "including the reasoning help with the",
      "offset": 4422.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "response quality? I think it does,",
      "offset": 4423.84,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "right? I mean, this is one of these",
      "offset": 4425.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "things where we started also by",
      "offset": 4426.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "borrowing from human documentation,",
      "offset": 4428.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "right? So, this was a process that was",
      "offset": 4430.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "originally explained to humans who were",
      "offset": 4431.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "going to push the buttons. And so we",
      "offset": 4433.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "took a combination of flowcharts that",
      "offset": 4434.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "existed to explain the flow of the",
      "offset": 4436.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "treatment and these kinds of you know",
      "offset": 4438.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "this is the message that you should send",
      "offset": 4440.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "in these situations and and this was",
      "offset": 4441.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "kind of the the hybrid output of those",
      "offset": 4443.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "two things. So I wouldn't say we did",
      "offset": 4444.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "aggressive testing on is it is it really",
      "offset": 4446.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "better or is it just that you know this",
      "offset": 4449.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "is good enough. It's more that like we",
      "offset": 4450.719,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "started with this framework based on the",
      "offset": 4452.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "human materials we had. A follow",
      "offset": 4453.6,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "question to that. Yeah. Did you find any",
      "offset": 4455.679,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "sacrifices that you had to make",
      "offset": 4459.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "as",
      "offset": 4463.28,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "form.",
      "offset": 4465.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Yeah. So question is uh maintaining this",
      "offset": 4467.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "document as human readable versus LM.",
      "offset": 4469.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Yes, there are trade-offs. I think",
      "offset": 4471.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "they're still worth it. We we may change",
      "offset": 4473.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "our mind at some point, right? So, you",
      "offset": 4475.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "know, imagine the workflow here being um",
      "offset": 4476.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "you know, this this Google doc is",
      "offset": 4478.64,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "maintained essentially by our",
      "offset": 4480.159,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "physician's assistant, right? She is the",
      "offset": 4481.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "co-owner of the blueprint maybe next to",
      "offset": 4482.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "me. Um when we when we make changes, we",
      "offset": 4484.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "talk about them together. We recommend",
      "offset": 4487.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "in this document and then accept them.",
      "offset": 4488.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and then effectively I I export it to",
      "offset": 4489.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "markdown and check it in. Right? That is",
      "offset": 4491.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "going to change a little bit. We're",
      "offset": 4493.84,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "going to build a lot of these tools into",
      "offset": 4494.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the database and so that that's really",
      "offset": 4495.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "where you'd be doing this instead. Um",
      "offset": 4497.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but because this is human, you know,",
      "offset": 4499.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "maintained, right? Because it is",
      "offset": 4501.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "basically, you know, still driven by the",
      "offset": 4502.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "team. Um yes, we we are making a",
      "offset": 4504.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "trade-off. I don't think it's a",
      "offset": 4506.719,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "trade-off that's that's super damaging.",
      "offset": 4507.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Based on your current design, yeah, just",
      "offset": 4512.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "now when you do the thing,",
      "offset": 4514.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "what does change after is it like a one",
      "offset": 4518.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "time or does it improve your answer in",
      "offset": 4521.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "the future or even changing the",
      "offset": 4524.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so at the moment? No. And the question",
      "offset": 4528.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "was just about uh the approve uh sort of",
      "offset": 4530.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "defer um you know feedback mechanism. Um",
      "offset": 4532.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "so actually I'll go back and just show",
      "offset": 4535.199,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "this really quick. This should be done",
      "offset": 4536.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "now. Um there we go. Um again we are we",
      "offset": 4537.6,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "are saving this in the sense that I can",
      "offset": 4542.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "see this in lang right. I can look at",
      "offset": 4544.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this and I can say well in you know",
      "offset": 4546.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "these cases where an approval was needed",
      "offset": 4547.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and in this case like just just as a a",
      "offset": 4549.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "visual you know sort of feedback",
      "offset": 4550.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "whenever you have this graph null start",
      "offset": 4552.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "right that that is one of these cases",
      "offset": 4554.48,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "where you know there was an approve",
      "offset": 4555.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "feedback defer choice um I could filter",
      "offset": 4557.199,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "by this and I could look at all of these",
      "offset": 4559.44,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "things and I could say well what kinds",
      "offset": 4560.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "of things were we actually trying to",
      "offset": 4562.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "approve or give feedback on um we don't",
      "offset": 4563.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "learn from them right we we as humans",
      "offset": 4566.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "will maybe update the blueprints we do",
      "offset": 4568.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "not put this back into training data",
      "offset": 4570.239,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "again for a bunch of reasons which are",
      "offset": 4571.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "are kind of specific to the situation um",
      "offset": 4572.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "but you So you can see here that like I",
      "offset": 4575.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "can go all the way down here and I'll",
      "offset": 4576.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "try to find this quickly. Um and you'll",
      "offset": 4577.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "get to a point where the human says all",
      "offset": 4579.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "right yeah here it is. So we get",
      "offset": 4581.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "feedback from yeah from the humano. This",
      "offset": 4584.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is essentially what happens whenever I",
      "offset": 4587.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "push that button and I say give feedback",
      "offset": 4589.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right the humano has feedback about your",
      "offset": 4591.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "unscent messages. The feedback is",
      "offset": 4592.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "mention their name. Um it just goes",
      "offset": 4594.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "right back to business. It's like okay",
      "offset": 4596.32,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "let me look at the messages that you",
      "offset": 4597.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "know I was sending. I'm going to update",
      "offset": 4598.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with this one. I'm gonna probably delete",
      "offset": 4600.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not sure actually no it just updated",
      "offset": 4603.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that one in place we rescored it one",
      "offset": 4604.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "thing we've said is that we do not",
      "offset": 4606.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "change the confidence score on something",
      "offset": 4608.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that a human reviewed we leave it where",
      "offset": 4610.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it was right we let them review it again",
      "offset": 4612.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right and so in all these cases this",
      "offset": 4614.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this is also a much quicker you know",
      "offset": 4616.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "simpler sort of operation right and so",
      "offset": 4617.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "you can see the evaluator here is",
      "offset": 4619.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "basically like yep that message is fine",
      "offset": 4620.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "but we're not going to do anything you",
      "offset": 4622.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "know really to change the the overall",
      "offset": 4623.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "score um so you know that's the kind of",
      "offset": 4625.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "thing that you know we can look at",
      "offset": 4628.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "afterwards Right. But we are not at this",
      "offset": 4630.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "point at least, you know, really trying",
      "offset": 4631.84,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "to feed that back into the model. It's",
      "offset": 4633.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "really just for the blueprints.",
      "offset": 4634.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Just get a sense of your metrics. Um, a",
      "offset": 4636.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "lot of these are, you know, over a",
      "offset": 4638.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "minute and it says about a couple",
      "offset": 4640,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "hundred thousand. How do you kind of",
      "offset": 4641.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "look like a necessary evil? The time it",
      "offset": 4643.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "takes, the cost.",
      "offset": 4646.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "No, no. I mean, it it is a necessary",
      "offset": 4649.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "evil and and actually just to point it",
      "offset": 4650.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "out, um, these costs I don't believe are",
      "offset": 4652.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "correct. Um, one of one of the",
      "offset": 4654.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "shortcomings of Langmith and I think",
      "offset": 4656.719,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "they've admitted this in various forms",
      "offset": 4657.92,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "is they don't really take into account",
      "offset": 4659.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the caching. Um, so these costs should",
      "offset": 4660.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "be lower than what you see here. Um, but",
      "offset": 4662.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "but fundamentally, yeah, these are",
      "offset": 4664.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "expensive operations and you know we",
      "offset": 4665.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "could change we could change some of",
      "offset": 4667.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "them at the potential cost of higher",
      "offset": 4669.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "error rates, right? Like we could try to",
      "offset": 4670.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "cache more and have you inherit threads",
      "offset": 4672.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "in progress and it would be faster,",
      "offset": 4674.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "right? Because you've already loaded",
      "offset": 4676.239,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "everything. it would be, you know,",
      "offset": 4677.28,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "potentially you're not reloading any",
      "offset": 4678.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "context and so, you know, you're you're",
      "offset": 4679.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "spending maybe less on tokens and you",
      "offset": 4681.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "just might have a higher error rate and",
      "offset": 4683.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and that's, you know, a thing we are",
      "offset": 4684.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "trading off.",
      "offset": 4686.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Is there some kind of knowledge base",
      "offset": 4688.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "that your model is taking to",
      "offset": 4691.199,
      "duration": 8.281
    },
    {
      "lang": "en",
      "text": "depending on the medicine?",
      "offset": 4695.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Uh yeah. So the question is just uh in",
      "offset": 4703.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "terms of the the knowledge basis. So let",
      "offset": 4705.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "me let me actually jump over and just",
      "offset": 4707.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "show this really quick. So I mentioned",
      "offset": 4708.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "these blueprints. Um I I'll jump over",
      "offset": 4710,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "now really into just what the the the",
      "offset": 4711.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "implementation looks like. So you can",
      "offset": 4713.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "see over here, you know, this idea of",
      "offset": 4715.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "for a VA, right? We have a handful of",
      "offset": 4717.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "documents here that are again exported",
      "offset": 4719.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "into Markdown. Um and I'll try to blow",
      "offset": 4721.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "this up because I know these are small.",
      "offset": 4723.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Um let me just shrink this down. Okay,",
      "offset": 4725.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so the idea here is that you know I've",
      "offset": 4728.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "got all of this, you know, uh sort of",
      "offset": 4730.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "framework data, right? The idea of",
      "offset": 4732.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "defining what do I mean by a blueprint,",
      "offset": 4734.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "right? We're doing we're we're defining",
      "offset": 4735.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "this every time not in um the the",
      "offset": 4737.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "prompt, right? We're doing this as part",
      "offset": 4740.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of the context window in part because we",
      "offset": 4742.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "do want this to be really flexible. If",
      "offset": 4744.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "you want to change the terms um you",
      "offset": 4745.6,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "should be able to do that, right? We",
      "offset": 4747.28,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "don't want the treatments to be",
      "offset": 4748.239,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "hamstrung by by terms we use for other",
      "offset": 4749.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "treatments. I define anchors. I talk",
      "offset": 4750.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "about schedules. I talk about scheduled",
      "offset": 4752.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "messages, right? So all of this stuff",
      "offset": 4753.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "exists in part just to to lay the",
      "offset": 4755.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "groundwork. And then this framework,",
      "offset": 4757.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right, is now referring to specific",
      "offset": 4759.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "documents, right? And so you can see",
      "offset": 4760.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "here like I again these documents are",
      "offset": 4762.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "all referring to each other. So I can go",
      "offset": 4763.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "through here and I can look you know and",
      "offset": 4765.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and click on these links and go straight",
      "offset": 4766.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to other things if I want to do",
      "offset": 4768.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "something around the knowledge base. So",
      "offset": 4770,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "the way that we do that is this triage",
      "offset": 4771.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "idea. Um so you know if something",
      "offset": 4772.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "happens that a blueprint doesn't address",
      "offset": 4775.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right. So the way that we tar it is",
      "offset": 4777.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "first check on the blueprint. If you",
      "offset": 4779.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "have approved language use it right send",
      "offset": 4780.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "it send it back for human review",
      "offset": 4782.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "whatever you need to do right but use",
      "offset": 4784.159,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "that approved language. If you don't",
      "offset": 4785.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think you can answer that question you",
      "offset": 4786.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "go and look at this which now again is",
      "offset": 4788.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is self-referential. We don't read the",
      "offset": 4790,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "entire scope of medically approved",
      "offset": 4792.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "knowledge all at once. We let the Ellen",
      "offset": 4794.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "decide are they complaining about",
      "offset": 4796.159,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "stomach pain or bleeding, right? You",
      "offset": 4797.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "know, if I can't find anything in any",
      "offset": 4799.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "one of these, I have a larger knowledge",
      "offset": 4800.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "base, right? Which is, you know, sort of",
      "offset": 4802.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "just a laundry list of like random",
      "offset": 4803.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "questions people ask. Um, we've chosen",
      "offset": 4805.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to do it this way in part because it is",
      "offset": 4807.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "human readable. It mirrors something the",
      "offset": 4808.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "client already mostly had, right? They",
      "offset": 4811.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "already had a lot of these structures.",
      "offset": 4812.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Um, and you know, we fundamentally did",
      "offset": 4814.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "not believe that it made sense to",
      "offset": 4816,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "overprocess, you know, things like a",
      "offset": 4818.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "rag. Now I will say that for the thing",
      "offset": 4820.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "that we have like kind of a backup you",
      "offset": 4821.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "know sort of knowledge store which is",
      "offset": 4823.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "almost entirely a CSV that probably is",
      "offset": 4824.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "suited for rag right it would be okay to",
      "offset": 4826.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "use a rag for that it doesn't get used",
      "offset": 4828.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that much right so in some sense it's",
      "offset": 4830.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "just not worth implementing that way at",
      "offset": 4832.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "least not yet",
      "offset": 4834.159,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "all right um yeah",
      "offset": 4836.96,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "yep",
      "offset": 4845.44,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "that is prompt level. So we the the",
      "offset": 4852.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "virtual OA and the and the evaluator",
      "offset": 4854.8,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "both have relatively small prompts which",
      "offset": 4856.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "I can show. So let me just see if I can",
      "offset": 4857.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "find them here. Um those prompts are are",
      "offset": 4859.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "they do reference each other right? So",
      "offset": 4863.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "imagine this being um you know again",
      "offset": 4865.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "built on the line chain stuff. So the",
      "offset": 4866.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "base agent class um this prompt is",
      "offset": 4867.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "basically aware of the other agent,",
      "offset": 4870.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "right? So in this case it's just two. So",
      "offset": 4872.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the prompts do speak about each other.",
      "offset": 4874.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "The evaluator knows about the virtual",
      "offset": 4876,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "away and vice versa, right? You know the",
      "offset": 4877.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "things that we try to do and you know",
      "offset": 4879.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "this is I think normal prompt",
      "offset": 4881.44,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "engineering stuff for people who have",
      "offset": 4882.719,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "really played with this stuff. You have",
      "offset": 4883.6,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "to tell it how to take turns. You have",
      "offset": 4884.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "to tell it that it you know if it gets",
      "offset": 4886.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "called on it has to talk, right? Like",
      "offset": 4887.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you know one one problem we have that we",
      "offset": 4889.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "have to sort of frequently do retries on",
      "offset": 4890.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is the LM thinks that everything's done.",
      "offset": 4892.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "It doesn't say anything and and the",
      "offset": 4894.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "whole thing dies. Um so you know you",
      "offset": 4895.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "have to talk but then you can be done,",
      "offset": 4897.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "right? You just have to say something.",
      "offset": 4899.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "um we have the basic idea of you have to",
      "offset": 4901.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "determine this overall confidence score",
      "offset": 4904.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "but we don't include this in the prompt",
      "offset": 4906.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "because we want to be able to show it to",
      "offset": 4908.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the virtual OA as well right so the",
      "offset": 4909.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "details of how you score something is is",
      "offset": 4911.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is factored out but the notion of here's",
      "offset": 4913.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "who you are here's this other guy is and",
      "offset": 4916,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "here's how you work together that is in",
      "offset": 4917.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the prompts",
      "offset": 4918.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "okay actually on that note let me jump",
      "offset": 4920.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "over and actually show you some of the",
      "offset": 4921.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the confidence stuff and the the",
      "offset": 4922.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "guidelines so um I'll start with",
      "offset": 4924.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "confidence I'll get into the guidelines",
      "offset": 4927.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "which are much much longer This again is",
      "offset": 4928.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "you know intended to be mostly LLM",
      "offset": 4931.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "readable. This is not something the",
      "offset": 4933.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "client generally maintains, right? So",
      "offset": 4934.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "this is not in the same category as",
      "offset": 4936.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "these blueprints. But the idea here is",
      "offset": 4937.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that you know I've got this confidence",
      "offset": 4939.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "score and you know I am trying to figure",
      "offset": 4941.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "out across these multiple dimensions. Do",
      "offset": 4943.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I know what's going on? You know here",
      "offset": 4945.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "are some examples. We we are trying to",
      "offset": 4947.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "be as prescriptive as possible with",
      "offset": 4948.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "examples of these different situations.",
      "offset": 4949.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Um do I know you know the knowledge that",
      "offset": 4951.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I need to know? Here's an example which",
      "offset": 4953.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "might speak to your question actually",
      "offset": 4955.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "over here. you know, the idea of do we",
      "offset": 4956.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "want the LM to use its world knowledge",
      "offset": 4960.239,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "to figure out that when I'm talking",
      "offset": 4961.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "about an antibiotic and I give a",
      "offset": 4963.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "specific antibiotic that it applies to",
      "offset": 4964.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the whole class of them. Yes, that",
      "offset": 4966.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that's a risk that we're kind of willing",
      "offset": 4968,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to take, right? We don't need to have an",
      "offset": 4969.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "explicit this specific antibiotic is",
      "offset": 4971.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "safe for this treatment, right? That",
      "offset": 4973.199,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "would very quickly spiral out of",
      "offset": 4974.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "control. So, we do have a handful of",
      "offset": 4975.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "places where we ask it. Use your own",
      "offset": 4977.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "judgment, but refer to, you know, the",
      "offset": 4978.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the knowledge base and the blueprints",
      "offset": 4980.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "for for your baseline. Um and then so",
      "offset": 4981.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "after I get through these categories",
      "offset": 4984.239,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "then I have this idea of deductions",
      "offset": 4985.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "right and the deductions here um are",
      "offset": 4986.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "specifically things like you know you",
      "offset": 4989.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "should deduct from the overall score not",
      "offset": 4991.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the individual messages right because an",
      "offset": 4993.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "individual message could be like yeah",
      "offset": 4995.28,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "this is exactly from the blueprint like",
      "offset": 4996.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it's the right thing to say but overall",
      "offset": 4997.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "these situations can be complicated",
      "offset": 4999.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right and so what we're trying to do is",
      "offset": 5002,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "explain that such that it can again",
      "offset": 5003.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "score the overall interaction in a way",
      "offset": 5004.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that surfaces it for for human review",
      "offset": 5006.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "um okay so I'm going to move on to the",
      "offset": 5009.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "guidelines because again there's just a",
      "offset": 5011.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "lot more in here. Um, this is long",
      "offset": 5012.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "enough that I'm not going to review",
      "offset": 5014.4,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "everything, but I'll try to get to some",
      "offset": 5015.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "of the biggest parts. Again, some of",
      "offset": 5016.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this is really simple, right? Like tool",
      "offset": 5018.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "calling. Um, one issue we've certainly",
      "offset": 5020,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "had over time is, you know, fabrication",
      "offset": 5021.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and and, you know, honestly,",
      "offset": 5023.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "instructions like this do help. Um, you",
      "offset": 5024.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "know, do not make up a tool call. Wait,",
      "offset": 5026.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "wait your turn, right? Call the tool and",
      "offset": 5028.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "step back. Um, there's a lot of stuff",
      "offset": 5030.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "around time, right? There is a lot of",
      "offset": 5032.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "stuff around, hey, you need to ask about",
      "offset": 5034.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it in the right way. You don't ask about",
      "offset": 5036.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it in a way that forces someone to tell",
      "offset": 5038.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you where they are. Right? there. People",
      "offset": 5040.08,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "are very sensitive about this. They",
      "offset": 5041.44,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "don't want, you know, people knowing",
      "offset": 5042.639,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "where they physically are, but you need",
      "offset": 5043.76,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "to know what their time is so that you",
      "offset": 5044.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "can schedule the messages for them,",
      "offset": 5046.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "right? Um, you want, you know, when you",
      "offset": 5047.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "work with time, um, you have to use",
      "offset": 5049.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "things like, you know, ISO time stamps,",
      "offset": 5052.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like that's how the rest of the system",
      "offset": 5054.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "works. Um, but calculating these things",
      "offset": 5055.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and keeping them all straight, it",
      "offset": 5057.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "requires a relatively smart model. So, a",
      "offset": 5058.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "lot of this, you know, has has sort of",
      "offset": 5060.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "grown over time to just work with the",
      "offset": 5061.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "idea that, you know, this is how you can",
      "offset": 5063.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "talk to models about this and do a",
      "offset": 5065.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "pretty good job um, setting anchors,",
      "offset": 5066.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "scheduling messages. Again, these are",
      "offset": 5069.76,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "all the things that are core parts of",
      "offset": 5071.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the system. This is not treatment",
      "offset": 5072.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "specific, right? This is all written to",
      "offset": 5074,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "be generic enough that I don't have to",
      "offset": 5076,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "rewrite this every time I add a new",
      "offset": 5077.76,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "drug, right? Which which is one of the",
      "offset": 5079.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "core requirements, right? We did not",
      "offset": 5080.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "want to have to do this in code. Yes.",
      "offset": 5082,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "So, this is a very big document.",
      "offset": 5084.159,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "Yep. Uh have you experimented with the",
      "offset": 5088.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "caching because this doesn't change.",
      "offset": 5091.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "Yeah, correct. No, we have. And so,",
      "offset": 5092.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "right now, and I'll get to caching",
      "offset": 5094.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "actually in just a second. I'm just",
      "offset": 5096,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "trying to manage time here, but I do",
      "offset": 5097.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "have time for that. like we are we are",
      "offset": 5098.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "doing some explicit system prompt",
      "offset": 5100.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "caching and then we are caching",
      "offset": 5102,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "explicitly um you know the the multiple",
      "offset": 5103.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "turns of the messages such that you know",
      "offset": 5105.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "each operation is you know I think the",
      "offset": 5107.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "average operation with just sort of the",
      "offset": 5110.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "baseline stuff is maybe 10 to 15,000",
      "offset": 5111.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "tokens right per turn all cached it adds",
      "offset": 5113.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "up right so you know you do have maybe",
      "offset": 5117.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "the average cost to generate a single",
      "offset": 5118.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "message somewhere in the 15 to 20 cent",
      "offset": 5120.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "range right it it's not cheap but we are",
      "offset": 5122.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "caching as aggressively as we can we",
      "offset": 5124.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "have thought about things all this is",
      "offset": 5126.8,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "brand new right the idea of the of the",
      "offset": 5128,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "hour cache, you know, that that Claude",
      "offset": 5129.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "just introduced. It's not clear to us",
      "offset": 5130.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that that would help because, you know,",
      "offset": 5132.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "we can't guarantee that the patient's",
      "offset": 5134.32,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "going to get back to us within, you",
      "offset": 5135.679,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "know, either five minutes or an hour,",
      "offset": 5136.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "right? It's just it's a bit of a risk to",
      "offset": 5138.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "take at the system level. Um, but if",
      "offset": 5140,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "anybody here actually knows more about",
      "offset": 5141.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "cloud caching than I do, please talk to",
      "offset": 5143.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "me because like we we ideally we would",
      "offset": 5145.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like to cache a lot of these documents.",
      "offset": 5147.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Um, it's just not clear if we can do",
      "offset": 5148.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "that across sessions. It's not clear if",
      "offset": 5150.159,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "you know we would get the benefits that",
      "offset": 5151.6,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "we're looking for. So, we've just tried",
      "offset": 5152.719,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "to be as aggressive as we can within a",
      "offset": 5154,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "single conversation.",
      "offset": 5155.6,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "That's a very long list of guidelines.",
      "offset": 5159.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "How did you come up with it and how do",
      "offset": 5161.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you optimize?",
      "offset": 5163.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yeah, I mean the look the the real",
      "offset": 5165.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "answer is that I mentioned before that",
      "offset": 5167.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "you know there's a few thousand lines of",
      "offset": 5169.6,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "code and a few thousand lines of prompt.",
      "offset": 5170.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "This is most of that prompt, right? I",
      "offset": 5172.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "mean this is a lot of it. Um it is",
      "offset": 5174.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "something where you know we have tuned",
      "offset": 5176.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "it over time. This is you know myself as",
      "offset": 5178.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "well as you know the the the physician",
      "offset": 5179.679,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "assistant like we have come up with",
      "offset": 5181.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "something that we believe is",
      "offset": 5182.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "fundamentally you know pretty good at",
      "offset": 5184,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "handling these you know generic",
      "offset": 5185.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "situations and when we find edge cases",
      "offset": 5186.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we we just modify these prompts. It",
      "offset": 5188.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "again it is not perfect. We could",
      "offset": 5190.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "definitely think about subdividing this.",
      "offset": 5191.84,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "We could think about moving some of it",
      "offset": 5193.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "into the prompts. Um but we think this",
      "offset": 5194.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "division is you know roughly correct for",
      "offset": 5196.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "keeping it generic so that it's you know",
      "offset": 5198.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "it handles a bunch of different",
      "offset": 5200.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "treatments and it it handles the",
      "offset": 5201.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "situations that we see across treatments",
      "offset": 5203.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "pretty well. Right. You will have cases",
      "offset": 5205.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "where you're doing medicine that's all",
      "offset": 5207.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "in one day. That that's relatively",
      "offset": 5208.56,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "unusual because you know that's",
      "offset": 5209.92,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "something you just send instructions",
      "offset": 5210.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "home. Um you know but there I mean I'll",
      "offset": 5212.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "show an example around ampic you know",
      "offset": 5214.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that's weekly monthly right like there's",
      "offset": 5215.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "there's much longer durations. We've",
      "offset": 5217.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "tried to get to a point of balancing it",
      "offset": 5219.76,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "where you know we we do end up with a",
      "offset": 5221.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "good result.",
      "offset": 5222.719,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "Okay. Uh yeah back there.",
      "offset": 5224.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 5238.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 5242.4,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah. So question is just about the",
      "offset": 5245.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "prompt length. Um so again not not to",
      "offset": 5247.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "dismiss that out of hand in claude terms",
      "offset": 5248.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this isn't actually that long right. I",
      "offset": 5251.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "mean this is like I said I think on",
      "offset": 5253.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "average 15,000 tokens. Um it still",
      "offset": 5254.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "leaves a lot of the window you know",
      "offset": 5256.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "behind right. it is it is not actually",
      "offset": 5258.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "so long that we start seeing really",
      "offset": 5260.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "crazy behaviors until we start doing",
      "offset": 5262.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "multiple turns like multiple",
      "offset": 5264.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "conversations in one thread right that's",
      "offset": 5266.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "where it starts to blow up um so we we",
      "offset": 5268.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "just genuinely have not gotten to a",
      "offset": 5270.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "point where we're like my guidelines are",
      "offset": 5271.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "too long you know the guidelines could",
      "offset": 5273.679,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "be a little shorter and I think we we",
      "offset": 5275.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "have optimized them in various places",
      "offset": 5276.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "over time but you know we've we've",
      "offset": 5278,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "crammed this into a box where we really",
      "offset": 5280.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "can sort of process one situation all in",
      "offset": 5281.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "one gulp without really feeling any any",
      "offset": 5283.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "pain.",
      "offset": 5285.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Okay. Yeah. Any examples on tools that",
      "offset": 5287.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you have? You mentioned tools. I'm not",
      "offset": 5290.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "sure. Yeah. Yeah. Oh, no. So, I I can I",
      "offset": 5291.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "can share a little bit of that. So, let",
      "offset": 5294,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "me uh let me go down here to the tools",
      "offset": 5295.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "code itself. So, so one thing to note,",
      "offset": 5298,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "this is a hybrid and I think I mentioned",
      "offset": 5299.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this really earlier on about um there is",
      "offset": 5301.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "some stuff coming from an MCP gateway,",
      "offset": 5303.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "right? So, in this case, you know, I'm",
      "offset": 5305.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "loading from files. It's just the file",
      "offset": 5306.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "system MCP. Again, all localized to our",
      "offset": 5308.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "VPC. So, there's nothing crazy going on",
      "offset": 5310.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "there. Um but I could instead load from,",
      "offset": 5312.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you know, my database, right? I could",
      "offset": 5314.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "choose to to have that be the place",
      "offset": 5316.159,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "where we interact. There's a bunch of",
      "offset": 5317.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "other tools, right? And so, you know,",
      "offset": 5318.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "this this actually is where probably",
      "offset": 5320.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "most of the code in my app actually is.",
      "offset": 5322.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Um, the list of tools is essentially",
      "offset": 5324.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "down here. And you can see it's things",
      "offset": 5327.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that enable interacting with the state,",
      "offset": 5329.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right? So, all of these functions that",
      "offset": 5332,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are looking at anchors and messages and",
      "offset": 5333.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "confidence and the treatment and patient",
      "offset": 5336,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "data, all of that stuff is local to my",
      "offset": 5337.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "graph run. I don't have an MCP for it. I",
      "offset": 5340.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "could, I just chose not to. Um, and you",
      "offset": 5342.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "know, in this case, like this code just",
      "offset": 5344.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "lives in this Python app. You could you",
      "offset": 5346.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "could very much refactor this out. Like",
      "offset": 5347.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "the state can still live here and the",
      "offset": 5349.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "code could be somewhere else. It's it's",
      "offset": 5350.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you know, it's up to you. Um, but one",
      "offset": 5352,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "note actually about all of this stuff is",
      "offset": 5354.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "I'm trying to find a good example here.",
      "offset": 5355.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "So I am aggressively using the command",
      "offset": 5357.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "object. Um, for anybody who who has",
      "offset": 5358.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "programmed with Langraph, the whole idea",
      "offset": 5361.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "behind this is that at any given point",
      "offset": 5362.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you're able to pass back a message and",
      "offset": 5365.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "this particular thing is just an error",
      "offset": 5366.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "message, but like you know you're able",
      "offset": 5368.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to pass back a message and a place to",
      "offset": 5369.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "go, right? So you can say here's the",
      "offset": 5371.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "here's the response and by the way I",
      "offset": 5373.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "know that the evaluator asked for this",
      "offset": 5375.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "so go back to the evaluator. You can",
      "offset": 5376.719,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "actually get around some of the graph",
      "offset": 5378,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "routing um this way. And so we we've",
      "offset": 5379.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we've definitely you know tried to work",
      "offset": 5380.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "this into both the MCP tools and the",
      "offset": 5382.48,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "state tools that we have. Yeah.",
      "offset": 5384.239,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "Yeah. Uh so question was about how to",
      "offset": 5395.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "how do we improve the prompt? So, um it",
      "offset": 5397.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "was really a fusion of um we were able",
      "offset": 5399.199,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "to go from essentially the physician's",
      "offset": 5402.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "assistant who had the most experience",
      "offset": 5405.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "with tricking things, right? Was coming",
      "offset": 5406.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "up with tricky situations, right? So, we",
      "offset": 5408.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "were able to test a lot of the edges",
      "offset": 5410.08,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "really just with her, you know, having",
      "offset": 5411.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "her pretend to be the patient. We then",
      "offset": 5412.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "scaled up to the full team of operations",
      "offset": 5414.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "associates who, you know, then tried to",
      "offset": 5416.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "trick it at a higher level, right? And,",
      "offset": 5418,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you know, were putting out things that",
      "offset": 5419.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "they'd seen from patients themselves,",
      "offset": 5420.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you know, trying to sort of um you know,",
      "offset": 5422.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "get to these complicated cases. Um, and",
      "offset": 5423.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "then, you know, with real people, you",
      "offset": 5426.159,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "know, we're able to take that a step",
      "offset": 5427.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "further. Um, but with those first two",
      "offset": 5428.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "levels, we're we're not seeing, you",
      "offset": 5430.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "know, a tremendous amount of stuff",
      "offset": 5432.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that's not expected. Again, this system",
      "offset": 5433.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "exists. If we'd been doing this from",
      "offset": 5435.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "absolute scratch, I think we would have",
      "offset": 5436.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "a lot tougher of a time coming up with",
      "offset": 5439.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "what we think the edges are. Whereas,",
      "offset": 5440.639,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "you know, this is a system that already",
      "offset": 5442.08,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "exists. There's a lot of conversations",
      "offset": 5443.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to draw from. We're able to run some of",
      "offset": 5444.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "those back. So, we'll look at at",
      "offset": 5446.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "conversations in the old system and",
      "offset": 5447.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "replay them here and essentially just",
      "offset": 5449.76,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "try to figure out, you know, where the",
      "offset": 5451.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "edges are.",
      "offset": 5452.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Is there",
      "offset": 5454.32,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "you state.",
      "offset": 5456,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Sure. So, is there a question about are",
      "offset": 5460.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the questions about um how to decide",
      "offset": 5462.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "what goes in the state or not? Um",
      "offset": 5463.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I mean the I guess the short answer is",
      "offset": 5467.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "everything that comes in in that initial",
      "offset": 5468.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "payload which I'll go back over here",
      "offset": 5470.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for. Um all of this stuff I think this",
      "offset": 5472,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "one's probably the better example. Yeah.",
      "offset": 5475.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "So all of this stuff over here this is",
      "offset": 5477.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "all state. um there's not really a",
      "offset": 5480.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "distinction like everything that comes",
      "offset": 5482.719,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "in to sort of preload the conversation",
      "offset": 5484.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is state some of it is editable um some",
      "offset": 5485.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "of it's not I'm trying to remember",
      "offset": 5488.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "examples like here examples are you",
      "offset": 5490.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "can't change the source I couldn't say",
      "offset": 5492.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you know in the context of the LM",
      "offset": 5494.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "operation this is not an ail patient",
      "offset": 5495.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "anymore right that's the LM is not",
      "offset": 5497.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "allowed to do that um it also can't",
      "offset": 5498.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "change past messages so the LM is not",
      "offset": 5500.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "allowed to look at the message queue and",
      "offset": 5502.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "say message five that went out three",
      "offset": 5503.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "days ago no longer exists like that",
      "offset": 5505.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "function doesn't exist um so we've sort",
      "offset": 5507.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of just calibrated to where the only",
      "offset": 5510.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "things it can do is read the entire",
      "offset": 5511.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "state, modify the patient data, modify",
      "offset": 5513.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "messages, modify anchors like like",
      "offset": 5516.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "unscent messages. So it's, you know,",
      "offset": 5517.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it's just a software choice like that's",
      "offset": 5520.32,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "how we architected it.",
      "offset": 5521.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "If you have a session",
      "offset": 5526.239,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "that went back",
      "offset": 5530.639,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "window",
      "offset": 5536.56,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "summarize it. Yeah. Yeah.",
      "offset": 5538.4,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "Sure. I mean I I think the I think the",
      "offset": 5546.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "real answer though is that hasn't",
      "offset": 5548.48,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "happened for us. like the way that we've",
      "offset": 5549.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "structured it. There is not a single",
      "offset": 5550.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "thinking operation that that goes long",
      "offset": 5552.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "enough to blow things up. Um but but I",
      "offset": 5554.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "will talk really quickly about um",
      "offset": 5556.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "retries. So we do have the notion and",
      "offset": 5559.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "I'll just pop this up maybe just an",
      "offset": 5560.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "easier way to see it. Um so we do have",
      "offset": 5562.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the notion inside the graph of you know",
      "offset": 5564.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "again the virtual OA talks to the",
      "offset": 5567.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "evaluator you know at at a certain a",
      "offset": 5568.719,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "certain point in the thing everybody",
      "offset": 5570.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "uses tools but then there are cases that",
      "offset": 5571.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "will cause the the graph to retry a",
      "offset": 5573.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "certain operation right so one of them",
      "offset": 5575.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is um one of the agents malforms a tool",
      "offset": 5577.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "call right it tries to call a tool but",
      "offset": 5580.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it uses the wrong JSON and you know",
      "offset": 5582.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "otherwise things would have died we can",
      "offset": 5583.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "detect that we delete the message and we",
      "offset": 5585.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "say you screwed up that tool call try",
      "offset": 5588.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "again right that you know keeps it",
      "offset": 5589.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "inside the graph, right? Essentially,",
      "offset": 5592.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "this retry node is then able to loop",
      "offset": 5593.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "back via that um command message. Uh",
      "offset": 5595.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it's not shown here on the graph, but",
      "offset": 5597.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "via that command object. It can say go",
      "offset": 5598.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "back to the virtual OA, try that again,",
      "offset": 5601.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "right? We also have some cases where",
      "offset": 5603.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "again, you know, we expect the model to",
      "offset": 5605.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "talk and it doesn't, right? There are",
      "offset": 5607.04,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "just a bunch of cases where Claude will",
      "offset": 5608.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "just end its turn prematurely. We detect",
      "offset": 5609.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that. We say you have to say something,",
      "offset": 5611.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "right? Like literally, that's the",
      "offset": 5613.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "message is like don't just say nothing.",
      "offset": 5614.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "If even if if you're going to end your",
      "offset": 5616.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "turn, just say I'm done, right? That's",
      "offset": 5617.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "enough, you know, for us to keep the",
      "offset": 5619.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "logic going. So it's things like that.",
      "offset": 5620.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Low confidence from the evaluator will",
      "offset": 5623.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "be trigger. I'm sorry, say it again. Low",
      "offset": 5624.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "confidence coming from the evaluator.",
      "offset": 5628,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "No, no. So low confidence is something",
      "offset": 5630.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we want to pass through to the human,",
      "offset": 5632.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right? So low confidence is a valid",
      "offset": 5633.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "response to the graph, right? You know,",
      "offset": 5635.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "like I have a low confidence message I",
      "offset": 5637.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "want a human to review. That's fine.",
      "offset": 5638.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Uh yeah, build your system prompt up.",
      "offset": 5641.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "It's long. It's pretty complicated. It's",
      "offset": 5644.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "not the system prompt. No, no, that's",
      "offset": 5646.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "thing. It's that's what that's why we",
      "offset": 5648.48,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "separated it. So yes, the guidelines",
      "offset": 5649.76,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "have expanded over time. Yep. Okay.",
      "offset": 5651.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Yeah. How do you make sure you don't",
      "offset": 5661.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Yeah. Perfect time to talk about evals.",
      "offset": 5662.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "So we're getting towards the end. Um let",
      "offset": 5664.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "me talk about evals a little bit just to",
      "offset": 5666.32,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "sort of give you guys a sense of what we",
      "offset": 5667.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "did here. So um this was this was a",
      "offset": 5668.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "weird one because if you go to Langmith",
      "offset": 5672.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "and I'll just I'm not sure I can find",
      "offset": 5673.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the exact place where this happens. Um",
      "offset": 5675.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "but there's is it under maybe it's under",
      "offset": 5676.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "data sets I I forget. There is a place",
      "offset": 5680,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "in here where you can basically say I",
      "offset": 5682,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "want to run you know an evaluation",
      "offset": 5683.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "against you know these these you know",
      "offset": 5685.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this this data set that I've defined in",
      "offset": 5687.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "lang. Um I do define data sets in lang.",
      "offset": 5688.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So I can see here assuming this loads up",
      "offset": 5691.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "which hopefully it will. So I've got a",
      "offset": 5692.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "happy path data set here that I defined",
      "offset": 5694.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "in length. I this isn't the whole thing.",
      "offset": 5696.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Again some of this is redacted but um if",
      "offset": 5698.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I look at these things what I'm really",
      "offset": 5700.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "doing is I'm saying okay this is an",
      "offset": 5701.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "interaction that we had in the past. Um,",
      "offset": 5703.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "you know, this was one that I ran last",
      "offset": 5705.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "week. Um, you know, here's, you know,",
      "offset": 5706.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the the input, right? This is the last",
      "offset": 5709.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "message from the patient. This",
      "offset": 5710.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "conversation, you know, a I've got this",
      "offset": 5712.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "initial state here that I can look at,",
      "offset": 5714.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "right? So, I can see, you know, what was",
      "offset": 5715.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "going on in the first place. Um, I see",
      "offset": 5716.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this conversation and, you know, I get",
      "offset": 5719.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to the end and I get a message out which",
      "offset": 5721.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "says, great, you're ready to start.",
      "offset": 5723.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Okay, so this is one part of my happy",
      "offset": 5724.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "path data set. The eval that I'm running",
      "offset": 5727.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "um are fundamentally it's a it's a",
      "offset": 5729.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "custom harness. I'll blow this up a",
      "offset": 5732.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "little bit. Hopefully, it's visible to",
      "offset": 5733.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "most folks. Um, but again, the idea here",
      "offset": 5734.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "was that we couldn't just say, you know,",
      "offset": 5738.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "when I ask for, you know, what the",
      "offset": 5741.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "weather is in San Francisco, it gives me",
      "offset": 5743.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "back, you know, cold and, you know,",
      "offset": 5744.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "foggy. Um, it had to be here's examples",
      "offset": 5746.239,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "of these input states and then, you",
      "offset": 5750.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "know, let's evaluate it, you know, sort",
      "offset": 5752.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "of an LLM as a judge form what the",
      "offset": 5754,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "output state looks like with the caveat",
      "offset": 5755.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "that one of the big things we wanted to",
      "offset": 5757.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "test was things like time operations.",
      "offset": 5758.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "So, I can't put in an eval from three",
      "offset": 5760.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "weeks ago and run it now and get",
      "offset": 5761.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "equivalent times. I have to either give",
      "offset": 5764,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it really specific guidance on how to",
      "offset": 5765.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "handle time or I have to replace all the",
      "offset": 5767.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "time stamps. Um, we ended up sort of",
      "offset": 5768.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "doing a hybrid of both. Um, and so what",
      "offset": 5770.239,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this did was my eval suite is a custom",
      "offset": 5772.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Python app stapled to a bash script.",
      "offset": 5776.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "This was all client coded. Um, I I got",
      "offset": 5778.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "what I wanted, but I can't vouch for",
      "offset": 5780.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "much more than that. Um, I load data",
      "offset": 5782.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "sets right from Langmith. I call",
      "offset": 5784.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "essentially the medical agent via in",
      "offset": 5787.36,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "this case like I'm running this locally",
      "offset": 5789.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like I could run this against my cloud",
      "offset": 5790.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "Lang graph instance but I'm literally",
      "offset": 5792,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "running this against Lang graph on my",
      "offset": 5793.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "laptop using that data set from",
      "offset": 5794.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Langsmith having pre-processed a bunch",
      "offset": 5796.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "of stuff around dates times you know",
      "offset": 5799.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sort of circumstances so that when I get",
      "offset": 5801.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "back the result I can not confuse the LM",
      "offset": 5803.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "as a judge about whether it's right or",
      "offset": 5805.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "not. Um and I'm using this LLM rubric",
      "offset": 5806.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "right to do this. And so let me see if I",
      "offset": 5809.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "can find my rubric here. So well here's",
      "offset": 5811.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "here's a couple examples. So I've got",
      "offset": 5814.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "this this YAML. So this is just this is",
      "offset": 5815.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "prompt fu and how it works. Um what I do",
      "offset": 5817.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is I basically say look I'm trying to",
      "offset": 5819.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "test you know this this custom thing",
      "offset": 5822.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that I'm going to call you know",
      "offset": 5824,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "essentially with with my um you know my",
      "offset": 5825.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "my custom harness and then I want you to",
      "offset": 5827.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "evaluate it you know with an LLM and in",
      "offset": 5829.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this case I think it's using GPT40. Um",
      "offset": 5830.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you know this the valuation rubric is",
      "offset": 5833.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "basically is everything basically",
      "offset": 5836,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "exactly the same? Do I see minor",
      "offset": 5837.84,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "discrepancies but I don't think they're",
      "offset": 5839.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "a big deal. I mean we're we're keeping",
      "offset": 5840.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "this very fuzzy. What we really want to",
      "offset": 5841.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "know is is anything completely busted,",
      "offset": 5843.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "right? And then there are cases where it",
      "offset": 5845.36,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "is. Um I had to put in specific notes",
      "offset": 5846.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "here like hey don't be picky about like",
      "offset": 5848.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "the you know different wording that you",
      "offset": 5850.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "might see in something like an anchor",
      "offset": 5851.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "right sometimes it'll say they will take",
      "offset": 5852.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the medicine it says they did take the",
      "offset": 5854.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "medicine like who cares like in this",
      "offset": 5855.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "case the spirit of it was right. Um and",
      "offset": 5857.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so you know and then times and dates",
      "offset": 5859.84,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like there's some specific language",
      "offset": 5861.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "here. So all of this turns into",
      "offset": 5862.48,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "basically this guy right here. So, um,",
      "offset": 5866,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "when I look at this, and again, I",
      "offset": 5869.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "realize there's a lot of text here. Um,",
      "offset": 5870.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it's not worth looking at all of it. I",
      "offset": 5872.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "ran these three examples from my data",
      "offset": 5874.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "set and I got, you know, basically a",
      "offset": 5875.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "passing grade, right? I'll I'll go into",
      "offset": 5878.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "the fail, one fail, one pass in a",
      "offset": 5879.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "second, but in each of these cases,",
      "offset": 5881.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "right, I can see what the LM as a judge",
      "offset": 5882.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "said, right? So, if I go down here, this",
      "offset": 5884.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is GPT40",
      "offset": 5887.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "going, you know, opining on, well, this",
      "offset": 5889.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "is the source data you gave me in the",
      "offset": 5891.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "sample output. Here's the run I just",
      "offset": 5892.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "did. Close enough, right? um you know it",
      "offset": 5894,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "does point out some things right so if I",
      "offset": 5897.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "ran these evals and I was like well it's",
      "offset": 5898.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "actually a big problem that you know the",
      "offset": 5900.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "reminder you know unscent message didn't",
      "offset": 5901.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "show up right I I could make a choice to",
      "offset": 5903.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "do that I could I could strengthen my",
      "offset": 5905.679,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "rubric but the way that we did this was",
      "offset": 5906.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "just to say look at any given point we",
      "offset": 5908.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "do need to be able to test the current",
      "offset": 5910.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "state of the system we want to do it",
      "offset": 5912.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "against you know first the happy path",
      "offset": 5913.76,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "and then we can certainly do it against",
      "offset": 5915.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "edge cases um we're actively maintaining",
      "offset": 5916.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this I think this might change once we",
      "offset": 5918.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "actually hand this over you know more",
      "offset": 5919.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "fully to the client right we want them",
      "offset": 5921.119,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "to have all the protection they might",
      "offset": 5922.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "need but it's this style right this",
      "offset": 5924.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "style of email. Does that answer your",
      "offset": 5925.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "question? More or less. Okay.",
      "offset": 5927.52,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "Okay. Um, yes. Go ahead.",
      "offset": 5930.239,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "Yeah. So, I mean, the short answer is",
      "offset": 5940.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "yes. Some of that I'm redacting for for",
      "offset": 5941.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "a couple reasons, but yeah, more or less",
      "offset": 5943.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we we started with a happy path. We do",
      "offset": 5945.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "have a handful of of specific like, hey,",
      "offset": 5946.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "this is busted and it's frequently",
      "offset": 5948.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "busted. Let's make sure it's not um you",
      "offset": 5949.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "know that, but it's just a different",
      "offset": 5951.76,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "data set. Yeah. Uh yeah.",
      "offset": 5953.04,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "Well, so again, the the part that I'm",
      "offset": 5966.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "showing here is entirely Python in a",
      "offset": 5968.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "line graph container. Um I would guess",
      "offset": 5970.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "it's, you know, maybe 4,000 lines of",
      "offset": 5973.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "code. Um most of it is honestly the tool",
      "offset": 5975.119,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "calls like that that just and I I'm sure",
      "offset": 5976.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "I could refactor that to be shorter",
      "offset": 5978.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "also. Um it's not much. It's really just",
      "offset": 5979.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "enough to run essentially this graph,",
      "offset": 5982.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? You know, I have to have the",
      "offset": 5984.96,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "routing between all of it. I have to",
      "offset": 5986.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "have the tools that it can call. Um",
      "offset": 5987.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "everything else is in the prompts and",
      "offset": 5988.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the guidelines, right? And so, you know,",
      "offset": 5990.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it really is more English than it is",
      "offset": 5992.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "code. On the other side, right, on the",
      "offset": 5994.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "other side of the box, right, this",
      "offset": 5996.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "thing, um that blue box is I don't know",
      "offset": 5997.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "exactly how much code, but it's entirely",
      "offset": 6000.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "um Node and React and and Um and",
      "offset": 6002.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and frankly, I haven't been that",
      "offset": 6005.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "involved in it. Sorry. Yep. I also",
      "offset": 6006.56,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "industry.",
      "offset": 6011.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6014.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "How do you think about this one?",
      "offset": 6016,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6020.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah. Great question. So, uh, questions",
      "offset": 6028.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "about scale. So, let me actually jump",
      "offset": 6030.4,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "over and show you one thing I should",
      "offset": 6031.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "have probably already shown, but I I",
      "offset": 6032.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "forgot. I mentioned before the idea of",
      "offset": 6034.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "us doing different treatments. So, um,",
      "offset": 6036.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "what I did in in in this particular",
      "offset": 6039.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "case, so you know, again, we were",
      "offset": 6041.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "focused on on the the early pregnancy",
      "offset": 6042.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "loss. Um, I did this, in fact, I think I",
      "offset": 6044.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "have this sitting here somewhere. So,",
      "offset": 6047.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "let me zoom out and find it and then",
      "offset": 6049.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I'll zoom back in. So, I took this to",
      "offset": 6050.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Klein and I basically said to Klein,",
      "offset": 6052.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "&quot;Hey, I've got Yeah, this should be it",
      "offset": 6054.32,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "right here.&quot; Um, I said, &quot;I'm looking to",
      "offset": 6056.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "make a new treatment, right? I defined",
      "offset": 6060.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "one for Aila. Here's the structure,",
      "offset": 6061.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "right? Have a look. Um, here's the link",
      "offset": 6063.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to, you know, Noon Nordisk's suggestions",
      "offset": 6065.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "on how to dose Osmpic. Um, make me",
      "offset": 6068,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "something new, right? Um, and it",
      "offset": 6070.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "basically went through this process, and",
      "offset": 6072.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I'll I'll shrink this down. Um, and",
      "offset": 6074.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "created, you know, a basic treatment,",
      "offset": 6076.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "you know, for Osmpic. It read these",
      "offset": 6078.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "files. It then decided, all right, I got",
      "offset": 6080.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it. Here's the thing I'm going to do.",
      "offset": 6082.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "Um, I just said, cool, go for it. And",
      "offset": 6083.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "here's a couple of tweaks based on the",
      "offset": 6086,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "thing that you said. So, I, again, this",
      "offset": 6087.199,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "is my client process. I use this all the",
      "offset": 6088.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "time. Um, and I ended up with what I",
      "offset": 6089.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think is a pretty serviceable treatment,",
      "offset": 6092.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "which I will very quickly show you here.",
      "offset": 6093.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "If I go back to these conversations, I'm",
      "offset": 6095.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "pretty sure I named these people all O.",
      "offset": 6098.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "So, there you go. Here's Oliver with",
      "offset": 6100.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Osmpic. Um, and you can see it's still",
      "offset": 6101.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Ava. I didn't change that, right? I I I",
      "offset": 6104.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you could obviously get to the point of",
      "offset": 6106.8,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "having it be a different personality,",
      "offset": 6107.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "but in this case, this is Ava with a new",
      "offset": 6109.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "treatment asking all about Ozmpic pens",
      "offset": 6111.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and helping me figure out their time.",
      "offset": 6113.84,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Same as we were before. It's a weekly",
      "offset": 6115.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "injection. I didn't change any code for",
      "offset": 6117.119,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "this. I literally threw this through",
      "offset": 6118.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "client got a new set of treatments out",
      "offset": 6120.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and it just kind of works. Yes. Yeah. In",
      "offset": 6122.32,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "your graph",
      "offset": 6125.76,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "that for it's for catching uh the retry",
      "offset": 6129.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "node. The question was um it's for",
      "offset": 6132.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "catching those errors around um",
      "offset": 6133.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "malforming tool calls is one easy way",
      "offset": 6135.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "for the graph to terminate. Right? So if",
      "offset": 6136.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it you know forgets a bracket and you",
      "offset": 6138.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "know sends back something that's invalid",
      "offset": 6140.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "JSON. Um Claude does this a lot less",
      "offset": 6141.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "now. Like Sonnet 4 is pretty good at it",
      "offset": 6144.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "but um Sonnet 3.5 was not nearly as",
      "offset": 6145.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "good. So, we can detect that and we can",
      "offset": 6148,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "just say, well, you're trying to make a",
      "offset": 6150.159,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "tool call because I see certain things",
      "offset": 6151.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "in here. I either see tool call ID as a",
      "offset": 6152.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "parameter. I see weird brackets, you",
      "offset": 6154.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "know, we can parse that in code. Um, and",
      "offset": 6156.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "then again, we wipe that message out and",
      "offset": 6158.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "we go back to the guy that called it and",
      "offset": 6160.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "said, &quot;Hey, you up, pardon my",
      "offset": 6161.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "French.&quot; Um, you know, try again. And",
      "offset": 6162.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "so, that idea of, you know, the retry",
      "offset": 6165.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "node, there's a handful of those",
      "offset": 6167.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "situations where we aren't going to take",
      "offset": 6168.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "over and do it in some deterministic",
      "offset": 6170.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "way. We're just telling the LM, you made",
      "offset": 6172.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "a mistake. Here's the character of your",
      "offset": 6174,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "mistake. Try again. And we do have to",
      "offset": 6175.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "wipe out its memory of that mistake",
      "offset": 6178.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "because it can get very confused. Like",
      "offset": 6180,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "one of the reason, one of the ways this",
      "offset": 6181.36,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "happened a lot earlier in our testing",
      "offset": 6182.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "was it would malform tool calls and then",
      "offset": 6183.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "hallucinate the results, right? And so",
      "offset": 6185.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "if you left the message in there, it",
      "offset": 6187.92,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "would think that it understood the",
      "offset": 6189.6,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "blueprint even though it had made the",
      "offset": 6190.639,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "entire blueprint up, right? So I don't",
      "offset": 6191.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "want to sugarcoat where like there are",
      "offset": 6193.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "some weird cases that if you don't very",
      "offset": 6194.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "carefully control for it, you can end up",
      "offset": 6196.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "with some very bad behaviors. But we",
      "offset": 6197.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "were able to catch the main ones and",
      "offset": 6199.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "essentially just give it another shot. I",
      "offset": 6200.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "was looking for like a loop. Oh, all",
      "offset": 6202.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "right. Sorry. The reason there's no loop",
      "offset": 6204.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "this this is just totally this is",
      "offset": 6205.92,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "actually one thing where I think",
      "offset": 6207.36,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Langmith and Lang graph could be better",
      "offset": 6208.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "at this. The retry node is capable of",
      "offset": 6209.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "calling back to the other ones using the",
      "offset": 6212.159,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "command object but it doesn't show up on",
      "offset": 6213.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the graph. So it turns out like they",
      "offset": 6215.119,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "were very invested Langmith was or Lang",
      "offset": 6216.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "graph in graph flows and then they",
      "offset": 6218.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "introduced this idea of well you don't",
      "offset": 6221.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "even really have to define it in the",
      "offset": 6222.8,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "graph you can send anything anywhere and",
      "offset": 6223.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so that's what we're using.",
      "offset": 6225.199,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6227.52,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6233.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "No, sure. But remember, sorry, the",
      "offset": 6244.48,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "question was about confidence scoring",
      "offset": 6245.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "and how do we get it to sort of to be",
      "offset": 6246.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "higher. Um, we want the score to be",
      "offset": 6248.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "lower when there is a complicated",
      "offset": 6250.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "situation, not because we think it's",
      "offset": 6253.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "wrong, but because we want a human to",
      "offset": 6254.56,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "review it.",
      "offset": 6256.08,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "Um, well, so sorry, what do you mean?",
      "offset": 6261.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Correct. Yes.",
      "offset": 6267.44,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "So if if the confidence score is above",
      "offset": 6272.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the threshold, meaning higher than it,",
      "offset": 6274.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "right? So let's say I have a confidence",
      "offset": 6275.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "score of 0.9. What that could mean,",
      "offset": 6276.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "right, is that I am sending multiple",
      "offset": 6279.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "messages at a time, but there's no other",
      "offset": 6281.28,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "reason for me to think that those",
      "offset": 6282.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "messages are wrong. Um, we have chosen",
      "offset": 6283.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "with the client to set the threshold",
      "offset": 6286.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "lower than that because they don't want",
      "offset": 6288.159,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "their humans having to get involved",
      "offset": 6289.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "every single time something is a little",
      "offset": 6291.119,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "complicated. But we've agreed that if",
      "offset": 6292.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's below 0 75, they should. It's",
      "offset": 6294.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "purely just a calibration, right? It's",
      "offset": 6296.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you you decide if you wanted your humans",
      "offset": 6298.239,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "involved in everything, set the",
      "offset": 6299.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "conference score to 100, right? You",
      "offset": 6300.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "could see every single thing that",
      "offset": 6302.56,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "happens. You're just clicking approve",
      "offset": 6303.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "all day. You're George Jetson. But um",
      "offset": 6304.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that's hopefully not what people",
      "offset": 6307.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "actually want, right? You want a bunch",
      "offset": 6308.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of these things if they are high",
      "offset": 6310.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "confidence and you know, fundamentally",
      "offset": 6311.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "recoverable, let's say. Like, you know,",
      "offset": 6314.08,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "there might be certain circumstances",
      "offset": 6315.52,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "where you don't want a message",
      "offset": 6316.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "automatically sent out. Hopefully, we",
      "offset": 6317.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "can control for that. But in general",
      "offset": 6319.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "like we wanted to set it in a place that",
      "offset": 6321.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "felt like we were going to get some",
      "offset": 6322.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "scale out of our humans, right? Which",
      "offset": 6324.32,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "meant messages going out automatically,",
      "offset": 6325.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "right? Uh yes.",
      "offset": 6330.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Yeah. trying to evaluate",
      "offset": 6345.52,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "whe",
      "offset": 6350.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6352.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Yeah. So the question is about",
      "offset": 6355.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "conflation of of confidence scoring and",
      "offset": 6356.159,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "complexity. Yes. 100% agree. And again",
      "offset": 6358.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "it's one of the things where I think",
      "offset": 6361.119,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "we're happy with how it sort of operates",
      "offset": 6362.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "now but I'm not sure that we're doing",
      "offset": 6364.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the confidence piece right. Um and at",
      "offset": 6366.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the same time like the concept of it I",
      "offset": 6369.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "think is is good right? You know do you",
      "offset": 6370.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "have the information you need? you know,",
      "offset": 6372.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "is is there anything like do you",
      "offset": 6374.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "understand the user's intent or you",
      "offset": 6375.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "know, did they say something ambiguous?",
      "offset": 6377.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "And we do see cases where it triggers,",
      "offset": 6378.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "right? It's not that we never see it,",
      "offset": 6380.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you know, correctly rate itself as being",
      "offset": 6381.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like, well, I'm not totally sure what",
      "offset": 6383.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "they said, but it's not as frequent as",
      "offset": 6384.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we would like. So, we combine it with",
      "offset": 6386.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "complexity in part because, you know, we",
      "offset": 6387.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "just want to get it below that threshold",
      "offset": 6389.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so that we can have a human review it.",
      "offset": 6391.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "It's not a perfect system, but it is a",
      "offset": 6392.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "system that we think works. So it's more",
      "offset": 6394.639,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "confidence.",
      "offset": 6396.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Correct.",
      "offset": 6403.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Uh correct. So to to that point, it's",
      "offset": 6406,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it's not confidence that the specific",
      "offset": 6408.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "response is exact and perfect and",
      "offset": 6410.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "whatever it is, it's confidence that we",
      "offset": 6412.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "don't think that there's a blend of",
      "offset": 6414.639,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "uncertainty on, you know, what the",
      "offset": 6416.239,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "response is and complexity of the",
      "offset": 6417.679,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "situation, right? Either of those things",
      "offset": 6419.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "can push it below the threshold.",
      "offset": 6420.639,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "How are you hosting?",
      "offset": 6423.76,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "Uh so this is all uh the question was",
      "offset": 6428.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "about hosting. So this is all um",
      "offset": 6430.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Langraph has a pre-built",
      "offset": 6432.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "containerization that you can use. We",
      "offset": 6434.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "are using it with a couple of",
      "offset": 6436.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "modifications. Um we are essentially",
      "offset": 6437.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "deploying both halves of this right so",
      "offset": 6440.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to go back to this we're deploying both",
      "offset": 6442,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "halves of this with Terraform. You know",
      "offset": 6443.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "everything's hooked up with GitHub",
      "offset": 6445.119,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "actions. I mean like you know we're",
      "offset": 6446.239,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "we're doing as much of as automatically",
      "offset": 6447.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "as we can but we are using most of the",
      "offset": 6448.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "built-in line graph containerization. I",
      "offset": 6450.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "thought there was a",
      "offset": 6453.199,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "platform",
      "offset": 6456,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "you do know I mean we're we we have a",
      "offset": 6460.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "conversation with with Langchain",
      "offset": 6462.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "actively about that. So the question",
      "offset": 6463.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "actually is also um the the specific",
      "offset": 6464.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "nature of how we need to be deployed. Um",
      "offset": 6467.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Langraph platform doesn't currently",
      "offset": 6469.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "support that right. So like there it's",
      "offset": 6470.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's all it's all evolving but um we we",
      "offset": 6472.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "have had those conversations Um, let me",
      "offset": 6474.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "pause for a second. We still have 10",
      "offset": 6476.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "minutes. Um, and so, or roughly 10",
      "offset": 6478.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "minutes, nine minutes. Um, let me just",
      "offset": 6480.08,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "double check my own list of things that",
      "offset": 6481.84,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "I wanted to talk about just to make sure",
      "offset": 6483.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "I didn't miss anything major. Um, talked",
      "offset": 6484.239,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "about that, talked about that.",
      "offset": 6487.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "All right. I I think we more or less hit",
      "offset": 6490.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "everything. Um, and and so I'm happy",
      "offset": 6492.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "just to open it up. Um, I have a couple",
      "offset": 6494.639,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "of final thoughts that I'll leave",
      "offset": 6495.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "sitting up here um if anyone's curious.",
      "offset": 6496.88,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "Um, any other questions?",
      "offset": 6499.199,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 6504.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Yeah. Uh so the question was just about",
      "offset": 6519.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "uh resource allocation to build these",
      "offset": 6520.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "things. So I mean I think the best way",
      "offset": 6522.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to say it is just that we had built a",
      "offset": 6524.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "handful of things like this. We had not",
      "offset": 6527.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "done it for healthcare, right? But we",
      "offset": 6529.52,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "were able to bring some things in in",
      "offset": 6531.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "terms of you know I had an open source",
      "offset": 6532.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Langraph project that I was comfortable",
      "offset": 6534.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "using as a base for for this client code",
      "offset": 6536.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "right again we we forked it we brought",
      "offset": 6537.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it in private um you know but it got us",
      "offset": 6539.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "you know part of the way there in in",
      "offset": 6541.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "part because this isn't a totally",
      "offset": 6543.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "special snowflake it's it's a workflow",
      "offset": 6544.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with tools so you know we have I think",
      "offset": 6546.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "spent less time on that upfront",
      "offset": 6549.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "scaffolding each time we've done it um",
      "offset": 6551.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and you know we're looking at another",
      "offset": 6553.44,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "project right now which would be that",
      "offset": 6554.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "much quicker so a lot of it is just once",
      "offset": 6555.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you do this and you understand the",
      "offset": 6558,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "mechanism getting to you know good",
      "offset": 6558.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "enough or getting to a starting point is",
      "offset": 6560.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "just a lot quicker. Um, it is something",
      "offset": 6561.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "though where like you know again we we",
      "offset": 6564.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "are we are still working on this and",
      "offset": 6566.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "we've been at it for a few months but I",
      "offset": 6567.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "think we probably built what would have",
      "offset": 6569.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "been a year's worth of conventional",
      "offset": 6570.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "software maybe more right you know in",
      "offset": 6572.719,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "that short period of time",
      "offset": 6575.119,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "anything else yeah so you mentioned you",
      "offset": 6579.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "did a lot of coding this can a little",
      "offset": 6581.119,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "bit I'm curious how much",
      "offset": 6584.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "lang",
      "offset": 6587.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 6589.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "well okay so so the uh the question was",
      "offset": 6590.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "is about uh vibe coding and how much the",
      "offset": 6592.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "tools know of these frameworks. So the",
      "offset": 6594.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "single biggest issue with vibe coding,",
      "offset": 6596.32,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "right, is when you're dealing with",
      "offset": 6597.679,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "something new enough that the models",
      "offset": 6598.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "don't really understand it. Um you can",
      "offset": 6600,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "always just send them the API docs and",
      "offset": 6602.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "they usually do pretty well. Um I ran",
      "offset": 6604,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "into plenty of places with Langraph",
      "offset": 6606.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "where nobody had tried to do this",
      "offset": 6608,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "before, no one had this exact bug and we",
      "offset": 6609.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "just had to actively debug it back and",
      "offset": 6611.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "forth. Um I can definitely endorse 03 is",
      "offset": 6613.119,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a better debugger than Claude. Um so you",
      "offset": 6615.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "know we we did have cases where we had",
      "offset": 6617.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "to do that. Um, but for the most part,",
      "offset": 6619.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like I mean, as long as you can at the",
      "offset": 6620.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "docks, you know, you you can get to a",
      "offset": 6623.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "reasonable place. And again, like I am a",
      "offset": 6625.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "former engineer. I I may eventually call",
      "offset": 6627.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "myself a current engineer, but I I kind",
      "offset": 6629.04,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "of wouldn't right now. Like I don't have",
      "offset": 6630.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "all the same practices and sort of all",
      "offset": 6631.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the same hygiene, you know, that our our",
      "offset": 6633.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "professional engineers do. But I do know",
      "offset": 6634.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "how to smell kind of bad behavior. And",
      "offset": 6637.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "I'm pretty good at prompting to get what",
      "offset": 6639.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I want. So that that's kind of how it",
      "offset": 6640.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "how it worked out.",
      "offset": 6642.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Um, any Oh, yeah. Please. In this",
      "offset": 6645.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "example,",
      "offset": 6647.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "No, there is MCP. MCP is is in a couple",
      "offset": 6651.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "places, right? So, if you look at the",
      "offset": 6653.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "connection there, there's actually two",
      "offset": 6656.159,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "and one of them I just didn't totally",
      "offset": 6657.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "label the blueprint knowledge base at",
      "offset": 6658.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the top that yellow box that is an MCP",
      "offset": 6660.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "connection in the Langraph context and",
      "offset": 6662.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "then vertically there's another set of",
      "offset": 6665.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "connections back to the database. So,",
      "offset": 6667.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "there's a couple places where it",
      "offset": 6668.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "happens, but again, it's mostly for the",
      "offset": 6670,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "inter the exchange of state and it's for",
      "offset": 6672.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "this, you know, reading of documents.",
      "offset": 6674.48,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "That's primarily where it happens.",
      "offset": 6675.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 6687.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Yep. Absolutely.",
      "offset": 6688.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6691.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Yep. So the question is about rapid fire",
      "offset": 6692.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "text messages. So yes, the the way we",
      "offset": 6694.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "handle that is is twofold. So one is",
      "offset": 6696.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "that um we do have a configurable I'm",
      "offset": 6698.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "not totally sure we have this turned on,",
      "offset": 6701.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "but we have the idea of a configurable",
      "offset": 6702.639,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "delay before we actually send it for",
      "offset": 6704,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "processing. So if someone is going to",
      "offset": 6705.52,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "send five text messages in a row, we",
      "offset": 6707.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "wait five seconds before doing anything,",
      "offset": 6708.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "right? Um that's one way we can catch",
      "offset": 6710.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it. But the other way is um we will",
      "offset": 6712.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "invalidate previous running threads if",
      "offset": 6714.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "someone texts in afterwards, right? We",
      "offset": 6717.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "don't want an inprocess response. We",
      "offset": 6719.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "want to take whatever the complete",
      "offset": 6721.599,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "context of the conversation was, send",
      "offset": 6723.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "all of that in, and then we respond to",
      "offset": 6725.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "five messages at once, right? So it's a",
      "offset": 6727.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "combination of smart retries and smart",
      "offset": 6729.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "invalidation.",
      "offset": 6731.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6739.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 6743.28,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6746.239,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Well, this question was about rogue",
      "offset": 6752,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "responses.",
      "offset": 6753.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Well, in general, we are we are giving",
      "offset": 6757.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "some pretty basic guidance around you're",
      "offset": 6759.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "only here to answer treatment related",
      "offset": 6761.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "questions. If someone wants to talk to",
      "offset": 6762.96,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "you about the weather, you just say,",
      "offset": 6764.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "&quot;I'm sorry. I can't help with that.&quot; Um,",
      "offset": 6765.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "you know, or other worse things. Um, so",
      "offset": 6766.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "generally speaking, that works pretty",
      "offset": 6769.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "well. Um, we have it pretty well tuned",
      "offset": 6770.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to escalate, right? If someone just is",
      "offset": 6773.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "essentially going off the rails and you",
      "offset": 6775.76,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "can't think of a response, you just say,",
      "offset": 6777.119,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "&quot;I'm sorry, I'm going to get somebody to",
      "offset": 6778.159,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "help you.&quot; And it sets the confidence",
      "offset": 6779.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "low and then a human can get involved.",
      "offset": 6780.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Um, it doesn't in practice happen that",
      "offset": 6782.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "much. I mean, again, if you're involved",
      "offset": 6784.08,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "in this, like, you know, if you're",
      "offset": 6785.28,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "involved in this and you take the time",
      "offset": 6786.719,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "to actually engage with the system, you",
      "offset": 6787.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "want the result and you probably want to",
      "offset": 6789.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "get back to your life. So we don't see a",
      "offset": 6790.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "tremendous amount of it but but that's",
      "offset": 6792.48,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the way that we would deal with it.",
      "offset": 6793.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "at any point in your development process",
      "offset": 6797.679,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "you get frustrated enough",
      "offset": 6799.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "uh yeah did did I get frustrated enough",
      "offset": 6804.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with Lang graph um at any point honestly",
      "offset": 6805.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so the the one thing I'll say and I this",
      "offset": 6808.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "this is just totally a personal project",
      "offset": 6810.159,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "um I was doing a side thing um around",
      "offset": 6812.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "college counseling um and I just have",
      "offset": 6815.119,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this sitting here because I've showed",
      "offset": 6817.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "off sometimes um and the point was I I",
      "offset": 6818.719,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "went into this project explicitly trying",
      "offset": 6822.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to avoid it I said I'm going to do a",
      "offset": 6824.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "very similar thing where I have an AI in",
      "offset": 6825.84,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "the middle of a workflow and I wanted to",
      "offset": 6827.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "ask questions and I wanted to think and",
      "offset": 6828.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "I do not want to use Lang graph or crew",
      "offset": 6830.239,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "or any of these other frameworks because",
      "offset": 6831.84,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "I don't want to be dependent on them. Um",
      "offset": 6832.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and so I asked you know I asked Klein to",
      "offset": 6834.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "write me a layer that was pretty good at",
      "offset": 6836.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you know uh talking to these models and",
      "offset": 6839.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "structuring a thinking process and it it",
      "offset": 6841.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "did fine. I mean the reason to have the",
      "offset": 6843.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "framework is in part because you know",
      "offset": 6845.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "again we won't be with this client",
      "offset": 6847.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "forever. We want them to have something",
      "offset": 6848.8,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "they can operate. we want them to have",
      "offset": 6849.92,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "something explainable and easy to use",
      "offset": 6851.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like this is all you know logs in Google",
      "offset": 6852.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "cloud like it's not the most fun",
      "offset": 6854.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "experience so you know it is very much",
      "offset": 6856.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "choose your tools wisely you know good",
      "offset": 6858.639,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and bad like you get a better experience",
      "offset": 6860.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "with the other ones speaking on that",
      "offset": 6861.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "uh uh thank you so much for asking me",
      "offset": 6866.239,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "that question um I I so clin over cursor",
      "offset": 6868.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "um I I will admit that cursor and and",
      "offset": 6871.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "windsurf is great by the way like I mean",
      "offset": 6874.56,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "there's a handful of them that I really",
      "offset": 6876.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "do like um cursor and windsurf as as two",
      "offset": 6877.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "examples are are are just trying to hit",
      "offset": 6880.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this sort of narrow um you know thing",
      "offset": 6882.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "about it has to cost $20 a month and",
      "offset": 6884.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "therefore it has to be heavily optimized",
      "offset": 6886.159,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "about how it sends tokens in different",
      "offset": 6887.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "places otherwise their economics blow",
      "offset": 6889.199,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "up. Um Klein doesn't do that. It's very",
      "offset": 6890.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "simple. It's really I mean it's very",
      "offset": 6893.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "smart but like it's just giving tools to",
      "offset": 6895.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "a smart model and that smart model can",
      "offset": 6897.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "cost whatever it costs. Klein is the",
      "offset": 6898.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "spiritual cousin to claude code and",
      "offset": 6900.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "codecs, right? I mean it's that style of",
      "offset": 6901.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "thing as opposed to an IDE that has to",
      "offset": 6903.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "hit this very narrow target and has to",
      "offset": 6905.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "do a lot of pre-optimization on how the",
      "offset": 6906.88,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "the tokens flow. Yeah.",
      "offset": 6908.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Yeah. So, I mean, honestly, this is also",
      "offset": 6919.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "another place where I will raise my hand",
      "offset": 6922.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "and say this is why I'm not a real",
      "offset": 6924.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "software engineer. I I don't have a",
      "offset": 6925.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "robust testing framework on my Python",
      "offset": 6927.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "code. I I haven't really needed it,",
      "offset": 6929.599,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "right? Or at the very least, like I'm",
      "offset": 6931.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "using eval and sort of overall",
      "offset": 6933.199,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "performance of the system as the better",
      "offset": 6934.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "benchmark, right? So, eval are",
      "offset": 6935.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "important. We have to have those. Um I",
      "offset": 6937.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "you know the other side of the code like",
      "offset": 6939.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you know Stride is a TDD shop like our",
      "offset": 6940.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "our our software engineers are very very",
      "offset": 6942.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "good at test-driven development and so",
      "offset": 6944.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the blue box is very well tested. Um but",
      "offset": 6946.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "just my code you know very much is is",
      "offset": 6949.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "eval sort of tested instead. Um it's not",
      "offset": 6951.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "a great answer but like that that is",
      "offset": 6953.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "kind of how I thought about it.",
      "offset": 6954.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "All right. Um I think we are at time.",
      "offset": 6958.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Thank you guys so much. This was",
      "offset": 6960.239,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "awesome. Um I'll be here if you want to",
      "offset": 6961.52,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "stick around.",
      "offset": 6962.96,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 6967.4,
      "duration": 3.22
    }
  ],
  "cleanText": "[Music]\n\nOkay. Um, hey everybody. Thank you so much for coming. Uh, really appreciate you being here. Um, this is a great show. I love this show. Um, I was here last year as an attendee. Um, spoke in New York at the New York Summit in February, and I'm really thrilled to be back. Um, so this is very much a show-and-tell. I said this in the Slack channel, so anybody's not in the Slack channel, feel free to join it. There's a couple links in there that might be helpful to you. Um, it is workshop LangGraph MCP agents if anybody needs that. Um, but fundamentally, uh, I'm just here to walk through some really interesting work, um, that my team's been doing around, um, building agent workflows for, uh, a healthcare use case. Um, and, uh, this is very much like the way we did it. Um, I'll get into some details about that. It's not the only way to do it. Um, and I'm hopeful that somebody in this audience might look at this and be like, \"That's dumb. You should do that better.\" And, you know, please raise your hand and tell me. Um, but, uh, but it's been really fun to build. I'm really, really happy with the results. And, uh, really excited to show you guys, um, what it's all about. Um, okay. So, I will go into presentation mode.\n\nAll right. Here we go. Okay. Uh, first, just a very quick couple things about Stride. Um, that's me if anybody needs my LinkedIn, but um, there's a couple other places you can find that. Um, we are a custom software consultancy. Um, so what that means in practice is, uh, whatever you need, we'll build it. We have been doing a whole lot of AI stuff. Um, this kind of falls into a few specific buckets. Um, we use a lot of AI for code generation. Um, we have a couple of both products and services we've built to do things like, uh, unit test creation and maintenance. We've done a bunch of stuff around, uh, modernization of super old, dumb code bases. Um, you know, things like, uh, you know, early 2000s era .NET is one of the things we specialize in. Um, but what I'm going to show you today is really, uh, what we do around agent workflows. So, the idea with, you know, this agent workflow stuff is really just that, you know, it's something that could be done with traditional software, and this thing I'm going to show you was done with traditional software in its first run. Um, but we have rebuilt it with an LLM at the core to make it more flexible, more capable, and, you know, ultimately, um, just a lot cooler. Um, so, uh, really excited to show you guys more about that. So, I'm going to start with a little bit of grounding. Um, I'm going to do a case study, um, which is very brief, but I'll give you a sense of kind of the problem we were trying to solve and how well I think we solved it. Um, and then we'll go as deep as we all want to go in terms of how it works. Um, let me ask this up front. If you have questions, please raise your hand. I will try to notice and I'll try to get to you. Um, there are some mics we could pass around. Um, but it probably is better for you just to shout it out and then I'll repeat it, um, into my mic. Um, and, and fundamentally, like, I have no idea if this is two hours worth of material. It probably is, but, you know, please, uh, keep me honest. Um, I'll talk about anything that is relevant to this that you guys want to talk about. So, um, the client here is Avila. So, Avila Science is a women's health, um, sort of institution, which is trying to help with, uh, the treatment of early pregnancy loss, um, otherwise usually known as miscarriage. Um, what this is, though, is specifically a treatment where what happens is that, you know, you experience the event, you end up at the hospital or at a clinic. They send you home with medication, right? The medication is something that then you have to administer yourself, um, at both a very traumatic time for you and your family, at a time when, you know, you need to keep track of when you are supposed to do things. It can be really challenging. Um, there's other use cases beyond this in terms of chemotherapy when people have trouble remembering what day it is, you know, let alone what they're supposed to be doing, right? You know, there are a variety of treatments that this is relevant for. Um, but Avila in particular has a system that they use to help people essentially administer these telemedicine regimes at home, right? And, um, that system is text message based, so everything here I'm going to show you is essentially a text messaging based, uh, engine with, you know, some core business logic, um, that helps people stay on track. It answers their questions. It checks in on them, you know, to make sure that the treatment went well. They still have a doctor relationship. This isn't replacing the doctor. It is simply helping to get people through this treatment without a doctor's direct support, at least a lot of the time. Um, so a few disclaimers up front. First of all, I'm going to show you a whole bunch of stuff here that is the client's actual code. Um, thank you so much to my client. Thanks to Avila for being so open with this. It's really awesome. I'm really happy to be able to show you as much as I'm going to show you. Um, I have redacted a few things. Um, I think what's left is still, you know, very much going to give you the character of the whole thing and an idea of how it works. Um, Stride, we are custom software people. So, we built custom software. I don't want to hide that part, right? It's possible to do a lot of this stuff with off-the-shelf tools. Um, but there were some specific requirements this client had that made it better, frankly, to build a lot of it custom. Um, so we did, but we did the best we could to use, you know, big swaths of off-the-shelf, right? So, you'll see a lot of LangGraph, Langchain, LangSmith, you know, a bunch of other things like that, you know, very much in here because we do believe that that adds value and that it fundamentally makes the system a lot more explainable. Um, and, you know, there are also some constraints in terms of how it's hosted. This is, you know, at least partially intersecting with patient data and various things like HIPPA and other privacy requirements. Um, the other thing, as I started with earlier, there's no right way to do this, but this one does work for us, and I think you'll see as we walk through it, some of the choices we made. Um, you know, there are definitely other ways we could have plugged the tools together. I think there's definitely other ways we could have done this workflow. Um, but we like how this came out. It preserved some of the things that we really knew were important to our client and that kind of reserve, um, you know, a lot of human judgment, you know, as opposed to sort of taking the elements entirely at their word, and this is very much a hybrid system with humans very much in the loop. Um, and again, as I mentioned before, um, I would really love it if you guys looked at what we're doing here and said, \"That's dumb,\" or \"Have you thought about this?\" right? Because, you know, this is a project which we've only been working on for a few months, but, you know, things have already evolved. That's the way it is in AI. So, I am certain, and I know of a handful of things where, you know, we could replace some of the choices we made with newer, more modern choices. Um, and at the same time, you know, there may be cases where, you know, I'm genuinely not using LangGraph. Right. I would love if someone raises their hand and tells me that. So, please do, uh, have that in the back of your brains.\n\nCool. Um, really briefly on the stack that we used and on the team that we built. So, the first thing, again, there's a lot of Langchain in here. Um, that is not because other frameworks can't do this. It's not because we couldn't build our own. The number one reason we went with this is because of how easy it is to explain the system to other people, right? You know, if you look at, and I'll show you the LangGraph stuff in particular, it was straightforward to go into our client, you know, on a very early day in the project and say, \"Hey, this is how this thing works. You can see it goes from here to here. There's loops here. Like, this is where we're doing our, um, you know, our evaluation of the process, and here's where humans come in.\" It was very straightforward to do that. Um, and I think it would have been a lot harder with something that was less visual and, frankly, just less well orchestrated. So, we're happy with this, right? There are some trade-offs to the Langchain tools, but they're mostly things we can live with. Um, we are using Claude in the examples that I'm going to show you here. Um, but the core code that we wrote works with Gemini, works with OpenAI. Um, there are a few reasons that we think Claude is better for this. I'll get into that as we go. Um, but, you know, there's no model-specific stuff really happening here. This is almost all just tool calling and MCP and, you know, other things that are pretty portable across most of the models.\n\nUm, the stack overall is not just the LLM piece, right? So, the LLM piece is Python and a LangGraph container. Um, and then the other piece, right, the piece that is a text message gateway and a database and a dashboard, which I'm going to show you pretty extensively, is Node.js and React and MongoDB and Twilio, and the whole thing is hosted in AWS. None of that has to be that way. That's just what we picked. Um, you know, the main reason we picked AWS was for, you know, that this has to support multiple different regions. We had to be able to deploy stuff, you know, entirely in Europe in a couple of cases, right? And so, we needed to make sure that we had, you know, a decent set of cloud connections that we could work with.\n\nUh, eval. So, I will show you the eval system that we built. Um, we were not able to use, or at least I shouldn't say not able. We chose not to use the stuff, um, entirely off the shelf from LangSmith. This is partly because I didn't really want to be fully locked into them. I wanted the data to live there. I wanted to be able to see, you know, the current system in LangSmith, but I wanted to have something separate. And it turns out that some of what we had to do to make the eval, um, you know, fundamentally functional, required a lot of pre-processing. So, we built an external harness that essentially pulls data out of LangSmith, processes it, and then runs things through PromptFu. Um, and one of the reasons we picked PromptFu, if anyone's ever worked with it, um, they have a very flexible, uh, they call it an LLM rubric. And, and so this is an LLM as a judge. You basically describe how you want the eval to work. Um, you feed the data in, and, you know, then it gives you a separate sort of visualization for that. So, we ended up very happy with it. It's not the only way to do it at all. It was definitely, you know, the thing that fit best for us.\n\nUh, the team. So, there were, uh, and still are, um, two software engineers, one designer, um, and me. And I'm just, I would not call myself a software engineer. That's why I didn't include myself in that pool. You can imagine there being two software engineers kind of maintaining the core system that has the gateway and the dashboard and the text message stuff, right? And the database. Um, I maintained and built basically everything on the LangGraph side, right? So, imagine this as being two separate systems that talk to each other through a well-defined contract. Um, and that those two software engineers understand roughly how my code works, but they really weren't maintaining it. You know, it was almost entirely me with AI friends. Um, and on that note, so everything I'm going to show you is the code that I wrote, and I want to be very clear. Um, I haven't been a real software engineer in a long time. I do have an engineering background. I spent seven years out of college, you know, hacking on mobile apps. Um, I took 15 years off and went to be a product person, and for about two years now, I've been back. But what that really means is just that, you know, essentially the stuff you're seeing, right, or the stuff that I'm going to show you is mostly, you know, code that I wrote with Klein. That's my personal favorite. Um, and so there's a bunch of options here. I like Klein best of all these options. Um, you can use anything you want. The code isn't actually that complicated. Like, I would estimate, and I haven't actually counted, but there's probably a few thousand lines of Python, and there's a few thousand lines of prompt. It's about equal, right? So, I vibecoded the Python, and I mostly handcoded the prompt. Um, not 100%, right? But that's the way to think about the division of labor here. Um, and for that matters, I mean, any of these tools can be great. The main reason I picked Klein was just because, you know, we did not need, you know, sort of a hyper-optimized, you know, um, like $20 a month flow. Like, I've spent a lot more than $20 a month on tokens. That's just the way it is. Um, you know, that it was worth spending the money to just have sort of the best available context of the model at any given point. Um, Klein is a very good way to do that.\n\nOkay. And there's a little bit of sample code. So, I did mention, and this is in the Slack channel as well. If you wanted to follow along with any of this, you could sort of do it by standing up your own little LangGraph container with MCP. You're more than welcome to do that. Um, everything I'm going to show you, though, is proprietary client code, so I obviously can't send you those links. So, if you'd like to, um, feel free to fire it up. Um, we have\n\n\nTwo hours, which is a really long period of time. If you're interested in spending a little time at the end of this actually working with some of this real code, I'm thrilled to do that. Um, so feel free to get yourself ready in the meantime.\n\nOkay, just a couple things up front just to make sure we're level set in terms of the terms and kind of the way that we're talking about this stuff. So, um, I do like the LangChain definition here of agent. Um, basically just because, you know, you'll see what we're doing here is using an LLM to control the flow, right, of this application. That is literally what this is. Um, and I like this and I don't know if Chris is here. He was at the last event in New York. I like this as a way of justifying the way that we tried to architect this system and why, right? So the idea of agents in production, right? You have to know what they're doing. You have to know, you know, that they can do it, and you have to be able to steer, right? If you only have a couple of these things, you end up with bad outcomes, right? And so I I I just like this framing of if you're capable, but you can't tell what it's doing, it's dangerous. If you know exactly what it's doing, but you can't control it, it does weird stuff and you can't help. Uh, please go ahead.\n\nBunch of us are looking for the Slack channel. Oh, uh, let me find that one more. It's right here actually. workshop langraph MCP agents. Got it. Okay, no problem. Okay, but uh and so transparency with no control is frustrating and control with no capability is useless. I I just love this framing. I think this is exactly the thing that we were trying to solve for. We needed something that was able to do the job, clear about what it was doing, and that was steerable by humans in a really obvious way. So, um with that said, I'm going to start with a case study, right? And this is going to be a little weird out of context, but hopefully this will give you a sense of what we were trying to solve for.\n\nSo the idea here was that there's an existing product, right? So there was a product out there that was essentially having uh you know, humans manually push buttons on a console that would enable uh a text message to go out, right? So you would read what the patient had said. They could say, \"I took my medication at 3 p.m.\" They could say, \"I'm bleeding and I don't know what's going on. Like, am I okay?\"\n\nUm, they could ask other sorts of questions about the treatment and a human would have to go into a piece of software and click you know a button that accurately reflected sort of where in the workflow somebody was, right? Because you know you can model a lot of this out, you know, imagine there being fantastically complicated flowcharts of all the things that can happen during a medical treatment. Um, so the AIA team had built this, right? They realized though that essentially to scale the human team to be able to serve a lot more patients was prohibitive, right? They needed too many people clicking too many buttons. They also realized they couldn't really scale the system to new treatments, right, which was something they wanted to do. That this isn't the only regimen that you needed to support. They had other ones. Um, and so the idea is that either they were going to rebuild the legacy software to be more flexible or they were going to essentially rebuild it to to to use a different kind of decisioning at the core. And and when they were looking at doing this, you know, LLM had started, I think, become capable enough to to handle this kind of work. Um, so what we built, what we did is we built for them a workflow and essentially a piece of software that connects to it that enabled them to do new treatments flexibly, right? So this idea of essentially defining a blueprint and a knowledge base is the way that we we thought about this. Um, and and essentially medically approved language, right? So one of the reasons that you had humans pressing buttons instead of typing text messages is because this is medical advice, right? You know, you you are not um you should not at least be giving medical advice um that differs substantially from from this approved language, right? There's reasons that this stuff, you know, is said the way that it's said. Um, you know, and and doctors have, you know, similar limitations. Um, we also built a self-evaluation function, which I'll go into tremendously, um, in a second. Uh, we wanted to make sure that we caught essentially situations that were complicated, um, and surfaced them for humans, right? Because we wanted to have a human in the loop, but we were trying to raise up the existing folks who were really just operating the system and and clicking all those buttons to be supervisors of agents that were doing that instead, right? That that really was the the model that we were working with at its core.\n\nSaw a question over here. Yeah, you may have said it. Were these operators?\n\nUh so the question is are these operators medically trained? There is a physician's assistant who essentially leads the operations team. So the way that you can think about it is that um she would be escalated to whenever something came up that was outside of the blueprint, right? So if you had a situation where they're just like, I'm really not sure what to do here, a Slack goes out to that channel with a physician's assistant in it who would then give medical advice. So, you know, again, this is one of the reasons it was hard to scale, right? Because, you know, you only had one of those people on this particular team. Sure. Um, and so, you know, to to sort of jump a little bit ahead, but hopefully you'll see why this is in a minute. This roughly and and again, we're we're still doing the measurement, right? We're still trying to figure out exactly what, you know, capacity has has gone up to. Um, we think it's something like 10x. We think that they can surface roughly 10x more people with this new approach. Um, now it's not free, right? We have to build the software. we have to pay for the tokens. Um tokens can get expensive. But if you think about, you know, just the scale issues involved in scaling up a team of people and again in building the software to be more flexible for more treatments, um we think this capacity increase is is very very much warranted and very much the thing that that you know solves solves the problem. Um and you can do new treatments in new workflows without writing more code. Right? That was the single biggest thing about this. And and you'll see what we're doing here is largely Google Docs, right? And you know, we have some more advanced techniques to to manage those things and inversion them over time, but but we're talking about being able to support whole new treatments and whole new workflows without going back to the code, right? That's hugely valuable to these guys.\n\nQuestion\n\nyou mentioned velocity\n\nmeasuring quality of care. So question was uh velocity increases. Is there a quality of care measure? Um short answer is it's early, right? I mean this is still a system that's you know in progress. it is being used with real people, but it's still very very much early on that the way that I think we're looking at it is that there would be some combination of the operators being the ultimate arbiter, right? They're going to be able to see these conversations and determine as they approve them, you know, as they review them like, hey, is this mostly getting it right? And then there there are sort of existing kind of seesat, you know, level measures that you can apply to the people who are on the other end of the treatment.\n\n10x sounds a little low. Is that because the operators don't approving everything that comes out right now? uh so they're not approving everything that comes out and I agree the 10x is kind of it's an order of magnitude not a precise measure right but I think in this case you'll see a couple of cases that require approval right and sort of why but the approval also is very quick right so I the argument is that you probably only see one of every 10 exchanges and when you see it it takes you roughly as long as it took the last time to just push the button right which was the thing they were already doing so that's kind of why we've benchmarked it there\n\nall right um so let's get into it a little bit. So, uh, this is just a snapshot of what this looks like in LangGraph. I'll show you the real thing in just a minute, and it's actually evolved a tiny bit since I took this picture. Um, but really what we're talking about here is, um, the the the people who operate the system today, we call them operations associates. So, what this is really doing is introducing a virtual operations associate. that operations associate is going to assess the state of essentially a conversation interaction with a patient. Um determine what the best response is both in terms of uh the text message you might send um the questions you might ask the actions you might take because some of this is about maintaining essentially a state for that patient, right? You know, you are you are at any given point trying to figure out um when is this person taking their medication, when did they take their medication, um you know, what medication do they have? Um what time is it for them, which is actually more important than than you may think. Um all of this has to be maintained, right, by the system. And so the virtual lawyer is doing all of that work and then it's passing essentially its proposal, right? It it basically comes up with I think this is what we should do and it passes it to an evaluator agent. There's a live LLM as a judge process separate from the evals which which we'll get to. But the live LLM as a judge is essentially saying, okay, given this thing that just happened. Um here is our assessment of a you know how right the LLM thinks it is. Um that's frankly very challenging. LLMs are very hard to convince that they're wrong about anything. But um it also is looking at the complexity, right? So even if the LM believes it's made all the right decisions, you can have it impartially say, \"Well, I changed this and I changed that and I'm scheduling a bunch of messages. That's complicated. maybe a human should look at this, right? So that's actually a lot easier to implement. Um, and both of these things are calling tools. The tools are a mix of MCP. Um, and so there there's sort of two versions of MCP here. I'm going to show you one which is basically just looking at local files just so I can show you all the stuff in my environment. Um, but there's also MCP going across the wire to the the larger software system and keeping all this stuff in a database, right? So there's there's a mix of those two things. Um, and the rest of the tools are about maintaining state because as a conversation is happening, the the LLM needs to know, you know, essentially, well, I made this update and that update and here's the current state that I'm working with and it has to be able to sort of manipulate these things in real time. That is not MCP. That's not going to a database anywhere. Like, this is happening entirely in sort of the live thread. And then once it finishes, then it gets pushed out and and essentially saved away.\n\nOkay. Um, again, we'll get into a lot more of that. I did want to spend a minute on the on the system architecture, right? And so I realize it's a little bit small. Go ahead.\n\nWhen you mention about the system state, I I heard before the L has a context or some state object.\n\nYou mention that you use tools. Are you talking about separate things or uh\n\nquestion was about how the state is managed in LangGraph. So um short answer is this may be one of the things where I'm not not doing it optimally by the way but um with langraph there is a state object that we load essentially when the request comes in from a JSON blob right we keep it alive inside the the graph run it is not directly accessible to the model right the at least not the way that we're doing it right so you you'll see actually as we get into this that you can see all the state coming in in Lang Smith right I can see like hey this is the whole thing that was was loaded I still have to repeat that in my first message to clawed right it doesn't actually show up you know in the same place and then I call the functions that state will evolve in terms of what's inside the graph run and then when it outputs it's the it's the Python code not the model which essentially takes all that state and then uh serializes it and sends it out. Um so you you'll see how it works but like I that's generally one of the things that I'm not sure I'm doing right.\n\nAnything else? Yep. Yeah. Somewhat related. You have one node for that virtual going back and forth tools right now. Yeah. I'm assuming the reason you haven't forcoded out that business logic more into separate nodes is you'll lose the workflow for the next time. Is that sort of the notion there? Yeah. So the question is why the essentially the virtual A is one one agent not you know a sort of a a precoded sort of version of here's how I administer the specific treatment. Yes. the the reason I think we kept it simple is because we did not want to be super treatment specific in how the architecture worked. But you could imagine doing, you know, a set of slightly smaller, you know, better tuned agents that were, you know, kind of taking care of elements of the task that was still pretty generic. The main reason I think it's not optimal to do that is is caching. Um, and this is another question where, um, you know, I I think I'm doing this right, but there are a lot of variations here. um caching the entire message stream is easier with with either one agent or with sort of one agent doing most of the work. Um we're using Claude. Claude has very explicit caching mechanisms. Um and every time I switch the system prompt, I think the cache blows up. And so fundamentally changing the agent identity does that. So that was one that's one reason we chose that. It's it's certainly not you know a hard and fast forever choice.\n\nWhat's the uh duration of the uh like we talking like months or so like how many messages? Yeah. So uh this use case the early pregnancy loss um it tends to be a treatment which takes I think three days\n\n\nEnd to end to administer most of the time, and then there's a check-in after that, right? So imagine that, probably within a week, the entire interaction with that patient is done unless they come back and just have questions later on, right? You know, there are some variants of this where you take a pregnancy test after six weeks, right? And so that's all fine. Um, the message history is preserved, but the computation that happens to generate each message is not, or at least not in, not in sort of the state that we behave. So, like, you know, the most complicated conversation I've seen was something like 150 texts. It's a lot in terms of, you know, a human keeping it in their brain. It's not that bad for an LLM, right? So, but it's that level.\n\nAll right. Um, so again, just to point out where the lines are here, right? So, as I kind of got off on a tangent, the top box is what we're going to be looking at here today, right? It's really a Python container with access locally to these blueprints, this knowledge base, right? We are also then maintaining some stuff over across the wire in this blue container. That's really where the dashboard I'm going to show you is. It's where the text message gateway is. Um, and it is where we're going to be moving, I think, a lot of that context, right? The blueprints, like all that stuff, really should live kind of in the more durable software container. Right now it lives, you know, close to the Python.\n\nOkay. Um, so let's get into it. Um, so the first thing I'll do here is just to show you, uh, kind of at a high level, what the software looks like. So, um, this again is, uh, the console, the dashboard, right? The thing that the operations associates, the humans, are going to be looking at. Um, and I'll show a couple things here just to give you the baseline, right? So the first thing here is this needs attention. So the current system basically has this needs attention flashing all the time. Every time a text message comes in from any patient, this thing is going off, right? You know, so there, and there's, you know, hundreds of patients, thousands of patients in the system at any time. So, you know, this needs attention used to be something that multiple people were having to stare at constantly, right? Just to make sure that they caught everything so that they got out messages in a reasonable time. Now, needs attention is really, you know, just sort of one thing at a time, right? And if I look here at the conversations, there we go. Um, you can see that the top one here actually needs a response. I'll get to that in a minute. But at any given point, right, this is my test environment. You know, I've got a handful of these conversations kind of already queued up. What I can see here, if I click into these things, is essentially, I'll just go back to the beginning here for the whole message history, and I'm going to toggle this rationale on. Um, what you're seeing is the entire conversation. Is that readable? So, I blow it up a little bit. Is that a little better? Okay. Um, so the idea here is, uh, the agent is named Ava, right? That's the personality that people are interacting with. Um, this language is all coming out of these blueprints, right, that I'll show you. And so this first message is just an initial message sent by the system essentially just to kick things off. So imagine someone is, they have a package of medication in their hand. They scan a QR code. They put in their phone number, they get this text message, right? And then they start talking. Um, so you can see here the kinds of things a patient is going to say are, you know, free form text, right? You know, this, I mean, they could say yes in any number of ways. The old system used to have literally different buttons for yes, like yes, I have the medicine. Yes, I heard you. I mean, it's like there's all sorts of variants, right? And because you did have to respond differently depending on what those things were. What we're able to do here is really just take, you know, these free form answers, interpret them, and then essentially provide a rationale for why you would say a given thing at a given time, right? So, this is equivalent to if you were doing this with a human and you ask the human, well, why did you say this? The LLM can provide this kind of context. So, this is Claude looking at the history here, and I, I'll show you what this looks like in LangGraph, which will make it a lot more obvious. And then saying, okay, here, here's the next thing that I should say. And my confidence that I should say it is 100%. Right? It's, it's usually very confident, right? But, but the point is, this whole process is largely going to go along in an automated fashion, right? You don't usually need humans involved because this is a very straightforward thing. They have their medicine. The next thing I need to know, and this is a very interesting part of this treatment, I need to know what time it is. These are text messages. We don't know anything about these people for a variety of reasons. It's kind of good that we don't know much about them, right? We don't want to have to deal with all of the stuff around provider confidentiality and patient data, right? So, one of the things that we need if we're going to go through this longitudinal treatment is to figure out what time it is for them and then essentially pull out that data and figure out what their local time is. Right? So, in this case, I was in Eastern time when I answered these questions. This is all me doing this, you know, from my laptop. Um, I tell it what time it is. It calculates an offset from UTC and says, \"Well, I guess you're in Eastern time, right?\" And then it sets this over here and it says, \"All right, from now on, I know that my patient is in Eastern time unless they tell me otherwise.\" And they could come back and tell you otherwise, right? That's something the old system really didn't have a good way to do. Um, but if the patient comes back and says, \"I'm on a plane. It's actually seven for me.\" We just update the time zone and move on. Right? This is a very flexible system that way. Um, then we get into this over here and we say, \"Okay, now that I know what time it is, I'm going to ask them if they've started their treatment, right?\" And, you know, there is a blueprint, right, which we'll get to, you know, that essentially just has, you know, the medicine that they're going to take in a very specific way to take it, right? The protocol. Um, the patient says, \"Well, no, I want to take it soon.\" You know, the Ava says, \"Cool. I'll text you when we're ready.\" And then it gives you know a regimen, which in this case, this is an SVG that we are stapling times and dates on top of, right? So, you know, fairly straightforward, we're doing this in software. The LLM's not doing it. The LLM is actually just passing along the instructions, you know, it says send the step one image and provide, you know, like this date and this time, and we substitute the rest of it in, and this goes out as an MMS, right? So this is a text message. Um, and so we provide this. The patient, you know, says, you know, in this case, we're, we're talking, you know, again, this is all the LLM reasoning through this, right? You know, I, I am sending this immediately because it's actually within sort of the 35-minute window that you've told me that I have to send these things. This is all business logic that the LLM is interpreting pretty much on the fly. Um, and then I have these reminders, right? I didn't get back to it. So, this is an important part. It sent me this thing and it thought that I was going to take it at 5:45. I didn't text it back, right? This is partly because I was maintaining the system myself and I forgot. So, I had to come back in the next day and catch up. Um, so it sent me an automated reminder because it scheduled one when it sent the first message. So part of this is the LLM only gets called when the patient says anything. So if they don't, you know, you have to make sure that you stay engaged, right? You don't do this overly, like we don't try to bother people beyond one or two reminders. It's their treatment. Um, but this bump sort of functionality was really important to the client, right? So we built it in. Um, so you can see here I came back the next day and I said, \"Yep, sorry. I did take it.\" You know, Ava confirms that I completed step one. And what it does is it sets this thing called an anchor, right? And it says, okay, you know, the patient was going to take it at 5:45, they confirmed that they did. And so now, you know, I can refer back to this. I know that this happened, right? And if the patient had then said, oh no, I screwed up. I actually haven't taken. I'll take it today. We just change the anchor. We update everything. Right? So this is a system that humans used to have to do. If a patient came back and said, I didn't take my medicine, you know, a human has to go in and manually update all the times and all the scheduled messages. And it was, it was a big pain in the butt. Um, yeah, please.\n\nImplement this functionality where patient report state.\n\nNot exactly. Um, the way that we do state, and I'll, I'll spend a lot of time on this, but the way that we do state is really just that with any given message from the patient, right? This entire system only kicks off when the patient sends a message. Um, what we do is we say, all right, given this state, what is the best response? And that response could be, I changed some of these anchors. I update their treatment phase. I schedule a bunch of messages. All that state is preserved so that the next time they write in, then, you know, we have that state to go on. Um, but again, we're not checking, right? There's no polling going on in the system where we're saying after 3 hours, did the patient text me back? We, we don't do that. We depend on the scheduled messages essentially just to nudge the patient. Um, if they choose to not say anything for three days and they come back after three days, we just pick up where we left off. Um, again, this is a choice. This is the way the client wants it. It's, it's intended to be low enough touch that it doesn't bother people, but high enough touch that it doesn't lose track.\n\nSure. Um, I'll pause here actually. Any other questions so far? Um, I realize I'm going through a lot. Yes. Do your anchors have to be sequential or can your user come in at any point?\n\nThey can. Great question. So the question was, do the anchors have to be sequential? Um, or like, do you have to go through these one step at a time? So, one of the great things, one of the best things about this system is that I could have, and, and I'm happy to try this when we go a little bit later, I could have basically said, \"Oh, yeah. I already took the first pill and I'm like in the middle of taking the second pill, you know, as like the first thing I say to Ava, and she would be like, \"Okay, cool. There's an anchor. Here's the next thing.\" It skips ahead and it doesn't force you to go through this prescriptive part of the blueprint, whereas the old system, you know, at least nominally did, right? Like you could, you could kind of skip ahead, but this automatically does it. You know, part of the instructions are, don't ask the patient a question they've already answered. Like, period, right? But that's annoying. Don't do that. Um, so, so yes, that's, that's very much in there.\n\nAnything else? Yeah. Does the concept of the internal state machine that is kind of determining all of this or is that kind of outsourced to the actual software?\n\nYeah. So question is, does the LLM have an internal representation of the state? Um, kind of sort of. So you'll, you'll see, um, when we get into the actual back and forth with Claude, um, in LangGraph, you as a human can see kind of where it starts, right? So every thread is going to show, like, all right, here's the incoming state. We repeat it essentially to Claude again. That's just one dot. I've never managed to connect with LangGraph, right? So we basically have to serialize the state and say, this is your, this is your starting point. But then the LLM has that in its window, and then, you know, it's going to cause changes to the state. It'll call functions that update the state. It can always ask again. It can say, well, what's the current state? You, I can go back and retrieve it. Um, but in the context of that one from when the patient responded, you know, to when I actually come up with my response to them, that whole thing is going to be in its memory at one moment.\n\nDid you ever run into issues with state?\n\nUm, so the, the short answer is, the question was, um, do we ever run into the state being too big? Uh, generally speaking, because of the way that we're kind of compressing and serializing at the end of the conversations, it doesn't ever get so big that it can't finish its job of responding to one situation, right? You know, like, patient said this, now I'm going to do this. We have considered having longer running threads where you kind of pick up in the middle and you've already, you can reload sort of the entire previous conversation. That does get weird, right? Especially with older Clouds, you would get it forgetting to sort of call tools the right way, and it have all sorts of JSON errors, right? We have a bunch of retry logic in there to kind of compensate for that. Um, so that's one reason we kept it short. We make it so that we basically throw everything out and restart when the patient gets back to us. In part because blueprints could change, right? You, a bunch of things could, could change in the meantime that might end up with weird states.\n\nSo on the management of states and taking decisions what to do next. Yeah.\n\nSo this is 100% LLM driven or there's some softer like logic around it as well. It's 100% LLM driven. Uh, sorry, the question was, uh, is the, is the steering\n\n\nDone by software, right? Any of that?\nSteering the answer is really no, it's not, um, except for when it surfaces to a human, right? And so when it goes to a human for approval, the human can use English and basically say, \"Yeah, change that word to that,\" and that message shouldn't go out, and you know, whatever. So, like, we, we actually, as part of the flexibility part, we are not building any software that manages the state. We just want you to talk to the LLM to do it, right? We think that's a better practice, right? It means, like, you know, you as a human just have to talk to it, and you don't have to figure out how to flip all the bits on this new console.\nDo you have any rack system? And second, if a patient is going off journey, how do you detect that?\nI'm sorry, what was the first question?\nDo you have any rack system or Rex? I'm sorry, I just don't understand.\nRetrieval. Oh. Oh, got it. Sorry. So, um, the question was, is there a RAG? Uh, no, there's not. And, and it's actually just because what we really did is we just came up with a structure for the documents that was self-referential. So, you read a very small document which says, \"Here's the treatment,\" right? \"If you need to read for this phase, go to this file, right? If you need for this phase, go to this file. If you have a question that doesn't fall underneath any of those things, here's a CSV with a bunch of questions and answers.\" We didn't do it as RAG in part because we didn't believe that either we could do a really good job of getting all the right information into the window. Like, we didn't think we'd be reliable enough about that. We just want to give the entire document. They're not that big. Um, and because this is clawed, right? It's, it's got a big enough window that we could just put the entire thing in there, you know, for, for most treatments. So, we chose to do that.\nWhat was your second question, though?\nIs the patient going off a typical journey? Yeah. How do you detect and intercept? Right. So, the question is if the patient goes off track. So, we, we have this idea of a blueprint, but then there are plenty of cases where the blueprint may, um, you know, not fully answer whatever the patient is, is, is bringing up. Um, like one example is the blueprint is very much about asking questions, right? So, you will say, \"Have you taken your medication yet? When do you plan to take your medication?\" The patient will say, \"My stomach hurts.\" Okay, so, yes, your stomach hurts, you didn't answer the question. What we do is the patient typically will get an answer to their question. So, one of the principles is always answer the patient's question, right? We don't ever want to leave them hanging, but then ask yours again. So, the idea is that at any given point, we can answer anything that they need, and as gently as we can, we'll try to pull them back onto the blueprint so that we understand where they are in the treatment. Um, it's not an exact science.\nBut, uh, is there a way to detect if someone...\nWell, the LLM does that effectively by knowing that it's supposed to keep people on the blueprint, but having an escape hatch for the knowledge base, essentially what we call it, right? Triage or knowledge base, you know, whatever you want to call it. Um, so, you know, we, we don't have an explicit bit sort of flipped in the system that will say this patient is off track. We just kind of know roughly where they are in the treatment, and if they want to answer, if they want to ask a bunch of questions, we, we'll just answer them until they, they are satisfied.\nOkay. Uh, yeah.\nYeah.\nYeah.\nSo, the question is, why did we choose LangChain, and would we still... Um, I, I will be very candid that the main reason that I chose LangChain is that I had personally gotten pretty comfortable with LangGraph as a demonstration of these concepts, right? It's not that... I mean, we did a lot of Autogen work back in the earlier days, right? You know, I've done a little bit with Crew AI. All of those frameworks can functionally do very similar things. LangGraph was the absolute best at explaining to people who were not neck-deep in this stuff how it worked, um, and because there was a path to production from there, I didn't feel a need to replatform and change all of it. We certainly thought about it, right? We considered, \"Well, what if we didn't do this in LangGraph? What would we gain?\" And but the answer is, you still have to implement observability in certain ways. You know, you don't necessarily get, you know, the support that you might get from LangChain if you end up in a place. Remember that we're also doing this for clients. We're not going to be there forever. Um, leaving them with something that they can call, you know, somebody to support is also a helpful aspect. So, I think I, I, I don't think I'd do it differently. I think it's really just that, you know, ultimately, you know, we're getting pushed, all of us, in the direction of using the native model tools for this, right? You know, OpenAI has the responses API, which lets you define tools. Cloud has its new stuff, right? Like, I don't really want to be locked in. Um, I, I am to some degree locked into LangChain now, but I, I prefer that, honestly, to being locked into the models. Um, these are, these are not performance-intensive things we're doing in terms of the software, right? Like, you know, I don't care that LangChain is sometimes a little slow. Um, I would rather have the option...\nSo, you said you're not using the scale. How...\nThose...\nUh, it is just that they have... Sorry, the question was about, um, if it's not RAG, how do we fetch documents? The documents refer to each other. So, you'll see that we have an overview MD, right? This is all in markdown. Um, there's an overview MD that tells you what other documents are involved in the treatment, right? There's some of the prompting which says, \"You can always request a triage overview,\" right, to, to try to handle problems. Um, and it'll be there, right, regardless of what the treatment is. So, it, it is very much just a document management thing. Um, RAG, the main issue is just that I, I don't think, and, and you know, this will probably be more obvious as we get into it, right? I don't think that you could really design a RAG which would pull back snippets of everything in sort of perfectly relevant, relevant ways. You really do kind of need to understand the shape of the whole treatment, right? To, to make a good decision, right? Otherwise, you're just going to pair it with whatever particular snippet the RAG happened to bring back, and then the logic all has to be in the RAG. It makes more sense, and it's more transparent, I think, to do it this way.\nMaybe you'll get to this in the state management later on. Are anchors predefined in the blueprint, or are they...\nThey are mostly predefined by the blueprint, and that we say as part of the overview, you know, the concept of an anchor is that it is a thing that happened or a thing that will happen, and here are the examples for this treatment, right? This is the thing that will happen or did happen in this treatment. Sorry, that was questions about the anchors. Yeah. So, when you...\nYou mentioned that you conversation and you keep track...\nUh, no, so that the state is essentially, um, you know, we, we call it, for reasons that only an engineer could love, we call it a schedule document, right? The idea is that for any given patient, there is a schedule that they're on, and the document snapshots their current state at a given point, right? And it's a version database. So, we could go back in time, and we could see what their document was three days ago. Um, but it has, at any given point, the messages that have been exchanged, any unsent messages that are scheduled, and enough state about their treatment to fill out this view. Yeah. Uh, yeah.\nSo, in this case, all of this stuff is locked away, right? So, I mean, just to go back to this diagram for a second, um, this entire thing is all behind, you know, AWS's VPC, right? So, like, there is no external access to the LLM, period. The only things it can talk to are essentially its own documents, you know, in, in local files, and to the, the blue box. So, you know, there, there certainly are vectors, but the vectors would be through the text messages, right? Not really through anything else.\nUh, yeah. Oh, I'm sorry, bunch of people. You first question regarding, I guess, it's twofold: is like, how are you assessing the confidence rate from the model's response, and the second is, how are you safe to get injection for malicious behavior? Yeah. Well, so, uh, the question was about prompt injection and generally sort of steering. Um, I mean, the, the basic answer is just that...\nYou could definitely try to trick the model by sending weird texts, right? And, and we do that as part of our, you know, sort of internal red teaming. Like, we have the entire team of Telemedicine Support Agents who have been spending, you know, weeks and months trying to trick this thing, um, and granted, they're not trying to trick it from a reveal proprietary personal medical data, you know, I mean, there, there's things like that. We also obscure a lot of that medical data. So, the things that get to the yellow box do not include phone numbers. They do not include anything other than the patient's identified first name. Um, so there's a lot of, there's a lot of that data that's kept only in the blue, which is a lot easier to, to defend against. Um, so, yeah, we, we, we very much do obscure the, the, the patient. We don't obscure the treatment, right? The treatment is fully visible to the LLM. Yeah. Cool. Yep.\nYeah. Hold that thought. I will get to that very, very shortly. Um, we're back there.\nUh, sorry. It's just a question about unclear instructions. Um, so, uh, when, when the situation is ambiguous, the LLM is told to look at the blueprint and pick the best possible answer. Now, if you don't believe the LMU, if you don't believe the, the answer is perfect, um, you should say so, right, in the rationale. So, if I go back over here, this idea of the rationale, if there is uncertainty on the model's, you know, point of view, it can say, \"Well, I picked this blueprint response, but I'm not sure that it's right.\" In practice, it's not great at doing that, right? But that is the idea. And then the evaluator is also going to look at this and say, \"Well, did you actually pick either the exact blueprint response word for word? Did you adapt it? You know, does this seem right to you?\" Like, we're trying to at least give a little bit of a layer before we get to humans. And then, hopefully, we, we can trap situations like that and say, \"Well, this is a complicated situation. A human should, should take a look.\" Um, it is not an exact science, though, like that's generally just true with this stuff.\nSorry, you in the back.\nYeah. Um, so the question was just about load and scale. So, uh, the look, the really short answer is that this, this system exists, right? There's an existing version of it that is humans pushing buttons. Um, that scale is, you know, again, let's say thousands, not millions of patients. Um, this opens up the possibility of doing more treatments, right? That's how we would get sort of additional patient scale. You can also sell this to new hospitals, new clinics, things like that. Um, so part of this is to get the scale to be larger. Um, we have not run into scale issues with, you know, just the, the conversations with Claude. You know, the software that we're building would scale much, much larger than thousands of users, right? You know, the, the text message gateway might actually be the, the biggest bottleneck. So, it's, it's honestly, it's a problem we want to have. Um, go ahead. So, um, you keep saying, \"Did you guys select thems?\" Or was there, like, a specific reason why you're going with 3.5 or whatever you...\nYeah. Uh, so, questions of model selection. Um, when we started this, right? And, and I think, you know, let's assume that we kicked this project off, you know, late last year, early this year, right? Um, we had to make a choice, and our main criteria were, it had to be a steerable model that we felt pretty good about, you know, transparency-wise. Um, you know, one example, just, just to give you a specific one, Mini is pretty good at this workflow, but it won't show its reasoning. Um, like, I mean, that's just one example, and like, it's not a dealbreaker, like, we can still see the rationale, like, there's some pieces of it, but I like being able to go into Lang, seeing the whole conversation, right? That, that really helps me out. Um, we needed, you know, again, flexible hosting, but I mean, all the clouds kind of do that. Frankly, we didn't want to deal with Microsoft, and we kind of preferred AWS to Google. That, that was kind of how we got there. But, you know, you can do this anywhere. It really was just we had to pick a horse, and we largely have not regretted it, and in part because we built enough flexibility where if I want to switch, I, I still can.\nYeah.\nYeah. Uh, so the question was about sensitivity of data through the text carriers and also about, uh, using the data to learn. Um, I'll do the learning first. Um, we don't, we, we do not take any of the responses and, and do anything to the models other than when we see situations that we as humans have evaluated and found wanting, um, we can tweak the prompting and the guidelines, right? But we are not putting this in any sort of durable form. Like, ultimately, you know, we believe the right model here is the provider interaction. If there's a provider involved that sticks around, right? The provider knows that you interact with the system, they can have, you know, whatever records they need. Um, otherwise, you know, we forget about you when your treatment is done. We think it's better that way. Um, on the, the sensitivity question. Yes, there is sensitivity involved, and at the same time, again, there's prior art with these products, right? There are existing systems which essentially take, you know, text messages in and provide medical advice. Um, we're just trying to stay within the guidelines of that. And again, that's one reason why we don't want the LLM actually to have any data that is not explicitly required just to do decisioning, right? It doesn't need anything beyond that to, to make a good decision.\nOkay.\nYeah.\nEvery response is 100% determining that, that's 100% what situations where that's notified. Yep. Sorry. Hold that thought, too, because I will get to that in just a second. Um, let me move on. Uh, please, like, bring these questions back up. I just want to get a little bit further so we can see some other, some other cool things about this. Um, I'm going to move on from this flow just because you can imagine that this is going over a period of days, right? There's another step here, step two, where there's, you know, more medicine being dispersed. Um, and then, you know, ultimately, we're going to get to the end, right? And...\n\n\nYou know, essentially, did you complete this? And then, okay, great, you know, this is what's going to happen to you. You know, you're going to see some bleeding. Um, and then we have this check-in, right? So imagine that this now is, you know, a full, let's say, three or four days later, right, after the treatment has begun. Um, you know, we check in, you know, the patient gets back to them or not, right? Remember, some of these patients will just be like, \"I'm done. I don't really need to talk to this thing anymore.\" But if they do, right, we continue with the treatment. We don't bother them. We just let them sort of resume where they left off.\n\nAgain, we have these rationales, you know, we have these questions. And then what I want to do here is just to show you briefly, um, sorry, I got to zoom back out so I get the full phone number, um, what it would look like to interact. So if I go here into my sandbox, um, imagine that normally this would be a text message. Um, so, you know, I would be doing this on my phone. Um, but here, you know, I can answer this question. If I had any pregnancy systems before, have they decreased? It's like, yes, uh, they have decreased.\n\nOkay, so I post this message. Now what's going to happen from here is thinking. So none of this is instant. And so now what I want to show you is what this looks like in LangGraph. So, um, you can see here a couple of things. Um, one is that this, this is now spinning. Um, so this thing that I just asked it is now in active processing. I'll show you what it looks like when we're done. Um, but I will give you just a brief look at, um, I think this is probably a useful one here. Um, what this actually looks like in terms of processing the state. Um, so I'll blow this up a little bit and make it a bit bigger.\n\nSo, um, what you can imagine this is using sonnet 4. Um, is that every time a message comes in from a patient, this is what I get. Okay, I get this description of, you know, everything that's going on here. I can see this is an Avila patient. I can see the thread that we're currently executing, right? Because you may need to resume these threads if you need to give feedback. Um, I have this idea of I'm in the 3-day check-in phase. So, that's the blueprint that I'm going to read. Um, and then I have a couple of things. I have these anchors, right, which, you know, you could see. I think this is exactly what you saw before. Um, you know, in that same patient. Um, these are all defined as, you know, actually a mix of UTC and and Eastern time stamps. Um, that's one of the problems that's hard to eradicate. Um, getting LLM to deal well with time is really tough. Um, but then I have this entire message queue, right? And this is the compressed state of the conversation to date, right? This does not include every message that Claude sent itself while it was thinking, right? That part is contained in these individual Langsmith threads. I could go back and I could look at this if I needed it. Um, but what I'm doing is I'm compressing and basically saying all I really care about is the actual messages that went back and forth. I want these rationale because I want to be able to review them, right? That helps me understand the decisioning that's going on here. Um, you know, I want these confidence scores so I can go back and look, you know, what did it think at any given point? And again, I'll show you one where the confidence was low. But these things can go on a little ways, right? This is probably, I don't know, 20, 25 messages, right? All of this goes in as initial context in the window, right? So, if you had 150 messages, all 150 of them are going to potentially go in. Now, we do have a function where you can optionally set it to compress and say, well, just show me the last 50, right? If I need to request more, I can do that. There's a way to do it. Um, but I don't need to have the entire thing in the window.\n\nUm, so I get down here. This is the last message from the patient, right? So the question was, \"Did you notice blood clots?\" I said, \"Yes, a few.\" Right? You know, that was that was what I as a patient said. Claude is now going to start processing this thing, right? So imagine, you know, this all being basically pasted into, you know, a Claude window and then having it go through this process and and call tools. So it starts by looking at directories that it's allowed to view. Again, this is a version where it's got the blueprints kind of all local and and it's it's talking to them this way. We have another version where it talks via MCP over to the the blue box, right? The larger system. Um, so it figures out what directory it has. It reads these basic ones because these need to be read in all cases. So these guidelines, right, the idea of how do you do your job, right? The idea of what the confidence framework looks like, the overview of the treatment, right? You know, those sorts of things. We read those up front. None of these is very large, right? And so you read all this stuff, you know, it it comes into the the window. Um, and then, you know, essentially it reads those descriptions and it says, \"Well, I was told as part of this that I have to read the current blueprint for this current phase,\" right? So I read that file individually. So a bunch of these early calls are just about setting up the context. This is not the only way to do it, right? I mean, this is the way that we've chosen to do it. Again, we chose not to do RAG for a couple of, you know, reasons around we just did not think we could get good enough results and because this is honestly easier to interpret, right? You can sort of tell what it's doing. Um, I get to the blueprint. The blueprint. And you we'll see more of these examples in a second, but the blueprint is basically this kind of structured bulleted list, right? Here's all the stuff that you might need to say to somebody, right? And you know, here's what you do when you know the user says a certain thing. This isn't actually that prescriptive. It's just structured, right? This isn't an if-then statement, right? It it's kind of like that, but it's not an actual if-then statement. So, like this format, you know, is one that we iterated on and got to a point where we actually get really good results. Um, but you know, it wasn't 100% obvious this is the way to do it up front. Um, you know, we started with charts. Um, and so now you get to this point where now you can see, okay, now I got to look at these, you know, uh, conversations. I got to figure out what's been going on here. And so you can see here, even though I passed in the state, it has a function to list messages. And so it basically says, \"All right, well, now that I sort of know what's going on, let me see the last five messages,\" right? And you can see here it's going to start sending, you know, a bunch of these in. Um, and so it does that. It looks to see if there's anything scheduled. There's not, right? And so now it says, \"All right, this is this is sort of the point where Claude does its little explaining thing. I understand what's going on. Patient's in the three-day check-in phase. I already asked about bleeding and cramping. I I asked about blood clots, and the patient, you know, basically just said yes, they have blood clots, and so I'm I'm just going to keep on going, right?\" And it goes to the next question about pregnancy systems. This message comes directly from the blueprint. Okay? And and I'll show you in a Google Doc form in a second what that looks like. Um, so it schedules it. That says you should send this message, you know, as as soon as you want to. And then we get over to this evaluator flow, right? And the evaluator says, \"All right, I'm going to look at this situation. I'm going to look at everything that requires confidence scoring, right? That new message is the only thing. It's it's the the only thing that just happened. Um, and I'm going to send it immediately. This is just a a time stamp for immediately. Um, I then get this kind of report, right? And the way that we set up our framework, um, and and I'll show it in code a little bit clearer is, you know, do we know what the user is saying? Do we know what to say and do we think that we did a good job?\" Again, this is a tough one, right? Um, generally speaking, the LLM, you know, says at all times, \"Yes, I know what I'm doing.\" And, you know, like buzz off. Um, but what I can also do is I can say, \"All right, then there's a bunch of cases in which if I set an anchor, if I updated the patient's data, like maybe I changed their time zone offset, maybe I changed their name, right? That's a weird thing that, you know, if it happened, you'd probably want a human to look at. Um, do I am I sending multiple messages? Do I send a am I sending duplicate messages accidentally? Do I have reminders for things that have already happened? All of those things would deduct from the score and cause a human to get involved. Right? That that's part of how we do this is to combine does the model think it's okay? Right? That's this top part. And then overall, is there a weird circumstance that I should try to catch, right? And that I should try to to to show people uh to show a human for review. Um, in this case, nothing came up. I update the confidence. It's confidence of 100%. Um, and then essentially the virtual OA, you know, as as a final thing, it's very hard to get Claude not to summarize itself. It does. Um, it basically just says, \"Here's everything I did. I'm good.\" And then if you go down here to the bottom, this is the output state. So this output state says, \"Well, I have 100% confidence, again, its version of it, that I did the right thing. I, you know, here's my anchors, here's my messages, and here's the unscent message that I'm I'm now going to send.\" And because it's 100% confidence, it just goes out, right? It goes back to the text message gateway and it just goes out. Um, that is a risk, right? You know, if you wanted to be perfectly safe, you have a human review all of these things. We don't want to do that because we're trying to scale, right? So, we are comfortable in general with things that are are, you know, coming back with 100% confidence that we just send those messages out.\n\nUh, question back there. Yeah.\n\nYeah.\n\nHaving trouble later on. Uh, yeah, so the question is just, uh, how do we determine sort of the the the the situations that might have confidence issues? Um, it is very hand-tuned and geared to this evaluation team, like basically the virtual OA team that exists now as as humans. Um, we will review, you know, in sort of spot checks, you know, a bunch of situations just to kind of see like, \"Hey, is does this seem like it's okay?\" Um, when the when a patient writes back and says, \"You got that wrong,\" like, \"That's not the time I said,\" like, \"You know, I'm actually taking it now.\" Um, the confidence system is pretty good at picking up that that happened and basically saying, \"All right, even if I think I'm confident, something's wrong, right? You know, a human should take a look at this.\" Um, but I mean, the the answer is it's it's more art than science. It's not something that we are perfect at even now, and because we want to scale, we've chosen to say, \"Look, the the worst that happens is essentially something weird happens and a couple of text messages go back and forth that are just wrong.\" Usually the human will get involved and say, \"That doesn't sound right to me.\" Right. It's not it's not a case where the patient is in danger. Um, you know, if they say, \"Well, I'm having these symptoms, you're not helping me,\" like a human will step in. Like that's that's something we're pretty good at flagging. Yeah.\n\nYeah.\n\nYeah. Yeah. I mean, so the the short answer is, um, we can look at interactions that ultimately are scored as low confidence, and then we can trace back from there, right? So a lot of what we're doing is when something gets flagged and a human is like, \"Well, there's something weird here,\" um, you know, we share those things internally, right? The the Slack channel that I was talking about before, where they talk to the physicians assistant, that's largely been repurposed to people saying, \"Hey, this behavior is off, like, can you go take a look?\" And that ends up essentially in my queue as, you know, \"I got to go check my evals. I got to see if there's something I can do to catch this, and maybe it's a matter of changing the behavior here.\" But so it's usually it when we when we know there's an issue, we can backtrack. That's the short answer.\n\nUh, over here, I'm curious, humans also make mistakes. Yep. Do you have any data from like % of human responses versus AI? Yep. The question was about human versus AI error response from prior data. So yeah, great question, and and yes, the answer is we do have that data, and that's one of the reasons that the client is as comfortable as they are with letting an LLM kind of run a muck, right? Is the idea that humans do make mistakes now, and when they get escalated, you know, it's something where you can look back and be like, \"Oh yeah, that was a little bit off.\" You you correct it and you move on. Um, this is kind of unique in that again, it it needs to be, you know, precisely worded. Like one of the biggest risks is just that you give sort of off-label medical advice. But if the idea is that like, \"Oh, you misunderstood and you have to go back and correct yourself,\" that's okay, right? It's it's that's not a fatal error, right? So, a lot of it is that, you know, we think that we can get better use out of our humans by reviewing these situations, you know, than we can out of just having them push the buttons because they will occasionally push buttons wrong, right? Same thing happens as as with the robots. So, um, talking about mistakes, and this has been running for a while.\n\nYeah. Have you thought about fine-tuning a model?\n\n\nWith deidentified messages like,\n\nrunning it back through? Yeah, I mean, the the...\n\nSo the question was about, um, how we thought about fine-tuning. Um, we have already seen two major model releases in the time we've been working on this. Um, we, we generally don't think that fine-tuning is a great use of our dollars. Um, it, it obviously, it could be cheaper. We, I mean, one example is, um, we tried, you know, at one point to use Haiku, um, and, you know, Haiku is not even that much cheaper. It's maybe a third the cost, right? Um, we, we got to a point where we made our blueprints better, in part because, like, we'd sort of had some shortcuts where we just didn't have to be as, as precise with Sonnet, right? You know, we had to be more precise with Hiku, and then it worked. Haiku did not get the time stuff. Haiku was terrible at figuring out what times it needed to sort of put on things. And so the, the kind of thing we would have to do there, like, it either just kind of requires a smarter model, and there were smarter models from multiple people, like 04 mini really is both, you know, I mean, it's, it costs a little bit less than Haiku, I think, right? And it, it was every bit as smart as Sonnet. We chose not to go with it in part because it wasn't as transparent. Um, but so, in, in general, we don't believe that fine-tuning is warranted because we think the models are just going to keep getting better and cheaper, and that we, you know, we'll be able to kind of switch wholesale as opposed to having fine-tuned something.\n\nSo when you're going through that sort of, you had this like chattiness with the model where it was describing its actions and then calling tools. Is that like, is that an intentional choice? I feel like you could just skip that, just outputs it. Well, so yes, it was, it was kind of intentional choice, right? This is partly that we, we already get the rationale and sort of the general, you know, explanation of its actions. Um, but there are times where you want to be like, look, why did it do this, and you know, if it's thinking out loud, it's a lot easier to catch. Um, so yes, it's possible that we could eradicate some of that. We don't really think the juice is worth the squeeze.\n\nMost likely you're going to suffer a secondary. How does the current structure set up so that you have a new anchor point to see this person?\n\nYep. Uh, great question. So, uh, questions about essentially multiple treatments or coming back again after, you know, having gone through a treatment. Um, there are a couple ways to do that. So one is that, um, you know, again, depending on how you get there, if you scan a QR code, that can start kind of a new activation, so we can know that you're coming in a second time. Um, but people will write back after, you know, two months and say, \"I have a question,\" right? And, and so we either can just reactivate that conversation. The other thing is different treatments would usually come from different phone numbers. So there's a few different ways to kind of disambiguate, you know, what somebody's actually up to. But that notion of, like, you know, the same thing happened to me again. I'm starting the regimen over again. Fundamentally, you could just explain it. You just say, like, \"Hey, I had a miscarriage two months ago. I had another one. Can you help me?\" And it would reset itself, right? The LM is smart enough to do that.\n\nCan I share some, uh, please? Because there's plenty to, there's plenty to share.\n\nI mean, intentionally, obviously, the intention of improving two...\n\nIntent, I guess I'm skeptical that doubling the costs are yielding better...\n\nquestion you have like a funnel of how often the evaluator might be...\n\nsecond question...\n\nthey're both right in this case, they are.\n\nYes, is there, was there an intentional decision to stick with rather than switching model where in theory, hypothetically, you got a different brain looking at the other thing? Yep.\n\nAnd last question, sorry.\n\nNo, no, please. Um, was the inclusion of this eval?\n\nSo, were there other impacts besides just like this is a performance thing that made it rigid? Yeah, so questions are all about sort of the evaluator node and the and the processes. So, uh, the the shortest possible answer is, um, yes, we're also skeptical about it. And at the same time, we think that there's still value in trying, you know, essentially it's given getting a second bite at the apple, right? We do think that just having a different system prompt in the same conversation does occasionally deliver better results, but you could have the the virtual OA evaluating the complexity of its own situation. I don't think you could get it to evaluate whether it was right or not, just typically LLM are terrible at that anyway. So I think the, I think the basic answer though is that we wanted the flexibility in part so we could do things like try a different model entirely, right? Or, you know, have something where maybe you, maybe you did fine-tune a model specifically to catch these errors, right? Like that, I think, wouldn't be crazy at all. Um, so yeah, we wanted kind of that optionality, and at this point, you know, it's still early enough, right? Again, it's running, it's out there, like, you know, we're still tuning it. Um, if we get to a point where we're like, look, the only issue with this is how much it costs or like specific details about like how good it is at catching errors, um, we'd, we'd go harder at that. But we're pretty, we're pretty happy with the balance of it. It usually escalates situations that need review, right? It will sometimes screw up something just because it thinks that it was easy and it wasn't. That, that does happen. The same thing happens with humans, right? So like, we, we sort of are meeting the bar that we'd set for ourselves in the first place.\n\nThat's a good distinction though. The evaluator has a different task of sorts. It does. It's not really the same thing two times. Correct. The evaluator is looking at it differently, and it has this explicit... and so one thing actually though is that the evaluator can see what the VA is supposed to do, right? It can see the guidelines. So it can, it is able to basically say, \"You didn't do that right because I know what you were told to do and you didn't do it.\" And likewise, the the virtual OA can see the evaluator's confidence framework, and it can say, \"Well, I'm going to be scored against these things. You know, I better get it right.\" Again, this is very much more art than science, but, but I mean, you're asking the right question about, like, could we just have either a more optimal or a cheaper way of doing it? I think the answer is yes.\n\nOkay, let me keep going for a second. Please just hold your thoughts. Um, so again, this idea of, like, every interaction looks like this. It is a starting state, a conversation, an ending state, which then goes back to the system. And so I wanted to show you here was if I go back to a conversation, right? In fact, let me just see what I got here. Oh, yeah. In fact, this, this answered. So I said, \"The pregnancy symptoms have decreased.\" The next question in the blueprint is, \"Do you think you're done?\" Right? You know, \"Do you believe that, you know, the miscarriage and sort of the the changes that these medicines were supposed to elicit have have completed, right?\" Um, and there's basically one more message after this which kind of confirms and says, \"Hey, let us know if you have any questions.\" But that kind of interaction, right, back and forth, back and forth, assessing the state as it currently exists is what this is built to do. And we're compressing after every one of these interactions into only the changes that happen to the state in a given time. Right? We're not saving, you know, in Langmith, we're saving the entire conversation, right? This data, sorry, that's the wrong tab. This data, you know, about like what the virtual lawyer and the evaluator said to each other and what tools they called. This is preserved in Langsmith. We don't get rid of this, right? But we do not save this in the state on the blue box, right? We, that's not part of the patient's interactions with us, and we don't reload it every time you go back with with a new message because that would ultimately both confuse things and and blow up the context window. So that's the way we've, uh, we've chosen to do it. Um, so let me, let me now show you this. Um, I have another conversation here which actually needs response. So, I'm going to grab this and put it in the sandbox so you can see what this looks like. Sorry.\n\nI just got persistence.\n\nUh, sorry. Persistence if you only have what?\n\nYou, you just don't save the the process of the model talking to itself, right? You, you have it. You can refer to it if you need to. It's a debugging tool. Yep.\n\nYep. Input and output is all that we snapshot in the in the larger system.\n\nOkay. So now let's look at this. So I think that's actually the wrong one. Let me go...\n\nsorry. Find this again.\n\nAll right. Yep. So this, this right here actually, you know, I can, I can, I don't have to go to the sandbox to look at this. This is an example of what happens when things are complicated enough that we're asking for human review. Okay. So in this case, I've just started this conversation. All right. And I said, \"Yep, I got my medicine came from the clinic. Here's my time.\" Now, this is a moment where in the treatment a lot of stuff is happening. I'm figuring out what time zone they're in, right? And I'm saving that as part of the patient data, right? So, in this case, I said I was on West Coast time. So, my time zone offset is 420 minutes before UTC. Um, I am going to a new phase of the treatment. I have my medicine. You know, now I'm not in onboarding anymore. I'm actually taking the medicine and I'm sending multiple messages. So, in the confidence framework, and I think I can find this, but uh, I won't dig into it until we get there. Um, in the confidence framework, we say when you have all of these changes at once, you should deduct from your confidence score. So, you see up here, this confidence of 70%. I have the threshold set at 75. So, for anything that's below 75%, I stop and I ask a human to either approve, right? So, if I were to approve this, it would just say, \"All right, these changes are fine.\" And in this case, the changes are fine. Um, or I could give feedback, right? I could say, and I'll try this now, and live demos be damned. Um, let's say, you know, I want to say, please mention the patient's name in your, in your next me, in, in your messages or in your message. So, I'll say submit feedback. Okay. And I'm working through about this because it's kind of an operational detail. Um, this is now going and thinking again. So, I'll have to reload this in a minute and and see what happened. But what's actually happening here, if I go over to Langsmith again, which I should be able to do, is see that what's happening now is that it is restarting a thread that I already started in progress. So the one exception to us wiping out its brain and reloading everything is when you come back with this feedback, right? Because you wanted to basically be able to pick up right in thread and say, \"Hey, you just did that wrong, but everything else here, like you need to be able to see how you got to that place, right?\" You know, so make the right decision and and finish it up. Um, and so I think let's find out here.\n\nAll right, still thinking. Um, oh, there we go. So, you can see the only change that happened here is that it mentioned her name, right? Otherwise is the same thing. Same time zone offset, same treatment phase, same reminder. Um, you can see the rationale here. Um, and and you can see here the rationale even includes this. I changed it to update the name. Now, you could imagine doing a version of this where I just had a little edit box and I said, \"I'm going to change this message.\" We chose not to do that, right? We want the LLM actually to drive these changes. We think that it's better for humans to speak to them as though they're talking to a person. Um, this is a debatable choice, but it is a choice that we made. Um, and part of that means we can be very, very flexible about the treatment, right? We can just give feedback on the situation rather than having to build some sort of tools that are are are flexible enough to deal with all different types of treatments. Um, but so here I'm just going to go ahead and say approve. And now those messages go out and the changes are made, right? I have, you know, my patient local time set and I know I'm in the next part of the blueprint. Okay, I'm going to pause here. I'm about to jump over to code. I think we have something like 45 minutes left. Um, any questions on any of this so far that are not? I just want to see the code because I can do that part over there.\n\nHave you heard any customers?\n\nYes. Uh, question is about feedback from patients and that it is emotional. So yes, absolutely. So remember this is a system the patient or the client is already running, right? So fundamentally, they already believe that they're talking to humans even when they're not exactly right. Even the humans pushing the buttons are just calling up essentially bot generated responses. Um, when things get emotional, um, humans can step in. You know, we, we tend to steer them towards kind of approved knowledge based responses. Like you don't want this to be something where it goes completely free form. There's, there's legal and other reasons not to do that. So by stepping in and having LLM make the decisions doesn't really change the kind of current context of these treatments. They're already getting, you know, basically this sort of medically approved feedback, you know, based on a certain flowchart, and if it goes somewhere, you know, a little crazy, the escalation point is usually to call someone, right? It's not, you know, we keep on talking forever in text because that's messy. Um, there are a bunch of points which I'm not going to be able to demo here which basically just say, \"Yeah, I'm sorry, I can't answer that question. Call 911, go to your doctor, whatever it is,\" right? But that, that is usually where it...\n\n\nGoes from there.\nThe blueprints look a lot like that.\nWell, it's a great question.\nSo, honestly, part of it is just that we needed to have something that the patient, or not the patients, the client was actually comfortable maintaining, right?\nBecause remember, part of it is that we do not want this in code, right?\nWe don't want this to be something where you can only maintain it if you have a technical person.\nThat's the problem they had before, right?\nAnd so just to jump over for a second, I'll show you what this was kind of looks like.\nSo this is essentially the thing that the client is maintaining.\nAnd I'll blow this up a little bit.\nI realize that is small.\nUm, but the idea here is that we're using terms and, and you know, we we'll see a bit more of this in the code.\nWe're using terms that are defined in the framework.\nA trigger is, you know, something that happens, you know, essentially after an event, right?\nUm, you know, we have the the conversation of these messages.\nWe always tell the LLM why this is important.\nIf we just had this this detail and we just said this is the message you send, I don't think it would perform as well.\nIt's much more helpful to actually give the LLM justification for why it would say something because then it makes better decisions.\nUm, one of the many quirks.\nUm, go ahead.\nOn that.\nI'm not sure if this is code or not for the next part, but how how complicated, how simple statements, or if you have any, any actually got lost in the Oh, sure.\nYep.\nSo, so the question was just about, you know, essentially why why do we have this framework and and why is it maybe not more declarative, right?\nIn terms of like specifically if then and that sort of thing, right?\nActually I I use similar with with a different index and like I got indus, so I couldn't go like very complicated, not a lot of nested, got to be like one, two levels, right?\nMy question.\nSo I I the so the answer is just about again how do you how do you define these things as clearly but you know maybe not complexely as as possible.\nRight?\nSo, um, this this framework tends to work where you're really just saying, look, I'm giving you this approved language and I'm trying to give you in the bold statements here, right?\nPrimarily, I'm trying to give you a sense of, you know, what what the conditioning really is.\nBut part of the reason that we did it this way is because, you know, if the patient writes back after this thing and he says, you know, yes, I have the medication and I took the pills and my stomach hurts and I'm confused, right?\nI mean, it could be all these things.\nWe wouldn't want to represent something like that in a flowchart.\nWhat we really want to do is just say, \"Look, this is the outline of the thing.\nYou can see it.\nYou know, if you need to jump ahead, jump ahead and don't ask the patient questions they've already answered.\"\nIt just turns out that this this framework really does work pretty well for letting the LLM do that sort of thing.\nIt's I know that's kind of a magic answer, but pretty good at it.\nUh yeah.\nNo, I mean Claude Yeah, Claude mostly nails it.\nMost of them do.\nYes.\nDoes including the instructions response quality?\nUh, sorry.\nWhat do you mean by including the instructions?\nIncluding the reasoning.\nOh, the reasoning.\nI I So, question was do does including the reasoning help with the response quality?\nI think it does, right?\nI mean, this is one of these things where we started also by borrowing from human documentation, right?\nSo, this was a process that was originally explained to humans who were going to push the buttons.\nAnd so we took a combination of flowcharts that existed to explain the flow of the treatment and these kinds of you know this is the message that you should send in these situations and and this was kind of the the hybrid output of those two things.\nSo I wouldn't say we did aggressive testing on is it is it really better or is it just that you know this is good enough.\nIt's more that like we started with this framework based on the human materials we had.\nA follow question to that.\nYeah.\nDid you find any sacrifices that you had to make as form?\nYeah.\nSo question is uh maintaining this document as human readable versus LM.\nYes, there are trade-offs.\nI think they're still worth it.\nWe we may change our mind at some point, right?\nSo, you know, imagine the workflow here being um, you know, this this Google doc is maintained essentially by our physician's assistant, right?\nShe is the co-owner of the blueprint maybe next to me.\nUm when we when we make changes, we talk about them together.\nWe recommend in this document and then accept them and then effectively I I export it to markdown and check it in.\nRight?\nThat is going to change a little bit.\nWe're going to build a lot of these tools into the database and so that that's really where you'd be doing this instead.\nUm but because this is human, you know, maintained, right?\nBecause it is basically, you know, still driven by the team.\nUm yes, we we are making a trade-off.\nI don't think it's a trade-off that's that's super damaging.\nBased on your current design, yeah, just now when you do the thing, what does change after is it like a one time or does it improve your answer in the future or even changing the So at the moment?\nNo.\nAnd the question was just about uh the approve uh sort of defer um you know feedback mechanism.\nUm so actually I'll go back and just show this really quick.\nThis should be done now.\nUm there we go.\nUm again we are we are saving this in the sense that I can see this in LangGraph, right?\nI can look at this and I can say well in you know these cases where an approval was needed and in this case like just just as a a visual you know sort of feedback whenever you have this graph null start right that that is one of these cases where you know there was an approve feedback defer choice um I could filter by this and I could look at all of these things and I could say well what kinds of things were we actually trying to approve or give feedback on um we don't learn from them right we we as humans will maybe update the blueprints we do not put this back into training data again for a bunch of reasons which are are kind of specific to the situation um but you So you can see here that like I can go all the way down here and I'll try to find this quickly.\nUm and you'll get to a point where the human says all right yeah here it is.\nSo we get feedback from yeah from the humano.\nThis is essentially what happens whenever I push that button and I say give feedback right the humano has feedback about your unscent messages.\nThe feedback is mention their name.\nUm it just goes right back to business.\nIt's like okay let me look at the messages that you know I was sending.\nI'm going to update with this one.\nI'm gonna probably delete not sure actually no it just updated that one in place we rescored it one thing we've said is that we do not change the confidence score on something that a human reviewed we leave it where it was right we let them review it again right and so in all these cases this this is also a much quicker you know simpler sort of operation right and so you can see the evaluator here is basically like yep that message is fine but we're not going to do anything you know really to change the the overall score um so you know that's the kind of thing that you know we can look at afterwards Right.\nBut we are not at this point at least, you know, really trying to feed that back into the model.\nIt's really just for the blueprints.\nJust get a sense of your metrics.\nUm, a lot of these are, you know, over a minute and it says about a couple hundred thousand.\nHow do you kind of look like a necessary evil?\nThe time it takes, the cost.\nNo, no.\nI mean, it it is a necessary evil and and actually just to point it out, um, these costs I don't believe are correct.\nUm, one of one of the shortcomings of LangGraph and I think they've admitted this in various forms is they don't really take into account the caching.\nUm, so these costs should be lower than what you see here.\nUm, but but fundamentally, yeah, these are expensive operations and you know we could change we could change some of them at the potential cost of higher error rates, right?\nLike we could try to cache more and have you inherit threads in progress and it would be faster, right?\nBecause you've already loaded everything.\nIt would be, you know, potentially you're not reloading any context and so, you know, you're you're spending maybe less on tokens and you just might have a higher error rate and and that's, you know, a thing we are trading off.\nIs there some kind of knowledge base that your model is taking to depending on the medicine?\nUh yeah.\nSo the question is just uh in terms of the the knowledge basis.\nSo let me let me actually jump over and just show this really quick.\nSo I mentioned these blueprints.\nUm I I'll jump over now really into just what the the the implementation looks like.\nSo you can see over here, you know, this idea of for a VA, right?\nWe have a handful of documents here that are again exported into Markdown.\nUm and I'll try to blow this up because I know these are small.\nUm let me just shrink this down.\nOkay, so the idea here is that you know I've got all of this, you know, uh sort of framework data, right?\nThe idea of defining what do I mean by a blueprint, right?\nWe're doing we're we're defining this every time not in um the the prompt, right?\nWe're doing this as part of the context window in part because we do want this to be really flexible.\nIf you want to change the terms um you should be able to do that, right?\nWe don't want the treatments to be hamstrung by by terms we use for other treatments.\nI define anchors.\nI talk about schedules.\nI talk about scheduled messages, right?\nSo all of this stuff exists in part just to to lay the groundwork.\nAnd then this framework, right, is now referring to specific documents, right?\nAnd so you can see here like I again these documents are all referring to each other.\nSo I can go through here and I can look you know and and click on these links and go straight to other things if I want to do something around the knowledge base.\nSo the way that we do that is this triage idea.\nUm so you know if something happens that a blueprint doesn't address right.\nSo the way that we tar it is first check on the blueprint.\nIf you have approved language use it right send it send it back for human review whatever you need to do right but use that approved language.\nIf you don't think you can answer that question you go and look at this which now again is is self-referential.\nWe don't read the entire scope of medically approved knowledge all at once.\nWe let the Ellen decide are they complaining about stomach pain or bleeding, right?\nYou know, if I can't find anything in any one of these, I have a larger knowledge base, right?\nWhich is, you know, sort of just a laundry list of like random questions people ask.\nUm, we've chosen to do it this way in part because it is human readable.\nIt mirrors something the client already mostly had, right?\nThey already had a lot of these structures.\nUm, and you know, we fundamentally did not believe that it made sense to overprocess, you know, things like a rag.\nNow I will say that for the thing that we have like kind of a backup you know sort of knowledge store which is almost entirely a CSV that probably is suited for rag right it would be okay to use a rag for that it doesn't get used that much right so in some sense it's just not worth implementing that way at least not yet.\nAll right.\nUm yeah.\nYep.\nThat is prompt level.\nSo we the the virtual OA and the and the evaluator both have relatively small prompts which I can show.\nSo let me just see if I can find them here.\nUm those prompts are are they do reference each other right?\nSo imagine this being um you know again built on the line chain stuff.\nSo the base agent class um this prompt is basically aware of the other agent, right?\nSo in this case it's just two.\nSo the prompts do speak about each other.\nThe evaluator knows about the virtual away and vice versa, right?\nYou know the things that we try to do and you know this is I think normal prompt engineering stuff for people who have really played with this stuff.\nYou have to tell it how to take turns.\nYou have to tell it that it you know if it gets called on it has to talk, right?\nLike you know one one problem we have that we have to sort of frequently do retries on is the LM thinks that everything's done.\nIt doesn't say anything and and the whole thing dies.\nUm so you know you have to talk but then you can be done, right?\nYou just have to say something.\nUm we have the basic idea of you have to determine this overall confidence score but we don't include this in the prompt because we want to be able to show it to the virtual OA as well right so the details of how you score something is is is factored out but the notion of here's who you are here's this other guy is and here's how you work together that is in the prompts.\nOkay actually on that note let me jump over and actually show you some of the the confidence stuff and the the guidelines so um I'll start with confidence I'll get into the guidelines which are much much longer This again is you know intended to be mostly LLM readable.\nThis is not something the client generally maintains, right?\nSo this is not in the same category as these blueprints.\nBut the idea here is that you know I've got this confidence score and you know I am trying to figure out across these multiple dimensions.\nDo I know what's going on?\nYou know here are some examples.\nWe we are trying to be as prescriptive as possible with examples of these different situations.\nUm do I know you know the knowledge that I need to know?\nHere's an example which might speak to your question actually over here.\nYou know, the idea of do we want the LM to use its world knowledge to figure out that when I'm talking about an antibiotic and I give a specific antibiotic that it applies to the whole class of them.\nYes, that that's a risk that we're kind of willing to take, right?\nWe don't need to have an explicit this specific antibiotic is safe for this treatment, right?\nThat would very quickly spiral out of control.\nSo, we do have a handful of places where we ask it.\nUse your own judgment, but refer to, you know, the the knowledge base and the blueprints.\n\n\nFor your baseline.\nUm, and then so, after I get through these categories, then I have this idea of deductions, right?\nAnd the deductions here, um, are specifically things like, you know, you should deduct from the overall score, not the individual messages, right?\nBecause an individual message could be like, yeah, this is exactly from the blueprint, like it's the right thing to say, but overall, these situations can be complicated, right?\nAnd so what we're trying to do is explain that such that it can again score the overall interaction in a way that surfaces it for human review.\nUm, okay, so I'm going to move on to the guidelines because again, there's just a lot more in here.\nUm, this is long enough that I'm not going to review everything, but I'll try to get to some of the biggest parts.\nAgain, some of this is really simple, right?\nLike tool calling.\nUm, one issue we've certainly had over time is, you know, fabrication and and, you know, honestly, instructions like this do help.\nUm, you know, do not make up a tool call.\nWait, wait your turn, right?\nCall the tool and step back.\nUm, there's a lot of stuff around time, right?\nThere is a lot of stuff around, hey, you need to ask about it in the right way.\nYou don't ask about it in a way that forces someone to tell you where they are.\nRight?\nPeople are very sensitive about this.\nThey don't want, you know, people knowing where they physically are, but you need to know what their time is so that you can schedule the messages for them, right?\nUm, you want, you know, when you work with time, um, you have to use things like, you know, ISO time stamps, like that's how the rest of the system works.\nUm, but calculating these things and keeping them all straight, it requires a relatively smart model.\nSo, a lot of this, you know, has has sort of grown over time to just work with the idea that, you know, this is how you can talk to models about this and do a pretty good job, um, setting anchors, scheduling messages.\nAgain, these are all the things that are core parts of the system.\nThis is not treatment specific, right?\nThis is all written to be generic enough that I don't have to rewrite this every time I add a new drug, right?\nWhich which is one of the core requirements, right?\nWe did not want to have to do this in code.\nYes.\nSo, this is a very big document.\nYep.\nUh, have you experimented with the caching because this doesn't change?\nYeah, correct.\nNo, we have.\nAnd so, right now, and I'll get to caching actually in just a second.\nI'm just trying to manage time here, but I do have time for that.\nLike we are, we are doing some explicit system prompt caching and then we are caching explicitly, um, you know, the the multiple turns of the messages such that, you know, each operation is, you know, I think the average operation with just sort of the baseline stuff is maybe 10 to 15,000 tokens, right?\nPer turn, all cached, it adds up, right?\nSo, you know, you do have maybe the average cost to generate a single message somewhere in the 15 to 20 cent range, right?\nIt it's not cheap, but we are caching as aggressively as we can.\nWe have thought about things.\nAll this is brand new, right?\nThe idea of the of the hour cache, you know, that that Claude just introduced.\nIt's not clear to us that that would help because, you know, we can't guarantee that the patient's going to get back to us within, you know, either five minutes or an hour, right?\nIt's just it's a bit of a risk to take at the system level.\nUm, but if anybody here actually knows more about cloud caching than I do, please talk to me because like we we ideally we would like to cache a lot of these documents.\nUm, it's just not clear if we can do that across sessions.\nIt's not clear if, you know, we would get the benefits that we're looking for.\nSo, we've just tried to be as aggressive as we can within a single conversation.\nThat's a very long list of guidelines.\nHow did you come up with it and how do you optimize?\nYeah, I mean the look, the the real answer is that I mentioned before that, you know, there's a few thousand lines of code and a few thousand lines of prompt.\nThis is most of that prompt, right?\nI mean, this is a lot of it.\nUm, it is something where, you know, we have tuned it over time.\nThis is you know, myself as well as you know, the the the physician assistant, like we have come up with something that we believe is fundamentally, you know, pretty good at handling these, you know, generic situations, and when we find edge cases, we we just modify these prompts.\nIt again, it is not perfect.\nWe could definitely think about subdividing this.\nWe could think about moving some of it into the prompts.\nUm, but we think this division is, you know, roughly correct for keeping it generic so that it's, you know, it handles a bunch of different treatments and it it handles the situations that we see across treatments pretty well.\nRight.\nYou will have cases where you're doing medicine that's all in one day.\nThat that's relatively unusual because, you know, that's something you just send instructions home.\nUm, you know, but there I mean, I'll show an example around ampic, you know, that's weekly, monthly, right?\nLike there's there's much longer durations.\nWe've tried to get to a point of balancing it where, you know, we we do end up with a good result.\nOkay.\nUh, yeah, back there.\nYeah.\nYeah.\nYeah.\nSo, question is just about the prompt length.\nUm, so again, not not to dismiss that out of hand, in Claude terms, this isn't actually that long, right?\nI mean, this is like I said, I think on average 15,000 tokens.\nUm, it still leaves a lot of the window, you know, behind, right?\nIt is it is not actually so long that we start seeing really crazy behaviors until we start doing multiple turns, like multiple conversations in one thread, right?\nThat's where it starts to blow up.\nUm, so we we just genuinely have not gotten to a point where we're like, my guidelines are too long.\nYou know, the guidelines could be a little shorter, and I think we we have optimized them in various places over time, but, you know, we've we've crammed this into a box where we really can sort of process one situation all in one gulp without really feeling any any pain.\nOkay.\nYeah.\nAny examples on tools that you have?\nYou mentioned tools.\nI'm not sure.\nYeah.\nYeah.\nOh, no.\nSo, I I can I can share a little bit of that.\nSo, let me uh let me go down here to the tools code itself.\nSo, so one thing to note, this is a hybrid, and I think I mentioned this really earlier on about um there is some stuff coming from an MCP gateway, right?\nSo, in this case, you know, I'm loading from files.\nIt's just the file system MCP.\nAgain, all localized to our VPC.\nSo, there's nothing crazy going on there.\nUm, but I could instead load from, you know, my database, right?\nI could choose to to have that be the place where we interact.\nThere's a bunch of other tools, right?\nAnd so, you know, this this actually is where probably most of the code in my app actually is.\nUm, the list of tools is essentially down here.\nAnd you can see it's things that enable interacting with the state, right?\nSo, all of these functions that are looking at anchors and messages and confidence and the treatment and patient data, all of that stuff is local to my graph run.\nI don't have an MCP for it.\nI could, I just chose not to.\nUm, and you know, in this case, like this code just lives in this Python app.\nYou could you could very much refactor this out.\nLike the state can still live here and the code could be somewhere else.\nIt's it's you know, it's up to you.\nUm, but one note actually about all of this stuff is I'm trying to find a good example here.\nSo I am aggressively using the command object.\nUm, for anybody who who has programmed with LangGraph, the whole idea behind this is that at any given point you're able to pass back a message and this particular thing is just an error message, but like you know you're able to pass back a message and a place to go, right?\nSo you can say here's the here's the response and by the way I know that the evaluator asked for this so go back to the evaluator.\nYou can actually get around some of the graph routing um this way.\nAnd so we we've we've definitely you know tried to work this into both the MCP tools and the state tools that we have.\nYeah.\nYeah.\nUh, so question was about how to how do we improve the prompt?\nSo, um, it was really a fusion of um, we were able to go from essentially the physician's assistant who had the most experience with tricking things, right?\nWas coming up with tricky situations, right?\nSo, we were able to test a lot of the edges really just with her, you know, having her pretend to be the patient.\nWe then scaled up to the full team of operations associates who, you know, then tried to trick it at a higher level, right?\nAnd, you know, were putting out things that they'd seen from patients themselves, you know, trying to sort of um, you know, get to these complicated cases.\nUm, and then, you know, with real people, you know, we're able to take that a step further.\nUm, but with those first two levels, we're we're not seeing, you know, a tremendous amount of stuff that's not expected.\nAgain, this system exists.\nIf we'd been doing this from absolute scratch, I think we would have a lot tougher of a time coming up with what we think the edges are.\nWhereas, you know, this is a system that already exists.\nThere's a lot of conversations to draw from.\nWe're able to run some of those back.\nSo, we'll look at at conversations in the old system and replay them here and essentially just try to figure out, you know, where the edges are.\nIs there you state.\nSure.\nSo, is there a question about are the questions about um how to decide what goes in the state or not?\nUm, I mean the I guess the short answer is everything that comes in in that initial payload, which I'll go back over here for.\nUm, all of this stuff over here, this is all state.\nUm, there's not really a distinction, like everything that comes in to sort of preload the conversation is state.\nSome of it is editable, um, some of it's not.\nI'm trying to remember examples, like here examples are you can't change the source.\nI couldn't say, you know, in the context of the LM operation, this is not an ail patient anymore, right?\nThat's the LM is not allowed to do that.\nUm, it also can't change past messages, so the LM is not allowed to look at the message queue and say message five that went out three days ago no longer exists, like that function doesn't exist.\nUm, so we've sort of just calibrated to where the only things it can do is read the entire state, modify the patient data, modify messages, modify anchors, like like unscent messages.\nSo it's, you know, it's just a software choice, like that's how we architected it.\nIf you have a session that went back window summarize it.\nYeah.\nYeah.\nSure.\nI mean, I I think the I think the real answer though is that hasn't happened for us.\nLike the way that we've structured it, there is not a single thinking operation that that goes long enough to blow things up.\nUm, but but I will talk really quickly about um retries.\nSo we do have the notion and I'll just pop this up, maybe just an easier way to see it.\nUm, so we do have the notion inside the graph of, you know, again, the virtual OA talks to the evaluator, you know, at at a certain a certain point in the thing, everybody uses tools, but then there are cases that will cause the the graph to retry a certain operation, right?\nSo one of them is um one of the agents malforms a tool call, right?\nIt tries to call a tool, but it uses the wrong JSON and you know, otherwise things would have died.\nWe can detect that, we delete the message and we say you screwed up that tool call, try again, right?\nThat you know, keeps it inside the graph, right?\nEssentially, this retry node is then able to loop back via that um command message.\nUh, it's not shown here on the graph, but via that command object, it can say go back to the virtual OA, try that again, right?\nWe also have some cases where again, you know, we expect the model to talk and it doesn't, right?\nThere are just a bunch of cases where Claude will just end its turn prematurely.\nWe detect that.\nWe say you have to say something, right?\nLike literally, that's the message is like don't just say nothing.\nIf even if if you're going to end your turn, just say I'm done, right?\nThat's enough, you know, for us to keep the logic going.\nSo it's things like that.\nLow confidence from the evaluator will be trigger.\nI'm sorry, say it again.\nLow confidence coming from the evaluator.\nNo, no.\nSo low confidence is something we want to pass through to the human, right?\nSo low confidence is a valid response to the graph, right?\nYou know, like I have a low confidence message, I want a human to review.\nThat's fine.\nUh, yeah, build your system prompt up.\nIt's long.\nIt's pretty complicated.\nIt's not the system prompt.\nNo, no, that's thing.\nIt's that's what that's why we separated it.\nSo yes, the guidelines have expanded over time.\nYep.\nOkay.\nYeah.\nHow do you make sure you don't Yeah.\nPerfect time to talk about evals.\nSo we're getting towards the end.\nUm, let me talk about evals a little bit just to sort of give you guys a sense of what we did here.\nSo, um, this was this was a weird one because if you go to Langmith and I'll just I'm not sure I can find the exact place where this happens.\nUm, but there's is it under maybe it's under data sets, I I forget.\nThere is a place in here where you can basically say I want to run, you know, an evaluation against, you know, these these, you know, this this data set that I've defined in lang.\nUm, I do define data sets in lang.\nSo I can see here, assuming this loads up, which hopefully it will.\nSo I've got a happy path data set here that I defined in length.\nI this isn't the whole thing.\nAgain, some of this is redacted, but um, if I look at these things, what I'm really doing is I'm saying, okay, this is an interaction that we had in the past.\nUm, you know, this was one that I ran last week.\nUm, you know, here's, you know, the the input, right?\nThis is the last message from the patient.\nThis conversation, you know, a I've got this initial state here that\n\n\nI can look at, right?\nSo, I can see, you know, what was going on in the first place.\nUm, I see this conversation and, you know, I get to the end and I get a message out which says, great, you're ready to start.\nOkay, so this is one part of my happy path data set.\nThe eval that I'm running, um, are fundamentally, it's a, it's a custom harness.\nI'll blow this up a little bit.\nHopefully, it's visible to most folks.\nUm, but again, the idea here was that we couldn't just say, you know, when I ask for, you know, what the weather is in San Francisco, it gives me back, you know, cold and, you know, foggy.\nUm, it had to be, here's examples of these input states and then, you know, let's evaluate it, you know, sort of an LLM as a judge form what the output state looks like with the caveat that one of the big things we wanted to test was things like time operations.\nSo, I can't put in an eval from three weeks ago and run it now and get equivalent times.\nI have to either give it really specific guidance on how to handle time or I have to replace all the time stamps.\nUm, we ended up sort of doing a hybrid of both.\nUm, and so what this did was my eval suite is a custom Python app stapled to a bash script.\nThis was all client coded.\nUm, I got what I wanted, but I can't vouch for much more than that.\nUm, I load data sets right from Langsmith.\nI call essentially the medical agent via, in this case, like I'm running this locally, like I could run this against my cloud LangGraph instance, but I'm literally running this against LangGraph on my laptop using that data set from Langsmith, having pre-processed a bunch of stuff around dates, times, you know, sort of circumstances so that when I get back the result, I can not confuse the LLM as a judge about whether it's right or not.\nUm, and I'm using this LLM rubric right to do this.\nAnd so let me see if I can find my rubric here.\nSo, well, here's a couple examples.\nSo I've got this YAML.\nSo this is just this is prompt fu and how it works.\nUm, what I do is I basically say, look, I'm trying to test, you know, this custom thing that I'm going to call, you know, essentially with my, um, you know, my custom harness, and then I want you to evaluate it, you know, with an LLM, and in this case, I think it's using GPT40.\nUm, you know, this the valuation rubric is basically, is everything basically exactly the same?\nDo I see minor discrepancies, but I don't think they're a big deal?\nI mean, we're keeping this very fuzzy.\nWhat we really want to know is, is anything completely busted, right?\nAnd then there are cases where it is.\nUm, I had to put in specific notes here, like, hey, don't be picky about like the, you know, different wording that you might see in something like an anchor, right?\nSometimes it'll say they will take the medicine, it says they did take the medicine, like, who cares?\nLike in this case, the spirit of it was right.\nUm, and so, you know, and then times and dates, like there's some specific language here.\nSo all of this turns into basically this guy right here.\nSo, um, when I look at this, and again, I realize there's a lot of text here.\nUm, it's not worth looking at all of it.\nI ran these three examples from my data set and I got, you know, basically a passing grade, right?\nI'll go into the fail, one fail, one pass in a second, but in each of these cases, right, I can see what the LM as a judge said, right?\nSo, if I go down here, this is GPT40 going, you know, opining on, well, this is the source data you gave me in the sample output.\nHere's the run I just did.\nClose enough, right?\nUm, you know, it does point out some things, right?\nSo if I ran these evals and I was like, well, it's actually a big problem that, you know, the reminder, you know, unscent message didn't show up, right?\nI could make a choice to do that.\nI could, I could strengthen my rubric, but the way that we did this was just to say, look, at any given point, we do need to be able to test the current state of the system.\nWe want to do it against, you know, first the happy path, and then we can certainly do it against edge cases.\nUm, we're actively maintaining this.\nI think this might change once we actually hand this over, you know, more fully to the client, right?\nWe want them to have all the protection they might need, but it's this style, right?\nThis style of email.\nDoes that answer your question?\nMore or less.\nOkay.\nOkay.\nUm, yes.\nGo ahead.\nYeah.\nSo, I mean, the short answer is yes.\nSome of that I'm redacting for a couple reasons, but yeah, more or less, we started with a happy path.\nWe do have a handful of specific like, hey, this is busted and it's frequently busted.\nLet's make sure it's not, um, you know, that, but it's just a different data set.\nYeah.\nUh, yeah.\nWell, so again, the part that I'm showing here is entirely Python in a line graph container.\nUm, I would guess it's, you know, maybe 4,000 lines of code.\nUm, most of it is honestly the tool calls like that that just, and I, I'm sure I could refactor that to be shorter also.\nUm, it's not much.\nIt's really just enough to run essentially this graph, right?\nYou know, I have to have the routing between all of it.\nI have to have the tools that it can call.\nUm, everything else is in the prompts and the guidelines, right?\nAnd so, you know, it really is more English than it is code.\nOn the other side, right, on the other side of the box, right, this thing, um, that blue box is, I don't know exactly how much code, but it's entirely, um, Node and React and and Um, and frankly, I haven't been that involved in it.\nSorry.\nYep.\nI also industry.\nYeah.\nHow do you think about this one?\nYeah.\nYeah.\nGreat question.\nSo, uh, questions about scale.\nSo, let me actually jump over and show you one thing I should have probably already shown, but I I forgot.\nI mentioned before the idea of us doing different treatments.\nSo, um, what I did in in in this particular case, so you know, again, we were focused on on the the early pregnancy loss.\nUm, I did this, in fact, I think I have this sitting here somewhere.\nSo, let me zoom out and find it and then I'll zoom back in.\nSo, I took this to Klein and I basically said to Klein, \"Hey, I've got Yeah, this should be it right here.\"\nUm, I said, \"I'm looking to make a new treatment, right?\nI defined one for Avila.\nHere's the structure, right?\nHave a look.\nUm, here's the link to, you know, Noon Nordisk's suggestions on how to dose Osmpic.\nUm, make me something new, right?\nUm, and it basically went through this process, and I'll I'll shrink this down.\nUm, and created, you know, a basic treatment, you know, for Osmpic.\nIt read these files.\nIt then decided, all right, I got it.\nHere's the thing I'm going to do.\nUm, I just said, cool, go for it.\nAnd here's a couple of tweaks based on the thing that you said.\nSo, I, again, this is my client process.\nI use this all the time.\nUm, and I ended up with what I think is a pretty serviceable treatment, which I will very quickly show you here.\nIf I go back to these conversations, I'm pretty sure I named these people all O.\nSo there you go.\nHere's Oliver with Osmpic.\nUm, and you can see it's still Ava.\nI didn't change that, right?\nI, I, you could obviously get to the point of having it be a different personality, but in this case, this is Ava with a new treatment asking all about Ozmpic pens and helping me figure out their time.\nSame as we were before.\nIt's a weekly injection.\nI didn't change any code for this.\nI literally threw this through client, got a new set of treatments out and it just kind of works.\nYes.\nYeah.\nIn your graph that for it's for catching uh the retry node.\nThe question was, um, it's for catching those errors around um malforming tool calls is one easy way for the graph to terminate.\nRight?\nSo if it, you know, forgets a bracket and you know, sends back something that's invalid JSON.\nUm, Claude does this a lot less now.\nLike Sonnet 4 is pretty good at it, but um, Sonnet 3.5 was not nearly as good.\nSo, we can detect that and we can just say, well, you're trying to make a tool call because I see certain things in here.\nI either see tool call ID as a parameter.\nI see weird brackets, you know, we can parse that in code.\nUm, and then again, we wipe that message out and we go back to the guy that called it and said, \"Hey, you up, pardon my French.\"\nUm, you know, try again.\nAnd so, that idea of, you know, the retry node, there's a handful of those situations where we aren't going to take over and do it in some deterministic way.\nWe're just telling the LM, you made a mistake.\nHere's the character of your mistake.\nTry again.\nAnd we do have to wipe out its memory of that mistake because it can get very confused.\nLike one of the reason, one of the ways this happened a lot earlier in our testing was it would malform tool calls and then hallucinate the results, right?\nAnd so if you left the message in there, it would think that it understood the blueprint even though it had made the entire blueprint up, right?\nSo I don't want to sugarcoat where like there are some weird cases that if you don't very carefully control for it, you can end up with some very bad behaviors.\nBut we were able to catch the main ones and essentially just give it another shot.\nI was looking for like a loop.\nOh, all right.\nSorry.\nThe reason there's no loop, this this is just totally, this is actually one thing where I think Langsmith and LangGraph could be better at this.\nThe retry node is capable of calling back to the other ones using the command object, but it doesn't show up on the graph.\nSo it turns out like they were very invested Langsmith was or LangGraph in graph flows and then they introduced this idea of, well, you don't even really have to define it in the graph, you can send anything anywhere, and so that's what we're using.\nYeah.\nYeah.\nNo, sure.\nBut remember, sorry, the question was about confidence scoring and how do we get it to sort of to be higher.\nUm, we want the score to be lower when there is a complicated situation, not because we think it's wrong, but because we want a human to review it.\nUm, well, so sorry, what do you mean?\nCorrect.\nYes.\nSo if if the confidence score is above the threshold, meaning higher than it, right?\nSo let's say I have a confidence score of 0.9.\nWhat that could mean, right, is that I am sending multiple messages at a time, but there's no other reason for me to think that those messages are wrong.\nUm, we have chosen with the client to set the threshold lower than that because they don't want their humans having to get involved every single time something is a little complicated.\nBut we've agreed that if it's below 0.75, they should.\nIt's purely just a calibration, right?\nIt's you you decide if you wanted your humans involved in everything, set the conference score to 100, right?\nYou could see every single thing that happens.\nYou're just clicking approve all day.\nYou're George Jetson.\nBut um, that's hopefully not what people actually want, right?\nYou want a bunch of these things if they are high confidence and you know, fundamentally recoverable, let's say.\nLike, you know, there might be certain circumstances where you don't want a message automatically sent out.\nHopefully, we can control for that.\nBut in general, like we wanted to set it in a place that felt like we were going to get some scale out of our humans, right?\nWhich meant messages going out automatically, right?\nUh, yes.\nYeah.\nTrying to evaluate whe\nYeah.\nYeah.\nSo the question is about conflation of of confidence scoring and complexity.\nYes.\n100% agree.\nAnd again, it's one of the things where I think we're happy with how it sort of operates now, but I'm not sure that we're doing the confidence piece right.\nUm, and at the same time, like the concept of it I think is is good, right?\nYou know, do you have the information you need?\nYou know, is is there anything like, do you understand the user's intent or you know, did they say something ambiguous?\nAnd we do see cases where it triggers, right?\nIt's not that we never see it, you know, correctly rate itself as being like, well, I'm not totally sure what they said, but it's not as frequent as we would like.\nSo, we combine it with complexity in part because, you know, we just want to get it below that threshold so that we can have a human review it.\nIt's not a perfect system, but it is a system that we think works.\nSo it's more confidence.\nCorrect.\nUh, correct.\nSo to to that point, it's it's not confidence that the specific response is exact and perfect and whatever it is, it's confidence that we don't think that there's a blend of uncertainty on, you know, what the response is and complexity of the situation, right?\nEither of those things can push it below the threshold.\nHow are you hosting?\nUh, so this is all, uh, the question was about hosting.\nSo this is all, um, Langraph has a pre-built containerization that you can use.\nWe are using it with a couple of modifications.\nUm, we are essentially deploying both halves of this, right?\nSo to go back to this, we're deploying both halves of this with Terraform.\nYou know, everything's hooked up with GitHub actions.\nI mean, like, you know, we're we're doing as much of as automatically as we can, but we are using most of the built-in line graph containerization.\nI thought there was a platform\nyou do know, I mean, we're we, we have a conversation with with Langchain actively about that.\nSo the question actually is also, um, the the specific nature of how we need to be deployed.\nUm, Langraph platform doesn't currently support that, right?\nSo like there, it's it's all, it's all evolving, but um, we, we have had those conversations.\nUm, let me pause for a second.\nWe still have 10 minutes.\nUm, and so,\n\n\nOr roughly 10 minutes, nine minutes.\nUm, let me just double check my own list of things that I wanted to talk about just to make sure I didn't miss anything major.\nUm, talked about that, talked about that.\nAll right.\nI I think we more or less hit everything.\nUm, and so I'm happy just to open it up.\nUm, I have a couple of final thoughts that I'll leave sitting up here if anyone's curious.\nUm, any other questions?\nYep.\nYeah.\nUh, so the question was just about uh, resource allocation to build these things.\nSo I mean, I think the best way to say it is just that we had built a handful of things like this.\nWe had not done it for healthcare, right?\nBut we were able to bring some things in in terms of, you know, I had an open source LangGraph project that I was comfortable using as a base for this client code, right?\nAgain, we forked it, we brought it in private.\nUm, you know, but it got us, you know, part of the way there in part because this isn't a totally special snowflake.\nIt's a workflow with tools, so you know, we have, I think, spent less time on that upfront scaffolding each time we've done it.\nUm, and you know, we're looking at another project right now, which would be that much quicker.\nSo a lot of it is just once you do this and you understand the mechanism, getting to, you know, good enough or getting to a starting point is just a lot quicker.\nUm, it is something though where, like, you know, again, we are we are still working on this, and we've been at it for a few months, but I think we probably built what would have been a year's worth of conventional software, maybe more, right?\nYou know, in that short period of time.\nAnything else?\nYeah, so you mentioned you did a lot of coding.\nThis can a little bit.\nI'm curious how much Lang...\nLike...\nWell, okay, so the uh, the question was is about uh, vibe coding and how much the tools know of these frameworks.\nSo the single biggest issue with vibe coding, right, is when you're dealing with something new enough that the models don't really understand it.\nUm, you can always just send them the API docs, and they usually do pretty well.\nUm, I ran into plenty of places with LangGraph where nobody had tried to do this before, no one had this exact bug, and we just had to actively debug it back and forth.\nUm, I can definitely endorse 03 is a better debugger than Claude.\nUm, so you know, we did have cases where we had to do that.\nUm, but for the most part, like, I mean, as long as you can at the docks, you know, you can get to a reasonable place.\nAnd again, like I am a former engineer.\nI I may eventually call myself a current engineer, but I I kind of wouldn't right now.\nLike I don't have all the same practices and sort of all the same hygiene, you know, that our professional engineers do.\nBut I do know how to smell kind of bad behavior, and I'm pretty good at prompting to get what I want.\nSo that that's kind of how it how it worked out.\nUm, any...\nOh, yeah.\nPlease.\nIn this example,\nNo, there is MCP.\nMCP is in a couple places, right?\nSo, if you look at the connection there, there's actually two, and one of them I just didn't totally label the blueprint knowledge base at the top, that yellow box that is an MCP connection in the LangGraph context, and then vertically there's another set of connections back to the database.\nSo, there's a couple places where it happens, but again, it's mostly for the inter the exchange of state, and it's for this, you know, reading of documents.\nThat's primarily where it happens.\nYep.\nYep.\nAbsolutely.\nYeah.\nYep.\nSo the question is about rapid fire text messages.\nSo yes, the way we handle that is twofold.\nSo one is that, um, we do have a configurable, I'm not totally sure we have this turned on, but we have the idea of a configurable delay before we actually send it for processing.\nSo if someone is going to send five text messages in a row, we wait five seconds before doing anything, right?\nUm, that's one way we can catch it.\nBut the other way is, um, we will invalidate previous running threads if someone texts in afterwards, right?\nWe don't want an in-process response.\nWe want to take whatever the complete context of the conversation was, send all of that in, and then we respond to five messages at once, right?\nSo it's a combination of smart retries and smart invalidation.\nYeah.\nYep.\nYeah.\nWell, this question was about rogue responses.\nWell, in general, we are we are giving some pretty basic guidance around you're only here to answer treatment-related questions.\nIf someone wants to talk to you about the weather, you just say, \"I'm sorry.\nI can't help with that.\"\nUm, you know, or other worse things.\nUm, so generally speaking, that works pretty well.\nUm, we have it pretty well tuned to escalate, right?\nIf someone just is essentially going off the rails and you can't think of a response, you just say, \"I'm sorry, I'm going to get somebody to help you.\"\nAnd it sets the confidence low, and then a human can get involved.\nUm, it doesn't in practice happen that much.\nI mean, again, if you're involved in this, like, you know, if you're involved in this and you take the time to actually engage with the system, you want the result, and you probably want to get back to your life.\nSo we don't see a tremendous amount of it, but that's the way that we would deal with it.\nAt any point in your development process, you get frustrated enough...\nUh, yeah, did did I get frustrated enough with LangGraph?\nUm, at any point, honestly, so the one thing I'll say, and I, this is just totally a personal project, um, I was doing a side thing, um, around college counseling, um, and I just have this sitting here because I've showed off sometimes, um, and the point was I I went into this project explicitly trying to avoid it.\nI said, I'm going to do a very similar thing where I have an AI in the middle of a workflow, and I wanted to ask questions, and I wanted to think, and I do not want to use LangGraph or crew or any of these other frameworks because I don't want to be dependent on them.\nUm, and so I asked, you know, I asked Klein to write me a layer that was pretty good at, you know, uh, talking to these models and structuring a thinking process, and it it did fine.\nI mean, the reason to have the framework is in part because, you know, again, we won't be with this client forever.\nWe want them to have something they can operate.\nWe want them to have something explainable and easy to use, like this is all, you know, logs in Google Cloud, like it's not the most fun experience, so you know, it is very much choose your tools wisely, you know, good and bad, like you get a better experience with the other ones.\nSpeaking on that...\nUh, uh, thank you so much for asking me that question.\nUm, I I so Klein over Cursor.\nUm, I I will admit that Cursor and and Windsurf is great, by the way, like I mean, there's a handful of them that I really do like.\nUm, Cursor and Windsurf as as two examples are are are just trying to hit this sort of narrow, um, you know, thing about it has to cost $20 a month, and therefore it has to be heavily optimized about how it sends tokens in different places, otherwise their economics blow up.\nUm, Klein doesn't do that.\nIt's very simple.\nIt's really, I mean, it's very smart, but like it's just giving tools to a smart model, and that smart model can cost whatever it costs.\nKlein is the spiritual cousin to Claude Code and Codecs, right?\nI mean, it's that style of thing as opposed to an IDE that has to hit this very narrow target and has to do a lot of pre-optimization on how the tokens flow.\nYeah.\nYeah.\nSo, I mean, honestly, this is also another place where I will raise my hand and say this is why I'm not a real software engineer.\nI I don't have a robust testing framework on my Python code.\nI I haven't really needed it, right?\nOr at the very least, like I'm using eval and sort of overall performance of the system as the better benchmark, right?\nSo, eval are important.\nWe have to have those.\nUm, I, you know, the other side of the code, like, you know, Stride is a TDD shop, like our our software engineers are very, very good at test-driven development, and so the blue box is very well tested.\nUm, but just my code, you know, very much is is eval sort of tested instead.\nUm, it's not a great answer, but like that that is kind of how I thought about it.\nAll right.\nUm, I think we are at time.\nThank you guys so much.\nThis was awesome.\nUm, I'll be here if you want to stick around.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:26.074Z"
}