{
  "episodeId": "8kMaTybvDUw",
  "channelSlug": "@aidotengineer",
  "title": "12-Factor Agents: Patterns of reliable LLM applications — Dex Horthy, HumanLayer",
  "publishedAt": "2025-07-03T20:50:54.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1.72,
      "duration": 6.91
    },
    {
      "lang": "en",
      "text": "Who here's building agents?",
      "offset": 15.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Who here's your hand up if you built",
      "offset": 17.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like 10 plus agents? Anyone here built",
      "offset": 19.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like a hundred agents?",
      "offset": 22.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "All right, we got a few. Awesome. Love",
      "offset": 24.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it. Um, so I think a lot of us have been",
      "offset": 25.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "on this journey of building agents. Um,",
      "offset": 27.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and what happened with me was, you know,",
      "offset": 31.039,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "I decided I want to build an agent. We",
      "offset": 32.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "figured out what we wanted it to do. Uh,",
      "offset": 33.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we want to move fast. We're developers,",
      "offset": 35.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "so we use libraries. We don't write",
      "offset": 37.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "everything from scratch. Um, and you get",
      "offset": 38.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it to like 70 80%. It's enough to get",
      "offset": 40.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "the CEO excited and get six more people",
      "offset": 41.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "added to your team. But then you kind of",
      "offset": 43.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "realize that 70 80% isn't quite good",
      "offset": 45.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "enough. And that if you want to get past",
      "offset": 47.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that 70 80% quality bar, you're seven",
      "offset": 50.559,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "layers deep in a call stack trying to",
      "offset": 53.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "reverse engineer how does this prompt",
      "offset": 55.199,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "get built or how do these tools get",
      "offset": 56.559,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "passed in? Where does this all come",
      "offset": 57.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "from? Uh and if you're like me, you",
      "offset": 59.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "eventually just throw it all away and",
      "offset": 60.96,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "start from scratch. Um or you may even",
      "offset": 62.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "find out that this is not a good problem",
      "offset": 64.879,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "for agents. I remember one of the first",
      "offset": 66.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "agents that I tried to build was uh a",
      "offset": 67.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "DevOps agent. I was like, &quot;Here's my",
      "offset": 70.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "make file. You can run make commands. Go",
      "offset": 71.76,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "build the project.&quot; Couldn't figure it",
      "offset": 73.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "out. Did all the things in the wrong",
      "offset": 75.439,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "order. I'm like, &quot;Cool, let's fix the",
      "offset": 76.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "prompt.&quot; And I over the la next two",
      "offset": 77.84,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "hours I'm more and more detail about",
      "offset": 80,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "what everything was and every single",
      "offset": 81.759,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "step and the exact until I got to the",
      "offset": 83.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "point where I was like this is the exact",
      "offset": 84.64,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "order to run the build steps. It was a",
      "offset": 85.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "cool exercise but at the end of it I was",
      "offset": 87.759,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like you know I could have written the",
      "offset": 89.6,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "bash script to do this in about 90",
      "offset": 90.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "seconds. Not every problem needs an",
      "offset": 92.479,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "agent. Um and so I've been on this",
      "offset": 94.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "journey. I think a lot of you have been",
      "offset": 97.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "on similar journeys. Um and what",
      "offset": 98.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "happened was is I went and talked um in",
      "offset": 101.92,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "trying to help people build better, more",
      "offset": 104.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "reliable agents. I talked to a 100 plus",
      "offset": 105.759,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "founders, builders, engineers. Um, and I",
      "offset": 108.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "started to notice patterns. One was that",
      "offset": 111.84,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "most production agents weren't that",
      "offset": 113.68,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "agentic at all. They were mostly just",
      "offset": 115.759,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "software, but that there were these core",
      "offset": 117.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "things that a lot of people were doing.",
      "offset": 119.84,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "There were these patterns that were",
      "offset": 121.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "making their LLM based apps really,",
      "offset": 123.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "really good. Um, and none of them were",
      "offset": 124.799,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "doing kind of a green field rewrite.",
      "offset": 126.88,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "Rather, they were taking these small",
      "offset": 128.479,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "modular concepts that didn't have names",
      "offset": 129.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and didn't have definitions and they",
      "offset": 131.68,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "were applying them to their existing",
      "offset": 133.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "code. Um, and what's really cool about",
      "offset": 134.879,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "this is I don't think you need an AI",
      "offset": 136.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "background to do this. This is software",
      "offset": 137.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "engineering 101. Well, probably not 101,",
      "offset": 139.599,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "but just like Heroku needed to define",
      "offset": 141.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "what it meant to build cloud. We didn't",
      "offset": 144.879,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "even call them cloud native back then,",
      "offset": 146.16,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "but this was how you built applications",
      "offset": 147.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that could run in the cloud 10 years",
      "offset": 148.879,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "ago. Um, I decided to put together what",
      "offset": 150.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "I thought would be the 12 factors of AI",
      "offset": 153.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "agents. Um, from everything that I've",
      "offset": 156.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "seen working in the field. Um, so we put",
      "offset": 159.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "up this GitHub repo. You can go read it.",
      "offset": 161.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Um, turns out a lot of other people",
      "offset": 163.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "agreed and felt the same thing. Um, so",
      "offset": 164.959,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we were on the front page of Hacker News",
      "offset": 167.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "all day, 200k impressions on social. Uh,",
      "offset": 168.879,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I'm just going to put this one up and no",
      "offset": 172,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "comment.",
      "offset": 173.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Um, and just for context, we got to like",
      "offset": 175.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "4,000 stars in like a month or two. Uh,",
      "offset": 178,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "there's 14 active contributors. Um, it's",
      "offset": 180.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "very easy to read that thing and and",
      "offset": 183.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "hear the talk and say like, oh, we're",
      "offset": 185.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "here, this is the anti-framework talk. I",
      "offset": 186.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "am not here to bash frameworks. I would",
      "offset": 189.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "think of this as much as anything as a",
      "offset": 191.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "wish list, a a list of feature requests",
      "offset": 193.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is how can we make frameworks serve the",
      "offset": 195.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "needs of really good builders who need a",
      "offset": 198,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "really high reliability and want to move",
      "offset": 199.92,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "fast still. Um, so what am I here to do?",
      "offset": 201.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Uh, I want you to kind of forget",
      "offset": 205.519,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "everything you know about agents and",
      "offset": 207.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "kind of rethink from first principles",
      "offset": 208.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "how we can apply everything we've",
      "offset": 210.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "learned from software engineering to the",
      "offset": 211.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "practice of building really reliable",
      "offset": 213.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "agents. Um, so we're going to mix the",
      "offset": 215.28,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "order up a little bit. If you want all",
      "offset": 217.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "12 factors in order, that's a 30-minute",
      "offset": 218.879,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "talk, so we're going to bundle some",
      "offset": 220.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "stuff together. There will be a QR code",
      "offset": 221.44,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "at the end. You can go dig through it at",
      "offset": 223.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "your leisure. Um, factor one, the most",
      "offset": 224.879,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "magical things that LMS can do. Has",
      "offset": 227.2,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "nothing to do with loops or switch",
      "offset": 229.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "statements or code or tools or anything.",
      "offset": 230.879,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "It is turning a sentence like this into",
      "offset": 233.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "JSON that looks like this. Doesn't even",
      "offset": 235.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "matter what you do with that JSON. Uh,",
      "offset": 237.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "those are what the other factors are",
      "offset": 239.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "for. But if you're doing that, that's",
      "offset": 241.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "one piece that you can bring into your",
      "offset": 242.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "app today. Um, factor four. This leads",
      "offset": 244,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "right to uh did anyone read this paper",
      "offset": 247.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "uh go to considered harmful or maybe",
      "offset": 249.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "just heard about it? I never actually",
      "offset": 251.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "read it. Uh but it was all about we had",
      "offset": 252.319,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this abstraction in the C programming",
      "offset": 253.84,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "language and a bunch of other",
      "offset": 255.439,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "programming languages at the time that",
      "offset": 256.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "said this thing go to it makes code",
      "offset": 258.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "terrible. It's the wrong abstraction. It",
      "offset": 260.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "no one should use it. I'm going to go",
      "offset": 262.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "ahead and go on a limb here and say tool",
      "offset": 264.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "use is harmful. And I put it in quotes",
      "offset": 267.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because I'm not talking about giving an",
      "offset": 269.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "agent access to the world. Obviously",
      "offset": 270.56,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "that's super badass. But what I think is",
      "offset": 272.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "making things hard is the idea that tool",
      "offset": 275.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "use is this magical thing where this",
      "offset": 277.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "ethereal alien entity is interacting",
      "offset": 279.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "with its environment because what is",
      "offset": 281.44,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "happening is our LM is putting out JSON.",
      "offset": 283.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "We're going to give that to some",
      "offset": 285.759,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "deterministic code that's going to do",
      "offset": 286.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "something and then maybe we'll feed it",
      "offset": 288.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "back. But again, those are other",
      "offset": 289.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "factors. So if you have structures like",
      "offset": 290.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this and you can get the LLM to output",
      "offset": 293.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "something that generates them, then you",
      "offset": 295.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "can pass it into a loop like this or a",
      "offset": 297.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "switch statement like this. There's",
      "offset": 299.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "nothing special about tools. It's just",
      "offset": 300.88,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "JSON and code. Uh, that's factor four.",
      "offset": 302.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Factor eight, and these are we're going",
      "offset": 306.479,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "to do a couple kind of bundled together",
      "offset": 307.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "here. Owning your control flow. Um, and",
      "offset": 309.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I want to take a step back and kind of",
      "offset": 312.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "talk about how we got here. Um, we've",
      "offset": 313.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "been writing DAGs in software for a long",
      "offset": 315.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "time. If you've written an if statement,",
      "offset": 317.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "you've written a directed graph. Uh,",
      "offset": 318.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "code is a graph. You may also be",
      "offset": 320.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "familiar with DAG orchestra. Anyone ever",
      "offset": 322.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "use like Airflow or Prefect or any of",
      "offset": 324.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "these things? Um, so like this kind of",
      "offset": 326.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "concept of breaking things up into nodes",
      "offset": 329.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "gives you certain reliability",
      "offset": 330.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "guarantees. But what agents were",
      "offset": 332.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "supposed to do, and I think a lot of",
      "offset": 334.16,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "people talk about this, and I think in",
      "offset": 335.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "some cases this is realized, is you",
      "offset": 336.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "don't have to write the deck. You just",
      "offset": 338.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "tell the LM, here's the goal, and LM",
      "offset": 340.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "will find its way there. And we model",
      "offset": 343.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this as a really simple loop. You know,",
      "offset": 345.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "LM is determining the next step. You're",
      "offset": 347.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "building up some context window until",
      "offset": 348.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "the LM says, &quot;Hey, we're done.&quot;",
      "offset": 350.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Um, so what this looks like kind of in",
      "offset": 353.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "practice is, you know, you have an event",
      "offset": 355.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "come in. You pass it into your prompt.",
      "offset": 356.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Uh, it says you want to call an API and",
      "offset": 358.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you get your result. Put that on the",
      "offset": 360.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "context window. Pass the whole thing",
      "offset": 362,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "back into the prompt. This is like the",
      "offset": 364.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "most naive simple way of building",
      "offset": 366,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "agents. And the LM's going to call a",
      "offset": 367.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "couple steps and then eventually it's",
      "offset": 369.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "going to say, cool, we've done all the",
      "offset": 371.6,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "tasks from the initial event, which",
      "offset": 372.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "maybe was a user message asking it to do",
      "offset": 374.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "something. Maybe it's an outage. Um, but",
      "offset": 376,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "then we get our final answer. And our",
      "offset": 378.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "materialized DAG is just these three",
      "offset": 380.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "steps in order. Um,",
      "offset": 382.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "turns out this doesn't really work. Uh,",
      "offset": 385.36,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "especially when you get to longer",
      "offset": 387.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "workflows. Mostly it's long context",
      "offset": 388.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "windows. There's other reasons you could",
      "offset": 390.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "poke at as well. Um, and people say,",
      "offset": 391.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "&quot;Oh, like even as anyone put like two",
      "offset": 393.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "million tokens into Gemini before and",
      "offset": 395.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "like try to see what happens.&quot; Like you",
      "offset": 397.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "can do it. You'll get an answer. The API",
      "offset": 399.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "will return you something. But I don't",
      "offset": 400.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think anyone will argue with you that",
      "offset": 402.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you will always get like tighter,",
      "offset": 404.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "better, higher reliability results by",
      "offset": 406.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "controlling and limiting the number of",
      "offset": 408.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tokens you put in that context window.",
      "offset": 410.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Um, so it doesn't quite work, but we're",
      "offset": 412.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "going to use that as our abstraction to",
      "offset": 414.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "build on. What's an agent really? You",
      "offset": 415.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "have your prompt, which gives",
      "offset": 417.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "instructions about how to select the",
      "offset": 419.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "next step. You have your switch",
      "offset": 420.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "statement, which takes whatever the",
      "offset": 422,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "model output JSON, uh, and does",
      "offset": 423.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "something with it. You have a way of",
      "offset": 425.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "building up your context window. And",
      "offset": 427.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "then you have a loop uh that determines",
      "offset": 428.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "when and where and how and why you exit.",
      "offset": 430.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um and if you own your control flow, you",
      "offset": 433.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "can do fun things like break and switch",
      "offset": 434.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and summarize and LM is judge and all",
      "offset": 436.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this stuff. Um and this leads right into",
      "offset": 437.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "kind of how we manage execution state",
      "offset": 440.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and business state of our agents. Um a",
      "offset": 442.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "lot of tools will give you things like",
      "offset": 445.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "current step, next step, retry counts,",
      "offset": 446.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "all these like DAG orchestrators, they",
      "offset": 448.479,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "all have this kind of concepts in them.",
      "offset": 449.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Um but you also have your business",
      "offset": 451.36,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "state. What are the messages that have",
      "offset": 452.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "happened? What data are we displaying to",
      "offset": 453.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the user? What things are we waiting on",
      "offset": 455.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "approval for? Um, and we want to be able",
      "offset": 456.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "to launch, pause, resume these things",
      "offset": 458.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like we do for any standard APIs. Um,",
      "offset": 460.479,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "this is all just software. And so if you",
      "offset": 463.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "can put your agent behind a REST API or",
      "offset": 465.599,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "an MCP server, um, and manage that loop",
      "offset": 468.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "in such a way that normal request comes",
      "offset": 472.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in and we load that context window to",
      "offset": 474.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the LLM. Um, we're going to allow our",
      "offset": 476.4,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "agent to call longunning tool. So we can",
      "offset": 478.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "interrupt the workflow, serialize that",
      "offset": 480.879,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "context window straight into a database",
      "offset": 482.639,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "because we own the context window. We'll",
      "offset": 484.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "get into that. Um, and then when we",
      "offset": 485.68,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "launch the workflow, um, eventually it's",
      "offset": 487.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going to call back with that state ID",
      "offset": 489.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and the result. We use the state ID to",
      "offset": 491.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "load the state back out of the database,",
      "offset": 493.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then we can append the result to the",
      "offset": 495.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "program and then send it right back into",
      "offset": 497.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the LM. The agent doesn't even know that",
      "offset": 499.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "things happened in the background. Um,",
      "offset": 500.8,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "agents are just software, so let's build",
      "offset": 503.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "software. Um, and building really good",
      "offset": 504.879,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "ones need requires a lot of flexibility.",
      "offset": 506.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "And so you really want to own that inner",
      "offset": 509.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "loop of of how all that stuff is fitting",
      "offset": 510.639,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "together. Um, that's unifying. That's",
      "offset": 512.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "pause and resume. Uh factor two, this",
      "offset": 515.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "one is I think most people find first is",
      "offset": 517.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "like you really want to own your",
      "offset": 519.279,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "prompts. There's some good extractions",
      "offset": 520.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that if you don't want to spend a lot of",
      "offset": 522,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "time handwriting a prompt, you can put",
      "offset": 523.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "stuff in and you'll get out um a really",
      "offset": 525.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "good set of primitives um and a really",
      "offset": 529.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "good prompt. Like this will make you a",
      "offset": 531.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "banger prompt that like you would have",
      "offset": 533.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "to go to prompt school for like three",
      "offset": 534.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "months to build a prompt this good, but",
      "offset": 536,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "eventually if you want to get past some",
      "offset": 538.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "quality bar, you're going to end up",
      "offset": 539.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "writing every single token by hand. Um",
      "offset": 541.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "because LM are pure focus functions and",
      "offset": 544.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the only thing that determines the",
      "offset": 546.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "reliability of your agent is how good of",
      "offset": 547.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "tokens can you get out and the only way",
      "offset": 550.08,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "the only thing that determines the",
      "offset": 552.08,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "tokens you get out other than like",
      "offset": 552.959,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "retraining your own model and something",
      "offset": 554.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like that is being really careful about",
      "offset": 555.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "what tokens you put in.",
      "offset": 557.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Um I don't know what's better. I don't",
      "offset": 560.16,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "know how you want to build your prompt,",
      "offset": 561.519,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "but I know the more things you can try",
      "offset": 562.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and the more knobs you can test and the",
      "offset": 563.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "more things you can evaluate, the more",
      "offset": 565.36,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "likely you are to find something really",
      "offset": 566.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "really good. Um owning your prompts, you",
      "offset": 568.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "also want to own how you build your",
      "offset": 570.88,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "context window. Um, so you can do the",
      "offset": 571.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "standard open II messages format or in",
      "offset": 573.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this moment where you're telling the LLM",
      "offset": 575.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "pick the next step, your only job is to",
      "offset": 577.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "tell it what's happened so far. You can",
      "offset": 579.519,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "put all that information however you",
      "offset": 581.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "want into a single user message and ask,",
      "offset": 582.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "hey, what's happening next or put in the",
      "offset": 584.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "system message. So you can model your",
      "offset": 586.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "event state, your thread model however",
      "offset": 588.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you want, um, and stringify it however",
      "offset": 590.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you want. And some of the traces that we",
      "offset": 593.12,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "use in some of the agents we build",
      "offset": 594.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "internally, I'll get into that in a sec,",
      "offset": 595.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "um, might look like this. Um, but if",
      "offset": 597.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you're not looking at every single token",
      "offset": 599.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and if you're not optimizing the density",
      "offset": 601.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and the clarity of the way that you're",
      "offset": 603.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "passing information to an LLM, you might",
      "offset": 606.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "be missing out on upside and quality. So",
      "offset": 609.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "LMS are pure functions, token in, tokens",
      "offset": 612.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "out, and everything everything in making",
      "offset": 614.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "agents good is context engineering. So",
      "offset": 616,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you have your prompt, you have your",
      "offset": 618.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "memory, you have your rag, you have your",
      "offset": 620.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "history. It's all just how do we get the",
      "offset": 621.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right tokens into the model. So it gives",
      "offset": 623.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "us a really good answer and solves the",
      "offset": 624.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "user's problem. solves my problem",
      "offset": 626.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "mostly. Um, I don't know what's better,",
      "offset": 628,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but I know you want to try everything.",
      "offset": 630.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Um, so let's own your context building.",
      "offset": 632.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Um, this one's a little controversial.",
      "offset": 634.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Uh, that's why it's a it's a standalone",
      "offset": 636.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "factor. And the way you make it good is",
      "offset": 638.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "by integrating it with other factors.",
      "offset": 640.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "But you could, um, when the model screws",
      "offset": 642.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "up and it calls an API wrong or it calls",
      "offset": 644.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "an API that's down, um, you could take",
      "offset": 646.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the tool call that it made and grab the",
      "offset": 648.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "error that was associated with it, put",
      "offset": 651.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that on the context window, and have it",
      "offset": 653.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "try again. Anyone ever had a bad time",
      "offset": 655.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "with this? Seen like this thing just",
      "offset": 657.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like kind of spin out and like go crazy",
      "offset": 660.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and lose context and just get stuck. Um,",
      "offset": 662.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's why you need to own your context",
      "offset": 665.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "window. Don't just blindly put things",
      "offset": 666.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "on. If you have errors and then you get",
      "offset": 668.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "a valid tool call, clear all the air",
      "offset": 670,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "pending errors out. Summarize them.",
      "offset": 671.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Don't put the whole stack trace on your",
      "offset": 673.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "context. Figure out what you want to",
      "offset": 674.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "tell the model so you get better",
      "offset": 676.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "results. Um, contacting humans with",
      "offset": 677.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "tools. This one's a little subtle. Um,",
      "offset": 680.48,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "but I've seen this is just like what",
      "offset": 682.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I've seen in the wild. almost everybody",
      "offset": 683.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is like avoiding this very important",
      "offset": 685.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "choice at the very beginning of output",
      "offset": 687.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "where you're deciding between tool call",
      "offset": 689.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and message to the human. Um if you can",
      "offset": 691.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "push that emphasis to a natural language",
      "offset": 694.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "token, you can one give the model",
      "offset": 695.839,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "different ways. You can be I'm done or I",
      "offset": 697.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "need clarification or I need to talk to",
      "offset": 699.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "a manager or whatever it is and two you",
      "offset": 700.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "push the intent on that first token",
      "offset": 703.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "generation and the sampling to something",
      "offset": 706,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that is natural language that the model",
      "offset": 708,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "understands. Um, so your traces might",
      "offset": 709.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "look like this if you're pulling in",
      "offset": 712.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "human input here. Um, this lets you",
      "offset": 713.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "build out auto outloop agents. I'm not",
      "offset": 715.519,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "going to talk about this. If you go on",
      "offset": 717.279,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "the site, there's a link to this this",
      "offset": 718.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "post. I've written a lot about this. Um,",
      "offset": 719.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "I don't know what's better, but you",
      "offset": 721.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "should probably try everything. Um,",
      "offset": 723.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that's contacting humans with tools.",
      "offset": 725.92,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "Goes right along with trigger things",
      "offset": 727.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "from anywhere and meet users exactly",
      "offset": 728.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "where they are. People don't want to",
      "offset": 730.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have seven tabs open of different chat",
      "offset": 732.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "GPT style agents. Just let people email",
      "offset": 734.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "with the agents you're building. Let",
      "offset": 736.72,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "them slack with the agents you're",
      "offset": 737.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "building. Discord, SMS, whatever it is.",
      "offset": 738.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "We see this taking off all over the",
      "offset": 741.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "place. Um, and you should have small",
      "offset": 742.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "focused agents. So, we talked about this",
      "offset": 745.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "structure and why it doesn't really",
      "offset": 746.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "work. So, what does work? Um, the things",
      "offset": 748.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that people are doing that work really",
      "offset": 751.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "well are micro agents. So, you still",
      "offset": 752.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "have a mostly deterministic DAG and you",
      "offset": 754.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "have these very small agent loops with",
      "offset": 756,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like three to 10 steps. We do this at",
      "offset": 757.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "human layer. We have a bot that manages",
      "offset": 759.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "our deployments. Most of our deploy",
      "offset": 761.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "pipeline is deterministic CI/CD code.",
      "offset": 763.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "But when we get to the point where the",
      "offset": 765.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "GitHub PR is merged and the tests are",
      "offset": 767.92,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "passing on development, excuse me, we",
      "offset": 771.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "send it to a model. We say get this",
      "offset": 775.279,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "thing deployed. Says cool, I'm going to",
      "offset": 776.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "deploy the front end. Uh, and then you",
      "offset": 778,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "can send that to a human. The human says",
      "offset": 779.839,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "actually no, do the back end first. This",
      "offset": 781.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "is taking natural language and turning",
      "offset": 782.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "it into JSON. That is the next step in",
      "offset": 783.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "our workflow. Um, backend gets proposed,",
      "offset": 785.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that gets approved, it gets deployed,",
      "offset": 788.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "then the agent knows, okay, I have to go",
      "offset": 790.079,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "back and deploy the front end. Once",
      "offset": 791.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that's all done and it's successful, we",
      "offset": 793.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "go right back out into deterministic",
      "offset": 795.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "code. So now we're going to run the end",
      "offset": 797.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to end test against prod. If it's done,",
      "offset": 798.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "otherwise we hand it back to a little",
      "offset": 800.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "roll back agent that is very similar on",
      "offset": 801.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the inside. Um I'm not going to go into",
      "offset": 803.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it, but here's it working in our Slack",
      "offset": 806,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "channel. Um yeah, 100 tools, 20 steps,",
      "offset": 807.36,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "easy. Um manageable context, clear",
      "offset": 811.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "responsibilities. Um a lot of people",
      "offset": 814.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "say, &quot;What what if LM do keep getting",
      "offset": 816,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "smarter? What if I can put two million",
      "offset": 817.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tokens in and it can do it?&quot; Um, and I",
      "offset": 818.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "think we very much will see something",
      "offset": 821.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like this where you start with a mostly",
      "offset": 823.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "deterministic workflow and you start",
      "offset": 825.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "sprinkling LMS into your code, into your",
      "offset": 827.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "backend, into your logic. Over time, the",
      "offset": 829.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "LMS are able to do bigger, more complex",
      "offset": 831.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "tasks until this whole API endpoint or",
      "offset": 833.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "pipeline or whatever it is is just run",
      "offset": 836.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "by an agent. That's great. Uh, but you",
      "offset": 838,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "still want to know how to engineer these",
      "offset": 840.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "things to get the best quality. This is",
      "offset": 842.079,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "someone from notebook LM and it's",
      "offset": 843.44,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "basically their take and I think they",
      "offset": 844.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "did this well is find something that is",
      "offset": 846.959,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "right at the boundary of what the model",
      "offset": 848.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "can do reliably like that it can't get",
      "offset": 850.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "right all the time and if you can figure",
      "offset": 852.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "out how to get it right reliably anyways",
      "offset": 854.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "because you've engineered reliability",
      "offset": 857.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "into your system then you will have",
      "offset": 858.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "created something magical and you will",
      "offset": 861.12,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "have created something that's better",
      "offset": 862.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "than what everybody else is building. So",
      "offset": 863.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that's small focused agents. There's a",
      "offset": 865.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "meme here about stateless reducers. I",
      "offset": 867.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "guess someone actually tweeted at me,",
      "offset": 869.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "it's not a reducer, it's a transducer",
      "offset": 870.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "because there's multiple steps. Um, but",
      "offset": 872.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "basically agents should be stateless.",
      "offset": 874.56,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "You should own the state, manage it",
      "offset": 875.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "however you want. Um, so we're all still",
      "offset": 877.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "finding the right abstractions. Um,",
      "offset": 879.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "there's a couple blog posts I link in",
      "offset": 881.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the paper. Um, frameworks versus",
      "offset": 882.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "libraries. There's a really old one from",
      "offset": 885.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Ruby Comp about like do we want",
      "offset": 886.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "duplication or is like do we want to try",
      "offset": 888.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to figure out these abstractions? Um, if",
      "offset": 890.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you want to make a 12-factor agent, we",
      "offset": 893.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "are working on something called create",
      "offset": 895.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "12-factor agent because I believe that",
      "offset": 896.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what agents need is not bootstrap. You",
      "offset": 898.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "don't need a wrapper around an internal",
      "offset": 900.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "thing. You need something more like shad",
      "offset": 902.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "CN, which is like scaffolded out and",
      "offset": 904.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "then I'll own it and I'll own the code",
      "offset": 905.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and I'm okay with that.",
      "offset": 907.279,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "So, in summary, agents are software. You",
      "offset": 909.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "all can build software. Anyone ever",
      "offset": 912.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "written a switch statement before?",
      "offset": 913.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "While loop. Yeah. Okay. So, we can do",
      "offset": 916.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "this stuff. LM are stateless functions,",
      "offset": 917.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "which means just make sure you put the",
      "offset": 919.839,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "right things in the context and you'll",
      "offset": 921.279,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "get the best results. Own your state and",
      "offset": 922.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "your control flow and just do it and",
      "offset": 924.56,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "just understand it because it's going to",
      "offset": 926,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "give you flexibility. And then find the",
      "offset": 927.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "bleeding edge. Find ways to do things",
      "offset": 929.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "better than everybody else by really",
      "offset": 931.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "curating what you put in the model and",
      "offset": 934,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "how you control what comes out. Um, and",
      "offset": 935.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "my tech agents are better with people.",
      "offset": 938.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Find ways to let agents collaborate with",
      "offset": 940.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "humans. Um, there are hard things in",
      "offset": 941.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "building agents, but you should probably",
      "offset": 944.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "do them anyways, at least for now, and",
      "offset": 946.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you should do most of them. Um, I think",
      "offset": 948.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "a lot of frameworks try to take away the",
      "offset": 950.16,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "hard AI parts of the problem so that you",
      "offset": 953.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "can just kind of drop it in and go. And,",
      "offset": 955.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "uh, I think it should be the opposite. I",
      "offset": 958.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think the tools that we get should take",
      "offset": 959.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "away the other hard parts so that we can",
      "offset": 962,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "spend all our time focusing on the hard",
      "offset": 963.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "AI parts, on getting the prompts right,",
      "offset": 965.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "on getting the flow right, on getting",
      "offset": 967.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the tokens right. So the reason why I'm",
      "offset": 968.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "here is like I do run a small business.",
      "offset": 970.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Um we have a startup where we try to",
      "offset": 973.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "help you do a lot of what we do we do in",
      "offset": 975.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the open is open source and I think it's",
      "offset": 977.759,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "really important and we need to work on",
      "offset": 979.519,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "it together. There's some other things",
      "offset": 980.639,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that are hard but not that important and",
      "offset": 982.079,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "not that interesting. So that's what",
      "offset": 983.759,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we're solving at human layer. Um working",
      "offset": 984.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "on something called the A2 protocol.",
      "offset": 988.079,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Come find me if you want to talk about",
      "offset": 989.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "this but this is a way to get like",
      "offset": 990.959,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "consolidation around how agents can",
      "offset": 992.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "contact humans. Um but mostly I just",
      "offset": 994.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "love automating things. I've built tons",
      "offset": 996.959,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and tons of agents internally for my",
      "offset": 998.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "personal stuff, for finding apartments,",
      "offset": 1000.32,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "for all kinds of internal business stuff",
      "offset": 1002,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "we do at human layer. Um, so thank you",
      "offset": 1003.759,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "all for watching. Let's go build",
      "offset": 1007.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "something. I'll see you in the hallway",
      "offset": 1010,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "track. Uh, I'd love to chat if you want",
      "offset": 1011.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "to riff on agents or building or control",
      "offset": 1013.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "flow or any of this stuff. That's",
      "offset": 1014.959,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "12actor agents.",
      "offset": 1016.56,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "[Applause]",
      "offset": 1018.2,
      "duration": 4.759
    }
  ],
  "cleanText": "[Music]\nWho here's building agents? Who here's your hand up if you built like 10 plus agents? Anyone here built like a hundred agents? All right, we got a few. Awesome. Love it. Um, so I think a lot of us have been on this journey of building agents. Um, and what happened with me was, you know, I decided I want to build an agent. We figured out what we wanted it to do. Uh, we want to move fast. We're developers, so we use libraries. We don't write everything from scratch. Um, and you get it to like 70 80%. It's enough to get the CEO excited and get six more people added to your team. But then you kind of realize that 70 80% isn't quite good enough. And that if you want to get past that 70 80% quality bar, you're seven layers deep in a call stack trying to reverse engineer how does this prompt get built or how do these tools get passed in? Where does this all come from? Uh and if you're like me, you eventually just throw it all away and start from scratch. Um or you may even find out that this is not a good problem for agents. I remember one of the first agents that I tried to build was uh a DevOps agent. I was like, \"Here's my make file. You can run make commands. Go build the project.\" Couldn't figure it out. Did all the things in the wrong order. I'm like, \"Cool, let's fix the prompt.\" And I over the la next two hours I'm more and more detail about what everything was and every single step and the exact until I got to the point where I was like this is the exact order to run the build steps. It was a cool exercise but at the end of it I was like you know I could have written the bash script to do this in about 90 seconds. Not every problem needs an agent. Um and so I've been on this journey. I think a lot of you have been on similar journeys. Um and what happened was is I went and talked um in trying to help people build better, more reliable agents. I talked to a 100 plus founders, builders, engineers. Um, and I started to notice patterns. One was that most production agents weren't that agentic at all. They were mostly just software, but that there were these core things that a lot of people were doing. There were these patterns that were making their LLM based apps really, really good. Um, and none of them were doing kind of a green field rewrite. Rather, they were taking these small modular concepts that didn't have names and didn't have definitions and they were applying them to their existing code. Um, and what's really cool about this is I don't think you need an AI background to do this. This is software engineering 101. Well, probably not 101, but just like Heroku needed to define what it meant to build cloud. We didn't even call them cloud native back then, but this was how you built applications that could run in the cloud 10 years ago. Um, I decided to put together what I thought would be the 12 factors of AI agents. Um, from everything that I've seen working in the field. Um, so we put up this github.com repo. You can go read it. Um, turns out a lot of other people agreed and felt the same thing. Um, so we were on the front page of news.ycombinator.com all day, 200k impressions on social. Uh, I'm just going to put this one up and no comment. Um, and just for context, we got to like 4,000 stars in like a month or two. Uh, there's 14 active contributors. Um, it's very easy to read that thing and and hear the talk and say like, oh, we're here, this is the anti-framework talk. I am not here to bash frameworks. I would think of this as much as anything as a wish list, a a list of feature requests is how can we make frameworks serve the needs of really good builders who need a really high reliability and want to move fast still. Um, so what am I here to do? Uh, I want you to kind of forget everything you know about agents and kind of rethink from first principles how we can apply everything we've learned from software engineering to the practice of building really reliable agents. Um, so we're going to mix the order up a little bit. If you want all 12 factors in order, that's a 30-minute talk, so we're going to bundle some stuff together. There will be a QR code at the end. You can go dig through it at your leisure. Um, factor one, the most magical things that LLMs can do. Has nothing to do with loops or switch statements or code or tools or anything. It is turning a sentence like this into JSON that looks like this. Doesn't even matter what you do with that JSON. Uh, those are what the other factors are for. But if you're doing that, that's one piece that you can bring into your app today. Um, factor four. This leads right to uh did anyone read this paper uh go to considered harmful or maybe just heard about it? I never actually read it. Uh but it was all about we had this abstraction in the C programming language and a bunch of other programming languages at the time that said this thing go to it makes code terrible. It's the wrong abstraction. It no one should use it. I'm going to go ahead and go on a limb here and say tool use is harmful. And I put it in quotes because I'm not talking about giving an agent access to the world. Obviously that's super badass. But what I think is making things hard is the idea that tool use is this magical thing where this ethereal alien entity is interacting with its environment because what is happening is our LLM is putting out JSON. We're going to give that to some deterministic code that's going to do something and then maybe we'll feed it back. But again, those are other factors. So if you have structures like this and you can get the LLM to output something that generates them, then you can pass it into a loop like this or a switch statement like this. There's nothing special about tools. It's just JSON and code. Uh, that's factor four. Factor eight, and these are we're going to do a couple kind of bundled together here. Owning your control flow. Um, and I want to take a step back and kind of talk about how we got here. Um, we've been writing DAGs in software for a long time. If you've written an if statement, you've written a directed graph. Uh, code is a graph. You may also be familiar with DAG orchestrators. Anyone ever use like Airflow or Prefect or any of these things? Um, so like this kind of concept of breaking things up into nodes gives you certain reliability guarantees. But what agents were supposed to do, and I think a lot of people talk about this, and I think in some cases this is realized, is you don't have to write the DAG. You just tell the LLM, here's the goal, and LLM will find its way there. And we model this as a really simple loop. You know, LLM is determining the next step. You're building up some context window until the LLM says, \"Hey, we're done.\" Um, so what this looks like kind of in practice is, you know, you have an event come in. You pass it into your prompt. Uh, it says you want to call an API and you get your result. Put that on the context window. Pass the whole thing back into the prompt. This is like the most naive simple way of building agents. And the LLM's going to call a couple steps and then eventually it's going to say, cool, we've done all the tasks from the initial event, which maybe was a user message asking it to do something. Maybe it's an outage. Um, but then we get our final answer. And our materialized DAG is just these three steps in order. Um, turns out this doesn't really work. Uh, especially when you get to longer workflows. Mostly it's long context windows. There's other reasons you could poke at as well. Um, and people say, \"Oh, like even as anyone put like two million tokens into Gemini before and like try to see what happens.\" Like you can do it. You'll get an answer. The API will return you something. But I don't think anyone will argue with you that you will always get like tighter, better, higher reliability results by controlling and limiting the number of tokens you put in that context window. Um, so it doesn't quite work, but we're going to use that as our abstraction to build on. What's an agent really? You have your prompt, which gives instructions about how to select the next step. You have your switch statement, which takes whatever the model output JSON, uh, and does something with it. You have a way of building up your context window. And then you have a loop uh that determines when and where and how and why you exit. Um and if you own your control flow, you can do fun things like break and switch and summarize and LLM is judge and all this stuff. Um and this leads right into kind of how we manage execution state and business state of our agents. Um a lot of tools will give you things like current step, next step, retry counts, all these like DAG orchestrators, they all have this kind of concepts in them. Um but you also have your business state. What are the messages that have happened? What data are we displaying to the user? What things are we waiting on approval for? Um, and we want to be able to launch, pause, resume these things like we do for any standard APIs. Um, this is all just software. And so if you can put your agent behind a REST API or an MCP server, um, and manage that loop in such a way that normal request comes in and we load that context window to the LLM. Um, we're going to allow our agent to call longunning tool. So we can interrupt the workflow, serialize that context window straight into a database because we own the context window. We'll get into that. Um, and then when we launch the workflow, um, eventually it's going to call back with that state ID and the result. We use the state ID to load the state back out of the database, and then we can append the result to the program and then send it right back into the LLM. The agent doesn't even know that things happened in the background. Um, agents are just software, so let's build software. Um, and building really good ones need requires a lot of flexibility. And so you really want to own that inner loop of of how all that stuff is fitting together. Um, that's unifying. That's pause and resume. Uh factor two, this one is I think most people find first is like you really want to own your prompts. There's some good extractions that if you don't want to spend a lot of time handwriting a prompt, you can put stuff in and you'll get out um a really good set of primitives um and a really good prompt. Like this will make you a banger prompt that like you would have to go to prompt school for like three months to build a prompt this good, but eventually if you want to get past some quality bar, you're going to end up writing every single token by hand. Um because LLMs are pure focus functions and the only thing that determines the reliability of your agent is how good of tokens can you get out and the only way the only thing that determines the tokens you get out other than like retraining your own model and something like that is being really careful about what tokens you put in. Um I don't know what's better. I don't know how you want to build your prompt, but I know the more things you can try and the more knobs you can test and the more things you can evaluate, the more likely you are to find something really really good. Um owning your prompts, you also want to own how you build your context window. Um, so you can do the standard open AI messages format or in this moment where you're telling the LLM pick the next step, your only job is to tell it what's happened so far. You can put all that information however you want into a single user message and ask, hey, what's happening next or put in the system message. So you can model your event state, your thread model however you want, um, and stringify it however you want. And some of the traces that we use in some of the agents we build internally, I'll get into that in a sec, um, might look like this. Um, but if you're not looking at every single token and if you're not optimizing the density and the clarity of the way that you're passing information to an LLM, you might be missing out on upside and quality. So LLMs are pure functions, token in, tokens out, and everything everything in making agents good is context engineering. So you have your prompt, you have your memory, you have your RAG, you have your history. It's all just how do we get the right tokens into the model. So it gives us a really good answer and solves the user's problem. solves my problem mostly. Um, I don't know what's better, but I know you want to try everything. Um, so let's own your context building. Um, this one's a little controversial. Uh, that's why it's a it's a standalone factor. And the way you make it good is by integrating it with other factors. But you could, um, when the model screws up and it calls an API wrong or it calls an API that's down, um, you could take the tool call that it made and grab the error that was associated with it, put that on the context window, and have it try again. Anyone ever had a bad time with this? Seen like this thing just like kind of spin out and like go crazy and lose context and just get stuck. Um, that's why you need to own your context window. Don't just blindly put things on. If you have errors and then you get a valid tool call, clear all the air pending errors out. Summarize them. Don't put the whole stack trace on your context. Figure out what you want to tell the model so you get better results. Um, contacting humans with tools. This one's a little subtle. Um, but I've seen this is just like what I've seen in the wild. almost everybody is like avoiding this very important choice at the very beginning of output where you're deciding between tool call and message to the human. Um if you can push that emphasis to a natural language token, you can one give the model different ways. You can be I'm done or I need clarification or I need to talk to a manager or whatever it is and two you push the intent on that first token generation and the sampling to something that is natural language that the model understands. Um, so your traces might look like this if you're pulling in human input here. Um, this lets you build out auto outloop agents. I'm not going to talk about this. If you go on the site, there's a link to this this post. I've written a\n\nA lot about this. Um, I don't know what's better, but you should probably try everything. Um, that's contacting humans with tools. Goes right along with trigger things from anywhere and meet users exactly where they are. People don't want to have seven tabs open of different ChatGPT style agents. Just let people email with the agents you're building. Let them Slack with the agents you're building. Discord, SMS, whatever it is. We see this taking off all over the place. Um, and you should have small focused agents. So, we talked about this structure and why it doesn't really work. So, what does work? Um, the things that people are doing that work really well are micro agents. So, you still have a mostly deterministic DAG and you have these very small agent loops with like three to 10 steps. We do this at HumanLayer. We have a bot that manages our deployments. Most of our deploy pipeline is deterministic CI/CD code. But when we get to the point where the GitHub PR is merged and the tests are passing on development, excuse me, we send it to a model. We say get this thing deployed. Says cool, I'm going to deploy the front end. Uh, and then you can send that to a human. The human says actually no, do the back end first. This is taking Natural Language to Tool Calls and turning it into JSON. That is the next step in our workflow. Um, backend gets proposed, that gets approved, it gets deployed, then the agent knows, okay, I have to go back and deploy the front end. Once that's all done and it's successful, we go right back out into deterministic code. So now we're going to run the end-to-end test against prod. If it's done, otherwise we hand it back to a little rollback agent that is very similar on the inside. Um, I'm not going to go into it, but here's it working in our Slack channel. Um, yeah, 100 tools, 20 steps, easy. Um, manageable context, clear responsibilities. Um, a lot of people say, \"What if LLMs do keep getting smarter? What if I can put two million tokens in and it can do it?\" Um, and I think we very much will see something like this where you start with a mostly deterministic workflow and you start sprinkling LLMs into your code, into your backend, into your logic. Over time, the LLMs are able to do bigger, more complex tasks until this whole API endpoint or pipeline or whatever it is is just run by an agent. That's great. Uh, but you still want to know how to engineer these things to get the best quality. This is someone from NotebookLM and it's basically their take and I think they did this well is find something that is right at the boundary of what the model can do reliably like that it can't get right all the time and if you can figure out how to get it right reliably anyways because you've engineered reliability into your system then you will have created something magical and you will have created something that's better than what everybody else is building. So that's small focused agents. There's a meme here about stateless reducers. I guess someone actually tweeted at me, it's not a reducer, it's a transducer because there's multiple steps. Um, but basically agents should be stateless. You should own the state, manage it however you want. Um, so we're all still finding the right abstractions. Um, there's a couple blog posts I link in the paper. Um, frameworks versus libraries. There's a really old one from RubyConf about like do we want duplication or is like do we want to try to figure out these abstractions? Um, if you want to make a 12-Factor Agent, we are working on something called create 12-Factor Agent because I believe that what agents need is not bootstrap. You don't need a wrapper around an internal thing. You need something more like shadcn, which is like scaffolded out and then I'll own it and I'll own the code and I'm okay with that. So, in summary, agents are software. You all can build software. Anyone ever written a switch statement before? While loop. Yeah. Okay. So, we can do this stuff. LLMs are stateless functions, which means just make sure you put the right things in the context and you'll get the best results. Own your state and your control flow and just do it and just understand it because it's going to give you flexibility. And then find the bleeding edge. Find ways to do things better than everybody else by really curating what you put in the model and how you control what comes out. Um, and my tech agents are better with people. Find ways to let agents collaborate with humans. Um, there are hard things in building agents, but you should probably do them anyways, at least for now, and you should do most of them. Um, I think a lot of frameworks try to take away the hard AI parts of the problem so that you can just kind of drop it in and go. And, uh, I think it should be the opposite. I think the tools that we get should take away the other hard parts so that we can spend all our time focusing on the hard AI parts, on getting the prompts right, on getting the flow right, on getting the tokens right. So the reason why I'm here is like I do run a small business. Um, we have a startup where we try to help you do a lot of what we do we do in the open is open source and I think it's really important and we need to work on it together. There's some other things that are hard but not that important and not that interesting. So that's what we're solving at HumanLayer. Um, working on something called the A2 protocol. Come find me if you want to talk about this but this is a way to get like consolidation around how agents can contact humans. Um, but mostly I just love automating things. I've built tons and tons of agents internally for my personal stuff, for finding apartments, for all kinds of internal business stuff we do at HumanLayer. Um, so thank you all for watching. Let's go build something. I'll see you in the hallway track. Uh, I'd love to chat if you want to riff on agents or building or control flow or any of this stuff. That's 12-Factor Agents.\n[Applause]",
  "dumpedAt": "2025-07-21T18:43:26.472Z"
}