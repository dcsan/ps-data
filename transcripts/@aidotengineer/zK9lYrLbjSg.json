{
  "episodeId": "zK9lYrLbjSg",
  "channelSlug": "@aidotengineer",
  "title": "AI Engineering with the Google Gemini 2.5 Model Family - Philipp Schmid, Google DeepMind",
  "publishedAt": "2025-07-11T19:00:06.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 0.33,
      "duration": 7.479
    },
    {
      "lang": "en",
      "text": "Okay. Hi everyone. Welcome, welcome. Um,",
      "offset": 15.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "so welcome to our workshop AI",
      "offset": 19.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "engineering with the Google Gemini 2.0",
      "offset": 22,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "family. So as it is a workshop, we are",
      "offset": 24.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "going to keep it super hands-on. So",
      "offset": 26.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "please keep all computer open. Uh you",
      "offset": 28.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "don't need any Google account um like",
      "offset": 31.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Google cloud account. You can use your",
      "offset": 33.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "personal Gmail. It will be completely",
      "offset": 35.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "free for you to use. So that's the",
      "offset": 36.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "point. Um before we get started, can you",
      "offset": 38.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "maybe help me understand how many of you",
      "offset": 41.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "has used Google Gemini before?",
      "offset": 43.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Oh wow. That's that's cool. That's a lot",
      "offset": 48.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of hands. More than the last time I gave",
      "offset": 50,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a talk like this. Um so what um we're",
      "offset": 51.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "going to do you have like three slides",
      "offset": 54.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "so don't worry not too much. But we're",
      "offset": 56.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "going to focus on Gemini 2.5. So there's",
      "offset": 58.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "a Gemini 2.5 Pro model and a Gemini 2.5",
      "offset": 60.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "flesh model. We're going to use the",
      "offset": 64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "flash model as it's available uh for",
      "offset": 65.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "free tier via API access. So we are",
      "offset": 67.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "going to do coding and both models are",
      "offset": 69.92,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "multimodal by default meaning they can",
      "offset": 72.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "understand text, images, audio, videos,",
      "offset": 75.439,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "documents and can generate text. We also",
      "offset": 78,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "have Gemini models which can generate",
      "offset": 80.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "images which we are also going to use.",
      "offset": 82.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And we have now Gemini models which can",
      "offset": 84.479,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "generate uh audio. So you can create",
      "offset": 86.479,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "speech from from text. Um if you are",
      "offset": 89.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "curious where you can find those nice",
      "offset": 92.479,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "model cards with all of the feature the",
      "offset": 94.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model context the output tokens it's on",
      "offset": 95.759,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the the Gemini docs. Um as mentioned we",
      "offset": 98.079,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "have two new um texttospech models since",
      "offset": 101.439,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "Google IO last week not a week before.",
      "offset": 104.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Um those are really cool. you will try",
      "offset": 107.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and see them later. And now for the",
      "offset": 109.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "important details. So I created a Slack",
      "offset": 111.52,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "channel. If you are on the AI",
      "offset": 113.2,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "engineering Slack channel, you should be",
      "offset": 114.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "able to find it. Um feel free to use it",
      "offset": 115.759,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "during the workshop. You can ask",
      "offset": 118.079,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "questions. I try to regularly check uh",
      "offset": 119.439,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to answer them or even afterwards if you",
      "offset": 121.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have questions, complete the workshop at",
      "offset": 124.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "home or next week. Uh I will take a look",
      "offset": 125.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and make sure that you get all of the",
      "offset": 128,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "answers. And then uh we have one QR",
      "offset": 129.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "code. Uh it's AI studio. You can also",
      "offset": 131.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "just enter in your browser AI.dev or",
      "offset": 133.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "AI.studio studio which will brings you",
      "offset": 135.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to AI studio and the other link is so",
      "offset": 137.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "there's a github repository uh with the",
      "offset": 140,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "workshop we are going to do the workshop",
      "offset": 141.92,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "I can like now switch directly to it so",
      "offset": 144.64,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 148.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "let's hope if the Wi-Fi give us some",
      "offset": 151.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "freedom so the good part about a",
      "offset": 153.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "workshop is that we have Google collabs",
      "offset": 154.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "so there's not a lot of downloading uh",
      "offset": 157.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "happening and it will all run in the",
      "offset": 159.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "collab environment if you have a Google",
      "offset": 161.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "account and the other thing is what we",
      "offset": 164.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "need to do is in AI studio to generate",
      "offset": 166.4,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "an AI key. So the GitHub repository is",
      "offset": 168.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "now loaded. In the GitHub repository, we",
      "offset": 170.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have a notebooks.",
      "offset": 173.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Uh yeah, of course. Sorry. So in the",
      "offset": 174.959,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "GitHub repository, we have a notebooks",
      "offset": 178.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh folder which includes all of our four",
      "offset": 180.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "workshops uh plus a zero one which is",
      "offset": 183.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "basically some minor instructions how to",
      "offset": 186.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "set up AI studio, how to get an API key",
      "offset": 188.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and how to send your first request. And",
      "offset": 191.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then uh we have one or the beginning",
      "offset": 193.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "section will be all about text",
      "offset": 195.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "generation um getting started getting",
      "offset": 196.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "familiar a bit with the SDK. Uh the",
      "offset": 198.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "second part will be all about",
      "offset": 200.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "multimodality. How can Gemini understand",
      "offset": 202,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "images, video, audios? How can we",
      "offset": 204.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "generate images or audio? And then the",
      "offset": 206.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "third um section will be about function",
      "offset": 209.28,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "calling, structured outputs, the native",
      "offset": 211.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "tools, how can I integrate Google search",
      "offset": 212.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "into it? And then I guess um with all of",
      "offset": 214.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the hype currently going on, we we look",
      "offset": 216.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "at how you can integrate MCP servers",
      "offset": 218.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "together with Gemini using it as a model",
      "offset": 220.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to call the different tools. Um also",
      "offset": 222.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "very nice. So there's a solutions um",
      "offset": 225.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "folder. The solutions folder includes",
      "offset": 227.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "the same notebooks but with the",
      "offset": 229.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "solutions. So all of the notebooks",
      "offset": 232.159,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "include to-do text and also some code",
      "offset": 234.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "snippets um and some comments. So",
      "offset": 237.439,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "there's a a mix between working code",
      "offset": 240.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "snippets, code snippets which has some",
      "offset": 242.319,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "pointers and straight up exercises with",
      "offset": 244.08,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "to-dos for for you to do. Um I will work",
      "offset": 247.439,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "with you through like the existing",
      "offset": 250.879,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "snippets and then everyone can work on",
      "offset": 252.879,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the exercises. The idea is to that we",
      "offset": 255.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "try to maybe use 30 minutes per",
      "offset": 257.759,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "different section. If you for example",
      "offset": 260.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are already very familiar with how what",
      "offset": 262.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I can do with text generation and I",
      "offset": 264.479,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "would like rather like look at the",
      "offset": 266.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "multimodalities parts or at the function",
      "offset": 267.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "calling parts feel free to like directly",
      "offset": 270.4,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "jump into the section and in general we",
      "offset": 272.24,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "want to keep it very open very dynamic",
      "offset": 274.479,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "if you have questions related to the",
      "offset": 277.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "content maybe unrelated please keep them",
      "offset": 279.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "coming ask them in slap raise your hands",
      "offset": 282.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I'm not sure maybe we have some",
      "offset": 284.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "microphones here as well so we can like",
      "offset": 285.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "give it to you to uh make it super",
      "offset": 287.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "interactive So I guess let's get",
      "offset": 289.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "started. So if you go to the notebooks,",
      "offset": 292.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "there's also a collab um button you can",
      "offset": 294.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "click which opens the notebook directly",
      "offset": 296.479,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "in Google Collab. And if you prefer like",
      "offset": 297.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "a local Jupyter environment, you can try",
      "offset": 299.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to clone the repository. Not sure if it",
      "offset": 301.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "works for Wi-Fi or not. Um I guess",
      "offset": 303.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Collab will be the easiest. And as",
      "offset": 305.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "mentioned before, the only requirement",
      "offset": 307.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you basically have is a working Google",
      "offset": 309.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "account. Can be your corporate one, can",
      "offset": 312.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "be your private one, can be one you",
      "offset": 315.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "create in the next five minutes. And the",
      "offset": 317.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "first step is uh what we need to do is",
      "offset": 320.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "basically go to AI studio for the ones",
      "offset": 322.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "who of you who are not familiar with AI",
      "offset": 324.479,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "studio. AI studio um is our developer",
      "offset": 326.479,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "platform to quickly test the models to",
      "offset": 329.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "experiment with the models uh and also",
      "offset": 332,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "keep it very similar to the development",
      "offset": 334.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "code you will be used. So if I try to",
      "offset": 336.639,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "run a request like um maybe let's ask",
      "offset": 338.88,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "something what's the AI",
      "offset": 342.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "engineering summit I can on the right",
      "offset": 345.759,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "side for example enable native tools",
      "offset": 348.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "connected with Google search I have our",
      "offset": 350.32,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "flash preview model I can run the",
      "offset": 351.919,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "request and",
      "offset": 353.759,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "we'll see how fast yeah the model is",
      "offset": 356.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "thinking and the nice part here is uh I",
      "offset": 358.88,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "can also directly get the SDK code from",
      "offset": 361.12,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "our request as soon as it's ready so if",
      "offset": 364.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "you are experimenting in AI studio and",
      "offset": 367.039,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you want to convert it into a Python",
      "offset": 368.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "script or want to play around with it",
      "offset": 370.479,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "extend it um that's all possible so the",
      "offset": 373.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "engineering summit refers to several",
      "offset": 376.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "events focusing on artificial",
      "offset": 378.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "intelligence and engineering that's",
      "offset": 380.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "great and that also matches like the one",
      "offset": 382.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "from New York which was done at this",
      "offset": 385.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "February school so what you need to do",
      "offset": 387.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "to get your API key at the top right is",
      "offset": 389.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh I can also make that bigger maybe",
      "offset": 392.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it's easier so we have a get API key at",
      "offset": 394.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the top um you go to it um sorry for",
      "offset": 396.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it's German uh for being it's German but",
      "offset": 400.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "on the top right corner there should be",
      "offset": 402.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "a create API key a blue button and in",
      "offset": 404.72,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "there you should it should open um s uh",
      "offset": 408.08,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "I can click it should open s model where",
      "offset": 411.68,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you can either select your Google cloud",
      "offset": 415.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "project if there's none you should be",
      "offset": 417.6,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "able to create one if yep",
      "offset": 420,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "uh yep Of",
      "offset": 424.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "course.",
      "offset": 426.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 428.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "parent.",
      "offset": 432.639,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay. Good idea. Okay. Uh once you",
      "offset": 438.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "select your Google Cloud project or",
      "offset": 442.479,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "create one, you should be able to create",
      "offset": 444,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "one. And once it is created, um you",
      "offset": 446.16,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "should have it available as a popup. If",
      "offset": 449.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "not, you can scroll down a bit. There",
      "offset": 451.199,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are your API keys. Um and then the",
      "offset": 452.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "second step would be to go into collab",
      "offset": 455.599,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "and to go on the left side uh in the",
      "offset": 458.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "navigation. So I can also let me quickly",
      "offset": 461.44,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "change that to light mode as well. Um",
      "offset": 463.28,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "but anyways left side there's a key",
      "offset": 468.479,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "which is called secrets and then you",
      "offset": 471.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "enter a name which is Gemini API key and",
      "offset": 473.919,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "then the value of your API key. All of",
      "offset": 476.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "what I walked through is also part of",
      "offset": 478.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the 00 um setup and authentication",
      "offset": 480.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "notebook. So if it was too fast, you can",
      "offset": 483.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like look up there should be screenshot",
      "offset": 485.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "of it where I clicked on where to add",
      "offset": 486.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "it. If you are running locally um you",
      "offset": 488.639,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "need to expose the um API key as",
      "offset": 491.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "environment variable with the same name",
      "offset": 493.919,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "should be also part of our um notebook.",
      "offset": 496.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "So in the first cell so basically what",
      "offset": 499.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "we try here is we check whether we are",
      "offset": 502.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "in Google Collab. If we are in Collab",
      "offset": 504.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 506.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Yeah, we don't worry like we go through",
      "offset": 510.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it one and then you have enough time to",
      "offset": 512.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "like five to 10 minutes to set it up",
      "offset": 514.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "yourself. Okay. Quickly that we",
      "offset": 515.919,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "Yep. Is there any poweroint?",
      "offset": 519.279,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "No, no PowerPoint code only. Um, so we",
      "offset": 522.32,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "we go through the API key setup in a",
      "offset": 527.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "minute. You have plenty of time to do it",
      "offset": 529.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "yourself. And if you have questions, I'm",
      "offset": 531.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "very happy to come to your place and",
      "offset": 532.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "help you get it created. just to",
      "offset": 534.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "complete the the setup. So once you have",
      "offset": 536.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "created your API key made it available",
      "offset": 538.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to collab or made it available in your",
      "offset": 540.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "environment best is to open the first uh",
      "offset": 542.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "notebook it has a super small code",
      "offset": 545.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "snippet in it which uses your API key",
      "offset": 547.36,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "and generates um uses Gemini 2.0 flash",
      "offset": 550.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "to generate a first string. So our goal",
      "offset": 553.839,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "next 5 to 10 minutes is trying to get",
      "offset": 556.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "this working. Okay.",
      "offset": 558.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "And again going back. So we have those",
      "offset": 561.279,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "QQR codes. One is for AI studio. It's",
      "offset": 564.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the left one. The other one is for the",
      "offset": 567.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "GitHub repository. You can also go to AI",
      "offset": 570.08,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "studio uh AI.dev to enter AI studio or",
      "offset": 572.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "go through Google search. And you can",
      "offset": 575.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "find the GitHub repository on my GitHub",
      "offset": 577.519,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "account. Um it's like",
      "offset": 579.519,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "gemini.2.5.aiengineering.workshop.",
      "offset": 580.399,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "And um I will in the meantime change the",
      "offset": 585.2,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "appearance.",
      "offset": 588.399,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah, sorry. Sorry.",
      "offset": 592,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "Um, so there are in the GitHub",
      "offset": 599.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "repository if you go to the notebook",
      "offset": 602.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "section and each of the notebooks at the",
      "offset": 604.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "top there's a button which opens collab",
      "offset": 606.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "directly with the notebook.",
      "offset": 609.12,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 612.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay. Quick check. Are we are we ready?",
      "offset": 615.839,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "any notes?",
      "offset": 620.16,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "Okay, cool.",
      "offset": 622.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "So",
      "offset": 626.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the first section will be all about the",
      "offset": 628.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "the default. Basically LLM started with",
      "offset": 632.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "being text only. We generated a text. So",
      "offset": 635.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "what would the first section basically",
      "offset": 637.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "covers um all of how can I generate",
      "offset": 640.079,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "text? How can I uh tech generate text",
      "offset": 643.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and have like a streaming response? How",
      "offset": 646.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "can I count my tokens? It's like always",
      "offset": 648.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "important, right? To understand how many",
      "offset": 651.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "tokens did I use, how much will it cost?",
      "offset": 652.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And there are like a few exercises for",
      "offset": 655.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you to try out different models, try out",
      "offset": 657.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "different prompts. It will also go a bit",
      "offset": 660.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "into detail on how the SDK works in",
      "offset": 662.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "terms of like which inputs you can",
      "offset": 665.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "provide. So in the Google AI SDK, we",
      "offset": 667.279,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "have um this concept of a client and",
      "offset": 670.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "client has the the models abstraction.",
      "offset": 673.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "And the model substraction has the",
      "offset": 675.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "method uh generate content or generate",
      "offset": 676.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "stream content and I can also make it",
      "offset": 679.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "maybe a bit bigger and uh each of the or",
      "offset": 682,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it has parameter for the model and the",
      "offset": 685.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "model ID is basically the Gemini model",
      "offset": 686.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we want to use which is defined at the",
      "offset": 688.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "top. Uh so all of those cells use the",
      "offset": 691.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the same concept. So all of the workshop",
      "offset": 694.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "section have the same. Um if you think",
      "offset": 697.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "okay 2.5 flash is not the right model",
      "offset": 699.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "for you, you can change it to a",
      "offset": 701.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "different model ID. If you have like an",
      "offset": 703.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "paid account and want to use the pro",
      "offset": 705.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "version, you can also change it. Um",
      "offset": 706.56,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "and um contents is basically our way to",
      "offset": 709.68,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "provide uh data or conversations, chats",
      "offset": 713.279,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "and messages to um Gemini. So the first",
      "offset": 716.079,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "um test basically is we ask it to",
      "offset": 720.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "generate three names for a coffee shop",
      "offset": 724,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that emphasizes sustainability",
      "offset": 725.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and we use the client models generate",
      "offset": 728.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "content. We have our model ID, our",
      "offset": 731.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "prompt and then we get our response from",
      "offset": 732.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "from Gemini.",
      "offset": 735.279,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "And um if you have already set up",
      "offset": 737.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "everything, you can try prompting a few",
      "offset": 740.959,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "things. Um ask it to explain some terms",
      "offset": 743.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "or maybe like just change the model ID",
      "offset": 746.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "and then we continue with um counting",
      "offset": 749.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "tokens. So there are exercises in there",
      "offset": 751.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "uh which don't have any code snippets.",
      "offset": 754.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "The solutions part of the workshop has",
      "offset": 756.959,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the code. So if you are getting stuck or",
      "offset": 759.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "if you want to look it up uh what I",
      "offset": 762.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "added uh feel free to take a look there",
      "offset": 764.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "but um definitely try it first yourself",
      "offset": 767.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "if you want to get familiar with the SDK",
      "offset": 769.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "there are plenty of other snippets or",
      "offset": 772.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "exercises basically just to u make sure",
      "offset": 773.839,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that you understand the concepts and um",
      "offset": 776.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "can practice it and there are other",
      "offset": 778.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "cells which are partially done so for",
      "offset": 781.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "example the one we have here which has",
      "offset": 783.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "some code comments and also some to-do",
      "offset": 786,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "calls here the idea is really to to",
      "offset": 788.32,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "force you not to learn new APIs. So next",
      "offset": 791.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to the um generate content method,",
      "offset": 794.399,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "there's also a count token um um API",
      "offset": 796.639,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "which we can use to count our tokens. So",
      "offset": 800.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "similar to our generate method, we",
      "offset": 804.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "provide our model ID and then our prompt",
      "offset": 806.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "here. And basically what the uh API does",
      "offset": 808.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "is now it counts only the tokens for our",
      "offset": 811.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "prompt since we we haven't generated",
      "offset": 814,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "something. So we can run it um and we",
      "offset": 816.639,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "get an input tokens of 11. So the Gemini",
      "offset": 819.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "tokenizer basically converted those two",
      "offset": 822.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "four six 89",
      "offset": 825.76,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "words plus a full stop to 11 tokens. Uh",
      "offset": 828.88,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "which is then um an estimate of roughly",
      "offset": 832.48,
      "duration": 4.33
    },
    {
      "lang": "en",
      "text": "0.00002.",
      "offset": 835.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 836.81,
      "duration": 6.23
    },
    {
      "lang": "en",
      "text": "Um the count tokens API doesn't expose",
      "offset": 839.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the pricing. So basically what I did",
      "offset": 843.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "here is like looked up the 2.5 flash",
      "offset": 844.8,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "pricing and calculated it. Um similar to",
      "offset": 847.12,
      "duration": 9.519
    },
    {
      "lang": "en",
      "text": "only counting the input tokens, we often",
      "offset": 853.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "times also want to count the output",
      "offset": 856.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "tokens to understand okay",
      "offset": 858.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "um how much does it cost. So in the next",
      "offset": 861.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "uh example we basically generate content",
      "offset": 863.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "and each response has a very nice method",
      "offset": 866,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "which is called like an abstraction",
      "offset": 869.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "which is called a text which allows us",
      "offset": 871.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "to actually uh easily access the",
      "offset": 873.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "generation but also has a usage meta",
      "offset": 875.279,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "data object and a usage meta data object",
      "offset": 878.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "includes all of our consumed tokens and",
      "offset": 881.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "generated tokens. So we have input",
      "offset": 884.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "tokens uh we have thought tokens. So",
      "offset": 885.6,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "Gemini uh 2.5 is a thinking model. So",
      "offset": 888.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "before generating your response, it",
      "offset": 891.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "first generates thinking tokens",
      "offset": 893.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "basically an abstraction where it uses",
      "offset": 895.68,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "more compute to have like more room to",
      "offset": 898.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "generate a good answer for you and then",
      "offset": 901.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "also the candidate tokens uh which are",
      "offset": 902.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the response tokens at the end and",
      "offset": 905.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that's how we can calculate the total",
      "offset": 907.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "cost of a request where we use the input",
      "offset": 909.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh token price and then our candidates",
      "offset": 912,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "tokens and of our tokens and um for this",
      "offset": 914.24,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "case it would be um less than like zero",
      "offset": 917.76,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "2 cents. Yep.",
      "offset": 921.199,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 926.72,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah. Is there a different price forken?",
      "offset": 930.16,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "No. So, um input and output tokens are",
      "offset": 934.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "calculated different um as",
      "offset": 937.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "yes. So we have prompt tokens is",
      "offset": 940.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "basically the input tokens of your",
      "offset": 942.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "prompt and then we have the candidate",
      "offset": 943.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "tokens which is the your response and",
      "offset": 945.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the thought tokens and those basically",
      "offset": 947.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "have the same pricing and the output",
      "offset": 950.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tokens are much more expensive than the",
      "offset": 952.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "input tokens because that's where the",
      "offset": 954.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "computation mostly happens and the input",
      "offset": 956.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is just one encoding. So that's why you",
      "offset": 958.16,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "always see like the big difference in",
      "offset": 959.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "output input pricing versus output",
      "offset": 961.759,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "pricing",
      "offset": 964.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "more.",
      "offset": 966.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Yeah. So for Gemini 2.0 flesh which is",
      "offset": 968.639,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "our most cost effective and cheapest",
      "offset": 972.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "model the input price for 1 million is",
      "offset": 974.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "10 cents and the output price is 40",
      "offset": 976.399,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "cents.",
      "offset": 978.16,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So you mean like why we got 639 for",
      "offset": 991.36,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "total? So that's sadly not like directly",
      "offset": 994.639,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "visible to us. So we can like look up",
      "offset": 998.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "thought summaries but basically the",
      "offset": 1000.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "model generates first of all like a lot",
      "offset": 1002.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "of like reasoning. Of course, in our",
      "offset": 1004.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "case, like we we ask it to generate a",
      "offset": 1006.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "haiku might be not like the most sorry",
      "offset": 1009.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "most difficult um prompt. Um you can",
      "offset": 1012.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "control the thought tokens with",
      "offset": 1014.72,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "something called thinking budget where",
      "offset": 1016,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you can limit how many tokens the model",
      "offset": 1017.759,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "has to think or to reason. So you have a",
      "offset": 1020.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "some sort of a cost control, but it's",
      "offset": 1023.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "basically done dynamically based on your",
      "offset": 1025.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "prompt. Okay. Yeah, there's also a",
      "offset": 1027.039,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "question. Yeah, I was just",
      "offset": 1029.039,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "looking",
      "offset": 1034.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Y",
      "offset": 1037.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "yes. So um I can let me open the docs",
      "offset": 1044.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and make make it easier. Um so um Gemini",
      "offset": 1047.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "2.5 flash is a hybrid model. So you can",
      "offset": 1050.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "use it with thinking and without",
      "offset": 1053.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "thinking and without thinking basically",
      "offset": 1055.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the computation is much more cost",
      "offset": 1058.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "effective as um you might know like the",
      "offset": 1060,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "transformers is all based on intention",
      "offset": 1063.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "which isn't like",
      "offset": 1065.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and bionics like kind of calculation. So",
      "offset": 1068.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it gets bigger and bigger which means it",
      "offset": 1070.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "gets more and more computens and um",
      "offset": 1072.16,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "without thinking it's for us much easier",
      "offset": 1075.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "or inefficient to run. So if you set um",
      "offset": 1077.919,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "thinking to",
      "offset": 1081.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "uh zero or like thinking budget to zero",
      "offset": 1083.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "uh you all you will have zero thought",
      "offset": 1086.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "tokens but you have like your candidate",
      "offset": 1088.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "tokens and those candidate tokens will",
      "offset": 1090.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "then be charged with 0.60 60 cents. But",
      "offset": 1092.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "once you once you use thinking meaning a",
      "offset": 1096.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "thinking budget greater zero, you will",
      "offset": 1098.799,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "um pay the price for the thinking tokens",
      "offset": 1101.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and for the output tokens. And that's",
      "offset": 1105.36,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "where's the the $3.5.",
      "offset": 1106.88,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "Yes, I will open the documentation. So",
      "offset": 1116.559,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "we'll see it in one second.",
      "offset": 1118.32,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "flag.",
      "offset": 1125.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Yeah. So, um in any case, if you have",
      "offset": 1128.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "like any questions, the Gemini docs are",
      "offset": 1130.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a great way to like find the answers or",
      "offset": 1132.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "like just ask Google or Gemini directly",
      "offset": 1134.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and on the model capabilities we have",
      "offset": 1137.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the thinking section and they are like",
      "offset": 1139.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the thinking budgets and if you want to",
      "offset": 1141.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "disable thinking you basically set the",
      "offset": 1142.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "think it budget to zero. Uh you can",
      "offset": 1144.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "control it. It could be a integer",
      "offset": 1147.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "between like 0 and 24,000 and then",
      "offset": 1149.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "setting think budget to zero disables",
      "offset": 1152.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "thinking. So that's your way to disable",
      "offset": 1154.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "thinking. Is the budget like a number of",
      "offset": 1156.799,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "reasoning to do?",
      "offset": 1160.16,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "No, it's tokens. So yeah, so we we have",
      "offset": 1163.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "yeah so we have seen in our example here",
      "offset": 1167.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we had 600 a bit more than 64 tokens. So",
      "offset": 1169.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "if we would set our thinking budget to",
      "offset": 1172.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "512, it would be a maximum of 512",
      "offset": 1174.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "thinking token.",
      "offset": 1178.16,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "How does it know?",
      "offset": 1180.32,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "No.",
      "offset": 1190.08,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 1192.559,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I guess. Okay. continuing uh with our um",
      "offset": 1196.88,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "notebook. So, and please continue",
      "offset": 1201.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "yourself like I'm like you can do it",
      "offset": 1205.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with your own tempo even like do it",
      "offset": 1207.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "faster or slower just to make sure that",
      "offset": 1209.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we are on like the same page. And I",
      "offset": 1211.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "guess like the most interesting part",
      "offset": 1213.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "about like streaming and like LM in",
      "offset": 1215.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "generals we all have seen it with chat",
      "offset": 1217.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "GPT is that waiting for the whole",
      "offset": 1218.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "response is a very bad user experience",
      "offset": 1221.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "right like who wants to wait like 60",
      "offset": 1223.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "seconds two minutes for a response. So",
      "offset": 1224.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that's why um everyone now kind of uses",
      "offset": 1227.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "streaming and with the Gemini and the",
      "offset": 1229.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Gemini SDK, it's like super easy. So",
      "offset": 1232.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "instead of having just generate content,",
      "offset": 1234.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we have generate content stream. Same",
      "offset": 1236.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "input parameters except that we now get",
      "offset": 1238.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "um an iterator back from our um call",
      "offset": 1241.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "which we can loop over and we can like",
      "offset": 1244.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "print our uh junk or like stream it back",
      "offset": 1247.36,
      "duration": 9.319
    },
    {
      "lang": "en",
      "text": "to our user using uh the HTTP service.",
      "offset": 1250.88,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "Okay,",
      "offset": 1258.08,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "cool.",
      "offset": 1260.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "And then similar to other models, Gemini",
      "offset": 1264.08,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "is a chat model, right? And um maybe you",
      "offset": 1267.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "are familiar with the OpenAI SDK where",
      "offset": 1271.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you have the concept of messages where",
      "offset": 1273.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you have like a different like inputs",
      "offset": 1275.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "per user turn, assistant turn, user turn",
      "offset": 1277.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and it makes it very I would say is",
      "offset": 1280.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "still complex to manage yourself because",
      "offset": 1282.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you need to keep track of it. To make it",
      "offset": 1284.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "easier, we um added something which is",
      "offset": 1286.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "called a chats API um which basically",
      "offset": 1288.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "does all of the state management uh on",
      "offset": 1291.919,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the client but as part of the SDK. So",
      "offset": 1294.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "you can create a chat uh with your uh",
      "offset": 1297.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "model and then you can basically send",
      "offset": 1300.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "messages into the chat session and the",
      "offset": 1302.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "user um like in this case it's like we",
      "offset": 1305.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "are planning for a trip. Um we send the",
      "offset": 1308.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "message we get back the response but",
      "offset": 1311.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "also our chat session includes the user",
      "offset": 1314.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "prompt and the assistant message. So",
      "offset": 1317.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "instead of needing to create this object",
      "offset": 1320.24,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "of user terms and model turns, we can",
      "offset": 1323.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "directly uh continue with like sending",
      "offset": 1326.159,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "our next message um asking",
      "offset": 1329.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "um for some",
      "offset": 1333.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "um good food uh recommendations. And",
      "offset": 1336,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "since we are in like a conversational",
      "offset": 1339.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "setting, the model knows that we are um",
      "offset": 1341.2,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "or like it mentioned for us to go to",
      "offset": 1343.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "different European cities and based on",
      "offset": 1347.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like the next request, it uses like the",
      "offset": 1350.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "whole like conversation as history um to",
      "offset": 1352.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "get our response. Uh can like also",
      "offset": 1355.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "quickly",
      "offset": 1357.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "print the response here",
      "offset": 1359.52,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 1362.88,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "get some different examples. And then if",
      "offset": 1370.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of course if you need to store it to a",
      "offset": 1373.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "database on general um you we have a",
      "offset": 1375.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "nice get um history method available",
      "offset": 1378.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "which allows you to basically retrieve",
      "offset": 1380.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the complete current state and you can",
      "offset": 1383.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like store it or update it or whatever",
      "offset": 1385.6,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "you want to do. Yeah,",
      "offset": 1388,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "that's only a client abstraction. So the",
      "offset": 1394.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "back end receives the same request if",
      "offset": 1397.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you would send it like as a single",
      "offset": 1399.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "request uh with an array. It's only a",
      "offset": 1400.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "client abstraction to make it easier for",
      "offset": 1403.6,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "developers people to quickly build.",
      "offset": 1406.08,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 1413.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And then um similar to OpenI or to other",
      "offset": 1414.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "models, you can like give the model some",
      "offset": 1417.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "kind of system instruction to have it",
      "offset": 1420.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "behave differently, responded in a",
      "offset": 1422.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "different language, make sure it",
      "offset": 1424.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "respects like policies or guideline you",
      "offset": 1426.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "provide. Um this can be done through a",
      "offset": 1428.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "generations uh config. So we have",
      "offset": 1431.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "another argument now in our model call",
      "offset": 1434.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "um next to the model ID and the content",
      "offset": 1436.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we have a config that we can provide our",
      "offset": 1438.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "systems instructions and similar to the",
      "offset": 1441.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "systems instruction we can provide um",
      "offset": 1443.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "other um generation configurations. So",
      "offset": 1445.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "temperatures can be used to make the",
      "offset": 1448.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "generation more creative or more",
      "offset": 1451.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "deterministic. So if you for example",
      "offset": 1453.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "build a retrieve lock manage generation",
      "offset": 1455.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "where you want the model really mostly",
      "offset": 1457.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "trying to use what you provide as a",
      "offset": 1459.84,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "context there you would normally set the",
      "offset": 1461.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "temperature to a very low um value. If",
      "offset": 1463.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "you work on some content writing",
      "offset": 1465.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "marketing you would set the temperature",
      "offset": 1467.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to a very high value. We can control the",
      "offset": 1469.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "max output tokens to make sure that we",
      "offset": 1471.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "are not exceeding some budget some",
      "offset": 1473.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "length and top P and top K are also ways",
      "offset": 1475.039,
      "duration": 7.801
    },
    {
      "lang": "en",
      "text": "to make our generation more diverse.",
      "offset": 1477.919,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1484,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Um, so let me open. So here we have the",
      "offset": 1486,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "similar in the config we have the",
      "offset": 1488.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "thinking config and in the thinking cons",
      "offset": 1490.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you can basically set the thinking",
      "offset": 1493.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "budget or also include the thoughts uh",
      "offset": 1495.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "in your request.",
      "offset": 1497.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 1499.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "And then um I think what's very more",
      "offset": 1501.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "unique about uh what we can do with",
      "offset": 1504.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Gemini is that we have direct direct",
      "offset": 1506.559,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "support for files. So in this case I",
      "offset": 1509.76,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "download the adventures of Tom Sawyer a",
      "offset": 1513.679,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "book completely store it in a file and I",
      "offset": 1516.559,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "use the files uh API to upload the file",
      "offset": 1520.48,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "to a Google cloud uh storage bucket um",
      "offset": 1523.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "there um free for you so like if you uh",
      "offset": 1527.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "don't want to use your own corporate",
      "offset": 1530.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "bucket or whatever bucket with each AI",
      "offset": 1532,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "studio account basically there's your",
      "offset": 1534.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "personal u bucket which stores the file",
      "offset": 1536.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "for I think one day But you can control",
      "offset": 1539.279,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "the the time to live and um instead of",
      "offset": 1542,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "um needing to provide the whole file",
      "offset": 1545.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "with your request which can be very um",
      "offset": 1547.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "intensive, you can upload it and then",
      "offset": 1550.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "instead just provide the reference to",
      "offset": 1553.039,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "the file and what um the Gemini API does",
      "offset": 1556.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "behind the scenes. Basically it",
      "offset": 1559.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "downloads the file on the back end and",
      "offset": 1561.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "makes it available inside your prompt.",
      "offset": 1564.159,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "And similar here we can um we uploaded",
      "offset": 1566.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "our book we passed it into our contents",
      "offset": 1569.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "uh list here. So we don't no longer have",
      "offset": 1572.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like a single prompt. We have now an",
      "offset": 1576,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "array with our file and we ask it to",
      "offset": 1577.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "summarize the book and was also done",
      "offset": 1579.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "while I'm talking. And then we can also",
      "offset": 1582.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "see okay the token usage now we had for",
      "offset": 1584.08,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "was 100,000 tokens. So much bigger than",
      "offset": 1587.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "what we tested before. And using the",
      "offset": 1591.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "file API makes it very easy uh also to",
      "offset": 1593.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "work with with PDFs which we'll um see",
      "offset": 1596.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "in like the next chapter. And then um as",
      "offset": 1598.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "an exercise for you is um to combine a",
      "offset": 1601.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "bit all of those things. So how can I",
      "offset": 1604.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "use a book to use the chat session to",
      "offset": 1605.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "chat with our model to help me better",
      "offset": 1608.24,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "understand it? Yep.",
      "offset": 1610.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "It's not system.",
      "offset": 1615.12,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "Yep. So what you do is basically you",
      "offset": 1618.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "upload the file from your client to the",
      "offset": 1621.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "cloud into a bucket and when you send",
      "offset": 1624.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the request part of your I mean I can",
      "offset": 1627.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like show it. Um part of your request uh",
      "offset": 1629.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "would be only the reference to where the",
      "offset": 1632.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "file is stored and what the Gemini API",
      "offset": 1634.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "does behind the scenes is it loads the",
      "offset": 1637.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "file into where the request runs and",
      "offset": 1639.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "then puts it into the context. So you",
      "offset": 1642.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "can use now this file pointer for all",
      "offset": 1644.88,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "requests and you don't need to send it",
      "offset": 1648.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "every time.",
      "offset": 1650.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "And here we also have like the URI. So",
      "offset": 1652.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that's basically where",
      "offset": 1655.919,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "um our file um is stored or can be",
      "offset": 1658.72,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "accessed as well. Yeah.",
      "offset": 1661.76,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "So is it only available to the chat?",
      "offset": 1671.76,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "to itself. It's only available to your",
      "offset": 1674.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "user. So when you send a request, you",
      "offset": 1677.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "send an API key and this API key is",
      "offset": 1679.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "basically used to get the file. So",
      "offset": 1682.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "nobody else can access the file.",
      "offset": 1685.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1688.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Okay. Cool.",
      "offset": 1690.48,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1692.72,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So if you would use a PDF which has like",
      "offset": 1704,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "more than a million tokens basically",
      "offset": 1706.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what would happen? You would receive an",
      "offset": 1707.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "error most likely uh with like saying",
      "offset": 1710.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "that uh the file is like too big in",
      "offset": 1713.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "terms of like a context. What you can do",
      "offset": 1715.679,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "is basically you can use the the file uh",
      "offset": 1718.159,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "the file to count the token. So we have",
      "offset": 1721.2,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "the file now here and we can client uh",
      "offset": 1724.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "models",
      "offset": 1728.96,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "count tokens",
      "offset": 1730.88,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "you would need to chunk it then so if it",
      "offset": 1737.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the file doesn't fit it doesn't fit and",
      "offset": 1739.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you would need to like think about okay",
      "offset": 1741.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "can I chunk it can I summarize it can I",
      "offset": 1743.039,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "maybe do use other techniques to first",
      "offset": 1746,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "extract the important information and",
      "offset": 1748.799,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "once I have context which is um smaller",
      "offset": 1750.96,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "than the maximum context of my model,",
      "offset": 1754.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you can provide it again.",
      "offset": 1757.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1759.84,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "No,",
      "offset": 1762.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "files upload.",
      "offset": 1769.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "No, no. So, it's like but it will be",
      "offset": 1772,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "deleted. So, don't expect the file you",
      "offset": 1775.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "upload now to be there an hour or a day.",
      "offset": 1778.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Um but you can like use the same concept",
      "offset": 1781.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "with uh vertex in your own bucket where",
      "offset": 1783.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you have like more control over it where",
      "offset": 1786.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you can say okay maybe I want to upload",
      "offset": 1788.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it using a different API call or already",
      "offset": 1790.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "have it available that also works.",
      "offset": 1792.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So there's an entry point saying use",
      "offset": 1796.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "this storage bucket with Vertex AI. Yes.",
      "offset": 1798.24,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "Yes. It's roughly the same but you need",
      "offset": 1802.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to set up the client differently.",
      "offset": 1805.679,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "Y",
      "offset": 1808.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1811.6,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah. Good question. So I mean we can",
      "offset": 1817.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "maybe directly jump into like the the",
      "offset": 1818.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the this the PDF section. So um continue",
      "offset": 1820.559,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "with section one or jump to section two",
      "offset": 1823.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "or three directly what I will do is like",
      "offset": 1826.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "in the section two which is all about",
      "offset": 1828.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "multimodality meaning we will cover uh",
      "offset": 1831.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "visual understanding audio understanding",
      "offset": 1834.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "videos and document processing and",
      "offset": 1836.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's where I will like jump to. Um and",
      "offset": 1838.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "part of the",
      "offset": 1840.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1843.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "okay being connected",
      "offset": 1846.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "part of the uh working with PDFs",
      "offset": 1848.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "basically is so similar to what we have",
      "offset": 1851.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "seen a minute ago we have a PDF in this",
      "offset": 1853.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "case it's basically an invoice from a",
      "offset": 1855.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "supermarket um I upload it and I ask the",
      "offset": 1856.88,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "model like what's the the total amount",
      "offset": 1861.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "uh we can run it and what happens behind",
      "offset": 1864.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the scenes I can show oh",
      "offset": 1866.399,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "it's not here. Wait one second.",
      "offset": 1869.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Okay, we don't have the file here. I'll",
      "offset": 1873.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "upload it quickly. But what happens um",
      "offset": 1874.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "behind the scenes is we run OCR on your",
      "offset": 1878,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "on your PDF and provide the PDF as",
      "offset": 1881.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "image. Um so you don't need to do it",
      "offset": 1884.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "manually. So there's no like hey it's a",
      "offset": 1887.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "PDF let's convert it to an image and",
      "offset": 1890,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then run OCR and then I provide the",
      "offset": 1892.48,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "image and the OCR that's not needed. Um",
      "offset": 1894.24,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "we are doing it for you.",
      "offset": 1898.799,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 1902.08,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 1907.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yep. Yep. Yep.",
      "offset": 1911.2,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Yeah. So",
      "offset": 1921.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "OR like the image understanding is not",
      "offset": 1923.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "perfect yet, right? If we reach a point",
      "offset": 1925.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "where the model understands it like in",
      "offset": 1928,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the same way as without the text, then I",
      "offset": 1931.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "guess there's no point. But like based",
      "offset": 1934.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "on like what we have seen and also what",
      "offset": 1936,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the industry does is you receive better",
      "offset": 1938.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "results when you provide the OCR plus",
      "offset": 1940.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the image. Um yeah, that's that's",
      "offset": 1943.039,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "basically it.",
      "offset": 1946,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Y",
      "offset": 1949.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the PDF itself,",
      "offset": 1951.2,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "do you actually look at it as an image?",
      "offset": 1953.6,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1957.919,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "No, I I think I I mean I don't know",
      "offset": 1965.519,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "exactly but I think it's just basic OCR,",
      "offset": 1967.44,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "nothing special, no magic and then a",
      "offset": 1971.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "screenshot of the PDF.",
      "offset": 1973.519,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Yeah. So it's both. Okay. Can try again.",
      "offset": 1976.64,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "And we have now the PDF available in",
      "offset": 1979.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "case um the workshop has multiple",
      "offset": 1981.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "sections with files which are being part",
      "offset": 1985.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of the repository. So if you run into",
      "offset": 1987.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like a similar issue especially for the",
      "offset": 1989.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "image understanding part or the audio",
      "offset": 1991.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "understanding part uh and you use collab",
      "offset": 1994,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you might need to download the files",
      "offset": 1996.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "manually and then upload it. But in our",
      "offset": 1998.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "case so we have now our invoice. I think",
      "offset": 2000.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we can like",
      "offset": 2002.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "quickly",
      "offset": 2005.279,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "show it.",
      "offset": 2006.96,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "So I was shopping in Germany. We have a",
      "offset": 2012.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "co supermarket called Rebe and I",
      "offset": 2014.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "basically bought some butter um some",
      "offset": 2017.2,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "bread like some sweet potatoes and uh we",
      "offset": 2020.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "prompted it and asked okay what's the",
      "offset": 2023.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "total amount? We can see here the total",
      "offset": 2025.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "amount is like 2020. Now let's see if we",
      "offset": 2027.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "got it correctly. And we got it",
      "offset": 2029.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "correctly. And it also correctly",
      "offset": 2031.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "extracted it in German even if I",
      "offset": 2033.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "prompted it in English which I think is",
      "offset": 2035.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "like pretty cool. Okay,",
      "offset": 2037.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "let's start with the image understanding",
      "offset": 2040.799,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "part.",
      "offset": 2043.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Yeah, please.",
      "offset": 2045.519,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2048.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I'm not exactly sure what happens. I",
      "offset": 2055.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "only know that by defining the thinking",
      "offset": 2057.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "budget, you can limit how many how many",
      "offset": 2060.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "tokens will be used or generated as a",
      "offset": 2063.2,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "maximum and very similar to what OpenAI",
      "offset": 2065.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "has with like low medium high effort. We",
      "offset": 2068.879,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "have a bit more granular control. So we",
      "offset": 2071.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "could technically do the same say like",
      "offset": 2074.639,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "low would be like a thousand token",
      "offset": 2077.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thinking, medium would be maybe like",
      "offset": 2078.879,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "12,000 token thinking and high would be",
      "offset": 2081.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "24,000 token thinking. uh and it would",
      "offset": 2083.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "then use like at a maximum those tokens",
      "offset": 2086.159,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to before generating your response. But",
      "offset": 2088.079,
      "duration": 9.32
    },
    {
      "lang": "en",
      "text": "what exactly happens I can tell you.",
      "offset": 2091.919,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Yeah. So without thinking what we have",
      "offset": 2103.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "seen especially on more like math type",
      "offset": 2105.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of question where the model benefits",
      "offset": 2107.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "from like the reasoning uh the",
      "offset": 2109.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "performance is a bit worse but for like",
      "offset": 2111.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "general everyday use especially image",
      "offset": 2114,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "understanding or like OCR you can like",
      "offset": 2116.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "easily run it without it. The truth of",
      "offset": 2119.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it would be you need to try and I think",
      "offset": 2121.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the the real benefit here is that you",
      "offset": 2124.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have those granular control. So you can",
      "offset": 2126.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "run evaluation of thinking budget zero,",
      "offset": 2128.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "1,000, 2,000, 4,000 and see how it",
      "offset": 2131.359,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "impacts your evaluation and then you can",
      "offset": 2134.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like calculate for yourself. Okay, how",
      "offset": 2137.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "much am I able or like what's my my",
      "offset": 2139.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "maximum cost of it and like what's the",
      "offset": 2142.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the accuracy I need to reach.",
      "offset": 2144.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Okay,",
      "offset": 2147.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "more questions or should we continue?",
      "offset": 2149.44,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "There's a question. Yeah.",
      "offset": 2152.079,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "No, it's like uh",
      "offset": 2156.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "there's a documentation for it, but it's",
      "offset": 2158.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like uh JSON PDFs, all different image",
      "offset": 2160.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "types, all different video types, all",
      "offset": 2164.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "different audio types. So, all of the",
      "offset": 2166.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "multimodal um features we support. Um if",
      "offset": 2168.32,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "it gets a bit more specific with like",
      "offset": 2172.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "JSX file and view files for like web",
      "offset": 2175.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "development, we are working on it. But",
      "offset": 2178.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it might I mean you will see an arrow",
      "offset": 2180.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and the easiest way is to just replace",
      "offset": 2182.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it with a doc txt.",
      "offset": 2184.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "No, I would say like those are like you",
      "offset": 2187.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "would need to like use like mark it down",
      "offset": 2190.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "or like another library to convert it",
      "offset": 2192.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and then or like copy paste the input",
      "offset": 2193.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 2196.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 2198.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So I'm not exactly sure what the",
      "offset": 2215.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "researchers did. The only thing I know",
      "offset": 2216.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is that we get better performance when",
      "offset": 2218.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "you provide the image plus the OCR.",
      "offset": 2220.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "So I guess there's a benefit of having",
      "offset": 2224.079,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "both.",
      "offset": 2226.16,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "saw.",
      "offset": 2234.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yes, kind of. So, the UI or the AI",
      "offset": 2241.44,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "studio of course doesn't use Python. Um,",
      "offset": 2244.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "but it calls the same APIs behind the",
      "offset": 2247.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "scenes. So, the API behind the file",
      "offset": 2249.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "upload one is the same API we call from",
      "offset": 2251.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "AI studio. We call the same exact model.",
      "offset": 2253.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "Both have the same uh exact parameters.",
      "offset": 2257.119,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "So you should be easily what you test",
      "offset": 2259.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "and experiment in AI studio. Can you",
      "offset": 2262.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "convert into code and run it locally?",
      "offset": 2265.76,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "There's also this um get code um button.",
      "offset": 2267.76,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "So if you are AI studio and I mean we",
      "offset": 2272.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "can quickly try uploading our invoice",
      "offset": 2276.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "again",
      "offset": 2278.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 2280.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "acknowledge ship",
      "offset": 2282.56,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "and use our prompt.",
      "offset": 2284.8,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "So",
      "offset": 2289.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 2293.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and now we run basically the same",
      "offset": 2295.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "request and we can now we also have this",
      "offset": 2297.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "um it's a bit hidden. It's like this",
      "offset": 2300.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "code um button at the top which where",
      "offset": 2302.64,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "you can get the exact same code",
      "offset": 2305.599,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "basically takes a few seconds as the",
      "offset": 2309.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Wi-Fi is super bad but here you get like",
      "offset": 2311.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the exact same Python code where you",
      "offset": 2313.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "create your client we have our model in",
      "offset": 2315.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "this case as we uploaded it manually we",
      "offset": 2317.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "provide the document not as a file URI",
      "offset": 2320.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we provide it directly as B 64 we have",
      "offset": 2322.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "our prompt uh we have the model request",
      "offset": 2325.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "already since we generated it and then",
      "offset": 2327.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you can continue.",
      "offset": 2329.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Okay, cool.",
      "offset": 2331.68,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2334.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah. So, I mean we can quickly so try",
      "offset": 2340.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it. So, we are here. We have our PDF. I",
      "offset": 2343.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "think I guess it's very interesting to",
      "offset": 2345.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "know like how many um tokens will we",
      "offset": 2346.56,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "use. So, um let's quickly",
      "offset": 2350.079,
      "duration": 9.76
    },
    {
      "lang": "en",
      "text": "count tokens. So,",
      "offset": 2355.68,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "If we go here,",
      "offset": 2359.839,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "we have our count tokens and now we use",
      "offset": 2365.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the same contents. We have the same",
      "offset": 2367.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "model ID, we have our prompt, then we",
      "offset": 2369.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "have our PDF",
      "offset": 2371.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and let's print our token count. And",
      "offset": 2373.599,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "alternatively what we also have done so",
      "offset": 2377.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "if you use the",
      "offset": 2380.079,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "uh response so you run a request already",
      "offset": 2384.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you should have access to the respage",
      "offset": 2386.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "metadata. Yes. And so we have our",
      "offset": 2388.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "account token. So our PDF here is",
      "offset": 2391.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "converted into like um roughly 500 uh",
      "offset": 2393.52,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "tokens. Um and the the prompt we have is",
      "offset": 2396.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "like around 20. And if we compare it to",
      "offset": 2400.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like the request we run, we see okay, we",
      "offset": 2402.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have the same exact amount of prompt",
      "offset": 2404.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tokens and we have prompt details. We",
      "offset": 2406.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "have our output tokens",
      "offset": 2408.88,
      "duration": 9.199
    },
    {
      "lang": "en",
      "text": "for tokens of 42 and candidate tokens of",
      "offset": 2413.2,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "78.",
      "offset": 2418.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Okay,",
      "offset": 2420.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "more questions or Yeah. Okay.",
      "offset": 2421.76,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "560.",
      "offset": 2424.88,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I think we don't charge for the OCR. I",
      "offset": 2428.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "think one image is roughly 500 images.",
      "offset": 2431.68,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "Uh like 500 tokens. Um",
      "offset": 2434.16,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "not for you. No.",
      "offset": 2440.079,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "Okay. Yeah.",
      "offset": 2444.88,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "So",
      "offset": 2455.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the PDF is converted into an image and",
      "offset": 2457.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we provide the image and if the image or",
      "offset": 2460.079,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "like the PDF has tables, visuals, mind",
      "offset": 2462.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "maps, uh the model is trained on similar",
      "offset": 2466.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "data. So it will definitely understand",
      "offset": 2468.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "parts of it. I mean we can we can try",
      "offset": 2470.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "it. I mean, maybe we can like search for",
      "offset": 2473.119,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "some mind map and ask it something.",
      "offset": 2476.079,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "Um, you maybe you can start thinking",
      "offset": 2481.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about a prompt",
      "offset": 2483.2,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "while I'm searching for a mind map",
      "offset": 2485.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "if Wi-Fi allows us.",
      "offset": 2490.319,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 2493.68,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay. We have our mind map. I guess it's",
      "offset": 2500.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "just a mind map on how to do mind maps.",
      "offset": 2504.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Um, any any idea what you would like to",
      "offset": 2506.96,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "know?",
      "offset": 2509.68,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Sorry.",
      "offset": 2513.68,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Like this. Okay.",
      "offset": 2522.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "I mean I'm opening it but I think on the",
      "offset": 2531.28,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "small scale it looks correct with how to",
      "offset": 2535.04,
      "duration": 7.799
    },
    {
      "lang": "en",
      "text": "mind map",
      "offset": 2539.52,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "answers perfect.",
      "offset": 2545.119,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "of the table.",
      "offset": 2553.44,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "Yeah. So where we have seen the most",
      "offset": 2580.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "success is that when you like separate a",
      "offset": 2582.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "bit. So there are already like very good",
      "offset": 2584.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "existing methods which allow you to",
      "offset": 2587.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "extract tables or other visuals from",
      "offset": 2588.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "documents and when you then like work",
      "offset": 2590.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "with those images and tables directly",
      "offset": 2593.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and not like you the way how we work",
      "offset": 2596.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "changes from like previously we provided",
      "offset": 2599.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "a table run OCR then ask our question",
      "offset": 2602,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "now we directly ask the question based",
      "offset": 2604.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "on the table as the models get so good",
      "offset": 2607.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "in like the multimodal understanding",
      "offset": 2609.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that it can combine like the different",
      "offset": 2610.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "aspects I mean we can try it maybe as a",
      "offset": 2612.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "good example as well. Um maybe we find",
      "offset": 2615.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "some nice I don't know",
      "offset": 2618.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "invoice image or something and then we",
      "offset": 2621.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "can like ask it maybe to add something",
      "offset": 2624.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "or to combine it which would be very",
      "offset": 2626.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "hard I guess for a normal model but in",
      "offset": 2627.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "general like especially Gemini is so",
      "offset": 2630.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "good with like the multimodal",
      "offset": 2633.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "understanding it also videos is like",
      "offset": 2635.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that's like my most new favorite thing",
      "offset": 2638.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "is like take a YouTube video which is um",
      "offset": 2640.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "below one hour put put it into AI studio",
      "offset": 2643.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and like have it summarize it or if you",
      "offset": 2645.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have like any specific question like it",
      "offset": 2647.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's so much faster than like sitting",
      "offset": 2649.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "there even like watching it in like two",
      "offset": 2651.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "speeds. You get your response in like 80",
      "offset": 2652.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "seconds or something and you can like",
      "offset": 2655.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "even like ask it to extract specific",
      "offset": 2657.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "timestamps on when somebody something",
      "offset": 2659.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "was set or to help you like section it",
      "offset": 2662.079,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "very easily.",
      "offset": 2664.64,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Sorry, can you speak a little bit",
      "offset": 2672.16,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "louder?",
      "offset": 2673.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I don't think so. So, AI studio is like",
      "offset": 2679.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "very developer centric and we don't want",
      "offset": 2682.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to do too much black magic. There are",
      "offset": 2685.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "safety filters and control which you can",
      "offset": 2688.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "configure in the SDK or in AI studio and",
      "offset": 2690.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "in studio is under advanced settings. I",
      "offset": 2693.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "have safety settings. I have all of them",
      "offset": 2696.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "off. But if you like want to filter on",
      "offset": 2698.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "very explicit content or hateful",
      "offset": 2702.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "content, we run some classifications",
      "offset": 2704.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "basically before and after to make sure",
      "offset": 2708.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that you are not creating uh for your",
      "offset": 2709.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "users harmful content.",
      "offset": 2712.4,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "Okay, I have our invoice image.",
      "offset": 2715.04,
      "duration": 8.6
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 2720.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay. What should we ask?",
      "offset": 2734.319,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "How much it would cost if we subtract",
      "offset": 2739.359,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "the pedal? Maybe.",
      "offset": 2741.92,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2746.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "the performance.",
      "offset": 2772.079,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "So I sadly don't have an answer for",
      "offset": 2784.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this. I think you can always think a bit",
      "offset": 2786.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "about like would we as a humans have",
      "offset": 2788.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "struggled to understand those PDFs if",
      "offset": 2791.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "they are like switch switched up. If yes",
      "offset": 2793.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then most likely the model will do as",
      "offset": 2796.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "well. I think one very nice part about",
      "offset": 2798,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Gemini AI studio and the Gemini API is",
      "offset": 2800.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you can get started very quickly like",
      "offset": 2804.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "all of us kind of set up within like 20",
      "offset": 2806.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to 30 minutes a free account uh with",
      "offset": 2808.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "access to Gemini 2.5 flash via API 2.5",
      "offset": 2811.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Pro in the UI. So the best thing always",
      "offset": 2815.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is like to test and to explore and",
      "offset": 2818,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "evaluate and even if you need to run",
      "offset": 2819.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like a thousand PDFs it's not very cost",
      "offset": 2822.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "or like expensive anymore. So really",
      "offset": 2824.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "best thing is to run your own evals to",
      "offset": 2827.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "get some more than like I try five PDFs",
      "offset": 2829.92,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "in the UI really look into it and if you",
      "offset": 2833.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "have like any questions or problems like",
      "offset": 2837.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "best way is to reach out to us. We have",
      "offset": 2839.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "teams helping and building with",
      "offset": 2841.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "customers and then we can like iterate",
      "offset": 2842.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "on it together. Going back to your PDF",
      "offset": 2845.44,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "any question any prompt idea?",
      "offset": 2848.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Yeah, I know you explained about",
      "offset": 2852.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "security but there are mandated",
      "offset": 2855.52,
      "duration": 8.799
    },
    {
      "lang": "en",
      "text": "security by law which are fire",
      "offset": 2859.44,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "or benchmarking automations or",
      "offset": 2864.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "guardrails",
      "offset": 2867.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "posture management",
      "offset": 2870.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "mandated by CISA.",
      "offset": 2873.04,
      "duration": 7.799
    },
    {
      "lang": "en",
      "text": "So when I implement those rules",
      "offset": 2876.079,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "for uh either defense or financial",
      "offset": 2881.44,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "applications,",
      "offset": 2886.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "how do you just set I know that Google",
      "offset": 2888.319,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "is pretty strong in setting up those",
      "offset": 2891.359,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "security measures",
      "offset": 2896.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "from a different perspective too. How do",
      "offset": 2898.48,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "you integrate this app and the other",
      "offset": 2900.64,
      "duration": 8.76
    },
    {
      "lang": "en",
      "text": "security measures.",
      "offset": 2905.839,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "So I guess for those type of",
      "offset": 2909.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "environments uh where you have a lot of",
      "offset": 2911.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "like compliance regulations best is to",
      "offset": 2913.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "work with Google cloud. So everything we",
      "offset": 2917.119,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "do in AI studio is also somewhat similar",
      "offset": 2919.92,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "available in Google cloud in Vertex AI",
      "offset": 2923.599,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "and Vert.ex AI provides more features",
      "offset": 2926.319,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "for those kind of use cases. they I'm",
      "offset": 2930,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "not exactly sure but um they definitely",
      "offset": 2932.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "have more like information they can",
      "offset": 2935.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "provide on how to handle all of those",
      "offset": 2938.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "things and those guardrails with Gemini",
      "offset": 2940.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so that",
      "offset": 2943.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "you're suggesting to",
      "offset": 2945.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "go",
      "offset": 2948.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "global",
      "offset": 2950.319,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "Google GCP environment when I go global",
      "offset": 2952.72,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "on GCP environment",
      "offset": 2957.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "then there are c",
      "offset": 2960,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "performance",
      "offset": 2962.72,
      "duration": 9.44
    },
    {
      "lang": "en",
      "text": "and uh cost being here such as",
      "offset": 2964.559,
      "duration": 11.921
    },
    {
      "lang": "en",
      "text": "so there's a guard for bas and there's a",
      "offset": 2972.16,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "guard for organization based and there's",
      "offset": 2976.48,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "a based on the industry industry based",
      "offset": 2980.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2984.24,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "we have to split those platforms into",
      "offset": 2985.839,
      "duration": 9.561
    },
    {
      "lang": "en",
      "text": "from most grand level.",
      "offset": 2990.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "So that's",
      "offset": 2996,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "when I do it globally at GCP levels",
      "offset": 2998.319,
      "duration": 8.961
    },
    {
      "lang": "en",
      "text": "more expensive and as well as can be a",
      "offset": 3004.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "performance issue. Yeah. So I know that",
      "offset": 3007.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there are regional endpoints for Gemini",
      "offset": 3009.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "and Vort.ex AI as well. And also at",
      "offset": 3011.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Cloud Next, um they announced a new",
      "offset": 3014.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Gemini",
      "offset": 3017.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "on device kind of thing where big",
      "offset": 3019.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "companies can buy basically a huge box",
      "offset": 3022.48,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "where Gemini is pre-installed and it",
      "offset": 3025.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "gets delivered to your environment. I'm",
      "offset": 3028.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "not exactly sure about the details.",
      "offset": 3030.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Easiest is like to to do a quick Google",
      "offset": 3032.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "search and and look for it. But those is",
      "offset": 3034.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "exactly where where Vortex provides you",
      "offset": 3037.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "more features and support than AI",
      "offset": 3039.52,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "studio. Yeah, thanks. One of those last",
      "offset": 3042.16,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "uh 2025 conference uh someone from",
      "offset": 3046.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Google",
      "offset": 3049.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "also represented and that's a good",
      "offset": 3051.359,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "platform but I didn't know how",
      "offset": 3054.4,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "to this is granular level. Yeah. How do",
      "offset": 3058.319,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "you most granular level to the global",
      "offset": 3062,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "level?",
      "offset": 3065.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Okay, maybe back to your PDF question uh",
      "offset": 3068.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "regarding uh what to ask. So instead of",
      "offset": 3071.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like asking what the the total sum was,",
      "offset": 3074.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "I asked it to to sum up the unit prices",
      "offset": 3076.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "which worked very nicely. Of course,",
      "offset": 3079.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it's a a very well formatted PDF in this",
      "offset": 3081.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "case, but the the image understanding is",
      "offset": 3084.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like very good. And the new way of how",
      "offset": 3086.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we should think about it is like I",
      "offset": 3089.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "should ask the question directly based",
      "offset": 3090.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "on the image before doing too much of",
      "offset": 3092.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "processing we we have been doing in the",
      "offset": 3095.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "past. Cool. Okay. We are now at like",
      "offset": 3097.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "half time almost. I guess it's time we",
      "offset": 3100.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "move maybe a bit away from all of the",
      "offset": 3102.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "multimodality part into more about the I",
      "offset": 3105.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "guess agendic um parts which are I would",
      "offset": 3108.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "say definitely more interesting at least",
      "offset": 3112.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "to me um especially if you combine in",
      "offset": 3113.76,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "with like the multimodality parts. So",
      "offset": 3116.72,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3120.88,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "okay um so part three is um all about",
      "offset": 3123.359,
      "duration": 8.801
    },
    {
      "lang": "en",
      "text": "structured output and function calling.",
      "offset": 3129.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Do you know what structured outputs and",
      "offset": 3132.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "function calling is and like how it",
      "offset": 3134.319,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "roughly works? Any hand signals? Yes or",
      "offset": 3136.079,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "everyone?",
      "offset": 3140.559,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "Okay, not bad.",
      "offset": 3142.64,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 3146.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "okay so",
      "offset": 3151.599,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "the part three um goes continues with",
      "offset": 3154.16,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "PDFs um as they are kind of very",
      "offset": 3158.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "interesting and also um we want to do",
      "offset": 3162.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "structured outputs. So structured",
      "offset": 3164.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "outputs is for us a way to create more",
      "offset": 3166.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "structured data structures from text. So",
      "offset": 3169.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "which we can use to work way more easily",
      "offset": 3171.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "afterwards, right? And at the end we",
      "offset": 3174.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "prefer structured output much better or",
      "offset": 3176.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "much more because we can integrate it",
      "offset": 3178.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "into other APIs. We can connect APIs and",
      "offset": 3180.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "Gemini supports or the SDK supports",
      "offset": 3184,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "pyantic. So pyantic is a very nice",
      "offset": 3186.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Python library which lets you create",
      "offset": 3187.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "those data structures and also we can",
      "offset": 3189.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "create n nested data structures. So here",
      "offset": 3192.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "we have a recipe with a name ingredients",
      "offset": 3194.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "which is a list of strings and then we",
      "offset": 3197.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "have a recipe list which is basically a",
      "offset": 3200.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "list of our recipes and we can provide",
      "offset": 3202.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it in our configuration. So similar to",
      "offset": 3205.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the generation arguments or our thinking",
      "offset": 3207.28,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "budget, we have um a response type and a",
      "offset": 3210.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "response schema we can provide. Here we",
      "offset": 3213.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "ask it, okay, can it generate two",
      "offset": 3215.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "popular cookie recipes for us? And we",
      "offset": 3217.44,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "basically force it to use our um",
      "offset": 3220.88,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "structure and I already like nicely um",
      "offset": 3224.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "printed it here. If we look at the the",
      "offset": 3228.559,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "raw response um of our model, we get",
      "offset": 3231.119,
      "duration": 10.321
    },
    {
      "lang": "en",
      "text": "back a a JSON uh with um all of the",
      "offset": 3235.68,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "different input fields. And there's also",
      "offset": 3241.44,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "a nice pass method um which allows us to",
      "offset": 3243.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "convert it back into our benic uh",
      "offset": 3247.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "schema. And then we can access all of",
      "offset": 3249.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 3252.24,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "the data points. And as we had our",
      "offset": 3253.839,
      "duration": 9.441
    },
    {
      "lang": "en",
      "text": "invoice uh we can now maybe like like I",
      "offset": 3258.8,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "complete the exercise with you. Um what",
      "offset": 3263.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "if we like so we ask about the total",
      "offset": 3266.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "amount right but when working with PDFs",
      "offset": 3268.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "normally we want to have structured data",
      "offset": 3270.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "as a result right text is not very",
      "offset": 3273.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "helpful for us when we want to put it",
      "offset": 3275.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "into a database or want to work with it.",
      "offset": 3277.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "We really need those data schemas and",
      "offset": 3279.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "what is very nice about Gemini is that",
      "offset": 3282.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we can basically combine both of it. So",
      "offset": 3284,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "we use our um structured output method",
      "offset": 3287.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "with our multimodal capabilities for",
      "offset": 3290.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "files and we can",
      "offset": 3292.72,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "provide our um",
      "offset": 3296.16,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "file.",
      "offset": 3299.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Oops.",
      "offset": 3303.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 3334.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Oh,",
      "offset": 3355.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we need our invoice. So maybe it's a",
      "offset": 3358.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "good example. So I didn't change the",
      "offset": 3361.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "recipe uh from our like data structure",
      "offset": 3363.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "what which we want to create and we ask",
      "offset": 3366.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it to extract the information from our",
      "offset": 3368.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "PDF. Our PDF is an invoice from a",
      "offset": 3370.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "supermarket. So it doesn't have a recipe",
      "offset": 3373.119,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "name ingredients. So Gemini did not",
      "offset": 3376.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "generate or hallucinated something. So",
      "offset": 3380.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we get back a empty recipe list. So if",
      "offset": 3382.64,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "we now change it to our invoice data um",
      "offset": 3385.52,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "we should hopefully",
      "offset": 3390.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "then see the correct extracted",
      "offset": 3393.92,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "information.",
      "offset": 3396.559,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 3404.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So, we extracted the date, all of the",
      "offset": 3405.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "items we bought, and all of the",
      "offset": 3409.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "different prices. And with that data,",
      "offset": 3411.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "now it makes it much more easier for us",
      "offset": 3414.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to work with. Right? If I have some kind",
      "offset": 3416.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "of automated system where I need to like",
      "offset": 3417.92,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "take in invoices, any kind of like PDF",
      "offset": 3420.799,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "document, I can now provide uh a",
      "offset": 3424.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "structured",
      "offset": 3427.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to what information I want to extract.",
      "offset": 3429.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And Gemini does basically all of the",
      "offset": 3431.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "matching for us, which is like super",
      "offset": 3433.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "nice. And function calling basically is",
      "offset": 3434.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "the same idea, but instead of um having",
      "offset": 3437.28,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "like a data structure, our output is a",
      "offset": 3440.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "name and the argument. So similar to",
      "offset": 3444.079,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "what we have um uh we we create this",
      "offset": 3446.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "case it's like a function declaration.",
      "offset": 3450.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "It's a structure of like how a function",
      "offset": 3452,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "signature is done. So we have a weather",
      "offset": 3454.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "function which has a name, a description",
      "offset": 3456.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and like the properties uh which we need",
      "offset": 3458.4,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "to provide. Um there's the same function",
      "offset": 3460.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "but just as Python code and with",
      "offset": 3463.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "function calling we provide the function",
      "offset": 3466.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "declaration and our prompt and then the",
      "offset": 3468.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "model generates aruct a structured",
      "offset": 3470.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "output which has the function name it",
      "offset": 3472.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "wants to call and the um input and",
      "offset": 3474,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "environment or the input arguments for",
      "offset": 3476.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "how to call it. So in our case, we have",
      "offset": 3478.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a weather function. We only have one",
      "offset": 3482.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "location. Um we provide it uh similar to",
      "offset": 3483.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "all of the other configurations in our",
      "offset": 3487.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "configuration uh argument. This time we",
      "offset": 3489.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "have tools and we want to know what the",
      "offset": 3492.079,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "weather is in Tokyo. And obviously",
      "offset": 3494.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "what's the weather fits into the",
      "offset": 3497.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "description of our function as it helps",
      "offset": 3499.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "us retrieve the current weather. So if",
      "offset": 3502.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "we run it, the model uh instead of",
      "offset": 3504.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "generating a nice response wants to call",
      "offset": 3507.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the get weather method and with the",
      "offset": 3510.4,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "location Tokyo. If I change the prompt",
      "offset": 3513.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "to hello.",
      "offset": 3516.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Oh,",
      "offset": 3519.68,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "makes sense. Uh one second.",
      "offset": 3521.599,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "So if I change to hello we don't have a",
      "offset": 3531.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "function call right like the model",
      "offset": 3534.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "correctly understands hey that's just a",
      "offset": 3536,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "greeting let's response and like how can",
      "offset": 3538,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "I help you so but we want to call a",
      "offset": 3540.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "function so",
      "offset": 3543.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we have what's the weather is in Tokyo",
      "offset": 3545.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and then the next step is for you as a",
      "offset": 3548.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "developer uh to call the function right",
      "offset": 3550.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the models cannot call or invoke",
      "offset": 3552.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "directly the function what you would",
      "offset": 3553.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "normally then have in your um code or in",
      "offset": 3555.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "your applications a way to identify okay",
      "offset": 3558,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "which function is false could be a",
      "offset": 3560,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "simple switch statement um to check okay",
      "offset": 3561.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "what's the name and if you get a name",
      "offset": 3564.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "call the method with the provided",
      "offset": 3566.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "argument and then what you do is the",
      "offset": 3568.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "output of your function is the next user",
      "offset": 3571.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "input uh so the model generates this",
      "offset": 3573.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "name and arguments object and we provide",
      "offset": 3576.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it as a user the output and in our case",
      "offset": 3579.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "it's the result and if we look at the",
      "offset": 3582.16,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "weather method we have it's basically",
      "offset": 3585.44,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "some dummy data about the temperature,",
      "offset": 3589.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the condition and where it is and then",
      "offset": 3591.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the the model generates a very nice",
      "offset": 3595.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "response. So we have user input, model",
      "offset": 3597.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "has a structured output, user provides",
      "offset": 3599.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the a structured response and then the",
      "offset": 3601.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "model generates a very nice",
      "offset": 3603.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "user-friendly response. So we call our",
      "offset": 3604.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "function, the function returns the",
      "offset": 3607.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "weather and then the model generates a",
      "offset": 3608.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "very nice response which is the weather",
      "offset": 3610.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "in Tokyo is sunny with a temperature of",
      "offset": 3612.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "22 degrees CC and it feels like 24",
      "offset": 3614.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "degrees CC. So you can think about it.",
      "offset": 3616.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Okay, that's how I can integrate tools",
      "offset": 3619.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "or make it or convert my LLM into an",
      "offset": 3621.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "agent more or less or a way to call",
      "offset": 3624.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "something. Um, the get a weather method",
      "offset": 3627.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "can be anything. It can be a database",
      "offset": 3629.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "call. It can be a real API call. It can",
      "offset": 3631.839,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "be, I don't know, reading emails,",
      "offset": 3636.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "sending emails, all of the things we",
      "offset": 3638.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "currently see with all of the MCP hype",
      "offset": 3640.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "basically going on. And MCP servers have",
      "offset": 3642.48,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "tools as well. And those tools of MCP",
      "offset": 3645.68,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "servers basically expose the same um",
      "offset": 3648.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "declarations. So an MCP servers has the",
      "offset": 3652.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "tools def defined get weather for",
      "offset": 3654.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "example if it is in weather MCP server",
      "offset": 3657.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "and it has an endpoint or a method which",
      "offset": 3659.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you can call which is list tools and",
      "offset": 3662.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "this list tools method would then return",
      "offset": 3664.559,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "the schemas of our functions which look",
      "offset": 3667.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "very similar to what we have created",
      "offset": 3671.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "here. And then what you do on your LLM",
      "offset": 3672.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "side or client side is you take those",
      "offset": 3675.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "schemas from the MCP server provide it",
      "offset": 3678,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "into your LLM call and then the LLM",
      "offset": 3680.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "generates depending on the context uh",
      "offset": 3682.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the output as well which is structured",
      "offset": 3685.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and then instead of calling the method",
      "offset": 3687.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "on the client as we did with our get",
      "offset": 3690.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "weather you would use the MCP client",
      "offset": 3692.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "again and then call the remote tool. So",
      "offset": 3694.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "very similar to what we have done here",
      "offset": 3697.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "on the client side manually but more",
      "offset": 3699.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "abstracted away and more managed. And of",
      "offset": 3702.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "course the benefit here is that not",
      "offset": 3704.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "every one of us needs to implement the",
      "offset": 3706.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "get weather method. It's way easier to",
      "offset": 3708.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "use like the weather MCP service from I",
      "offset": 3710.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "don't know some weather provider.",
      "offset": 3712.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Similar um you don't want to use maybe",
      "offset": 3714.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "your own personal Google Drive MCP",
      "offset": 3716.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "server. This would be if Google creates",
      "offset": 3718.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "those. So that's the whole idea why MCP",
      "offset": 3720.96,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "servers is kind of so cool. So and um",
      "offset": 3723.359,
      "duration": 8.121
    },
    {
      "lang": "en",
      "text": "yeah yeah yeah",
      "offset": 3728.24,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "so currently we are working on improving",
      "offset": 3731.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and extending um but currently the",
      "offset": 3734.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "suggestion is probably between five to",
      "offset": 3736.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "10 and um if you have more tools you can",
      "offset": 3738.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "use embedding models to basically filter",
      "offset": 3742.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you have the user and what you would do",
      "offset": 3744.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "is basically run some similarity",
      "offset": 3746.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "matching between the descriptions and",
      "offset": 3748.559,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "like what makes sense what doesn't make",
      "offset": 3750,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "sense and then you only um put the top",
      "offset": 3751.359,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "tool so to speak.",
      "offset": 3755.599,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "I'm not sure about the claim from",
      "offset": 3769.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Andropic and like how you do it. The",
      "offset": 3771.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "only thing I can share is that Gemini",
      "offset": 3772.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "2.5 Pro was the first model to complete",
      "offset": 3774.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Pokemon Blue uh which ran I think for",
      "offset": 3777.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "like 200 hours straight. Um the only",
      "offset": 3780.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like the big challenge here is like so",
      "offset": 3783.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "we have like a limited context right",
      "offset": 3785.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "which is for Gemini 1 million. So if you",
      "offset": 3786.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "would continue for two hours you would",
      "offset": 3789.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "definitely run out of like those two",
      "offset": 3791.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "like 1 million tokens and entropic. I'm",
      "offset": 3793.359,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "not sure what that context currently is",
      "offset": 3795.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "but I'm pretty sure two hours is not",
      "offset": 3797.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "enough to run out. So what you most",
      "offset": 3799.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "likely do is you summarize you compress",
      "offset": 3800.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the context and the conversation and",
      "offset": 3804.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "what you provide to your model. So yes,",
      "offset": 3806.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I'm pretty sure Gemini can run for more",
      "offset": 3809.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "than two hours, but it depends on like",
      "offset": 3810.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "what you want to solve and how you are",
      "offset": 3812.48,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "going to solve it. Yep.",
      "offset": 3814.319,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Is that correct?",
      "offset": 3822.72,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "Yeah, that's correct. So native tools",
      "offset": 3828,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "are the next section. We can go into it",
      "offset": 3830.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "um in one second. And yes the currently",
      "offset": 3832.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the native tools are not like being",
      "offset": 3835.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "returned in a way we had with like user",
      "offset": 3837.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "assistant that's basically happening",
      "offset": 3839.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "behind the scenes and what the user gets",
      "offset": 3841.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is the final uh good assistant response",
      "offset": 3843.92,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "um that's great feedback um regarding",
      "offset": 3847.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like can we have them or not like very",
      "offset": 3850.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "happy to take it to the team I can",
      "offset": 3851.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "definitely see why it would be helpful",
      "offset": 3853.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "for you for like people to directly",
      "offset": 3855.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "build with it um but for now it's not",
      "offset": 3858,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "the case and speaking of native tools So",
      "offset": 3860.799,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "um Gemini can is basically trained to do",
      "offset": 3863.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "native things. So function calling is",
      "offset": 3867.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "very generic. We generate the",
      "offset": 3869.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "declaration and can try to do",
      "offset": 3872.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "everything. Uh but native tools are much",
      "offset": 3874.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "easier to use as you don't need to",
      "offset": 3876.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "define or create a declaration and they",
      "offset": 3878.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "work basically on the backend side. So",
      "offset": 3881.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you don't need to execute anything. And",
      "offset": 3883.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "as native tools we currently have Google",
      "offset": 3885.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "search available. Um basically all of",
      "offset": 3887.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the native tools are in AI studio here.",
      "offset": 3889.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "So we have structured output which is",
      "offset": 3891.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "not a native tool which we used to get",
      "offset": 3893.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "back the structures. Code execution is a",
      "offset": 3895.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "native tools and basically means that we",
      "offset": 3897.92,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "or Gemini runs code for us. So",
      "offset": 3901.28,
      "duration": 10.88
    },
    {
      "lang": "en",
      "text": "uh function to sort the top five",
      "offset": 3906.24,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "cities",
      "offset": 3912.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "based on",
      "offset": 3913.839,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "population.",
      "offset": 3915.839,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So what it can do is like run Python",
      "offset": 3924.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "code for us. So if you prompt it to",
      "offset": 3926.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "solve a task using Python,",
      "offset": 3929.68,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "um it should run for us the Python code.",
      "offset": 3932.559,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "So it generates the Python code and it",
      "offset": 3937.039,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "did not run it. That's a bad example.",
      "offset": 3939.839,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "Sorry.",
      "offset": 3945.119,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "Oh, did I? Ah, so my bad, not Gemini's",
      "offset": 3947.44,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "bad.",
      "offset": 3951.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Use Python.",
      "offset": 3953.92,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "Let's see what it does. Um,",
      "offset": 3956.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "okay. Yeah, now it gets Thanks. Perfect.",
      "offset": 3962.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "So, we have some reasoning and then it",
      "offset": 3965.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "generates executable code. Executable",
      "offset": 3967.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "code is also provided via the API um",
      "offset": 3969.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "which runs for us Python. So, it writes",
      "offset": 3973.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the Python script. It executes the code",
      "offset": 3975.599,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "um and it generates the mudplot lip",
      "offset": 3979.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "chart. And normally we okay in the",
      "offset": 3982.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "notebook we I have an example available",
      "offset": 3984.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "on how you get the chart. If we look",
      "offset": 3986.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "into the code execution tool. So similar",
      "offset": 3989.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "story here we run it and it returns the",
      "offset": 3992.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "markdown and you can also like it can",
      "offset": 3995.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "regenerate and return um images. So next",
      "offset": 3997.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "to code execution tool there is the URL",
      "offset": 4000.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "context tool which basically allows you",
      "offset": 4003.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "to provide a URL as part of your prompt",
      "offset": 4006.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and we extract the information from the",
      "offset": 4008.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "website behind the scenes and make it",
      "offset": 4010.799,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "available into your uh context. So",
      "offset": 4012.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "instead of going to a website command A",
      "offset": 4015.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "command C command f uh we do it for you.",
      "offset": 4017.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So in this case I asked it okay like",
      "offset": 4020.4,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "what is the other benefits of Python um",
      "offset": 4022.4,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "from like the URL you can provide up to",
      "offset": 4027.039,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "20 URLs in one request behind the scenes",
      "offset": 4029.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "it goes to the URL extracts the",
      "offset": 4032.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "information provides it as part of your",
      "offset": 4034.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "prompt and if we look into our prompt",
      "offset": 4036.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "yeah here uh it returned our nice smart",
      "offset": 4037.92,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "plot plot um chart uh which is very cool",
      "offset": 4040.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "and then a final",
      "offset": 4044.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "tool is of course Google and Google",
      "offset": 4047.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "search kind of makes sense. Um, so let",
      "offset": 4050.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "me find it. Yeah, you can allow or",
      "offset": 4053.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "enable Google search which then um",
      "offset": 4056.88,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "allows Gemini to use or do a Google",
      "offset": 4060.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "search behind the scenes. And what",
      "offset": 4063.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "happens here is that it takes our prompt",
      "offset": 4065.359,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "um in this case what are the latest",
      "offset": 4068.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "developments in renewable energies and",
      "offset": 4071.039,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "first it converts it into one or",
      "offset": 4074.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "multiple Google search queries. Then it",
      "offset": 4075.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "executes those those Google search",
      "offset": 4078.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "queries, provides it back to the model",
      "offset": 4080,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and then the model generates your final",
      "offset": 4082.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "response from your user input and from",
      "offset": 4083.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "all of the search results. Currently we",
      "offset": 4086.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "don't as mentioned don't export or",
      "offset": 4088.64,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "expose those tool calls but um I guess",
      "offset": 4091.76,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "especially helpful or interesting for",
      "offset": 4096.239,
      "duration": 8.801
    },
    {
      "lang": "en",
      "text": "um Google search is that we have uh",
      "offset": 4099.92,
      "duration": 9.359
    },
    {
      "lang": "en",
      "text": "a crowning metadata",
      "offset": 4105.04,
      "duration": 7.239
    },
    {
      "lang": "en",
      "text": "object.",
      "offset": 4109.279,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 4113.759,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So we have crowning support uh which",
      "offset": 4116.799,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "basically allows or points exactly to",
      "offset": 4119.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "where it got or like which information",
      "offset": 4122.239,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "refers to which um source and also",
      "offset": 4124.319,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "grounding meta information which",
      "offset": 4129.44,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "websites there is um which websites were",
      "offset": 4131.359,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "crawled and in our case so we have the",
      "offset": 4135.359,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "yes there",
      "offset": 4139.199,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "so um for both code execution and",
      "offset": 4144.64,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "crowning and URL context we increase the",
      "offset": 4148.719,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "context size right those contexts will",
      "offset": 4151.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "be built uh tokens will be built uh for",
      "offset": 4154.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Google search um there's a free tier",
      "offset": 4156.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "from I think 1500 Google searches which",
      "offset": 4159.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "are free and then afterwards I think",
      "offset": 4161.6,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "there are $35 per 1,000 searches uh code",
      "offset": 4164.08,
      "duration": 10.239
    },
    {
      "lang": "en",
      "text": "execution the Python running is free um",
      "offset": 4168.56,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "well context text is currently in",
      "offset": 4174.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "preview. So I'm not sure what the if and",
      "offset": 4176,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "what the pricing will be. Um yeah, and I",
      "offset": 4178.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "think like what's what's very cool and",
      "offset": 4182.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "um is part of the exercise if you",
      "offset": 4184.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "already have done it or going to do it",
      "offset": 4186.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "later is you can combine those. So you",
      "offset": 4187.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "can use the Google search tool with the",
      "offset": 4190.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "URL context tool or the code exe",
      "offset": 4191.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "execution tool with the Google search",
      "offset": 4194.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tool to",
      "offset": 4196.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "basically have it done more agentically",
      "offset": 4198,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like first search for like what's the",
      "offset": 4200.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "latest react or Python version then",
      "offset": 4202.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "write a Python script and run it and",
      "offset": 4204.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "return it. Um which makes it very nice",
      "offset": 4206.4,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "to use.",
      "offset": 4209.76,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Sorry.",
      "offset": 4214.32,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah, we have heard like a few people",
      "offset": 4218.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "would like to have a deep research API.",
      "offset": 4220.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I think the more people ask for it, the",
      "offset": 4223.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "more likely it will going to be. Um",
      "offset": 4225.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "maybe something I can like plug and",
      "offset": 4227.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "share is so I um as part of like our",
      "offset": 4229.679,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "team, we um yesterday we open sourced an",
      "offset": 4232.719,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "example for how you can build your um",
      "offset": 4237.12,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "your own deep research using Lancraft",
      "offset": 4242.239,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "with Gemini. very similar to uh right",
      "offset": 4244.8,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "very similar to what basically the the",
      "offset": 4249.679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "whole deep research agents do. We have a",
      "offset": 4251.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "question we generate queries we then run",
      "offset": 4254.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "like multiple web searches and then like",
      "offset": 4257.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "reflect and see okay was the user",
      "offset": 4260,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "question already asked do I need to do",
      "offset": 4262.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "more research and then you have this",
      "offset": 4264.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "kind of loop for okay do I need to",
      "offset": 4265.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "search other tools uh it's completely",
      "offset": 4267.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "open source it uses all of the things we",
      "offset": 4269.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "have currently seen today Gemini 2.5",
      "offset": 4271.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "flash Gemini 2.0 O. Um, so if you want",
      "offset": 4273.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "your own deep research, that's probably",
      "offset": 4276.96,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "the best way to start.",
      "offset": 4278.64,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 4291.679,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So, so there is additional pricing for",
      "offset": 4302.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the Google search.",
      "offset": 4305.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "We had it like two minutes ago.",
      "offset": 4308.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Basically, 1,500 uh searches are free.",
      "offset": 4310.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "If you use the native tool, then it",
      "offset": 4313.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "costs money. And all of those tools in a",
      "offset": 4314.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "way enrich your context. So if you use",
      "offset": 4317.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the URL context tool which goes to a",
      "offset": 4320.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "blog post and the blog post has 10,000",
      "offset": 4322.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "tokens, you pay for those 10,000 tokens",
      "offset": 4325.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "as it's included into your prompt and",
      "offset": 4327.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then tries to answer your your question.",
      "offset": 4330.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "For like the function calling we had",
      "offset": 4332.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's a bit different. Of course we",
      "offset": 4335.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "generate tokens but like the structured",
      "offset": 4336.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "output tokens are less uh and you do the",
      "offset": 4338.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "function calling or like the the Python",
      "offset": 4342.159,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "call on your side.",
      "offset": 4344.64,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Yeah, so that's definitely a good",
      "offset": 4367.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "question. So function calling of course",
      "offset": 4368.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is much more than one tool and never",
      "offset": 4370.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "going to be calling the weather API",
      "offset": 4373.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "because I mean we have weather apps",
      "offset": 4375.199,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "right um but it's a good example to show",
      "offset": 4377.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "but what's very cool is so there's",
      "offset": 4380.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "parallel function calling so uh we are",
      "offset": 4382.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in the Gemini documentation again and",
      "offset": 4385.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "let's assume you want to have a party",
      "offset": 4387.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and you have function to like power your",
      "offset": 4389.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "disco ball pole ball start the music and",
      "offset": 4391.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "dim the lights right if you like want to",
      "offset": 4393.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "say okay set us into a party mode you",
      "offset": 4396.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "would expect all of them to start at the",
      "offset": 4398.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "same time. And uh with Gemini, we have",
      "offset": 4400,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "parallel two calling. So basically what",
      "offset": 4402.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Gemini would do instead of generating",
      "offset": 4404.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "one uh object as an output, it generates",
      "offset": 4406.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "a list with basically three objects for",
      "offset": 4408.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "all of the functions you need to call",
      "offset": 4411.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and then you iterate uh over those and",
      "offset": 4413.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "call them. Parallel tool calling works",
      "offset": 4416.4,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "if the inputs and outputs of the tools",
      "offset": 4418.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "are not dependent of each other. Right?",
      "offset": 4422.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "I can start my disco ball and the music",
      "offset": 4424,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "at the same time and not okay I first",
      "offset": 4426.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "need to start my disco ball and if it",
      "offset": 4428.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "runs I can like start the music. Um if",
      "offset": 4430.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you have more of those sequential two",
      "offset": 4433.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "calling um basically you need the input",
      "offset": 4435.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh sorry you need the output of the",
      "offset": 4437.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "first function for the input of the",
      "offset": 4439.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "second function basically if you I don't",
      "offset": 4441.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "know you have like some kind of smart",
      "offset": 4443.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "home system um and you want to set the",
      "offset": 4445.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "temperature based on like the outside",
      "offset": 4448.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "weather you would first need to check",
      "offset": 4450.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "what's the weather and then like set",
      "offset": 4452,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "your temperature inside of your house.",
      "offset": 4453.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Um here would you basically provide",
      "offset": 4455.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "instructions uh using the system prompt",
      "offset": 4457.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to your model to say okay um to change",
      "offset": 4459.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the weather you first need to look up",
      "offset": 4462.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the weather and then set a temperature",
      "offset": 4465.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and what Gemini would do basically you",
      "offset": 4466.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "have your user prompt it generates the",
      "offset": 4468.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "function the fun structured function you",
      "offset": 4471.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "call the function and instead of",
      "offset": 4473.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "generating a user friendly response it",
      "offset": 4475.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "generates another function call and then",
      "offset": 4477.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you can call it so basically it",
      "offset": 4479.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "continues this function calling loop",
      "offset": 4481.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "before generating a very nice user",
      "offset": 4483.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "friendly output.",
      "offset": 4485.44,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Yes, kind of. I mean, I can like So,",
      "offset": 4492.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "there's I put up an example uh for like",
      "offset": 4495.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the sequential",
      "offset": 4498.8,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "um function calling. Uh there",
      "offset": 4500.64,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 4509.12,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "multip function calls and some",
      "offset": 4514.32,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "sequential and some",
      "offset": 4516.32,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "workflow something.",
      "offset": 4520.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Yeah. So currently there's not a good",
      "offset": 4523.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "way to handle like functions who could",
      "offset": 4525.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "time out or like the best way. So it's a",
      "offset": 4527.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "a bit like I would say parallel calling",
      "offset": 4530.239,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is like you need to wait for all of the",
      "offset": 4533.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "results to put in into the next one. If",
      "offset": 4536.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "one takes longer, you can basically",
      "offset": 4538.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "provide an error message or not ready",
      "offset": 4540.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "yet or something. But you would need to",
      "offset": 4542.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "to explore how it works on the live API",
      "offset": 4545.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "which is basically our way to create",
      "offset": 4548.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "real-time agents. They are working",
      "offset": 4550.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "something called asynchronous function",
      "offset": 4552.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "calling where um the conversation",
      "offset": 4554.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "continues. So when you think about a",
      "offset": 4557.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "customer support agent which you can",
      "offset": 4559.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "which you talk to, right? It would be",
      "offset": 4561.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "very weird if the agent stops for like",
      "offset": 4563.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "three minutes and doesn't say something",
      "offset": 4565.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "because it needs to look up your",
      "offset": 4567.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "information. So that that's what they",
      "offset": 4569.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "call asynchronous function calling. So",
      "offset": 4570.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you can continue the conversation with",
      "offset": 4572.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the agent but the agent runs a two call",
      "offset": 4574.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and then like injects the response uh",
      "offset": 4577.199,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "later but that's only for the live API.",
      "offset": 4579.28,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "Yep. So um documentation live API",
      "offset": 4584.48,
      "duration": 10.4
    },
    {
      "lang": "en",
      "text": "um it was launched on I think in IO or",
      "offset": 4590.96,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "cloud next. Um, so yeah, there there you",
      "offset": 4594.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "have like a tool use with life API and",
      "offset": 4598.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "there's asynchronous function calling",
      "offset": 4601.52,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "um with code snippets for for Python and",
      "offset": 4604.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "JavaScript.",
      "offset": 4608.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Cool.",
      "offset": 4610,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 4612.4,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "No, it's it's more like I start a",
      "offset": 4617.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "function call now. I need to continue my",
      "offset": 4619.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "conversation. I get back the response",
      "offset": 4621.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "later. So instead of like getting the",
      "offset": 4623.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "response, what you get back, yes, what",
      "offset": 4626.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you get back from the model could be for",
      "offset": 4628.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "example interrupted or scheduled so that",
      "offset": 4630.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "you as a developer know okay I started",
      "offset": 4633.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "something the model knows it started",
      "offset": 4636.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "something and then you can inject once",
      "offset": 4638.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the your function is the output is ready",
      "offset": 4640.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "put it back into the conversation and",
      "offset": 4644.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "then the model uses that information to",
      "offset": 4646.08,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "continue.",
      "offset": 4648.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Y",
      "offset": 4654.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "um so in AI studio we don't have a",
      "offset": 4660.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "feature like this. Um but you can like",
      "offset": 4662.719,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "use third party tools like lang or arise",
      "offset": 4665.04,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "AI Phoenix uh but Vortex AI supports",
      "offset": 4668.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "those features.",
      "offset": 4672.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 4674.8,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "Yeah, that was question. Yeah.",
      "offset": 4677.04,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "Yeah,",
      "offset": 4684.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I don't know. The only thing I know is",
      "offset": 4689.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that so previously there was only Gemini",
      "offset": 4691.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "2.0 flesh available in the live API. But",
      "offset": 4693.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "now we have Gemini 2.5 flash. So",
      "offset": 4696.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "basically the model we used in like the",
      "offset": 4698.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Jupyter notebooks. Um I guess you have",
      "offset": 4700.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to try or maybe like reach out to the",
      "offset": 4703.679,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "people who are working more with it.",
      "offset": 4705.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "Okay, more questions.",
      "offset": 4712.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 4714.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So, URL context r tool basically works",
      "offset": 4722.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "based on the link you provide. So, if",
      "offset": 4724.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "you provide your own personal blog, it",
      "offset": 4727.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "goes to that link and tries to extract",
      "offset": 4730.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the information and uses it in the",
      "offset": 4732.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "context. and search basically uses",
      "offset": 4734.48,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "Google search creates one or multiple",
      "offset": 4737.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "search queries based on the prompt",
      "offset": 4740.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "searches and then provides the outputs",
      "offset": 4742.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "into your prompt. So if you already know",
      "offset": 4745.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "where the source is for your prompt to",
      "offset": 4748.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "create a best answer you all context",
      "offset": 4751.84,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "could be a good use case for it.",
      "offset": 4754.08,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "I don't know.",
      "offset": 4769.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "You pay wall content. You mean like",
      "offset": 4771.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "pages?",
      "offset": 4774.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "I don't know. I would not expect to have",
      "offset": 4777.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it ex like but the live API supports",
      "offset": 4779.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "function calling. So if you have a",
      "offset": 4783.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "subscription or like a way to access",
      "offset": 4786,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this paid content, you can create your",
      "offset": 4788.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "own function which the model can then",
      "offset": 4790.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "invoke and use as a context for for its",
      "offset": 4792.64,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "um information.",
      "offset": 4796.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Okay, cool. Then last section I guess",
      "offset": 4799.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "every one of you might have heard about",
      "offset": 4802.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "model context protocol by now and that's",
      "offset": 4804.56,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "like either our solution to how we all",
      "offset": 4807.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "align on how to call agents or like",
      "offset": 4810.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "what's the right thing to do. I'm a big",
      "offset": 4812.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "fan of it. Um I think it makes it way",
      "offset": 4814.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "more accessible for people to build",
      "offset": 4817.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "agents especially if we get more of",
      "offset": 4819.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "first party remote MCP servers um where",
      "offset": 4820.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you can basically focus on building your",
      "offset": 4824,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "agent instead of creating all of those",
      "offset": 4826.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "functions. I mean I'm pretty sure there",
      "offset": 4828.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "are like now a million get weather",
      "offset": 4829.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "functions which can be used and I hope",
      "offset": 4831.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "with um MCP we can like fix this. Uh",
      "offset": 4833.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "what we um shipped and announced at",
      "offset": 4836.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Google IO is um native integration of",
      "offset": 4838.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "MCP servers inside the Google geni SDK.",
      "offset": 4840.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "So the SDK which we have used during the",
      "offset": 4844.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "whole workshop um allows us to directly",
      "offset": 4846.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "use MCP servers and sessions which makes",
      "offset": 4849.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it even easier for all of us to",
      "offset": 4852.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "integrate it. So when we um look back at",
      "offset": 4854.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "our function calling example, we needed",
      "offset": 4857.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to call the function, create a",
      "offset": 4860.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "declaration and all of those different",
      "offset": 4861.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "things. And now with the MCP uh",
      "offset": 4863.84,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "integration, we can basically uh only",
      "offset": 4866.32,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "start or initialize our client here. I",
      "offset": 4870.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "created an MCP weather service which",
      "offset": 4873.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "kind of has the same functionality. And",
      "offset": 4876.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "um all of the things we now need to do",
      "offset": 4879.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is like we use our generate content",
      "offset": 4882.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "method and in our tools argument we",
      "offset": 4884.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "provide our session. So we start our",
      "offset": 4887.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "server,",
      "offset": 4890.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we create our session",
      "offset": 4891.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and we provide the session to um the",
      "offset": 4894.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Gemini SDK and then what happens behind",
      "offset": 4897.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the scenes is basically the same",
      "offset": 4900.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "function call loop we did manually. It's",
      "offset": 4902.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "like okay what are the tools available?",
      "offset": 4904.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "get all of the tools from the MCP",
      "offset": 4908,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "servers, put them into the LLM call. If",
      "offset": 4909.6,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "the LLM call makes a function call,",
      "offset": 4913.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "extract the function call, call the MCP",
      "offset": 4916.56,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "server, get the response from the MC MCP",
      "offset": 4919.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "server, put it back into our",
      "offset": 4922.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "conversation, have the model generate",
      "offset": 4924.48,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "final response. Um, I guess it's um",
      "offset": 4926.639,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "probably time to find out if Collab has",
      "offset": 4930.239,
      "duration": 8.121
    },
    {
      "lang": "en",
      "text": "node installed and to see if it works.",
      "offset": 4932.719,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 4942.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay. It doesn't.",
      "offset": 4947.76,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "What I can do is I can quickly set it up",
      "offset": 4958,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "locally and show you in one second",
      "offset": 4960.719,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "on how it works.",
      "offset": 4964.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 5045.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay. So, we are back. I'm in cursor.",
      "offset": 5056.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Same notebook,",
      "offset": 5059.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "same setup. And now I can like use it.",
      "offset": 5061.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "And now basically what happens if I",
      "offset": 5064.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "let's not ask about London, right? We",
      "offset": 5067.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "are in San Francisco. Um let's ask it",
      "offset": 5069.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "about the weather in San Francisco. It",
      "offset": 5071.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "now does all of like the different",
      "offset": 5074.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "function calls and loops behind the",
      "offset": 5076.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "scenes. The MCP server is very simple.",
      "offset": 5078,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "It uses the open Meteor API which is",
      "offset": 5080.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "kind of free to use on a small scale for",
      "offset": 5082.719,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "testing. Um, so we generate the",
      "offset": 5085.28,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "output, run all of the MCP calls, might",
      "offset": 5090.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "take a bit longer for the API call, and",
      "offset": 5093.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then we get back, okay, the weather in",
      "offset": 5095.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "San Francisco today will be 70° Celsius",
      "offset": 5097.44,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "at zero, and then all of the other",
      "offset": 5100.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "numbers. And that's basically now all",
      "offset": 5104.159,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "you need to to combine or connect an MCP",
      "offset": 5106.48,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "server with the the Gemini SDK which is",
      "offset": 5109.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like I mean it fits on a single screen",
      "offset": 5112.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "like which is easy enough for you to get",
      "offset": 5114.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "started. Um we use here like a a local",
      "offset": 5116.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "running MCP server but it works the same",
      "offset": 5119.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "way with a remote MCP server that's um",
      "offset": 5121.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "also the exercise uh for for this um",
      "offset": 5124.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "part of the workshop. So basically deep",
      "offset": 5127.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Viki from uh the cognition AI guys",
      "offset": 5130.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "hinder Devon they had a have a very nice",
      "offset": 5132.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "remote MCP server which um can talk to",
      "offset": 5134.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "GitHub repositories. So instead of like",
      "offset": 5137.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "creating a sitio client you can use the",
      "offset": 5140.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "streamable HTTP client and connect a",
      "offset": 5142.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "remote MCP server to Gemini to talk to",
      "offset": 5145.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "it and um yeah basically benefit for one",
      "offset": 5148.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "of the advancements currently happening",
      "offset": 5151.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "in AI.",
      "offset": 5153.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Do we have more questions?",
      "offset": 5155.36,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "Y",
      "offset": 5158.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "yeah.",
      "offset": 5162,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So ADK is um for those of you who don't",
      "offset": 5169.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "know is like agent development kit. It's",
      "offset": 5171.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "an uh agent library which adds a lot of",
      "offset": 5174.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "more abstraction on top of like the the",
      "offset": 5177.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "client uh SDK we used. It makes it a lot",
      "offset": 5180.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "easier to do all of the tool calling. um",
      "offset": 5183.679,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "integrates MCP server as well makes it",
      "offset": 5186.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "much more easier to manage like multiple",
      "offset": 5188.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "agents or deploy it to cloud. So it has",
      "offset": 5190.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a lot of like I would say batteries",
      "offset": 5193.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "included. Uh question again depends on",
      "offset": 5195.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you if you would like to use uh",
      "offset": 5197.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "frameworks or rather prefer building it",
      "offset": 5200.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "yourself. I see a bit benefit of like",
      "offset": 5202.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "getting started very quickly using",
      "offset": 5204.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "agentic frameworks, but the more",
      "offset": 5206.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "abstraction you add, right? The more the",
      "offset": 5208.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "less you know on the first time and then",
      "offset": 5210.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "maybe you need you need to dig a bit",
      "offset": 5213.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "deeper later and all of the the",
      "offset": 5214.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "advancements or speed you get in the",
      "offset": 5217.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "beginning will be your slowdowns in the",
      "offset": 5218.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "future. But ADK is definitely a great",
      "offset": 5221.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "way to start. They also have like tons",
      "offset": 5223.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of examples. It has support for Gemini",
      "offset": 5225.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "and the Gemini API. So definitely a good",
      "offset": 5227.84,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "way to to take a look. Yep.",
      "offset": 5230.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Um, no",
      "offset": 5240.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I don't think so. Uh, but what you can",
      "offset": 5242.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "do with YouTube links or YouTube videos",
      "offset": 5245.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is we can ask it to return timestamps",
      "offset": 5247.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "for you as the response and it works",
      "offset": 5249.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "very nicely. So it is very accurately",
      "offset": 5252.719,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "because so basically a video is more or",
      "offset": 5255.44,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "less just many images behind each other",
      "offset": 5259.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "right and currently uh when we process",
      "offset": 5262.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "videos it will be done at one frames per",
      "offset": 5265.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "second and you will always have like the",
      "offset": 5267.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "time stamp the image the time stamp the",
      "offset": 5270.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "image that's how Gemini exactly knows at",
      "offset": 5272.639,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "what place in the video is I don't know",
      "offset": 5275.92,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "a man jumping or dancing or something.",
      "offset": 5278.8,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "It's analyzing one frame per second.",
      "offset": 5283.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 5286.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "It depends on your I mean we can I mean",
      "offset": 5288.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it's part of the section. Um I use that",
      "offset": 5290.719,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "all the time. Cool. Cool.",
      "offset": 5293.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Yeah. But yeah, basically what happens",
      "offset": 5297.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "with videos is I mean videos are 24",
      "offset": 5298.719,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "frames per second or even more that like",
      "offset": 5301.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "easily lets the the context explode. So",
      "offset": 5304.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "currently what is done is that is we",
      "offset": 5307.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "have one frame per second. So if you",
      "offset": 5309.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "have 60 seconds, it would be 60 images.",
      "offset": 5311.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Uh which makes it also easy to count how",
      "offset": 5314.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "many tokens it would be. And that's also",
      "offset": 5316.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "how we can fit in um a 1 hour long video",
      "offset": 5318.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "into 1 million tokens. Basically using",
      "offset": 5322.719,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "the transcript of YouTube or",
      "offset": 5325.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "No, it's",
      "offset": 5329.92,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "it's all multi-modal natively.",
      "offset": 5332.159,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "No, I mean you can like easily upload",
      "offset": 5336.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "normal.mpp4 MP4 files and you can ask it",
      "offset": 5338.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like what was say or said in in the",
      "offset": 5341.199,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "video.",
      "offset": 5343.52,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay, cool.",
      "offset": 5347.92,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 5350.719,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 5357.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So I'm also like maybe for you if you",
      "offset": 5363.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "don't know is like memory is basically",
      "offset": 5367.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "next to tools what makes an agent an",
      "offset": 5369.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "agent and for memory we have short-term",
      "offset": 5372.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "memory and long-term memory. Short-term",
      "offset": 5374.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "memory is basically your conversation",
      "offset": 5376.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "which is part of your current like state",
      "offset": 5378.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "when you talk to an agent. And long-term",
      "offset": 5380.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "memory is basically",
      "offset": 5382.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "memory or like information about a",
      "offset": 5384.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "previous interaction or something about",
      "offset": 5386.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the user. And long-term memory is not",
      "offset": 5389.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "like part of the LLM which we need to",
      "offset": 5391.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "provide externally. And me zero is like",
      "offset": 5393.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "something I looked at it because it's",
      "offset": 5396.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "very nice. It's does it implicitly. So",
      "offset": 5398,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "basically it takes the conversation and",
      "offset": 5400.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "tries to create a nice abstraction for",
      "offset": 5402.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you we which you can include. Um I'm not",
      "offset": 5404.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "sure like what the current state is like",
      "offset": 5407.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "how or what works well with Gemini. I'm",
      "offset": 5409.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "like what I always see or where Gemini",
      "offset": 5412.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "really shines is the long context. So",
      "offset": 5415.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the better your information extraction",
      "offset": 5418.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is and like what you provide to the",
      "offset": 5420.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "model based on the memory I think it",
      "offset": 5421.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "works very well. Not sure about those",
      "offset": 5423.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "tool calling memory kind of systems.",
      "offset": 5426.239,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 5429.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. So we are working on 2",
      "offset": 5433.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "million. Um, not sure when it is like",
      "offset": 5436.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "generally available, but",
      "offset": 5438.88,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "yeah, like there that was research, but",
      "offset": 5442.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you know, like the bigger you go, the",
      "offset": 5444.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "more expensive you get. And do you want",
      "offset": 5446.32,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "to pay like $50 per 1 million tokens?",
      "offset": 5448.239,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 5453.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "More questions or everyone working?",
      "offset": 5457.12,
      "duration": 8.119
    },
    {
      "lang": "en",
      "text": "Yep. Then",
      "offset": 5461.76,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "before",
      "offset": 5465.6,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "functions. Yeah.",
      "offset": 5468.639,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 5471.199,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "No. So the the difference with MCPS now",
      "offset": 5476.8,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "and what we had previously is that we",
      "offset": 5481.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "don't need to write and define the",
      "offset": 5483.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "functions and the function declarations.",
      "offset": 5486.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "The LLM sees at the end the same exact",
      "offset": 5489.679,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "exact exact same thing. So if I create",
      "offset": 5493.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "my function declarations manually with a",
      "offset": 5496.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "JSON schema or if I retrieve them from",
      "offset": 5498.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "an MCP server, those are the same things",
      "offset": 5501.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "for the LLM. It's just makes it much",
      "offset": 5503.679,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "easier to not need to rewrite the same",
      "offset": 5506.32,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "functionalities over and over again.",
      "offset": 5510.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Right? If you work at a company and you",
      "offset": 5513.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "want to integrate APIs which you have",
      "offset": 5515.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "internally, there's a pretty big chance",
      "offset": 5518.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "that two teams write the same function",
      "offset": 5521.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "declaration and the same rapper to call",
      "offset": 5523.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "it. And the idea here is to only have",
      "offset": 5525.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "one team needing to write it and",
      "offset": 5528.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "everyone to benefit from it. The same is",
      "offset": 5530.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "with like public APIs like Google Maps,",
      "offset": 5532.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "G Drive. So MCPS gives us a way to",
      "offset": 5535.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "collaboratively create like the best or",
      "offset": 5539.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like the standard as a way to how to",
      "offset": 5541.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "call it and then everyone kind of can",
      "offset": 5543.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "can implement the MCP tools uh which you",
      "offset": 5545.44,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "need for your use case.",
      "offset": 5547.92,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "It's so an MCP server for example",
      "offset": 5558.239,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "exposes four tools. um you provide all",
      "offset": 5561.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "of those four input schemas which are",
      "offset": 5565.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "like function declarations with a name,",
      "offset": 5567.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "a description and the parameters to your",
      "offset": 5569.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "LLM call and then based on the prompt",
      "offset": 5571.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and those declarations the LLM in this",
      "offset": 5575.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "case Gemini decides if it should call a",
      "offset": 5577.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "tool which tool could to call or should",
      "offset": 5580.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "it call multiple tools and that's the",
      "offset": 5583.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "same logic we had for normal or have for",
      "offset": 5586.4,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "normal function calling. So there's no",
      "offset": 5590.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "no difference in that way. The only",
      "offset": 5592.639,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "difference we see is that",
      "offset": 5595.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "MCP servers being easily available or",
      "offset": 5598.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "imple implementing many tools, people",
      "offset": 5600.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "start to be very easy on how many to",
      "offset": 5603.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "add, right? So we see people at like 50",
      "offset": 5605.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "100 tools. So we need to improve our",
      "offset": 5608.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "LLMs to be able to have them really",
      "offset": 5610.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "know, okay, which is the right tool to",
      "offset": 5613.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "use when you have like 50 different",
      "offset": 5614.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "tools available at once.",
      "offset": 5616.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "So there are examples for ADK to work",
      "offset": 5630.8,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "with A2A. So A2A stands for agent to",
      "offset": 5634.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "agent protocol which is done by the",
      "offset": 5637.679,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Google cloud team. The idea here is uh",
      "offset": 5639.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "to allow companies to build agents in",
      "offset": 5642.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "different frameworks like lench chain,",
      "offset": 5645.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "langcraft, llama index and then have a",
      "offset": 5646.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "an easy way to create multi- aent",
      "offset": 5649.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "systems that one agent can call the",
      "offset": 5652.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "other agent without needing to implement",
      "offset": 5654.239,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "comp complex logic. Um but the Gemini",
      "offset": 5657.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "SDK is like not directly",
      "offset": 5661.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "but there are great examples for it.",
      "offset": 5663.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Cool.",
      "offset": 5667.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 5668.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "So we are working on basically browser",
      "offset": 5681.76,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "use or computer use use cases which is",
      "offset": 5685.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "available in preview uh which we are",
      "offset": 5689.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "currently testing with a few companies",
      "offset": 5690.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "um would that would allow Gemini to",
      "offset": 5693.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "control UI so it could basically go to",
      "offset": 5695.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "whatever website you want it to go to",
      "offset": 5697.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and then there's sorry the URL context",
      "offset": 5699.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "tool where you can provide a website and",
      "offset": 5702.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "we would programmatically try to extract",
      "offset": 5706,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "information from it. Of course, it's",
      "offset": 5708.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like a if it's a super heavy JavaScript",
      "offset": 5710,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "website, there's not much to extract.",
      "offset": 5711.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "That's where like the the browser use",
      "offset": 5714.48,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "agent would then u be useful",
      "offset": 5716.159,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 5720.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Yeah, kind of. Yeah, hopefully coming",
      "offset": 5722.96,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "soon.",
      "offset": 5726.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So it will return structured outputs",
      "offset": 5739.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "again based on what you provide. So you",
      "offset": 5742.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "can control a local environment but we",
      "offset": 5744.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are also working with the cloudr run",
      "offset": 5746.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "team to make it super easy to you for",
      "offset": 5748.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you to run so that you can like run a",
      "offset": 5750.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "chrome instance on cloudr run uh which",
      "offset": 5752.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "gemini can talk to and control or you",
      "offset": 5754.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "can like control a local",
      "offset": 5756.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "instance",
      "offset": 5759.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "yeah yeah",
      "offset": 5761.679,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "do authentication",
      "offset": 5764.639,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "that's a good question I think the whole",
      "offset": 5769.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "industry is currently trying to answer",
      "offset": 5770.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like what's best way to handle agent",
      "offset": 5772.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "authentication. So the MCP um as a",
      "offset": 5774.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "protocol itself supports OO off. So",
      "offset": 5777.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "basically when you want to connect to an",
      "offset": 5780.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "MCP server which is protected you would",
      "offset": 5782.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "get back a 403 with like not authorized",
      "offset": 5785.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "which can then trigger an off flow on",
      "offset": 5788.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "your client. Maybe you have seen it in",
      "offset": 5790.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like claw desktop where you get a popup",
      "offset": 5792.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like to log in with I don't know like",
      "offset": 5794.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "your like atina account or something. So",
      "offset": 5797.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that's like one way of doing it, but it",
      "offset": 5800,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "definitely needs more work, right? Do",
      "offset": 5803.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you want your agent to access all of",
      "offset": 5805.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "your emails or all of your GitHub repos",
      "offset": 5807.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "repositories or how can you like scope",
      "offset": 5810.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "it down to like only the one specific",
      "offset": 5812.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "repository, but they are currently like",
      "offset": 5814.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actively working on it. I know that of",
      "offset": 5816.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "zero will be here tomorrow as well.",
      "offset": 5818.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Those guys are very great to talk to. I",
      "offset": 5820.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "think I know that they're doing a lot",
      "offset": 5822.56,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "there. So,",
      "offset": 5823.84,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "yeah.",
      "offset": 5829.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Do you mean um citation in when you just",
      "offset": 5850.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "send a regular prompt or citation when",
      "offset": 5853.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you like use Google search and that have",
      "offset": 5855.679,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "citations?",
      "offset": 5858.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 5864.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "So that's available in like the Google",
      "offset": 5865.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "search tool. Um and it has information",
      "offset": 5867.6,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "about which websites was used to to",
      "offset": 5871.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "retrieve. You can um click on the link",
      "offset": 5874.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "directly to see where you land. And then",
      "offset": 5876.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "there's this um",
      "offset": 5878.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "metadata or like chunking or like",
      "offset": 5881.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "crowning support which basically has a",
      "offset": 5883.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "start index and an end index from the",
      "offset": 5886.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "answer and then also which sources were",
      "offset": 5888.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "used. So you technically can highlight",
      "offset": 5891.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "or put numbers behind it to help users",
      "offset": 5893.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "understand okay that part of the",
      "offset": 5898,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "response is generated based on those two",
      "offset": 5899.679,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "um links.",
      "offset": 5903.36,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "You mean in in general there are",
      "offset": 5922.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "sometimes citation meta data or",
      "offset": 5924.32,
      "duration": 8.839
    },
    {
      "lang": "en",
      "text": "which citation data do you mean",
      "offset": 5928.56,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "okay I know or I only know that's part",
      "offset": 5935.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "of like legal and like training that if",
      "offset": 5938.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "there's something referred directly you",
      "offset": 5942,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "need to like provide it. That's why it",
      "offset": 5944,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is there. That's why it is not always",
      "offset": 5946.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "there. Um it can be there but it doesn't",
      "offset": 5948.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "must be and it's not like for a user to",
      "offset": 5951.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "hey that's coming from there but it's",
      "offset": 5954.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "more like um compliance basically why we",
      "offset": 5955.92,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "need to have it.",
      "offset": 5959.119,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "I only know about like the web search",
      "offset": 5969.36,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "feature.",
      "offset": 5971.76,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Ah okay. So you provide as a context",
      "offset": 5980.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "like a document.",
      "offset": 5983.04,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 5985.52,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 5991.52,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "No, that's like a very good question.",
      "offset": 6007.84,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Currently, we don't have like the same",
      "offset": 6009.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "experience as Entropic has. Um I guess",
      "offset": 6011.119,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the base is like to really try and like",
      "offset": 6014.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "different prompting strategies and like",
      "offset": 6016.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "what you want to achieve. Um yeah. No,",
      "offset": 6018.48,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "but it's like good good feedback.",
      "offset": 6022,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Sadly, not yet. I hope hopefully one day",
      "offset": 6036.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that you can like just here's Gmail,",
      "offset": 6039.679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "here's Drive and let's chat. Um, but",
      "offset": 6042.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "currently there's no public remote MCP",
      "offset": 6044.719,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "server.",
      "offset": 6047.119,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I don't know exactly what what are the",
      "offset": 6052.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "reasons. I think the more people ask for",
      "offset": 6054.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it, the easier it will be that we get",
      "offset": 6056.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "once. So, if you like that's great",
      "offset": 6058.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "feedback. more people um if the more",
      "offset": 6060.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "people are going to use MCP servers the",
      "offset": 6063.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "higher the chances are going to be and I",
      "offset": 6065.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "think in general for MCP to succeed we",
      "offset": 6068,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "need more first party remote servers",
      "offset": 6070.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "right because we cannot build a secure",
      "offset": 6072.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "GitHub MCP servers that's something",
      "offset": 6076.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "GitHub needs to do because they know how",
      "offset": 6077.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "their off system works how you can scope",
      "offset": 6079.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "it um so we really need those first",
      "offset": 6082.239,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "party MCP servers in the long time",
      "offset": 6084.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "for for AI mode. I have no idea. I think",
      "offset": 6096.719,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "like so AI mode you mean about like",
      "offset": 6100.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "inside Google search?",
      "offset": 6102.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I think in general overall same goes for",
      "offset": 6105.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "like normal Google search is like have",
      "offset": 6107.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "high quality content um try to stick to",
      "offset": 6110,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "like um web standards. Um, I guess same",
      "offset": 6112.639,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "works here as well,",
      "offset": 6116.159,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "but I don't know. Sorry.",
      "offset": 6119.199,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6121.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "it should. So if you uploaded a PDF, you",
      "offset": 6132.639,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "need to make sure that you put a PDF at",
      "offset": 6135.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the first part of your prompt. So you",
      "offset": 6137.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like the the automatic um caching works",
      "offset": 6139.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "from the beginning to the end, right? If",
      "offset": 6143.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you change the beginning, you can never",
      "offset": 6144.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "cache like the long document. But if you",
      "offset": 6145.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "put the the PDF in the beginning and",
      "offset": 6148.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "change the prompt behind it, it should",
      "offset": 6150.8,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "work. Yes.",
      "offset": 6152.56,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6156.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Sometimes there's a case where index",
      "offset": 6159.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "provides the starter index out of range",
      "offset": 6162.4,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "for the length of the really",
      "offset": 6166,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "curious if you any similar experiences.",
      "offset": 6169.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "No, but if you like have an example for",
      "offset": 6172.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "us to reproduce and to share that would",
      "offset": 6174.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "be very helpful. The only thing I know",
      "offset": 6176.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "is that so sometimes there can be a",
      "offset": 6178.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "start index which can be null which",
      "offset": 6180.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "basically means it's zero like it starts",
      "offset": 6182.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "at the beginning.",
      "offset": 6184.8,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 6192.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "No, like if you have an example, please",
      "offset": 6196.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "like send it to me on Twitter or",
      "offset": 6197.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "somewhere like Okay, that that would be",
      "offset": 6199.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "very helpful because that should not be",
      "offset": 6202.239,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "the case.",
      "offset": 6203.76,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "Yeah. Is there any way to actually get",
      "offset": 6210.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "that was",
      "offset": 6214.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "citation from the website that it refers",
      "offset": 6216.88,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "to. So that you only get the website.",
      "offset": 6219.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "You just get the Yeah. No, like so",
      "offset": 6223.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "currently you get like the the start and",
      "offset": 6225.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "end of the response which refers to",
      "offset": 6228.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "something but not the web part. But",
      "offset": 6231.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that's like like please put it all",
      "offset": 6233.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "together and like send it to us like",
      "offset": 6235.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "very happy to like talk to the Google",
      "offset": 6237.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "search team who is like building the",
      "offset": 6239.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "native tool. Um, yes, super helpful with",
      "offset": 6241.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "like to better understand like what you",
      "offset": 6243.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "need.",
      "offset": 6245.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay,",
      "offset": 6248.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "cool then. Thanks all for coming. Um,",
      "offset": 6252.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "please continue with the workshop. Try",
      "offset": 6255.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it out. If you have like any questions,",
      "offset": 6257.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we are very happy to receive any",
      "offset": 6260.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "positive, negative feedback, any ideas,",
      "offset": 6262.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "any pain points you have. Um, we are",
      "offset": 6264.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "available on like social channels. You",
      "offset": 6267.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "can like find me Philip Schmeid",
      "offset": 6268.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "basically everywhere. If not, we open",
      "offset": 6270.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the GitHub issue or be very noisy about",
      "offset": 6272.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "when when something doesn't work. Uh we",
      "offset": 6275.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "we always try to make sure it we fix it.",
      "offset": 6277.6,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "Cool. Thanks,",
      "offset": 6280.4,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 6284.57,
      "duration": 3.15
    }
  ],
  "cleanText": "[Music]\nOkay.\nHi everyone.\nWelcome, welcome.\nUm, so welcome to our workshop AI engineering with the Google Gemini 2.5 Model Family.\nSo as it is a workshop, we are going to keep it super hands-on.\nSo please keep all computer open.\nUh, you don't need any Google account, um, like Google Cloud account.\nYou can use your personal Gmail.\nIt will be completely free for you to use.\nSo that's the point.\nUm, before we get started, can you maybe help me understand how many of you have used Google Gemini before?\nOh wow.\nThat's that's cool.\nThat's a lot of hands.\nMore than the last time I gave a talk like this.\nUm, so what, um, we're going to do, you have like three slides, so don't worry, not too much.\nBut we're going to focus on Gemini 2.5.\nSo there's a Gemini 2.5 Pro model and a Gemini 2.5 Flash model.\nWe're going to use the flash model as it's available, uh, for free tier via API access.\nSo we are going to do coding, and both models are multimodal by default, meaning they can understand text, images, audio, videos, documents, and can generate text.\nWe also have Gemini models which can generate images, which we are also going to use.\nAnd we have now Gemini models which can generate, uh, audio.\nSo you can create speech from text.\nUm, if you are curious where you can find those nice model cards with all of the feature, the model context, the output tokens, it's on the Gemini docs.\nUm, as mentioned, we have two new, um, text-to-speech models since Google IO last week, not a week before.\nUm, those are really cool.\nYou will try and see them later.\nAnd now for the important details.\nSo I created a Slack channel.\nIf you are on the AI Engineer Slack channel, you should be able to find it.\nUm, feel free to use it during the workshop.\nYou can ask questions.\nI try to regularly check, uh, to answer them or even afterwards if you have questions, complete the workshop at home or next week.\nUh, I will take a look and make sure that you get all of the answers.\nAnd then, uh, we have one QR code.\nUh, it's AI studio.\nYou can also just enter in your browser AI.dev or AI.studio, which will brings you to AI studio, and the other link is, so there's a GitHub repository, uh, with the workshop.\nWe are going to do the workshop.\nI can like now switch directly to it, so, um, let's hope if the Wi-Fi gives us some freedom.\nSo the good part about a workshop is that we have Google Collabs, so there's not a lot of downloading, uh, happening, and it will all run in the collab environment if you have a Google account.\nAnd the other thing is what we need to do is in AI studio to generate an AI key.\nSo the GitHub repository is now loaded.\nIn the GitHub repository, we have a notebooks.\nUh, yeah, of course.\nSorry.\nSo in the GitHub repository, we have a notebooks, uh, folder which includes all of our four workshops, uh, plus a zero one, which is basically some minor instructions how to set up AI studio, how to get an API key, and how to send your first request.\nAnd then, uh, we have one or the beginning section will be all about text generation, um, getting started, getting familiar a bit with the SDK.\nUh, the second part will be all about multimodality.\nHow can Gemini understand images, video, audios?\nHow can we generate images or audio?\nAnd then the third, um, section will be about function calling, structured outputs, the native tools, how can I integrate Google search into it?\nAnd then I guess, um, with all of the hype currently going on, we we look at how you can integrate MCP servers together with Gemini using it as a model to call the different tools.\nUm, also very nice.\nSo there's a solutions, um, folder.\nThe solutions folder includes the same notebooks but with the solutions.\nSo all of the notebooks include to-do text and also some code snippets, um, and some comments.\nSo there's a mix between working code snippets, code snippets which has some pointers and straight up exercises with to-dos for for you to do.\nUm, I will work with you through like the existing snippets, and then everyone can work on the exercises.\nThe idea is to that we try to maybe use 30 minutes per different section.\nIf you, for example, are already very familiar with how what I can do with text generation and I would like rather like look at the multimodalities parts or at the function calling parts, feel free to like directly jump into the section, and in general, we want to keep it very open, very dynamic.\nIf you have questions related to the content, maybe unrelated, please keep them coming, ask them in Slack, raise your hands.\nI'm not sure maybe we have some microphones here as well, so we can like give it to you to, uh, make it super interactive.\nSo I guess let's get started.\nSo if you go to the notebooks, there's also a collab, um, button you can click which opens the notebook directly in Google Collab.\nAnd if you prefer like a local Jupyter environment, you can try to clone the repository.\nNot sure if it works for Wi-Fi or not.\nUm, I guess Collab will be the easiest.\nAnd as mentioned before, the only requirement you basically have is a working Google account.\nCan be your corporate one, can be your private one, can be one you create in the next five minutes.\nAnd the first step is, uh, what we need to do is basically go to AI studio.\nFor the ones of you who are not familiar with AI studio, AI studio, um, is our developer platform to quickly test the models, to experiment with the models, uh, and also keep it very similar to the development code you will be used.\nSo if I try to run a request like, um, maybe let's ask something, what's the AI engineering summit?\nI can on the right side, for example, enable native tools connected with Google search.\nI have our flash preview model.\nI can run the request, and we'll see how fast, yeah, the model is thinking, and the nice part here is, uh, I can also directly get the SDK code from our request as soon as it's ready.\nSo if you are experimenting in AI studio and you want to convert it into a Python script or want to play around with it, extend it, um, that's all possible.\nSo the engineering summit refers to several events focusing on artificial intelligence and engineering, that's great, and that also matches like the one from New York, which was done at this February school.\nSo what you need to do to get your API key at the top right is, uh, I can also make that bigger, maybe it's easier.\nSo we have a get API key at the top, um, you go to it, um, sorry for it's German, uh, for being it's German, but on the top right corner, there should be a create API key, a blue button, and in there you should, it should open, um, s, uh, I can click it, it should open s model where you can either select your Google Cloud project, if there's none, you should be able to create one if, yep, uh, yep.\nOf course.\nUm, parent.\nOkay.\nGood idea.\nOkay.\nUh, once you select your Google Cloud project or create one, you should be able to create one.\nAnd once it is created, um, you should have it available as a popup.\nIf not, you can scroll down a bit.\nThere are your API keys.\nUm, and then the second step would be to go into collab and to go on the left side, uh, in the navigation.\nSo I can also let me quickly change that to light mode as well.\nUm, but anyways, left side, there's a key which is called secrets, and then you enter a name which is Gemini API key and then the value of your API key.\nAll of what I walked through is also part of the 00, um, setup and authentication notebook.\nSo if it was too fast, you can like look up there, should be screenshot of it where I clicked on where to add it.\nIf you are running locally, um, you need to expose the, um, API key as environment variable with the same name, should be also part of our, um, notebook.\nSo in the first cell, so basically what we try here is we check whether we are in Google Collab.\nIf we are in Collab, Yep.\nYeah, we don't worry, like we go through it one, and then you have enough time to like five to 10 minutes to set it up yourself.\nOkay.\nQuickly that we, Yep.\nIs there any PowerPoint?\nNo, no PowerPoint, code only.\nUm, so we, we go through the API key setup in a minute.\nYou have plenty of time to do it yourself.\nAnd if you have questions, I'm very happy to come to your place and help you get it created.\nJust to complete the the setup.\nSo once you have created your API key, made it available to collab or made it available in your environment, best is to open the first, uh, notebook.\nIt has a super small code snippet in it which uses your API key and generates, um, uses Gemini 2.0 flash to generate a first string.\nSo our goal next 5 to 10 minutes is trying to get this working.\nOkay.\nAnd again going back.\nSo we have those QR codes.\nOne is for AI studio.\nIt's the left one.\nThe other one is for the GitHub repository.\nYou can also go to AI studio, uh, AI.dev to enter AI studio or go through Google search.\nAnd you can find the GitHub repository on my GitHub account.\nUm, it's like gemini.2.5.aiengineering.workshop.\nAnd, um, I will in the meantime change the appearance.\nYeah, sorry.\nSorry.\nUm, so there are in the GitHub repository, if you go to the notebook section, and each of the notebooks at the top, there's a button which opens collab directly with the notebook.\nOkay.\nOkay.\nQuick check.\nAre we are we ready?\nAny notes?\nOkay, cool.\nSo the first section will be all about the the default.\nBasically LLM started with being text only.\nWe generated a text.\nSo what would the first section basically covers, um, all of how can I generate text?\nHow can I, uh, tech generate text and have like a streaming response?\nHow can I count my tokens?\nIt's like always important, right?\nTo understand how many tokens did I use, how much will it cost?\nAnd there are like a few exercises for you to try out different models, try out different prompts.\nIt will also go a bit into detail on how the SDK works in terms of like which inputs you can provide.\nSo in the Google AI SDK, we have, um, this concept of a client, and client has the the models abstraction.\nAnd the model substraction has the method, uh, generate content or generate stream content, and I can also make it maybe a bit bigger, and, uh, each of the or it has parameter for the model, and the model ID is basically the Gemini model we want to use, which is defined at the top.\nUh, so all of those cells use the the same concept.\nSo all of the workshop section have the same.\nUm, if you think okay, 2.5 flash is not the right model for you, you can change it to a different model ID.\nIf you have like an paid account and want to use the pro version, you can also change it.\nUm, and, um, contents is basically our way to provide, uh, data or conversations, chats, and messages to, um, Gemini.\nSo the first, um, test basically is we ask it to generate three names for a coffee shop that emphasizes sustainability, and we use the client models generate content.\nWe have our model ID, our prompt, and then we get our response from Gemini.\nAnd, um, if you have already set up everything, you can try prompting a few things.\nUm, ask it to explain some terms or maybe like just change the model ID, and then we continue with, um, counting tokens.\nSo there are exercises in there, uh, which don't have any code snippets.\nThe solutions part of the workshop has the code.\nSo if you are getting stuck or if you want to look it up, uh, what I added, uh, feel free to take a look there, but, um, definitely try it first yourself if you want to get familiar with the SDK.\nThere are plenty of other snippets or exercises basically just to u make sure that you understand the concepts and, um, can practice it, and there are other cells which are partially done, so for example, the one we have here which has some code comments and also some to-do calls here.\nThe idea is really to to force you not to learn new APIs.\nSo next to the, um, generate content method, there's also a count token, um, um, API which we can use to count our tokens.\nSo similar to our generate method, we provide our model ID and then our prompt here.\nAnd basically what the, uh, API does is now it counts only the tokens for our prompt since we we haven't generated something.\nSo we can run it, um, and we get an input tokens of 11.\nSo the Gemini tokenizer basically converted those two, four, six, eight, nine words plus a full stop to 11 tokens, uh, which is then, um, an estimate of roughly 0.00002.\n[Music]\nUm, the count tokens API doesn't expose the pricing.\nSo basically what I did here is like looked up the 2.5 flash pricing and calculated it.\nUm, similar to only counting the input tokens, we often times also want to count the output tokens to understand, okay, um, how much does it cost.\nSo in the next, uh, example, we basically generate content, and each response has a very nice method which is called like an abstraction which is called a text, which allows us to actually, uh, easily access the generation, but also has a usage meta data object, and a usage meta data object includes all of our consumed tokens and generated tokens.\nSo we have input tokens, uh, we have thought tokens.\nSo Gemini, uh, 2.5 is a thinking model.\nSo before generating your response, it first generates thinking tokens, basically an abstraction where it uses more compute to have like more room to generate a good answer for you, and then also the candidate tokens, uh, which are the response tokens at the end, and that's how we can calculate the total cost of a request where we use the input, uh, token price and then our candidates tokens and of our tokens, and, um, for this case, it would be, um, less than like zero 2 cents.\nYep.\nYep.\nYeah.\nIs there a different price forken?\nNo.\nSo, um, input and output tokens are calculated different, um, as yes.\nSo we have prompt tokens is basically the input tokens of your prompt, and then we have the candidate tokens, which is the your response and the thought tokens, and those basically have the same pricing, and the output tokens are much more expensive than the input tokens because that's where the computation mostly happens, and the input is just one encoding.\nSo that's why you always see like the big difference in output input pricing versus output pricing more.\nYeah.\nSo for Gemini 2.0 flesh, which is our most cost effective and cheapest model, the input price for 1 million is 10 cents and the output price is 40 cents.\nSo you mean like why we got 639 for total?\nSo that's sadly not like directly visible to us.\nSo we can like look up thought summaries, but basically the model generates first of all like a lot of like reasoning.\nOf course, in our case, like we we ask it to generate a haiku might be not like the most sorry, most difficult, um, prompt.\nUm, you can control the thought tokens with something called thinking budget where you can limit how many tokens the model has to think or to reason.\nSo you have a some sort of a cost control, but it's basically done dynamically based on your prompt.\n\n\nOkay. Yeah, there's also a question. Yeah, I was just looking.\n\nYes. So, um, I can let me open the docs and make it easier. Um, so, um, Google Gemini 2.5 flash is a hybrid model. So you can use it with thinking and without thinking. And without thinking, basically, the computation is much more cost-effective. As, um, you might know, like the transformers is all based on intention, which isn't like and bionics, like kind of calculation. So it gets bigger and bigger, which means it gets more and more compute, and, um, without thinking, it's for us much easier or inefficient to run. So if you set, um, thinking to, uh, zero, or like thinking budget to zero, uh, you all you will have zero thought tokens, but you have like your candidate tokens, and those candidate tokens will then be charged with 0.60, 60 cents. But once you, once you use thinking, meaning a thinking budget greater than zero, you will, um, pay the price for the thinking tokens and for the output tokens. And that's where's the the $3.5.\n\nYes, I will open the documentation. So we'll see it in one second.\n\nFlag.\n\nYeah. So, um, in any case, if you have like any questions, the Gemini docs are a great way to like find the answers or like just ask Google or Gemini directly. And on the model capabilities, we have the thinking section, and they are like the thinking budgets, and if you want to disable thinking, you basically set the think it budget to zero. Uh, you can control it. It could be a integer between like 0 and 24,000, and then setting think budget to zero disables thinking. So that's your way to disable thinking. Is the budget like a number of reasoning to do?\n\nNo, it's tokens. So yeah, so we, we have, yeah, so we have seen in our example here, we had 600, a bit more than 64 tokens. So if we would set our thinking budget to 512, it would be a maximum of 512 thinking token.\n\nHow does it know?\n\nNo.\n\nOkay.\n\nI guess. Okay. Continuing, uh, with our, um, notebook. So, and please continue yourself, like, I'm like, you can do it with your own tempo, even like do it faster or slower, just to make sure that we are on like the same page. And I guess like the most interesting part about like streaming and like LM in generals, we all have seen it with chat GPT, is that waiting for the whole response is a very bad user experience, right? Like, who wants to wait like 60 seconds, two minutes for a response. So that's why, um, everyone now kind of uses streaming, and with the Gemini and the Gemini SDK, it's like super easy. So instead of having just generate content, we have generate content stream. Same input parameters, except that we now get, um, an iterator back from our, um, call, which we can loop over, and we can like print our, uh, junk or like stream it back to our user using, uh, the HTTP service.\n\nOkay,\n\ncool.\n\nAnd then similar to other models, Gemini is a chat model, right? And, um, maybe you are familiar with the OpenAI SDK, where you have the concept of messages, where you have like a different like inputs per user turn, assistant turn, user turn, and it makes it very, I would say, is still complex to manage yourself because you need to keep track of it. To make it easier, we, um, added something which is called a chats API, um, which basically does all of the state management, uh, on the client, but as part of the SDK. So you can create a chat, uh, with your, uh, model, and then you can basically send messages into the chat session, and the user, um, like in this case, it's like we are planning for a trip. Um, we send the message, we get back the response, but also our chat session includes the user prompt and the assistant message. So instead of needing to create this object of user terms and model turns, we can directly, uh, continue with like sending our next message, um, asking, um, for some, um, good food, uh, recommendations. And since we are in like a conversational setting, the model knows that we are, um, or like it mentioned for us to go to different European cities, and based on like the next request, it uses like the whole like conversation as history, um, to get our response. Uh, can like also quickly print the response here and get some different examples. And then if, of course, if you need to store it to a database on general, um, you, we have a nice get, um, history method available, which allows you to basically retrieve the complete current state, and you can like store it or update it or whatever you want to do. Yeah,\n\nthat's only a client abstraction. So the back end receives the same request if you would send it like as a single request, uh, with an array. It's only a client abstraction to make it easier for developers, people to quickly build.\n\nYep.\n\nAnd then, um, similar to OpenI or to other models, you can like give the model some kind of system instruction to have it behave differently, responded in a different language, make sure it respects like policies or guideline you provide. Um, this can be done through a generations, uh, config. So we have another argument now in our model call, um, next to the model ID and the content, we have a config that we can provide our systems instructions, and similar to the systems instruction, we can provide, um, other, um, generation configurations. So temperatures can be used to make the generation more creative or more deterministic. So if you, for example, build a retrieve lock manage generation, where you want the model really mostly trying to use what you provide as a context, there you would normally set the temperature to a very low, um, value. If you work on some content writing, marketing, you would set the temperature to a very high value. We can control the max output tokens to make sure that we are not exceeding some budget, some length, and top P and top K are also ways to make our generation more diverse.\n\nYeah.\n\nUm, so let me open. So here we have the similar in the config, we have the thinking config, and in the thinking cons, you can basically set the thinking budget or also include the thoughts, uh, in your request.\n\nOkay.\n\nAnd then, um, I think what's very more unique about, uh, what we can do with Gemini is that we have direct, direct support for files. So in this case, I download the adventures of Tom Sawyer, a book, completely store it in a file, and I use the files, uh, API to upload the file to a Google Cloud, uh, storage bucket, um, there, um, free for you. So like, if you, uh, don't want to use your own corporate bucket or whatever bucket, with each AI studio account, basically, there's your personal U bucket, which stores the file for, I think, one day. But you can control the the time to live, and, um, instead of, um, needing to provide the whole file with your request, which can be very, um, intensive, you can upload it, and then instead just provide the reference to the file, and what, um, the Gemini API does behind the scenes. Basically, it downloads the file on the back end and makes it available inside your prompt. And similar here, we can, um, we uploaded our book, we passed it into our contents, uh, list here. So we don't no longer have like a single prompt. We have now an array with our file, and we ask it to summarize the book, and was also done while I'm talking. And then we can also see, okay, the token usage now we had for was 100,000 tokens. So much bigger than what we tested before. And using the file API makes it very easy, uh, also to work with with PDFs, which we'll, um, see in like the next chapter. And then, um, as an exercise for you is, um, to combine a bit all of those things. So how can I use a book to use the chat session to chat with our model to help me better understand it? Yep.\n\nIt's not system.\n\nYep. So what you do is basically you upload the file from your client to the cloud into a bucket, and when you send the request, part of your, I mean, I can like show it. Um, part of your request, uh, would be only the reference to where the file is stored, and what the Gemini API does behind the scenes is it loads the file into where the request runs and then puts it into the context. So you can use now this file pointer for all requests, and you don't need to send it every time.\n\nAnd here we also have like the URI. So that's basically where, um, our file, um, is stored or can be accessed as well. Yeah.\n\nSo is it only available to the chat?\n\nTo itself. It's only available to your user. So when you send a request, you send an API key, and this API key is basically used to get the file. So nobody else can access the file.\n\nYeah.\n\nOkay. Cool.\n\nYeah.\n\nSo if you would use a PDF which has like more than a million tokens, basically what would happen? You would receive an error most likely, uh, with like saying that, uh, the file is like too big in terms of like a context. What you can do is basically you can use the the file, uh, the file to count the token. So we have the file now here, and we can client, uh, models, count tokens.\n\nYou would need to chunk it then. So if it the file doesn't fit, it doesn't fit, and you would need to like think about, okay, can I chunk it? Can I summarize it? Can I maybe do use other techniques to first extract the important information, and once I have context which is, um, smaller than the maximum context of my model, you can provide it again.\n\nYeah.\n\nNo,\n\nfiles upload.\n\nNo, no. So, it's like, but it will be deleted. So, don't expect the file you upload now to be there an hour or a day. Um, but you can like use the same concept with, uh, Vertex in your own bucket, where you have like more control over it, where you can say, okay, maybe I want to upload it using a different API call or already have it available, that also works.\n\nSo there's an entry point saying use this storage bucket with Vertex AI. Yes.\n\nYes. It's roughly the same, but you need to set up the client differently.\n\nY\n\nYeah.\n\nYeah. Good question. So I mean, we can maybe directly jump into like the the the this the PDF section. So, um, continue with section one or jump to section two or three directly. What I will do is like in the section two, which is all about multimodality, meaning we will cover, uh, visual understanding, audio understanding, videos, and document processing, and that's where I will like jump to. Um, and part of the, um, okay, being connected, part of the, uh, working with PDFs, basically, is so similar to what we have seen a minute ago. We have a PDF, in this case, it's basically an invoice from a supermarket. Um, I upload it, and I ask the model like, what's the the total amount? Uh, we can run it, and what happens behind the scenes, I can show, oh, it's not here. Wait one second.\n\nOkay, we don't have the file here. I'll upload it quickly. But what happens, um, behind the scenes is we run OCR on your, on your PDF and provide the PDF as image. Um, so you don't need to do it manually. So there's no like, hey, it's a PDF, let's convert it to an image and then run OCR, and then I provide the image and the OCR, that's not needed. Um, we are doing it for you.\n\nOkay.\n\nYep.\n\nYep. Yep. Yep.\n\nYeah. So,\n\nor like the image understanding is not perfect yet, right? If we reach a point where the model understands it like in the same way as without the text, then I guess there's no point. But like based on like what we have seen and also what the industry does is you receive better results when you provide the OCR plus the image. Um, yeah, that's that's basically it.\n\nY\n\nthe PDF itself,\n\ndo you actually look at it as an image?\n\nYeah.\n\nNo, I, I think I, I mean, I don't know exactly, but I think it's just basic OCR, nothing special, no magic, and then a screenshot of the PDF.\n\nYeah. So it's both. Okay. Can try again.\n\nAnd we have now the PDF available in case, um, the workshop has multiple sections with files which are being part of the repository. So if you run into like a similar issue, especially for the image understanding part or the audio understanding part, uh, and you use collab, you might need to download the files manually and then upload it. But in our case, so we have now our invoice. I think we can like quickly show it.\n\nSo I was shopping in Germany. We have a co supermarket called Rebe, and I basically bought some butter, um, some bread, like some sweet potatoes, and, uh, we prompted it and asked, okay, what's the total amount? We can see here the total amount is like 2020. Now let's see if we got it correctly. And we got it correctly. And it also correctly extracted it in German, even if I prompted it in English, which I think is like pretty cool. Okay, let's start with the image understanding part.\n\nYeah, please.\n\nYeah.\n\nI'm not exactly sure what happens. I only know that by defining the thinking budget, you can limit how many, how many tokens will be used or generated as a maximum, and very similar to what OpenAI has with like low, medium, high effort. We have a bit more granular control. So we could technically do the same, say like low would be like a thousand token thinking, medium would be maybe like 12,000 token thinking, and high would be 24,000 token thinking. Uh, and it would then use like at a maximum those tokens to before generating your response. But what exactly happens I can tell you.\n\nYeah. So without thinking, what we have seen, especially on more like math type of question, where the model benefits from like the reasoning, uh, the performance is a bit worse, but for like general everyday use, especially image understanding or like OCR, you can like easily run it without it. The truth of it would be you need to try, and I think the the real benefit here is that you have those granular control. So you can run evaluation of thinking budget zero, 1,000, 2,000, 4,000, and see how it impacts your evaluation, and then you can like calculate for yourself. Okay, how much am I able or like what's my my maximum cost of it and like what's the the accuracy I need to reach.\n\nOkay,\n\nmore questions or should we continue?\n\nThere's a question. Yeah.\n\nNo, it's like, uh, there's a documentation for it, but it's like, uh, JSON PDFs, all different image types, all different video types, all different audio types. So, all of the multimodal, um, features we support. Um, if it gets a bit more specific with like JSX file and view files for like web development, we are working on it. But it might, I mean, you will see an arrow, and the easiest way is to just replace it with a doc txt.\n\nNo, I would say like those are like you would need to like use like mark it down or like another library to convert it and then or like copy paste the input and\n\nYep.\n\nSo I'm not exactly sure what the researchers did. The only thing I know is that we get better performance when you provide the image plus the OCR.\n\nSo I guess there's a benefit of having both.\n\nsaw.\n\nYes, kind of. So, the UI or the AI studio, of course, doesn't use Python. Um, but it calls the same APIs behind the scenes. So, the API behind the file upload one is the same API\n\n\nWe call from AI studio. We call the same exact model. Both have the same exact parameters. So you should be easily what you test and experiment in AI studio. Can you convert into code and run it locally? There's also this get code button. So if you are AI studio, and I mean, we can quickly try uploading our invoice again and acknowledge ship and use our prompt. So here and now we run basically the same request, and we can now we also have this, it's a bit hidden. It's like this code button at the top, which where you can get the exact same code. Basically takes a few seconds as the Wi-Fi is super bad, but here you get like the exact same Python code where you create your client. We have our model, in this case, as we uploaded it manually, we provide the document not as a file URI, we provide it directly as B64. We have our prompt, we have the model request already since we generated it, and then you can continue.\n\nOkay, cool. Yeah. Yeah. So, I mean, we can quickly so try it. So, we are here. We have our PDF. I think I guess it's very interesting to know like how many tokens will we use. So, let's quickly count tokens. So, if we go here, we have our count tokens, and now we use the same contents. We have the same model ID, we have our prompt, then we have our PDF, and let's print our token count. And alternatively, what we also have done, so if you use the response, so you run a request already, you should have access to the respage metadata. Yes. And so we have our account token. So our PDF here is converted into like roughly 500 tokens. And the prompt we have is like around 20. And if we compare it to like the request we run, we see, okay, we have the same exact amount of prompt tokens, and we have prompt details. We have our output tokens for tokens of 42 and candidate tokens of 78.\n\nOkay, more questions or yeah. Okay. 560. I think we don't charge for the OCR. I think one image is roughly 500 images. Uh, like 500 tokens. Um, not for you. No. Okay. Yeah. So the PDF is converted into an image, and we provide the image, and if the image or like the PDF has tables, visuals, mind maps, the model is trained on similar data. So it will definitely understand parts of it. I mean, we can we can try it. I mean, maybe we can like search for some mind map and ask it something. Um, you maybe you can start thinking about a prompt while I'm searching for a mind map if Wi-Fi allows us. Okay. Okay. We have our mind map. I guess it's just a mind map on how to do mind maps. Um, any idea what you would like to know? Sorry. Like this. Okay. I mean, I'm opening it, but I think on the small scale, it looks correct with how to mind map answers perfect.\n\nof the table. Yeah. So where we have seen the most success is that when you like separate a bit. So there are already like very good existing methods which allow you to extract tables or other visuals from documents, and when you then like work with those images and tables directly and not like you the way how we work changes from like previously we provided a table, run OCR, then ask our question, now we directly ask the question based on the table as the models get so good in like the multimodal understanding that it can combine like the different aspects. I mean, we can try it maybe as a good example as well. Um, maybe we find some nice, I don't know, invoice image or something, and then we can like ask it maybe to add something or to combine it, which would be very hard, I guess, for a normal model, but in general, like especially Gemini is so good with like the multimodal understanding. It also videos is like that's like my most new favorite thing is like take a YouTube video, which is below one hour, put it into AI studio and like have it summarize it, or if you have like any specific question, like it it's so much faster than like sitting there even like watching it in like two speeds. You get your response in like 80 seconds or something, and you can like even like ask it to extract specific timestamps on when somebody something was set or to help you like section it very easily.\n\nSorry, can you speak a little bit louder? I don't think so. So, AI studio is like very developer centric, and we don't want to do too much black magic. There are safety filters and control which you can configure in the SDK or in AI studio, and in studio is under advanced settings. I have safety settings. I have all of them off. But if you like want to filter on very explicit content or hateful content, we run some classifications basically before and after to make sure that you are not creating for your users harmful content. Okay, I have our invoice image. Okay. Okay. What should we ask? How much it would cost if we subtract the pedal? Maybe. Yeah. the performance.\n\nSo I sadly don't have an answer for this. I think you can always think a bit about like would we as a humans have struggled to understand those PDFs if they are like switched up. If yes, then most likely the model will do as well. I think one very nice part about Gemini AI studio and the Gemini API is you can get started very quickly, like all of us kind of set up within like 20 to 30 minutes, a free account with access to Gemini 2.5 flash via API 2.5 Pro in the UI. So the best thing always is like to test and to explore and evaluate, and even if you need to run like a thousand PDFs, it's not very cost or like expensive anymore. So really best thing is to run your own evals to get some more than like I try five PDFs in the UI, really look into it, and if you have like any questions or problems, like best way is to reach out to us. We have teams helping and building with customers, and then we can like iterate on it together. Going back to your PDF, any question, any prompt idea?\n\nYeah, I know you explained about security, but there are mandated security by law which are fire or benchmarking automations or guardrails posture management mandated by CISA. So when I implement those rules for either defense or financial applications, how do you just set I know that Google is pretty strong in setting up those security measures from a different perspective, too. How do you integrate this app and the other security measures? So I guess for those type of environments where you have a lot of like compliance regulations, best is to work with Google Cloud. So everything we do in AI studio is also somewhat similar available in Google Cloud in Vertex AI, and Vertex AI provides more features for those kind of use cases. They I'm not exactly sure, but they definitely have more like information they can provide on how to handle all of those things and those guardrails with Gemini so that you're suggesting to go global Google GCP environment when I go global on GCP environment, then there are c performance and cost being here such as so there's a guard for bas and there's a guard for organization based and there's a based on the industry industry based. So we have to split those platforms into from most grand level. So that's when I do it globally at GCP levels, more expensive and as well as can be a performance issue. Yeah. So I know that there are regional endpoints for Gemini and Vertex AI as well. And also at Cloud Next, they announced a new Gemini on device kind of thing where big companies can buy basically a huge box where Gemini is pre-installed and it gets delivered to your environment. I'm not exactly sure about the details. Easiest is like to to do a quick Google search and and look for it. But those is exactly where Vertex provides you more features and support than AI studio. Yeah, thanks. One of those last 2025 conference, someone from Google also represented, and that's a good platform, but I didn't know how to this is granular level. Yeah. How do you most granular level to the global level?\n\nOkay, maybe back to your PDF question regarding what to ask. So instead of like asking what the total sum was, I asked it to sum up the unit prices, which worked very nicely. Of course, it's a very well formatted PDF in this case, but the image understanding is like very good. And the new way of how we should think about it is like I should ask the question directly based on the image before doing too much of processing we we have been doing in the past. Cool. Okay. We are now at like half time almost. I guess it's time we move maybe a bit away from all of the multimodality part into more about the I guess agendic parts, which are I would say definitely more interesting at least to me, especially if you combine in with like the multimodality parts. So um, okay, um, so part three is um all about structured output and function calling. Do you know what structured outputs and function calling is and like how it roughly works? Any hand signals? Yes or everyone? Okay, not bad. Um, okay, so the part three um goes continues with PDFs um as they are kind of very interesting and also um we want to do structured outputs. So structured outputs is for us a way to create more structured data structures from text. So which we can use to work way more easily afterwards, right? And at the end we prefer structured output much better or much more because we can integrate it into other APIs. We can connect APIs and Gemini supports or the SDK supports pyantic. So pyantic is a very nice Python library which lets you create those data structures and also we can create n nested data structures. So here we have a recipe with a name ingredients, which is a list of strings, and then we have a recipe list, which is basically a list of our recipes, and we can provide it in our configuration. So similar to the generation arguments or our thinking budget, we have a response type and a response schema we can provide. Here we ask it, okay, can it generate two popular cookie recipes for us? And we basically force it to use our structure, and I already like nicely printed it here. If we look at the raw response of our model, we get back a JSON with all of the different input fields. And there's also a nice pass method which allows us to convert it back into our benic schema. And then we can access all of the the data points. And as we had our invoice, we can now maybe like like I complete the exercise with you. Um, what if we like so we ask about the total amount, right? But when working with PDFs, normally we want to have structured data as a result, right? Text is not very helpful for us when we want to put it into a database or want to work with it. We really need those data schemas, and what is very nice about Gemini is that we can basically combine both of it. So we use our structured output method with our multimodal capabilities for files, and we can provide our file. Oops. Okay. Oh, we need our invoice. So maybe it's a good example. So I didn't change the recipe from our like data structure what which we want to create, and we ask it to extract the information from our PDF. Our PDF is an invoice from a supermarket. So it doesn't have a recipe name ingredients. So Gemini did not generate or hallucinated something. So we get back a empty recipe list. So if we now change it to our invoice data, we should hopefully then see the correct extracted information. Yes. So, we extracted the date, all of the items we bought, and all of the different prices. And with that data, now it makes it much more easier for us to work with, right? If I have some kind of automated system where I need to like take in invoices, any kind of like PDF document, I can now provide a structured to what information I want to extract. And Gemini does basically all of the matching for us, which is like super nice. And function calling basically is the same idea, but instead of having like a data structure, our output is a name and the argument. So similar to what we have we we create this case, it's like a function declaration. It's a structure of like how a function signature is done. So we have a weather function which has a name, a description, and like the properties which we need to provide. Um, there's the same function, but just as Python code, and with function calling, we provide the function declaration and our prompt, and then the model generates aruct a structured output which has the function name it wants to call and the input and environment or the input arguments for how to call it. So in our case, we have a weather function. We only have one location. Um, we provide it similar to all of the other configurations in our configuration argument. This time we have tools, and we want to know what the weather is in Tokyo. And obviously what's the weather fits into the description of our function as it helps us retrieve the current weather. So if we run it, the model instead of generating a nice response wants to call the get weather method and with the location Tokyo. If I change the prompt to hello. Oh, makes sense. Uh one second. So if I change to hello, we don't have a function call, right? Like the model correctly understands, hey, that's just a greeting, let's response and like how can I help you? So, but we want to call a function, so we have what's the weather is in Tokyo, and then the next step is for you as a developer to call the function, right? The models cannot call or invoke directly the function what you would normally then have in your code or in your applications a way to identify, okay, which function is false, could be a simple switch statement to check, okay, what's the name, and if you get a name, call the method with the provided argument, and then what you do is the output of your function is the next user input. So the model generates this name and arguments object, and we provide it as a user the output, and in our case, it's the result, and if we look at the weather method, we have it's basically some dummy data about the temperature, the condition, and where it is, and then the model generates a very nice response. So we have user input, model has a structured output, user provides the a structured response, and then the model generates a very nice user-friendly response. So we call our function, the function returns the weather, and then the model generates a very nice response, which is the weather in Tokyo is sunny with a temperature of 22 degrees CC and it feels like 24 degrees CC. So you can think about it. Okay, that's how I can integrate tools or make it or convert my LLM into an agent more or less or a way to call something. Um, the get a weather method can be anything. It can be a database call. It can be a real API call. It can be, I don\n\n\nI don't know, reading emails, sending emails, all of the things we currently see with all of the MCP hype basically going on.\nAnd MCP servers have tools as well.\nAnd those tools of MCP servers basically expose the same um declarations.\nSo an MCP servers has the tools def defined get weather, for example, if it is in weather MCP server, and it has an endpoint or a method which you can call, which is list tools, and this list tools method would then return the schemas of our functions, which look very similar to what we have created here.\nAnd then what you do on your LLM side or client side is you take those schemas from the MCP server, provide it into your LLM call, and then the LLM generates, depending on the context, uh, the output as well, which is structured, and then instead of calling the method on the client as we did with our get weather, you would use the MCP client again and then call the remote tool.\nSo very similar to what we have done here on the client side manually, but more abstracted away and more managed.\nAnd of course, the benefit here is that not every one of us needs to implement the get weather method.\nIt's way easier to use like the weather MCP service from, I don't know, some weather provider.\nSimilar um, you don't want to use maybe your own personal Google Drive MCP server.\nThis would be if Google creates those.\nSo that's the whole idea why MCP servers is kind of so cool.\nSo and um, yeah, yeah, yeah.\nSo currently we are working on improving and extending um, but currently the suggestion is probably between five to 10, and um, if you have more tools, you can use embedding models to basically filter.\nYou have the user and what you would do is basically run some similarity matching between the descriptions and like what makes sense, what doesn't make sense, and then you only um put the top tool, so to speak.\nI'm not sure about the claim from Andropic and like how you do it.\nThe only thing I can share is that Google Gemini 2.5 Pro was the first model to complete Pokemon Blue, uh, which ran, I think, for like 200 hours straight.\nUm, the only like the big challenge here is like, so we have like a limited context, right, which is for Gemini, 1 million.\nSo if you would continue for two hours, you would definitely run out of like those two like 1 million tokens and entropic.\nI'm not sure what that context currently is, but I'm pretty sure two hours is not enough to run out.\nSo what you most likely do is you summarize, you compress the context and the conversation and what you provide to your model.\nSo yes, I'm pretty sure Gemini can run for more than two hours, but it depends on like what you want to solve and how you are going to solve it.\nYep.\nIs that correct?\nYeah, that's correct.\nSo native tools are the next section.\nWe can go into it um in one second.\nAnd yes, the currently the native tools are not like being returned in a way we had with like user assistant, that's basically happening behind the scenes, and what the user gets is the final uh good assistant response.\nUm, that's great feedback um regarding like can we have them or not, like very happy to take it to the team.\nI can definitely see why it would be helpful for you for like people to directly build with it, um, but for now it's not the case.\nAnd speaking of native tools.\nSo um, Gemini can is basically trained to do native things.\nSo function calling is very generic.\nWe generate the declaration and can try to do everything.\nUh, but native tools are much easier to use as you don't need to define or create a declaration, and they work basically on the backend side.\nSo you don't need to execute anything.\nAnd as native tools, we currently have Google search available.\nUm, basically all of the native tools are in AI studio here.\nSo we have structured output, which is not a native tool, which we used to get back the structures.\nCode execution is a native tool, and basically means that we or Gemini runs code for us.\nSo uh function to sort the top five cities based on population.\nSo what it can do is like run Python code for us.\nSo if you prompt it to solve a task using Python, um, it should run for us the Python code.\nSo it generates the Python code and it did not run it.\nThat's a bad example.\nSorry.\nOh, did I?\nAh, so my bad, not Gemini's bad.\nUse Python.\nLet's see what it does.\nUm, okay.\nYeah, now it gets.\nThanks.\nPerfect.\nSo, we have some reasoning and then it generates executable code.\nExecutable code is also provided via the API, um, which runs for us Python.\nSo, it writes the Python script.\nIt executes the code, um, and it generates the mudplot lip chart.\nAnd normally we okay in the notebook, we I have an example available on how you get the chart.\nIf we look into the code execution tool.\nSo similar story here, we run it and it returns the markdown, and you can also like it can regenerate and return um images.\nSo next to code execution tool, there is the URL context tool, which basically allows you to provide a URL as part of your prompt, and we extract the information from the website behind the scenes and make it available into your uh context.\nSo instead of going to a website, command A, command C, command F, uh, we do it for you.\nSo in this case, I asked it, okay, like what is the other benefits of Python, um, from like the URL, you can provide up to 20 URLs in one request, behind the scenes, it goes to the URL, extracts the information, provides it as part of your prompt, and if we look into our prompt, yeah, here, uh, it returned our nice smart plot plot um chart, uh, which is very cool.\nAnd then a final tool is of course Google, and Google search kind of makes sense.\nUm, so let me find it.\nYeah, you can allow or enable Google search, which then um allows Gemini to use or do a Google search behind the scenes.\nAnd what happens here is that it takes our prompt, um, in this case, what are the latest developments in renewable energies, and first it converts it into one or multiple Google search queries.\nThen it executes those those Google search queries, provides it back to the model, and then the model generates your final response from your user input and from all of the search results.\nCurrently we don't, as mentioned, don't export or expose those tool calls, but um, I guess especially helpful or interesting for um Google search is that we have uh a crowning metadata object.\nYes.\nSo we have crowning support, uh, which basically allows or points exactly to where it got or like which information refers to which um source and also grounding meta information which websites there is um which websites were crawled and in our case, so we have the yes there.\nSo um for both code execution and crowning and URL context, we increase the context size, right?\nThose contexts will be built, uh, tokens will be built, uh, for Google search, um, there's a free tier from, I think, 1,500 Google searches, which are free, and then afterwards, I think there are $35 per 1,000 searches.\nUh, code execution, the Python running is free, um, well context text is currently in preview.\nSo I'm not sure what the if and what the pricing will be.\nUm, yeah, and I think like what's what's very cool and um is part of the exercise if you already have done it or going to do it later is you can combine those.\nSo you can use the Google search tool with the URL context tool or the code execution tool with the Google search tool to basically have it done more agentically, like first search for like what's the latest react or Python version, then write a Python script and run it and return it.\nUm, which makes it very nice to use.\nSorry.\nYeah, we have heard like a few people would like to have a deep research API.\nI think the more people ask for it, the more likely it will going to be.\nUm, maybe something I can like plug and share is so I um as part of like our team, we um yesterday we open sourced an example for how you can build your um your own deep research using Lancraft with Gemini, very similar to uh right, very similar to what basically the the whole deep research agents do.\nWe have a question, we generate queries, we then run like multiple web searches and then like reflect and see, okay, was the user question already asked, do I need to do more research, and then you have this kind of loop for, okay, do I need to search other tools, uh, it's completely open source, it uses all of the things we have currently seen today, Gemini 2.5 flash, Gemini 2.0 O.\nUm, so if you want your own deep research, that's probably the best way to start.\nYeah.\nSo, so there is additional pricing for the Google search.\nWe had it like two minutes ago.\nBasically, 1,500 uh searches are free.\nIf you use the native tool, then it costs money.\nAnd all of those tools in a way enrich your context.\nSo if you use the URL context tool, which goes to a blog post and the blog post has 10,000 tokens, you pay for those 10,000 tokens as it's included into your prompt and then tries to answer your your question.\nFor like the function calling we had, it's a bit different.\nOf course we generate tokens, but like the structured output tokens are less, uh, and you do the function calling or like the the Python call on your side.\nYeah, so that's definitely a good question.\nSo function calling, of course, is much more than one tool and never going to be calling the weather API because I mean we have weather apps, right?\nUm, but it's a good example to show, but what's very cool is, so there's parallel function calling.\nSo uh, we are in the Gemini documentation again, and let's assume you want to have a party and you have function to like power your disco ball, pole ball, start the music and dim the lights, right?\nIf you like want to say, okay, set us into a party mode, you would expect all of them to start at the same time.\nAnd uh, with Gemini, we have parallel two calling.\nSo basically what Gemini would do, instead of generating one uh object as an output, it generates a list with basically three objects for all of the functions you need to call, and then you iterate uh over those and call them.\nParallel tool calling works if the inputs and outputs of the tools are not dependent of each other.\nRight?\nI can start my disco ball and the music at the same time and not okay, I first need to start my disco ball and if it runs, I can like start the music.\nUm, if you have more of those sequential two calling, um, basically you need the input uh, sorry, you need the output of the first function for the input of the second function, basically if you, I don't know, you have like some kind of smart home system, um, and you want to set the temperature based on like the outside weather, you would first need to check what's the weather and then like set your temperature inside of your house.\nUm, here would you basically provide instructions uh using the system prompt to your model to say, okay, um, to change the weather, you first need to look up the weather and then set a temperature, and what Gemini would do, basically you have your user prompt, it generates the function, the fun structured function, you call the function, and instead of generating a user friendly response, it generates another function call, and then you can call it, so basically it continues this function calling loop before generating a very nice user friendly output.\nYes, kind of.\nI mean, I can like So, there's I put up an example uh for like the sequential um function calling.\nUh there is multip function calls and some sequential and some workflow something.\nYeah.\nSo currently there's not a good way to handle like functions who could time out or like the best way.\nSo it's a a bit like I would say parallel calling is like you need to wait for all of the results to put in into the next one.\nIf one takes longer, you can basically provide an error message or not ready yet or something.\nBut you would need to to explore how it works on the live API, which is basically our way to create real-time agents.\nThey are working something called asynchronous function calling where um the conversation continues.\nSo when you think about a customer support agent which you can which you talk to, right?\nIt would be very weird if the agent stops for like three minutes and doesn't say something because it needs to look up your information.\nSo that that's what they call asynchronous function calling.\nSo you can continue the conversation with the agent, but the agent runs a two call and then like injects the response uh later, but that's only for the live API.\nYep.\nSo um documentation live API um it was launched on I think in IO or cloud next.\nUm, so yeah, there there you have like a tool use with life API and there's asynchronous function calling um with code snippets for for Python and JavaScript.\nCool.\nYeah.\nNo, it's it's more like I start a function call now.\nI need to continue my conversation.\nI get back the response later.\nSo instead of like getting the response, what you get back, yes, what you get back from the model could be for example interrupted or scheduled so that you as a developer know, okay, I started something, the model knows it started something, and then you can inject once the your function is the output is ready, put it back into the conversation, and then the model uses that information to continue.\nY\num so in AI studio, we don't have a feature like this.\nUm but you can like use third party tools like lang or arise AI Phoenix uh but Vortex AI supports those features.\nOkay.\nYeah, that was question.\nYeah.\nYeah, I don't know.\nThe only thing I know is that so previously there was only Gemini 2.0 flash available in the live API.\nBut now we have Gemini 2.5 flash.\nSo basically the model we used in like the Jupyter notebooks.\nUm I guess you have to try or maybe like reach out to the people who are working more with it.\nOkay, more questions.\nYep.\nSo, URL context r tool basically works based on the link you provide.\nSo, if you provide your own personal blog, it goes to that link and tries to extract the information and uses it in the context.\nAnd search basically uses Google search, creates one or multiple search queries based on the prompt, searches, and then provides the outputs into your prompt.\nSo if you already know where the source is for your prompt to create a best answer, you all context could be a good use case for it.\nI don't know.\nYou pay wall content.\nYou mean like pages?\nI don't know.\nI would not expect to have it ex like, but the live API supports function calling.\nSo if you have a subscription or like a way to access this paid content, you can create your own function which the model can then invoke and use as a context for for its um information.\nOkay, cool.\nThen last section, I guess every one of you might have heard about model context protocol by now, and that's like either our solution to how we all align on how to call agents or like what's the right thing to do.\nI'm a big fan of it.\nUm, I think it makes it way more accessible for people to build agents, especially if we get more of first party remote MCP servers, um, where you can basically focus on building your agent instead of creating all of those functions.\nI mean, I'm pretty sure there are like now a million get weather functions which can be used, and I hope with um MCP we can like\n\n\nFix this.\nUh, what we, um, shipped and announced at Google IO is, um, native integration of MCP servers inside the Google Gemini SDK. So the SDK, which we have used during the whole workshop, um, allows us to directly use MCP servers and sessions, which makes it even easier for all of us to integrate it. So when we, um, look back at our function calling example, we needed to call the function, create a declaration, and all of those different things. And now with the MCP, uh, integration, we can basically, uh, only start or initialize our client here. I created an MCP weather service, which kind of has the same functionality. And, um, all of the things we now need to do is like, we use our generate content method, and in our tools argument, we provide our session. So we start our server, we create our session, and we provide the session to, um, the Gemini SDK, and then what happens behind the scenes is basically the same function call loop we did manually. It's like, okay, what are the tools available? Get all of the tools from the MCP servers, put them into the LLM call. If the LLM call makes a function call, extract the function call, call the MCP server, get the response from the MC MCP server, put it back into our conversation, have the model generate final response. Um, I guess it's, um, probably time to find out if Collab has node installed and to see if it works. Okay. Okay. It doesn't. What I can do is I can quickly set it up locally and show you in one second on how it works. Okay. Okay. So, we are back. I'm in cursor. Same notebook, same setup. And now I can like use it. And now basically what happens if I, let's not ask about London, right? We are in San Francisco. Um, let's ask it about the weather in San Francisco. It now does all of like the different function calls and loops behind the scenes. The MCP server is very simple. It uses the open Meteor API, which is kind of free to use on a small scale for testing. Um, so we generate the output, run all of the MCP calls, might take a bit longer for the API call, and then we get back, okay, the weather in San Francisco today will be 70° Celsius at zero, and then all of the other numbers. And that's basically now all you need to combine or connect an MCP server with the Gemini SDK, which is like, I mean, it fits on a single screen, like, which is easy enough for you to get started. Um, we use here like a local running MCP server, but it works the same way with a remote MCP server. That's, um, also the exercise, uh, for this, um, part of the workshop. So basically, Deep Viki from, uh, the cognition AI guys, Hinder Devon, they had a, have a very nice remote MCP server, which, um, can talk to GitHub repositories. So instead of like creating a sitio client, you can use the streamable HTTP client and connect a remote MCP server to Gemini to talk to it. And, um, yeah, basically benefit for one of the advancements currently happening in AI. Do we have more questions? Yeah. So ADK is, um, for those of you who don't know, is like agent development kit. It's an, uh, agent library, which adds a lot of more abstraction on top of like the client, uh, SDK we used. It makes it a lot easier to do all of the tool calling. Um, integrates MCP server as well, makes it much more easier to manage like multiple agents or deploy it to cloud. So it has a lot of, like, I would say, batteries included. Uh, question again depends on you if you would like to use, uh, frameworks or rather prefer building it yourself. I see a bit benefit of like getting started very quickly using agentic frameworks, but the more abstraction you add, right? The more, the less you know on the first time, and then maybe you need, you need to dig a bit deeper later, and all of the advancements or speed you get in the beginning will be your slowdowns in the future. But ADK is definitely a great way to start. They also have like tons of examples. It has support for Gemini and the Gemini API. So definitely a good way to take a look. Yep. Um, no, I don't think so. Uh, but what you can do with YouTube links or YouTube videos is we can ask it to return timestamps for you as the response, and it works very nicely. So it is very accurately because, so basically a video is more or less just many images behind each other, right? And currently, uh, when we process videos, it will be done at one frame per second, and you will always have like the timestamp, the image, the timestamp, the image. That's how Gemini exactly knows at what place in the video is, I don't know, a man jumping or dancing or something. It's analyzing one frame per second. Yes. It depends on your, I mean, we can, I mean, it's part of the section. Um, I use that all the time. Cool. Cool. Yeah. But yeah, basically what happens with videos is, I mean, videos are 24 frames per second or even more, that like easily lets the context explode. So currently what is done is that is we have one frame per second. So if you have 60 seconds, it would be 60 images. Uh, which makes it also easy to count how many tokens it would be. And that's also how we can fit in, um, a 1 hour long video into 1 million tokens. Basically using the transcript of YouTube or No, it's, it's all multi-modal natively. No, I mean, you can like easily upload normal .mp4 MP4 files, and you can ask it like what was say or said in the video. Okay, cool. Yeah. Yeah. So I'm also like, maybe for you, if you don't know, is like memory is basically next to tools, what makes an agent an agent. And for memory, we have short-term memory and long-term memory. Short-term memory is basically your conversation, which is part of your current, like, state when you talk to an agent. And long-term memory is basically memory or like information about a previous interaction or something about the user. And long-term memory is not like part of the LLM, which we need to provide externally. And me zero is like something I looked at it because it's very nice. It's does it implicitly. So basically, it takes the conversation and tries to create a nice abstraction for you, which you can include. Um, I'm not sure like what the current state is, like, how or what works well with Gemini. I'm like, what I always see or where Gemini really shines is the long context. So the better your information extraction is and like what you provide to the model based on the memory, I think it works very well. Not sure about those tool calling memory kind of systems. Yep. Yeah. Yeah. So we are working on 2 million. Um, not sure when it is like generally available, but yeah, like there that was research, but you know, like the bigger you go, the more expensive you get. And do you want to pay like $50 per 1 million tokens? Yeah. More questions or everyone working? Yep. Then before functions. Yeah. Yeah. No. So the difference with MCPS now and what we had previously is that we don't need to write and define the functions and the function declarations. The LLM sees at the end the same exact, exact, exact same thing. So if I create my function declarations manually with a JSON schema or if I retrieve them from an MCP server, those are the same things for the LLM. It's just makes it much easier to not need to rewrite the same functionalities over and over again. Right? If you work at a company and you want to integrate APIs which you have internally, there's a pretty big chance that two teams write the same function declaration and the same rapper to call it. And the idea here is to only have one team needing to write it and everyone to benefit from it. The same is with like public APIs like Google Maps, G Drive. So MCPS gives us a way to collaboratively create like the best or like the standard as a way to how to call it, and then everyone kind of can implement the MCP tools, uh, which you need for your use case. It's so an MCP server, for example, exposes four tools. Um, you provide all of those four input schemas, which are like function declarations with a name, a description, and the parameters to your LLM call, and then based on the prompt and those declarations, the LLM, in this case, Gemini, decides if it should call a tool, which tool could to call, or should it call multiple tools, and that's the same logic we had for normal or have for normal function calling. So there's no, no difference in that way. The only difference we see is that MCP servers being easily available or implementing many tools, people start to be very easy on how many to add, right? So we see people at like 50, 100 tools. So we need to improve our LLMs to be able to have them really know, okay, which is the right tool to use when you have like 50 different tools available at once. So there are examples for ADK to work with A2A. So A2A stands for agent to agent protocol, which is done by the Google Cloud team. The idea here is, uh, to allow companies to build agents in different frameworks like LangChain, LangCraft, Llama Index, and then have a an easy way to create multi-agent systems that one agent can call the other agent without needing to implement complex logic. Um, but the Gemini SDK is like not directly, but there are great examples for it. Cool. Yeah. So we are working on basically browser use or computer use use cases, which is available in preview, uh, which we are currently testing with a few companies. Um, would that would allow Gemini to control UI, so it could basically go to whatever website you want it to go to. And then there's, sorry, the URL context tool where you can provide a website, and we would programmatically try to extract information from it. Of course, it's like a, if it's a super heavy JavaScript website, there's not much to extract. That's where like the browser use agent would then be useful for. Yeah, kind of. Yeah, hopefully coming soon. So it will return structured outputs again based on what you provide. So you can control a local environment, but we are also working with the Cloud Run team to make it super easy to you for you to run so that you can like run a Chrome instance on Cloud Run, uh, which Gemini can talk to and control, or you can like control a local instance. Yeah, yeah. Do authentication? That's a good question. I think the whole industry is currently trying to answer like what's the best way to handle agent authentication. So the MCP, um, as a protocol itself supports OO off. So basically, when you want to connect to an MCP server which is protected, you would get back a 403 with like not authorized, which can then trigger an off flow on your client. Maybe you have seen it in like claw desktop where you get a popup like to log in with, I don't know, like your like Atina account or something. So that's like one way of doing it, but it definitely needs more work, right? Do you want your agent to access all of your emails or all of your GitHub repositories, or how can you like scope it down to like only the one specific repository, but they are currently like actively working on it. I know that of zero will be here tomorrow as well. Those guys are very great to talk to. I think I know that they're doing a lot there. So, yeah. Do you mean, um, citation in when you just send a regular prompt or citation when you like use Google search and that have citations? Yep. So that's available in like the Google search tool. Um, and it has information about which websites was used to retrieve. You can, um, click on the link directly to see where you land. And then there's this, um, metadata or like chunking or like crowning support, which basically has a start index and an end index from the answer, and then also which sources were used. So you technically can highlight or put numbers behind it to help users understand, okay, that part of the response is generated based on those two, um, links. You mean in, in general, there are sometimes citation meta data or which citation data do you mean? Okay, I know, or I only know that's part of like legal and like training that if there's something referred directly, you need to like provide it. That's why it is there. That's why it is not always there. Um, it can be there, but it doesn't must be, and it's not like for a user to, hey, that's coming from there, but it's more like, um, compliance, basically, why we need to have it. I only know about like the web search feature. Ah, okay. So you provide as a context like a document. Okay. Yeah. No, that's like a very good question. Currently, we don't have like the same experience as Entropic has. Um, I guess the base is like to really try and like different prompting strategies and like what you want to achieve. Um, yeah. No, but it's like good, good feedback. Sadly, not yet. I hope, hopefully one day that you can like, just here's Gmail, here's Drive, and let's chat. Um, but currently there's no public remote MCP server. I don't know exactly what, what are the reasons. I think the more people ask for it, the easier it will be that we get once. So, if you like, that's great feedback. The more people, um, if the more people are going to use MCP servers, the higher the chances are going to be, and I think in general for MCP to succeed, we need more first-party remote servers, right? Because we cannot build a secure GitHub MCP servers, that's something GitHub needs to do because they know how their off system works, how you can scope it. Um, so we really need those first-party MCP servers in the long time. For, for AI mode, I have no idea. I think like, so AI mode, you mean about like inside Google search? I think in general, overall, same goes for like normal Google search is like have high-quality content, um, try to stick to like, um, web standards. Um, I guess same works here as well, but I don't know. Sorry. Yeah. It should. So if you uploaded a PDF, you need to make sure that you put a PDF at the first part of your prompt. So you like the automatic, um, caching works from the beginning to the end, right? If you change the beginning, you can never cache like the long document. But if you put the PDF in the beginning and change the prompt behind it, it should work. Yes. Yeah. Sometimes there's a case where index provides the starter index out of range for the length of the really curious if you any similar experiences. No, but if you like have an example for us to reproduce and to share, that would be very helpful. The only thing I know is that, so sometimes there can be a start index which can be null, which basically means it's zero, like it starts at the beginning. Yeah. No, like if you have an example, please like send it to me on Twitter or somewhere like, Okay, that, that would be very helpful.\n\n\nBecause that should not be the case.\n\nYeah.\nIs there any way to actually get that was a citation from the website that it refers to, so that you only get the website?\nYou just get the Yeah.\nNo, like so currently you get like the the start and end of the response which refers to something but not the web part.\nBut that's like like please put it all together and like send it to us.\nLike, very happy to like talk to the Google search team who is like building the native tool.\nUm, yes, super helpful with like to better understand like what you need.\n\nOkay, cool then.\nThanks all for coming.\nUm, please continue with the workshop.\nTry it out.\nIf you have like any questions, we are very happy to receive any positive, negative feedback, any ideas, any pain points you have.\nUm, we are available on like social channels.\nYou can like find me Philipp Schmid basically everywhere.\nIf not, we open the GitHub issue or be very noisy about when when something doesn't work.\nUh we we always try to make sure it we fix it.\n\nCool.\nThanks,\n\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:24.557Z"
}