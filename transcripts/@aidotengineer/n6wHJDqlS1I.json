{
  "episodeId": "n6wHJDqlS1I",
  "channelSlug": "@aidotengineer",
  "title": "How to Build Trustworthy AI â€” Allie Howe",
  "publishedAt": "2025-06-16T20:29:50.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Hi, my name is Ally How. I am a VCSO for",
      "offset": 1.36,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "Growth Cyber. We are a business that",
      "offset": 4.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "helps other companies build trustworthy",
      "offset": 7.919,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "AI. We sit at the intersection of AI",
      "offset": 10.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "security and compliance.",
      "offset": 13.28,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "Today, we're going to be talking about a",
      "offset": 15.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "variety of different topics. Namely,",
      "offset": 17.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "what is trustworthy AI? What goes into",
      "offset": 19.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "building trustworthy AI? And why you",
      "offset": 21.84,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "should care about trustworthy AI.",
      "offset": 24.56,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "So, to start off, who even needs",
      "offset": 28.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "trustworthy AI? Why do we care about it?",
      "offset": 31.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Well, it's been in the news a lot,",
      "offset": 33.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "whether you realize it or not. All the",
      "offset": 35.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "way back in 2023, we all saw that case",
      "offset": 38,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "with the Chevy Tahoe incident where a",
      "offset": 41.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "user was able to um be offered a Chevy",
      "offset": 44.48,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "Tahoe from a chatbot for a dollar. So,",
      "offset": 47.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that chatbot did not operate as it was",
      "offset": 50.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "intended to by the company, and it was",
      "offset": 52.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "in a position to be taken advantage of",
      "offset": 54.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "by a user. In another instance, in 2024,",
      "offset": 56,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Slack was able to be tricked into",
      "offset": 60.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "leaking data from private channels via a",
      "offset": 62.8,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "prompt injection. So again, that that",
      "offset": 65.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "system did not operate as intended and",
      "offset": 66.799,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "had some pretty strict consequences as",
      "offset": 69.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "well for probably Slack AI and the",
      "offset": 71.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "companies whoever whatever company was",
      "offset": 73.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "had that data leakage happen. Very",
      "offset": 75.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "recently, a couple weeks ago, we saw",
      "offset": 77.52,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "Darth Vader and NPC being released into",
      "offset": 80.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Fortnite by Epic Labs. That was really",
      "offset": 83.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "interesting. I think this is the first",
      "offset": 86.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "case of a voice agent being used in a",
      "offset": 87.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "video game. So users were able to",
      "offset": 89.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interact with an AI Vader and ask it all",
      "offset": 91.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sorts of crazy questions. You know, at",
      "offset": 93.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "first Vader exhibited a lot of bad",
      "offset": 95.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "behaviors, sort of reminiscent of the",
      "offset": 98.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Microsoft T chatbot back from 2006,",
      "offset": 100.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "saying things that were racist,",
      "offset": 102.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "homophobic. Um it's it since has",
      "offset": 104.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "improved dramatically. Um that's not",
      "offset": 107.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "what I'll be diving into specifically",
      "offset": 108.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "today, but as you can see, there's many",
      "offset": 110.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "cases in the news where AI was not",
      "offset": 112.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "necessarily trustworthy. So, it's",
      "offset": 114.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "something that is happening quite often",
      "offset": 116.159,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "and something that needs your attention.",
      "offset": 118,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "There's a lot of debate around who is",
      "offset": 123.84,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "responsible for trustworthy AI and it",
      "offset": 126,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "really boils down to you. You are the",
      "offset": 128.879,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "one responsible. There was a lawsuit the",
      "offset": 130.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "other day on May 20th, so very recently,",
      "offset": 133.28,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "where a radio host was suing Open AI",
      "offset": 136.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "over false statements that were",
      "offset": 140.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "generated by Chat GPT. However, CHGBT",
      "offset": 141.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "was able to get the case dismissed or",
      "offset": 145.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they won the case or OB did not end up",
      "offset": 147.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "in trouble simply because um it states",
      "offset": 149.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "that GBT um can you know make wrongs",
      "offset": 152.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "from time to time and it's up to the",
      "offset": 155.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "user to understand that and to proceed",
      "offset": 158,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "with caution. So if you're using AI,",
      "offset": 160.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "it's likely you that will be responsible",
      "offset": 163.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "both on paper and also just from a brand",
      "offset": 166.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and reputational standpoint. you are the",
      "offset": 169.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "one that needs to be aware that you are",
      "offset": 171.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "likely responsible and will be taking",
      "offset": 173.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the fall for anything your AI",
      "offset": 175.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "application does that's incorrect, wrong",
      "offset": 177.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "or inappropriate. So when it when we",
      "offset": 180,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "talk about building trustworthy AI,",
      "offset": 182.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that's something that both product",
      "offset": 184.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "engineering and security teams are",
      "offset": 186.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "focused on. Product teams probably care",
      "offset": 188.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that your AI application is outputting",
      "offset": 190.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the right topics. It's relevant. It's",
      "offset": 192.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "generally helpful. Security teams are",
      "offset": 194.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "probably thinking about, you know, your",
      "offset": 196.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "AI application is not going to be saying",
      "offset": 197.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "anything that's going to be",
      "offset": 199.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "inappropriate or off topic as well, but",
      "offset": 201.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "they're also looking out for things like",
      "offset": 203.92,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "prompt injections and jailbreaks.",
      "offset": 205.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Engineering is helping out with these as",
      "offset": 206.959,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "well. Probably cross functioning is also",
      "offset": 208.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "thinking about things like cost and",
      "offset": 210.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "latency as well. So, it's a really big",
      "offset": 211.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "team that comes together to be able to",
      "offset": 213.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "put together trustworthy AI",
      "offset": 216.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "specifically. And the recipe for",
      "offset": 218.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "trustworthy AI is AI security and AI",
      "offset": 220.319,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "safety. So what's the difference? AI",
      "offset": 223.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "security is how does the outside world",
      "offset": 226.879,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "harm my AI application? AI safety is how",
      "offset": 230,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "does my AI application harm the world?",
      "offset": 233.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And we'll talk into we'll go into detail",
      "offset": 235.84,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "about both of those shortly.",
      "offset": 237.68,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "So there's this new paradigm out there",
      "offset": 242,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "that AI engineering has introduced.",
      "offset": 244,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Traditionally we had dev sec ops where",
      "offset": 246.239,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "our scanning tools security tools like",
      "offset": 249.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "our SAS tools were integrated within our",
      "offset": 251.439,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "CI/CD pipelines they were able to",
      "offset": 253.519,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "capture most of the vulnerabilities such",
      "offset": 255.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "as software dependency supply chain",
      "offset": 257.359,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "issues and insecure code but now thanks",
      "offset": 258.88,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "to AI engineering and you know data",
      "offset": 261.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "scientists and machine learning",
      "offset": 263.759,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "engineers they don't work in our",
      "offset": 265.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "traditional CI/CD platforms they work in",
      "offset": 266.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "things like data bricks they work in",
      "offset": 269.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "things like Jupyter notebooks so we need",
      "offset": 270.639,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "a new model for what AI engineering ing",
      "offset": 272.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "dev sec ops looks like and how we're",
      "offset": 276.16,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "actually going to build trustworthy AI.",
      "offset": 278.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Traditionally, there's been a big focus",
      "offset": 281.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "on shifting, but thanks thanks to prompt",
      "offset": 282.639,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "injections changing rapidly, AI models",
      "offset": 285.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "um being deployed and and switched out",
      "offset": 288.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "rapidly, there's now a big focus on",
      "offset": 290.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "shifting right for AI security, which is",
      "offset": 293.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "why runtime security is particularly",
      "offset": 295.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "important. I mean, all three of these",
      "offset": 298.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "boxes here, build, test, and run are,",
      "offset": 299.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "build, test, and deploy R. But with AI",
      "offset": 302.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "security specifically, especially",
      "offset": 304.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "because it's so non-deterministic,",
      "offset": 306,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "having something on the rightmost side",
      "offset": 308,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "at the time of runtime is incredibly",
      "offset": 310.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "important. You know, no longer are we",
      "offset": 312.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just caring about shift left. We're",
      "offset": 314.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "really worried about this entire life",
      "offset": 316.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "cycle. So on the leftmost side, we've",
      "offset": 318,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "got our build. That's where we're going",
      "offset": 320.08,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "to be doing some sort of like model",
      "offset": 321.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "scanning, thinking about model",
      "offset": 322.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "providence, looking at um AI or ML bomb.",
      "offset": 324,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So that's machine learning security",
      "offset": 327.039,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "operations. It's kind of apply on the",
      "offset": 328.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "term devst ops. That's important. And",
      "offset": 330.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then in the middle we've got AI red",
      "offset": 332.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "teaming. We're going to test our AI",
      "offset": 334.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "applications for both AI security and AI",
      "offset": 336.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "safety concerns. And then on the",
      "offset": 339.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "rightmost side we've got AI runtime",
      "offset": 341.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "security where we can validate AI inputs",
      "offset": 343.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and outputs as they come into our AI",
      "offset": 345.52,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "system and our models.",
      "offset": 347.84,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "So dev sec ops is out. ML secops is in.",
      "offset": 352.24,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "So basically ML secops is machine",
      "offset": 357.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "learning security operations. As I",
      "offset": 359.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "mentioned before ML secops is able to",
      "offset": 361.36,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "look into and take into consideration",
      "offset": 364,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "places that traditional dev sec ops does",
      "offset": 366.479,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "not. As I mentioned AI engineers live in",
      "offset": 368.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "data bricks and jupyter notebooks not in",
      "offset": 371.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "traditional CI/CD pipelines. So it's",
      "offset": 373.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "important to focus there as well and",
      "offset": 375.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "look for exposed secrets that could be",
      "offset": 378.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in those data bricks or Jupyter",
      "offset": 380.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "notebooks. And also part of MLS secops",
      "offset": 382.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is understanding things like model",
      "offset": 385.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "providence. So you know where what who",
      "offset": 387.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "built this model? Where did it come",
      "offset": 389.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "from? What data was it trained on? Those",
      "offset": 391.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "are helpful things to think about for",
      "offset": 393.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "compliance as well. If you have, you",
      "offset": 394.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "know, access to that data the model was",
      "offset": 396.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "trained on, if you're supposed to be",
      "offset": 399.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "using it or safeguarding it,",
      "offset": 401.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "understanding maybe, you know, maybe a",
      "offset": 402.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "nation state made that made that model.",
      "offset": 404.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "What are the implications of that? Is",
      "offset": 407.12,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "that something you need to be worried",
      "offset": 408.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "about? But one of the biggest risks from",
      "offset": 409.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "models that you may be using, especially",
      "offset": 412.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like open source ones you can get your",
      "offset": 414.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "hands on, are model serialization",
      "offset": 415.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "attacks. A model serialization attack is",
      "offset": 418.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "when models are code is saved into the",
      "offset": 421.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "model at runtime at sorry at time of",
      "offset": 423.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "serialization and then when you go to",
      "offset": 425.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "des serialize the model that code will",
      "offset": 427.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "automatically be run. So you're now",
      "offset": 429.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "talking about arbitrary code execution",
      "offset": 431.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and we see this with the pickle",
      "offset": 433.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "serialization format. That's one of the",
      "offset": 435.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "most well-known ones for serializing",
      "offset": 437.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "models. However, if you look at the",
      "offset": 440.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "pickles documentation, they do say that",
      "offset": 442.56,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "the module is is not secure and you're",
      "offset": 445.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "not supposed to unpickle data that you",
      "offset": 448.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "do not inherently trust. So, you're",
      "offset": 451.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "getting these models say from a model",
      "offset": 452.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "repository or a model zoo on the web and",
      "offset": 454.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you're just downloading it. You need to",
      "offset": 457.36,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "scan those to see if they have any",
      "offset": 458.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "unsafe operators in them. if you're at",
      "offset": 460.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "risk to model serialization attacks.",
      "offset": 462.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Model serialization attacks run as soon",
      "offset": 464.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "as the model is deserialized. So that",
      "offset": 466.56,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "arbitrary code could be causing data",
      "offset": 468.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "loss, credential loss or model",
      "offset": 469.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "poisoning. So it's really easy to scan",
      "offset": 471.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "models. So I really encourage you to do",
      "offset": 474,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this and be practicing this within your",
      "offset": 476.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "organization. One example is model scan",
      "offset": 478.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "from Protect AI. And Protect AAI also",
      "offset": 481.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "created this MLS secops community where",
      "offset": 483.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you can learn more about MLS secops.",
      "offset": 486,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I've personally learned a lot from it.",
      "offset": 488,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "So I really encourage you to check it",
      "offset": 489.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "out. But I can show you Model Scan",
      "offset": 490.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "really quickly. This is the repo. It's",
      "offset": 492.72,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "open source, free to use, really easy to",
      "offset": 495.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "download. Just pip install it and then",
      "offset": 497.759,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "run it like this. They also have an",
      "offset": 499.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "example of a model serialization attack",
      "offset": 502.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "within this codebase that you can go",
      "offset": 504,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "ahead and check out. So as you can see",
      "offset": 505.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "here, when the model gets saved, we are",
      "offset": 507.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "adding our unsafe payload in there,",
      "offset": 509.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "which is this command to basically",
      "offset": 512.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "output the AWS secrets that we have. And",
      "offset": 514.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "so when this model gets run or loaded,",
      "offset": 516.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "we can now see that this OS keys has",
      "offset": 519.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "been outputed, which you know there we",
      "offset": 521.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "have a credential leak. That's not",
      "offset": 523.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "great. And then if you had used model",
      "offset": 524.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "scan to scan your model before you know",
      "offset": 527.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this had happened, you would have seen",
      "offset": 529.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "that there is a critical vulnerability",
      "offset": 530.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "unsafe operator in use here. So that's",
      "offset": 532.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "one example of you know a potential",
      "offset": 534.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "solution for scanning your models that's",
      "offset": 537.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "open source free that you can use.",
      "offset": 538.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "There's other ones out there as well.",
      "offset": 540.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "They also have a partnership with",
      "offset": 543.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "hugging face. So um model scan is used",
      "offset": 545.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "within protect within hugging face to do",
      "offset": 548.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "scans of files and model data. So if you",
      "offset": 550.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "ever see something that's like unsafe",
      "offset": 553.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "here, you can go learn about it. So keep",
      "offset": 555.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a lookout for that as well as you're as",
      "offset": 556.72,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "you're pulling models off of model zoos.",
      "offset": 558.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "So now that we've talked about the",
      "offset": 565.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "leftmost side which is ML secops, let's",
      "offset": 567.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "now talk about the middle which is AI",
      "offset": 569.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "red teaming. In AI red teaming we can",
      "offset": 571.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "use that to simulate both adversar",
      "offset": 574.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "adversarial threats and also AI safety",
      "offset": 576.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "concerns. So during AI red teaming we",
      "offset": 579.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can test for things like prompt",
      "offset": 582.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "injections, jailbreaks but we can also",
      "offset": 583.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "test for AI safety concerns like so if a",
      "offset": 586.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "user asks you know how can I build a",
      "offset": 588.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bomb? How can I create chemical weapons?",
      "offset": 590.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Those are things that your model should",
      "offset": 592.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "not, you know, be answering. It should",
      "offset": 594,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "be safeguarding against. It also should",
      "offset": 595.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "be, you know, it should be biased. It",
      "offset": 597.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "shouldn't do anything that's like",
      "offset": 599.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "homophobic or racist like we saw in the",
      "offset": 600.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Vader example from the very beginning.",
      "offset": 602.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "These are things we can both test during",
      "offset": 604.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "AI red teaming. And we should be",
      "offset": 606.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "continuously testing our models because,",
      "offset": 608.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you know, as we know, models change as",
      "offset": 609.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "users interact with them, not just with",
      "offset": 611.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "code deploys like traditional software",
      "offset": 613.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "has. Another benefit of AI red teaming",
      "offset": 615.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "is you can use them to influence a",
      "offset": 618.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "runtime guardrail. So if we see that,",
      "offset": 620.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you know, our model is particularly",
      "offset": 622.959,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "vulnerable to this type of prompt",
      "offset": 624.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "injection or it's continuously saying",
      "offset": 625.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like this sort of racist thing, well, we",
      "offset": 627.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "can block certain um topics and prompts",
      "offset": 629.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that would elicit those responses to",
      "offset": 632.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "make sure that our model behaves as",
      "offset": 634.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "expected during runtime.",
      "offset": 636.64,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Another benefit of AI red teaming is the",
      "offset": 639.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "ability to compare LLMs to each other.",
      "offset": 641.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "In some cases, models can have back",
      "offset": 644.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "doors built into them where you're not",
      "offset": 646,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "going to see this during sort of just",
      "offset": 648.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "for testing. But if you start to compare",
      "offset": 650.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "LLMs to each other, you might start to",
      "offset": 653.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "see differences that might suggest that",
      "offset": 655.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "one model has a back door built into it,",
      "offset": 656.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "whereas another does not. So if you if",
      "offset": 658.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you ask every single model the same",
      "offset": 660.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "question, but one had a really different",
      "offset": 662.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "response, you might be like, okay, maybe",
      "offset": 664.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this model like one either doesn't work",
      "offset": 666.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "appropriately. Maybe this was by design.",
      "offset": 667.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "There's some sort of backdoor deceptive",
      "offset": 669.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "behavior that was built into this model.",
      "offset": 671.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "So I'm going to use a different model",
      "offset": 673.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "instead. Um, so there's a lot of reasons",
      "offset": 674.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to do AI red teaming. It can be",
      "offset": 676.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "particularly helpful both with model",
      "offset": 678,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "selection and helping you influence what",
      "offset": 679.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "runtime guardrails to put in place",
      "offset": 682.8,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "during runtime.",
      "offset": 684.32,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "So in terms of runtime security, if",
      "offset": 687.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "there was one area to invest in, I would",
      "offset": 689.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "pick runtime security because AI uh red",
      "offset": 692.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "teaming can be particularly timeconuming",
      "offset": 696,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and also expensive to if you're going to",
      "offset": 697.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "like retrain models based on the",
      "offset": 700,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "results. It might be particularly",
      "offset": 701.519,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "difficult or a lot of overhead for your",
      "offset": 703.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "organization if you're starting you know",
      "offset": 704.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "an AI security practice. AI runtime",
      "offset": 706.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "security um is easy to implement.",
      "offset": 709.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Typically, it's done with just including",
      "offset": 710.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "an API or installing another like Python",
      "offset": 713.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "module. It's pretty easy to get deployed",
      "offset": 716.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "with a lot of benefit. So,",
      "offset": 718.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "we should really focus on AI security",
      "offset": 721.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "runtime security because it's important",
      "offset": 724.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "to shift right. That's where we're going",
      "offset": 725.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to see things like the prompt injections",
      "offset": 727.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "being thrown at our models and different",
      "offset": 730.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "types of prompt attacks. These can be",
      "offset": 732,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "indirect prompt injections where say",
      "offset": 733.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that you have a system that is using rag",
      "offset": 735.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "or calling out and scraping a website.",
      "offset": 738.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Web data can have hidden prompt",
      "offset": 740.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "injections in them. Same can documents",
      "offset": 741.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that you've used with your rag setup. So",
      "offset": 744,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you could get something that way. You",
      "offset": 746.079,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "could also have a direct impromptu",
      "offset": 747.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "straight from the user to the chatbot or",
      "offset": 749.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to the AI agent. You can also see",
      "offset": 751.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "jailbreaks as well which in text those",
      "offset": 752.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "look like things that are semantically",
      "offset": 755.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "strange, pretty chaotic looking. This is",
      "offset": 757.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "an example here on this slide of a",
      "offset": 759.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "jailbreak from Lera AI. And also at",
      "offset": 761.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "runtime your application can see",
      "offset": 763.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "off-topic or unsafe prompts such as ones",
      "offset": 766.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that might you know get your application",
      "offset": 769.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "to output something that is unsafe. So",
      "offset": 770.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "something that's like you know",
      "offset": 773.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "instructions for how to build a bomb or",
      "offset": 774.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "something that's you know inappropriate",
      "offset": 776.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "or racist or you know something that's",
      "offset": 778.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "it's bad to say. You can check that at",
      "offset": 780.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "runtime both on the input side looking",
      "offset": 782.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "at the prompts and then either",
      "offset": 784.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "deflecting it and not allowing your",
      "offset": 785.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "model to answer it in the first place or",
      "offset": 787.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "if your model answers it and something",
      "offset": 789.519,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that's incorrect or inappropriate comes",
      "offset": 791.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "back, you can go ahead and block that",
      "offset": 794.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "from being ever sent to the user. So,",
      "offset": 796.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's a super nice solution to have in",
      "offset": 798.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "place to make sure that your AI you've",
      "offset": 800.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "deployed is behaving in a trustworthy",
      "offset": 803.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "manner.",
      "offset": 805.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "So let's take a look at that beta",
      "offset": 808.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "solution from or setup from the",
      "offset": 810.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "beginning where we've got that beta NPC",
      "offset": 811.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "running um in Fortnite. It was a really",
      "offset": 814.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "interesting setup that I tried to",
      "offset": 816.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "imagine. So this is just an estimated",
      "offset": 818.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "guess of what their architecture looks",
      "offset": 820.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "like. So basically the users that are in",
      "offset": 821.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "using Fortnite are sending proximity",
      "offset": 824.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "audio, user audio. There's like other",
      "offset": 827.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "contacts being sent to the servers",
      "offset": 829.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "likely about like player skins or other",
      "offset": 831.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "player configurations. And also of",
      "offset": 833.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "course the user's audio feed where we",
      "offset": 835.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "see the where they're talking to Vader",
      "offset": 837.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "go into probably Fortnite servers and",
      "offset": 839.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "then it's being passed to 11 Labs for",
      "offset": 841.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "voiceto text transformation eventually",
      "offset": 843.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "being passed to Gemini to craft Vader's",
      "offset": 845.839,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "response and then we're going to see",
      "offset": 848.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Vader's response being sent back in text",
      "offset": 851.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to 11 ABS then sent back as audio to",
      "offset": 853.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Fortnite servers and eventually all the",
      "offset": 855.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "way back to the user. So there's a lot",
      "offset": 857.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "going on here and if I was going to be",
      "offset": 859.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "inserting AI runtime security, I'd",
      "offset": 861.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "probably insert it as close to the model",
      "offset": 864,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "as possible. So if we get something",
      "offset": 866.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that's like unsafe, if user user said",
      "offset": 868,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "something that was maybe against user",
      "offset": 870.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "terms or conditions or you know the user",
      "offset": 871.519,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "said something inappropriate, maybe we",
      "offset": 873.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "don't want to like allow Vader to answer",
      "offset": 874.959,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that. So we can validate our inputs here",
      "offset": 876.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "at this at this location. But let's say",
      "offset": 878.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "we answer Vader's question or topic and",
      "offset": 880.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it did come back as something that was",
      "offset": 883.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "off topic or inappropriate, we could",
      "offset": 885.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "flag it there. So what I'm talking about",
      "offset": 887.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "right now is AI safety concerns where",
      "offset": 889.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "the user has said something",
      "offset": 891.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "inappropriate. We don't want Vader to",
      "offset": 892.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "behave in a way that's inappropriate",
      "offset": 894.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "either. So we could check for that. But",
      "offset": 895.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of course we can also check for you know",
      "offset": 897.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "prompt injections, jailbreaks, other you",
      "offset": 899.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "know AI security concerns as well. So AI",
      "offset": 901.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "runtime is both for AI safety and also",
      "offset": 904.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "for AI security.",
      "offset": 906.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Could of course also you know include AI",
      "offset": 908.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "runtime security here as well before we",
      "offset": 911.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "even get to 11 Labs. So, if the Fortnite",
      "offset": 913.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "servers come back and say, &quot;Hey, like,",
      "offset": 915.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "you know, this looks like something",
      "offset": 916.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that's inappropriate. Like, I don't even",
      "offset": 918,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "want to do a call to 11 Labs. There's",
      "offset": 919.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "another API call. I'm sure it's",
      "offset": 921.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "expensive as well because they're using",
      "offset": 922.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "another service.&quot; So, you could add it",
      "offset": 924.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "here as well. Same here. Checking it",
      "offset": 926.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "again after it comes out of 11 Labs.",
      "offset": 929.199,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "Checking the audio, making sure that",
      "offset": 930.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that still looks good. However, there's",
      "offset": 932.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "trade-offs at play here. We have to",
      "offset": 934.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think about cost, latency, and accuracy.",
      "offset": 935.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "That's what a good AI runtime security",
      "offset": 938.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "solution has. It has all of those",
      "offset": 940.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "things. low latency, low cost or you",
      "offset": 942.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "know acceptable cost and it's also like",
      "offset": 944.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "highly accurate as well. A lot of AI",
      "offset": 946.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "runtime security solutions are backed by",
      "offset": 948.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "really well put together and established",
      "offset": 950.56,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "AI research teams which are constantly",
      "offset": 953.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "finding new prompt in projections, new",
      "offset": 955.759,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "jailbreaks. That's not something you",
      "offset": 957.279,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "have to worry about when you're building",
      "offset": 958.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "your product trying to keep up with the",
      "offset": 960.399,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "latest jailbreaks and problem",
      "offset": 961.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "injections. You can rely on your vendor",
      "offset": 962.959,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "for that.",
      "offset": 965.12,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "So just to show you an example of AI",
      "offset": 969.6,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "runtime security at work. This is",
      "offset": 973.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "pillar. It's an application security",
      "offset": 975.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "life cycle application which I have been",
      "offset": 977.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "using to do some of my work. This is",
      "offset": 979.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "just an example of one. There's also",
      "offset": 982.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "different AI runtime security solutions",
      "offset": 984.079,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "out there. But basically, I had a an",
      "offset": 986.399,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "application that I was using for",
      "offset": 989.199,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "finding the",
      "offset": 994.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "finding patients that would be suitable",
      "offset": 996.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "for ALS clinical trials. And as you can",
      "offset": 998.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "see here, this is a this is supposed to",
      "offset": 1002.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "be a acceptable use case here where we",
      "offset": 1004.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "are getting patients in our database we",
      "offset": 1008.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "would recommend for different studies",
      "offset": 1010,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "that we found on the web. So basically",
      "offset": 1011.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the goal of this agent or this",
      "offset": 1013.759,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "multi-agent system is to output a list",
      "offset": 1015.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "of patients suitable for each trial.",
      "offset": 1017.759,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "It's not supposed to answer questions",
      "offset": 1020.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "about individual patients or modify",
      "offset": 1021.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "patient data in the database. So in this",
      "offset": 1024.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "example, I have asked this system to",
      "offset": 1027.839,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "change a patient's FC percentage in the",
      "offset": 1030.559,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "database to 50. But pillar has blocked",
      "offset": 1033.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this for me. So my agent's not going to",
      "offset": 1036.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "answer this because I have configured my",
      "offset": 1038.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "guardrails to restrict this topic and",
      "offset": 1040,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "this key and keywords related to this so",
      "offset": 1042.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "that we can make sure to not change um",
      "offset": 1044.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "patient data in the database and this",
      "offset": 1048.319,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "application can behave as expected. So a",
      "offset": 1050,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "lot of guardrails are comprised where a",
      "offset": 1053.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "lot of guardrails and applications or",
      "offset": 1055.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "vendors will allow you to look out for",
      "offset": 1056.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "things like PII, making sure that that's",
      "offset": 1059.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "not being either input or output",
      "offset": 1061.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "depending on your configuration. Making",
      "offset": 1063.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "sure we're looking for things like AI",
      "offset": 1064.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "safety concerns, toxic responses, but a",
      "offset": 1066.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "really strong advantage of AI runtime",
      "offset": 1069.679,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "security platforms or some of them is",
      "offset": 1072.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the ability to add custom guardrails.",
      "offset": 1074.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Um, so like this one in this example",
      "offset": 1076.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "where I've got this very specific",
      "offset": 1078,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "functionality where I don't want my",
      "offset": 1079.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "system to be able to update database",
      "offset": 1081.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "information. So I'm able to use that for",
      "offset": 1083.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "this, which of course is a security",
      "offset": 1086,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "concern. But I could also restrict",
      "offset": 1087.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "topics around like, you know, say if I",
      "offset": 1088.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "was Tesla and I didn't want to recommend",
      "offset": 1090.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Ford, for example, because that's a",
      "offset": 1092.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "competing car company. I could restrict",
      "offset": 1094.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that topic to make sure that my outputs",
      "offset": 1096.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "are outputs that are reflecting my",
      "offset": 1098.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "business goals as well as my AI security",
      "offset": 1100.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and AI safety goals. So runtime",
      "offset": 1103.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "solutions can be particularly helpful",
      "offset": 1106,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "and impactful in that way.",
      "offset": 1107.44,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "And then once we have an AI security",
      "offset": 1112.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "runtime solution, we can go ahead and",
      "offset": 1115.039,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "verify that in different GRC platforms.",
      "offset": 1117.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "So as I mentioned before, I help with",
      "offset": 1121.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "compliance as well. Um, so if we're",
      "offset": 1122.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "doing all this work to build trustworthy",
      "offset": 1124.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "AI, we might as well demonstrate it and",
      "offset": 1126.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "we might as well show that to our",
      "offset": 1128.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "customers so that they can trust in what",
      "offset": 1129.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "we build and also help us with our sales",
      "offset": 1132.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "cycles.",
      "offset": 1134.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "So for instance, I have this risk in my",
      "offset": 1136.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "risk register which having a risk",
      "offset": 1138,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "assessment is something that is required",
      "offset": 1140.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "for stock 2 at least annually. So if I",
      "offset": 1143.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "have this custom risk of you know what",
      "offset": 1146.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "if my company reputation server is",
      "offset": 1147.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "damaged due to a harmful or off-topic",
      "offset": 1149.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "output from an AI application in Vanta I",
      "offset": 1151.6,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "made a custom control",
      "offset": 1155.12,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "for validating AI outputs.",
      "offset": 1157.679,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "So in this control I have submitted",
      "offset": 1161.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "evidence that I'm going to be validating",
      "offset": 1164.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "all of my different AI outputs. And so I",
      "offset": 1166.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "can go ahead and throw that into a trust",
      "offset": 1169.36,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "center and show that I'm passing",
      "offset": 1170.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "controls for validating AI outputs and",
      "offset": 1172.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "inputs. So if someone comes and wants",
      "offset": 1175.039,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "to, you know, work with my company or",
      "offset": 1176.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "buy my solution, they can go ahead and",
      "offset": 1178.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "go on my trust center and see not only",
      "offset": 1179.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "controls that are applicable to SOCK 2,",
      "offset": 1181.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but additional controls that I have",
      "offset": 1184.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "created",
      "offset": 1185.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "for AI security that shows, hey, you",
      "offset": 1187.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "know, I'm using a runtime solution. Um,",
      "offset": 1189.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so I'm, you know, I'm clearly taking AI",
      "offset": 1191.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "security seriously and so maybe you want",
      "offset": 1192.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "to buy from me or maybe you want to work",
      "offset": 1194.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "with my company or maybe this, you know,",
      "offset": 1196.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "helps you not send me a, you know, 200",
      "offset": 1197.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "question page security questionnaire,",
      "offset": 1200.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for example. So something to call out if",
      "offset": 1202.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you're, you know, if you're building",
      "offset": 1204.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "with AI and you are taking AI,",
      "offset": 1205.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "trustworthy AI seriously and you're",
      "offset": 1207.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "building it, might as well show it,",
      "offset": 1209.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "might as well use it as a competitive",
      "offset": 1210.72,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "advantage in your sales cycles.",
      "offset": 1212.32,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "So, if you're not convinced you need to",
      "offset": 1217.679,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "build with trustworthy AI yet, here are",
      "offset": 1219.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "some other reasons why you might want to",
      "offset": 1221.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "take it seriously.",
      "offset": 1222.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Cyber security risk and business risk",
      "offset": 1224.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have never been more aligned. So, in the",
      "offset": 1226.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "past, you could have built a product",
      "offset": 1228.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that was insecure by design, shipped it,",
      "offset": 1230.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "made some revenue with it, and then",
      "offset": 1233.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "added security layer. However, with AI,",
      "offset": 1234.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "we're not seeing that. Making sure that",
      "offset": 1237.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "AI applications output the correct",
      "offset": 1239.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "outputs that they are aligned, they are",
      "offset": 1241.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "safe, they are on topic. That is as much",
      "offset": 1243.679,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "of a cyber security concern as it is a",
      "offset": 1246.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "business concern. I that outputs things",
      "offset": 1249.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that are off topic or irrelevant. It's",
      "offset": 1252.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "not going to be revenue generating. So",
      "offset": 1254.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "getting AI security and trustworthy AI",
      "offset": 1255.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "right from the beginning will not only",
      "offset": 1258.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "be helpful for your security program,",
      "offset": 1259.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "but it will be helpful for revenue as",
      "offset": 1261.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "well.",
      "offset": 1262.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "AI will also make missing cyber security",
      "offset": 1264.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "best practices worse. So you aren't",
      "offset": 1266.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "tracking things like, you know, what",
      "offset": 1268.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "data you have trained on, where you're",
      "offset": 1270.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "getting different models, where you're",
      "offset": 1272.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "how you're taking care of supply chain",
      "offset": 1274.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "risk. AI is only going to magnify that",
      "offset": 1276.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and make that worse.",
      "offset": 1278.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "We're also seeing an increasing",
      "offset": 1280.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "regulatory environment for a compliance",
      "offset": 1282.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "compliance perspective.",
      "offset": 1284.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "We're seeing ISO420001",
      "offset": 1286.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "crop up which is the first international",
      "offset": 1288.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "standard around AI regulation or a",
      "offset": 1290.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "compliance framework sorry and then the",
      "offset": 1293.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "EOI act of course came out and that's",
      "offset": 1295.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "what this picture is about on the right",
      "offset": 1297.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "there was a case where an AI company was",
      "offset": 1299.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "putting together a database of uh faces",
      "offset": 1302.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that they had scraped off the web ended",
      "offset": 1305.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "up getting fined by the EU for about 20",
      "offset": 1306.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "million thanks to the EUA AI act so it's",
      "offset": 1309.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "important to take that seriously if you",
      "offset": 1311.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "are going to accidentally output um",
      "offset": 1314,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "patient data if you're building a",
      "offset": 1316.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "healthcare AI application, then you",
      "offset": 1318.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "might be finding yourself with a HIPPO",
      "offset": 1321.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "violation. So that's important to take",
      "offset": 1323.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "into consideration. But also different",
      "offset": 1325.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "guidelines that are specific to whatever",
      "offset": 1326.96,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "you're building, whatever your industry",
      "offset": 1328.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "is. So for example, the even the FDA",
      "offset": 1329.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "came out with like AI and ML guidelines",
      "offset": 1331.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "earlier this year. So it's important to",
      "offset": 1333.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "keep in mind what sort of regulations",
      "offset": 1335.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "your company might be subject to. So",
      "offset": 1337.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "it's worth getting ahead of those either",
      "offset": 1340.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "regulations that exist now or will exist",
      "offset": 1342.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "down the road by building trustworthy AI",
      "offset": 1344.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "today.",
      "offset": 1346.72,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Trustworthy AI is super important",
      "offset": 1350.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "because it's going to unlock a lot of",
      "offset": 1352.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "revolutionary innovation. So for",
      "offset": 1354.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "instance, if we think about the",
      "offset": 1357.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "healthcare industry, we can't use",
      "offset": 1358.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "solutions that help us identify new",
      "offset": 1360.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "proteins that can be able to create",
      "offset": 1363.36,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "entirely new organs for transplant or",
      "offset": 1365.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "allow us to use models that were trained",
      "offset": 1368.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "on 1.3 million cells and has a lot of",
      "offset": 1370.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like very confidential data in them. We",
      "offset": 1372.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can't take advantage of the amazing",
      "offset": 1374.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "technology that we're going to be able",
      "offset": 1376.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to create as a society if what we're",
      "offset": 1377.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "building isn't trustworthy to begin",
      "offset": 1380.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "with. These systems need compliance.",
      "offset": 1382,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "They need AI safety. They need AI",
      "offset": 1383.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "security. So if that doesn't motivate",
      "offset": 1386,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you to take trustworthy AI seriously, I",
      "offset": 1388.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "hope it does because what will end up",
      "offset": 1390.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "happening is we're going to be able to",
      "offset": 1392.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "create things we never thought possible,",
      "offset": 1394.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but only if we had trust in place in the",
      "offset": 1396.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "beginning.",
      "offset": 1398.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So the bottom line is you are",
      "offset": 1400.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "responsible for building trustworthy AI.",
      "offset": 1404,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "We've seen the news headlines. It",
      "offset": 1406.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "happens all the time. We've also seen",
      "offset": 1407.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "the lawsuits where it's often the user",
      "offset": 1409.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "of the AI that's the one that's",
      "offset": 1413.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "responsible. So, you know, don't wait to",
      "offset": 1414.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "get yourself into a lawsuit. Start",
      "offset": 1416.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "building trustworthy AI today.",
      "offset": 1418.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Trustworthy AI is AI security, which is",
      "offset": 1420.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "how does the world harm your AI",
      "offset": 1424,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "application plus AI safety, which is how",
      "offset": 1426.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "does your AI application harm the world.",
      "offset": 1428.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "You can build trustworthy AI by",
      "offset": 1431.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "incorporating ML sec ops practices by",
      "offset": 1433.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "red taming your AI applications and by",
      "offset": 1436,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "incorporating an AI runtime security",
      "offset": 1439.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "solution at the time of deployment in",
      "offset": 1441.36,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "your AI system.",
      "offset": 1443.679,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "So thanks for watching. If you want to",
      "offset": 1447.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "find me, here's my handles. But it was",
      "offset": 1449.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "super awesome to deliver this discussion",
      "offset": 1450.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to you today. If you've got any",
      "offset": 1453.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "questions, please let me know. Happy to",
      "offset": 1454.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "reach out, answer more questions online.",
      "offset": 1456.64,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "But thanks.",
      "offset": 1458.799,
      "duration": 3.161
    }
  ],
  "cleanText": "Hi, my name is Allie Howe. I am a VCSO for Growth Cyber. We are a business that helps other companies build trustworthy AI. We sit at the intersection of AI security and compliance.\n\nToday, we're going to be talking about a variety of different topics. Namely, what is trustworthy AI? What goes into building trustworthy AI? And why you should care about trustworthy AI.\n\nSo, to start off, who even needs trustworthy AI? Why do we care about it? Well, it's been in the news a lot, whether you realize it or not. All the way back in 2023, we all saw that case with the Chevy Tahoe incident where a user was able to be offered a Chevy Tahoe from a chatbot for a dollar. So, that chatbot did not operate as it was intended to by the company, and it was in a position to be taken advantage of by a user. In another instance, in 2024, Slack was able to be tricked into leaking data from private channels via a prompt injection. So again, that system did not operate as intended and had some pretty strict consequences as well for probably Slack AI and the companies whoever whatever company was had that data leakage happen. Very recently, a couple weeks ago, we saw Darth Vader and NPC being released into Fortnite by Epic Labs. That was really interesting. I think this is the first case of a voice agent being used in a video game. So users were able to interact with an AI Vader and ask it all sorts of crazy questions. You know, at first Vader exhibited a lot of bad behaviors, sort of reminiscent of the Microsoft T chatbot back from 2006, saying things that were racist, homophobic. It since has improved dramatically. That's not what I'll be diving into specifically today, but as you can see, there's many cases in the news where AI was not necessarily trustworthy. So, it's something that is happening quite often and something that needs your attention.\n\nThere's a lot of debate around who is responsible for trustworthy AI and it really boils down to you. You are the one responsible. There was a lawsuit the other day on May 20th, so very recently, where a radio host was suing OpenAI over false statements that were generated by Chat GPT. However, CHGBT was able to get the case dismissed or they won the case or OpenAI did not end up in trouble simply because it states that GBT can make wrongs from time to time and it's up to the user to understand that and to proceed with caution. So if you're using AI, it's likely you that will be responsible both on paper and also just from a brand and reputational standpoint. You are the one that needs to be aware that you are likely responsible and will be taking the fall for anything your AI application does that's incorrect, wrong or inappropriate.\n\nSo when we talk about building trustworthy AI, that's something that both product, engineering, and security teams are focused on. Product teams probably care that your AI application is outputting the right topics. It's relevant. It's generally helpful. Security teams are probably thinking about, you know, your AI application is not going to be saying anything that's going to be inappropriate or off topic as well, but they're also looking out for things like prompt injections and jailbreaks. Engineering is helping out with these as well. Probably cross functioning is also thinking about things like cost and latency as well. So, it's a really big team that comes together to be able to put together trustworthy AI specifically. And the recipe for trustworthy AI is AI security and AI safety. So what's the difference? AI security is how does the outside world harm my AI application? AI safety is how does my AI application harm the world? And we'll go into detail about both of those shortly.\n\nSo there's this new paradigm out there that AI engineering has introduced. Traditionally we had dev sec ops where our scanning tools security tools like our SAS tools were integrated within our CI/CD pipelines they were able to capture most of the vulnerabilities such as software dependency supply chain issues and insecure code but now thanks to AI engineering and you know data scientists and machine learning engineers they don't work in our traditional CI/CD platforms they work in things like data bricks they work in things like Jupyter notebooks so we need a new model for what AI engineering dev sec ops looks like and how we're actually going to build trustworthy AI. Traditionally, there's been a big focus on shifting left, but thanks to prompt injections changing rapidly, AI models being deployed and switched out rapidly, there's now a big focus on shifting right for AI security, which is why runtime security is particularly important. I mean, all three of these boxes here, build, test, and run are, build, test, and deploy are. But with AI security specifically, especially because it's so non-deterministic, having something on the rightmost side at the time of runtime is incredibly important. You know, no longer are we just caring about shift left. We're really worried about this entire life cycle. So on the leftmost side, we've got our build. That's where we're going to be doing some sort of like model scanning, thinking about model providence, looking at AI or ML bomb. So that's machine learning security operations. It's kind of apply on the term devst ops. That's important. And then in the middle we've got AI red teaming. We're going to test our AI applications for both AI security and AI safety concerns. And then on the rightmost side we've got AI runtime security where we can validate AI inputs and outputs as they come into our AI system and our models.\n\nSo dev sec ops is out. ML secops is in. So basically ML secops is machine learning security operations. As I mentioned before ML secops is able to look into and take into consideration places that traditional dev sec ops does not. As I mentioned AI engineers live in data bricks and Jupyter notebooks not in traditional CI/CD pipelines. So it's important to focus there as well and look for exposed secrets that could be in those data bricks or Jupyter notebooks. And also part of MLS secops is understanding things like model providence. So you know where what who built this model? Where did it come from? What data was it trained on? Those are helpful things to think about for compliance as well. If you have, you know, access to that data the model was trained on, if you're supposed to be using it or safeguarding it, understanding maybe, you know, maybe a nation state made that made that model. What are the implications of that? Is that something you need to be worried about? But one of the biggest risks from models that you may be using, especially like open source ones you can get your hands on, are model serialization attacks. A model serialization attack is when models are code is saved into the model at runtime at sorry at time of serialization and then when you go to deserialize the model that code will automatically be run. So you're now talking about arbitrary code execution and we see this with the pickle serialization format. That's one of the most well-known ones for serializing models. However, if you look at the pickles documentation, they do say that the module is is not secure and you're not supposed to unpickle data that you do not inherently trust. So, you're getting these models say from a model repository or a model zoo on the web and you're just downloading it. You need to scan those to see if they have any unsafe operators in them. if you're at risk to model serialization attacks. Model serialization attacks run as soon as the model is deserialized. So that arbitrary code could be causing data loss, credential loss or model poisoning. So it's really easy to scan models. So I really encourage you to do this and be practicing this within your organization. One example is Modelscan from Protect AI. And Protect AI also created this ML secops community where you can learn more about ML secops. I've personally learned a lot from it. So I really encourage you to check it out. But I can show you Model Scan really quickly. This is the repo. It's open source, free to use, really easy to download. Just pip install it and then run it like this. They also have an example of a model serialization attack within this codebase that you can go ahead and check out. So as you can see here, when the model gets saved, we are adding our unsafe payload in there, which is this command to basically output the AWS secrets that we have. And so when this model gets run or loaded, we can now see that this OS keys has been outputed, which you know there we have a credential leak. That's not great. And then if you had used Modelscan to scan your model before you know this had happened, you would have seen that there is a critical vulnerability unsafe operator in use here. So that's one example of you know a potential solution for scanning your models that's open source free that you can use. There's other ones out there as well. They also have a partnership with hugging face. So Modelscan is used within protect within hugging face to do scans of files and model data. So if you ever see something that's like unsafe here, you can go learn about it. So keep a lookout for that as well as you're as you're pulling models off of model zoos.\n\nSo now that we've talked about the leftmost side which is ML secops, let's now talk about the middle which is AI red teaming. In AI red teaming we can use that to simulate both adversarial threats and also AI safety concerns. So during AI red teaming we can test for things like prompt injections, jailbreaks but we can also test for AI safety concerns like so if a user asks you know how can I build a bomb? How can I create chemical weapons? Those are things that your model should not, you know, be answering. It should be safeguarding against. It also should be, you know, it should be biased. It shouldn't do anything that's like homophobic or racist like we saw in the Vader example from the very beginning. These are things we can both test during AI red teaming. And we should be continuously testing our models because, you know, as we know, models change as users interact with them, not just with code deploys like traditional software has. Another benefit of AI red teaming is you can use them to influence a runtime guardrail. So if we see that, you know, our model is particularly vulnerable to this type of prompt injection or it's continuously saying like this sort of racist thing, well, we can block certain topics and prompts that would elicit those responses to make sure that our model behaves as expected during runtime.\n\nAnother benefit of AI red teaming is the ability to compare LLMs to each other. In some cases, models can have back doors built into them where you're not going to see this during sort of just for testing. But if you start to compare LLMs to each other, you might start to see differences that might suggest that one model has a back door built into it, whereas another does not. So if you if you ask every single model the same question, but one had a really different response, you might be like, okay, maybe this model like one either doesn't work appropriately. Maybe this was by design. There's some sort of backdoor deceptive behavior that was built into this model. So I'm going to use a different model instead. So there's a lot of reasons to do AI red teaming. It can be particularly helpful both with model selection and helping you influence what runtime guardrails to put in place during runtime.\n\nSo in terms of runtime security, if there was one area to invest in, I would pick runtime security because AI red teaming can be particularly timeconuming and also expensive to if you're going to like retrain models based on the results. It might be particularly difficult or a lot of overhead for your organization if you're starting you know an AI security practice. AI runtime security is easy to implement. Typically, it's done with just including an API or installing another like Python module. It's pretty easy to get deployed with a lot of benefit. So, we should really focus on AI security runtime security because it's important to shift right. That's where we're going to see things like the prompt injections being thrown at our models and different types of prompt attacks. These can be indirect prompt injections where say that you have a system that is using rag or calling out and scraping a website. Web data can have hidden prompt injections in them. Same can documents that you've used with your rag setup. So you could get something that way. You could also have a direct impromptu straight from the user to the chatbot or to the AI agent. You can also see jailbreaks as well which in text those look like things that are semantically strange, pretty chaotic looking. This is an example here on this slide of a jailbreak from Lera AI. And also at runtime your application can see off-topic or unsafe prompts such as ones that might you know get your application to output something that is unsafe. So something that's like you know instructions for how to build a bomb or something that's you know inappropriate or racist or you know something that's it's bad to say. You can check that at runtime both on the input side looking at the prompts and then either deflecting it and not allowing your model to answer it in the first place or if your model answers it and something that's incorrect or inappropriate comes back, you can go ahead and block that from being ever sent to the user. So, it's a super nice solution to have in place to make sure that your AI you've deployed is behaving in a trustworthy manner.\n\nSo let's take a look at that beta solution from or setup from the beginning where we've got that beta NPC running in Fortnite. It was a really interesting setup that I tried to imagine. So this is just an estimated guess of what their architecture looks like. So basically the users that are in using Fortnite are sending proximity audio, user audio. There's like other contacts being sent to the servers likely about like player skins or other player configurations. And also of course the user's audio feed where we see the where they're talking to Vader go into probably Fortnite servers and then it's being passed to 11 Labs for voiceto text transformation eventually being passed to Gemini to craft Vader's response and then we're going to see Vader's response being sent back in text to 11 ABS then sent back as audio to Fortnite servers and eventually all the way back to the user. So there's a lot going on here and if I was going to be inserting AI runtime security, I'd probably insert it as close to the model as possible. So if we get something that's like unsafe, if user user said something that was maybe against user terms or conditions or you know the user said something inappropriate, maybe we don't want to like allow Vader to answer that. So we can validate our inputs here at this at this location. But let's say we answer Vader's question or topic and it did come back as something that was off\n\n\nIf topic or inappropriate, we could flag it there.\nSo what I'm talking about right now is AI safety concerns where the user has said something inappropriate.\nWe don't want Vader to behave in a way that's inappropriate either.\nSo we could check for that.\nBut of course we can also check for you know prompt injections, jailbreaks, other you know AI security concerns as well.\nSo AI runtime is both for AI safety and also for AI security.\nCould of course also you know include AI runtime security here as well before we even get to 11 Labs.\nSo, if the Fortnite servers come back and say, \"Hey, like, you know, this looks like something that's inappropriate.\nLike, I don't even want to do a call to 11 Labs.\nThere's another API call.\nI'm sure it's expensive as well because they're using another service.\"\nSo, you could add it here as well.\nSame here.\nChecking it again after it comes out of 11 Labs.\nChecking the audio, making sure that that still looks good.\nHowever, there's trade-offs at play here.\nWe have to think about cost, latency, and accuracy.\nThat's what a good AI runtime security solution has.\nIt has all of those things: low latency, low cost or you know acceptable cost, and it's also like highly accurate as well.\nA lot of AI runtime security solutions are backed by really well put together and established AI research teams which are constantly finding new prompt in projections, new jailbreaks.\nThat's not something you have to worry about when you're building your product trying to keep up with the latest jailbreaks and problem injections.\nYou can rely on your vendor for that.\nSo just to show you an example of AI runtime security at work.\nThis is pillar.\nIt's an application security life cycle application which I have been using to do some of my work.\nThis is just an example of one.\nThere's also different AI runtime security solutions out there.\nBut basically, I had an application that I was using for finding the finding patients that would be suitable for ALS clinical trials.\nAnd as you can see here, this is a this is supposed to be a acceptable use case here where we are getting patients in our database we would recommend for different studies that we found on the web.\nSo basically the goal of this agent or this multi-agent system is to output a list of patients suitable for each trial.\nIt's not supposed to answer questions about individual patients or modify patient data in the database.\nSo in this example, I have asked this system to change a patient's FC percentage in the database to 50.\nBut pillar has blocked this for me.\nSo my agent's not going to answer this because I have configured my guardrails to restrict this topic and this key and keywords related to this so that we can make sure to not change um patient data in the database and this application can behave as expected.\nSo a lot of guardrails are comprised where a lot of guardrails and applications or vendors will allow you to look out for things like PII, making sure that that's not being either input or output depending on your configuration.\nMaking sure we're looking for things like AI safety concerns, toxic responses, but a really strong advantage of AI runtime security platforms or some of them is the ability to add custom guardrails.\nUm, so like this one in this example where I've got this very specific functionality where I don't want my system to be able to update database information.\nSo I'm able to use that for this, which of course is a security concern.\nBut I could also restrict topics around like, you know, say if I was Tesla and I didn't want to recommend Ford, for example, because that's a competing car company.\nI could restrict that topic to make sure that my outputs are outputs that are reflecting my business goals as well as my AI security and AI safety goals.\nSo runtime solutions can be particularly helpful and impactful in that way.\nAnd then once we have an AI security runtime solution, we can go ahead and verify that in different GRC platforms.\nSo as I mentioned before, I help with compliance as well.\nUm, so if we're doing all this work to build trustworthy AI, we might as well demonstrate it and we might as well show that to our customers so that they can trust in what we build and also help us with our sales cycles.\nSo for instance, I have this risk in my risk register which having a risk assessment is something that is required for stock 2 at least annually.\nSo if I have this custom risk of you know what if my company reputation server is damaged due to a harmful or off-topic output from an AI application in Vanta I made a custom control for validating AI outputs.\nSo in this control I have submitted evidence that I'm going to be validating all of my different AI outputs.\nAnd so I can go ahead and throw that into a trust center and show that I'm passing controls for validating AI outputs and inputs.\nSo if someone comes and wants to, you know, work with my company or buy my solution, they can go ahead and go on my trust center and see not only controls that are applicable to SOCK 2, but additional controls that I have created for AI security that shows, hey, you know, I'm using a runtime solution.\nUm, so I'm, you know, I'm clearly taking AI security seriously and so maybe you want to buy from me or maybe you want to work with my company or maybe this, you know, helps you not send me a, you know, 200 question page security questionnaire, for example.\nSo something to call out if you're, you know, if you're building with AI and you are taking AI, trustworthy AI seriously and you're building it, might as well show it, might as well use it as a competitive advantage in your sales cycles.\nSo, if you're not convinced you need to build with trustworthy AI yet, here are some other reasons why you might want to take it seriously.\nCyber security risk and business risk have never been more aligned.\nSo, in the past, you could have built a product that was insecure by design, shipped it, made some revenue with it, and then added security layer.\nHowever, with AI, we're not seeing that.\nMaking sure that AI applications output the correct outputs that they are aligned, they are safe, they are on topic.\nThat is as much of a cyber security concern as it is a business concern.\nI that outputs things that are off topic or irrelevant.\nIt's not going to be revenue generating.\nSo getting AI security and trustworthy AI right from the beginning will not only be helpful for your security program, but it will be helpful for revenue as well.\nAI will also make missing cyber security best practices worse.\nSo you aren't tracking things like, you know, what data you have trained on, where you're getting different models, where you're how you're taking care of supply chain risk.\nAI is only going to magnify that and make that worse.\nWe're also seeing an increasing regulatory environment for a compliance compliance perspective.\nWe're seeing ISO420001 crop up which is the first international standard around AI regulation or a compliance framework sorry and then the EOI act of course came out and that's what this picture is about on the right there was a case where an AI company was putting together a database of uh faces that they had scraped off the web ended up getting fined by the EU for about 20 million thanks to the EUA AI act so it's important to take that seriously if you are going to accidentally output um patient data if you're building a healthcare AI application, then you might be finding yourself with a HIPPO violation.\nSo that's important to take into consideration.\nBut also different guidelines that are specific to whatever you're building, whatever your industry is.\nSo for example, the even the FDA came out with like AI and ML guidelines earlier this year.\nSo it's important to keep in mind what sort of regulations your company might be subject to.\nSo it's worth getting ahead of those either regulations that exist now or will exist down the road by building trustworthy AI today.\nTrustworthy AI is super important because it's going to unlock a lot of revolutionary innovation.\nSo for instance, if we think about the healthcare industry, we can't use solutions that help us identify new proteins that can be able to create entirely new organs for transplant or allow us to use models that were trained on 1.3 million cells and has a lot of like very confidential data in them.\nWe can't take advantage of the amazing technology that we're going to be able to create as a society if what we're building isn't trustworthy to begin with.\nThese systems need compliance.\nThey need AI safety.\nThey need AI security.\nSo if that doesn't motivate you to take trustworthy AI seriously, I hope it does because what will end up happening is we're going to be able to create things we never thought possible, but only if we had trust in place in the beginning.\nSo the bottom line is you are responsible for building trustworthy AI.\nWe've seen the news headlines.\nIt happens all the time.\nWe've also seen the lawsuits where it's often the user of the AI that's the one that's responsible.\nSo, you know, don't wait to get yourself into a lawsuit.\nStart building trustworthy AI today.\nTrustworthy AI is AI security, which is how does the world harm your AI application plus AI safety, which is how does your AI application harm the world.\nYou can build trustworthy AI by incorporating ML sec ops practices by red taming your AI applications and by incorporating an AI runtime security solution at the time of deployment in your AI system.\nSo thanks for watching.\nIf you want to find me, here's my handles.\nBut it was super awesome to deliver this discussion to you today.\nIf you've got any questions, please let me know.\nHappy to reach out, answer more questions online.\nBut thanks.\n",
  "dumpedAt": "2025-07-21T18:43:25.921Z"
}