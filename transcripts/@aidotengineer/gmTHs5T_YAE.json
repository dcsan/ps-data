{
  "episodeId": "gmTHs5T_YAE",
  "channelSlug": "@aidotengineer",
  "title": "Optimizing inference for voice models in production - Philip Kiely, Baseten",
  "publishedAt": "2025-07-01T07:00:06.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1.72,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "Uh hello everyone. Thank you so much for",
      "offset": 14.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "being here for uh sticking around for",
      "offset": 16.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this talk. Um I'm going to be talking",
      "offset": 18.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "about optimizing inference for voice",
      "offset": 20.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "models in production. I'm going to be",
      "offset": 22,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "talking mostly about the runtime",
      "offset": 24.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "component but also just a little bit on",
      "offset": 26.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the infrastructure side. Um just a quick",
      "offset": 28.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "introduction. Um I'm Philip from B 10.",
      "offset": 31.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Base 10 is a model inference platform.",
      "offset": 33.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Uh we run production workloads for a",
      "offset": 35.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "wide variety of AI native startups and",
      "offset": 38.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "enterprises. Um I'm based in here in SF.",
      "offset": 40.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Um I actually just moved here. It's",
      "offset": 43.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "really awesome. My favorite part about",
      "offset": 45.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "being in SF is much better sports teams",
      "offset": 46.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "than I had in Chicago. Um and uh one of",
      "offset": 48.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "my favorite voice models is Orpheus TTS.",
      "offset": 52.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um which we're going to be talking about",
      "offset": 55.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "a whole bunch today. Um quick agenda. So",
      "offset": 56.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "we're going to talk about TTS model",
      "offset": 60.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "architecture like what is a text to",
      "offset": 62,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "speech model actually when you look on",
      "offset": 63.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the config in hugging face. Uh what sort",
      "offset": 65.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of performance metrics are we looking",
      "offset": 68.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "at? What sort of optimization techniques",
      "offset": 69.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "can we do to make the model better? Um",
      "offset": 72.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "how do we measure whether or not we",
      "offset": 74.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "succeeded? And then finally, what can we",
      "offset": 76.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "do on the infrastructure and client code",
      "offset": 78.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to not shoot ourselves in the foot after",
      "offset": 80.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "doing a ton of runtime work and then",
      "offset": 82.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "just adding all that latency back by not",
      "offset": 84.799,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "doing our client code correctly.",
      "offset": 87.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So architecture,",
      "offset": 89.759,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this is one of the things I've been",
      "offset": 91.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "learning this year which has been pretty",
      "offset": 93.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "great uh to to realize it's made life a",
      "offset": 94.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "lot simpler at the runtime level. Now",
      "offset": 97.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this is wrong like the the the thing up",
      "offset": 99.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here uh that that I'm going to say is",
      "offset": 101.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that like everything is an LLM. uh that",
      "offset": 103.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is that is wrong but it's useful. Um",
      "offset": 106.24,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "there's kind of like two types of",
      "offset": 108.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "models. There's autogressive",
      "offset": 109.439,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "transformers models that are LLM or very",
      "offset": 111.119,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "LLM adjacent. Um you see this in",
      "offset": 113.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "embeddings. You see this in",
      "offset": 115.759,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "transcription with stuff like whisper",
      "offset": 117.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "TTS um is another example. You also have",
      "offset": 119.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the more like diffuser image type",
      "offset": 121.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "models. Um which is like a very",
      "offset": 123.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "different optimization problem. But",
      "offset": 125.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "something that's cool is because TTS",
      "offset": 127.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "models are so architecturally similar to",
      "offset": 129.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "LLMs or in many cases derived directly",
      "offset": 132.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "from LLMs, we can access the rich",
      "offset": 134.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "ecosystem of LLM tooling and use it to",
      "offset": 137.12,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "make TTS models better. So the TTS model",
      "offset": 139.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that we're going to be using and as an",
      "offset": 143.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "example all day um is Orpheus TTS. We're",
      "offset": 145.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "using it for two reasons. Okay, three",
      "offset": 148.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "reasons. The two reasons are because",
      "offset": 150.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it's open source and it's really good.",
      "offset": 152,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And also I think uh Elias and Amu and",
      "offset": 153.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "everyone at Canopy Labs is really",
      "offset": 156,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "awesome. So uh that's the that's the",
      "offset": 157.44,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "third reason we're we're talking about",
      "offset": 159.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "their model. But it's a Llama 3.2 3B",
      "offset": 160.959,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "backbone. So like if you look at this is",
      "offset": 164.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the little like config from hugging face",
      "offset": 166.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "copy and pasted onto the screen. It's a",
      "offset": 169.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "llama for causal LM architecture. And so",
      "offset": 171.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "because of that we can do like all of",
      "offset": 174.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "our normal llama stuff to this model and",
      "offset": 176.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "make it faster. Um they did a couple",
      "offset": 178.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "things. I mean they did a bunch of",
      "offset": 181.12,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "things to make it work but a couple",
      "offset": 182.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "things that are relevant here. There is",
      "offset": 183.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a larger vocab size because you need all",
      "offset": 185.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the speech specific tokens like laugh",
      "offset": 187.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and stuff. Um and then they also",
      "offset": 189.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "extended the context length for thrope",
      "offset": 191.84,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "scaling. So we got to make sure",
      "offset": 193.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "everything we do supports that. So",
      "offset": 195.519,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "performance metrics like what do we want",
      "offset": 198.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to actually do here? Uh we we think",
      "offset": 199.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "about LLM metrics um a little bit here.",
      "offset": 203.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Uh we we just look at them a little bit",
      "offset": 205.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "differently. So in LLMs you talk about",
      "offset": 208.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "time to first token. Now we're talking",
      "offset": 210.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "about time to first bite or sometimes",
      "offset": 212.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "even time to first sentence. Uh we we",
      "offset": 213.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "need a little bit more of a useful",
      "offset": 216.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "output um from the model before we",
      "offset": 218.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "really start feeling good about our",
      "offset": 220,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "response time. We do think about tokens",
      "offset": 221.28,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "per second although we're going to think",
      "offset": 223.519,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "about it differently which I'll explain",
      "offset": 224.879,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "later. And we mostly think about",
      "offset": 226.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "throughput which is you know how many",
      "offset": 228.319,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "requests are we able to serve at a given",
      "offset": 229.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "time.",
      "offset": 231.519,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So on that, you know, goals perspective,",
      "offset": 232.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "if you ask me like, &quot;Hey, Philip, how do",
      "offset": 235.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you want to optimize llama in general?&quot;",
      "offset": 237.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I'll like I'll say, &quot;Well, we want a lot",
      "offset": 239.599,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of TPS. We want hundred. We want 500",
      "offset": 241.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "TPS. We want a thousand tokens per",
      "offset": 243.519,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "second. We want as many tokens per",
      "offset": 245.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "second as we can get.&quot; With voice",
      "offset": 246.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "models, you actually don't necessarily",
      "offset": 248.799,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "need that. In many cases, you only want",
      "offset": 250.4,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "as many tokens per second as you need",
      "offset": 252.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "for a real-time stream. for Orpheus",
      "offset": 254.879,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that's like 83 tokens per second which",
      "offset": 256.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for like a three billion parameter LLM",
      "offset": 259.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "is nothing. Um but what we actually want",
      "offset": 261.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "to do instead is we want to once we hit",
      "offset": 264.639,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "that mark start optimizing for time to",
      "offset": 267.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "first bite so that our latency is really",
      "offset": 270.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "good and start optimizing for",
      "offset": 271.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "concurrency so that we can get more",
      "offset": 273.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "connections and spend less on GPUs.",
      "offset": 275.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "So our goal in general if all of these",
      "offset": 278.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "very nice and definitely not AI",
      "offset": 281.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "generated people are all the different",
      "offset": 283.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "like voices that our that our model is",
      "offset": 284.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "capable of creating these are all the",
      "offset": 287.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "voice agents that we're running. How can",
      "offset": 288.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we make all of these people fit on one",
      "offset": 290.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "or even less than one GPU? That's the",
      "offset": 293.36,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "goal. So how do we do it?",
      "offset": 296.4,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Bunch of ways. Um so first off it's an",
      "offset": 300.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "LLM. Um, if you are running an LLM with",
      "offset": 303.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "like VLM for example, um, you can",
      "offset": 306.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "generally in many cases get better",
      "offset": 308.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "performance with Tensor Rotlm. Uh,",
      "offset": 310.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Tensor Rot is something that we've been",
      "offset": 312.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "using at Base 10 a lot. I like to joke",
      "offset": 314.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that I'm the unofficial marketing",
      "offset": 316.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "department for Tensor RT LLM because of",
      "offset": 317.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "how much I talk about it. Uh, but it",
      "offset": 320,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "really is fast. It can be a little bit",
      "offset": 321.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "complicated from a developer experience",
      "offset": 323.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "perspective to get up and running with",
      "offset": 325.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "it, but once you are up and running, um,",
      "offset": 327.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "it it works really well. Uh, we can also",
      "offset": 329.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "just like quantize the model. Um, even",
      "offset": 331.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "though it's small, you can always make",
      "offset": 333.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it faster by making it smaller. Uh, with",
      "offset": 334.56,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Hopper architecture, we quantize this",
      "offset": 336.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "model to FP8 pretty successfully. I know",
      "offset": 339.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "usually quantizing really small models",
      "offset": 341.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "like this can lead to performance",
      "offset": 343.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "degragation, but for this model, it's",
      "offset": 345.199,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "working pretty well in FP8, even when we",
      "offset": 347.199,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "quantize the KV cache. And then a lot of",
      "offset": 349.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the other runtime stuff is actually more",
      "offset": 351.759,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "like audio specific than it is LLM",
      "offset": 353.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "specific. So, one of the big challenges",
      "offset": 356.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that we don't have with LLMs, which are",
      "offset": 358.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "just parsing nice, convenient bits of",
      "offset": 360.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "text back and forth, is you have your",
      "offset": 362.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "audio, you have your audio codec, you",
      "offset": 363.919,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "have your decoding, all that kind of",
      "offset": 365.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "stuff. So, we use snack, which I was",
      "offset": 366.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "very disappointed to learn is not an",
      "offset": 369.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "actual tasty snack, um, but an audio",
      "offset": 371.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "decoder. And we actually use torch",
      "offset": 373.199,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "compile, um, and torch compile you might",
      "offset": 375.44,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "be used to running on, you know, a a",
      "offset": 378.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "model u compiling your model weights to",
      "offset": 381.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "make your runtime faster. We're actually",
      "offset": 384.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "using the same kind of system with torch",
      "offset": 386.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "compile and with um PyTorch inference",
      "offset": 388.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "mode on the audio decoder and running",
      "offset": 390.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that on the GPU. Um we make sure that",
      "offset": 393.36,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "all the token batching um token level",
      "offset": 396.56,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "batching works well throughout the",
      "offset": 398.479,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "entire pipeline and support multiple",
      "offset": 399.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "streaming protocols. Um yeah, so these",
      "offset": 402.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "are the engine settings that you would",
      "offset": 405.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "need. um you've got the you know",
      "offset": 407.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "quantization type of FP8 KV the FP8",
      "offset": 408.88,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "context uh FMHA um in order to you know",
      "offset": 412.16,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "support the uh support the um hopper",
      "offset": 416.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "architecture and the",
      "offset": 419.599,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "uh quantization there. Um and here's a",
      "offset": 422.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "quick code sample of I got a little",
      "offset": 426.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "ahead of my slides I guess. Here's a",
      "offset": 428.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "little quick code sample of the audio",
      "offset": 429.599,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "decoding. Um so we are basically you",
      "offset": 431.599,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "know batching. Um usually we would talk",
      "offset": 435.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "about continuous batching when we're",
      "offset": 438.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "talking about LLM optimization. We want",
      "offset": 440,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to package all those tokens together. In",
      "offset": 442.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this case we are doing dynamic batching.",
      "offset": 444.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Um so we're trying to pack as much into",
      "offset": 446.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a batch as we can but every 15",
      "offset": 448.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "milliseconds we're going to uh shoot it",
      "offset": 450.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "out. Um you've got that timeout setup",
      "offset": 452.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "here. Um if you want to trade off um for",
      "offset": 454.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a little bit of latency for more",
      "offset": 457.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "throughput you can make that batch",
      "offset": 458.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "bigger. Um, so yeah, we don't have token",
      "offset": 460.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "level continuous batching yet here, but",
      "offset": 462.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we do have dynamic batching, which is",
      "offset": 464.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "going to get you pretty close. Um, and",
      "offset": 466.16,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "because of this, actually something",
      "offset": 468.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "that's that I was surprised about uh",
      "offset": 469.919,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "when we profiled this is that our TTS",
      "offset": 472.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "imple implementation with Orpheus is",
      "offset": 475.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "actually in many cases CPUbound, which",
      "offset": 477.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "is kind of where you want to be. Um, you",
      "offset": 479.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "can throw more CPUs at a resource uh",
      "offset": 481.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "pretty pretty efficiently. Um even",
      "offset": 484.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "though the next token production and the",
      "offset": 486.24,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "audio co decoding are both on the GPU um",
      "offset": 487.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "both of those loops hit the CPU at",
      "offset": 490.879,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "different points um and that can",
      "offset": 492.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually be the bottleneck in the number",
      "offset": 494.479,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "of simultaneous streams that we're able",
      "offset": 496,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to create.",
      "offset": 497.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "So how do we do like I just showed you a",
      "offset": 500.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "lot of code and talked through it really",
      "offset": 502.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "quickly without really getting into",
      "offset": 504.319,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "depth. Uh that could all just be smoke",
      "offset": 505.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and mirrors. Uh let's see let's see if",
      "offset": 507.759,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it's actually any faster. Um, so again,",
      "offset": 509.68,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "the number one thing is going to be",
      "offset": 512.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "simultaneous streams because you want to",
      "offset": 513.919,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "be able to be very costefficient and use",
      "offset": 517.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "fewer GPU resources to serve a, you",
      "offset": 519.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "know, large amount of traffic. And in",
      "offset": 522.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this case, our base implementation, I",
      "offset": 524.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "don't necessarily want to like call",
      "offset": 526.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "anyone out because there's a lot of",
      "offset": 528,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "really good ways to run this model. Um,",
      "offset": 529.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you can get really good performance with",
      "offset": 531.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "VLM. Um but this is just kind of like",
      "offset": 533.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the offtheshelf um just take it run it",
      "offset": 535.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "completely standard uh implementation.",
      "offset": 539.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "So with variable traffic we're able to",
      "offset": 542,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "support 16 simultaneous streams and with",
      "offset": 544,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "constant traffic 24 simultaneous streams",
      "offset": 546.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "on um an H100 MIG. So this is actually",
      "offset": 549.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "half an H100 GPU. It's a skew that we do",
      "offset": 552.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a lot because it's really good for these",
      "offset": 554.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "small models where you want the Hopper",
      "offset": 556.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "performance, the Hopper architecture",
      "offset": 558.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "uplift in Tensor RTLM, the FP8 support,",
      "offset": 560.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "but you don't want to pay for like an",
      "offset": 563.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "entire 80 gigabyte GPU for just a 3",
      "offset": 565.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "billion parameter model. So, you know,",
      "offset": 567.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "we're seeing much better um much much",
      "offset": 570.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "better concurrency. So, if you kind of",
      "offset": 573.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like price that out with like our list",
      "offset": 575.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "prices and stuff, um, you can get, you",
      "offset": 576.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "know, a few cents per hour of of",
      "offset": 579.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "conversation, um, which is going to be,",
      "offset": 581.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you know, substantially better than if",
      "offset": 583.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you're if you have the volume for it.",
      "offset": 585.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "It's going to be substantially better",
      "offset": 587.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "than paying for a sort of like pertoken",
      "offset": 588.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "type API.",
      "offset": 591.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "But, okay, sure, maybe it's cheap at",
      "offset": 593.519,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "scale, but is it fast? Yes, it's fast.",
      "offset": 595.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "Um so with the you know with the tot",
      "offset": 598.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "implementation on the MIGs and on the",
      "offset": 601.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "H100s we can actually get all the way",
      "offset": 603.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "down to 150 millisecond time to first",
      "offset": 605.519,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "bite um in like real world uh testing",
      "offset": 607.839,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that we've done. Now that we'll we'll",
      "offset": 610.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "talk in a minute like that doesn't mean",
      "offset": 613.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "your whole pipeline is that fast. That's",
      "offset": 614.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "just like one part of the pipeline. Uh",
      "offset": 616.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "but it's it's important because you know",
      "offset": 618.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you definitely don't want to be spending",
      "offset": 621.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "a lot of time waiting around for that",
      "offset": 622.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "first token.",
      "offset": 624.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So",
      "offset": 627.279,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "to kind of transition into that that",
      "offset": 628.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "discussion of like what can go wrong",
      "offset": 631.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "here like you have this graph and you",
      "offset": 633.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "have this you know nice uh config that I",
      "offset": 635.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "had up here and you're like all right",
      "offset": 639.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "cool I'm going to take this I'm going to",
      "offset": 640.56,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "put it in production and I'm going to",
      "offset": 641.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "see the results that uh that he put up",
      "offset": 643.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "on screen and it's going to work great",
      "offset": 645.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and the answer is no it's not it's a",
      "offset": 647.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "little bit harder than that. Um, so the",
      "offset": 649.519,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "thing is like non-runtime factors when",
      "offset": 652.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we get especially with these small",
      "offset": 654.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "models and with these multimodal systems",
      "offset": 656.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "can actually be like way more important",
      "offset": 659.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "than your runtime. Um, and that's your",
      "offset": 661.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "lat your infrastructure and your client",
      "offset": 664.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "code because you know I I showed here",
      "offset": 666.16,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "all right maybe maybe I got it you know",
      "offset": 670.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "I I cut the runtime in in half um from",
      "offset": 672.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "from the base implementation I I saved a",
      "offset": 674.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "couple hundred milliseconds very easy to",
      "offset": 677.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "add those couple hundred milliseconds",
      "offset": 679.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "back and well beyond that by you know",
      "offset": 681.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "sending my query to New York instead of",
      "offset": 683.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "California or by having to establish a",
      "offset": 686.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "session every time I uh you know run my",
      "offset": 689.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "client code. Um, so a few like pitfalls",
      "offset": 691.44,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "to avoid. Um, number one, like if you go",
      "offset": 695.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "in, you know, our our model library or",
      "offset": 698.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "something and we're just trying to get",
      "offset": 701.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you started very quickly um with this",
      "offset": 702.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with this kind of inference sample. Um,",
      "offset": 704.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it's basically going to be, hey, use",
      "offset": 706.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "requests, make a stream, stream it to",
      "offset": 708.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "your local computer and start, you know,",
      "offset": 710.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "playing it on ffmpeg or something. The",
      "offset": 712.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "issue is that here like the requests are",
      "offset": 716,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "going to be sent sequentially and you",
      "offset": 717.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "need to create a new session every time.",
      "offset": 719.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "um that takes time. So if you're using",
      "offset": 721.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "this in production, you want a code",
      "offset": 723.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sample. By the way, this is all up on",
      "offset": 726.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "our GitHub. Um you'll want a code sample",
      "offset": 728.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that looks a lot more like a",
      "offset": 730.639,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "benchmarking script where you're using a",
      "offset": 731.839,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "um multipprocess pool. um you're sharing",
      "offset": 735.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the you're sharing the session between",
      "offset": 738.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "all of these different requests and",
      "offset": 740.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "you're actually sending traffic with the",
      "offset": 742.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "concurrency that allows you to, you",
      "offset": 744.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "know, saturate this benchmark with, you",
      "offset": 747.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "know, the the multiple concurrent",
      "offset": 749.2,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "requests.",
      "offset": 750.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Finally, both of these code samples,",
      "offset": 753.92,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "they do sit on top of HTTP and HTTP",
      "offset": 755.76,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "streaming. Um, in many cases, if you're",
      "offset": 759.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "implementing voice pipelines, you're",
      "offset": 761.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "going to use something like LiveKit or",
      "offset": 763.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Pipecat or something. And you're also",
      "offset": 764.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "potentially going to be using a",
      "offset": 766.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "different protocol. You're going to be",
      "offset": 767.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "using something like websockets or gRPC,",
      "offset": 769.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "which we do have support for. Um, and",
      "offset": 771.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "finally, I wanted to leave you on the",
      "offset": 775.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "thought that these uh, you know, these",
      "offset": 776.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "models are only one part of a voice",
      "offset": 779.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "agent pipeline. So like we can spend a",
      "offset": 781.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "lot more than 15 minutes actually",
      "offset": 783.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "talking about like the very detailed uh",
      "offset": 785.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "implementation mechanics of making your",
      "offset": 788.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "voice model faster of you know we we",
      "offset": 791.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "haven't even touched on stuff like",
      "offset": 794.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "fine-tuning the model um you know custom",
      "offset": 795.519,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "voices zeroot voice cloning um being",
      "offset": 797.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "able to you know remove static and",
      "offset": 800.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "popping at the end of messages there's",
      "offset": 802.8,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "there's there's a lot of work to do just",
      "offset": 805.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "on the voice part but it really only is",
      "offset": 806.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "onethird of the problem when I think",
      "offset": 809.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "about voice agents. I think about three",
      "offset": 811.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "parts, listening, thinking, talking. Um,",
      "offset": 812.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and the most important thing here is",
      "offset": 815.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "again, while you can have great run",
      "offset": 817.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "times, the infrastructure to connect",
      "offset": 820.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these three together is really what's",
      "offset": 822.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "going to determine your latency. Being",
      "offset": 823.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "able to go from one model to have the",
      "offset": 826,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "next one running in the same data center",
      "offset": 829.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "with, you know, minimal like minimal",
      "offset": 831.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "network overhead in between the two. uh",
      "offset": 834.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "even things as simple as not having to",
      "offset": 837.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "go off and do a hair pin um at the DNS",
      "offset": 839.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "level and come back. If that saves you",
      "offset": 842,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "10 milliseconds on every step and your",
      "offset": 844.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "voice pipeline has this and you know a",
      "offset": 846.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "chunking algorithm, it's got an",
      "offset": 848,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "interruption model and so you end up",
      "offset": 849.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "having four or five steps. Well, there",
      "offset": 852.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "just hair pinning alone is costing you",
      "offset": 854.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "40 or 50 milliseconds and that can be",
      "offset": 856.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "10% of your SLA for for a voice model.",
      "offset": 859.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "So yeah, uh that's that's my that's my",
      "offset": 862.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "main point here is that as much fun as",
      "offset": 865.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "it is to talk about the runtime stuff",
      "offset": 867.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and as much work as we do there, the the",
      "offset": 869.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "infrastructure and the client",
      "offset": 871.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "implementation is equally important if",
      "offset": 873.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "not more so. Anyway, thank uh so yeah,",
      "offset": 875.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that's the that's the review. Um thank",
      "offset": 878.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you all for coming through. Uh I have uh",
      "offset": 880.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we're doing an event next week at uh",
      "offset": 882.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Fogo to Chiao which is going to be",
      "offset": 884.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "pretty fun. I'm going to be talking in",
      "offset": 886.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "more detail about building some uh",
      "offset": 888.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "systems with open source models and",
      "offset": 890.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "there's also going to be a lot of stake.",
      "offset": 892.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "So definitely come on through uh if",
      "offset": 894.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "you're interested and um I'm on I'm on",
      "offset": 895.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Twitter, I'm on LinkedIn, so it's base",
      "offset": 898.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "10. Um hit me up if you have any",
      "offset": 900.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "questions about this or anything else",
      "offset": 901.839,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "model performance. Thank you so much and",
      "offset": 903.279,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "I'll let you go 8 seconds early.",
      "offset": 905.6,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 911.58,
      "duration": 3
    }
  ],
  "cleanText": "[Music]\n\nHello everyone. Thank you so much for being here, for sticking around for this talk. I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component, but also just a little bit on the infrastructure side. Just a quick introduction. I'm Philip Kiely from Baseten. Baseten is a model inference platform. We run production workloads for a wide variety of AI native startups and enterprises. I'm based here in San Francisco. I actually just moved here. It's really awesome. My favorite part about being in San Francisco is much better sports teams than I had in Chicago. And one of my favorite voice models is Orpheus TTS, which we're going to be talking about a whole bunch today. Quick agenda. So we're going to talk about TTS model architecture, like what is a text-to-speech model actually when you look on the config in Hugging Face? What sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? How do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly?\n\nSo, architecture, this is one of the things I've been learning this year, which has been pretty great, to realize it's made life a lot simpler at the runtime level. Now, this is wrong, like the thing up here, that I'm going to say is that like everything is an LLM. That is wrong, but it's useful. There's kind of like two types of models. There's autoregressive transformers models that are LLM or very LLM adjacent. You see this in embeddings. You see this in transcription with stuff like Whisper. TTS is another example. You also have the more like diffuser image type models, which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs, or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using as an example all day is Orpheus TTS. We're using it for two reasons. Okay, three reasons. The two reasons are because it's open source and it's really good. And also, I think Elias and Amu and everyone at Canopy Labs is really awesome. So that's the third reason we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little config from Hugging Face, copy and pasted onto the screen. It's a Llama for causal LM architecture. And so because of that, we can do like all of our normal Llama stuff to this model and make it faster. They did a couple things. I mean, they did a bunch of things to make it work, but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. And then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics, like what do we want to actually do here? We think about LLM metrics a little bit here. We just look at them a little bit differently. So in LLMs, you talk about time to first token. Now we're talking about time to first byte, or sometimes even time to first sentence. We need a little bit more of a useful output from the model before we really start feeling good about our response time. We do think about tokens per second, although we're going to think about it differently, which I'll explain later. And we mostly think about throughput, which is, you know, how many requests are we able to serve at a given time.\n\nSo on that, you know, goals perspective, if you ask me like, \"Hey, Philip, how do you want to optimize Llama in general?\" I'll say, \"Well, we want a lot of TPS. We want 100. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get.\" With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. For Orpheus, that's like 83 tokens per second, which for like a three billion parameter LLM is nothing. But what we actually want to do instead is we want to, once we hit that mark, start optimizing for time to first byte so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs.\n\nSo our goal in general, if all of these very nice and definitely not AI generated people are all the different like voices that our model is capable of creating, these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. So how do we do it?\n\nBunch of ways. So first off, it's an LLM. If you are running an LLM with like VLM, for example, you can generally, in many cases, get better performance with TensorRT-LLM. TensorRT-LLM is something that we've been using at Baseten a lot. I like to joke that I'm the unofficial marketing department for TensorRT-LLM because of how much I talk about it. But it really is fast. It can be a little bit complicated from a developer experience perspective to get up and running with it, but once you are up and running, it works really well. We can also just like quantize the model. Even though it's small, you can always make it faster by making it smaller. With Hopper architecture, we quantize this model to FP8 pretty successfully. I know usually quantizing really small models like this can lead to performance degradation, but for this model, it's working pretty well in FP8, even when we quantize the KV cache. And then a lot of the other runtime stuff is actually more like audio specific than it is LLM specific. So, one of the big challenges that we don't have with LLMs, which are just parsing nice, convenient bits of text back and forth, is you have your audio, you have your audio codec, you have your decoding, all that kind of stuff. So, we use Snack, which I was very disappointed to learn is not an actual tasty snack, but an audio decoder. And we actually use torch compile, and torch compile you might be used to running on, you know, a model, compiling your model weights to make your runtime faster. We're actually using the same kind of system with torch compile and with PyTorch inference mode on the audio decoder and running that on the GPU. We make sure that all the token batching, token level batching works well throughout the entire pipeline and support multiple streaming protocols. Yeah, so these are the engine settings that you would need. You've got the, you know, quantization type of FP8 KV, the FP8 context, FMHA, in order to, you know, support the Hopper architecture and the quantization there. And here's a quick code sample of, I got a little ahead of my slides, I guess. Here's a little quick code sample of the audio decoding. So we are basically, you know, batching. Usually we would talk about continuous batching when we're talking about LLM optimization. We want to package all those tokens together. In this case, we are doing dynamic batching. So we're trying to pack as much into a batch as we can, but every 15 milliseconds, we're going to shoot it out. You've got that timeout setup here. If you want to trade off for a little bit of latency for more throughput, you can make that batch bigger. So yeah, we don't have token level continuous batching yet here, but we do have dynamic batching, which is going to get you pretty close. And because of this, actually something that's that I was surprised about when we profiled this is that our TTS implementation with Orpheus is actually in many cases CPU-bound, which is kind of where you want to be. You can throw more CPUs at a resource pretty efficiently. Even though the next token production and the audio co-decoding are both on the GPU, both of those loops hit the CPU at different points, and that can actually be the bottleneck in the number of simultaneous streams that we're able to create.\n\nSo how do we do, like I just showed you a lot of code and talked through it really quickly without really getting into depth. That could all just be smoke and mirrors. Let's see if it's actually any faster. So again, the number one thing is going to be simultaneous streams because you want to be able to be very cost-efficient and use fewer GPU resources to serve a, you know, large amount of traffic. And in this case, our base implementation, I don't necessarily want to like call anyone out because there's a lot of really good ways to run this model. You can get really good performance with VLM. But this is just kind of like the off-the-shelf, just take it, run it, completely standard implementation. So with variable traffic, we're able to support 16 simultaneous streams, and with constant traffic, 24 simultaneous streams on an H100 MIG. So this is actually half an H100 GPU. It's a skew that we do a lot because it's really good for these small models where you want the Hopper performance, the Hopper architecture uplift in TensorRT-LLM, the FP8 support, but you don't want to pay for like an entire 80 gigabyte GPU for just a 3 billion parameter model. So, you know, we're seeing much better, much, much better concurrency. So, if you kind of like price that out with like our list prices and stuff, you can get, you know, a few cents per hour of conversation, which is going to be, you know, substantially better than if you're, if you have the volume for it. It's going to be substantially better than paying for a sort of like per-token type API.\n\nBut, okay, sure, maybe it's cheap at scale, but is it fast? Yes, it's fast. So with the, you know, with the tot implementation on the MIGs and on the H100s, we can actually get all the way down to 150 millisecond time to first byte in like real world testing that we've done. Now that we'll, we'll talk in a minute, like that doesn't mean your whole pipeline is that fast. That's just like one part of the pipeline. But it's important because, you know, you definitely don't want to be spending a lot of time waiting around for that first token.\n\nSo, to kind of transition into that discussion of like what can go wrong here, like you have this graph and you have this, you know, nice config that I had up here, and you're like, \"All right, cool, I'm going to take this, I'm going to put it in production, and I'm going to see the results that he put up on screen, and it's going to work great.\" And the answer is no, it's not. It's a little bit harder than that. So, the thing is, like non-runtime factors, when we get especially with these small models and with these multimodal systems, can actually be like way more important than your runtime. And that's your infrastructure and your client code, because, you know, I showed here, all right, maybe, maybe I got it, you know, I cut the runtime in half from the base implementation, I saved a couple hundred milliseconds. Very easy to add those couple hundred milliseconds back and well beyond that by, you know, sending my query to New York instead of California, or by having to establish a session every time I, you know, run my client code. So, a few like pitfalls to avoid. Number one, like if you go in, you know, our model library or something, and we're just trying to get you started very quickly with this kind of inference sample, it's basically going to be, \"Hey, use requests, make a stream, stream it to your local computer and start, you know, playing it on ffmpeg or something.\" The issue is that here, like the requests are going to be sent sequentially, and you need to create a new session every time. That takes time. So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. You'll want a code sample that looks a lot more like a benchmarking script where you're using a multiprocess pool. You're sharing the session between all of these different requests, and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the multiple concurrent requests.\n\nFinally, both of these code samples, they do sit on top of HTTP and HTTP streaming. In many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. And finally, I wanted to leave you on the thought that these, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed implementation mechanics of making your voice model faster, of, you know, we haven't even touched on stuff like fine-tuning the model, you know, custom voices, zero-shot voice cloning, being able to, you know, remove static and popping at the end of messages. There's a lot of work to do just on the voice part, but it really only is one-third of the problem when I think about voice agents. I think about three parts: listening, thinking, talking. And the most important thing here is again, while you can have great runtimes, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal, like minimal network overhead in between the two, even things as simple as not having to go off and do a hairpin at the DNS level and come back. If that saves you 10 milliseconds on every step, and your voice pipeline has this, and you know, a chunking algorithm, it's got an interruption model, and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds, and that can be 10%.\n\n\nOf your SLA for a voice model.\nSo yeah, uh that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the infrastructure and the client implementation is equally important if not more so.\nAnyway, thank you.\nSo yeah, that's the review.\nUm thank you all for coming through.\nUh I have we're doing an event next week at Fogo to Chiao which is going to be pretty fun.\nI'm going to be talking in more detail about building some systems with open source models and there's also going to be a lot of stake.\nSo definitely come on through if you're interested and I'm on Twitter, I'm on LinkedIn, so it's Baseten.\nUm hit me up if you have any questions about this or anything else model performance.\nThank you so much and I'll let you go 8 seconds early.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.719Z"
}