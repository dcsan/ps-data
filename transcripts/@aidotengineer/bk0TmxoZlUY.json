{
  "episodeId": "bk0TmxoZlUY",
  "channelSlug": "@aidotengineer",
  "title": "Evals 101 â€” Doug Guthrie, Braintrust",
  "publishedAt": "2025-06-27T10:53:13.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 4.04,
      "duration": 5.11
    },
    {
      "lang": "en",
      "text": "Hey everybody, my name is Doug Guthrie.",
      "offset": 15.519,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Uh, I'm a solutions engineer at Brain",
      "offset": 18.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Trust. Um, as you can see here, we're an",
      "offset": 21.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "endto-end developer platform for",
      "offset": 23.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "building AI products. We do evals. If",
      "offset": 25.359,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "you watched the the keynote this",
      "offset": 28.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "morning, you saw our founding engineer",
      "offset": 29.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "jumping up and down on stage yelling",
      "offset": 31.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "evals. I am not going to do that. I'm",
      "offset": 33.28,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "not as uh as funny or cool as him, but",
      "offset": 35.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh we should all be very excited about",
      "offset": 38.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "eval here.",
      "offset": 40.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Very brief agenda of of what we'll cover",
      "offset": 42.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "today in this sort of uh this intro uh",
      "offset": 45.2,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "very very brief company overview. Uh",
      "offset": 48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "give you an intro to evalu",
      "offset": 50.879,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "why would you even uh start thinking",
      "offset": 54,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "about using them? what are they? Uh what",
      "offset": 55.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "are the different components that you",
      "offset": 58.079,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "need to create an eval? Um some more",
      "offset": 60.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "brain trust specific things via uh",
      "offset": 63.199,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "running evals via our SDK. You'll see uh",
      "offset": 65.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "in the examples um that you can run",
      "offset": 68.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "evals both uh in the platform itself as",
      "offset": 71.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "well as the SDK. I think it's it's a",
      "offset": 74.08,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "really kind of cool thing that we can",
      "offset": 75.76,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "connect uh maybe the local development",
      "offset": 77.439,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that we're that we're doing with what",
      "offset": 79.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "we're doing within the platform. Uh and",
      "offset": 81.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "then how we then move to production. uh",
      "offset": 84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this is uh maybe human review. This is",
      "offset": 86.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "online scoring. So how well is our our",
      "offset": 88.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "application performing uh in production",
      "offset": 91.04,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and then lastly a little bit of human in",
      "offset": 93.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the loop uh getting user feedback from",
      "offset": 95.759,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "your users? How do we now uh take some",
      "offset": 97.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of these uh these production logs and",
      "offset": 100.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "feed them into the data sets that we're",
      "offset": 102.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "using in our eval uh creating this",
      "offset": 104.4,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "really great flywheel effect.",
      "offset": 106.56,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Cool. Quick quick company overview. Um",
      "offset": 111.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you see up there maybe in the top right",
      "offset": 114.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "uh some of our fun excuse me some of our",
      "offset": 116.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "investors. Uh maybe a quick call out on",
      "offset": 118.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the on the leadership side. Anker Goyle",
      "offset": 121.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is our CEO. Uh maybe the the reason to",
      "offset": 123.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "call out that is Anker in the last uh",
      "offset": 127.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "two two stops. He he essentially built",
      "offset": 130.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "brain uh brain trust uh from scratch the",
      "offset": 132.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "last two places. and and this is really",
      "offset": 135.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "where he found the the idea to like like",
      "offset": 137.599,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "maybe this is actually a a thing that",
      "offset": 140.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that people need and and really like the",
      "offset": 142.239,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "origination story of brain trust. Uh the",
      "offset": 144.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "other thing to call out here is like we",
      "offset": 148.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "already have a lot of companies using",
      "offset": 150,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "brain trust in production today. This is",
      "offset": 152.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "just a a few of uh the the companies",
      "offset": 154.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that are that are utilizing us for for",
      "offset": 157.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "evals for uh for observability of their",
      "offset": 159.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "their genai applications.",
      "offset": 161.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "So, I won't bore you too much with with",
      "offset": 165.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that, but uh let's jump into this. If",
      "offset": 167.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you're at the keynote, you probably saw",
      "offset": 169.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "a a similar slide here. I didn't I",
      "offset": 170.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "didn't take it out, but uh you see like",
      "offset": 172.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the tech luminaries here, I think, as as",
      "offset": 175.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Manu referenced them talking about evals",
      "offset": 177.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and the importance of them. I think this",
      "offset": 180.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "is a you know, obviously this is an",
      "offset": 182,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "intro to eval uh track and what better",
      "offset": 183.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "way to start this out with uh some some",
      "offset": 186.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "really um you know, influential people",
      "offset": 188.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in the space talking about eval and and",
      "offset": 190.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "why they are so important.",
      "offset": 193.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "So why would you think about eval?",
      "offset": 195.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Right? They they they help you answer",
      "offset": 197.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "questions. So here's here's a few of",
      "offset": 199.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "them. Um when I change the underlying",
      "offset": 201.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "model to my application, is it is it",
      "offset": 204.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "getting better or is it getting worse?",
      "offset": 207.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "When I change my prompt, right, when I",
      "offset": 209.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "change uh certain certain things about",
      "offset": 211.04,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "the application, is it getting better or",
      "offset": 213.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "is it getting worse? So we want to get",
      "offset": 214.799,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "away from uh not having a rigorous sort",
      "offset": 216.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "of process around building with large",
      "offset": 219.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "language models which as you all know",
      "offset": 222.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "right non-deterministic outputs uh",
      "offset": 224,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "creates somewhat of a challenge and",
      "offset": 226.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "without uh eval becomes really really",
      "offset": 227.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "hard to uh to create a good application",
      "offset": 231.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that we can put into production. Maybe",
      "offset": 233.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "some other ones like obviously being",
      "offset": 235.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "able to uh detect regressions within the",
      "offset": 237.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "within the code. I think the other the",
      "offset": 240.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "other thing that Anker mentioned uh to",
      "offset": 242.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "me when I first started which I didn't",
      "offset": 244.319,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "really mention this but uh this is my my",
      "offset": 246.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "third week at at brain trust as a",
      "offset": 248.159,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "solutions engineer but one of the things",
      "offset": 249.92,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "that he mentioned to me that I thought",
      "offset": 251.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "really resonated was uh eval are a",
      "offset": 252.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "really great way I think people think of",
      "offset": 255.76,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "them as almost like unit tests for uh",
      "offset": 257.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "for you know our applications but he",
      "offset": 259.759,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "kind of described it another way of like",
      "offset": 262.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "this is a really great way to for us to",
      "offset": 263.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "play offense uh as opposed to just",
      "offset": 265.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "playing defense where I think maybe unit",
      "offset": 267.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "tests are are kind of used for This can",
      "offset": 268.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "actually be used as a tool to to really",
      "offset": 271.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "help uh create a lot of rigor around us",
      "offset": 274.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "building and developing these",
      "offset": 276.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "applications and ensuring that we",
      "offset": 278.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "actually build uh things that we can put",
      "offset": 280.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "into production.",
      "offset": 282.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Maybe from like a business perspective,",
      "offset": 284.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "uh why would you think about running",
      "offset": 286.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "evals or using evals? Uh here's a few",
      "offset": 288.479,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "here. uh if you have um eval running",
      "offset": 291.44,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "both offline and online, you create the",
      "offset": 295.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this feedback loop or this flywheel",
      "offset": 297.759,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "effect. I think Manu mentioned in the in",
      "offset": 299.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the keynote the keynote, excuse me. Um",
      "offset": 301.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and this flywheel effect allows us to,",
      "offset": 303.68,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "you know, uh cut dev time, allows us to",
      "offset": 306.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "enhance the quality of this application",
      "offset": 309.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that we're putting out in production. If",
      "offset": 311.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you're able to connect the things that",
      "offset": 312.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "are happening in in real life with your",
      "offset": 314.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "users in production and being able to uh",
      "offset": 316.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "filter those logs down, add those SP",
      "offset": 319.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "spans to data sets, inform what you're",
      "offset": 321.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "doing in an offline way creates uh that",
      "offset": 323.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that flywheel effect that becomes really",
      "offset": 326.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "powerful from a development perspective.",
      "offset": 328,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Uh again, a little bit more on the on",
      "offset": 332.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "the customer side. Here's a few of our",
      "offset": 333.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "customers and some of the outcomes that",
      "offset": 335.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "they've that they've seen using brain",
      "offset": 336.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "trust, whether it's moving a little bit",
      "offset": 338.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "faster, pushing more AI features into",
      "offset": 340.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "production or just increasing the",
      "offset": 343.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "quality of of the applications that they",
      "offset": 345.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have. Here's a few of the the outcomes",
      "offset": 347.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that we've seen or that they have seen.",
      "offset": 349.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So, let's let's start talking a little",
      "offset": 352.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "bit about the the core concepts of of",
      "offset": 354.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "brain trust. Obviously, we're here to",
      "offset": 356.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "talk about evals. uh the the the things",
      "offset": 358.479,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "that I think um you see like these arrow",
      "offset": 362.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "these arrows going uh one way and and",
      "offset": 364.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the other this again is that that",
      "offset": 366.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "flywheel effect that that I described",
      "offset": 368.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "earlier. Um there's the the prompt",
      "offset": 370.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "engineering aspect of this uh in brain",
      "offset": 372.8,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "trust think of uh this playground that",
      "offset": 374.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "we have as an IDE for LLM outputs. Uh",
      "offset": 377.039,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the playgrounds allow for that rapid",
      "offset": 380.639,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "prototyping as we make those changes as",
      "offset": 382.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we change the underlying model. what is",
      "offset": 384.319,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the impact to that to that uh particular",
      "offset": 386.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "uh task or that application and those",
      "offset": 389.199,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "evaluability",
      "offset": 391.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "aspect right this is the the logs that",
      "offset": 396.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we're generating in production the the",
      "offset": 398.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "ability to uh have a human review those",
      "offset": 401.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "logs in a really easy uh intuitive",
      "offset": 403.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interface and then have user feedback",
      "offset": 405.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "from actual users be logged into the",
      "offset": 407.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "application as well",
      "offset": 409.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "so what is an eval you probably heard",
      "offset": 412.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "this several several times today",
      "offset": 415.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "throughout the week if you stop by the",
      "offset": 417.199,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "booth. But sort of our definition here",
      "offset": 418.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is that structured test that checks how",
      "offset": 420.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "well your AI system performs, right? It",
      "offset": 422.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "helps you measure these things that are",
      "offset": 425.039,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "important. Quality, reliability, uh",
      "offset": 427.039,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "correctness.",
      "offset": 429.199,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So what are the ingredients in an eval?",
      "offset": 432.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "Right? I've been talking a little bit",
      "offset": 434.8,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "about task, right? This is the thing uh",
      "offset": 435.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the code or the prompt that we want to",
      "offset": 437.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "uh evaluate. The the really cool thing",
      "offset": 440,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "about brain trust is that this can be as",
      "offset": 442.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "simple as a as a s excuse me a single",
      "offset": 444.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "prompt or it could be this this full",
      "offset": 446.8,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "sort of agentic workflow where we're uh",
      "offset": 448.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "calling out to tools. There's no sort of",
      "offset": 451.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "limit onto the the complexity that we",
      "offset": 453.039,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "put into this task. The only thing it",
      "offset": 455.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "requires is an input and an output. The",
      "offset": 457.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "second thing is a data set. This is our",
      "offset": 460.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "uh real world examples. This is uh",
      "offset": 462.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "essentially what we're going to run the",
      "offset": 464.8,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "task against to understand how well uh",
      "offset": 466,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "our application is performing. And how",
      "offset": 469.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we do that is via scores. So the score",
      "offset": 470.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is really the the logic behind your",
      "offset": 473.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "eval. There's there's a couple different",
      "offset": 476,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "ways to think about this. There's the",
      "offset": 478.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "LLM as a judge type score. So uh you",
      "offset": 479.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "give it the the output and some criteria",
      "offset": 482.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "and it is able to assess uh like say I",
      "offset": 485.039,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "want uh based on this output is this",
      "offset": 487.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "excellent, is this fair, is this poor?",
      "offset": 490.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And then those outputs then correspond",
      "offset": 493.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "to you know zero point five or one. Uh",
      "offset": 495.12,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "you also have codebased scores right",
      "offset": 498.319,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "these are maybe a little bit more",
      "offset": 499.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "heruristic or binary but uh we can use",
      "offset": 500.879,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "both of these to really aid in the",
      "offset": 503.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "development of that of that eval and",
      "offset": 505.599,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "ensuring that we're building a really",
      "offset": 507.44,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "good application.",
      "offset": 509.039,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "Uh I think I just sort of mentioned this",
      "offset": 513.2,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "here as well but like the the two mental",
      "offset": 514.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "models here of of eval there's there's",
      "offset": 516.719,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "offline and online. Offline is",
      "offset": 518.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pre-production. This is us actually",
      "offset": 521.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "doing that that iteration. It's uh",
      "offset": 522.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "identifying and resolving issues before",
      "offset": 525.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "deployment. Uh this is where we're",
      "offset": 528,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "defining those tasks. It's where we're",
      "offset": 529.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "defining those scores. Uh online evals,",
      "offset": 531.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this is that real-time tracing of of the",
      "offset": 534.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "application in production. It's logging",
      "offset": 536.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the model inputs and the outputs uh the",
      "offset": 539.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "intermediate steps, the tool calls,",
      "offset": 541.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "everything that's happening. It allows",
      "offset": 543.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "us to diagnose performance and",
      "offset": 544.959,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "reliability issues, latency uh based on",
      "offset": 547.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "how you instrument your your application",
      "offset": 550.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "with brain trust. We can pull back lots",
      "offset": 551.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of different metrics related to cost and",
      "offset": 554,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "tokens and duration and all of these",
      "offset": 556,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "things be help inform uh how we build",
      "offset": 558.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this application.",
      "offset": 561.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Um I'll I'll jump into a little bit more",
      "offset": 563.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of like how we can instrument our app",
      "offset": 565.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "for online evals. Uh we're going to",
      "offset": 567.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "first I think talk a little bit more of",
      "offset": 570.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "the offline",
      "offset": 572.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Before doing that, maybe just like level",
      "offset": 573.839,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "set on how to improve. I think one of",
      "offset": 576.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "the the the the things that I've seen uh",
      "offset": 580.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "in the last few days here, the",
      "offset": 582.399,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "conversations that I've had, it's like",
      "offset": 583.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "almost how do I get started or what do I",
      "offset": 585.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "do if X or you know those types of",
      "offset": 587.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "questions. Another thing I had heard",
      "offset": 590.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "from Anker uh very early on is that like",
      "offset": 592.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "just get started create that baseline",
      "offset": 594.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that you can then iterate and build",
      "offset": 597.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "from. Uh I think a lot of people get",
      "offset": 598.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "caught in like creating this this golden",
      "offset": 600.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "data set of test cases uh that they they",
      "offset": 602.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "can then like iterate from. Start you",
      "offset": 605.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "you don't necessarily have to do that.",
      "offset": 608,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Start and build that uh that baseline.",
      "offset": 609.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Establish that foundation that you can",
      "offset": 612.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "then improve upon. But this is a really",
      "offset": 614.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "good sort of matrix of like uh if I have",
      "offset": 616.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "good output but a low score, what do I",
      "offset": 618.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "do? Right? Improve your evals. If I have",
      "offset": 621.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "bad output and a high score, improve",
      "offset": 623.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "your your evals or your scoring. but",
      "offset": 625.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "really good kind of like highle uh",
      "offset": 628,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "understanding of of where to start to",
      "offset": 630.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "target your your efforts when you are",
      "offset": 632.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "building these apps and you're and",
      "offset": 634.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you're creating evals.",
      "offset": 635.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So let's jump into the actual components",
      "offset": 638.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that I just that I just talked about. So",
      "offset": 641.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "within the brain trust platform we have",
      "offset": 642.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a task. Uh again this could be a prompt.",
      "offset": 644.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "This could be like this full agentic",
      "offset": 647.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "workflow. Uh very basic you see that",
      "offset": 648.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that gif running. This is a prompt",
      "offset": 650.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "within the platform. You uh specify an",
      "offset": 652.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "underlying model that you want it to",
      "offset": 655.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "use. You give it a a system prompt. You",
      "offset": 656.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "can also uh give it access to tools. It",
      "offset": 659.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "has access to musta mustache templating.",
      "offset": 661.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "So you can pass in variables like user",
      "offset": 664.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "questions or um you know the input from",
      "offset": 666.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the user or its chat history or",
      "offset": 669.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "metadata. Right? So when we actually go",
      "offset": 670.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and want to parse through these logs,",
      "offset": 673.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the metadata becomes actually beneficial",
      "offset": 674.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh enabling us to do that in a really",
      "offset": 677.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "easy way. Um going forward uh maybe we",
      "offset": 679.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "have a multi-turn sort of chat type",
      "offset": 683.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "scenario where we want to add uh",
      "offset": 685.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "additional messages for the system and",
      "offset": 687.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the assistant and the user uh and our",
      "offset": 689.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tool calls as well. The uh the the",
      "offset": 691.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "platform allows for that just via this",
      "offset": 693.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "uh plus this messages uh button and then",
      "offset": 695.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you're able to add those different",
      "offset": 698.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "messages to the prompt.",
      "offset": 699.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Um also we can add tools. So oftentimes",
      "offset": 703.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the the prompt will will need access to",
      "offset": 706,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to something, right? Maybe it's a a rag",
      "offset": 707.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "type workflow, maybe it's doing web",
      "offset": 710.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "search, whatever it is, we can now use",
      "offset": 712.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "those tools as part of that prompt,",
      "offset": 714.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "right? And so when you you sort of",
      "offset": 716.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "encode in that prompt like make sure you",
      "offset": 718.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "use X tool, uh this prompt has access to",
      "offset": 720.24,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "that tool while it's running.",
      "offset": 723.279,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "Uh the last one, this is actually a",
      "offset": 727.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "feature that is in beta right now. uh",
      "offset": 729.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it's actually creating more of that",
      "offset": 731.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "agentic type workflow within your uh",
      "offset": 733.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "within the brain trust platform itself.",
      "offset": 735.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "So it's it's a way right now at least to",
      "offset": 737.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "chain together prompts where the the",
      "offset": 740.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "output of one now becomes the input of",
      "offset": 742.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the other. But if you think back maybe",
      "offset": 744.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to to this slide, if I have sort of um",
      "offset": 746,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this prompt that has access to tools,",
      "offset": 748.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you you create a pretty powerful system",
      "offset": 751.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "here where you're able to go from uh",
      "offset": 753.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "maybe that first step that has access to",
      "offset": 755.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "a certain tool and we get some output",
      "offset": 757.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "from that. we can then go to that next",
      "offset": 759.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "step that has access to maybe some other",
      "offset": 761.519,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "tools, right? This sort of like maybe",
      "offset": 763.519,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "multi- aent type of workflow we can",
      "offset": 764.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "create with the underlying tools within",
      "offset": 766.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "those prompts.",
      "offset": 768.639,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "The second thing that I talked about is",
      "offset": 772,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "data sets, right? These are our test",
      "offset": 774.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "cases that we want to give to the the",
      "offset": 775.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "task to run. So we can sort of iterate",
      "offset": 778.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "over that. We can get the output and",
      "offset": 780.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "then going down a little bit further",
      "offset": 782.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually score that. But this is um",
      "offset": 784.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "obviously really important when we're",
      "offset": 786.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "when we're running our eval. And then",
      "offset": 787.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "when we are uh trying to pull from",
      "offset": 789.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "production, right, the the actual logs",
      "offset": 791.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that are happening, we can add those",
      "offset": 793.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "those uh spans, those traces to the data",
      "offset": 795.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sets in a really easy way. And I can",
      "offset": 798.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "show you what that looks like. But uh if",
      "offset": 799.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you look there at the bottom, the only",
      "offset": 801.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "thing that's required is the the input.",
      "offset": 802.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Uh you also have the ability to add uh",
      "offset": 804.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "expected. So what is the expected output",
      "offset": 807.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "for that input, right? you can sort of",
      "offset": 809.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "like uh create some sort of score that",
      "offset": 811.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "looks at the output with the expected.",
      "offset": 813.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "There's a a score called Levvenstein",
      "offset": 816.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that allows you to you know measure the",
      "offset": 818.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the the difference between those two. So",
      "offset": 820,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you can do some different things based",
      "offset": 821.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "on what you provide to that data set.",
      "offset": 823.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "You also have metadata as well. Again",
      "offset": 825.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "being able to filter down different",
      "offset": 826.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "things pulling the data set maybe into",
      "offset": 828.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "your own codebase and I want to filter",
      "offset": 830.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "by again x y or z via the metadata.",
      "offset": 831.68,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "That's all possible.",
      "offset": 834.56,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "Uh I mentioned this a little bit ago,",
      "offset": 838,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "but just start small and iterate, right?",
      "offset": 839.839,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "You don't have to create this golden",
      "offset": 841.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "data set to to get started here. Um just",
      "offset": 842.959,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "just start and then uh then continue to",
      "offset": 846.24,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "iterate and build from that baseline. Uh",
      "offset": 848.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the human review portion also becomes",
      "offset": 851.199,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "really powerful again when we have stuff",
      "offset": 853.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "uh being logged within production having",
      "offset": 855.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "humans actually go through those logs",
      "offset": 857.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and you there's lots of different ways",
      "offset": 859.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to filter it down to the things that",
      "offset": 861.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "they should be looking at and then we",
      "offset": 862.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "can now decide to add those things to",
      "offset": 864.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "certain data sets that then inform uh",
      "offset": 866.8,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "the offline evals that we're running.",
      "offset": 869.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Uh the last thing, the last ingredient",
      "offset": 873.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "here that we need for our excuse me uh",
      "offset": 875.68,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "for our evals are our scores. We have",
      "offset": 878.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "both codebased scores, right? Again,",
      "offset": 881.839,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "this is like more of those binary type",
      "offset": 883.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "conditions, but you can actually uh code",
      "offset": 885.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "TypeScript or Python. You can do that",
      "offset": 887.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "within the UI as you see over there on",
      "offset": 890.079,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "the bottom left. Or you can within your",
      "offset": 891.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "own codebase, create that score and then",
      "offset": 893.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "push it into Brain Trust. So we can use",
      "offset": 895.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it in the platform. Other users who",
      "offset": 897.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "maybe aren't in the codebase can use",
      "offset": 899.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that score as well.",
      "offset": 901.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "The other score that we have access to",
      "offset": 902.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "is called LLM as a judge. So this allows",
      "offset": 904.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "us to use an LLM to sort of judge the",
      "offset": 907.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "output. We can give it the the set of",
      "offset": 909.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "criteria that indicates what a good or a",
      "offset": 911.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "fair or a bad score or whatever it is.",
      "offset": 914.8,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "You you get to decide uh what that looks",
      "offset": 916.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like. So you give it that criteria and",
      "offset": 918.959,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "it says if it's good uh I want to do a",
      "offset": 921.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "one. If it's bad I want to do a zero.",
      "offset": 924,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "But this starts to create the scores uh",
      "offset": 926.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that we can use in that offline in that",
      "offset": 928.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "online sense. The other thing to call",
      "offset": 930.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "out here is that we have um internally",
      "offset": 932.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "built a package called auto evals. So",
      "offset": 935.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this is something that you can now pull",
      "offset": 938.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "into your project. These are out of the",
      "offset": 939.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "box scores that are both LLM as a judge",
      "offset": 941.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "as well as codebased. And so it just",
      "offset": 943.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "allows you to get started very very",
      "offset": 945.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "quickly. Another thing I heard Anker",
      "offset": 947.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "mention is um maybe starting with",
      "offset": 949.199,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Levvenstein maybe not the best score in",
      "offset": 952.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "a lot of cases but again it establishes",
      "offset": 954.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "a baseline very very little development",
      "offset": 956.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "work for for uh our users but it creates",
      "offset": 958.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that thing that we can then build from",
      "offset": 961.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and now you have uh a direction a",
      "offset": 963.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "direction to go in to go build maybe",
      "offset": 965.92,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "that more custom score.",
      "offset": 967.519,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "some of the things that we've heard from",
      "offset": 971.759,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "our customers. Uh some tips that um you",
      "offset": 973.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "know important to think about a lot of",
      "offset": 975.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "our customers are using higher quality",
      "offset": 978.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "models for scoring. Uh even if the",
      "offset": 980.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "prompt uses a cheaper model just just",
      "offset": 982.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "makes a lot of sense like while we're",
      "offset": 984.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "running that application to use the",
      "offset": 986,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "cheaper model but use the the more",
      "offset": 987.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "expensive one to actually go out and",
      "offset": 989.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "score it. Um also break your scoring",
      "offset": 991.44,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "into uh very focused areas. So uh the",
      "offset": 994.32,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "example that I'll show is a an",
      "offset": 997.759,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "application that that generates a change",
      "offset": 999.759,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "log from a series of commits. So I could",
      "offset": 1001.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "create a score that says assess my",
      "offset": 1004.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "accuracy, my formatting and my",
      "offset": 1005.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "correctness. Or I could create three",
      "offset": 1008.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "different scores that assess accuracy",
      "offset": 1010.639,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "and then formatting and then",
      "offset": 1012.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "correctness. So have your scores be very",
      "offset": 1013.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "targeted to the thing uh that they're",
      "offset": 1016.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "supposed to be doing. Uh test your score",
      "offset": 1018,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "pump, excuse me, prompts in the",
      "offset": 1021.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "playground before use. And then avoid",
      "offset": 1022.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "overloading the score or prompt with",
      "offset": 1025.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "context. Uh focus it on the relevant",
      "offset": 1026.72,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "input and the output.",
      "offset": 1028.4,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "Uh a couple things here. Here's where",
      "offset": 1033.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "over on the left we have our",
      "offset": 1036.24,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "playgrounds. This is where we do that",
      "offset": 1037.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that sort of like rapid iteration where",
      "offset": 1039.439,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we can pull in those prompts. We can",
      "offset": 1041.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "pull in those agents, add our data sets,",
      "offset": 1042.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and add our scores. and we can click run",
      "offset": 1044.959,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and it'll go out and sort of churn",
      "offset": 1047.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "through that data set that we've defined",
      "offset": 1049.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and will give you a sense for how well",
      "offset": 1050.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "your task is performing against the data",
      "offset": 1052.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "set with the scores that we defined. But",
      "offset": 1055.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this is the place where uh developers uh",
      "offset": 1056.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "PMs we even have a healthcare company",
      "offset": 1059.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that has doctors coming into the",
      "offset": 1061.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "platform and interacting with the",
      "offset": 1063.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "playground and even doing human review",
      "offset": 1064.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "as well. Uh depends a little bit",
      "offset": 1066.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "obviously on on the organization. The",
      "offset": 1068.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thing on the right is our experiments.",
      "offset": 1070.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "This is our sort of like snapshot in",
      "offset": 1072.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "time of those eval. So imagine now like",
      "offset": 1074.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "as we are doing this development and",
      "offset": 1077.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "we're trying to understand uh like the",
      "offset": 1078.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "last you know the last month or so are",
      "offset": 1081.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "are we getting better right the the",
      "offset": 1082.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "changes that we are making the model",
      "offset": 1085.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "changes whatever it is are we improving",
      "offset": 1086.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "our our application and the experiments",
      "offset": 1088.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "is a really great way to to understand",
      "offset": 1091.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that.",
      "offset": 1093.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Uh really important maybe to call out as",
      "offset": 1095.039,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "well. You can see in the bottom right",
      "offset": 1096.64,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "the eval can happen from uh the",
      "offset": 1097.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "application, right? The the brain trust",
      "offset": 1099.919,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "platform as well as via the SDK.",
      "offset": 1101.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "Cool. Maybe just really quick because uh",
      "offset": 1106.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "nobody likes looking at at slides all",
      "offset": 1108.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the time. I I certainly don't. Uh maybe",
      "offset": 1110.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "if you haven't seen brain trust yet,",
      "offset": 1112.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this is maybe a good a good quick demo.",
      "offset": 1114.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Uh so again like the the uh the idea",
      "offset": 1117.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "here is I have this this application.",
      "offset": 1119.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "I'll just give you I'll show you over",
      "offset": 1121.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "here. uh you give it a GitHub repository",
      "offset": 1122.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "URL. It uh grabs the most recent commits",
      "offset": 1126,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and then creates a change log from",
      "offset": 1129.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there. And then once this completes, you",
      "offset": 1131.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can even provide some user feedback. But",
      "offset": 1134.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this is the thing that we want to uh",
      "offset": 1135.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "evaluate.",
      "offset": 1138,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So what I can do uh I'll go into my my",
      "offset": 1139.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "playground, right? This is the place",
      "offset": 1142.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "where I can start to uh run those those",
      "offset": 1143.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "uh those experiments or I can start to",
      "offset": 1146.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "iterate on that prompt that I have from",
      "offset": 1148.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "my project. I've actually loaded in two",
      "offset": 1151.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "different uh two different prompts. So",
      "offset": 1153.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "maybe I'll before going into the",
      "offset": 1155.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "playground, I've actually created these",
      "offset": 1156.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "two prompts within my codebase and I",
      "offset": 1159.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "pushed them into the brain trust",
      "offset": 1160.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "platform. I've also created a data set",
      "offset": 1162.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "in that codebase and I've also created",
      "offset": 1165.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "some scores. Right? These are the",
      "offset": 1167.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "ingredients that we need to run our",
      "offset": 1168.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "eval.",
      "offset": 1170.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So now when we have we have those we",
      "offset": 1172.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have those different components. Now",
      "offset": 1174.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we're able to start to iterate here. So",
      "offset": 1176.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I'm going to actually create a net new",
      "offset": 1178.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "playground",
      "offset": 1179.76,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "and I will load in one of these prompts.",
      "offset": 1181.36,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "So again, here's my first prompt. Uh my",
      "offset": 1187.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "first prompt has a model associated with",
      "offset": 1189.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it. What becomes really cool here is the",
      "offset": 1191.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "ability to to like to iterate on the",
      "offset": 1193.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "underlying model, right? I think a lot",
      "offset": 1195.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "of us are we have access to a lot of",
      "offset": 1197.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "underlying providers and we want to be",
      "offset": 1199.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "able to understand if I change this or",
      "offset": 1201.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "if I you know a provider adds a new",
      "offset": 1203.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "model what is the impact to my",
      "offset": 1206.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "application. So I can duplicate this",
      "offset": 1207.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "prompt and maybe change this to GPT41. I",
      "offset": 1210.559,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "can run this.",
      "offset": 1214.32,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Before I run it, I have to add all of my",
      "offset": 1219.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "components. So I'll add my my data set",
      "offset": 1220.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then I can add my different scores",
      "offset": 1223.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that I've uh that I've configured here",
      "offset": 1224.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "for my change log.",
      "offset": 1226.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "And so I can click run and then now",
      "offset": 1229.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we'll understand here what is the effect",
      "offset": 1231.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of changing the model uh for this",
      "offset": 1233.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "particular task with the scores that",
      "offset": 1235.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I've configured against this data set.",
      "offset": 1237.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So this will churn through all of these",
      "offset": 1239.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "in in parallel and we'll start to get",
      "offset": 1240.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "some results back. Uh lots of different",
      "offset": 1242.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "ways to actually start to look at this",
      "offset": 1244.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "data. I always like coming over to the",
      "offset": 1246.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "summary layout because I can understand",
      "offset": 1248,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like um you can see over here this is my",
      "offset": 1249.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "base task right here and then my",
      "offset": 1252.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "comparison task. So I can understand uh",
      "offset": 1254.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "looks like you know on average uh the",
      "offset": 1256.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "base is performing a little bit better",
      "offset": 1259.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "than my uh comparison task on my",
      "offset": 1261.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "completeness score. Uh it's fairing a",
      "offset": 1263.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "little bit worse on my accuracy score.",
      "offset": 1265.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Uh both of them are you know 0% on my",
      "offset": 1267.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "formatting. So probably have some work",
      "offset": 1270.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to do there. But you can start to see",
      "offset": 1271.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "how you can use this type of interface",
      "offset": 1274.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "to iterate very quickly. Right now uh",
      "offset": 1276.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the other thing that um maybe shouldn't",
      "offset": 1279.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "do but uh I can't resist because we just",
      "offset": 1282.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "released this today is this new loop",
      "offset": 1284.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "feature. So imagine you are uh you know",
      "offset": 1286.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "you're a user within brain trust and",
      "offset": 1289.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "before this you would sort of manually",
      "offset": 1291.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "iterate here creating net new prompts uh",
      "offset": 1293.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "making modifications changing the model.",
      "offset": 1296.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "What if you could now uh utilize AI to",
      "offset": 1298.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "go and do that for you? So in a sort of",
      "offset": 1301.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like cursor like uh interface, we can",
      "offset": 1303.28,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "ask it to optimize a prompt. And I think",
      "offset": 1306.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the really unique thing here is it has",
      "offset": 1308.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "access to those uh those evaluation",
      "offset": 1310.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "results. And so when it goes to go uh",
      "offset": 1312.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "change that prompt, it understands that",
      "offset": 1315.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it changes the prompt, it runs the",
      "offset": 1317.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "evaluation. It understands if it got",
      "offset": 1319.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "better relative to the scores that we",
      "offset": 1321.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "defined. So you can see it's going to go",
      "offset": 1323.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "through here. It'll fetch some eval",
      "offset": 1325.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "results. Uh you'll probably see a diff",
      "offset": 1326.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "here very very soon. if we don't I won't",
      "offset": 1329.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I won't hang out here too long, but I do",
      "offset": 1331.84,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "want to highlight one of the things that",
      "offset": 1333.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "we are releasing that really enables our",
      "offset": 1334.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "users to iterate in a really uh really",
      "offset": 1336.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "fast way. So, here's my my change. We",
      "offset": 1338.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "can click accept and then it'll actually",
      "offset": 1342.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "go out and run that eval again or it",
      "offset": 1345.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "would uh I think I have an issue with my",
      "offset": 1347.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "anthropic API keys. But the idea here",
      "offset": 1349.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "again is like we can create that very",
      "offset": 1352.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "rapid uh iterative feedback loop here",
      "offset": 1354.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "within the playground.",
      "offset": 1356.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "The other thing here is uh we can run",
      "offset": 1359.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "these as experiments. So this is um this",
      "offset": 1360.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "is where we can start to create those",
      "offset": 1364.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "snapshots in time of that eval and again",
      "offset": 1365.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "see as I make these changes to that that",
      "offset": 1368.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "application how is it sort of performed",
      "offset": 1371.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "over time. I want to make sure I don't I",
      "offset": 1373.84,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "don't want to go down. I don't want to",
      "offset": 1375.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like uh decrease the performance of my",
      "offset": 1376.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "scores relative to obviously the last",
      "offset": 1378.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "time it ran, but uh looking out over the",
      "offset": 1380.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "last month, six months, whatever it is",
      "offset": 1382.48,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "that we're tracking.",
      "offset": 1384,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Cool.",
      "offset": 1389.679,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So that was very very uh brief sort of",
      "offset": 1394.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "intro to like eval via the UI, right?",
      "offset": 1396.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Again, like just to summarize, we need a",
      "offset": 1399.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "task, we need a data set, and we need at",
      "offset": 1401.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "least one score. We can pull those into",
      "offset": 1403.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the playground and now we can start to",
      "offset": 1405.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "iterate. We can save these via",
      "offset": 1407.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "experiments. Uh and now we can we have a",
      "offset": 1409.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "way in which we can understand how well",
      "offset": 1411.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this application is performing. Right?",
      "offset": 1413.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "This is no longer like qualitative,",
      "offset": 1416.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "right? This isn't like hey I think this",
      "offset": 1417.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "got better that output looks better.",
      "offset": 1419.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "There is actual rigor behind this. Now",
      "offset": 1421.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "um customers oftentimes ask though like",
      "offset": 1424.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I don't really want to use or I'm not",
      "offset": 1426.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "going to use the the platform as much.",
      "offset": 1428.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I'd rather use this from my my codebase.",
      "offset": 1430.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Is that possible? uh and it is uh so uh",
      "offset": 1432.799,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "we have a Python SDK, we have a",
      "offset": 1436.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "TypeScript SDK there there's some other",
      "offset": 1438.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "ones as well. Go, Java, Cotlin. Uh for",
      "offset": 1440,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the most part, most of our users are",
      "offset": 1442.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "using uh Python or TypeScript. Here's a",
      "offset": 1443.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "just couple examples of what this might",
      "offset": 1446.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "look like from an SDK perspective. Um,",
      "offset": 1448.24,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "actually, if you all aren't opposed to",
      "offset": 1452,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "looking at uh some code, here's just a a",
      "offset": 1454.159,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "really basic example of uh defining a",
      "offset": 1457.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "prompt within my my codebase and then",
      "offset": 1459.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "pushing it into brain trust. So, just",
      "offset": 1462.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "leveraging that that Python SDK. Uh,",
      "offset": 1464.799,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "another example should come over here",
      "offset": 1467.039,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "creating a score. So, you give it sort",
      "offset": 1471.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of like the the things that it's looking",
      "offset": 1473.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "for, but now I've sort of defined this",
      "offset": 1475.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "score within my codebase. Uh, it's",
      "offset": 1477.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "version controlled. Uh also the the",
      "offset": 1479.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "prompts that you create within the UI",
      "offset": 1481.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "are version controlled as well. Um but",
      "offset": 1482.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this is just another way to start to",
      "offset": 1484.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "interact with with brain trust. So again",
      "offset": 1486,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "scores we could do data sets uh and then",
      "offset": 1488.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you can even do uh prompts up here as",
      "offset": 1490.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "well I believe. So here's my eval data",
      "offset": 1492.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "set. Here's my change log to prompt.",
      "offset": 1494.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Again being able to start from the",
      "offset": 1497.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "codebase and actually push them into the",
      "offset": 1498.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "platform is possible. Just depends on",
      "offset": 1500.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the organization where they want to",
      "offset": 1502.559,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "start.",
      "offset": 1503.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "The the other thing here is like this is",
      "offset": 1505.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "more on the like the components of the",
      "offset": 1507.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "eval side. So that's that top portion.",
      "offset": 1508.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Define those assets in code. Run that",
      "offset": 1510.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "brain trust push and now you have access",
      "offset": 1512.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to that in that brain trust library. The",
      "offset": 1514.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "other one is actually like defining the",
      "offset": 1517.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "evals in code, right? So what that looks",
      "offset": 1518.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like is is slightly different. Um come",
      "offset": 1520.32,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "over here. So we have our eval",
      "offset": 1523.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "class that's coming from our brain trust",
      "offset": 1526.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "SDK. Again, it's looking for the exact",
      "offset": 1528.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "same things that I just described,",
      "offset": 1530.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? A data set that we can use from",
      "offset": 1531.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Brain Trust itself. Uh the task that we",
      "offset": 1533.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "want to invoke and then the scores. So",
      "offset": 1536.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "again defining this here within uh",
      "offset": 1539.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "within your codebase certainly possible",
      "offset": 1541.279,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then I can run you know a command",
      "offset": 1543.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that actually runs that eval within",
      "offset": 1547.039,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "brain trust. So from here go into brain",
      "offset": 1548.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "trust see the eval running this is now",
      "offset": 1552.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "experiment that I can view over time. So",
      "offset": 1555.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "again like you saw two different types",
      "offset": 1557.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of workflows here uh again catering to",
      "offset": 1559.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "maybe two different personas or again",
      "offset": 1561.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the way in which organizations want to",
      "offset": 1563.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "work it's up to them. Brain Trust is",
      "offset": 1565.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "very flexible in how we allow our users",
      "offset": 1567.279,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "to uh to consume or use the platform.",
      "offset": 1569.679,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "Uh probably jumped ahead a little bit,",
      "offset": 1582.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "but this is sort of a recap of what I",
      "offset": 1584.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "just showed you. Again, from your code,",
      "offset": 1587.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you you create your prompts, your",
      "offset": 1588.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "scores, your data sets, you can push",
      "offset": 1590.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "them in there. Uh maybe just importantly",
      "offset": 1592.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "important to highlight here of like why",
      "offset": 1595.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you would do this. Uh you want to source",
      "offset": 1596.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "control your your prompts. Uh the big",
      "offset": 1598.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "one here to call out is the online",
      "offset": 1600.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "scoring. I have a section in a little",
      "offset": 1602.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "bit diving a little bit deeper into",
      "offset": 1604.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "that. Uh but if you want to use those",
      "offset": 1606.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "scores that we define in the data set,",
      "offset": 1608.559,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we should push them into brain trust so",
      "offset": 1610.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that we can create online scores. We can",
      "offset": 1611.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "understand how our application is",
      "offset": 1614,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "performing in production relative to",
      "offset": 1615.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "those scores that we want or that we're",
      "offset": 1617.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "using within our offline evals.",
      "offset": 1618.799,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "what I just showed you maybe another uh",
      "offset": 1622.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "another variation of that that eval",
      "offset": 1624.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "within our code again defining that data",
      "offset": 1626,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "set defining that task defining those",
      "offset": 1628,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "scores becomes very very easy to now",
      "offset": 1629.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "connect these two things it's just again",
      "offset": 1632.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "up to you to decide where you are where",
      "offset": 1634.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you want to do this the other thing to",
      "offset": 1636.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "call out here is that this can be run",
      "offset": 1637.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "via CI/CD we do have some customers that",
      "offset": 1639.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "that want to run their eval as part of",
      "offset": 1642,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the CI process so understanding in a",
      "offset": 1643.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "more automated way right the the score",
      "offset": 1646.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for A and C whatever they've configured",
      "offset": 1648.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "has it gotten better has it gotten",
      "offset": 1650.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "worse, this becomes maybe a check as",
      "offset": 1652.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "part of CI. There is uh if you look",
      "offset": 1653.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "within our documentation, there's a a",
      "offset": 1655.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "GitHub action example that shows you how",
      "offset": 1657.36,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "you could set this up.",
      "offset": 1659.52,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Cool. Let's let's move to production.",
      "offset": 1663.6,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Moving to production entails setting up",
      "offset": 1668.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "logging, right? It entails instrumenting",
      "offset": 1671.12,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "our application with uh with brain trust",
      "offset": 1673.039,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "um code. uh being able to like say I",
      "offset": 1677.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "want to wrap this this uh LLM client. I",
      "offset": 1679.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "want to wrap this particular function",
      "offset": 1682.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "when it goes to call that tool becomes",
      "offset": 1684.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "very very easy to do that. But so so why",
      "offset": 1686.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "should you do it? Um I think I've",
      "offset": 1688.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "probably said it said it numerous times",
      "offset": 1690,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "here. But we want to measure quality on",
      "offset": 1691.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "live traffic, right? We actually want to",
      "offset": 1693.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "understand how well our application is",
      "offset": 1695.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "performing with those scores. really",
      "offset": 1697.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "great to use during offline eval becomes",
      "offset": 1699.279,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "uh our aid in ensuring that we that we",
      "offset": 1702.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "build really good applications that",
      "offset": 1704.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "we're not creating regressions but also",
      "offset": 1706,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "really important to monitor that live",
      "offset": 1708.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "traffic. The other really important",
      "offset": 1709.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thing to call out I think is that that",
      "offset": 1712.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "flywheel effect that it creates. So we",
      "offset": 1713.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "have these data sets that we use to",
      "offset": 1716.799,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "inform our offline evals. It's very very",
      "offset": 1718.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "easy now to take the the logs that are",
      "offset": 1721.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "generated within production and add",
      "offset": 1723.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "those back to data sets. This also um",
      "offset": 1725.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "speaks to some of that human review",
      "offset": 1728.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "component where we want to now bring",
      "offset": 1730.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "those humans in. They can start to",
      "offset": 1732.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "review some of the logs that are",
      "offset": 1734.559,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "relevant like maybe there's user",
      "offset": 1735.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "feedback equals zero, maybe there's a",
      "offset": 1737.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "comment or whatever it is, but like they",
      "offset": 1739.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "can filter down to those particular",
      "offset": 1740.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "things and as they find really",
      "offset": 1742.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "interesting maybe test cases, it's very",
      "offset": 1744.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "very easy to add those uh back to the",
      "offset": 1746.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "data set that we use in our offline",
      "offset": 1748.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "evals. So I think the the feedback loop",
      "offset": 1750.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "or the flywheel effect that that this",
      "offset": 1752.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "creates is one of the really fundamental",
      "offset": 1754.559,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "value props of of the platform.",
      "offset": 1756.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "So how do we do this? Uh there there's a",
      "offset": 1760.159,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "couple different ways, right? We're",
      "offset": 1762.24,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "first going to initialize a logger. This",
      "offset": 1763.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "is just going to authenticate us into",
      "offset": 1764.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Brain Trust and point us to a project.",
      "offset": 1766.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Uh you may have seen when I open up the",
      "offset": 1769.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "platform, I had numerous projects inside",
      "offset": 1770.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "of there. You can almost think of a a",
      "offset": 1772.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "project as a as a container for that",
      "offset": 1774.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "feature, right? So you probably have",
      "offset": 1776.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "multiple AI features that you're",
      "offset": 1778.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "building. I want to have a container for",
      "offset": 1780.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "feature A for those prompts, those",
      "offset": 1782.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "scores, those data sets. You could",
      "offset": 1784.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "certainly utilize those things across",
      "offset": 1786.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "projects, but it becomes a really good",
      "offset": 1787.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "sort of way to uh containerize the",
      "offset": 1789.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "things that are important for that",
      "offset": 1791.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "feature. Then you can start really",
      "offset": 1792.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "basic, right? You can wrap a an LLM",
      "offset": 1794.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "client. Uh so when you saw those some of",
      "offset": 1797.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "those metrics with like tokens and l and",
      "offset": 1800.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "duration and costs uh just very",
      "offset": 1802.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "basically within the the uh the script",
      "offset": 1805.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "or excuse me the the code here I just",
      "offset": 1807.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "wrapped that open AAI client and now I'm",
      "offset": 1809.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just sort of ingesting all of that those",
      "offset": 1811.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "metrics into my logs. That's the easiest",
      "offset": 1813.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "way to get started. You obviously",
      "offset": 1816.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "probably want to uh do a little bit",
      "offset": 1818.399,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "more. Again maybe you want to uh",
      "offset": 1820.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "understand when you know that LLM",
      "offset": 1822.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "invokes a tool. So, I want to trace uh I",
      "offset": 1824.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "can add a trace decorator on top of a",
      "offset": 1826.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "function. Uh I can even use some of the",
      "offset": 1829.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "like the brain trust low-level like span",
      "offset": 1831.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "elements to create custom logs. And I",
      "offset": 1834.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "want to customize the input and I want",
      "offset": 1836.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to customize the output and the metadata",
      "offset": 1838.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that we that we log to that span. So",
      "offset": 1839.919,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "again, you can you can start very basic",
      "offset": 1842.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "with wrapping in a client and then go",
      "offset": 1844.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "down to like the individual span itself",
      "offset": 1846.24,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "specifying that input and that output.",
      "offset": 1848,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "This leads us to online scoring, right?",
      "offset": 1854.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "This is I talked a little bit about",
      "offset": 1856.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this, right? This is where like when our",
      "offset": 1857.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "logs are coming in, we can actually",
      "offset": 1859.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "configure within the platform those",
      "offset": 1861.279,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "scores that we want to run and we can",
      "offset": 1863.279,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "specify sort of a sampling rate. So, we",
      "offset": 1864.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "don't necessarily run that score across",
      "offset": 1866.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "every single log that comes in. Maybe",
      "offset": 1868.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "it's 10%, 20%, so on. Um, but it but it",
      "offset": 1870.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "creates that really tight feedback loop",
      "offset": 1874,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that that I've been talking about. Uh al",
      "offset": 1875.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "also maybe just important to mention the",
      "offset": 1878.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "early regression alerts. So we can",
      "offset": 1880.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "create automations within the brain",
      "offset": 1882.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "trust platform. If my score drops below",
      "offset": 1884,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "a certain threshold, let's create an",
      "offset": 1886.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "alert uh with it with our automation",
      "offset": 1888.399,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "feature.",
      "offset": 1890.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "This is just uh and I can maybe walk",
      "offset": 1893.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "through what this looks like instead of",
      "offset": 1895.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "showing you here. Uh the custom views.",
      "offset": 1897.039,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "This is where like there's a lot of",
      "offset": 1901.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "really rich information within these",
      "offset": 1903.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "logs and it becomes really important I",
      "offset": 1905.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think again for the human review",
      "offset": 1907.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "component to like filter these down to",
      "offset": 1909.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the things that they care about or the",
      "offset": 1910.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "things that anybody cares about. So we",
      "offset": 1912.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "can create custom views within brain",
      "offset": 1914.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "trust with the appropriate filters uh",
      "offset": 1916.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and then it's very easy for that human",
      "offset": 1918.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to go into what we call human review",
      "offset": 1919.919,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "mode within brain trust and sort of",
      "offset": 1921.679,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "parse through those logs uh the ones",
      "offset": 1923.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that are the the ones that are going to",
      "offset": 1925.519,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "be most meaningful to them.",
      "offset": 1927.12,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "Let me uh let me connect some of those",
      "offset": 1933.519,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "those dots there. So",
      "offset": 1935.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "again, showing you some code, maybe",
      "offset": 1939.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "good, maybe bad, but um I'm guessing",
      "offset": 1940.88,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "there's some technical people in the",
      "offset": 1942.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "room that that don't mind here.",
      "offset": 1943.6,
      "duration": 9.319
    },
    {
      "lang": "en",
      "text": "So if I look for um the",
      "offset": 1947.36,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "uh you may have s seen in one of those",
      "offset": 1953.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "slides, there is a the V vers Versel AI",
      "offset": 1954.64,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "SDK. I want to wrap this AI SDK model.",
      "offset": 1957.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Again, this allows us to just create all",
      "offset": 1960.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of those metrics within brain trust with",
      "offset": 1963.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "just zero lift from us as a developer.",
      "offset": 1964.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "This becomes really easy to do. Uh you",
      "offset": 1967.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "can also see where I have specified that",
      "offset": 1969.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "span itself, right? I actually want to",
      "offset": 1971.519,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "define the inputs and the outputs of",
      "offset": 1973.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that. The reason you would do that is",
      "offset": 1975.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because you have a specific data set",
      "offset": 1978.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with a structure that you want to ensure",
      "offset": 1980.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "maps to that. So like when you are",
      "offset": 1982.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "within those logs parsing through them",
      "offset": 1983.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "becomes really easy to add those spans",
      "offset": 1986,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "back to that data set. So ensuring that",
      "offset": 1987.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that data structure is sort of",
      "offset": 1990,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "consistent across offline and online",
      "offset": 1991.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "becomes really important again to create",
      "offset": 1993.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that feedback loop. So this is you know",
      "offset": 1995.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "very high level of like how we can start",
      "offset": 1997.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to create those spans.",
      "offset": 1999.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Um now that we do right we can now go in",
      "offset": 2001.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "uh the platform and start to configure",
      "offset": 2004.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "our online scoring. So this is here just",
      "offset": 2005.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "within this configuration pane I can",
      "offset": 2008,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "click online scoring.",
      "offset": 2010.159,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "I'll just delete I'll create a new rule.",
      "offset": 2012.64,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "Uh so my new rule and here's where we",
      "offset": 2016.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "can add different scores, right?",
      "offset": 2020.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Obviously I have a few here that I've",
      "offset": 2022,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "been using for the offline evals. I",
      "offset": 2023.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "don't necessarily need to select all of",
      "offset": 2026.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "them, but I certainly can.",
      "offset": 2027.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "And then I want to uh apply a sampling",
      "offset": 2030.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "rate. So I want to actually give you an",
      "offset": 2033.039,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "example of what this looks like. So I'm",
      "offset": 2035.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "going to do 100%. The other thing to",
      "offset": 2036.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "call out here is that you can apply",
      "offset": 2038.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these to the individual spans themselves",
      "offset": 2040,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and not the entire root span. So where",
      "offset": 2042.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this becomes beneficial is like when you",
      "offset": 2044.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "are invoking maybe tool calls, you're",
      "offset": 2046.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "invoking like a a rag workflow and you",
      "offset": 2048.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "actually want to create a score on",
      "offset": 2052,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "whether or not the thing that it gave",
      "offset": 2053.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "back uh is actually relevant to the",
      "offset": 2055.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "user's query. So we can actually create",
      "offset": 2058,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a score specifically for that and",
      "offset": 2059.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "highlight what that span is here. So",
      "offset": 2061.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "again very very uh flexible in how you",
      "offset": 2064.079,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "apply these scores to the things that",
      "offset": 2066.399,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "are happening online.",
      "offset": 2068.159,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "Now when I come back here to the",
      "offset": 2072.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "application",
      "offset": 2074.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and we'll just run this again uh",
      "offset": 2075.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "creating that change log",
      "offset": 2078.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you'll now start to see here within uh",
      "offset": 2081.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the logs",
      "offset": 2083.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "this will start to show up and then",
      "offset": 2085.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you'll start to see these scores be",
      "offset": 2087.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "generated. Right? Again, this is where",
      "offset": 2088.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "like you can now start to understand",
      "offset": 2090.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "over time in production. How are these",
      "offset": 2092.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "things doing? How are they fairing?",
      "offset": 2094.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Where can we get better? Uh again, now",
      "offset": 2096,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "we can connect again like the things",
      "offset": 2098.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that are happening in our offline evals",
      "offset": 2099.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "with the things that are happening with",
      "offset": 2101.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "online. The other thing to call out here",
      "offset": 2102.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is the uh the feedback mechanism, right?",
      "offset": 2105.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Uh we certainly have uh the ability to",
      "offset": 2107.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "do like human review, but oftentimes you",
      "offset": 2110.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "want your users to provide feedback as",
      "offset": 2112.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "well. And so this is just a basic",
      "offset": 2114.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "example of a thumbs up, thumbs down. And",
      "offset": 2116.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "you can even provide a comment here.",
      "offset": 2118.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "This can now be logged to Brain Trust.",
      "offset": 2123.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "So I should see over here my user",
      "offset": 2125.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "feedback. So here's my my comment and",
      "offset": 2128.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "then I have my user feedback score. But",
      "offset": 2131.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "now I can also do something like this.",
      "offset": 2134.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "So again, maybe I want to filter my logs",
      "offset": 2136.079,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "down to where user feedback is zero. So",
      "offset": 2138.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "click that button. I'm going to change",
      "offset": 2142.48,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "this to zero.",
      "offset": 2144,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "Right? I don't have any rows yet like",
      "offset": 2150.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that. But now I can save this as a view",
      "offset": 2152.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and people who are now using this as",
      "offset": 2155.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "human review can filter this down to",
      "offset": 2156.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "where user feedback equals zero and we",
      "offset": 2159.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "can figure out what's going on, right?",
      "offset": 2161.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "What are the things that that fell down",
      "offset": 2162.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "here within this application that we",
      "offset": 2164.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "need to go fix. The other thing I'll",
      "offset": 2165.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "highlight here is our sort of human",
      "offset": 2168,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "review component. Uh actually",
      "offset": 2169.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "um you click that that button or you can",
      "offset": 2173.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "hit just R and it opens up this this",
      "offset": 2174.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "different pane of your log. So it's a",
      "offset": 2177.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "paired down version of what you just saw",
      "offset": 2180,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "there. It's a little bit easier for a",
      "offset": 2182.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "human to go through and actually uh look",
      "offset": 2183.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "at that that that input and that output.",
      "offset": 2185.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "But you as a as a user of Brain Trust",
      "offset": 2188.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "can configure the human review scores",
      "offset": 2190.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "that you would like to uh to use. So I",
      "offset": 2192.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "have this add something here. So maybe",
      "offset": 2195.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this is a little bit more free text. I",
      "offset": 2197.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have a better score. Again, these are",
      "offset": 2199.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the things that that you can add to your",
      "offset": 2201.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "platform that map to the the the review,",
      "offset": 2203.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "excuse me, the scores that you want your",
      "offset": 2206.32,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "your humans to to add to those logs.",
      "offset": 2208.48,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "Um, just really quick, I I'll highlight",
      "offset": 2215.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "some of these things here. Um, this is",
      "offset": 2217.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "what it starts to look like when you",
      "offset": 2220.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "instrument your uh your application with",
      "offset": 2222.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "those different um wrappers or those",
      "offset": 2224.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "different trace functions. uh I'm able",
      "offset": 2226.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "to understand at a very granular level,",
      "offset": 2228.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "excuse me, granular level the things",
      "offset": 2231.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "that it's doing, right? So I essentially",
      "offset": 2233.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "have these tool calls where it's going",
      "offset": 2234.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "out and it's grabbing the commits from",
      "offset": 2236,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "GitHub. It's understanding what the",
      "offset": 2237.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "latest release is and it's fetching the",
      "offset": 2239.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "commits from that latest release and now",
      "offset": 2240.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "I can generate that change log. But",
      "offset": 2242.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "again, the the really unique thing here",
      "offset": 2245.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and maybe a different example of this",
      "offset": 2246.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "is I can start to score those individual",
      "offset": 2250.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "things that are happening. So this is a",
      "offset": 2252.16,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "different uh application with a",
      "offset": 2254.96,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "with this example. So if I open this up,",
      "offset": 2259.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I have these uh this conversational",
      "offset": 2262.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "analytics application. So a user can ask",
      "offset": 2264.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "a question, it can return back some",
      "offset": 2266.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "data. But this application goes through",
      "offset": 2268.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "these various steps, right? The first",
      "offset": 2270.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "step is to rephrase the question that",
      "offset": 2272.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the user asked. So imagine like there's",
      "offset": 2273.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this chat history that we can load in as",
      "offset": 2276.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "input and the LLM needs to rephrase that",
      "offset": 2278.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "user question. If the LLM does a really",
      "offset": 2280.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "bad job of rephrasing this question,",
      "offset": 2282.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "everything as a result of this will fall",
      "offset": 2284.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "down. probably not going to get a right",
      "offset": 2286.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "a right answer. So what I can do is",
      "offset": 2288,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "create a score specifically for that",
      "offset": 2290.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "span to understand how well the LLM did",
      "offset": 2292.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "in rephrasing that question can also",
      "offset": 2295.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "understand the intent that I was able to",
      "offset": 2297.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "derive or the LLM was able to derive",
      "offset": 2300.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "from that question. Is that right? But",
      "offset": 2301.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you start to think of like these these",
      "offset": 2304.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "more complex type of applications that",
      "offset": 2305.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "you build. You need to be able to",
      "offset": 2307.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "understand the individual steps that are",
      "offset": 2309.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "happening and brain trust allows for",
      "offset": 2310.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that very very easily via these scores",
      "offset": 2312.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and then being able to apply them not",
      "offset": 2314.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "only again while you're in offline eval",
      "offset": 2316.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "kind of mode but also online right we",
      "offset": 2318.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "want to uh understand these logs and be",
      "offset": 2320.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "able to apply these scores at the",
      "offset": 2322.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "individual span level this becomes",
      "offset": 2323.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "pretty powerful as well.",
      "offset": 2324.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "Um, I think I actually stole from my my",
      "offset": 2331.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "next section,",
      "offset": 2333.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "my human in the loop. Kind of walked",
      "offset": 2335.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "through this uh a little bit. Um, maybe",
      "offset": 2336.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just another call out if you happen to",
      "offset": 2339.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "be at one of our workshops on Tuesday,",
      "offset": 2341.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "um, Sarah from Notion, uh, who's a a",
      "offset": 2344.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Brain Trust customer talked a little bit",
      "offset": 2346.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "about how they think about human in the",
      "offset": 2348.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "loop. And I think it's it's important to",
      "offset": 2350.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "consider like the the size of her",
      "offset": 2352.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "organization and what they're doing. Um,",
      "offset": 2353.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "she mentioned that like she has a",
      "offset": 2357.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "special type of role that they use for",
      "offset": 2359.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "human in the loop type of interaction,",
      "offset": 2361.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "right? There is it's almost like a",
      "offset": 2363.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "product manager mixed with an LLM",
      "offset": 2365.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "specialist. Uh, they're the people that",
      "offset": 2367.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "are going through and doing those human",
      "offset": 2369.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "reviews. Smaller organizations, she made",
      "offset": 2370.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a comment that was it actually makes a",
      "offset": 2374.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "lot of sense for the engineers, some",
      "offset": 2375.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "engineers to actually go through and do",
      "offset": 2377.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this as well. Becomes really powerful to",
      "offset": 2379.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "pair like the automation with the human",
      "offset": 2381.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "component of this. like this is not",
      "offset": 2384.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "going to go away. I think it it adds",
      "offset": 2385.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "value to the process.",
      "offset": 2387.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 2390.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "again, I think I just stole from myself",
      "offset": 2392.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "like why why this matters, right? This",
      "offset": 2393.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "is really critical for the the quality",
      "offset": 2394.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and the reliability of your application.",
      "offset": 2396.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Uh it provides that that ground truth",
      "offset": 2398.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "for for what you're for what you're",
      "offset": 2400.96,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "doing.",
      "offset": 2402.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Two types of human in the loop uh",
      "offset": 2405.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "interactions here. I walked you through",
      "offset": 2407.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that human review. Um give me one second",
      "offset": 2408.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and I'll call you. Yeah. uh the the two",
      "offset": 2411.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "excuse me the two types the human review",
      "offset": 2414.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "uh being able to like create that that",
      "offset": 2416.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "uh interface within brain trust that",
      "offset": 2418.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "allows that user to kind of parse",
      "offset": 2420.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "through the logs in a really easy manner",
      "offset": 2421.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "as well as configuring scores that allow",
      "offset": 2423.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "them to add the relevant scores to that",
      "offset": 2425.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "particular log and then the user",
      "offset": 2428.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "feedback. This is actually coming from",
      "offset": 2430.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "our users in the application. Again",
      "offset": 2431.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "being able to create sort of uh views on",
      "offset": 2433.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "top of that feedback that then power uh",
      "offset": 2435.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "maybe the human review and then creates",
      "offset": 2437.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that flywheel effect uh that we that we",
      "offset": 2439.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that we want.",
      "offset": 2441.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "That's all I have today. Appreciate you",
      "offset": 2443.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "all coming out here and and listening to",
      "offset": 2445.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "me. But yeah, you had a question.",
      "offset": 2447.52,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "Thanks.",
      "offset": 2450.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah, the question is around like how",
      "offset": 2476.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "are we using uh human review and like",
      "offset": 2478.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "some of the logs and informing the the",
      "offset": 2480.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "offline eval portion of this largely",
      "offset": 2483.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "that cool uh yeah one thing I maybe I",
      "offset": 2486,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "didn't highlight here is so maybe back",
      "offset": 2488.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "within brain trust",
      "offset": 2490.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I'm going to go back to my initial",
      "offset": 2493.119,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "project.",
      "offset": 2494.72,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So imagine now like we have we have all",
      "offset": 2499.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "of these logs. We filtered it down to a",
      "offset": 2501.599,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "particular Oh,",
      "offset": 2503.28,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "are we still showing",
      "offset": 2508.56,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "on the screen?",
      "offset": 2514.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Awesome. Thank you. Um, yeah. So,",
      "offset": 2520.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "imagine like we have this uh this this",
      "offset": 2522.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "process now, right, where we're we're",
      "offset": 2524.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "doing that human review. We filtered it",
      "offset": 2526.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "down to the records that are meaningful",
      "offset": 2527.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "for whatever reason. It becomes really",
      "offset": 2529.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "easy again to connect what's happening",
      "offset": 2531.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "within production. So, I maybe I select",
      "offset": 2533.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "all of these rows or I select individual",
      "offset": 2535.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "rows, but I can add these back to the",
      "offset": 2537.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "data set that we're using within those",
      "offset": 2539.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "offline evals. I I think I've said this",
      "offset": 2541.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like a hundred times over this",
      "offset": 2543.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "conference, this flywheel effect. This",
      "offset": 2545.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is like I think what's missing",
      "offset": 2547.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "oftentimes when we're building these",
      "offset": 2549.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "these AI applications and what brain",
      "offset": 2550.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "trust allows for really seamlessly.",
      "offset": 2552.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Yeah.\n Uh I have two questions. Uh the",
      "offset": 2555.04,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "first one is about production. Uh is it",
      "offset": 2558.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "possible to have multiple models in",
      "offset": 2562.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "production and compare how they behave?",
      "offset": 2564.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yeah, I I don't see why not. Like my",
      "offset": 2567.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "guess is in the underlying application",
      "offset": 2569.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you're swapping them out",
      "offset": 2571.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like having like AB tests, you know, I",
      "offset": 2573.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "can have like a two or three or four and",
      "offset": 2576,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "easily compare.\n Absolutely. Yeah. Um",
      "offset": 2578.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "let's see if I have an example here.",
      "offset": 2580.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "You're able to to group some of these",
      "offset": 2584.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "scores. Uh maybe this is sort of an",
      "offset": 2586.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "example of of what you're talking about.",
      "offset": 2588.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "So like maybe within production we have",
      "offset": 2589.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "different models running. uh this this",
      "offset": 2591.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sort of view here allows us to",
      "offset": 2593.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "understand like the the models that",
      "offset": 2595.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "we're using under the hood. Uh and this",
      "offset": 2598.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "is just you know you could do this",
      "offset": 2600.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "within production as well and sort of do",
      "offset": 2601.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that AB testing.\n Cool. Uh my my second",
      "offset": 2603.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "question is about humans in the loop,",
      "offset": 2606.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "right? Um",
      "offset": 2608.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "let's suppose that I have multiple",
      "offset": 2610.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "humans and um they behave uh slightly",
      "offset": 2612.56,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "different as a scorer scorers. Do you",
      "offset": 2616.72,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "have anything or what what is the vision",
      "offset": 2620.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to do with that? Like is there a way",
      "offset": 2623.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that I can actually compare how they're",
      "offset": 2626.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "scoring or something like that or not",
      "offset": 2628.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "really?",
      "offset": 2629.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "So different users can maybe have",
      "offset": 2631.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "different sort of criteria for scoring.",
      "offset": 2633.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Maybe the first thing I would say to",
      "offset": 2635.119,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "that is like there there should be like",
      "offset": 2636.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "maybe a rubric for your users who are",
      "offset": 2637.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "interacting with human review. So you're",
      "offset": 2640,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "not creating that. Uh you certainly have",
      "offset": 2642.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the ability to see like who is scoring",
      "offset": 2644.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "different things within the platform.",
      "offset": 2646.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Um, I'm not sure if you're able to pull",
      "offset": 2649.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that as like a data set to like assess",
      "offset": 2650.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the differences there, but maybe like",
      "offset": 2652.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "before it gets to that place, like have",
      "offset": 2654.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "a rubric, have a guideline of of what",
      "offset": 2656.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "scoring looks like for your humans.",
      "offset": 2659.68,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "Okay. Thank you.\n Yeah, of course.",
      "offset": 2662.079,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "Hi. Um, so the scorers I'm used to",
      "offset": 2666.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "working with for like LLM as a judge are",
      "offset": 2669.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "like they're relativistic, right? So",
      "offset": 2672.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "they can't tell you is the answer",
      "offset": 2675.68,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "relevancy good or bad for a single run",
      "offset": 2678.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "but it can tell you how it compares to",
      "offset": 2681.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "previous iteration of like the same test",
      "offset": 2684.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "set for example.",
      "offset": 2687.76,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "Um, do you guys use LM as a judge scores",
      "offset": 2690.079,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "for online",
      "offset": 2694.079,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "or is and like",
      "offset": 2696.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "how are are they relativistic like that",
      "offset": 2699.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "or do you have some way to be like this",
      "offset": 2702.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is a good answer\n you know in and of",
      "offset": 2704.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "itself for this sample or because it's",
      "offset": 2707.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "all you have new data coming in right",
      "offset": 2710,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "yeah I think a lot of our customers who",
      "offset": 2712.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "are are thinking about this are like",
      "offset": 2713.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "almost doing eval",
      "offset": 2715.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like trying to understand did the LLM as",
      "offset": 2717.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a judge actually do a good job there. So",
      "offset": 2720.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like when that actually runs there's a",
      "offset": 2722.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "rationale behind it and so you can sort",
      "offset": 2724,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "of run an eval of those LLMs as a judge.",
      "offset": 2726.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I think Sarah from notion in our",
      "offset": 2730,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "workshop described sort of a process",
      "offset": 2731.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "like that within notion but I think",
      "offset": 2733.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "that's that's sort of like where I would",
      "offset": 2735.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "aim you.\n Okay.\n Cool. Thanks.\n Cool.",
      "offset": 2736.56,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "Awesome.",
      "offset": 2741.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Are any of your customers doing eval",
      "offset": 2746.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "before they launch? Like I'm working",
      "offset": 2750.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "with a government.",
      "offset": 2753.28,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "They don't want to launch until we show",
      "offset": 2755.359,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "some accuracy levels.\n Yeah.\n So, we're",
      "offset": 2759.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "getting our subject matter experts to",
      "offset": 2762.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "enter in all the questions that they",
      "offset": 2765.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "have. Right. They have huge data sets of",
      "offset": 2767.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "thousands of questions, believe me, as a",
      "offset": 2769.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "government. Um,",
      "offset": 2772,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and then we're using measures like",
      "offset": 2774.88,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "you're talking about.",
      "offset": 2777.68,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "Do you have a way to do that? Like I",
      "offset": 2781.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "guess it I guess it's the same is it?\n So",
      "offset": 2783.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "what you're describing is what we call",
      "offset": 2788.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "offline evals, right? This is",
      "offset": 2790.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "development, right?\n Um, we can actually",
      "offset": 2793.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "do this testing\n before we get into",
      "offset": 2795.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "production. And this is what I was",
      "offset": 2798.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "talking about like establish that",
      "offset": 2799.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "baseline, right?\n Using those scores,",
      "offset": 2801.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "using that that data set that you've",
      "offset": 2803.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "already um created, but this all happens",
      "offset": 2805.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "before we get into production, right?",
      "offset": 2808.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "And then you can like one of the things",
      "offset": 2809.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that I heard from from somebody earlier",
      "offset": 2811.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "is like one of my challenging things of",
      "offset": 2812.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "building this AI application is uh",
      "offset": 2815.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "establishing trust or creating that",
      "offset": 2818.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "trust in this thing. That's part of what",
      "offset": 2821.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this is, right? It's like it's showing",
      "offset": 2823.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "showing uh those people the scores of",
      "offset": 2825.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "that application. So you start to",
      "offset": 2828.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "iterate on this thing. Maybe it starts",
      "offset": 2830.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "at 20% then it goes to 30 then at 40 and",
      "offset": 2831.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so on. That to me is the thing that you",
      "offset": 2834.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "use to create that trust and uh create",
      "offset": 2836.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that like ground swell to push it into",
      "offset": 2838.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "production.\n Okay. Yeah, that's what it",
      "offset": 2840.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that is what we're trying to do. So but",
      "offset": 2842.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I wondered if I can see the tool does",
      "offset": 2844.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that. Thank you.\n Yeah, of course. Yeah.",
      "offset": 2847.04,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Time for one more.\n Thanks. Um quick",
      "offset": 2850.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "question. I love the CI/CD components.",
      "offset": 2853.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Um we're trying to build a lot of um",
      "offset": 2855.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we're trying to build like ML as a as a",
      "offset": 2857.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "platform for our team. Um so we're get",
      "offset": 2859.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "into evals and stuff like that. So how",
      "offset": 2861.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "much of how much of the monitoring",
      "offset": 2863.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "dashboard you have in brain trust can",
      "offset": 2866.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually be like take the data taken out",
      "offset": 2868.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and posted in a unified dashboard",
      "offset": 2870.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "somewhere else.\n Yeah. All of this is",
      "offset": 2872.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "available via SDK, right? You can pull",
      "offset": 2874.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "down experiments, you can pull down data",
      "offset": 2877.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "sets. Uh so you're able to you know pull",
      "offset": 2879.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "this down like we have we have a",
      "offset": 2882.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "customer that is actually building their",
      "offset": 2883.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "own UI on top of like the the SDK",
      "offset": 2885.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "itself. Like so they built their own",
      "offset": 2888.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sort of like components utilizing the",
      "offset": 2889.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "SDK and pulling the the sort of things",
      "offset": 2892.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that we've logged the experiments that",
      "offset": 2894.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "we have in the application into into",
      "offset": 2895.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "their own UI. So certainly possible.",
      "offset": 2898.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Awesome.\n Oh great. Thanks.\n All right.",
      "offset": 2900.16,
      "duration": 3.1
    },
    {
      "lang": "en",
      "text": "Cool. Thanks everybody.",
      "offset": 2901.599,
      "duration": 4.231
    },
    {
      "lang": "en",
      "text": "[Applause]",
      "offset": 2903.26,
      "duration": 9.279
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 2905.83,
      "duration": 6.709
    }
  ],
  "cleanText": "[Music]\nHey everybody, my name is Doug Guthrie.\nUh, I'm a solutions engineer at Braintrust.\nUm, as you can see here, we're an end-to-end developer platform for building AI products.\nWe do evals.\nIf you watched the keynote this morning, you saw our founding engineer jumping up and down on stage yelling evals.\nI am not going to do that.\nI'm not as uh as funny or cool as him, but uh we should all be very excited about evals here.\nVery brief agenda of what we'll cover today in this sort of uh this intro:\n*   Very, very brief company overview.\n*   Give you an intro to evals.\n*   Why would you even uh start thinking about using them?\n*   What are they?\n*   What are the different components that you need to create an eval?\n*   Some more Braintrust specific things via uh running evals via our SDK.\nYou'll see uh in the examples um that you can run evals both uh in the platform itself as well as the SDK.\nI think it's it's a really kind of cool thing that we can connect uh maybe the local development that we're that we're doing with what we're doing within the platform.\nUh and then how we then move to production.\nUh this is uh maybe human review.\nThis is online scoring.\nSo how well is our application performing uh in production and then lastly a little bit of human in the loop uh getting user feedback from your users?\nHow do we now uh take some of these uh these production logs and feed them into the data sets that we're using in our eval uh creating this really great flywheel effect.\nCool.\nQuick quick company overview.\nUm you see up there maybe in the top right uh some of our fun, excuse me, some of our investors.\nUh maybe a quick call out on the on the leadership side.\nAnker Goyle is our CEO.\nUh maybe the the reason to call out that is Anker in the last uh two two stops.\nHe he essentially built Braintrust uh from scratch the last two places.\nAnd and this is really where he found the the idea to like like maybe this is actually a a thing that that people need and and really like the origination story of Braintrust.\nUh the other thing to call out here is like we already have a lot of companies using Braintrust in production today.\nThis is just a a few of uh the the companies that are that are utilizing us for for evals for uh for observability of their their genAI applications.\nSo, I won't bore you too much with with that, but uh let's jump into this.\nIf you're at the keynote, you probably saw a similar slide here.\nI didn't I didn't take it out, but uh you see like the tech luminaries here, I think, as as Manu referenced them talking about evals and the importance of them.\nI think this is a you know, obviously this is an intro to eval uh track and what better way to start this out with uh some some really um you know, influential people in the space talking about eval and and why they are so important.\nSo why would you think about eval?\nRight?\nThey they they help you answer questions.\nSo here's here's a few of them.\nUm when I change the underlying model to my application, is it getting better or is it getting worse?\nWhen I change my prompt, right, when I change uh certain certain things about the application, is it getting better or is it getting worse?\nSo we want to get away from uh not having a rigorous sort of process around building with large language models which as you all know right non-deterministic outputs uh creates somewhat of a challenge and without uh eval becomes really really hard to uh to create a good application that we can put into production.\nMaybe some other ones like obviously being able to uh detect regressions within the within the code.\nI think the other the other thing that Anker mentioned uh to me when I first started which I didn't really mention this but uh this is my my third week at at Braintrust as a solutions engineer but one of the things that he mentioned to me that I thought really resonated was uh eval are a really great way I think people think of them as almost like unit tests for uh for you know our applications but he kind of described it another way of like this is a really great way to for us to play offense uh as opposed to just playing defense where I think maybe unit tests are are kind of used for This can actually be used as a tool to to really help uh create a lot of rigor around us building and developing these applications and ensuring that we actually build uh things that we can put into production.\nMaybe from like a business perspective, uh why would you think about running evals or using evals?\nUh here's a few here.\nUh if you have um eval running both offline and online, you create the this feedback loop or this flywheel effect.\nI think Manu mentioned in the in the keynote the keynote, excuse me.\nUm and this flywheel effect allows us to, you know, uh cut dev time, allows us to enhance the quality of this application that we're putting out in production.\nIf you're able to connect the things that are happening in in real life with your users in production and being able to uh filter those logs down, add those SP spans to data sets, inform what you're doing in an offline way creates uh that that flywheel effect that becomes really powerful from a development perspective.\nUh again, a little bit more on the on the customer side.\nHere's a few of our customers and some of the outcomes that they've that they've seen using Braintrust, whether it's moving a little bit faster, pushing more AI features into production or just increasing the quality of of the applications that they have.\nHere's a few of the the outcomes that we've seen or that they have seen.\nSo, let's let's start talking a little bit about the the core concepts of of Braintrust.\nObviously, we're here to talk about evals.\nUh the the the things that I think um you see like these arrow these arrows going uh one way and and the other this again is that that flywheel effect that that I described earlier.\nUm there's the the prompt engineering aspect of this uh in Braintrust think of uh this playground that we have as an IDE for LLM outputs.\nUh the playgrounds allow for that rapid prototyping as we make those changes as we change the underlying model.\nWhat is the impact to that to that uh particular uh task or that application and those evaluability aspect right this is the the logs that we're generating in production the the ability to uh have a human review those logs in a really easy uh intuitive interface and then have user feedback from actual users be logged into the application as well.\nSo what is an eval?\nYou probably heard this several several times today throughout the week if you stop by the booth.\nBut sort of our definition here is that structured test that checks how well your AI system performs, right?\nIt helps you measure these things that are important:\n*   Quality\n*   Reliability\n*   Correctness\n\nSo what are the ingredients in an eval?\nRight?\nI've been talking a little bit about task, right?\nThis is the thing uh the code or the prompt that we want to uh evaluate.\nThe the really cool thing about Braintrust is that this can be as simple as a as a s excuse me a single prompt or it could be this this full sort of agentic workflow where we're uh calling out to tools.\nThere's no sort of limit onto the the complexity that we put into this task.\nThe only thing it requires is an input and an output.\nThe second thing is a data set.\nThis is our uh real world examples.\nThis is uh essentially what we're going to run the task against to understand how well uh our application is performing.\nAnd how we do that is via scores.\nSo the score is really the the logic behind your eval.\nThere's there's a couple different ways to think about this.\nThere's the LLM as a judge type score.\nSo uh you give it the the output and some criteria and it is able to assess uh like say I want uh based on this output is this excellent, is this fair, is this poor?\nAnd then those outputs then correspond to you know zero point five or one.\nUh you also have codebased scores right these are maybe a little bit more heruristic or binary but uh we can use both of these to really aid in the development of that of that eval and ensuring that we're building a really good application.\nUh I think I just sort of mentioned this here as well but like the the two mental models here of of eval there's there's offline and online.\nOffline is pre-production.\nThis is us actually doing that that iteration.\nIt's uh identifying and resolving issues before deployment.\nUh this is where we're defining those tasks.\nIt's where we're defining those scores.\nUh online evals, this is that real-time tracing of of the application in production.\nIt's logging the model inputs and the outputs uh the intermediate steps, the tool calls, everything that's happening.\nIt allows us to diagnose performance and reliability issues, latency uh based on how you instrument your your application with Braintrust.\nWe can pull back lots of different metrics related to cost and tokens and duration and all of these things be help inform uh how we build this application.\nUm I'll I'll jump into a little bit more of like how we can instrument our app for online evals.\nUh we're going to first I think talk a little bit more of the offline.\nBefore doing that, maybe just like level set on how to improve.\nI think one of the the the the things that I've seen uh in the last few days here, the conversations that I've had, it's like almost how do I get started or what do I do if X or you know those types of questions.\nAnother thing I had heard from Anker uh very early on is that like just get started create that baseline that you can then iterate and build from.\nUh I think a lot of people get caught in like creating this this golden data set of test cases uh that they they can then like iterate from.\nStart you you don't necessarily have to do that.\nStart and build that uh that baseline.\nEstablish that foundation that you can then improve upon.\nBut this is a really good sort of matrix of like uh if I have good output but a low score, what do I do?\nRight?\nImprove your evals.\nIf I have bad output and a high score, improve your your evals or your scoring.\nBut really good kind of like highle uh understanding of of where to start to target your your efforts when you are building these apps and you're and you're creating evals.\nSo let's jump into the actual components that I just that I just talked about.\nSo within the Braintrust platform we have a task.\nUh again this could be a prompt.\nThis could be like this full agentic workflow.\nUh very basic you see that that gif running.\nThis is a prompt within the platform.\nYou uh specify an underlying model that you want it to use.\nYou give it a a system prompt.\nYou can also uh give it access to tools.\nIt has access to mustache templating.\nSo you can pass in variables like user questions or um you know the input from the user or its chat history or metadata.\nRight?\nSo when we actually go and want to parse through these logs, the metadata becomes actually beneficial uh enabling us to do that in a really easy way.\nUm going forward uh maybe we have a multi-turn sort of chat type scenario where we want to add uh additional messages for the system and the assistant and the user uh and our tool calls as well.\nThe uh the the platform allows for that just via this uh plus this messages uh button and then you're able to add those different messages to the prompt.\nUm also we can add tools.\nSo oftentimes the the prompt will will need access to to something, right?\nMaybe it's a a rag type workflow, maybe it's doing web search, whatever it is, we can now use those tools as part of that prompt, right?\nAnd so when you you sort of encode in that prompt like make sure you use X tool, uh this prompt has access to that tool while it's running.\nUh the last one, this is actually a feature that is in beta right now.\nUh it's actually creating more of that agentic type workflow within your uh within the Braintrust platform itself.\nSo it's it's a way right now at least to chain together prompts where the the output of one now becomes the input of the other.\nBut if you think back maybe to to this slide, if I have sort of um this prompt that has access to tools, you you create a pretty powerful system here where you're able to go from uh maybe that first step that has access to a certain tool and we get some output from that.\nWe can then go to that next step that has access to maybe some other tools, right?\nThis sort of like maybe multi- aent type of workflow we can create with the underlying tools within those prompts.\nThe second thing that I talked about is data sets, right?\nThese are our test cases that we want to give to the the task to run.\nSo we can sort of iterate over that.\nWe can get the output and then going down a little bit further actually score that.\nBut this is um obviously really important when we're when we're running our eval.\nAnd then when we are uh trying to pull from production, right, the the actual logs that are happening, we can add those those uh spans, those traces to the data sets in a really easy way.\nAnd I can show you what that looks like.\nBut uh if you look there at the bottom, the only thing that's required is the the input.\nUh you also have the ability to add uh expected.\nSo what is the expected output for that input, right?\nYou can sort of like uh create some sort of score that looks at the output with the expected.\nThere's a a score called Levvenstein that allows you to you know measure the the the difference between those two.\nSo you can do some different things based on what you provide to that data set.\nYou also have metadata as well.\nAgain being able to filter down different things pulling the data set maybe into your own codebase and I want to filter by again x y or z via the metadata.\nThat's all possible.\nUh I mentioned this a little bit ago, but just start small and iterate, right?\nYou don't have to create this golden data set to to get started here.\nUm just just start and then uh then continue to iterate and build from that baseline.\nUh the human review portion also becomes really powerful again when we have stuff uh being logged within production having humans actually go through those logs and you there's lots of different ways to filter it down to the things that they should be looking at and then we can now decide to add those things to certain data sets that then inform uh the offline evals that we're running.\nUh the last thing, the last ingredient here that we need for our excuse me uh for our evals are our scores.\nWe have both codebased scores, right?\nAgain, this is like more of those binary type conditions, but you can actually uh code TypeScript or Python.\nYou can do that within the UI as you see over there on the bottom left.\nOr you can within your own codebase, create that score and then push it into Braintrust.\nSo we can use it in the platform.\nOther users who maybe aren't in the codebase can use that.\n\n\nScore as well.\nThe other score that we have access to is called LLM as a judge.\nSo this allows us to use an LLM to sort of judge the output.\nWe can give it the set of criteria that indicates what a good or a fair or a bad score or whatever it is.\nYou get to decide what that looks like.\nSo you give it that criteria, and it says, \"If it's good, I want to do a one.\nIf it's bad, I want to do a zero.\"\nBut this starts to create the scores that we can use in that offline, in that online sense.\nThe other thing to call out here is that we have internally built a package called auto evals.\nSo this is something that you can now pull into your project.\nThese are out-of-the-box scores that are both LLM as a judge as well as code-based.\nAnd so it just allows you to get started very, very quickly.\nAnother thing I heard Anker mention is maybe starting with Levenshtein, maybe not the best score in a lot of cases, but again, it establishes a baseline, very, very little development work for our users, but it creates that thing that we can then build from, and now you have a direction to go in, to go build maybe that more custom score.\nSome of the things that we've heard from our customers, some tips that are important to think about.\nA lot of our customers are using higher-quality models for scoring, even if the prompt uses a cheaper model.\nIt just makes a lot of sense, like, while we're running that application to use the cheaper model, but use the more expensive one to actually go out and score it.\nAlso, break your scoring into very focused areas.\nSo the example that I'll show is an application that generates a change log from a series of commits.\nSo I could create a score that says, \"Assess my accuracy, my formatting, and my correctness.\"\nOr I could create three different scores that assess accuracy and then formatting and then correctness.\nSo have your scores be very targeted to the thing that they're supposed to be doing.\nTest your score prompts in the playground before use, and then avoid overloading the score or prompt with context.\nFocus it on the relevant input and the output.\nA couple things here.\nHere's where, over on the left, we have our playgrounds.\nThis is where we do that rapid iteration, where we can pull in those prompts.\nWe can pull in those agents, add our data sets, and add our scores, and we can click run, and it'll go out and sort of churn through that data set that we've defined and will give you a sense for how well your task is performing against the data set with the scores that we defined.\nBut this is the place where developers, PMs, we even have a healthcare company that has doctors coming into the platform and interacting with the playground and even doing human review as well.\nIt depends a little bit, obviously, on the organization.\nThe thing on the right is our experiments.\nThis is our sort of like snapshot in time of those evals.\nSo imagine now, like, as we are doing this development and we're trying to understand, like, the last, you know, the last month or so, are we getting better, right?\nThe changes that we are making, the model changes, whatever it is, are we improving our application?\nAnd the experiments is a really great way to understand that.\nReally important, maybe to call out as well.\nYou can see in the bottom right, the eval can happen from the application, right?\nThe Braintrust platform as well as via the SDK.\nCool.\nMaybe just really quick, because nobody likes looking at slides all the time.\nI certainly don't.\nMaybe if you haven't seen Braintrust yet, this is maybe a good, a good quick demo.\nSo again, like the idea here is I have this application.\nI'll just give you, I'll show you over here.\nYou give it a GitHub repository URL.\nIt grabs the most recent commits and then creates a change log from there.\nAnd then once this completes, you can even provide some user feedback.\nBut this is the thing that we want to evaluate.\nSo what I can do, I'll go into my playground, right?\nThis is the place where I can start to run those experiments, or I can start to iterate on that prompt that I have from my project.\nI've actually loaded in two different, two different prompts.\nSo maybe I'll, before going into the playground, I've actually created these two prompts within my codebase, and I pushed them into the Braintrust platform.\nI've also created a data set in that codebase, and I've also created some scores, right?\nThese are the ingredients that we need to run our eval.\nSo now when we have, we have those, we have those different components, now we're able to start to iterate here.\nSo I'm going to actually create a net new playground, and I will load in one of these prompts.\nSo again, here's my first prompt.\nMy first prompt has a model associated with it.\nWhat becomes really cool here is the ability to, like, to iterate on the underlying model, right?\nI think a lot of us are, we have access to a lot of underlying providers, and we want to be able to understand if I change this, or if I, you know, a provider adds a new model, what is the impact to my application?\nSo I can duplicate this prompt and maybe change this to GPT41.\nI can run this.\nBefore I run it, I have to add all of my components.\nSo I'll add my data set, and then I can add my different scores that I've configured here for my change log.\nAnd so I can click run, and then now we'll understand here what is the effect of changing the model for this particular task with the scores that I've configured against this data set.\nSo this will churn through all of these in parallel, and we'll start to get some results back.\nLots of different ways to actually start to look at this data.\nI always like coming over to the summary layout because I can understand, like, you can see over here, this is my base task right here, and then my comparison task.\nSo I can understand, looks like, you know, on average, the base is performing a little bit better than my comparison task on my completeness score.\nIt's fairing a little bit worse on my accuracy score.\nBoth of them are, you know, 0% on my formatting.\nSo probably have some work to do there.\nBut you can start to see how you can use this type of interface to iterate very quickly.\nRight now, the other thing that maybe shouldn't do, but I can't resist because we just released this today, is this new loop feature.\nSo imagine you are, you know, you're a user within Braintrust, and before this, you would sort of manually iterate here, creating net new prompts, making modifications, changing the model.\nWhat if you could now utilize AI to go and do that for you?\nSo in a sort of like cursor-like interface, we can ask it to optimize a prompt.\nAnd I think the really unique thing here is it has access to those evaluation results.\nAnd so when it goes to go change that prompt, it understands that it changes the prompt, it runs the evaluation.\nIt understands if it got better relative to the scores that we defined.\nSo you can see it's going to go through here.\nIt'll fetch some eval results.\nYou'll probably see a diff here very, very soon.\nIf we don't, I won't, I won't hang out here too long, but I do want to highlight one of the things that we are releasing that really enables our users to iterate in a really, really fast way.\nSo, here's my change.\nWe can click accept, and then it'll actually go out and run that eval again, or it would, I think I have an issue with my Anthropic API keys.\nBut the idea here again is like we can create that very rapid iterative feedback loop here within the playground.\nThe other thing here is we can run these as experiments.\nSo this is where we can start to create those snapshots in time of that eval, and again, see as I make these changes to that application, how is it sort of performed over time.\nI want to make sure I don't, I don't want to go down.\nI don't want to, like, decrease the performance of my scores relative to, obviously, the last time it ran, but looking out over the last month, six months, whatever it is that we're tracking.\nCool.\nSo that was a very, very brief sort of intro to, like, eval via the UI, right?\nAgain, like, just to summarize, we need a task, we need a data set, and we need at least one score.\nWe can pull those into the playground, and now we can start to iterate.\nWe can save these via experiments, and now we can, we have a way in which we can understand how well this application is performing, right?\nThis is no longer like qualitative, right?\nThis isn't like, \"Hey, I think this got better, that output looks better.\"\nThere is actual rigor behind this.\nNow, customers oftentimes ask, though, like, \"I don't really want to use, or I'm not going to use the platform as much.\nI'd rather use this from my codebase.\nIs that possible?\"\nAnd it is.\nSo we have a Python SDK, we have a TypeScript SDK, there are some other ones as well: Go, Java, Kotlin.\nFor the most part, most of our users are using Python or TypeScript.\nHere's a couple examples of what this might look like from an SDK perspective.\nActually, if you all aren't opposed to looking at some code, here's just a really basic example of defining a prompt within my codebase and then pushing it into Braintrust.\nSo, just leveraging that Python SDK.\nAnother example should come over here, creating a score.\nSo you give it sort of like the things that it's looking for, but now I've sort of defined this score within my codebase.\nIt's version controlled.\nAlso, the prompts that you create within the UI are version controlled as well.\nBut this is just another way to start to interact with Braintrust.\nSo again, scores, we could do data sets, and then you can even do prompts up here as well, I believe.\nSo here's my eval data set.\nHere's my change log to prompt.\nAgain, being able to start from the codebase and actually push them into the platform is possible.\nJust depends on the organization where they want to start.\nThe other thing here is like this is more on the components of the eval side.\nSo that's that top portion.\nDefine those assets in code.\nRun that Braintrust push, and now you have access to that in that Braintrust library.\nThe other one is actually like defining the evals in code, right?\nSo what that looks like is slightly different.\nCome over here.\nSo we have our eval class that's coming from our Braintrust SDK.\nAgain, it's looking for the exact same things that I just described, right?\nA data set that we can use from Braintrust itself, the task that we want to invoke, and then the scores.\nSo again, defining this here within your codebase is certainly possible, and then I can run, you know, a command that actually runs that eval within Braintrust.\nSo from here, go into Braintrust, see the eval running.\nThis is now an experiment that I can view over time.\nSo again, like you saw two different types of workflows here, again, catering to maybe two different personas, or again, the way in which organizations want to work, it's up to them.\nBraintrust is very flexible in how we allow our users to consume or use the platform.\nProbably jumped ahead a little bit, but this is sort of a recap of what I just showed you.\nAgain, from your code, you create your prompts, your scores, your data sets, you can push them in there.\nMaybe just importantly, important to highlight here of why you would do this.\nYou want to source control your prompts.\nThe big one here to call out is the online scoring.\nI have a section in a little bit diving a little bit deeper into that.\nBut if you want to use those scores that we define in the data set, we should push them into Braintrust so that we can create online scores.\nWe can understand how our application is performing in production relative to those scores that we want or that we're using within our offline evals.\nWhat I just showed you, maybe another variation of that, that eval within our code, again, defining that data set, defining that task, defining those scores becomes very, very easy to now connect these two things.\nIt's just again up to you to decide where you are, where you want to do this.\nThe other thing to call out here is that this can be run via CI/CD.\nWe do have some customers that want to run their eval as part of the CI process, so understanding in a more automated way, right?\nThe score for A and C, whatever they've configured, has it gotten better, has it gotten worse?\nThis becomes maybe a check as part of CI.\nThere is, if you look within our documentation, there's a GitHub action example that shows you how you could set this up.\nCool.\nLet's move to production.\nMoving to production entails setting up logging, right?\nIt entails instrumenting our application with Braintrust code, being able to, like, say, \"I want to wrap this LLM client.\nI want to wrap this particular function when it goes to call that tool,\" becomes very, very easy to do that.\nBut so, why should you do it?\nI think I've probably said it numerous times here, but we want to measure quality on live traffic, right?\nWe actually want to understand how well our application is performing with those scores.\nIt's really great to use during offline eval, becomes our aid in ensuring that we build really good applications, that we're not creating regressions, but also really important to monitor that live traffic.\nThe other really important thing to call out, I think, is that flywheel effect that it creates.\nSo we have these data sets that we use to inform our offline evals.\nIt's very, very easy now to take the logs that are generated within production and add those back to data sets.\nThis also speaks to some of that human review component where we want to now bring those humans in.\nThey can start to review some of the logs that are relevant, like, maybe there's user feedback equals zero, maybe there's a comment or whatever it is, but like, they can filter down to those particular things, and as they find really interesting maybe test cases, it's very, very easy to add those back to the data set that we use in our offline evals.\nSo I think the feedback loop or the flywheel effect that this creates is one of the really fundamental value props of the platform.\nSo how do we do this?\nThere are a couple different ways, right?\nWe're first going to initialize a logger.\nThis is just going to authenticate us into Braintrust and point us to a project.\nYou may have seen when I open up the platform, I had numerous projects inside of there.\nYou can almost think\n\n\nOf a project as a container for that feature, right?\nSo you probably have multiple AI features that you're building.\nI want to have a container for feature A for those prompts, those scores, those data sets.\nYou could certainly utilize those things across projects, but it becomes a really good sort of way to containerize the things that are important for that feature.\nThen you can start really basic, right?\nYou can wrap an LLM client.\nSo when you saw those some of those metrics with like tokens and duration and costs, just very basically within the script or excuse me, the code here, I just wrapped that OpenAI client, and now I'm just sort of ingesting all of those metrics into my logs.\nThat's the easiest way to get started.\nYou obviously probably want to do a little bit more.\nAgain, maybe you want to understand when you know that LLM invokes a tool.\nSo, I want to trace.\nI can add a trace decorator on top of a function.\nI can even use some of the Braintrust low-level like span elements to create custom logs.\nAnd I want to customize the input, and I want to customize the output and the metadata that we log to that span.\nSo again, you can start very basic with wrapping in a client and then go down to like the individual span itself, specifying that input and that output.\nThis leads us to online scoring, right?\nThis is I talked a little bit about this, right?\nThis is where like when our logs are coming in, we can actually configure within the platform those scores that we want to run, and we can specify sort of a sampling rate.\nSo, we don't necessarily run that score across every single log that comes in.\nMaybe it's 10%, 20%, so on.\nUm, but it creates that really tight feedback loop that I've been talking about.\nUh, also maybe just important to mention the early regression alerts.\nSo we can create automations within the Braintrust platform.\nIf my score drops below a certain threshold, let's create an alert with our automation feature.\nThis is just, and I can maybe walk through what this looks like instead of showing you here, the custom views.\nThis is where like there's a lot of really rich information within these logs, and it becomes really important, I think, again, for the human review component to like filter these down to the things that they care about or the things that anybody cares about.\nSo we can create custom views within Braintrust with the appropriate filters, and then it's very easy for that human to go into what we call human review mode within Braintrust and sort of parse through those logs, the ones that are the ones that are going to be most meaningful to them.\nLet me let me connect some of those dots there.\nSo again, showing you some code, maybe good, maybe bad, but um, I'm guessing there's some technical people in the room that don't mind here.\nSo if I look for um, the, uh, you may have seen in one of those slides, there is the Vercel AI SDK.\nI want to wrap this AI SDK model.\nAgain, this allows us to just create all of those metrics within Braintrust with just zero lift from us as a developer.\nThis becomes really easy to do.\nUh, you can also see where I have specified that span itself, right?\nI actually want to define the inputs and the outputs of that.\nThe reason you would do that is because you have a specific data set with a structure that you want to ensure maps to that.\nSo like when you are within those logs, parsing through them becomes really easy to add those spans back to that data set.\nSo ensuring that that data structure is sort of consistent across offline and online becomes really important again to create that feedback loop.\nSo this is you know, very high level of like how we can start to create those spans.\nUm, now that we do, right, we can now go in the platform and start to configure our online scoring.\nSo this is here just within this configuration pane.\nI can click online scoring.\nI'll just delete, I'll create a new rule.\nUh, so my new rule, and here's where we can add different scores, right?\nObviously, I have a few here that I've been using for the offline evals.\nI don't necessarily need to select all of them, but I certainly can.\nAnd then I want to apply a sampling rate.\nSo I want to actually give you an example of what this looks like.\nSo I'm going to do 100%.\nThe other thing to call out here is that you can apply these to the individual spans themselves and not the entire root span.\nSo where this becomes beneficial is like when you are invoking maybe tool calls, you're invoking like a RAG workflow, and you actually want to create a score on whether or not the thing that it gave back is actually relevant to the user's query.\nSo we can actually create a score specifically for that and highlight what that span is here.\nSo again, very, very flexible in how you apply these scores to the things that are happening online.\nNow when I come back here to the application and we'll just run this again, creating that change log, you'll now start to see here within the logs, this will start to show up, and then you'll start to see these scores be generated.\nRight?\nAgain, this is where like you can now start to understand over time in production, how are these things doing?\nHow are they fairing?\nWhere can we get better?\nUh, again, now we can connect again like the things that are happening in our offline evals with the things that are happening with online.\nThe other thing to call out here is the feedback mechanism, right?\nUh, we certainly have the ability to do like human review, but oftentimes you want your users to provide feedback as well.\nAnd so this is just a basic example of a thumbs up, thumbs down, and you can even provide a comment here.\nThis can now be logged to Braintrust.\nSo I should see over here my user feedback.\nSo here's my comment, and then I have my user feedback score.\nBut now I can also do something like this.\nSo again, maybe I want to filter my logs down to where user feedback is zero.\nSo click that button.\nI'm going to change this to zero.\nRight?\nI don't have any rows yet like that.\nBut now I can save this as a view, and people who are now using this as human review can filter this down to where user feedback equals zero, and we can figure out what's going on, right?\nWhat are the things that fell down here within this application that we need to go fix?\nThe other thing I'll highlight here is our sort of human review component.\nUh, actually, um, you click that button or you can hit just R, and it opens up this different pane of your log.\nSo it's a paired down version of what you just saw there.\nIt's a little bit easier for a human to go through and actually look at that input and that output.\nBut you as a user of Braintrust can configure the human review scores that you would like to use.\nSo I have this add something here.\nSo maybe this is a little bit more free text.\nI have a better score.\nAgain, these are the things that you can add to your platform that map to the review, excuse me, the scores that you want your humans to add to those logs.\nUm, just really quick, I'll highlight some of these things here.\nUm, this is what it starts to look like when you instrument your application with those different wrappers or those different trace functions.\nUh, I'm able to understand at a very granular level, excuse me, granular level, the things that it's doing, right?\nSo I essentially have these tool calls where it's going out and it's grabbing the commits from GitHub.\nIt's understanding what the latest release is, and it's fetching the commits from that latest release, and now I can generate that change log.\nBut again, the really unique thing here, and maybe a different example of this, is I can start to score those individual things that are happening.\nSo this is a different application with this example.\nSo if I open this up, I have this conversational analytics application.\nSo a user can ask a question, it can return back some data.\nBut this application goes through these various steps, right?\nThe first step is to rephrase the question that the user asked.\nSo imagine like there's this chat history that we can load in as input, and the LLM needs to rephrase that user question.\nIf the LLM does a really bad job of rephrasing this question, everything as a result of this will fall down, probably not going to get a right answer.\nSo what I can do is create a score specifically for that span to understand how well the LLM did in rephrasing that question, can also understand the intent that I was able to derive or the LLM was able to derive from that question.\nIs that right?\nBut you start to think of like these more complex type of applications that you build.\nYou need to be able to understand the individual steps that are happening, and Braintrust allows for that very, very easily via these scores and then being able to apply them not only again while you're in offline eval kind of mode, but also online, right?\nWe want to understand these logs and be able to apply these scores at the individual span level.\nThis becomes pretty powerful as well.\nUm, I think I actually stole from my next section, my human in the loop.\nKind of walked through this a little bit.\nUm, maybe just another call out.\nIf you happen to be at one of our workshops on Tuesday, um, Sarah from Notion, who's a Braintrust customer, talked a little bit about how they think about human in the loop.\nAnd I think it's important to consider like the size of her organization and what they're doing.\nUm, she mentioned that like she has a special type of role that they use for human in the loop type of interaction, right?\nThere is it's almost like a product manager mixed with an LLM specialist.\nUh, they're the people that are going through and doing those human reviews.\nSmaller organizations, she made a comment that was it actually makes a lot of sense for the engineers, some engineers to actually go through and do this as well.\nBecomes really powerful to pair like the automation with the human component of this.\nLike this is not going to go away.\nI think it adds value to the process.\nUm, again, I think I just stole from myself, like why this matters, right?\nThis is really critical for the quality and the reliability of your application.\nUh, it provides that ground truth for what you're for what you're doing.\nTwo types of human in the loop interactions here.\nI walked you through that human review.\nUm, give me one second and I'll call you.\nYeah.\nUh, the two, excuse me, the two types, the human review, uh, being able to like create that interface within Braintrust that allows that user to kind of parse through the logs in a really easy manner, as well as configuring scores that allow them to add the relevant scores to that particular log and then the user feedback.\nThis is actually coming from our users in the application.\nAgain, being able to create sort of views on top of that feedback that then power maybe the human review and then creates that flywheel effect that we that we that we want.\nThat's all I have today.\nAppreciate you all coming out here and listening to me.\nBut yeah, you had a question.\nThanks.\nYeah, the question is around like how are we using human review and like some of the logs and informing the offline eval portion of this largely that cool.\nUh, yeah, one thing I maybe I didn't highlight here is so maybe back within Braintrust, I'm going to go back to my initial project.\nSo imagine now like we have we have all of these logs.\nWe filtered it down to a particular.\nOh, are we still showing on the screen?\nAwesome.\nThank you.\nUm, yeah.\nSo, imagine like we have this process now, right, where we're we're doing that human review.\nWe filtered it down to the records that are meaningful for whatever reason.\nIt becomes really easy again to connect what's happening within production.\nSo, I maybe I select all of these rows or I select individual rows, but I can add these back to the data set that we're using within those offline evals.\nI I think I've said this like a hundred times over this conference, this flywheel effect.\nThis is like I think what's missing oftentimes when we're building these these AI applications and what Braintrust allows for really seamlessly.\nYeah.\nUh, I have two questions.\nUh, the first one is about production.\nUh, is it possible to have multiple models in production and compare how they behave?\nYeah, I I don't see why not.\nLike my guess is in the underlying application, you're swapping them out, like having like AB tests, you know, I can have like a two or three or four and easily compare.\nAbsolutely.\nYeah.\nUm, let's see if I have an example here.\nYou're able to group some of these scores.\nUh, maybe this is sort of an example of what you're talking about.\nSo like maybe within production we have different models running.\nUh, this this sort of view here allows us to understand like the models that we're using under the hood.\nUh, and this is just you know, you could do this within production as well and sort of do that AB testing.\nCool.\nUh, my my second question is about humans in the loop, right?\nUm, let's suppose that I have multiple humans and um, they behave slightly different as a scorer scorers.\nDo you have anything or what what is the vision to do with that?\nLike is there a way that I can actually compare how they're scoring or something like that or not really?\nSo different users can maybe have different sort of criteria for scoring.\nMaybe the first thing I would say to that is like there there should be like maybe a rubric for your users who are interacting with human review.\nSo you're not creating that.\nUh, you certainly have the ability to see like who is scoring different things within the platform.\nUm, I'm not sure if you're able to pull that as like a data set to like assess the differences there, but maybe like before it gets to that place, like have a rubric, have a guideline of of what scoring looks like for your humans.\nOkay.\nThank you.\nYeah, of course.\nHi.\nUm, so the scorers I'm used to working with for like LLM as a judge are like they're relativistic, right?\nSo they can't tell you is the answer relevancy good or bad for a single run, but it can tell you how it compares to previous iteration of like the same test set, for example.\nUm, do you guys use LLM as a judge scores for online or is and like how are are they relativistic like that or do you have some way to be like this is a good answer, you know, in and of itself for this sample or because it's all you have new data coming in, right?\nYeah, I think a lot of our customers who are are thinking\n\n\nAbout this are like, almost doing Evals 101, like trying to understand, did the LLM as a judge actually do a good job there?\nSo, like when that actually runs, there's a rationale behind it, and so you can sort of run an eval of those LLMs as a judge.\nI think Sarah from Notion in our workshop described sort of a process like that within Notion, but I think that's that's sort of like where I would aim you.\nOkay.\nCool.\nThanks.\nCool.\nAwesome.\nAre any of your customers doing eval before they launch?\nLike I'm working with a government.\nThey don't want to launch until we show some accuracy levels.\nYeah.\nSo, we're getting our subject matter experts to enter in all the questions that they have.\nRight.\nThey have huge data sets of thousands of questions, believe me, as a government.\nUm, and then we're using measures like you're talking about.\nDo you have a way to do that?\nLike I guess it I guess it's the same, is it?\nSo what you're describing is what we call offline evals, right?\nThis is development, right?\nUm, we can actually do this testing before we get into production.\nAnd this is what I was talking about, like establish that baseline, right?\nUsing those scores, using that that data set that you've already um created, but this all happens before we get into production, right?\nAnd then you can like one of the things that I heard from from somebody earlier is like one of my challenging things of building this AI application is uh establishing trust or creating that trust in this thing.\nThat's part of what this is, right?\nIt's like it's showing showing uh those people the scores of that application.\nSo you start to iterate on this thing.\nMaybe it starts at 20%, then it goes to 30, then at 40, and so on.\nThat to me is the thing that you use to create that trust and uh create that like ground swell to push it into production.\nOkay.\nYeah, that's what it that is what we're trying to do.\nSo but I wondered if I can see the tool does that.\nThank you.\nYeah, of course.\nYeah.\nTime for one more.\nThanks.\nUm quick question.\nI love the CI/CD components.\nUm we're trying to build a lot of um we're trying to build like ML as a as a platform for our team.\nUm so we're get into evals and stuff like that.\nSo how much of how much of the monitoring dashboard you have in Braintrust can actually be like take the data taken out and posted in a unified dashboard somewhere else?\nYeah.\nAll of this is available via SDK, right?\nYou can pull down experiments, you can pull down data sets.\nUh so you're able to you know pull this down like we have we have a customer that is actually building their own UI on top of like the the SDK itself.\nLike so they built their own sort of like components utilizing the SDK and pulling the the sort of things that we've logged the experiments that we have in the application into into their own UI.\nSo certainly possible.\nAwesome.\nOh great.\nThanks.\nAll right.\nCool.\nThanks everybody.\n[Applause]\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.264Z"
}