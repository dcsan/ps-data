{
  "episodeId": "PjaVHm_3Ljg",
  "channelSlug": "@aidotengineer",
  "title": "Transforming search and discovery using LLMs â€” Tejaswi & Vinesh, Instacart",
  "publishedAt": "2025-07-16T18:01:03.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1.72,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "Hi, good afternoon everyone. Uh my name",
      "offset": 15.679,
      "duration": 6.001
    },
    {
      "lang": "en",
      "text": "is Vines and he's the we are part of the",
      "offset": 18.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "search and machine learning team at",
      "offset": 21.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Instacart. So today we'd like to talk to",
      "offset": 23.039,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you about how we are using LMS to",
      "offset": 25.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "transform our search and discovery.",
      "offset": 27.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "Um, so yeah, so first a little bit about",
      "offset": 30,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "ourselves. Yeah, as I mentioned, we are",
      "offset": 32.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "part of the search and discovery ML team",
      "offset": 34.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "at Instacart. And for those of you who",
      "offset": 36.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "may not be part who may not be familiar",
      "offset": 38.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "with Instacart, it's the leader in",
      "offset": 40.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "online grocery in North America. Uh, and",
      "offset": 42.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "our mission is to create a world where",
      "offset": 45.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "everyone has access to the food they",
      "offset": 47.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "love and more time to enjoy it together.",
      "offset": 49.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So coming to what we'll actually talk",
      "offset": 52.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "about today. Uh first we'll talk about",
      "offset": 54.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the importance of search uh in grocery",
      "offset": 56.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "e-commerce. Uh then we'll look into some",
      "offset": 58.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "of the challenges facing conventional",
      "offset": 61.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "search engines. Uh and then actually get",
      "offset": 63.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to the meat of the talk today which is",
      "offset": 65.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "how we are using LMS to solve some of",
      "offset": 67.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "these problems. Uh finally we'll finish",
      "offset": 69.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "with some key takeaways from today's",
      "offset": 72.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "talk.",
      "offset": 73.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "So coming to the importance of search",
      "offset": 75.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and grocery commerce. Uh I think we've",
      "offset": 77.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "all gone grocery shopping. Customers",
      "offset": 79.119,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "come with long shopping lists. Uh and",
      "offset": 80.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it's the same on the platform as well.",
      "offset": 83.439,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "People are looking for tens of items. Uh",
      "offset": 85.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and of these a majority of them are just",
      "offset": 88.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "restocking purchases that is things that",
      "offset": 91.04,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the customer has bought in the past. Uh",
      "offset": 93.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and the remaining are items that the",
      "offset": 95.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "user is trying out for the first time.",
      "offset": 97.2,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "So uh and and a majority of these",
      "offset": 99.439,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "purchases come from search. So uh search",
      "offset": 101.439,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "has a dual role. It needs to both",
      "offset": 104.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "support uh quick and efficient uh it",
      "offset": 106.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "needs to have the customer quickly and",
      "offset": 109.68,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "efficiently find the product they're",
      "offset": 110.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "looking for and also enable this new",
      "offset": 112.479,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "product discovery. Uh and new product",
      "offset": 114.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "discovery isn't just important for the",
      "offset": 117.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "customer. It's also great for our",
      "offset": 119.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "advertisers because it helps them",
      "offset": 121.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "showcase new products. Uh and it's also",
      "offset": 123.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "good for the platform because overall it",
      "offset": 125.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "encourages larger basket sizes. Uh so",
      "offset": 128.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "let's see what some problems are with",
      "offset": 130.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "our existing setup that sort of uh makes",
      "offset": 132.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "this hard. Uh so so to begin with uh we",
      "offset": 134.8,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "have two classes of queries that are are",
      "offset": 138.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "generally more challenging especially",
      "offset": 140.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "from an e-commerce perspective. Uh the",
      "offset": 142.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "first are overly broad queries uh in",
      "offset": 145.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this case like on the left the snacks",
      "offset": 147.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "query where there are tons of products",
      "offset": 149.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that map to that query. Uh and now",
      "offset": 151.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "because our models are trained on",
      "offset": 154.239,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "engagement data, if we aren't exposing",
      "offset": 156.239,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "these products uh to the user, it's hard",
      "offset": 158.879,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to actually collect engagement data to",
      "offset": 161.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to them, rank them up high. So the",
      "offset": 163.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "traditional cold start problem in a way.",
      "offset": 165.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Uh then uh as you can see on the query",
      "offset": 167.44,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "on the right we have very specific",
      "offset": 169.36,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "queries like unsweetened plantbased",
      "offset": 170.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "yogurt where the user is looking for",
      "offset": 172.879,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "something very specific and these",
      "offset": 174.959,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "queries uh don't happen very frequently",
      "offset": 176.879,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "which means that we just don't have",
      "offset": 179.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "enough engagement data to train the",
      "offset": 181.04,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "models on. Uh and while we have uh done",
      "offset": 182.64,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "quite a bit of work to sort of um",
      "offset": 186.879,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "improve this, the challenge that we",
      "offset": 189.519,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "continually keep facing is that while",
      "offset": 192.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "recall improves, precision is still a",
      "offset": 193.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "challenge, especially in a prelim world.",
      "offset": 196.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Uh the next class of problems is how do",
      "offset": 198.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "we actually support that new item",
      "offset": 200.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "discovery as we spoke about. So when a",
      "offset": 202.239,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "customer walks into a grocery store,",
      "offset": 204.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "let's say into a pasta aisle, they might",
      "offset": 207.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "see new brands of pasta that they would",
      "offset": 209.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "want to try out. Uh along with that they",
      "offset": 211.599,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "would also see pasta sauce and every",
      "offset": 213.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "other thing that's needed to make a bowl",
      "offset": 215.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "of pasta. Uh and customers would want a",
      "offset": 217.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "similar experience on our site. Uh we",
      "offset": 220.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "have heard multiple feedback multiple",
      "offset": 222.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "rounds of feedback from our customers",
      "offset": 225.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that hey I can find the product that I'm",
      "offset": 226.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "that I want via search but when I'm",
      "offset": 228.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "trying to find any other related",
      "offset": 231.599,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "products it's it's a bit of a dead end.",
      "offset": 233.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I would need to make multiple searches",
      "offset": 235.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to get to where I want to. So this was a",
      "offset": 237.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "problem that we wanted to solve as well.",
      "offset": 239.519,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "And yeah, as I mentioned, prel this was",
      "offset": 242.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "a a hard problem because of the lack of",
      "offset": 245.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "engagement data, etc. So let's see how",
      "offset": 247.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "we actually use thems to sort of solve",
      "offset": 250.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "these problems. U I'll sort of talk",
      "offset": 252.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "specifically about how we use TMs to",
      "offset": 254.56,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "uplevel our query understanding module.",
      "offset": 257.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Now query understanding as I'm sure most",
      "offset": 260.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of you know uh is the most upstream part",
      "offset": 262.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "of the search stack. uh and very",
      "offset": 264.639,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "accurate outputs are needed to sort of",
      "offset": 267.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "enable better retrieval and recall uh",
      "offset": 269.919,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and finally improve our ranking results.",
      "offset": 272.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Uh so our query understanding module has",
      "offset": 275.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "multiple models in them like query",
      "offset": 277.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "normalization, query tagging, query",
      "offset": 279.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "classification, category classification",
      "offset": 281.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "etc. So in the interest of time uh I'll",
      "offset": 283.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "just pick a couple of models and talk",
      "offset": 286.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "about how we u sort of really improved",
      "offset": 289.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "them. Uh the first is our query to",
      "offset": 291.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "category a product category classifier.",
      "offset": 294.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Uh essentially we are taking a query and",
      "offset": 297.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "mapping it to a category in our",
      "offset": 299.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "taxonomy. Uh so as an example if you",
      "offset": 301.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "take a query like watermelon that maps",
      "offset": 303.919,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "to categories like fruits, organic",
      "offset": 306.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "fruit, foods etc. Uh and our taxonomy",
      "offset": 308.8,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "has about 10,000 labels of it 6,000 are",
      "offset": 312.24,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "more commonly used. So because a product",
      "offset": 314.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "a query can map to multiple labels. This",
      "offset": 317.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is essentially a multilabel",
      "offset": 320,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "classification problem. Um, and in the",
      "offset": 321.199,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "past our traditional models, uh, which",
      "offset": 325.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "were we actually had it a couple of",
      "offset": 327.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "different models. One was a fast",
      "offset": 330.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "textbased uh, neural network which",
      "offset": 332.32,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "essentially modeled the semantic",
      "offset": 334.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "relationship between the query and the",
      "offset": 336.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "category. Uh, and then as a fallback we",
      "offset": 338.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "had an npmi model which was a",
      "offset": 340.4,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "statistical co-occurrence model between",
      "offset": 342.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the query and the category. Now, while",
      "offset": 344.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "these uh techniques work great for the",
      "offset": 346.639,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "for the head and torso queries, we had",
      "offset": 349.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "really low coverage for our tail queries",
      "offset": 352.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "because again, we just didn't have",
      "offset": 354,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "enough engagement data to train the",
      "offset": 355.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "models on. Um, and to be honest, we",
      "offset": 356.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "actually tried more sophisticated",
      "offset": 359.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "bird-based models as well. Uh, and while",
      "offset": 361.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "we did see some improvement, the lack of",
      "offset": 364.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "engagement data meant that for the",
      "offset": 366.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "increased latency, we didn't see the",
      "offset": 368.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "wins that we actually hoped for.",
      "offset": 370.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So um so this is where we actually tried",
      "offset": 373.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to use an LLM. Uh first we took all of",
      "offset": 375.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "our queries uh and we along with the",
      "offset": 378,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "taxonomy if we fed it into an LLM and",
      "offset": 380.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "asked it to predict the most relevant",
      "offset": 382.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "categories for that query. Now the",
      "offset": 384.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "output that came back was decent.",
      "offset": 387.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Actually when we all looked at it it",
      "offset": 389.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "made a lot of sense. Uh but when we",
      "offset": 391.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "actually ran an online AB test the",
      "offset": 393.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "results weren't as great. Uh and one",
      "offset": 395.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "particular example that illustrates this",
      "offset": 397.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "point very well is a query like protein.",
      "offset": 399.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Uh uh users that come to Instacart when",
      "offset": 402.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "they type something like protein,",
      "offset": 405.36,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "they're looking for maybe protein",
      "offset": 406.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "shakes, uh protein bars or other protein",
      "offset": 408.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "supplements. The LLM on the other hand",
      "offset": 410.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "thinks that pro when a user types",
      "offset": 413.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "protein, they're looking for maybe",
      "offset": 415.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "chicken, tofu or other protein foods. So",
      "offset": 417.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "this mismatch wherein the LLM doesn't",
      "offset": 420.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "truly understand Instacart user behavior",
      "offset": 422.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "was really the cause of the problem.",
      "offset": 425.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "So to sort of maybe improve our results,",
      "offset": 428.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "we sort of switch the problem around, we",
      "offset": 431.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "took the most commonly converting",
      "offset": 433.919,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "categories or the top K converting",
      "offset": 436.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "categories for each query and fed that",
      "offset": 437.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "as additional context to the LLN. Um,",
      "offset": 439.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and then I'm sort of simplifying this a",
      "offset": 442.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "bit. there's a bunch of uh ranking and",
      "offset": 444.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "downstream validation that happens. But",
      "offset": 446.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "essentially that that was what we did.",
      "offset": 449.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "We generated a bunch of candidates uh",
      "offset": 451.199,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "rank candidates and this greatly",
      "offset": 453.039,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "simplified the problem for the LLM as",
      "offset": 455.919,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "well. Uh and again to illustrate this",
      "offset": 457.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "with an example uh take a query like",
      "offset": 459.759,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Wner soda. Uh our previous model",
      "offset": 462,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "actually identified this as a as a brand",
      "offset": 464.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of fruit fl or a fruit flavored soda",
      "offset": 467.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "which is not incorrect uh but it's not",
      "offset": 469.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "very precise either. Now the LLM did a",
      "offset": 471.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "much better job. It identified it as a",
      "offset": 474.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "brand of ginger ale. And with this our",
      "offset": 476.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "downstream retrieval and ranking",
      "offset": 479.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "improved greatly as well. And as you can",
      "offset": 481.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "see from uh the results below uh",
      "offset": 483.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "especially for tail queries we saw a big",
      "offset": 486.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "improvement. Our precision improved by",
      "offset": 488.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "our 18 percentage points and our recall",
      "offset": 490.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "improved by our 70 percentage points",
      "offset": 492.479,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "which is actually pretty significant for",
      "offset": 494.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "our tail queries. Um and maybe to very",
      "offset": 495.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "briefly look at our prompt. As you can",
      "offset": 499.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "see it's very simple. uh we are",
      "offset": 500.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "essentially passing in the C the top",
      "offset": 503.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "converted categories as context uh there",
      "offset": 505.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "are a bunch of guidelines about what the",
      "offset": 507.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "LLM should actually outd do and and",
      "offset": 509.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "that's it so this was all that is needed",
      "offset": 511.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to sort of enable this uh again I'm",
      "offset": 514.399,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "simplifying the overall flow but uh the",
      "offset": 516.64,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "general concepts are pretty",
      "offset": 519.599,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "straightforward",
      "offset": 521.519,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so coming to the another model uh the",
      "offset": 523.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "query rewrites model is actually pretty",
      "offset": 526.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "important as well uh from uh a from an",
      "offset": 528.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "e-commerce perspective, especially at",
      "offset": 531.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Instacart because not all retailers are",
      "offset": 533.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "created equal. Some have large",
      "offset": 536.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "cataloges, some have very small",
      "offset": 537.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "cataloges. The same query may not always",
      "offset": 539.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "return results. And that is where a",
      "offset": 541.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "rewrite is really helpful. For example,",
      "offset": 543.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "going from a query like 1% milk to just",
      "offset": 545.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "milk would at least return results that",
      "offset": 547.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the customer can decide to buy or not.",
      "offset": 550.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Uh and again, our previous approach",
      "offset": 552.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "which was trained on uh engagement data",
      "offset": 555.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "didn't do too well. it suffered or it",
      "offset": 557.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "did decently well on head and torso",
      "offset": 560.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "queries but it suffered from a lack of",
      "offset": 562,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "engagement data on tail queries. Uh so",
      "offset": 564.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "by using an LLM similar to how we did",
      "offset": 566.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for the product category classifier uh",
      "offset": 568.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we were able to generate very precise",
      "offset": 571.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "rewrites. Uh in the example here you can",
      "offset": 573.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "see that there's a a substitute a broad",
      "offset": 576.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and a synonymous rewrite. So for the",
      "offset": 578.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "case of avocado oil, a substitute is",
      "offset": 580.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "olive oil. A broader rewrite is um",
      "offset": 582.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "healthy cooking oil and a synonymous",
      "offset": 585.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "rewrite is just avocado extract. And",
      "offset": 587.279,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "again uh just just looking at the",
      "offset": 591.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "results from this and if you we saw a",
      "offset": 593.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "bunch of offline improvements and just",
      "offset": 596,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "moving from uh from using third party",
      "offset": 598.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "LLMs here just going from more simpler",
      "offset": 601.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "models to better models improved uh the",
      "offset": 603.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "results quite a bit. This is based off",
      "offset": 607.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of our human evaluation data. Uh so as",
      "offset": 608.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "you can see just improving the models",
      "offset": 611.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "itself improved the overall performance",
      "offset": 613.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "of the task and in terms of online",
      "offset": 615.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "improvements we actually saw a large",
      "offset": 617.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "drop in the number of queries without",
      "offset": 619.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "any results. This is pretty significant",
      "offset": 622.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "again because uh we could now actually",
      "offset": 624.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "show results to users where they",
      "offset": 627.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "previously saw empty results uh which",
      "offset": 629.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "was great for the business.",
      "offset": 631.519,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "So uh coming to the sort sort of the",
      "offset": 634.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "important part of this which is how we",
      "offset": 637.519,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "actually scored and served the the data",
      "offset": 639.12,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "the thing is that Instacart has a pre",
      "offset": 643.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "has a pretty idiosyncratic",
      "offset": 646.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "u query pattern. There's a very fat head",
      "offset": 648.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and torso set of queries and we have a",
      "offset": 650.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "sort of a long tail. So by comput",
      "offset": 653.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "premputing the outputs for for all of",
      "offset": 655.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the head and torso queries offline in a",
      "offset": 659.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "batch mode we were able to sort of uh",
      "offset": 661.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "cache all of this data and then at",
      "offset": 664.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "online when a query comes in we could",
      "offset": 666.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just serve it off of the cache with very",
      "offset": 668.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "low impact on latency uh and fall back",
      "offset": 671.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to our existing models for the long tail",
      "offset": 674.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of queries.",
      "offset": 676,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "And again, this worked really well",
      "offset": 677.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "because it didn't uh impact our latency",
      "offset": 679.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "while it greatly improved our coverage",
      "offset": 683.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "for the long tail of queries. Now, for",
      "offset": 684.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the the really long tail where I said we",
      "offset": 687.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "would fall back to our existing models,",
      "offset": 689.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "we're actually trying to replace them",
      "offset": 692.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "with a distilled lama model uh so that",
      "offset": 693.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "we can actually do a much better job",
      "offset": 698,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "compared to the existing models. Um so",
      "offset": 699.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "yeah to sort of summarize uh essentially",
      "offset": 702.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "what we saw was that uh from a query",
      "offset": 705.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "understanding perspective we have a",
      "offset": 707.279,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "bunch of models uh and just using our",
      "offset": 709.279,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "hybrid approach greatly improved their",
      "offset": 712.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "performance but what's actually more",
      "offset": 715.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "interesting is that today query",
      "offset": 716.959,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "understanding consists of a bunch of",
      "offset": 718.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "models and as Yazu was talking about in",
      "offset": 720.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the Netflix talk managing all of these",
      "offset": 722.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "models is actually complex from a system",
      "offset": 725.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "perspective so uh consolidating all of",
      "offset": 727.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "these into an SLM or a or maybe a large",
      "offset": 730.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "language model uh can make the results a",
      "offset": 733.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "lot more consistent. And I'll finish it",
      "offset": 736.32,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "off by giving an example here. Uh",
      "offset": 738.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "there's a query hum that we sort of saw",
      "offset": 741.519,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "some interesting issues with uh which is",
      "offset": 744.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "which is spelled hmm.",
      "offset": 746.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Uh the actual query the our our query",
      "offset": 749.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "brand tagger identified the brand",
      "offset": 752.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "correctly as a brand of kombucha but",
      "offset": 754.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "then our spell corrector unfortunately",
      "offset": 757.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "corrected it as hummus. So the results",
      "offset": 759.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "were really confusing to users uh and",
      "offset": 762,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "was pretty bad but by using a more",
      "offset": 764.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "unified model I think the results were",
      "offset": 766.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "much better. The second is by passing in",
      "offset": 768.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "by using an LLM for query understanding",
      "offset": 771.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "uh we can actually pass in extra",
      "offset": 774.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "context. Um so instead of just",
      "offset": 775.519,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "generating results for that query in",
      "offset": 778.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "isolation, we can really try to",
      "offset": 780.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "understand what the customer's mission",
      "offset": 783.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is. Um so for example, detect if they're",
      "offset": 785.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "actually here to buy ingredients for a",
      "offset": 787.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "recipe, etc. And then generate the",
      "offset": 789.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "content for that. So to talk more about",
      "offset": 792.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that, uh I have the here.",
      "offset": 794.079,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Thank you, Anish. Uh now I'll quickly",
      "offset": 797.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "talk about how we used LLMs for showing",
      "offset": 800,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "more discovery oriented content in",
      "offset": 802,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "search results page. Uh just to re",
      "offset": 803.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "restate the problem. Uh, our users found",
      "offset": 806.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that while our search engine was very",
      "offset": 808.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "good at showing exactly the the results",
      "offset": 810.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that they exactly wanted to see, once",
      "offset": 812.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "they added an item to the cart, they",
      "offset": 814.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "couldn't do anything useful with the",
      "offset": 816.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "search results page. They either had to",
      "offset": 817.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "do like another search or go to another",
      "offset": 819.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "page to fulfill their next intent to",
      "offset": 821.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "some starts. Uh, tradition solving this",
      "offset": 823.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "with traditional methods would require",
      "offset": 825.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like a lot of feature engineering or",
      "offset": 827.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "manual work. Uh, LLM solved this problem",
      "offset": 829.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "for us and I will talk about how. Uh so",
      "offset": 831.519,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this is how it looked in the end. So for",
      "offset": 834.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "queries like swordfish uh let's say",
      "offset": 836.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "there are no exact results we used llms",
      "offset": 838.639,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to generate substitute results like",
      "offset": 841.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "other seafood alternatives meaty fish",
      "offset": 842.959,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "like tilapia and whatnot u and similarly",
      "offset": 845.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "for queries like sushi where there were",
      "offset": 848.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "a lot of exact results let's say uh we",
      "offset": 850.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "would show at the bottom of the search",
      "offset": 853.279,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "results page we would show things like",
      "offset": 854.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Asian cooking cooking ingredients or",
      "offset": 856.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Japanese drinks and so on uh in order to",
      "offset": 858.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like you know get the users to engage.",
      "offset": 860.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "uh I'll talk about the techniques here",
      "offset": 863.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "but uh both of these uh both of these",
      "offset": 864.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "discovery oriented results we saw like",
      "offset": 868.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "improve uh led to like improvement in",
      "offset": 870.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "engagement as well as improvement in",
      "offset": 872.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "revenue uh for our per each search. Uh",
      "offset": 874,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "cool uh like I said I'll get into the",
      "offset": 877.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "techniques but let's first talk about",
      "offset": 879.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the requirements to generate such",
      "offset": 881.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "content. Uh first uh obviously we wanted",
      "offset": 882.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to generate content that is incremented",
      "offset": 885.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "to the current solutions. We don't want",
      "offset": 886.639,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "duplicates to what we were already",
      "offset": 888,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "showing. And the second requirement and",
      "offset": 889.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the most important one is we wanted all",
      "offset": 891.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "of the LLM answers or or the generation",
      "offset": 894.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "to be aligned with Instacart's domain",
      "offset": 896.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "knowledge. What does this mean? So if a",
      "offset": 898.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "query if a user searches for a query",
      "offset": 900.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "called dishes, L&M should understand",
      "offset": 902.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that it refers to like cookware and not",
      "offset": 904.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "food. Uh and vice versa for a query like",
      "offset": 906.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Thanksgiving dishes, right? So with",
      "offset": 909.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these requirements in mind, we set up",
      "offset": 911.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "with we started with like a very basic",
      "offset": 913.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "generation approach. So what did we do?",
      "offset": 915.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "We took the query and we told the LLM,",
      "offset": 916.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "hey, you are an AI assistant and your",
      "offset": 918.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "job is to generate two shopping lists.",
      "offset": 921.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "One is a list of complimentary items and",
      "offset": 923.199,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "another is a list of like uh substitute",
      "offset": 925.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "items for a given query, right? Um",
      "offset": 927.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "looked good. Uh I mean like so so we saw",
      "offset": 930.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the results, they looked pretty good. Uh",
      "offset": 932.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "our PMs vetted everything. We looked at",
      "offset": 934.24,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "everything. uh and and like vines said",
      "offset": 936.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "in in like QUU we when we launched this",
      "offset": 939.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "to our users uh we saw that the results",
      "offset": 941.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "were good but users weren't engaging it",
      "offset": 944.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "as much as we would have liked it to so",
      "offset": 946.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "we went back to the drawing board and we",
      "offset": 948.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "were like we tried to analyze what was",
      "offset": 950.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "going on and what we realized quickly",
      "offset": 952,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "was while LLM's answers uh were like",
      "offset": 954.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "common sense like answers and so on and",
      "offset": 957.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "such they weren't really what users were",
      "offset": 959.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "looking for uh taking the protein",
      "offset": 961.519,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "example again like uh users when they",
      "offset": 963.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "search for protein They look for protein",
      "offset": 965.759,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "bars and protein shakes rather than what",
      "offset": 967.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "LLM would give us an answer which is",
      "offset": 970.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "chicken, turkey and tofu and whatnot.",
      "offset": 972.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Right? So uh so what we did was we",
      "offset": 974.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "augmented the prompt with Instacart",
      "offset": 978,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "domain knowledge. So uh in one case what",
      "offset": 980.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we did was we took the query and then we",
      "offset": 982.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "augmented it with like here here is the",
      "offset": 984.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "query and here are the top converting",
      "offset": 986.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "categories uh for this particular query",
      "offset": 988.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "along with any annotations from the",
      "offset": 990.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "query understanding model like hey here",
      "offset": 992.639,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "is a brand present in the query here is",
      "offset": 994.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like a uh dietary attribute present in",
      "offset": 995.759,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the query and so on as such. Uh in",
      "offset": 998.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "another case we were like here is the",
      "offset": 1000.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "query and here are the subsequent",
      "offset": 1002,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "queries that users did once they issued",
      "offset": 1003.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "this particular query. So once you",
      "offset": 1006.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "augmented the prompt with this",
      "offset": 1008.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "additional metadata about how Instacart",
      "offset": 1010,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "users behave, the the the results were",
      "offset": 1012,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "far more better. I don't have the time",
      "offset": 1014.959,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to show like the before and after, but",
      "offset": 1016.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like I said, we definitely saw like a",
      "offset": 1019.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "huge improvement in both engagement as",
      "offset": 1020.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "well as revenue. Uh I'll quickly talk",
      "offset": 1023.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "about like how we served uh all of these",
      "offset": 1026.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "contents uh like very similar to QU.",
      "offset": 1028.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "It's impractical to call the LLM in real",
      "offset": 1030.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "time because of latency and maybe cost",
      "offset": 1033.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "concerns sometimes. So what we did was",
      "offset": 1034.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "uh we took all of our uh historical",
      "offset": 1037.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "search logs. We called LLM in like a",
      "offset": 1040.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "batch mode and stored everything. So",
      "offset": 1042.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "query content metadata along with even",
      "offset": 1044.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the products that could potentially show",
      "offset": 1046.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "up in the carousel and online it's just",
      "offset": 1048.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "a very quick look up from a feature",
      "offset": 1051.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "store. Uh and that's how we were able to",
      "offset": 1053.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like uh serve all of these",
      "offset": 1055.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "recommendations in like blazing fast",
      "offset": 1057.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "time. Uh again things weren't as simple",
      "offset": 1058.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "as as we making them out to be. the the",
      "offset": 1061.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like when I said the overall concept is",
      "offset": 1064.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "simple. The the prompt itself is very",
      "offset": 1066,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "simple but there were three key",
      "offset": 1067.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "challenges that we solved along the way.",
      "offset": 1069.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Uh one is aligning generation with",
      "offset": 1071.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "business metrics like revenue. Uh this",
      "offset": 1073.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "was very important to select topline",
      "offset": 1075.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "bins. So we iterated over the prompts",
      "offset": 1077.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and the kind of metadata that we that we",
      "offset": 1079.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "would feed to the LLM in order to",
      "offset": 1081.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "achieve this. Second, we spent a lot of",
      "offset": 1083.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "time on ranking uh on improv improving",
      "offset": 1086.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the ranking of the content itself and so",
      "offset": 1089.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "on as such. So our traditional PCTR PCBR",
      "offset": 1090.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "models did not work. So we had to like",
      "offset": 1093.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "employ strategies like uh diversity",
      "offset": 1095.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "based ranking and so on and so forth to",
      "offset": 1097.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "get users to engage with the content. Uh",
      "offset": 1099.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and then the third thing is evaluating",
      "offset": 1101.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the content itself. So one is making",
      "offset": 1103.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sure that hey whatever LLM is giving uh",
      "offset": 1105.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is one right it's not hallucinating",
      "offset": 1108.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "something uh and second it adhered to",
      "offset": 1109.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "like what Instacart or what we need as a",
      "offset": 1112.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "product right cool. Uh so summarizing",
      "offset": 1114.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the the key takeaways from our talk uh",
      "offset": 1117.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "LLM's world knowledge was super",
      "offset": 1119.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "important uh to improve uh query",
      "offset": 1121.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "understanding predictions for especially",
      "offset": 1124.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "for the tail queries. Uh while LLMs were",
      "offset": 1125.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "super helpful we really found success by",
      "offset": 1128.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "combining the domain knowledge of",
      "offset": 1131.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Instacart with LLMs uh in order to see",
      "offset": 1133.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the topline wins that we saw. Uh and the",
      "offset": 1135.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "third and the last one is evaluating the",
      "offset": 1138.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "content as well as the cure predictions",
      "offset": 1140.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and so on as such was far more important",
      "offset": 1142.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and far more difficult uh than we",
      "offset": 1145.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "anticipated. We used LLM as a judge in",
      "offset": 1147.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "order to make this happen but very very",
      "offset": 1149.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "important step and we realized that kind",
      "offset": 1151.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of late. So yeah that's all from us.",
      "offset": 1153.039,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "We'll take questions now.",
      "offset": 1155.6,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "Thank you Jri Vines. Um yeah we'll take",
      "offset": 1159.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "questions at the mic while the next",
      "offset": 1162,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "speaker gets set up. Hi uh thanks for",
      "offset": 1163.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the talk. Um have you also been trying",
      "offset": 1165.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "around queries which are very long in",
      "offset": 1168.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "natural language like uh I want these",
      "offset": 1171.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "three items and these five items like",
      "offset": 1174.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "what we would do it on CH chat GPT or",
      "offset": 1176,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it's still like single item that's the",
      "offset": 1178.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "focus.",
      "offset": 1180.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Uh yeah I think we have we have actually",
      "offset": 1183.039,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "launched something in the past uh like",
      "offset": 1186.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "ask Instacart if you heard of it which",
      "offset": 1188.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "essentially takes natural language",
      "offset": 1191.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "queries and tries to map that to search",
      "offset": 1192.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "intent. So, for example, you might ask,",
      "offset": 1194.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "you might say healthy foods for a",
      "offset": 1197.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "three-year-old baby or something like",
      "offset": 1199.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that. And so, that would map to things",
      "offset": 1200.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "like fruit slices. Uh, I don't know if",
      "offset": 1202.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "three-year-old toddlers can eat popcorn,",
      "offset": 1205.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but something along those lines. And and",
      "offset": 1207.44,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "then we had our usual ranking recall and",
      "offset": 1210.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "ranking stack sort of retrieve those",
      "offset": 1213.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "results. So, any learnings from that",
      "offset": 1214.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "experiment for you? Yeah, so so I think",
      "offset": 1217.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "we actually have a lot of learnings from",
      "offset": 1220.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that. Essentially",
      "offset": 1221.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "as this we already mentioned uh we need",
      "offset": 1223.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to inject a lot of Instacart context",
      "offset": 1225.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "into the model to be able to get decent",
      "offset": 1228,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "results. The evaluation part is really",
      "offset": 1230.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "key. Uh so having a robust automated",
      "offset": 1233.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "evaluation pipeline was important. And",
      "offset": 1235.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "lastly passing context that is for",
      "offset": 1238.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "example if it's a",
      "offset": 1240.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "let's say it's a mother's day query and",
      "offset": 1242.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "let's say we come up with the individual",
      "offset": 1244.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "search intents as perfumes you really",
      "offset": 1247.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "want women's perfumes to be in there",
      "offset": 1250.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "whereas when we just had perfumes we",
      "offset": 1252.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "could see all kinds of items so passing",
      "offset": 1254,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that context from the LLM to the",
      "offset": 1256.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "downstream systems is really important.",
      "offset": 1258.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Thanks. Yeah we have a lot of examples",
      "offset": 1260.64,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "where we failed. We can talk about",
      "offset": 1262.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1268.69,
      "duration": 2.71
    }
  ],
  "cleanText": "[Music]\nHi, good afternoon everyone. My name is Vinesh, and he's the... We are part of the search and machine learning team at Instacart. So today we'd like to talk to you about how we are using LLMs to transform our search and discovery.\n\nUm, so yeah, so first a little bit about ourselves. Yeah, as I mentioned, we are part of the search and discovery ML team at Instacart. And for those of you who may not be part... who may not be familiar with Instacart, it's the leader in online grocery in North America. Uh, and our mission is to create a world where everyone has access to the food they love and more time to enjoy it together.\n\nSo coming to what we'll actually talk about today. Uh, first we'll talk about the importance of search in grocery e-commerce. Uh, then we'll look into some of the challenges facing conventional search engines. Uh, and then actually get to the meat of the talk today, which is how we are using LLMs to solve some of these problems. Uh, finally, we'll finish with some key takeaways from today's talk.\n\nSo coming to the importance of search and grocery commerce. Uh, I think we've all gone grocery shopping. Customers come with long shopping lists. Uh, and it's the same on the platform as well. People are looking for tens of items. Uh, and of these, a majority of them are just restocking purchases, that is, things that the customer has bought in the past. Uh, and the remaining are items that the user is trying out for the first time. So, uh, and a majority of these purchases come from search. So, uh, search has a dual role. It needs to both support quick and efficient... uh, it needs to have the customer quickly and efficiently find the product they're looking for and also enable this new product discovery. Uh, and new product discovery isn't just important for the customer. It's also great for our advertisers because it helps them showcase new products. Uh, and it's also good for the platform because overall it encourages larger basket sizes. Uh, so let's see what some problems are with our existing setup that sort of uh, makes this hard. Uh, so to begin with, uh, we have two classes of queries that are generally more challenging, especially from an e-commerce perspective. Uh, the first are overly broad queries, uh, in this case, like on the left, the snacks query, where there are tons of products that map to that query. Uh, and now because our models are trained on engagement data, if we aren't exposing these products uh, to the user, it's hard to actually collect engagement data to them, rank them up high. So the traditional cold start problem in a way. Uh, then, uh, as you can see on the query on the right, we have very specific queries like unsweetened plant-based yogurt, where the user is looking for something very specific, and these queries uh, don't happen very frequently, which means that we just don't have enough engagement data to train the models on. Uh, and while we have uh, done quite a bit of work to sort of um, improve this, the challenge that we continually keep facing is that while recall improves, precision is still a challenge, especially in a prelim world. Uh, the next class of problems is how do we actually support that new item discovery as we spoke about. So when a customer walks into a grocery store, let's say into a pasta aisle, they might see new brands of pasta that they would want to try out. Uh, along with that, they would also see pasta sauce and every other thing that's needed to make a bowl of pasta. Uh, and customers would want a similar experience on our site. Uh, we have heard multiple feedback, multiple rounds of feedback from our customers that, hey, I can find the product that I'm... that I want via search, but when I'm trying to find any other related products, it's... it's a bit of a dead end. I would need to make multiple searches to get to where I want to. So this was a problem that we wanted to solve as well. And yeah, as I mentioned, prel, this was a hard problem because of the lack of engagement data, etc. So let's see how we actually use LLMs to sort of solve these problems. Uh, I'll sort of talk specifically about how we use LLMs to uplevel our query understanding module. Now query understanding, as I'm sure most of you know, uh, is the most upstream part of the search stack. uh, and very accurate outputs are needed to sort of enable better retrieval and recall uh, and finally improve our ranking results. Uh, so our query understanding module has multiple models in them, like query normalization, query tagging, query classification, category classification, etc. So in the interest of time, uh, I'll just pick a couple of models and talk about how we sort of really improved them. Uh, the first is our query to category, a product category classifier. Uh, essentially we are taking a query and mapping it to a category in our taxonomy. Uh, so as an example, if you take a query like watermelon, that maps to categories like fruits, organic fruit, foods, etc. Uh, and our taxonomy has about 10,000 labels, of it, 6,000 are more commonly used. So because a product... a query can map to multiple labels, this is essentially a multilabel classification problem. Um, and in the past, our traditional models, uh, which were... we actually had it a couple of different models. One was a fast text-based uh, neural network, which essentially modeled the semantic relationship between the query and the category. Uh, and then as a fallback, we had an NPMI model, which was a statistical co-occurrence model between the query and the category. Now, while these uh, techniques work great for the... for the head and torso queries, we had really low coverage for our tail queries because again, we just didn't have enough engagement data to train the models on. Um, and to be honest, we actually tried more sophisticated bird-based models as well. Uh, and while we did see some improvement, the lack of engagement data meant that for the increased latency, we didn't see the wins that we actually hoped for.\n\nSo, um, so this is where we actually tried to use an LLM. Uh, first we took all of our queries, uh, and we, along with the taxonomy, if we fed it into an LLM and asked it to predict the most relevant categories for that query. Now the output that came back was decent. Actually, when we all looked at it, it made a lot of sense. Uh, but when we actually ran an online AB test, the results weren't as great. Uh, and one particular example that illustrates this point very well is a query like protein. Uh, uh, users that come to Instacart, when they type something like protein, they're looking for maybe protein shakes, uh, protein bars, or other protein supplements. The LLM, on the other hand, thinks that pro... when a user types protein, they're looking for maybe chicken, tofu, or other protein foods. So this mismatch, wherein the LLM doesn't truly understand Instacart user behavior, was really the cause of the problem.\n\nSo to sort of maybe improve our results, we sort of switch the problem around. We took the most commonly converting categories or the top K converting categories for each query and fed that as additional context to the LLM. Um, and then I'm sort of simplifying this a bit. There's a bunch of uh, ranking and downstream validation that happens. But essentially, that that was what we did. We generated a bunch of candidates, uh, rank candidates, and this greatly simplified the problem for the LLM as well. Uh, and again, to illustrate this with an example, uh, take a query like Wner soda. Uh, our previous model actually identified this as a... as a brand of fruit fl... or a fruit-flavored soda, which is not incorrect, uh, but it's not very precise either. Now the LLM did a much better job. It identified it as a brand of ginger ale. And with this, our downstream retrieval and ranking improved greatly as well. And as you can see from uh, the results below, uh, especially for tail queries, we saw a big improvement. Our precision improved by our 18 percentage points, and our recall improved by our 70 percentage points, which is actually pretty significant for our tail queries. Um, and maybe to very briefly look at our prompt. As you can see, it's very simple. Uh, we are essentially passing in the C... the top converted categories as context. Uh, there are a bunch of guidelines about what the LLM should actually outd... do, and and that's it. So this was all that is needed to sort of enable this. Uh, again, I'm simplifying the overall flow, but uh, the general concepts are pretty straightforward.\n\nSo coming to the another model, uh, the query rewrites model is actually pretty important as well, uh, from uh, a... from an e-commerce perspective, especially at Instacart, because not all retailers are created equal. Some have large catalogs, some have very small catalogs. The same query may not always return results. And that is where a rewrite is really helpful. For example, going from a query like 1% milk to just milk would at least return results that the customer can decide to buy or not. Uh, and again, our previous approach, which was trained on uh, engagement data, didn't do too well. It suffered or it did decently well on head and torso queries, but it suffered from a lack of engagement data on tail queries. Uh, so by using an LLM, similar to how we did for the product category classifier, uh, we were able to generate very precise rewrites. Uh, in the example here, you can see that there's a substitute, a broad, and a synonymous rewrite. So for the case of avocado oil, a substitute is olive oil. A broader rewrite is um, healthy cooking oil, and a synonymous rewrite is just avocado extract. And again, uh, just... just looking at the results from this, and if you... we saw a bunch of offline improvements, and just moving from uh, from using third-party LLMs here, just going from more simpler models to better models improved uh, the results quite a bit. This is based off of our human evaluation data. Uh, so as you can see, just improving the models itself improved the overall performance of the task, and in terms of online improvements, we actually saw a large drop in the number of queries without any results. This is pretty significant again, because uh, we could now actually show results to users where they previously saw empty results, uh, which was great for the business.\n\nSo, uh, coming to the sort... sort of the important part of this, which is how we actually scored and served the... the data. The thing is that Instacart has a pre... has a pretty idiosyncratic... u... query pattern. There's a very fat head and torso set of queries, and we have a sort of a long tail. So by comput... precomputing the outputs for... for all of the head and torso queries offline in a batch mode, we were able to sort of uh, cache all of this data, and then at online, when a query comes in, we could just serve it off of the cache with very low impact on latency, uh, and fall back to our existing models for the long tail of queries. And again, this worked really well because it didn't uh, impact our latency, while it greatly improved our coverage for the long tail of queries. Now, for the the really long tail, where I said we would fall back to our existing models, we're actually trying to replace them with a distilled Lama model, uh, so that we can actually do a much better job compared to the existing models. Um, so yeah, to sort of summarize, uh, essentially what we saw was that uh, from a query understanding perspective, we have a bunch of models, uh, and just using our hybrid approach greatly improved their performance, but what's actually more interesting is that today query understanding consists of a bunch of models, and as Yazu was talking about in the Netflix talk, managing all of these models is actually complex from a system perspective. So uh, consolidating all of these into an SLM or a... or maybe a large language model uh, can make the results a lot more consistent. And I'll finish it off by giving an example here. Uh, there's a query hum that we sort of saw some interesting issues with, uh, which is... which is spelled hmm. Uh, the actual query... the our... our query brand tagger identified the brand correctly as a brand of kombucha, but then our spell corrector unfortunately corrected it as hummus. So the results were really confusing to users, uh, and was pretty bad, but by using a more unified model, I think the results were much better. The second is by passing in... by using an LLM for query understanding, uh, we can actually pass in extra context. Um, so instead of just generating results for that query in isolation, we can really try to understand what the customer's mission is. Um, so for example, detect if they're actually here to buy ingredients for a recipe, etc., and then generate the content for that. So to talk more about that, uh, I have the... here.\n\nThank you, Anish. Uh, now I'll quickly talk about how we used LLMs for showing more discovery-oriented content in search results page. Uh, just to restate the problem. Uh, our users found that while our search engine was very good at showing exactly the... the results that they exactly wanted to see, once they added an item to the cart, they couldn't do anything useful with the search results page. They either had to do like another search or go to another page to fulfill their next intent to some starts. Uh, tradition... solving this with traditional methods would require like a lot of feature engineering or manual work. Uh, LLM solved this problem for us, and I will talk about how. Uh, so this is how it looked in the end. So for queries like swordfish, uh, let's say there are no exact results, we used LLMs to generate substitute results like other seafood alternatives, meaty fish like tilapia and whatnot. u, and similarly for queries like sushi, where there were a lot of exact results, let's say, uh, we would show at the bottom of the search results page, we would show things like Asian cooking... cooking ingredients or Japanese drinks and so on, uh, in order to like, you know, get the users to engage. Uh, I'll talk about the techniques here, but uh, both of these uh, both of these discovery-oriented results, we saw like improve... uh, led to like improvement in engagement as well as improvement in revenue uh, for our per each search. Uh, cool. Uh, like I said, I'll get into the techniques, but let's first talk about the requirements to generate such content. Uh, first, uh, obviously we wanted to generate content that is incremented to the current solutions. We don't want duplicates to what we were already showing. And the second requirement, and the most important one, is we wanted all of the LLM answers or... or the generation to be aligned with Instacart's domain knowledge. What does this mean? So if a query... if a user searches for a query called dishes, L&M should understand that it refers to like cookware and not food. Uh, and vice versa for a query like Thanksgiving dishes, right? So with these requirements in mind, we set up with... we started with like a very basic generation approach. So what did we do? We took the query and we told the LLM, hey, you are an AI assistant, and your job is to generate two shopping lists. One is a list of complimentary items, and another is a list of like uh, substitute items for a given query, right? Um, looked good. Uh, I mean, like, so... so we saw the results, they looked pretty good. Uh, our PMs vetted everything. We looked at everything. uh, and and...\n\n\nLike Vinesh said,\n\nIn, in like QUU, we, when we launched this to our users, uh, we saw that the results were good, but users weren't engaging it as much as we would have liked it to. So we went back to the drawing board, and we were like, we tried to analyze what was going on, and what we realized quickly was, while LLMs' answers, uh, were like common sense like answers and so on and such, they weren't really what users were looking for. Uh, taking the protein example again, like, uh, users when they search for protein, they look for protein bars and protein shakes rather than what LLM would give us an answer, which is chicken, turkey, and tofu and whatnot. Right? So, uh, so what we did was we augmented the prompt with Instacart domain knowledge. So, uh, in one case, what we did was we took the query and then we augmented it with like, here, here is the query and here are the top converting categories, uh, for this particular query along with any annotations from the query understanding model, like, hey, here is a brand present in the query, here is like a, uh, dietary attribute present in the query and so on as such. Uh, in another case, we were like, here is the query and here are the subsequent queries that users did once they issued this particular query. So once you augmented the prompt with this additional metadata about how Instacart users behave, the the the results were far more better. I don't have the time to show like the before and after, but like I said, we definitely saw like a huge improvement in both engagement as well as revenue. Uh, I'll quickly talk about like how we served uh all of these contents uh like very similar to QU. It's impractical to call the LLM in real time because of latency and maybe cost concerns sometimes. So what we did was, uh, we took all of our, uh, historical search logs. We called LLM in like a batch mode and stored everything. So query content metadata along with even the products that could potentially show up in the carousel and online, it's just a very quick look up from a feature store. Uh, and that's how we were able to like, uh, serve all of these recommendations in like blazing fast time. Uh, again, things weren't as simple as as we making them out to be. The the like when I said the overall concept is simple. The the prompt itself is very simple, but there were three key challenges that we solved along the way. Uh, one is aligning generation with business metrics like revenue. Uh, this was very important to select topline bins. So we iterated over the prompts and the kind of metadata that we that we would feed to the LLM in order to achieve this. Second, we spent a lot of time on ranking, uh, on improving the ranking of the content itself and so on as such. So our traditional PCTR PCBR models did not work. So we had to like employ strategies like, uh, diversity based ranking and so on and so forth to get users to engage with the content. Uh, and then the third thing is evaluating the content itself. So one is making sure that, hey, whatever LLM is giving, uh, is one right, it's not hallucinating something, uh, and second, it adhered to like what Instacart or what we need as a product, right? Cool. Uh, so summarizing the the key takeaways from our talk, uh, LLM's world knowledge was super important, uh, to improve, uh, query understanding predictions for especially for the tail queries. Uh, while LLMs were super helpful, we really found success by combining the domain knowledge of Instacart with LLMs, uh, in order to see the topline wins that we saw. Uh, and the third and the last one is evaluating the content as well as the cure predictions and so on as such was far more important and far more difficult, uh, than we anticipated. We used LLM as a judge in order to make this happen, but very, very important step, and we realized that kind of late. So yeah, that's all from us. We'll take questions now.\nThank you Jri Vines. Um, yeah, we'll take questions at the mic while the next speaker gets set up. Hi, uh, thanks for the talk. Um, have you also been trying around queries which are very long in natural language, like, uh, I want these three items and these five items, like what we would do it on CH chat GPT or it's still like single item that's the focus?\nUh, yeah, I think we have, we have actually launched something in the past, uh, like ask Instacart, if you heard of it, which essentially takes natural language queries and tries to map that to search intent. So, for example, you might ask, you might say healthy foods for a three-year-old baby or something like that. And so, that would map to things like fruit slices. Uh, I don't know if three-year-old toddlers can eat popcorn, but something along those lines. And and then we had our usual ranking recall and ranking stack sort of retrieve those results. So, any learnings from that experiment for you? Yeah, so, so I think we actually have a lot of learnings from that. Essentially, as this, we already mentioned, uh, we need to inject a lot of Instacart context into the model to be able to get decent results. The evaluation part is really key. Uh, so having a robust automated evaluation pipeline was important. And lastly, passing context that is, for example, if it's a, let's say it's a mother's day query and let's say we come up with the individual search intents as perfumes, you really want women's perfumes to be in there, whereas when we just had perfumes, we could see all kinds of items, so passing that context from the LLM to the downstream systems is really important.\nThanks. Yeah, we have a lot of examples where we failed. We can talk about.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.398Z"
}