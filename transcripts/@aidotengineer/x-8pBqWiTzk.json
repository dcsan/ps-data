{
  "episodeId": "x-8pBqWiTzk",
  "channelSlug": "@aidotengineer",
  "title": "MCP: Origins and Requests For Startups â€” Theodora Chu, Model Context Protocol PM, Anthropic",
  "publishedAt": "2025-06-18T22:55:21.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1.72,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "right? Hello everyone. Who's excited to",
      "offset": 15.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "chat about MCP today?",
      "offset": 17.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Okay, we can we can work on that. We can",
      "offset": 20.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "get it a little bit better by the end of",
      "offset": 21.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this talk. Uh but I'm Theo. I am a",
      "offset": 23.039,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "product manager at Anthropic work on",
      "offset": 26.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "MCP. Uh prior to this was also a startup",
      "offset": 28.8,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "founder uh working in the AI space. Um",
      "offset": 32.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "couple fun facts about me because",
      "offset": 35.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "everyone says make yourself a little bit",
      "offset": 36.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "more personable. Uh is that I like",
      "offset": 38.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "playing poker mostly losing money at",
      "offset": 41.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "poker, not uh making money at poker. Uh",
      "offset": 43.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and I also really like coffee. So, uh,",
      "offset": 46.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "if you're, you know, a huge coffee fan,",
      "offset": 49.039,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "um, and want to talk about the best",
      "offset": 51.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "coffee in San Francisco, hit me up after",
      "offset": 54.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the talk. But you didn't come here to",
      "offset": 55.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "talk about me. You came here to learn",
      "offset": 58.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "about MCP. So, let's talk about MCP.",
      "offset": 60.48,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "I was told not to say MCP is the best",
      "offset": 65.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "thing since sliced bread. Uh, which I",
      "offset": 68.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "won't say, but mostly because I don't",
      "offset": 70.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually think it's the best thing since",
      "offset": 73.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "sliced bread. Uh my goal here today is",
      "offset": 74.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to really walk you through the origin",
      "offset": 77.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "story of MCP, why we launched it, uh",
      "offset": 79.28,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "give you a better sense of, you know,",
      "offset": 82.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "where it can actually help you in your",
      "offset": 85.439,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "workflow. Uh and then ultimately give",
      "offset": 87.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you a sense of the types of questions",
      "offset": 90.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that I'm frequently hearing, where I",
      "offset": 92.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "think there's a lot of value to build in",
      "offset": 95.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the ecosystem, and let you decide for",
      "offset": 96.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "yourself whether or not it is actually",
      "offset": 99.04,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "the best thing since sliced bread.",
      "offset": 100.72,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "So, scrolling all the way back to uh",
      "offset": 104.799,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "midl last year, the co-creators of MCP,",
      "offset": 108.32,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "David and Justin, had this idea. Uh they",
      "offset": 111.759,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "were seeing that, you know, classic two",
      "offset": 115.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "engineers in a garage style. They were",
      "offset": 117.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "seeing that they were constantly copying",
      "offset": 119.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and pasting context from outside of the",
      "offset": 122.079,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "context window into the context window.",
      "offset": 125.28,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "So, you're doing your workflow and",
      "offset": 127.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "suddenly you're remembering that there",
      "offset": 129.759,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "was a Slack message. which was really",
      "offset": 131.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "important that had a lot of context that",
      "offset": 132.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you could just copy in. Um, so you're",
      "offset": 134.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "constantly kind of copying things back",
      "offset": 136.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "and forth from Slack. Maybe you're",
      "offset": 137.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "copying things in from Sentry, your",
      "offset": 139.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "error logs. Uh, but they were kind of",
      "offset": 142,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "realizing, hey, it would be so great if",
      "offset": 144.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Claude or any LLM could just kind of",
      "offset": 147.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "climb out of its box, reach out into the",
      "offset": 151.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "real world and bring that context and",
      "offset": 153.28,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "those actions uh to the model. And so",
      "offset": 155.92,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "the genesis of MCP was really around",
      "offset": 158.879,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "this big question of uh not just context",
      "offset": 162.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "but model agency. How do you actually",
      "offset": 165.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "give the model the ability to interact",
      "offset": 168.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "with the outside world?",
      "offset": 171.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And so as they started thinking about",
      "offset": 173.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this uh they came to the conclusion that",
      "offset": 176.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it had to be an open-source standardized",
      "offset": 179.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "protocol in order for this to make sense",
      "offset": 182,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "uh at scale. And the reason is of course",
      "offset": 185.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "as you all know if you want to build an",
      "offset": 187.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "integration uh and the you know the the",
      "offset": 189.68,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "actor uh or the client in this case that",
      "offset": 193.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "has to uh leverage that integration is a",
      "offset": 196.319,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "is using a closed source ecosystem then",
      "offset": 199.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you need maybe a BD or partnerships uh",
      "offset": 201.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "angle with that client to actually get",
      "offset": 205.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "access to the team to integrate with",
      "offset": 207.68,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "them. You then have to align on the",
      "offset": 209.28,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "right interface and then you get to",
      "offset": 210.799,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "actually build the thing itself. Um and",
      "offset": 212.879,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "so the idea here was that model agency",
      "offset": 215.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "was the biggest thing that was stopping",
      "offset": 218.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "uh LLMs from actually reaching the next",
      "offset": 221.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "stage of usefulness and intelligence. As",
      "offset": 223.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we saw that reasoning models were",
      "offset": 226.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "becoming uh more and more the future",
      "offset": 228.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "that tool calling was getting better. We",
      "offset": 232,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "really wanted to make sure that we were",
      "offset": 234.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "making it possible for everyone to get",
      "offset": 236.159,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "involved in that ecosystem and actually",
      "offset": 238,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "allow uh the models to again have",
      "offset": 239.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "agency.",
      "offset": 243.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Uh so they form a small tiger team",
      "offset": 244.64,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "internally uh work on this protocol and",
      "offset": 247.68,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "launch it at our company hack week in uh",
      "offset": 252,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "November of last year. And this was",
      "offset": 254.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "really the first turning point of MCP.",
      "offset": 256.639,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "It went viral as you can imagine.",
      "offset": 258.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Engineers from various teams were",
      "offset": 260.959,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "working on building MCPs to automate",
      "offset": 264.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "their own workflows. They were working",
      "offset": 266.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "on MCPs to uh automate other teams",
      "offset": 267.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "workflows. Uh this was really kind of a",
      "offset": 270.479,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "cool moment to see how it went from",
      "offset": 272.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "again like two engineers in a garage all",
      "offset": 275.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "the way to uh this is a major moment in",
      "offset": 277.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "turning point where we think we actually",
      "offset": 280.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "unlocked some uh true value for for",
      "offset": 282.4,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "other people. And so we ultimately ended",
      "offset": 284.96,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "up open sourcing uh MCP in November of",
      "offset": 287.759,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "last year and that's when uh we",
      "offset": 290.479,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "introduced it to the rest of the world.",
      "offset": 292.88,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "But as most builders know, uh when you",
      "offset": 295.759,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "build something 0ero to one, you think",
      "offset": 298.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the launch moment is going to be really",
      "offset": 300.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "impactful. But it actually usually is",
      "offset": 302.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "not. Uh at launch, most people were",
      "offset": 304.96,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "saying things like, &quot;What's MCP?&quot; or",
      "offset": 308.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "even worse, or maybe, you know,",
      "offset": 311.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "rightfully so, what's MPC? Uh and more",
      "offset": 313.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "often than not, we got this question of",
      "offset": 317.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "I don't really understand why you need a",
      "offset": 319.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "new protocol. I don't really understand",
      "offset": 321.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "why it has to be open source. Camp",
      "offset": 323.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "models call tools already. Uh this was",
      "offset": 326,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the slew of questions that kind of came",
      "offset": 329.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "uh again and again for probably from the",
      "offset": 332.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "era of November all the way even to uh",
      "offset": 334.96,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "early uh early this year. And it really",
      "offset": 338,
      "duration": 9.039
    },
    {
      "lang": "en",
      "text": "took uh making it possible for builders",
      "offset": 342.16,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "to kind of get their hands dirty uh with",
      "offset": 347.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "building MCPS to automate their own",
      "offset": 349.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "workflow for for uh for this to take",
      "offset": 351.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "off. And so the next turning point uh as",
      "offset": 354.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Henry alluded to was when Cursor kind of",
      "offset": 357.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "adopted MCP and after that a lot of",
      "offset": 359.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "other coding tools also adopted MCP um",
      "offset": 362.4,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "VS Code uh source graph uh etc. But we",
      "offset": 365.6,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "had a lot of coding ideides um start",
      "offset": 370.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "adopting MCP and that's really where",
      "offset": 372.639,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that uh next stage of momentum came in",
      "offset": 374.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "where agent uh agency was given to",
      "offset": 378.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "builders to actually build uh MCPS for",
      "offset": 380.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "themselves",
      "offset": 382.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and more recently we've seen uh kind of",
      "offset": 384.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "another turning point where Google,",
      "offset": 387.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Microsoft, OpenAI uh and many others",
      "offset": 389.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "have uh also adopted MCP. So really",
      "offset": 391.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "excited to see this kind of become more",
      "offset": 394.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and more uh the standard. But ultimately",
      "offset": 396.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "uh standards uh become standards because",
      "offset": 399.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "they are actually useful to builders.",
      "offset": 402.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "And so uh I uh kind of want to ask all",
      "offset": 404.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "of you to to keep us honest. Um",
      "offset": 408.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "contribute when you see you know issues",
      "offset": 410.639,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "with uh the way that the the protocol is",
      "offset": 413.36,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "built today. uh or uh if you uh even",
      "offset": 415.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "want to take that one step further and",
      "offset": 419.599,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "submit a PR directly to the GitHub repo",
      "offset": 421.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and uh fix the issue that'd be even",
      "offset": 423.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "better. Um but our goal here is really",
      "offset": 425.44,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "to make it maximally useful for uh for",
      "offset": 427.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you all and for uh model providers. So",
      "offset": 430.479,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "uh thank you for for your help in even",
      "offset": 433.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "getting us to the point where I can be",
      "offset": 435.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "speaking on stage uh about this uh less",
      "offset": 437.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "than one year later.",
      "offset": 440.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "So just to get a little bit deeper into",
      "offset": 443.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "uh what we were solving for at the start",
      "offset": 446.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "of building MCP is again this kind of",
      "offset": 448.8,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "idea of of model agency. Um and part of",
      "offset": 452.08,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "that means uh agents is kind of the",
      "offset": 456.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "direction that that we think is is going",
      "offset": 458.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to be the future. That's no surprise to",
      "offset": 460.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "anyone in this room. You are probably",
      "offset": 462.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "going to hear the word agents said in",
      "offset": 464.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "every talk if not almost every talk. Uh",
      "offset": 467.599,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "but the way that we think about agents",
      "offset": 470.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "is that you are giving the model or",
      "offset": 474.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you're rather depending on the model's",
      "offset": 477.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "intelligence to choose actions and",
      "offset": 479.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "decide uh what to do. Uh in the same way",
      "offset": 481.599,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that you know maybe when you talk to a",
      "offset": 484.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "human and you ask them uh for a",
      "offset": 487.199,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "response, you don't know exactly what",
      "offset": 489.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the response is, but based on your",
      "offset": 490.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "understanding of maybe the task that",
      "offset": 492.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you've given them, your hope is that",
      "offset": 494.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they are going to give you the right",
      "offset": 496.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "response. And uh we want to kind of",
      "offset": 498.08,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "enable that world where you're uh uh",
      "offset": 500.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "depending on the model's intelligence",
      "offset": 503.599,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "scaling over time. So uh that leads to",
      "offset": 505.199,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "principles in how we actually build the",
      "offset": 510.479,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "protocol itself. Uh recently we uh",
      "offset": 512.64,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "launched the support for streamable HTTP",
      "offset": 516.08,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "which uh changes the the transport from",
      "offset": 518.399,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "SSE. uh and as you all might know",
      "offset": 521.599,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "streamable HTTP is is more the uh",
      "offset": 525.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "enables more birectionality and so that",
      "offset": 527.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "was uh a very controversial decision",
      "offset": 530.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "actually but uh if you're keeping agents",
      "offset": 532.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in mind as the future makes a lot of",
      "offset": 534.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "sense because you want to make sure that",
      "offset": 536.48,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "agents can kind of communicate with each",
      "offset": 537.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "other. The other thing that we believe",
      "offset": 539.279,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "uh is that there will be a lot more",
      "offset": 542.959,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "servers than there are clients. Uh this",
      "offset": 545.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we could be totally wrong on this. Uh I",
      "offset": 548.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "would love to see where the future plays",
      "offset": 551.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "out. But because we think that there",
      "offset": 553.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "will be a lot more servers than there",
      "offset": 555.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "are clients, uh we optimized for server",
      "offset": 557.36,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "simplicity and for the server uh server",
      "offset": 560.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "builders to have better tooling. And",
      "offset": 563.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that does mean when we have to make a",
      "offset": 565.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "trade-off between client complexity or",
      "offset": 567.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "server complexity, we tend to optimize",
      "offset": 569.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "for pushing the complexity down to the",
      "offset": 571.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "client. So apologize in advance to",
      "offset": 574,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "client builders. Uh but it was an",
      "offset": 576,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "intentional decision. again uh would",
      "offset": 578.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "would uh be curious to see if if this",
      "offset": 581.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "plays out uh the way that that we",
      "offset": 583.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thought it would.",
      "offset": 584.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So I'm going to speedun through uh some",
      "offset": 587.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "project updates mostly because other",
      "offset": 590.16,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "talks are going to go much more in",
      "offset": 592.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "detail here. Um but last six months we",
      "offset": 593.519,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "launched uh ability for uh folks to",
      "offset": 596.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "build remote MCPs.",
      "offset": 599.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "We fixed o",
      "offset": 601.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which we got wrong initially. Thank you.",
      "offset": 603.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Uh I know that was a huge huge thing",
      "offset": 606.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that that we got wrong initially, but it",
      "offset": 608.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is now fixed uh in the draft spec and so",
      "offset": 609.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "would love folks to you know continue",
      "offset": 612.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "helping to push on on these things that",
      "offset": 615.6,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "they see don't match their mental model.",
      "offset": 618.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Uh this was actually fixed via a series",
      "offset": 620.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "of of people from the community jumping",
      "offset": 624.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "in to work on saying hey this is how you",
      "offset": 626.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "know uh O works with identity providers",
      "offset": 629.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and here's how we can update the",
      "offset": 632.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "protocol. So very much a community uh",
      "offset": 633.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "community effort. Um again uh launched",
      "offset": 637.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "removable HTTP as the primary transport.",
      "offset": 640.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Uh and lastly made a couple of updates",
      "offset": 643.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh to developer experience um by",
      "offset": 645.839,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "updating our SDKs and also uh making",
      "offset": 648.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "updates to inspector which if you aren't",
      "offset": 651.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "familiar with is a really good uh",
      "offset": 653.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "debugging tool for for your server. I",
      "offset": 655.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "think it is probably our most",
      "offset": 657.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "underutilized uh tool.",
      "offset": 658.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Looking forward, we're going to be",
      "offset": 662.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "focusing a lot more on uh that agent",
      "offset": 663.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "experience. So, we just added",
      "offset": 666.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "elicitation uh to the draft spec. This",
      "offset": 669.12,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "uh allows servers to ask for more",
      "offset": 673.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "information from end users. So, you can",
      "offset": 677.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "imagine you're building a uh maybe",
      "offset": 678.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you're building a flight booking tool",
      "offset": 682.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "and uh the end user says, &quot;Hey, book me",
      "offset": 684.16,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "the best flight to Atlanta.&quot; And so as",
      "offset": 688,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the server you have a question which is",
      "offset": 690.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "what does best mean to you? Is it",
      "offset": 692.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "cheapest or is it fastest? So you ask",
      "offset": 694.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the end user uh and now you can pass",
      "offset": 696.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "through that elicitation. The end user",
      "offset": 699.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "can respond and have that response",
      "offset": 700.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "ultimately sent back to the server. Uh",
      "offset": 702.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "we are also making progress on the",
      "offset": 705.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "registry API which would make it a lot",
      "offset": 707.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "easier for models to actually find MCPS",
      "offset": 710,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "that weren't already given to them up",
      "offset": 714,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "front. So this is again kind of on that",
      "offset": 716.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "theme of model agency. Uh we're really",
      "offset": 718.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "betting on the intelligence of models",
      "offset": 720.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "going up over time.",
      "offset": 722.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Again working on uh developer",
      "offset": 724.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "experience. We've heard often from you",
      "offset": 727.12,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "all that there are uh that you know",
      "offset": 729.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you'd love to understand what kind of",
      "offset": 732.959,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the best patterns are in the ecosystem",
      "offset": 734.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "or what the standards are. And so we",
      "offset": 736.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "want to make sure that there are open",
      "offset": 739.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "source examples that uh that both we've",
      "offset": 740.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "contributed to and also the community",
      "offset": 743.279,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "can contribute to to kind of help build",
      "offset": 744.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "those standards and patterns together.",
      "offset": 746.399,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "And lastly uh we're making sure that MCP",
      "offset": 748.48,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "stays open uh forever and we are",
      "offset": 753.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "investing heavily in thinking about the",
      "offset": 756.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "next phase of governance. Uh so there",
      "offset": 758.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "will be more updates on that soon.",
      "offset": 761.04,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "And just to do a quick call out to uh",
      "offset": 764,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the graphic in in the bottom. So a lot",
      "offset": 766.959,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "of people have asked uh us what it looks",
      "offset": 770.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like to actually build an agent with",
      "offset": 773.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "MCP. Our take is that an agent really",
      "offset": 774.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "is, you know, just a server acting as a",
      "offset": 778,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "client and vice versa. Uh where you can",
      "offset": 780.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "then kind of chat back and forth with",
      "offset": 782.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "other agents, uh other servers, other",
      "offset": 785.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "clients. Um so I won't go into too much",
      "offset": 787.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "detail there. I know a lot of other",
      "offset": 789.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "people are going to be uh talking about",
      "offset": 791.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "agents in more detail, but just wanted",
      "offset": 793.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to make sure that uh I call that out",
      "offset": 796.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "here.",
      "offset": 798.88,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "So, the uh thing that everyone has",
      "offset": 801.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "probably been waiting for and that I've",
      "offset": 805.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "been told uh over and over again when",
      "offset": 807.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "when I talk to founders uh what they're",
      "offset": 809.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "asking me about is uh what should I",
      "offset": 811.519,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "build in this space? you know if uh MCP",
      "offset": 814.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "becomes a standard what is where are the",
      "offset": 817.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "interesting opportunities so before",
      "offset": 819.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "jumping into this the first thing I'll",
      "offset": 822.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "say is that we are really early right",
      "offset": 823.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "now and that means that even if the",
      "offset": 826.959,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "standard exists we still need the",
      "offset": 829.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "ecosystem to be filled out and I uh",
      "offset": 831.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "would urge you to build more and more",
      "offset": 835.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and more servers if I had to put a",
      "offset": 836.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "waiting on these three bullet points I",
      "offset": 838.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "would put 80% on the first one 10% on",
      "offset": 840.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the second one and 10% on the third one",
      "offset": 842.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Um so we have a lot of opportunity to",
      "offset": 844.88,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "build a lot more servers uh that are",
      "offset": 847.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "higher quality uh and for different",
      "offset": 850.639,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "verticals. Um and just to touch quickly",
      "offset": 853.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "on what I mean by higher quality. Uh a",
      "offset": 856.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "lot of people you know maybe hot take",
      "offset": 860.24,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "but I think a lot of people are wrapping",
      "offset": 862,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "their API endpoints one to one and just",
      "offset": 863.519,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "exposing that as tools. I don't think",
      "offset": 866.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that's the right way to build an MCP",
      "offset": 868.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "server. That in and of itself could",
      "offset": 870.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "probably be a 20-minute talk. Uh but",
      "offset": 872,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "what you really have to remember when",
      "offset": 875.76,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "you're building a server is that you",
      "offset": 877.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have three users. You have the end user,",
      "offset": 878.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the client developer, and the model. So",
      "offset": 880.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "a lot of people forget that the model is",
      "offset": 883.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "a user here as well. You want to uh just",
      "offset": 885.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "as you would for API design, you want to",
      "offset": 887.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "think about what are the use cases that",
      "offset": 889.76,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "your end users are going to have. What",
      "offset": 891.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "are the prompts that they might actually",
      "offset": 892.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "be uh putting into the the model? and",
      "offset": 894.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "ultimately what are the tools that you",
      "offset": 897.839,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "then need to expose to the model to",
      "offset": 899.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "enable the model to respond correctly to",
      "offset": 901.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "those uh to those prompts. So uh higher",
      "offset": 903.68,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "quality servers uh and also servers for",
      "offset": 907.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "different verticals. A lot of the",
      "offset": 910.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "servers today um have been for dev",
      "offset": 911.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "tools. We would love to see uh this",
      "offset": 914.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "expand to be useful beyond engineers",
      "offset": 917.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "into verticals like sales, finance,",
      "offset": 920.079,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "legal, education, pick your poison, uh",
      "offset": 922.16,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "whatever you know best. um that uh we we",
      "offset": 925.199,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "would just love to see more servers. The",
      "offset": 928.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "next piece is on simplifying server",
      "offset": 931.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "building. So again, as I mentioned, we",
      "offset": 933.68,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "believe strongly that uh servers are",
      "offset": 936.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "going to be the vast majority of the",
      "offset": 939.199,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "ecosystem. There will of course be a lot",
      "offset": 940.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of clients as well, but we think the uh",
      "offset": 942.959,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "order of magnitude of of servers is",
      "offset": 945.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going to uh outweigh the order of",
      "offset": 947.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "magnitude of clients. And so would love",
      "offset": 948.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to see a lot more tooling to actually",
      "offset": 951.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "make it easier and easier to build",
      "offset": 952.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "servers. um both for enterprises uh that",
      "offset": 954.56,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "are deploying MCPs internally uh as",
      "offset": 958.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "interfaces between teams and for indie",
      "offset": 961.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "hackers uh and everything in between",
      "offset": 963.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that uh are building MCPS for external",
      "offset": 965.36,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "users. So anything from hosting tooling,",
      "offset": 968.48,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "testing tooling, uh eval deployment,",
      "offset": 972.16,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "etc.",
      "offset": 975.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "And then uh I snuck a bullet in here",
      "offset": 976.959,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's maybe a little bit more of a",
      "offset": 979.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "moonshot and a bet on the future, but",
      "offset": 981.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the uh there's a bullet for automated",
      "offset": 983.68,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "MCP server generation. And uh again, if",
      "offset": 986.24,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "you kind of think back to our bet on",
      "offset": 989.759,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "model intelligence and model agency for",
      "offset": 992.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the future, uh at some point models will",
      "offset": 995.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "be so good at writing code and",
      "offset": 998.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "interacting with the external world that",
      "offset": 1001.12,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "they will actually be able to write",
      "offset": 1002.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "their own MCPS on the fly in real time.",
      "offset": 1004.079,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "And so, uh, this might be a little early",
      "offset": 1007.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "for where we are today, but I do think",
      "offset": 1011.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that there will be an opportunity for",
      "offset": 1012.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "automated MCP generation, um, as models",
      "offset": 1015.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "get smarter and smarter.",
      "offset": 1017.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "And, uh, last but not least, uh, wanted",
      "offset": 1020,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to do a quick call out for any tooling",
      "offset": 1023.04,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "around AI security, observability, uh,",
      "offset": 1025.12,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "auditing, etc. I don't think this is",
      "offset": 1027.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "actually specific to MCP. This is true",
      "offset": 1030.959,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "for any AI application. But I think the",
      "offset": 1033.12,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "more that you enable those applications",
      "offset": 1036.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to have access to the outside world to",
      "offset": 1038.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "start playing with uh real data uh of",
      "offset": 1040.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "course the security and privacy etc",
      "offset": 1043.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "implications also go up and so I think",
      "offset": 1045.839,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "if you're going to build uh a startup in",
      "offset": 1048.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "that space now is is the time.",
      "offset": 1050.08,
      "duration": 9.02
    },
    {
      "lang": "en",
      "text": "So with that uh happy MCP. Thank you.",
      "offset": 1054.96,
      "duration": 11.77
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1059.1,
      "duration": 7.63
    }
  ],
  "cleanText": "[Music]\nHello everyone. Who's excited to chat about MCP today?\n\nOkay, we can work on that. We can get it a little bit better by the end of this talk. Uh, but I'm Theo. I am a product manager at Anthropic, working on MCP. Uh, prior to this, I was also a startup founder, uh, working in the AI space. Um, a couple fun facts about me, because everyone says make yourself a little bit more personable, uh, is that I like playing poker, mostly losing money at poker, not uh, making money at poker. Uh, and I also really like coffee. So, uh, if you're, you know, a huge coffee fan, um, and want to talk about the best coffee in San Francisco, hit me up after the talk. But you didn't come here to talk about me. You came here to learn about MCP. So, let's talk about MCP.\n\nI was told not to say MCP is the best thing since sliced bread, uh, which I won't say, but mostly because I don't actually think it's the best thing since sliced bread. Uh, my goal here today is to really walk you through the origin story of MCP, why we launched it, uh, give you a better sense of, you know, where it can actually help you in your workflow, uh, and then ultimately give you a sense of the types of questions that I'm frequently hearing, where I think there's a lot of value to build in the ecosystem, and let you decide for yourself whether or not it is actually the best thing since sliced bread.\n\nSo, scrolling all the way back to uh, mid last year, the co-creators of MCP, David and Justin, had this idea. Uh, they were seeing that, you know, classic two engineers in a garage style. They were seeing that they were constantly copying and pasting context from outside of the context window into the context window. So, you're doing your workflow and suddenly you're remembering that there was a Slack message, which was really important that had a lot of context that you could just copy in. Um, so you're constantly kind of copying things back and forth from Slack. Maybe you're copying things in from Sentry, your error logs. Uh, but they were kind of realizing, hey, it would be so great if Claude or any LLM could just kind of climb out of its box, reach out into the real world and bring that context and those actions uh, to the model. And so the genesis of MCP was really around this big question of uh, not just context but model agency. How do you actually give the model the ability to interact with the outside world?\n\nAnd so as they started thinking about this, uh, they came to the conclusion that it had to be an open-source standardized protocol in order for this to make sense uh, at scale. And the reason is, of course, as you all know, if you want to build an integration, uh, and the, you know, the, the actor, uh, or the client in this case, that has to uh, leverage that integration is a, is using a closed-source ecosystem, then you need maybe a BD or partnerships uh, angle with that client to actually get access to the team to integrate with them. You then have to align on the right interface, and then you get to actually build the thing itself. Um, and so the idea here was that model agency was the biggest thing that was stopping uh, LLMs from actually reaching the next stage of usefulness and intelligence. As we saw that reasoning models were becoming uh, more and more the future, that tool calling was getting better. We really wanted to make sure that we were making it possible for everyone to get involved in that ecosystem and actually allow uh, the models to again have agency.\n\nUh, so they formed a small tiger team internally, uh, worked on this protocol and launched it at our company hack week in uh, November of last year. And this was really the first turning point of MCP. It went viral, as you can imagine. Engineers from various teams were working on building MCPs to automate their own workflows. They were working on MCPs to uh, automate other teams' workflows. Uh, this was really kind of a cool moment to see how it went from again, like two engineers in a garage all the way to uh, this is a major moment in turning point where we think we actually unlocked some uh, true value for for other people. And so we ultimately ended up open sourcing uh, MCP in November of last year, and that's when uh, we introduced it to the rest of the world.\n\nBut as most builders know, uh, when you build something zero to one, you think the launch moment is going to be really impactful. But it actually usually is not. Uh, at launch, most people were saying things like, \"What's MCP?\" or even worse, or maybe, you know, rightfully so, \"What's MPC?\" Uh, and more often than not, we got this question of, \"I don't really understand why you need a new protocol. I don't really understand why it has to be open source. Camp models call tools already.\" Uh, this was the slew of questions that kind of came uh, again and again for probably from the era of November all the way even to uh, early uh, early this year. And it really took uh, making it possible for builders to kind of get their hands dirty uh, with building MCPS to automate their own workflow for for uh, for this to take off. And so the next turning point, uh, as Henry alluded to, was when Cursor kind of adopted MCP, and after that a lot of other coding tools also adopted MCP, um, VS Code, uh, Sourcegraph, uh, etc. But we had a lot of coding IDEs, um, start adopting MCP, and that's really where that uh, next stage of momentum came in, where agency was given to builders to actually build uh, MCPS for themselves.\n\nAnd more recently, we've seen uh, kind of another turning point where Google, Microsoft, OpenAI, uh, and many others have uh, also adopted MCP. So, really excited to see this kind of become more and more uh, the standard. But ultimately, uh, standards uh, become standards because they are actually useful to builders.\n\nAnd so uh, I uh, kind of want to ask all of you to to keep us honest. Um, contribute when you see you know issues with uh, the way that the protocol is built today, uh, or uh, if you uh, even want to take that one step further and submit a PR directly to the GitHub repo and uh, fix the issue, that'd be even better. Um, but our goal here is really to make it maximally useful for uh, for you all and for uh, model providers. So, uh, thank you for for your help in even getting us to the point where I can be speaking on stage uh, about this uh, less than one year later.\n\nSo just to get a little bit deeper into uh, what we were solving for at the start of building MCP is again this kind of idea of model agency. Um, and part of that means uh, agents is kind of the direction that that we think is is going to be the future. That's no surprise to anyone in this room. You are probably going to hear the word agents said in every talk, if not almost every talk. Uh, but the way that we think about agents is that you are giving the model or you're rather depending on the model's intelligence to choose actions and decide uh, what to do. Uh, in the same way that you know, maybe when you talk to a human and you ask them uh, for a response, you don't know exactly what the response is, but based on your understanding of maybe the task that you've given them, your hope is that they are going to give you the right response. And uh, we want to kind of enable that world where you're uh, uh, depending on the model's intelligence scaling over time. So uh, that leads to principles in how we actually build the protocol itself. Uh, recently we uh, launched the support for streamable HTTP, which uh, changes the transport from SSE. uh, and as you all might know, streamable HTTP is is more the uh, enables more birectionality, and so that was uh, a very controversial decision actually, but uh, if you're keeping agents in mind as the future, it makes a lot of sense because you want to make sure that agents can kind of communicate with each other. The other thing that we believe uh, is that there will be a lot more servers than there are clients. Uh, this we could be totally wrong on this. Uh, I would love to see where the future plays out. But because we think that there will be a lot more servers than there are clients, uh, we optimized for server simplicity and for the server uh, server builders to have better tooling. And that does mean when we have to make a trade-off between client complexity or server complexity, we tend to optimize for pushing the complexity down to the client. So apologize in advance to client builders. Uh, but it was an intentional decision. Again, uh, would would uh, be curious to see if if this plays out uh, the way that that we thought it would.\n\nSo I'm going to speedrun through uh, some project updates, mostly because other talks are going to go much more in detail here. Um, but last six months we launched uh, ability for uh, folks to build remote MCPs.\n\nWe fixed O, which we got wrong initially. Thank you. Uh, I know that was a huge, huge thing that that we got wrong initially, but it is now fixed uh, in the draft spec, and so would love folks to you know, continue helping to push on on these things that they see don't match their mental model. Uh, this was actually fixed via a series of people from the community jumping in to work on saying, \"Hey, this is how you know uh, O works with identity providers, and here's how we can update the protocol.\" So, very much a community uh, community effort. Um, again, uh, launched removable HTTP as the primary transport. Uh, and lastly, made a couple of updates uh, to developer experience, um, by updating our SDKs and also uh, making updates to inspector, which if you aren't familiar with, is a really good uh, debugging tool for for your server. I think it is probably our most underutilized uh, tool.\n\nLooking forward, we're going to be focusing a lot more on uh, that agent experience. So, we just added elicitation uh, to the draft spec. This uh, allows servers to ask for more information from end users. So, you can imagine you're building a uh, maybe you're building a flight booking tool, and uh, the end user says, \"Hey, book me the best flight to Atlanta.\" And so as the server, you have a question, which is, \"What does best mean to you? Is it cheapest or is it fastest?\" So you ask the end user uh, and now you can pass through that elicitation. The end user can respond and have that response ultimately sent back to the server. Uh, we are also making progress on the registry API, which would make it a lot easier for models to actually find MCPS that weren't already given to them up front. So this is again kind of on that theme of model agency. Uh, we're really betting on the intelligence of models going up over time.\n\nAgain, working on uh, developer experience. We've heard often from you all that there are uh, that you know, you'd love to understand what kind of the best patterns are in the ecosystem or what the standards are. And so we want to make sure that there are open source examples that uh, that both we've contributed to and also the community can contribute to to kind of help build those standards and patterns together.\n\nAnd lastly, uh, we're making sure that MCP stays open uh, forever, and we are investing heavily in thinking about the next phase of governance. Uh, so there will be more updates on that soon.\n\nAnd just to do a quick call out to uh, the graphic in in the bottom. So a lot of people have asked uh, us what it looks like to actually build an agent with MCP. Our take is that an agent really is, you know, just a server acting as a client and vice versa, uh, where you can then kind of chat back and forth with other agents, uh, other servers, other clients. Um, so I won't go into too much detail there. I know a lot of other people are going to be uh, talking about agents in more detail, but just wanted to make sure that uh, I call that out here.\n\nSo, the uh, thing that everyone has probably been waiting for, and that I've been told uh, over and over again when when I talk to founders, uh, what they're asking me about is uh, what should I build in this space? You know, if uh, MCP becomes a standard, what is where are the interesting opportunities? So before jumping into this, the first thing I'll say is that we are really early right now, and that means that even if the standard exists, we still need the ecosystem to be filled out, and I uh, would urge you to build more and more and more servers. If I had to put a weighting on these three bullet points, I would put 80% on the first one, 10% on the second one, and 10% on the third one.\n\nUm, so we have a lot of opportunity to build a lot more servers uh, that are higher quality uh, and for different verticals. Um, and just to touch quickly on what I mean by higher quality. Uh, a lot of people, you know, maybe hot take, but I think a lot of people are wrapping their API endpoints one to one and just exposing that as tools. I don't think that's the right way to build an MCP server. That in and of itself could probably be a 20-minute talk. Uh, but what you really have to remember when you're building a server is that you have three users. You have the end user, the client developer, and the model. So a lot of people forget that the model is a user here as well. You want to uh, just as you would for API design, you want to think about what are the use cases that your end users are going to have. What are the prompts that they might actually be uh, putting into the the model? And ultimately, what are the tools that you then need to expose to the model to enable the model to respond correctly to those uh, to those prompts. So uh, higher quality servers uh, and also servers for different verticals. A lot of the servers today um, have been for dev tools. We would love to see uh, this expand to be useful beyond engineers into verticals like sales, finance, legal, education, pick your poison, uh, whatever you know best. Um, that uh, we we would just love to see more servers. The next piece is on simplifying server building. So again, as I mentioned, we believe strongly that uh, servers are going to be the vast majority of the ecosystem. There will of course be a lot of clients as well, but we think the uh, order of magnitude of of servers is going to uh, outweigh the order of magnitude of clients. And so would love to see a lot more tooling to actually make it easier and easier to build servers, um, both for enterprises uh, that are deploying MCPs internally uh, as interfaces between teams and for indie hackers uh, and everything in between that uh, are building MCPS for external users. So anything from hosting tooling, testing tooling, uh, eval deployment, etc.\n\nAnd then uh, I snuck a bullet in here that's maybe a little bit more of a moonshot and a bet on the future, but the uh, there's a bullet for automated MCP server generation. And uh, again, if you kind of think back to our bet on model intelligence and model agency for the future, uh, at some point models will\n\n\nBe so good at writing code and\ninteracting with the external world that\nthey will actually be able to write\ntheir own MCPs on the fly in real time.\n\nAnd so, uh, this might be a little early\nfor where we are today, but I do think\nthat there will be an opportunity for\nautomated Model Context Protocol generation, um, as models\nget smarter and smarter.\n\nAnd, uh, last but not least, uh, wanted\nto do a quick call out for any tooling\naround AI security, observability, uh,\nauditing, etc.\nI don't think this is\nactually specific to MCP.\nThis is true\nfor any AI application.\nBut I think the\nmore that you enable those applications\nto have access to the outside world to\nstart playing with uh real data uh of\ncourse the security and privacy etc\nimplications also go up and so I think\nif you're going to build uh a startup in\nthat space now is the time.\n\nSo with that uh happy MCP.\nThank you.\n\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.445Z"
}