{
  "episodeId": "9iN-cPnp7xg",
  "channelSlug": "@aidotengineer",
  "title": "[Evals Workshop] Mastering AI Evaluation: From Playground to Production",
  "publishedAt": "2025-07-01T04:43:29.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 3.55,
      "duration": 4.45
    },
    {
      "lang": "en",
      "text": "Hey everyone,",
      "offset": 15.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "thanks for joining us for the eval",
      "offset": 16.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "session today. This is the first",
      "offset": 18.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "workshop that we'll be leading. There's",
      "offset": 21.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "another one at 3:30, so you get to be",
      "offset": 22.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the first people to to go through it.",
      "offset": 24.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Uh, very exciting stuff. If you've",
      "offset": 26.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "gotten a chance to sign up for Brain",
      "offset": 29.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Trust, uh, you know, please do that now.",
      "offset": 30.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "If not, we also have some workshop",
      "offset": 33.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "materials and a Slack channel for you to",
      "offset": 34.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "follow along.",
      "offset": 37.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "In the Slack channel, we also sent out a",
      "offset": 39.44,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "poll if you'd like to respond with a",
      "offset": 42.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "little emoji underneath the message.",
      "offset": 44.879,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "That'd be great. Uh, in the Slack",
      "offset": 46.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "channel also, there is the the workshop",
      "offset": 49.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "guide. So, in in case you're not uh able",
      "offset": 51.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to get the QR code for whatever reason,",
      "offset": 53.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "uh go into the Slack channel and you'll",
      "offset": 55.68,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "be able to pull up that document.",
      "offset": 57.28,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "Before we jump in, obviously maybe just",
      "offset": 61.68,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "a quick intro of uh Carlos and I. My",
      "offset": 63.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "name is Doug. I am a solutions engineer",
      "offset": 65.439,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "at Brain. Um have a background in data",
      "offset": 68.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "and finance. Actually, my my third week",
      "offset": 71.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "here at Brain. Um but but looking",
      "offset": 73.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "forward to kind of leading you all",
      "offset": 75.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "through the platform and giving you a",
      "offset": 77.2,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "sense for how you can master evals with",
      "offset": 78.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Brain Trust.",
      "offset": 80.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Yeah, my name's Carlos Essan. I'm also a",
      "offset": 82,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "source engineer helping out with some of",
      "offset": 84.56,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "our great customers at Brain Trust. I'm",
      "offset": 87.04,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "a little bit more tenured, been here six",
      "offset": 89.439,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "weeks and before I was in the info world",
      "offset": 91.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "working at Hashi Corb doing some uh",
      "offset": 94.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "stuff with Terraform and Vault. Uh but",
      "offset": 96.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "yeah, super exciting to be here today at",
      "offset": 98.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the AI world fair. Uh we have a lot of",
      "offset": 100.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "exciting things to go over with you.",
      "offset": 102.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "So just to go over the highle agenda,",
      "offset": 106.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "we're going to be alternating between",
      "offset": 108.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "lectures with slides and hands-on",
      "offset": 110.479,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "activities. So we're going to start with",
      "offset": 113.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "understanding, you know, why even eval,",
      "offset": 115.2,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "what is an evalu",
      "offset": 116.96,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "uh task there and then move back into",
      "offset": 126.399,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "the lecture, talk about the SDK, how you",
      "offset": 129.599,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "can do the same thing via the SDK. it",
      "offset": 132.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "can be a bit more powerful in certain",
      "offset": 134.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "situations as well. Then go into",
      "offset": 135.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "production like logging. So day2 stuff,",
      "offset": 138.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "how are you how are you observing your",
      "offset": 141.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "users interacting with your production",
      "offset": 143.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "app or production feature? And then",
      "offset": 145.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "finally, we're going to be incorporating",
      "offset": 148.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "some human the loop. So trying to uh",
      "offset": 149.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "establish some ground truth for the",
      "offset": 151.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "ideal responses, improve your data sets,",
      "offset": 153.68,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and overall improve the performance of",
      "offset": 156.72,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "your of your app.",
      "offset": 158.879,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "If you've gotten a chance to check out",
      "offset": 164.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "the poll in the Slack, uh, feel free to",
      "offset": 165.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "submit a response. Really curious to see",
      "offset": 169.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "how everybody is currently evaluating",
      "offset": 171.92,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "your AI systems.",
      "offset": 173.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Just as a, you know, question that I'm",
      "offset": 178.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "out of curiosity, could I ask for a show",
      "offset": 180.959,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "of hands, how many people have seen",
      "offset": 183.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Brain Trust before, gone to brain.dev,",
      "offset": 185.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and interacted with it? Cool.",
      "offset": 188.08,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "That's great. So, we have some some uh",
      "offset": 191.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "pupils that have already gone and and",
      "offset": 194.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "explored a little bit. That's exciting.",
      "offset": 197.12,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "And a lot of people that are brand new.",
      "offset": 198.56,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "So, starting off with just an",
      "offset": 203.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "introduction. What are evals? Uh how do",
      "offset": 205.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you get started?",
      "offset": 207.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "First, I wanted to just show off some",
      "offset": 210.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "mentions of evals in the public space.",
      "offset": 213.36,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Uh you may recognize some of these",
      "offset": 215.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "names. they see the importance of eval",
      "offset": 217.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "which you know may or not uh point you",
      "offset": 220.239,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "that that this is something important",
      "offset": 223.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that we should be thinking about when",
      "offset": 224.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "pushing changes into production when",
      "offset": 227.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we're developing these these AI",
      "offset": 229.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "features.",
      "offset": 231.28,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "So why even do eval? Well, they they",
      "offset": 234.08,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "help you answer questions. That's",
      "offset": 239.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "ultimately what they're for. You know,",
      "offset": 240.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "uh what type of model should I use?",
      "offset": 242.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "What's the best cost for my use case?",
      "offset": 244.319,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "what's going to perform best in all of",
      "offset": 248,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the edge cases that my users will be",
      "offset": 250.959,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "interacting with.",
      "offset": 253.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Is it going to be consistent with my",
      "offset": 255.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "brand? Is it going to uh you know uh",
      "offset": 256.639,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "talk to the end customer to the end user",
      "offset": 261.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in the same voice that I would want a",
      "offset": 263.759,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "human? Am I improving the system over",
      "offset": 265.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "time? Am I able to catch bugs? Am I able",
      "offset": 269.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to troubleshoot effectively? So all of",
      "offset": 272,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "this can be can be answered with the",
      "offset": 274.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "help of eval which is what we'll be",
      "offset": 276.639,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "discussing today.",
      "offset": 278.4,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "The best LLMs uh don't always guarantee",
      "offset": 284.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "consistent performance. So this is why",
      "offset": 287.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you need to have a testing framework in",
      "offset": 289.28,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "place. Right? We have hallucinations uh",
      "offset": 292.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "occurring at a pretty high rate.",
      "offset": 295.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Performance is also degrading when you",
      "offset": 297.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "make changes. it's difficult to",
      "offset": 300.72,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "guarantee that the change that you're",
      "offset": 302.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "putting through isn't going to regress",
      "offset": 303.759,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "the the application and you know the",
      "offset": 306.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "changing a prompt even if it may seem",
      "offset": 310.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "like it's improving it will actually",
      "offset": 313.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "regress it. So you need to have some uh",
      "offset": 314.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "scientific empirical way of testing",
      "offset": 318.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "these changes and making sure that your",
      "offset": 320.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "AI feature is performing at the level",
      "offset": 323.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "that your users expect.",
      "offset": 324.639,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "So how do evals help your business?",
      "offset": 328.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Well, they cut dev time. You'll be able",
      "offset": 330.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to push changes into production a lot",
      "offset": 332.56,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "faster. Uh, eval will live at the center",
      "offset": 335.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "of your development life cycle. They",
      "offset": 337.759,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "will reduce costs as the due to the",
      "offset": 340.479,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "automated nature of eval. You'll replace",
      "offset": 343.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "manual review. Uh, it will then lead to",
      "offset": 346.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "faster iteration, faster releases.",
      "offset": 349.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "You'll also be able to optimize the",
      "offset": 351.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "model that you're using, make sure that",
      "offset": 352.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it's the best bang for buck. Your",
      "offset": 354,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "quality will go up and you'll be able to",
      "offset": 356.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "scale your teams. It will enable",
      "offset": 358.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "non-technical users and technical users",
      "offset": 360.479,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "to also have a say in the prompt choice",
      "offset": 362.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "in the model choice and just the overall",
      "offset": 366.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "um management of the performance in in",
      "offset": 369.039,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "the production traffic.",
      "offset": 372.4,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "These are some of Brain Trust's customer",
      "offset": 377.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "outcomes. So, we've been able to uh help",
      "offset": 378.88,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "some of these great companies move a lot",
      "offset": 382,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "faster, increase their team",
      "offset": 383.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "productivity, and increase their AI",
      "offset": 385.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "product quality.",
      "offset": 387.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "So now moving into some of the core",
      "offset": 391.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "concepts of brain trust. Uh so we're",
      "offset": 393.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "really targeting three things. Prompt",
      "offset": 395.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "engineering, right? So we're thinking",
      "offset": 397.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "about how we're writing the prompts.",
      "offset": 399.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "What's the best way to provide context",
      "offset": 402.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "on our specific use case to the prompt",
      "offset": 405.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "so that we are optimizing its response.",
      "offset": 407.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Right? The middle piece evals. Are we",
      "offset": 410.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "measuring improvements? Are we measuring",
      "offset": 412.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "regressions? Is this being done in a",
      "offset": 414.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "statistical way uh that's easy to",
      "offset": 417.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "review, easy to understand? And then",
      "offset": 420.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "finally, AI observability. Are we",
      "offset": 422.479,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "capturing what's happening in",
      "offset": 424.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "production? Do we know if our users are",
      "offset": 425.919,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "happy with the outputs, unhappy? Are we",
      "offset": 428.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "able to uh prioritize certain responses",
      "offset": 430.639,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "so that we can keep iterating, keep",
      "offset": 434.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "improving?",
      "offset": 436.4,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Great. So now moving uh to the eval",
      "offset": 440.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "section. So what what is an eval? So the",
      "offset": 442.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the definition we've come up with is",
      "offset": 444.8,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that it's a structured test that checks",
      "offset": 446.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "how well your AI systems perform. It",
      "offset": 448.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "helps you measure quality, reliability,",
      "offset": 450.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and correctness across various",
      "offset": 452.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "scenarios. Ideally, you're capturing",
      "offset": 454.639,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "every scenario that a user will live",
      "offset": 456.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "through when interacting with your AI",
      "offset": 459.039,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "feature.",
      "offset": 460.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "When it comes to brain trust and writing",
      "offset": 464.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "evals, there's really three ingredients",
      "offset": 466.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that you need to understand to be able",
      "offset": 468.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "to to work effectively. The first is a",
      "offset": 470.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "task. So this is the thing that you're",
      "offset": 472.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "testing, right? This is the code or",
      "offset": 474.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "prompt that you want to evaluate. It can",
      "offset": 476.639,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "be a single prompt or a full agentic",
      "offset": 479.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "workflow. The complexity is really up to",
      "offset": 481.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "you. The one requirement is that it has",
      "offset": 483.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "an input and an output.",
      "offset": 485.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Then we have our data set. So this is",
      "offset": 488.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the set of real world examples or test",
      "offset": 490.479,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "cases that we want to uh push through",
      "offset": 492.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the task to see how it performs. And",
      "offset": 495.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "then the score that's the logic behind",
      "offset": 498.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "uh the the evouse. So, how are we",
      "offset": 500.72,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "grading the output of our prompt on our",
      "offset": 502.96,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "data set? Uh, these can be LLM as a",
      "offset": 505.759,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "judge scores or they can be full code uh",
      "offset": 508.479,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "functions",
      "offset": 511.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and the caveat is that they need to",
      "offset": 513.279,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "output a score from 0 to one which will",
      "offset": 515.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "then be converted into a percentage.",
      "offset": 517.599,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Question.\n Yeah.\n What are the test cases?",
      "offset": 520.8,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Is it also agent generated?",
      "offset": 523.519,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "It can be at first. The question was, is",
      "offset": 527.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the data set synthetic? Can it be",
      "offset": 529.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "synthetic? And the answer is, it's a",
      "offset": 532.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "great way to get started quickly is",
      "offset": 535.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "having an AI generate those initial use",
      "offset": 537.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "cases, but as you progress, as you",
      "offset": 539.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "mature, it's great to ground those in",
      "offset": 541.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "logs so you're capturing the real user",
      "offset": 544.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "traffic, the real interactions that",
      "offset": 547.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "users are having and integrating those",
      "offset": 548.64,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "into your data sets.",
      "offset": 550.48,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "Great. So now I wanted to talk about",
      "offset": 558.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "offline evals and online eval. So",
      "offset": 560.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "there's two mental models to think",
      "offset": 562.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "through. Offline evals are what you're",
      "offset": 564.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "doing in development, right? So this is",
      "offset": 566.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the structured testing of the AI model",
      "offset": 568.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "of the the prompt uh that you are going",
      "offset": 571.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to then eventually uh show off to to",
      "offset": 573.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "customers in production. So this is for",
      "offset": 576.64,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "proactive identification of issues,",
      "offset": 579.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "right? uh this is what we'll be doing",
      "offset": 582.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "today in the playground in brain trust",
      "offset": 585.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and also via the SDK. Uh but then on the",
      "offset": 587.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "other side is online eval. So this is in",
      "offset": 590.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "production uh real traffic is being",
      "offset": 593.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "captured and being measured. It's being",
      "offset": 596.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "graded just like your offline evals are",
      "offset": 598,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "being graded. And this is going to allow",
      "offset": 600.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you to diagnose problems, monitor the",
      "offset": 602.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "overall performance and capture user",
      "offset": 604.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "feedback in real time so that you can",
      "offset": 607.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "understand, oh, this edge case isn't",
      "offset": 610.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "included in my data set. This is a weak",
      "offset": 612.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "point in my current AI product. I need",
      "offset": 614.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "to spend some time uh attacking it and",
      "offset": 617.279,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "improving it.",
      "offset": 619.279,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "A big question that we get asked is what",
      "offset": 624.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "should I improve? Right? Uh I have my",
      "offset": 627.519,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "prompt, I have my eval, you know, how do",
      "offset": 629.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "I know what's wrong? Uh and I think this",
      "offset": 633.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "matrix really helps simplify this",
      "offset": 636.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "question. So if you have a good output",
      "offset": 638.079,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "from, you know, your own judgment",
      "offset": 641.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "looking at what the LLM is giving you",
      "offset": 644,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and it's a high score, then great,",
      "offset": 646,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "right? you've verified yourself that the",
      "offset": 648,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "output is is high quality and also the",
      "offset": 649.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "the scores the evals have also come to",
      "offset": 652.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the same conclusion. If you think it's a",
      "offset": 654.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "good output but it's a low score then",
      "offset": 657.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's a signal that you need to improve",
      "offset": 659.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "your evals. Maybe the score isn't",
      "offset": 661.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "actually representing what a human would",
      "offset": 663.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "think, right? Uh if it's a bad output",
      "offset": 665.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "but a high score, same thing, right? It",
      "offset": 668.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "doesn't match what a human would think",
      "offset": 670.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "looking at the output. So you need to",
      "offset": 672.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "improve your evals. And then finally, if",
      "offset": 673.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it's a bad output and a low score, your",
      "offset": 675.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "evals are working correctly. That's",
      "offset": 678.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "good. And now you need to focus on",
      "offset": 679.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "improving your AI app. So, I hope this",
      "offset": 681.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "helps explain how you should be thinking",
      "offset": 684.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "through uh your scores and and what to",
      "offset": 686.16,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "what to tackle in which moment.",
      "offset": 688.72,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "So, now we're going to zoom into each of",
      "offset": 694.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "those ingredients or components starting",
      "offset": 696.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "off with the task. Uh so, as I",
      "offset": 698.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "mentioned, right, a task is really just",
      "offset": 700.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "an input and an output. It can be a",
      "offset": 702.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "single LM call or a whole agentic",
      "offset": 704.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "workflow. It's really up to you what you",
      "offset": 707.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "want to test. Uh so in this pattern,",
      "offset": 709.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we're just going to be creating a simple",
      "offset": 712.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "prompt. Uh this is what the activity",
      "offset": 714.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "today is going to encompass",
      "offset": 716.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and uh you can use dynamic templating",
      "offset": 718.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "with mustache. So you can provide your",
      "offset": 720.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "data set rows as part of the the prompt",
      "offset": 722.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and that will be tested uh and you'll",
      "offset": 725.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "get to see that in action soon.",
      "offset": 727.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "What if you have more than just a",
      "offset": 731.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "prompt? What if you have a multi-turn",
      "offset": 732.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "chat a whole conversation that you want",
      "offset": 734.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to evaluate? So you can do that in brain",
      "offset": 736.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "stress today. You can provide the whole",
      "offset": 738.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "conversation as extra messages. So",
      "offset": 740.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "providing that whole chain of of uh",
      "offset": 742.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "messages back and forth with the user",
      "offset": 744.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and the assistant. You can include tool",
      "offset": 746.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "calls as well to simulate those tool",
      "offset": 748.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "calls and evaluate that big chunk that",
      "offset": 750.32,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "context that whole conversation at once.",
      "offset": 752.88,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Tools are also something supported in",
      "offset": 758.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "brain trust. So that's oftent times",
      "offset": 760.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "something that your AI applications will",
      "offset": 762,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "leverage uh talking to external services",
      "offset": 764,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "or grabbing information from somewhere",
      "offset": 766.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "else. Uh so you can add tools to brain",
      "offset": 768.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "trust and have the tool available for",
      "offset": 771.44,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "your prompt uh to use",
      "offset": 774.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "and just to mention that's great for rag",
      "offset": 780.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "use cases. So I know that's a a hot word",
      "offset": 782,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "right now. So if if you have that in you",
      "offset": 784.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "know in mind brain trust can handle it.",
      "offset": 786.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "We support tools. We support rag um",
      "offset": 789.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "agents another hot word right now. So we",
      "offset": 792.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "also allow you to chain your prompts,",
      "offset": 795.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "right? So you can have three prompts",
      "offset": 797.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "chained together. The output of the",
      "offset": 799.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "first prompt will become the input of",
      "offset": 801.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the next and so on. Uh so you can start",
      "offset": 802.959,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "uh testing end to end uh all these",
      "offset": 805.839,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "prompts back and forth, right? And and",
      "offset": 810.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "do the same thing that you would with",
      "offset": 812.16,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "with a single prompt.",
      "offset": 813.279,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "Great. Okay, so now moving into data",
      "offset": 817.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "sets. So this is the test cases, right?",
      "offset": 819.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "You're going to keep iterating over",
      "offset": 821.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "time, but initially maybe you're using",
      "offset": 823.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "something synthetic. Um, there are three",
      "offset": 825.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "fields that you need to understand for a",
      "offset": 827.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "data set. Only one of them is required",
      "offset": 829.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "though, and that's the input. So that is",
      "offset": 831.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the the prompt, the user provided uh",
      "offset": 833.12,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "use case, the the prompt that would be",
      "offset": 838,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "provided by by the user would be the",
      "offset": 839.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "input. You could think of it that way.",
      "offset": 841.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And then you have the expected column",
      "offset": 843.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "which is optional which is the",
      "offset": 845.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "anticipated output or the ideal response",
      "offset": 846.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of that that prompt. And then finally",
      "offset": 849.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you have your metadata which can allow",
      "offset": 852.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "you to to capture any additional",
      "offset": 854.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "information that you may want to",
      "offset": 856.079,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "associate to that specific row in the",
      "offset": 857.519,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "data set.",
      "offset": 859.36,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "Some tips for data sets is to start",
      "offset": 863.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "small and iterate. Uh it doesn't need to",
      "offset": 865.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "be the largest data set of all time. It",
      "offset": 868.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "doesn't need to include all of your use",
      "offset": 870.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "cases, right? Just get started. Use",
      "offset": 872.16,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "synthetic data at first. And the",
      "offset": 875.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "important piece is to keep improving,",
      "offset": 877.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "right? Keep iterating. So if you start",
      "offset": 878.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "logging your real user interactions, you",
      "offset": 880.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "know, even if it's just in staging or",
      "offset": 882.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "internally in your organization, you can",
      "offset": 884.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "start to increase the the scope of the",
      "offset": 886.24,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "data set and it will start to become",
      "offset": 890.48,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "closer to the the overall uh domain of",
      "offset": 892.639,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "of use cases that that users will will",
      "offset": 896.959,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "interact with. And then finally, you",
      "offset": 899.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "want to start implementing human review.",
      "offset": 902.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Uh this will allow you to establish",
      "offset": 904.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "ground truth, improve your data set,",
      "offset": 905.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "improve the expected column. um which",
      "offset": 907.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "will be great for for your evals",
      "offset": 909.76,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "and su zooming into scores. So this is",
      "offset": 914.32,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "uh you have two options here and the",
      "offset": 917.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "type of score that you want to use. LM",
      "offset": 919.519,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "is a judge. This is great for uh more",
      "offset": 921.519,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "subjective or contextual feedback.",
      "offset": 924.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "What would a human need to understand",
      "offset": 928.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "when looking at the output? What",
      "offset": 930.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "criteria would they consider? Uh this is",
      "offset": 932.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "more of a qualitative question that you",
      "offset": 934.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "want an answer to, right? using that LM",
      "offset": 937.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "as a judge. On the codebased score, this",
      "offset": 938.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "is deterministic, right? So, you would",
      "offset": 941.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "want exact or binary conditions. This is",
      "offset": 943.68,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "more of an objective question. Um,",
      "offset": 947.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and it's the important piece is to try",
      "offset": 950.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to use both. So, you want some LLM as a",
      "offset": 952.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "judge scores, but you also would like",
      "offset": 954.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "some codebased scores, and they they'll",
      "offset": 956.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "help you meet in the middle and",
      "offset": 958.8,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "understand the the quality.",
      "offset": 960.079,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "So, some tips here. uh you know, if",
      "offset": 964.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you're using an LM as a judge, maybe use",
      "offset": 966.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "a higher quality model, a more expensive",
      "offset": 968.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "model to to grade the the cheaper model.",
      "offset": 970.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Uh make sure that the LM as a judge has",
      "offset": 973.04,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "a focus. So don't give it, you know,",
      "offset": 975.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "four or five criteria to consider. Zoom",
      "offset": 977.759,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "into one specific piece and expand and",
      "offset": 980.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "explain the steps it should think",
      "offset": 982.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "through to to come to its conclusion.",
      "offset": 984.399,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "If you're writing LM as a judge, maybe",
      "offset": 988.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you should eval the judge and make sure",
      "offset": 990.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that the prompt that you're using is",
      "offset": 992.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "matching what a human would think. So",
      "offset": 994.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that's another great way of improving",
      "offset": 996.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "your scores. And you know, just make",
      "offset": 997.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sure that it's confined and you're not",
      "offset": 1001.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "overloading it with all the context in",
      "offset": 1002.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the world, right? You want it to be",
      "offset": 1004.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "focused on the relevant input and output",
      "offset": 1005.68,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "uh for consistency.",
      "offset": 1008.24,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "Great. Almost at the end here. So the",
      "offset": 1012.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "there's two things to understand about",
      "offset": 1014,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the brain trust UI specifically. So",
      "offset": 1015.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there's the the playgrounds and this is",
      "offset": 1018,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for quick iteration of your prompts uh",
      "offset": 1020.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "agent scores data sets right it's really",
      "offset": 1022.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "effective for comparing uh you can do AB",
      "offset": 1025.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "testing with prompts you can do AB",
      "offset": 1028.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "testing with with models and then you",
      "offset": 1029.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "can save a snapshot of the playground to",
      "offset": 1032.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "your experiments view and the",
      "offset": 1035.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "experiments is for comparison over time",
      "offset": 1036.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "so you'll be able to track how your",
      "offset": 1039.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "scores change over weeks months uh and",
      "offset": 1041.12,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "everything that your team is doing",
      "offset": 1045.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "across the UI across the SDK will also",
      "offset": 1046.959,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "aggregate in the experiments view. So",
      "offset": 1050.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you can analyze everything and",
      "offset": 1052,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "understand okay this new model came out",
      "offset": 1053.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "today how is it performing to the prompt",
      "offset": 1055.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "from two weeks ago.",
      "offset": 1057.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Great. So now we've reached the first",
      "offset": 1060.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "activity. Uh so if you could please go",
      "offset": 1062.559,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "to the activity document and it will",
      "offset": 1065.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "take you through the journey of of",
      "offset": 1068.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "running your first evout in the brain",
      "offset": 1070.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "UI.",
      "offset": 1072.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Please raise your hand if you have any",
      "offset": 1074.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "questions or run into any issues.",
      "offset": 1076.08,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "We'll be walking around and just uh",
      "offset": 1079.44,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "making sure there's no blockers.",
      "offset": 1082.16,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Yeah, if you check Slack, we'll also go",
      "offset": 1090.24,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "back to the QR code.",
      "offset": 1093.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "So, did everybody have a chance to get",
      "offset": 1096.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these QR codes?",
      "offset": 1098.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Uh the middle one is going to be the",
      "offset": 1100.88,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "most important. And this is where you're",
      "offset": 1102.48,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "going to access the materials for the",
      "offset": 1103.36,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "workshop.",
      "offset": 1104.72,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah, I'll repeat the question. The the",
      "offset": 1114.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "question was around uh extra messages in",
      "offset": 1115.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the prompt and if you are overseeing",
      "offset": 1118.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "agents and you know multiple types of",
      "offset": 1121.039,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "users, multiple different roles are uh",
      "offset": 1124,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "all talking back and forth and you want",
      "offset": 1127.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "to distinguish",
      "offset": 1128.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "their roles, right? and all being all",
      "offset": 1130.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "within the the playground UI. Um",
      "offset": 1132.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "so you can right now there is no",
      "offset": 1136,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "additional delineation between uh the",
      "offset": 1138.48,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "assistant the user the tool call and um",
      "offset": 1141.36,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "I believe that's it. So having the the",
      "offset": 1146.4,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "user be branched and it play different",
      "offset": 1150,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "roles is something that you would need",
      "offset": 1153.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to rely on the the SDK for that",
      "offset": 1154.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "additional flexibility.",
      "offset": 1157.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "my other question,",
      "offset": 1160.32,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "right?",
      "offset": 1163.6,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah. And and we'll cover the SDK in the",
      "offset": 1169.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "in the next section. I think maybe the",
      "offset": 1171.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "biggest takeaway is that there is no",
      "offset": 1172.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "limit really on the complexity that that",
      "offset": 1174.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "you feed to that like as that task,",
      "offset": 1176.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "right? The the only requirement is that",
      "offset": 1179.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "input and that output. like maybe",
      "offset": 1181.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "something is a little bit more tailored",
      "offset": 1183.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to the brain trust playground and the UI",
      "offset": 1184.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "where some things are actually a little",
      "offset": 1186.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "bit more tailored to that SDK. Uh so",
      "offset": 1188.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that's that's we can jump into that in",
      "offset": 1191.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that next section. Um maybe it makes",
      "offset": 1193.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "sense to like as we're going through",
      "offset": 1195.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "like the the workshop I'll kind of walk",
      "offset": 1196.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "through this as well just so you can all",
      "offset": 1199.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "kind of see me go through it. But feel",
      "offset": 1201.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "free to raise your hand. Uh we can we",
      "offset": 1203.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "can walk around and and answer",
      "offset": 1205.6,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "questions.",
      "offset": 1207.039,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "for Slack.\n Are you able to access the",
      "offset": 1214.559,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "document via the\n is the the slide deck's",
      "offset": 1217.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "not public? Is that\n it's not\n Oh, that",
      "offset": 1221.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "for the slide deck. Is this\n Yeah, we",
      "offset": 1224.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we've just Yeah, the question was this",
      "offset": 1226.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is all in the UI, right? Is this That's",
      "offset": 1228.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the only place we've been thus far,",
      "offset": 1231.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "right?\n Uh just talking a little bit",
      "offset": 1232.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "about the different components of the",
      "offset": 1234.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "brain trust platform.",
      "offset": 1236.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Um, so let me let me walk through that,",
      "offset": 1239.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "right? Give you a sense for what we just",
      "offset": 1241.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "kind of showed to you in slides, right?",
      "offset": 1242.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "So you can't access the slide deck.",
      "offset": 1244.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "We can we can update that.\n Yeah.\n Yeah.",
      "offset": 1248.4,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "Um, just let's kind of walk through this",
      "offset": 1251.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh so we can get a sense for what what",
      "offset": 1254.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "we're what we're building here. Uh some",
      "offset": 1256.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "of the things that that Carlos just",
      "offset": 1258.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "walked through. Um I have a lot of this",
      "offset": 1259.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "stuff already installed. I hope that you",
      "offset": 1261.919,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "kind of walked through this, right? We",
      "offset": 1263.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "need uh certain things on our system to",
      "offset": 1264.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "actually go and run this. So we have",
      "offset": 1266.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "node, we have get. Um we're going to",
      "offset": 1268.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "sign up for a brain trust account.",
      "offset": 1270.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Creating a brain trust or already done",
      "offset": 1273.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this, so I'm not going to kind of bore",
      "offset": 1275.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you through that that step right there.",
      "offset": 1276.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "This project unreleased AI. Um if if you",
      "offset": 1278.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "if you don't do that, you'll see two",
      "offset": 1282.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "projects in your account, but um just",
      "offset": 1283.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this is where we're going to actually",
      "offset": 1285.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "create our prompts and our scores and",
      "offset": 1287.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "our data set from the repo that we're",
      "offset": 1289.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going to clone into uh our local",
      "offset": 1291.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "machine.",
      "offset": 1293.28,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Part of this demo requires an OpenAI API",
      "offset": 1298,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "key. That's just what we're using under",
      "offset": 1300.559,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "the hood. That's certainly not a",
      "offset": 1301.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "limitation of of brain trust. Um you can",
      "offset": 1303.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "use and maybe just to kind of highlight",
      "offset": 1305.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "something here.",
      "offset": 1307.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "You can uh you can use really any AI",
      "offset": 1309.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "provider out there. So if you've gone",
      "offset": 1312.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "into Brain Trust account, you've",
      "offset": 1314,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "probably seen this. You've entered your",
      "offset": 1315.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "OpenAI API key here. This is what is",
      "offset": 1316.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "going to allow you to to run those",
      "offset": 1319.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "prompts in the playground. You can see",
      "offset": 1320.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you have access to many other providers.",
      "offset": 1322.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "You have access to cloud providers like",
      "offset": 1324.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "uh bedrock and and so on. And then you",
      "offset": 1326.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "can even uh use your own custom",
      "offset": 1328.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "provider. But for this workshop right",
      "offset": 1330.4,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "now we are using open AI.\n Question.",
      "offset": 1332,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "Sorry.\n Are you able to run",
      "offset": 1339.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "locally models?\n Yes.\n Yeah. Question was",
      "offset": 1342.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "can you run brain trust locally using",
      "offset": 1346,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "local models? Yeah. Um we we have uh if",
      "offset": 1348.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you look a little bit further out",
      "offset": 1351.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "there's a section for what we call",
      "offset": 1352.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "remote evals. Might not have time to get",
      "offset": 1354.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to it in this particular section but",
      "offset": 1356.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "know that you can go to that and and",
      "offset": 1358.32,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "play with that feature as well.",
      "offset": 1360.159,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 1365.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "sorry coming back down here. Um",
      "offset": 1367.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so we're going to clone this repo.",
      "offset": 1371.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Right. This is a right this is the",
      "offset": 1372.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "application that we're creating. The",
      "offset": 1374,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "idea is to uh give it a a GitHub URL and",
      "offset": 1375.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "look for most recent commits since the",
      "offset": 1379.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "last release and then summarize those",
      "offset": 1382.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "right for us as as developers. So that's",
      "offset": 1384.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the application that we're going to use.",
      "offset": 1387.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Uh we're going to create some different",
      "offset": 1388.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "API keys locally. So if you've cloned",
      "offset": 1390.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "your repo, uh you'll have av.local",
      "offset": 1392.48,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "file. I'll show you my example.",
      "offset": 1396.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "You're going to also input these your",
      "offset": 1399.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "brain API key here and then your OpenAI",
      "offset": 1400.799,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "API key. This is optional down here. Uh",
      "offset": 1403.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it's just if you don't want to get rate",
      "offset": 1406.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "limited by GitHub. Probably not probably",
      "offset": 1408.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "not going to create a lot of requests",
      "offset": 1410.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "right now. So you probably don't need",
      "offset": 1411.28,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "this.",
      "offset": 1412.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Um really important step here. So I'm",
      "offset": 1417.039,
      "duration": 8.12
    },
    {
      "lang": "en",
      "text": "going to come back into uh brain trust.",
      "offset": 1420,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "So as part of our install, we're",
      "offset": 1428.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "actually going to go create some of",
      "offset": 1430.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "these resources. uh within our brain",
      "offset": 1431.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "trust project that we just created. So",
      "offset": 1434.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I'm going to run pnpm install.",
      "offset": 1436.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "This will actually go push some of these",
      "offset": 1439.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "resources and you'll find these uh in",
      "offset": 1441.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the brain trust folder the resources and",
      "offset": 1443.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we'll jump into that but just wanted to",
      "offset": 1446.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "highlight that. So now if I look back",
      "offset": 1448.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "into my project I should see that",
      "offset": 1450.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "unreleased AI and the different things",
      "offset": 1452,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that we've created. We have two",
      "offset": 1454.48,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "different prompts now right? These are",
      "offset": 1455.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the the prompts that we use to generate",
      "offset": 1456.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the change log as well as the the test",
      "offset": 1458.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "cases, the data set that we'll use as as",
      "offset": 1460.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "part of this.",
      "offset": 1463.44,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1467.76,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "All right, let me stop there. Anybody",
      "offset": 1482.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "having issues just kind of going through",
      "offset": 1484.559,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "that initial setup phase?",
      "offset": 1486,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "Excuse me.",
      "offset": 1493.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Uh, we haven't made those public yet. I",
      "offset": 1495.279,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "think\n I'm trying to do that now.\n Yeah.",
      "offset": 1497.6,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Do you have a do you have Slack? Uh,",
      "offset": 1503.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Were",
      "offset": 1506.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you able to join the workshop eval",
      "offset": 1508.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "channel?",
      "offset": 1510.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah, the Wi-Fi is not working.",
      "offset": 1516.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Okay. Well, I I'll walk you through.",
      "offset": 1519.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "Yeah.\n How are we connecting the repo to",
      "offset": 1522.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the project?",
      "offset": 1525.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Um, yeah. So, when we ran PNPM install,",
      "offset": 1527.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "we ran a a script in the background",
      "offset": 1530.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "called just brain trust push. And if I",
      "offset": 1533.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "look at uh that file here, there's",
      "offset": 1535.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "different things that we've configured,",
      "offset": 1537.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "right? We've configured uh my change",
      "offset": 1539.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "log. So this is actually the brain trust",
      "offset": 1541.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "uh SDK under the hood. Uh this is where",
      "offset": 1543.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we're creating that prompt in that",
      "offset": 1545.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "project unreleased AI. And so there's a",
      "offset": 1547.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "couple things that we can do here from",
      "offset": 1550.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "an SDK perspective. This is like you",
      "offset": 1552.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think about uh you know version",
      "offset": 1554.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "controlling all of these different",
      "offset": 1555.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "things and actually pushing them into",
      "offset": 1557.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the brain trust UI. So there there's a",
      "offset": 1559.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "lot of different ways to work with brain",
      "offset": 1561.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "trust. I think we mentioned earlier",
      "offset": 1563.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "either via just like the the UI or",
      "offset": 1565.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "actually via the SDK, but that's that's",
      "offset": 1567.039,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "how a lot of this stuff got created.",
      "offset": 1569.36,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Cool. Let's let's kind of walk through",
      "offset": 1573.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "this uh this first activity. Um we're",
      "offset": 1575.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "going to we're going to access the",
      "offset": 1578.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "unreleased AI project. So if we go to",
      "offset": 1580.4,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the prompts, so this is what we just",
      "offset": 1583.6,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "created.",
      "offset": 1585.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "Uh we created two different prompts,",
      "offset": 1586.799,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "right? This is essentially what we can",
      "offset": 1588.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "start to play around with, right? We",
      "offset": 1589.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have this one uh and and Carlos",
      "offset": 1591.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "mentioned earlier there there's this",
      "offset": 1593.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "mustache syntax. We can actually input",
      "offset": 1594.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "uh variables here into into our prompts",
      "offset": 1597.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and this is going to actually map to the",
      "offset": 1600.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the different data sets that we can",
      "offset": 1602.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually use as part of this project. So",
      "offset": 1603.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "here's our first prompt.\n That's about",
      "offset": 1606.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "impossible to see.\n That was impossible.",
      "offset": 1607.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Okay.\n No, no, that's fine. I appreciate",
      "offset": 1610.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that. Thank you.\n And the lighting is",
      "offset": 1612,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "hard. So\n yeah.\n Oh, maybe we can\n change",
      "offset": 1614,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the appearance to light mode or no.",
      "offset": 1617.039,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Yeah. Where's that? put the user up",
      "offset": 1619.039,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "here.",
      "offset": 1622.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "How's that?",
      "offset": 1624.559,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "Thank you. No, I appreciate that.",
      "offset": 1627.84,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "How's this?",
      "offset": 1632.08,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Okay, cool. Um, so really just reviewing",
      "offset": 1635.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the stuff that we created. We created",
      "offset": 1638.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "these two prompts. Here's our data set",
      "offset": 1639.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that we're going to use when we run our",
      "offset": 1642.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "evals and our experiments. Uh, you can",
      "offset": 1643.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "get a sense for for this. Here's my",
      "offset": 1646.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "input. I have a series of commits and",
      "offset": 1648.159,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "then I have a repository URL and I have",
      "offset": 1650.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the when the last release was that's",
      "offset": 1654.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "that since field. So these this is again",
      "offset": 1657.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the thing that we'll use inside of that",
      "offset": 1660.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "playground to create uh to create evals",
      "offset": 1661.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and and to use to iterate from. And then",
      "offset": 1664.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the last thing that uh I'll call out",
      "offset": 1667.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 1669.36,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "is the scores. Uh so we created a few",
      "offset": 1671.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "different scores that we'll want to use",
      "offset": 1674.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "to actually score uh these prompts. So",
      "offset": 1676.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "we have an accuracy, formatting, and",
      "offset": 1679.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "completeness score. And again, this is",
      "offset": 1681.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "just in that that repo and that",
      "offset": 1682.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "resources.ts file. Uh we have maybe just",
      "offset": 1684.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to to point out uh linking a little bit",
      "offset": 1687.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of what Carlos was talking about to the",
      "offset": 1689.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actual code that you're seeing. We have",
      "offset": 1690.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "LLM as judge scores. As you can see",
      "offset": 1692.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "here, uh really again trying to pinpoint",
      "offset": 1695.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "here the accuracy, right? uh we're not",
      "offset": 1699.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "overloading a single LLM as a judge",
      "offset": 1701.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "score with accuracy, completeness, and",
      "offset": 1703.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "formatting. Going to be very uh sort of",
      "offset": 1705.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "um detailed or you know scoped down to",
      "offset": 1708.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that particular thing. And then last one",
      "offset": 1710.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we have a codebased score, right? So",
      "offset": 1712.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "this is a little bit more uh binary,",
      "offset": 1714.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "right? Is the the formatting of this",
      "offset": 1716.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "change log that the LLM generated does",
      "offset": 1719.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "it map to what we expect? And so we can",
      "offset": 1721.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "use some code to do that. So that's what",
      "offset": 1723.2,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "we created via that script.",
      "offset": 1725.919,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "Yeah.\n How do we get that sandbox",
      "offset": 1730.64,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "project?",
      "offset": 1734.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah. So, if you go back to the the lab",
      "offset": 1737.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "setup when you run when you run that",
      "offset": 1739.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "install. So, I'm using PNPM.",
      "offset": 1742.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "All right. I don't know if if you use",
      "offset": 1745.76,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "that if you're using that locally, you",
      "offset": 1746.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "can you can also use npm.\n So, what is",
      "offset": 1748.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the key for OpenAI API key?\n What is the",
      "offset": 1750.799,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "key? Do you not have an OpenAI API key?",
      "offset": 1754,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "instruction.",
      "offset": 1757.919,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay. Um I don't know if we have one to",
      "offset": 1767.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "distribute at the moment.",
      "offset": 1770.24,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "Yeah. If you don't have",
      "offset": 1773.84,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "I'll do this later. Well, if if you go",
      "offset": 1777.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "like the part of the setup here, right",
      "offset": 1780.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "when we go to if you come in here to a",
      "offset": 1782.799,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "playground as an example,",
      "offset": 1785.2,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "right? And we're going to pull in one of",
      "offset": 1789.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "those prompts. So, we can pull in both",
      "offset": 1791.36,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "of these prompts to do again like what",
      "offset": 1792.64,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "Carlos was talking about that that's",
      "offset": 1793.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sort of like AB testing.",
      "offset": 1795.279,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "It's going to ask for some open AAI",
      "offset": 1798.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "models, right? It's if you don't",
      "offset": 1800.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "configure an OpenAI API key inside of",
      "offset": 1802.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "your Brain Trust account, you don't have",
      "offset": 1805.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a provider to actually run this this",
      "offset": 1807.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "task against.",
      "offset": 1809.36,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Um, but but this is this is the",
      "offset": 1813.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "playground, right? This is what Carlos",
      "offset": 1815.2,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "was talking a little bit about earlier",
      "offset": 1816.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "uh about being able to do some AB",
      "offset": 1818.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "testing, right? I have uh my two prompts",
      "offset": 1820,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that I've loaded in. The idea here is to",
      "offset": 1823.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to load in those different ingredients,",
      "offset": 1825.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "right? Our tasks, our our data set, and",
      "offset": 1827.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "then our scores. So you'll you'll look",
      "offset": 1829.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "down here. We're going to select that",
      "offset": 1831.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "data set and then we're going to select",
      "offset": 1833.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the different scores that we want to",
      "offset": 1836,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "score this task against. And I'll load",
      "offset": 1837.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in my accuracy, formatting, and",
      "offset": 1839.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "completeness.",
      "offset": 1841.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Can do a couple things here. I can click",
      "offset": 1843.039,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "run. This will actually in parallel go",
      "offset": 1844.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "through that uh that data set and use",
      "offset": 1847.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "each task that we've defined and then it",
      "offset": 1850.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "will score those. Right? So the idea",
      "offset": 1852.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "here is to again like this provides that",
      "offset": 1854.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sort of rapid rapid iterative uh",
      "offset": 1856.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "feedback loop that we often times need",
      "offset": 1858.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to build these types of products. So",
      "offset": 1860.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "here are my, you know, like my example",
      "offset": 1863.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "rows. Again, these could be synthetic.",
      "offset": 1865.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "These could be a a small subset of rows",
      "offset": 1866.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that are coming back from my",
      "offset": 1868.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "application. But now I can get a sense",
      "offset": 1869.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for prompt A, prompt B. How are these",
      "offset": 1871.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "performing with my scores? Uh, you know,",
      "offset": 1874.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "relative to the things that I have here.",
      "offset": 1877.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "But uh then I'm able to do a lot of",
      "offset": 1879.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "different things here. I I can uh look",
      "offset": 1881.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "at maybe a summary layout, get a sense",
      "offset": 1883.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "for the scores. So uh at the top, this",
      "offset": 1885.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is sort of like my baseline. up here is",
      "offset": 1888.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "my base task and this is my comparison",
      "offset": 1890.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "task. So you can get a very high level",
      "offset": 1892.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "uh look at these different scores and",
      "offset": 1895.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "how they fared with the different",
      "offset": 1897.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "prompts that we've loaded in here. Now",
      "offset": 1899.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "the other thing that we can do that",
      "offset": 1901.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Carlos mentioned is uh experiments. So",
      "offset": 1902.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "often times we'll want to capture this",
      "offset": 1904.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "these scores over time. So when we do",
      "offset": 1907.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "make changes we understand how those",
      "offset": 1909.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "scores fared a week ago, a month ago or",
      "offset": 1912,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "or whatever it is. So I can click this",
      "offset": 1914.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "experiments button and you'll see the",
      "offset": 1916.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "different things that we've loaded up",
      "offset": 1918.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "into this playground are already uh here",
      "offset": 1919.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "within this uh modal. We'll click create",
      "offset": 1921.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and this will actually create the",
      "offset": 1924.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "experiment that you can go to here. And",
      "offset": 1926.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "again this will uh if I click maybe one",
      "offset": 1929.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "out this is allow us to track this over",
      "offset": 1932,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "time. This is what we can also lay in uh",
      "offset": 1935.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "in our CI kind of workflow. So we go",
      "offset": 1937.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "make a change to that prompt make a",
      "offset": 1939.919,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "change to that model. What are the",
      "offset": 1941.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "impacts to uh the scores relative to",
      "offset": 1942.559,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "what we had over history?",
      "offset": 1945.84,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "Sorry, can you\n what is the completeness",
      "offset": 1951.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "score?\n What is the completeness score?",
      "offset": 1953.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Yeah, we can dig into that a little bit.",
      "offset": 1955.519,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "Um this is an LLM as a judge score. So",
      "offset": 1956.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "the idea, right, we're just going to",
      "offset": 1961.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "give it um instructions. The LLM is",
      "offset": 1963.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "going to score the output based on what",
      "offset": 1965.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we've provided in this prompt. Uh you'll",
      "offset": 1968.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "also note I'm just really pulling in the",
      "offset": 1970.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "uh the structure of my data set, right?",
      "offset": 1973.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "And so you obviously can write and and",
      "offset": 1976.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "another thing that Carlos mentioned is",
      "offset": 1978.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "like scoring the score or eval evaling",
      "offset": 1980.159,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the score. So how well is this thing",
      "offset": 1982.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "actually doing um based on the output",
      "offset": 1984.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "that that we are seeing within our",
      "offset": 1987.6,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "application.",
      "offset": 1989.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay, that's really activity one, right?",
      "offset": 1999.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "It's uh reviewing some of that stuff and",
      "offset": 2001.519,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "then it's creating that playground and",
      "offset": 2004.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "showing you all like the the sort of way",
      "offset": 2005.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that we can uh iterate here within brain",
      "offset": 2007.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "trust to create better AI or genai",
      "offset": 2010.559,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "products, right? Like this allows me to",
      "offset": 2014.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "now well okay so maybe this isn't the",
      "offset": 2016.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "right model. Maybe if I do uh maybe I",
      "offset": 2018.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "want to see this new GPT model. I can",
      "offset": 2020.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "run this and now I can see how the the",
      "offset": 2023.279,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "model changed for that particular score",
      "offset": 2026,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "uh how the scores changed when I changed",
      "offset": 2029.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the underlying model. Right? But now I",
      "offset": 2031.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have these you have like all of these",
      "offset": 2033.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "different inputs that could happen to",
      "offset": 2035.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the these applications. It's a way for",
      "offset": 2036.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "us to track and understand when I do go",
      "offset": 2039.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and tweak this thing. There's actual",
      "offset": 2041.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "data behind it, right? This isn't like",
      "offset": 2043.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "vibe check. This isn't um yep, I think",
      "offset": 2045.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that looks good. I looked at this row.",
      "offset": 2047.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "It seems like it's better output. This",
      "offset": 2049.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "is data behind it and we can actually",
      "offset": 2051.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "understand as we tweak that prompt,",
      "offset": 2053.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "tweak that model, how does that impact",
      "offset": 2055.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "our uh our scoring and then again you",
      "offset": 2057.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "can like overlay this within CI and so",
      "offset": 2059.44,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "on.",
      "offset": 2061.599,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Question.",
      "offset": 2064.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Yeah. Um, I pulled the project down and",
      "offset": 2066.48,
      "duration": 9.399
    },
    {
      "lang": "en",
      "text": "uh I have the brain trust",
      "offset": 2070.56,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "and everything just don't know where I",
      "offset": 2076.639,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "do the",
      "offset": 2082.879,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "project that is on my machine to the",
      "offset": 2086.159,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Yeah. Okay. So, you you cloned the repo.",
      "offset": 2089.679,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2092.079,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Can we get a where people are.\n Yeah,",
      "offset": 2095.52,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "please. We can back up considerably. I I",
      "offset": 2098.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "can I can just, you know, with you we",
      "offset": 2102.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "can fix this. Um, you've cloned the",
      "offset": 2104.48,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "repo, correct?\n Yeah.\n Okay.",
      "offset": 2107.2,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "In your enenv.local, do you have",
      "offset": 2111.359,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "aenv.local file?\n I should have.\n Uh,",
      "offset": 2113.599,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "there is aenv.local.ample",
      "offset": 2117.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "file. So, you can copy that into the",
      "offset": 2120.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "env.local. Those are the the keys that",
      "offset": 2122.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "we want to fill in. Have you filled",
      "offset": 2124.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "those in?\n No, I did not.\n Okay. So, if",
      "offset": 2125.28,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "you haven't within your brain trust or",
      "offset": 2130,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "created an API key,",
      "offset": 2132.72,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "am I the only one?\n I'm guessing no.",
      "offset": 2139.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yeah, I don't think so. I think the",
      "offset": 2141.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "internet connection is probably the",
      "offset": 2143.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "biggest thing we're fighting here.\n Yeah,",
      "offset": 2145.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "luckily all this all the instructions",
      "offset": 2147.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "will be available after the workshop.",
      "offset": 2149.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Same with the slides. I'm about to share",
      "offset": 2151.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the the slide link. So tough to update",
      "offset": 2153.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "with the connection, but um at least",
      "offset": 2156.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "seeing Doug go through the same process",
      "offset": 2159.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "will give you an understanding of, you",
      "offset": 2162,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know, what we were hoping you'd have",
      "offset": 2164.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that hands-on experience doing. Um we",
      "offset": 2165.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "don't have too much time to to wait on",
      "offset": 2167.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this specific activity. So I think in a",
      "offset": 2170.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "few minutes we'll keep going and",
      "offset": 2172.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "hopefully you'll be able to set up your",
      "offset": 2175.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "keys so that you can catch up when when",
      "offset": 2178,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you have some time.",
      "offset": 2181.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Yeah, just in the interest of time,",
      "offset": 2183.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "we'll we'll probably move forward, but",
      "offset": 2184.56,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "just to complete that, if you go into",
      "offset": 2186,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "your settings within your project or",
      "offset": 2187.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "excuse me, within your uh brain trust or",
      "offset": 2189.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you'll see API keys. This is where",
      "offset": 2191.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "you're going to create this API key.\n You",
      "offset": 2194.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "did that. Okay, perfect. Create that.",
      "offset": 2197.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "You put that in thatv.local file and",
      "offset": 2199.599,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "then you run pnpm install.\n So that that",
      "offset": 2202.64,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "is my in my home directory, right?",
      "offset": 2205.839,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "That's it's wherever you cloned that",
      "offset": 2210.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "repo.\n So at the root of that uh you put",
      "offset": 2212.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "the the brain trust API key in",
      "offset": 2216.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "yourv.local file\n and you should have a",
      "offset": 2218.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "spot for it already if you're using sort",
      "offset": 2221.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of the template from the example.",
      "offset": 2223.359,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "And then when you run pnpm install",
      "offset": 2226.24,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "again just to highlight this",
      "offset": 2229.68,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "it's actually running this command brain",
      "offset": 2233.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "trust push. So, we're taking the the",
      "offset": 2235.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "resources that we've configured in our",
      "offset": 2238.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in our project and is pushing it via our",
      "offset": 2240.56,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "API key to the brain trust or",
      "offset": 2243.2,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "cool.",
      "offset": 2247.119,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "All right.",
      "offset": 2253.04,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "uh maybe going forward a little bit here",
      "offset": 2266.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to talk about uh the flip side of this",
      "offset": 2269.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "right we've been kind of in the UI for",
      "offset": 2272.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the most part uh in our playground doing",
      "offset": 2274.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that that iteration changing prompts",
      "offset": 2277.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "changing models and so on I think it's",
      "offset": 2279.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "important to understand that we can do a",
      "offset": 2281.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "lot of this via the SDK as well uh we",
      "offset": 2283.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "also have a Python SDK if that's the",
      "offset": 2286.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "kind of uh flavor you're you're most uh",
      "offset": 2288.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "used to using or the language but uh",
      "offset": 2291.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that that top portion here uh is",
      "offset": 2294.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "essentially what we did right in that",
      "offset": 2296.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that install that post install script we",
      "offset": 2298.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "defined our assets in code we defined",
      "offset": 2301.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "scores we defined prompts um we defined",
      "offset": 2303.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "a data set even and then we push that",
      "offset": 2305.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "into our brain trust or that the benefit",
      "offset": 2308.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "here is that again we can use our repo",
      "offset": 2310,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "right we can leverage version control to",
      "offset": 2312.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "ensure that the things that we want uh",
      "offset": 2315.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to change are actually version",
      "offset": 2317.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "controlled alongside of everything else",
      "offset": 2318.4,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "that we're building within that",
      "offset": 2319.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "application so there are really two",
      "offset": 2320.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "modes to actually work with with the",
      "offset": 2322.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "brain trust platform it's its UI or its",
      "offset": 2324,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "SDK. Again, there's really no limits",
      "offset": 2326.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that we place on on the on the user of",
      "offset": 2327.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the platform. It's going to cater to",
      "offset": 2330.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "maybe a different uh persona, a",
      "offset": 2331.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "different use case. Uh but you can use",
      "offset": 2333.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "kind of both of these different things.",
      "offset": 2336.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "The other thing that we haven't done yet",
      "offset": 2338.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "uh that we did uh within the playground",
      "offset": 2340.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "when we ran that experiment, we we ran",
      "offset": 2342.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "those evals, right? We actually",
      "offset": 2344,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "evaluated uh the task against the data",
      "offset": 2345.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "set with our scores. You can also define",
      "offset": 2348.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "evals in code, right? You can define the",
      "offset": 2350.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "eval within your repo and a very similar",
      "offset": 2352.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "command brain trust eval. Now we can",
      "offset": 2355.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "push that up to the brain trust platform",
      "offset": 2358,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and essentially run that same thing",
      "offset": 2360.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "track it in that experiments. I have now",
      "offset": 2362,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "an understanding over time how my uh my",
      "offset": 2364.32,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "evals are performing as I go and change",
      "offset": 2367.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "things all those different things.",
      "offset": 2369.359,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "A little bit more uh insight here you",
      "offset": 2372.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "probably saw with some of that code.",
      "offset": 2374.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "This is the uh the the command we're",
      "offset": 2376.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "going to run brain trust push. And you",
      "offset": 2378.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "essentially give it either the name of a",
      "offset": 2380.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "file that have eval or you can give it",
      "offset": 2382.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the name of a folder uh as long as your",
      "offset": 2384.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "uh your files have eval.ts as their",
      "offset": 2387.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "naming convention. And we're just going",
      "offset": 2390.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to go run those evals in that parallel",
      "offset": 2392.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "fashion that you saw within the UI.",
      "offset": 2394.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "A couple things maybe I mentioned",
      "offset": 2398.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "earlier but just important to highlight.",
      "offset": 2400,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "You should do this when you want source",
      "offset": 2401.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "controlled prompt versioning. You want",
      "offset": 2402.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "consistent usage across your your",
      "offset": 2404.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "different environments and you also want",
      "offset": 2406.56,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "to leverage online scoring.",
      "offset": 2408.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Uh mentioned this obviously the the",
      "offset": 2413.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "eval.ts that's essentially what the SDK",
      "offset": 2416.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is looking for uh when we go and run",
      "offset": 2418.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "those evals. It makes it really easy to",
      "offset": 2420.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "run a a larger subset of those without",
      "offset": 2422.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "specifying each single file that you",
      "offset": 2425.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "want to go run those eval. Um but you",
      "offset": 2427.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "can see and I'll I'll let me jump into",
      "offset": 2430.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the actual activity. You can see the",
      "offset": 2432.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "eval that we've created. It's it's what",
      "offset": 2434.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "we've been talking about, right? The",
      "offset": 2436.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "three ingredients that we need. We need",
      "offset": 2437.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that task, we need that data set, and",
      "offset": 2439.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then we need at least uh one score there",
      "offset": 2441.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "as well.",
      "offset": 2443.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "How do you bootstrap a data set like",
      "offset": 2446.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "with multiple examples? Is there given",
      "offset": 2447.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "feedback in loop or is an LLM being used",
      "offset": 2449.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to create?",
      "offset": 2452.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Yeah, so the question was like how do",
      "offset": 2454.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you bootstrap a data set? Um I think",
      "offset": 2455.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it's a good question. I I think you",
      "offset": 2458.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "could certainly start with synthetic",
      "offset": 2460,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "data or you could even start with um you",
      "offset": 2461.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "know you you release a feature right",
      "offset": 2464.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you're going to you're going to start",
      "offset": 2466.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "logging that feature this is another",
      "offset": 2467.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thing that we haven't yet covered but",
      "offset": 2469.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you can actually use the logs from that",
      "offset": 2471.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to add to the data set so now you have",
      "offset": 2473.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "actual real life data thing to not do is",
      "offset": 2475.44,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "wait until you have 100 200 like you",
      "offset": 2478.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "have this golden data set if you think",
      "offset": 2481.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "back to that matrix that Carlos was",
      "offset": 2483.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "showing there are the different ways in",
      "offset": 2485.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "which we could start to think about",
      "offset": 2487.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "improving in the application based on",
      "offset": 2488.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what we observe. So start with something",
      "offset": 2490.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "small and again it could be synthetic",
      "offset": 2492.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "but then you can once you start to",
      "offset": 2494.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "evaluate it then you have a different um",
      "offset": 2495.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you have different inputs on what you",
      "offset": 2499.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "need to do to go and improve an a scorer",
      "offset": 2500.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "or improve the application. But it's",
      "offset": 2503.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "it's really I think maybe up to you the",
      "offset": 2505.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the like the best practice or the thing",
      "offset": 2508.319,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "that I would think about is like don't",
      "offset": 2509.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "don't stop yourself because you only",
      "offset": 2511.119,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "have a small subset of data.",
      "offset": 2513.2,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 2520.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Yeah.\n So, for the for the test you're",
      "offset": 2522.56,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "running, if you're using an LLM as a",
      "offset": 2526.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "judge, so like for the percent",
      "offset": 2529.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "completeness score, you're using GPT41",
      "offset": 2532.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "as a judge.",
      "offset": 2535.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "That's subjectively scoring the test",
      "offset": 2537.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "that you're running, right? So, like\n for",
      "offset": 2541.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the for the for two runs that are that",
      "offset": 2544,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "one after the other, you could end up",
      "offset": 2546.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "with different scores, right? If you're",
      "offset": 2548.24,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "using like LL as a judge to run.",
      "offset": 2551.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 2555.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so the question was like, would I get",
      "offset": 2557.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "different scores?",
      "offset": 2559.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Um, because I'm using an LLM to do this,",
      "offset": 2561.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "right? And it's not really",
      "offset": 2564.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "deterministic. Um, I think that's the",
      "offset": 2565.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "the reason why you would use a better",
      "offset": 2568.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model so you don't see something like",
      "offset": 2570.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that. Um, I don't know, Carlos, you have",
      "offset": 2572.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "any other thoughts there?\n Yeah, I mean,",
      "offset": 2575.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I think you're speaking to the nature of",
      "offset": 2577.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "an LLM being non-deterministic. So, yes,",
      "offset": 2579.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "there may be some variability. What we",
      "offset": 2582.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "see our customers do, especially with",
      "offset": 2584.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "the SDK, is do trial evals. So, you will",
      "offset": 2586.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "run it maybe five times and then take",
      "offset": 2590.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the average of those. So, there are",
      "offset": 2592.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "things that you can do to try to beat",
      "offset": 2593.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that, but it is the nature of the beast",
      "offset": 2595.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and you have to learn to work with it.",
      "offset": 2597.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "And then the other thing is how are you",
      "offset": 2600.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "scoring like a percent completeness if",
      "offset": 2604,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the task of the LLM is like to judge",
      "offset": 2606.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like put it in categories like excellent",
      "offset": 2609.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "good are you mapping those to scores or",
      "offset": 2611.92,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "yeah so I think that the question is and",
      "offset": 2616.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "if I come to the score and you look at",
      "offset": 2620.079,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "the completeness so the LLM here yeah it",
      "offset": 2621.92,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "has to decide based again on like the",
      "offset": 2626.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "criteria that we give it um if it comes",
      "offset": 2628.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "up with excellent that maps to one and",
      "offset": 2631.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "so on. But again, this the score that it",
      "offset": 2634.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "gives has to be between zero and one.",
      "offset": 2636.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Yeah,",
      "offset": 2640.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it's really helpful when you're using an",
      "offset": 2641.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "LLM as a judge to go through the brain",
      "offset": 2644.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "trust logs and read the rationale. So,",
      "offset": 2646.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it will explain why it chose, you know,",
      "offset": 2648.8,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "100% or 75% and you can use that to tune",
      "offset": 2651.92,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "the LM as a judge and improve it. um",
      "offset": 2655.68,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "it likely will not, you know, you don't",
      "offset": 2661.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "want it to say 100% for everything,",
      "offset": 2662.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "right? If if that's the case, you need",
      "offset": 2665.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "to improve your evals. And even if it's",
      "offset": 2667.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "saying, you know, 30% for everything, it",
      "offset": 2670.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "doesn't necessarily mean that it's",
      "offset": 2673.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "performing horribly. Uh what really",
      "offset": 2674.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "matters is the baseline that you're",
      "offset": 2676.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "comparing it to. You know, how did it",
      "offset": 2678.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "perform yesterday on these scores on",
      "offset": 2680.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "this data set? Uh you shouldn't be",
      "offset": 2682.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "comparing just, you know, it needs to be",
      "offset": 2684.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "80%. Not necessarily. It matters what",
      "offset": 2686.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "happened yesterday, what you're",
      "offset": 2689.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "comparing to previously.",
      "offset": 2690.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yeah. And this is where it becomes",
      "offset": 2694.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "really beneficial to to be able to",
      "offset": 2695.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "actually drill into what happened within",
      "offset": 2697.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that task. Uh so being able to not only",
      "offset": 2700,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "understand the the calls that the LLM",
      "offset": 2702.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "makes, the tools that it invokes, but",
      "offset": 2704.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "actually drilling into those scores that",
      "offset": 2706.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we're using and then like Carlos",
      "offset": 2708,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "mentioned, what is the rationale that it",
      "offset": 2709.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "gave uh to give it a good score here?",
      "offset": 2711.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Does that make sense? This becomes again",
      "offset": 2713.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "another way of like this is the human",
      "offset": 2716.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "review portion or part of building a",
      "offset": 2717.839,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "genai app.\n Yeah.\n Does brain tree offer",
      "offset": 2720.24,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "any kind of like optimization?",
      "offset": 2725.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "So you have your email down you got tons",
      "offset": 2728.48,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "of different.",
      "offset": 2731.52,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "Yeah, that's a good question and it's",
      "offset": 2739.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "something that we're thinking a lot",
      "offset": 2740.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "about is how can we add LLM to optimize",
      "offset": 2741.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "this process for you. Not just for",
      "offset": 2745.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "prompt like you mentioned, but also for",
      "offset": 2747.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "data sets and for scores. We're one to",
      "offset": 2749.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "two weeks away from releasing our first",
      "offset": 2753.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "version of this. Uh we're going to call",
      "offset": 2755.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it loop and it will do exactly what",
      "offset": 2757.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you're saying. It will help you optimize",
      "offset": 2759.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the prompts and improve your emails.",
      "offset": 2761.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Just to build on that, imagine like this",
      "offset": 2763.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "um this feature loop. it has access to",
      "offset": 2766.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "previous results as it's doing it. So it",
      "offset": 2768.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "understands when it makes that change,",
      "offset": 2771.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "is it better than it was previously. So",
      "offset": 2773.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it's it's it's sort of like that agentic",
      "offset": 2775.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "type workflow where it has access to",
      "offset": 2778.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "tools, but it's able to uh to iterate on",
      "offset": 2779.68,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "that prompt and run those experiments uh",
      "offset": 2783.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "with you of course like in the middle to",
      "offset": 2786.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to prompt it to do different things, but",
      "offset": 2787.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it has access to those previous exper",
      "offset": 2789.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "experiments and the results of that. So",
      "offset": 2791.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it knows like the general direction it",
      "offset": 2793.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "needs to go to get improvements.\n Yeah.",
      "offset": 2795.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "So how do you build that?",
      "offset": 2798.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "We use brain trust. Yeah. Exactly. It's",
      "offset": 2801.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a way of dog fooding. So the question",
      "offset": 2804.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "was how do we evalow our AI feature? And",
      "offset": 2806.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's you know of course we have to use",
      "offset": 2808.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "brain trust and it's it's honestly",
      "offset": 2809.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "really cool to to look at the project",
      "offset": 2811.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and see all the logs coming in and and",
      "offset": 2813.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "looking at the scores that we've chosen",
      "offset": 2816,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to go with. Um yeah, brain trust is",
      "offset": 2817.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "really helping and it was actually",
      "offset": 2819.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "something that Anker our CEO has talked",
      "offset": 2821.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "about the process of of actually getting",
      "offset": 2824.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to a point where we were excited to",
      "offset": 2827.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "release something like this. Previously",
      "offset": 2829.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "the models were not performing to the",
      "offset": 2831.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "level that we were needing them to",
      "offset": 2834.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "perform to. So every few months he would",
      "offset": 2836.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "run a new benchmark on this specific use",
      "offset": 2838.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "case and it wasn't until you know the",
      "offset": 2840.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "last month that a model finally uh",
      "offset": 2842.48,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "reached that that expectation.",
      "offset": 2845.2,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Yeah,\n they gave me a mic, so hopefully",
      "offset": 2850,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you can hear me. Um, cascading off the",
      "offset": 2851.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "gentleman in the front's question around",
      "offset": 2854.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "um, the subjectivity of using the LLMs",
      "offset": 2857.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "as a judge in these types of cases,",
      "offset": 2860.079,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "do you offer any way to gain access to",
      "offset": 2863.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "that rationale programmatically such",
      "offset": 2866.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that you could evaluate the thought",
      "offset": 2868.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "process of the LLM as it's doing? It's",
      "offset": 2870.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "kind of meta like onestep review, but",
      "offset": 2872.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "adding in that second layer where you",
      "offset": 2874.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "could identify weak spots perhaps if",
      "offset": 2876.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "there's something that's hyper workflow",
      "offset": 2878.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "oriented or has a very strict process",
      "offset": 2881.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "you're looking for the LLM to follow.",
      "offset": 2882.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Yeah, I mean I think you probably saw",
      "offset": 2886.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "when I highlighted here, correct me or",
      "offset": 2887.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "thumbs up if you saw this or Yeah. Okay.",
      "offset": 2890.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "This is all accessible via via API. Uh",
      "offset": 2892.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "coming back down here. So like the the",
      "offset": 2896.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "rationale that the LLM gave, we could",
      "offset": 2899.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "certainly build something on on top of",
      "offset": 2901.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "uh the these ration uh and then generate",
      "offset": 2904.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "you know again like eval type of",
      "offset": 2907.68,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "workflow.",
      "offset": 2910,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Cool.",
      "offset": 2915.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah.\n Yeah. Question.\n So I'm just",
      "offset": 2918.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "curious like\n there's a mic if you'd like",
      "offset": 2921.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "it. Thank you.",
      "offset": 2924.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Yeah. So, uh yeah, I'm I'm just curious",
      "offset": 2927.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like how should we build our confidence",
      "offset": 2928.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "around like you know the result of OM as",
      "offset": 2931.44,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "a judge because I mean uh you know how",
      "offset": 2934.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "do we trust the result because is after",
      "offset": 2937.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "all the model like evaluate the data set",
      "offset": 2940.24,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "and it's could like we could get like",
      "offset": 2944.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "you know maybe a good result but",
      "offset": 2946.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "actually it's like maybe large model is",
      "offset": 2949.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just overconfidence or something like",
      "offset": 2952.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that. It's like we need to evaluate or",
      "offset": 2953.599,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "eval results using like humans like at",
      "offset": 2956.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the beginning so that we can build a",
      "offset": 2959.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "confidence like yeah just curious any",
      "offset": 2961.359,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "experience like you have here.\n Yeah",
      "offset": 2964.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "definitely that's a great question. So",
      "offset": 2966.559,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "um I guess everybody heard don't need to",
      "offset": 2968.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "repeat. Um",
      "offset": 2969.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "I think there's two things that you can",
      "offset": 2972.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "do. One that you mentioned which is",
      "offset": 2973.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "involving a human reviewing that LM as a",
      "offset": 2975.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "judge and",
      "offset": 2978.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "confirming that is it's thinking in the",
      "offset": 2980.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "right way. It's outputting the correct",
      "offset": 2982.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "score. Um another approach that you",
      "offset": 2984.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "could do as well as the human in the",
      "offset": 2988.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "loop is uh using deterministic scores.",
      "offset": 2989.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "So full coding functions that are trying",
      "offset": 2993.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to grade the same type of criteria using",
      "offset": 2995.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "regular expressions or some other logic",
      "offset": 2998.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that and you can approximate right. So",
      "offset": 3001.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "if the LM is a judge is giving a zero",
      "offset": 3003.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "but the you know the deterministic code",
      "offset": 3005.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "score is giving a way higher score then",
      "offset": 3008.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you know that there's something that",
      "offset": 3010.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "needs attention needs to be fixed. the",
      "offset": 3011.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "matrix as well that we showed at the",
      "offset": 3013.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "beginning that pointed to you know",
      "offset": 3015.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "should you improve your evals or improve",
      "offset": 3016.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "your AI app that's also a great resource",
      "offset": 3018.64,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "yes thank you",
      "offset": 3023.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "um how are you guys thinking about the",
      "offset": 3026.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "role if you think there is a role um for",
      "offset": 3028.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "traditional machine learning models in",
      "offset": 3030.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "eval",
      "offset": 3032.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "totally deterministic code and then on",
      "offset": 3034.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the other you have LLM as a judge do you",
      "offset": 3036.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think there's kind of a middle ground",
      "offset": 3038.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for things like intent classification",
      "offset": 3040.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "models entity",
      "offset": 3042.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "uh recognition, uh sentiment",
      "offset": 3043.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "classification and clustering and and",
      "offset": 3046,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "kind of more traditional machine",
      "offset": 3047.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "learning approaches that kind of sit",
      "offset": 3049.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "somewhere in between, you know, the the",
      "offset": 3051.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the totally deterministic versus totally",
      "offset": 3053.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "non-deterministic spectrum of code",
      "offset": 3055.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "versus LLMs. Like do do you think that",
      "offset": 3058.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "there's a role for those type of models",
      "offset": 3059.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "and what do you think that looks like?",
      "offset": 3061.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Yeah, that's a good question. Um",
      "offset": 3064.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I think it's still to be determined how",
      "offset": 3068.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "this all shakes out. There are some",
      "offset": 3070.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "customers, companies that we talk to",
      "offset": 3074.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that are going full deterministic. They",
      "offset": 3075.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "don't use LM as a judge and then there",
      "offset": 3077.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "are others that are very much going in",
      "offset": 3079.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the LLM as a judge route. And I think",
      "offset": 3080.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the reason that there's a split is",
      "offset": 3083.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "because they both work. Uh so you know I",
      "offset": 3085.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "don't know I don't know how this will",
      "offset": 3088.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "eventually shake out if we'll reach in",
      "offset": 3089.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "the middle or if you know determinism",
      "offset": 3091.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "will win. Um, what I can say though is",
      "offset": 3093.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that it's highly dependent on the use",
      "offset": 3095.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "case, the problem that you're solving",
      "offset": 3097.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and experiment with both and then you",
      "offset": 3099.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "can determine which one is working best.",
      "offset": 3101.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I guess those are like also largely",
      "offset": 3105.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "codebased, right? The the things that",
      "offset": 3106.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you're talking about and so maybe they",
      "offset": 3108.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "lean a little bit more toward towards",
      "offset": 3110.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that.\n Yeah, I mean I'd say they're still",
      "offset": 3111.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "kind of neural approaches\n are still",
      "offset": 3114.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "entirely deterministic, but thank you.",
      "offset": 3117.44,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "Yeah. No, I got you. Um it's it's sort",
      "offset": 3121.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "of like that that middle ground. Um",
      "offset": 3123.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "yeah, I don't I don't have a great",
      "offset": 3127.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "answer for you. I do think uh using",
      "offset": 3128.319,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "brain trust uh you you do have the",
      "offset": 3131.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "ability to at least configure both of",
      "offset": 3133.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "these the LM as a as a judge and then",
      "offset": 3135.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the scorer and then you can again using",
      "offset": 3137.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the the human review process find the",
      "offset": 3140.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "ones that that actually map to the the",
      "offset": 3143.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "right output the best and then that's",
      "offset": 3146.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "how you start to build your application.",
      "offset": 3148.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "But it's it's a really good question.",
      "offset": 3149.92,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "Thank you.",
      "offset": 3152.24,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "How's the activity going? I know we are",
      "offset": 3156.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "getting you know last 25 minutes of of",
      "offset": 3159.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the session. We still have two more uh",
      "offset": 3162.24,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "little slide chunks to go through. So",
      "offset": 3165.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "maybe in you know two minutes or now we",
      "offset": 3169.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "could move to the slides and then keep",
      "offset": 3171.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "it going. Again this will all be",
      "offset": 3174.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "available after uh so feel free to keep",
      "offset": 3175.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "working on it.",
      "offset": 3178.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah, maybe just really quick, we could",
      "offset": 3180,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "run the the eval just from here. You",
      "offset": 3181.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "could see um",
      "offset": 3183.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "we're actually going to take the",
      "offset": 3186.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "the eval defined here, right? So, we",
      "offset": 3189.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "have our our task, we have our scores,",
      "offset": 3192.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "then we have our data set essentially",
      "offset": 3195.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "what we just did within the UI pushing",
      "offset": 3196.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that into the brain trust platform. Uh",
      "offset": 3199.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then you can even see like this is",
      "offset": 3201.28,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "where it's running right now.",
      "offset": 3202.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "So again, being able to do these in a",
      "offset": 3207.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "couple different ways, either via the",
      "offset": 3210.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the SDK or from the the playground",
      "offset": 3211.52,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "itself.",
      "offset": 3214.079,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I think you explained this already and",
      "offset": 3220.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "maybe I was distracted by the Wi-Fi, but",
      "offset": 3222.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "how do I think about the difference",
      "offset": 3224.4,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "between the playground and the",
      "offset": 3225.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "experiment?\n Yeah, that's that's a great",
      "offset": 3226.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "question. Let's see if we can just",
      "offset": 3229.119,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "quickly go back to the slide.",
      "offset": 3230.16,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Playground you could think of as quick",
      "offset": 3241.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "iteration",
      "offset": 3242.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "experiment. So a playground ephemeral",
      "offset": 3244.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "right experiments long lived historical",
      "offset": 3246.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "analysis",
      "offset": 3249.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if that helps answer your question. Um",
      "offset": 3251.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "they're becoming more and more the same.",
      "offset": 3254,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "You know historically the experiments",
      "offset": 3256.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "had a bit more bells and whistles. So I",
      "offset": 3257.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you know typically teams would gravitate",
      "offset": 3260.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "towards the experiments but we found",
      "offset": 3261.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that they really liked the quick",
      "offset": 3263.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "iteration they really liked using the UI",
      "offset": 3266,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and so we started beefing it up and now",
      "offset": 3268.4,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "they become pretty much the same. So so",
      "offset": 3270.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "yeah playground more ephemeral quick",
      "offset": 3273.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "iteration you want to save the work that",
      "offset": 3275.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you've done to an experiment so that you",
      "offset": 3278.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "can later review it and see the scores",
      "offset": 3280.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "update and change over over time.\n I",
      "offset": 3283.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "think it throws me off. So when I do an",
      "offset": 3286.72,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "evaluate",
      "offset": 3288.319,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "in my text editor, I should use the UI",
      "offset": 3291.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "playground UI for that.\n Yes. And remote",
      "offset": 3295.04,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "evals which will allow you to define via",
      "offset": 3298.079,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "the SDK the eval but then expose it into",
      "offset": 3302.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the playground. So some it's it's like",
      "offset": 3305.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the bonus activity in the document at",
      "offset": 3307.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the very end. So maybe you should check",
      "offset": 3310.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that out and we won't have time in this",
      "offset": 3312.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "session but if you come to the one at",
      "offset": 3314.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "3:30 we will.",
      "offset": 3315.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Any other questions?",
      "offset": 3319.359,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "Okay, cool. So, moving into lecture",
      "offset": 3323.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "three. Uh, so this is, you know, once",
      "offset": 3326.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you've finished development, it's",
      "offset": 3328.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "reaching customers. You're in",
      "offset": 3330.559,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "production, right? Now, what do you do?",
      "offset": 3332.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Well, the the important thing is",
      "offset": 3334.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "logging. You want some observability.",
      "offset": 3335.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "You want to understand what's going on.",
      "offset": 3337.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "How are they using it? Are there any",
      "offset": 3339.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "gaps? Are they unhappy?",
      "offset": 3341.04,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "It can help you debug. uh a lot faster.",
      "offset": 3344.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "It can allow you to measure quality",
      "offset": 3348.24,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "instantly on that live traffic. You can",
      "offset": 3350.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "turn those production traces, what",
      "offset": 3355.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you're logging, you can turn it into a",
      "offset": 3357.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "data set and bring it back into the",
      "offset": 3359.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "playground, keep improving the prompt",
      "offset": 3361.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and you know it allows a lot of",
      "offset": 3364.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "nontechnical people to understand what",
      "offset": 3366.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the end user is thinking. So you can",
      "offset": 3369.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "close this feedback loop. We have a lot",
      "offset": 3371.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "of PMsmemes",
      "offset": 3373.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "using brain trust and going through the",
      "offset": 3376.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "logs and and looking at that user",
      "offset": 3377.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "feedback to understand what gaps and and",
      "offset": 3379.92,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "improvements may exist.",
      "offset": 3382.64,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "So how do you log into brain trust? Uh",
      "offset": 3387.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "well this is done via the SDK right it",
      "offset": 3389.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "needs to plug into your production code.",
      "offset": 3391.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Uh so these are some of the the steps",
      "offset": 3394.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "here or the the tools that you can use.",
      "offset": 3396.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "So you know you need you need to",
      "offset": 3398.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "initialize a logger that will",
      "offset": 3400,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "authenticate into brain trust. It will",
      "offset": 3401.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "connect it to a project. So now your",
      "offset": 3403.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "logs will go to a specific project in",
      "offset": 3405.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "brain trust. Um some great ways to to",
      "offset": 3407.119,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "get started with really one line of code",
      "offset": 3411.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is to wrap your LLM client. So you can",
      "offset": 3412.88,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "use wrap open AAI uh around that LLM",
      "offset": 3415.76,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "client and now any communication will",
      "offset": 3420.4,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "get logged with your prompt response.",
      "offset": 3423.359,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "also uh metrics. So how many tokens were",
      "offset": 3427.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "sent back and forth, the latency, all",
      "offset": 3430.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "errors, everything just by adding that",
      "offset": 3432.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that wrap open AAI. You can do the same",
      "offset": 3434.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "with Versell AI SDK. Uh or you could use",
      "offset": 3436.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "OTEL. So we also integrate with OTEL. Um",
      "offset": 3440.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so if you want to go that route, it's",
      "offset": 3443.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "also available.",
      "offset": 3444.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "If you want to log and trace arbitrary",
      "offset": 3446.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "functions, we also support that. You can",
      "offset": 3450.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "just use a trace decorator around the",
      "offset": 3452,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "function. um really helpful for keeping",
      "offset": 3454.559,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "track of any uh functions that are",
      "offset": 3458.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "helpful to to understand and keep track",
      "offset": 3462.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of. And then if you need to add",
      "offset": 3464.559,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "additional information like metadata,",
      "offset": 3466.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "you can use span.log. So it's it's very",
      "offset": 3468.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "capable, very flexible, but there's",
      "offset": 3470.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "still these, you know, oneline code ways",
      "offset": 3473.119,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "to to get started.",
      "offset": 3475.52,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "So now that you're pushing all of your",
      "offset": 3481.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "logs into Brain Trust, you're capturing,",
      "offset": 3484.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you're observing real user traffic, now",
      "offset": 3486.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we're going to get into that, you know,",
      "offset": 3489.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "online scoring piece as opposed to",
      "offset": 3490.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "offline, right? So online is measuring",
      "offset": 3492.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the quality of live traffic. So you can",
      "offset": 3495.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "decide how many logs that are coming in",
      "offset": 3497.92,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "will get evaluated and scored. Uh it",
      "offset": 3501.599,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "could be 100%, it could be 1%, it's up",
      "offset": 3504.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to you. This allows you to set early",
      "offset": 3507.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "regression alerts. So if it starts",
      "offset": 3509.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "dropping below, you know, 70%, 60%,",
      "offset": 3511.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "ultimately it depends on the score that",
      "offset": 3514.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you're using and what you've established",
      "offset": 3516.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "as the baseline. But if it starts",
      "offset": 3518.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "dropping below a critical amount, you",
      "offset": 3520.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "can set up alerts and notify the correct",
      "offset": 3522.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "team. Uh you can also AB test different",
      "offset": 3523.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "prompts. You can set up tagging and",
      "offset": 3526.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "understand, oh, this trace coming from",
      "offset": 3528.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this user is from prompt A versus this",
      "offset": 3530.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "one from prompt B. And you can compare",
      "offset": 3534,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "uh the the grades, right? that the score",
      "offset": 3536.079,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "results coming back in.",
      "offset": 3539.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So, this is great for just improving",
      "offset": 3542.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "feedback, moving quickly, and",
      "offset": 3544.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "understanding if there's been a drop in",
      "offset": 3546,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "quality.",
      "offset": 3547.52,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "How do you create an online scoring",
      "offset": 3551.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "rule? Well, everything is done via the",
      "offset": 3553.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "UI. You go to your project",
      "offset": 3555.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "configurations, click on online scoring,",
      "offset": 3557.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and then you can add your rule. This is",
      "offset": 3559.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "where you'll define what scores you want",
      "offset": 3561.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "to be used on that live traffic. And",
      "offset": 3564.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "then uh crucially that sampling rate. So",
      "offset": 3566.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "maybe at the beginning you start with a",
      "offset": 3570,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "lower sampling rate and then you can",
      "offset": 3571.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "increase it once you trust the metrics",
      "offset": 3572.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "coming in. You can also choose what span",
      "offset": 3575.44,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "you want this online score to run on. So",
      "offset": 3578.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it defaults to the root span but you can",
      "offset": 3582.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "get more granular and specify you know I",
      "offset": 3584.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "want this nested child span to be uh",
      "offset": 3586.72,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "scored.",
      "offset": 3589.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Once you start collecting these logs,",
      "offset": 3596,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "collecting these online scores,",
      "offset": 3598.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "oftentimes teams want to view them in",
      "offset": 3600.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "interesting ways and customize the",
      "offset": 3603.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "lenses on those logs. So that's where",
      "offset": 3605.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "custom views come in. You can apply",
      "offset": 3607.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "filters, sorts, you can customize",
      "offset": 3609.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "columns on the logs with whatever",
      "offset": 3612.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "information you'd like. And now you can",
      "offset": 3615.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "start saving those views and making them",
      "offset": 3618.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "available for the rest of your team to",
      "offset": 3621.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just come to the logs and select, oh, I",
      "offset": 3623.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "want to go to, you know, the logs under",
      "offset": 3625.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "50% view or, you know, their own custom",
      "offset": 3628.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "view that they've made that's specific",
      "offset": 3631.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "to what they care about. Uh, so it's",
      "offset": 3633.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it's a great way of collaborating and",
      "offset": 3634.799,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and uh speeding up the process of",
      "offset": 3636.799,
      "duration": 7.241
    },
    {
      "lang": "en",
      "text": "viewing the important things to you.",
      "offset": 3639.599,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "Great. So, you know, we went through the",
      "offset": 3645.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "slides. Now, we would jump back in to",
      "offset": 3647.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 3650.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to the activity document. There we can",
      "offset": 3651.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "look at the actual uh code, see where",
      "offset": 3654.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "the the logging is being captured in our",
      "offset": 3657.119,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "files, spin up the application. So, you",
      "offset": 3660.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "can actually view the application in",
      "offset": 3663.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "your dev environment, interact with it,",
      "offset": 3665.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "and you'll see those prompts and outputs",
      "offset": 3667.44,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "being logged in Brain Trust. Uh, so if",
      "offset": 3671.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "if you've gotten that far and you have",
      "offset": 3673.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the the dependencies installed, then I",
      "offset": 3675.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "would recommend doing a PNPM dev and now",
      "offset": 3677.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you'll have your application up and",
      "offset": 3680.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "running, interact with it a few times",
      "offset": 3682.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and uh you'll see that populate in your",
      "offset": 3684.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "project logs.",
      "offset": 3687.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Once you do that, then you can go to",
      "offset": 3689.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "your online scoring settings, set up a",
      "offset": 3691.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "rule, and you can keep interacting with",
      "offset": 3694.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the app. And now you'll see it um",
      "offset": 3696.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "populate with that online score that you",
      "offset": 3698.559,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "just enabled.",
      "offset": 3700.64,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "Maybe quick example for for those that",
      "offset": 3708.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "are still having sort of Wi-Fi issues.",
      "offset": 3709.92,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "Uh come back down here.",
      "offset": 3712.799,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "So, I'm going to come and uh spin this",
      "offset": 3719.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "up. So, as Carlos mentioned, uh PNPMdev,",
      "offset": 3721.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "this is going to spin up that server on",
      "offset": 3724.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "localhost 3000. Uh you should see",
      "offset": 3726.96,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "something that looks like this.",
      "offset": 3731.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Uh there's a few things that you can",
      "offset": 3734.64,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "just click these. This is sort of like",
      "offset": 3735.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "the easy button to get going. Uh this is",
      "offset": 3737.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the the GitHub URL. It's again it's",
      "offset": 3739.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "looking for the the commits that have",
      "offset": 3741.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "been made since the last release and",
      "offset": 3742.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it's going to summarize uh again using",
      "offset": 3745.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "the the prompts that we've configured",
      "offset": 3747.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here and then start to categorize them.",
      "offset": 3748.799,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "But now the the interesting part of this",
      "offset": 3751.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "if you come back into the brain trust",
      "offset": 3754.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "platform",
      "offset": 3755.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "is if you look at the logs. So this is",
      "offset": 3757.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "what just happened on the brain trust",
      "offset": 3760,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "side. Uh so we sort of have this top",
      "offset": 3762,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "level uh trace the generate change log",
      "offset": 3764.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "request and then essentially the the the",
      "offset": 3766.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "tool calls you can think of as right",
      "offset": 3768.799,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we're getting the commits we're getting",
      "offset": 3770.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the latest release. We're fetching those",
      "offset": 3771.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "commits we're then loading that prompt.",
      "offset": 3773.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "So we're actually loading the prompt",
      "offset": 3775.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "from from brain trust and then you can",
      "offset": 3777.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "start to you know you can click through",
      "offset": 3779.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "a lot of these and as Carlos mentioned",
      "offset": 3780.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "when you use those wrappers you get all",
      "offset": 3782.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "of this sort of goodness out of the box",
      "offset": 3784.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "right so what are the the number of",
      "offset": 3785.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "prompt tokens what is the estimated cost",
      "offset": 3786.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this is uh this becomes really helpful",
      "offset": 3789.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "as you start to monitor over time right",
      "offset": 3791.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "uh probably not set up yet because we",
      "offset": 3794.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "don't have uh too much going on but like",
      "offset": 3796.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "actually understanding what does that",
      "offset": 3798,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "token uh amount look like over time what",
      "offset": 3799.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "does the cost look like over time as we",
      "offset": 3801.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "change models uh and so on. But that's",
      "offset": 3803.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "how really easy this is. And maybe just",
      "offset": 3806,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to complete that that loop, if you come",
      "offset": 3808.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "over to",
      "offset": 3810.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the uh resource, not the resources, the",
      "offset": 3812.079,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "app generate route.ts, you'll start to",
      "offset": 3815.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "see some of this stuff. So, I'll just",
      "offset": 3817.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "highlight a couple things. We're",
      "offset": 3818.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "wrapping this SDK AI SDK model. So, this",
      "offset": 3820.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "again is is how we're really getting all",
      "offset": 3824.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of that that that metrics and uh it's",
      "offset": 3825.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it's really allowing for us to log a lot",
      "offset": 3828.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "of that information with very little",
      "offset": 3830.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "lift from from ourselves as developers.",
      "offset": 3832.079,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "But then you also have the ability to uh",
      "offset": 3834.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "configure things in a different way. So",
      "offset": 3837.039,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "maybe we have um different inputs or",
      "offset": 3838.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "different outputs that we want to",
      "offset": 3840.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually log in a particular span or",
      "offset": 3841.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually we want to log metadata, right?",
      "offset": 3843.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "This becomes really powerful when we",
      "offset": 3845.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "want to actually go into those views,",
      "offset": 3846.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "right? And we can actually start to",
      "offset": 3849.2,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "filter these things down. We can even",
      "offset": 3850.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "filter by that metadata. So uh this is",
      "offset": 3851.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "where again you can hit the easy button.",
      "offset": 3854.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we're going to wrap our our LLM client",
      "offset": 3856.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "with those SDKs or we can actually get a",
      "offset": 3858.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "little bit more detailed and start to",
      "offset": 3861.119,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "log uh the particular input and output",
      "offset": 3862.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "information that we want metadata and",
      "offset": 3865.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "now we can sort of uh set these",
      "offset": 3866.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "different things. So if I come out here",
      "offset": 3868.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "uh I can even create uh actually when we",
      "offset": 3871.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "look when we add in the scores here we",
      "offset": 3873.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "can create filters based on those",
      "offset": 3875.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "scores. So I want to create a view that",
      "offset": 3877.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "says hey anytime my completeness score",
      "offset": 3878.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "goes below 50% I want to create a view",
      "offset": 3880.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "for this. This is going to enable my",
      "offset": 3882.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "human reviewers to go in and actually",
      "offset": 3884.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "understand that. And then if you look up",
      "offset": 3886.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "here in the top right, we can actually",
      "offset": 3888,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "add this span to a data set really",
      "offset": 3889.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "easily. So we find this thing, right?",
      "offset": 3890.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "That'll actually add a lot of value in",
      "offset": 3893.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that offline eval type of process. Click",
      "offset": 3894.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this. Now we have a net new row in that",
      "offset": 3897.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "data set that now adds a lot of value,",
      "offset": 3899.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "right? This is sort of like that that",
      "offset": 3901.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "feedback loop, right? We've we've done",
      "offset": 3902.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that offline eval type of work. We have",
      "offset": 3904.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uh found the right prompt, the right",
      "offset": 3907.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "model, all these different things. It's",
      "offset": 3908.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "in production. We are logging it. Now",
      "offset": 3910.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we're sort of um understanding that",
      "offset": 3912.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "maybe in a human review type of way. Add",
      "offset": 3913.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that span of the data set. This adds",
      "offset": 3915.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "again to the the offline type of",
      "offset": 3917.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "portion. Again, you just see this like",
      "offset": 3919.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this sort of flywheel effect of creating",
      "offset": 3921.2,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "really powerful Genai apps.",
      "offset": 3922.88,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "Yes.\n Is there an uh generated for this",
      "offset": 3931.119,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "online log as well? Like as we ran it,",
      "offset": 3934.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "is there an eval created for it? and do",
      "offset": 3938,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we add it to the data set based on if",
      "offset": 3939.839,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the eval score is good because we don't",
      "offset": 3941.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "want like bad examples in the data set.",
      "offset": 3943.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "That's one way of thinking it. You don't",
      "offset": 3946.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "need to run an eval as the user's",
      "offset": 3948,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "interacting with it. That's what the",
      "offset": 3950.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "online scoring does. So once you set up",
      "offset": 3952.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the online scoring rule, it'll output a",
      "offset": 3954.319,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "score based on the judge that you've",
      "offset": 3957.44,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "chosen as the the online scoring rule.",
      "offset": 3960.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Um and exactly what you said, right? You",
      "offset": 3964.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "could either filter it and select the",
      "offset": 3966.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "good responses, add those to a data set",
      "offset": 3969.119,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "or vice versa, select the bad responses",
      "offset": 3971.359,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "and understand why are they bad, how do",
      "offset": 3975.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "I improve them, make them better",
      "offset": 3977.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "just to complete that and and kind of uh",
      "offset": 3980.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "how do we configure this? Right? We we",
      "offset": 3984,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Carlos walked through like what this",
      "offset": 3985.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "would look like. Um this is you know my",
      "offset": 3987.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "score my rule, right? Obviously, you",
      "offset": 3989.839,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "would call that something a little bit",
      "offset": 3991.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "better, but I actually want to, you",
      "offset": 3992.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "know, uh, add in my scores for those",
      "offset": 3994.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "online logs, right? Then you would, you",
      "offset": 3997.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "probably wouldn't do 100%, but we're",
      "offset": 3999.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "just going to do that for for this",
      "offset": 4000.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "instance. And now when I come back to uh",
      "offset": 4002.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "my application here, right? And maybe I",
      "offset": 4005.039,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "want to do uh just do a quick refresh.",
      "offset": 4007.92,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "So now when these logs uh happen within",
      "offset": 4013.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the brain trust side, we're actually",
      "offset": 4015.599,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "going to run those scores against that",
      "offset": 4016.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "output. So we'll understand based on",
      "offset": 4018.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "what happened here, how did it score on",
      "offset": 4020.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "formatting, how did it score on",
      "offset": 4022.4,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "correctness. Um",
      "offset": 4024.4,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "and then this also uh can now layer into",
      "offset": 4029.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "so you can see here we have a 25% on the",
      "offset": 4032.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "accuracy on 100% on the uh the",
      "offset": 4034.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "completeness. So maybe we have a little",
      "offset": 4037.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "bit work to do. But now if I click into",
      "offset": 4039.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this, this is where you can start to now",
      "offset": 4041.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "create different things within uh the",
      "offset": 4043.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "like the the view portion here to ensure",
      "offset": 4047.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "like so this is a filter. So maybe I",
      "offset": 4049.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "want to change this to anything that is",
      "offset": 4051.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "less than let's say 50%.",
      "offset": 4053.28,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "Now I can save this as a view and my",
      "offset": 4057.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "human reviewers are able to now come in",
      "offset": 4060.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "here, open up this view and look at for",
      "offset": 4062.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "look look at all of the logs where my uh",
      "offset": 4065.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "accuracy score is less than 50%. And now",
      "offset": 4067.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we can again create that sort of uh",
      "offset": 4070.48,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "iterative feedback loop.",
      "offset": 4072.079,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "Any questions on this section?",
      "offset": 4081.119,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "Yeah, maybe a good segue to uh the the",
      "offset": 4088.16,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "human in the loop. Um this this becomes",
      "offset": 4091.44,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "really uh you almost you almost really",
      "offset": 4095.119,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "Oh, sorry. Um I had a question about",
      "offset": 4098.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "using brain trust and implementing it on",
      "offset": 4102.08,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "existing projects. Is it is it something",
      "offset": 4105.04,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "that's easy to do with like a um",
      "offset": 4108.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "something like Langmith? can just add",
      "offset": 4112.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like a couple lines and it'll trace",
      "offset": 4114,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "everything for you. Is it the same in",
      "offset": 4115.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "brain trust? Is it or do you have to",
      "offset": 4117.679,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "refactor all your prompts to use like",
      "offset": 4119.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the brain trust prompts?",
      "offset": 4121.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "It's it's essentially the same thing. So",
      "offset": 4124.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "like you have linksmith has code to wrap",
      "offset": 4126.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "an LLM client, right? Or it has uh",
      "offset": 4128.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "decorators to put on functions.\n It's the",
      "offset": 4131.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "exact same thing on the on the brain",
      "offset": 4134.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "trust side.\n Got it.\n Yeah. So then if you",
      "offset": 4135.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "do that, is it easy to then use um like",
      "offset": 4138.08,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "create data sets from those logs?\n Yeah,",
      "offset": 4141.44,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "absolutely. As long as you're uh the the",
      "offset": 4144.64,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "logs that you're producing map to the",
      "offset": 4147.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "structure of the data set that you've",
      "offset": 4151.759,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "created, then absolutely it becomes",
      "offset": 4153.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "really just you click that button. We're",
      "offset": 4155.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "going to add that span to the data set",
      "offset": 4156.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and it becomes really easy to connect",
      "offset": 4158.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "those two things.\n Cool. Thanks.\n Yeah, of",
      "offset": 4160.4,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "course.\n Yeah, good question.",
      "offset": 4164.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Um, okay. Yeah. So, let's talk maybe a",
      "offset": 4167.839,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "little bit really quickly about the the",
      "offset": 4169.52,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "question over here.",
      "offset": 4171.679,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "Is there a way to just override that",
      "offset": 4175.44,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "certain",
      "offset": 4177.679,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "sample set?",
      "offset": 4181.92,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "The way that you would go about that is",
      "offset": 4191.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "changing the span that is targeted. So",
      "offset": 4193.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "instead of it applying to the root span,",
      "offset": 4196.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you would specify a span that only",
      "offset": 4198.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "happens if a certain criteria is met.",
      "offset": 4201.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Right?\n So then it could be 100% or 50%",
      "offset": 4203.6,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "of just when that span appears.",
      "offset": 4206.64,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Okay. So yeah, this is where uh we could",
      "offset": 4214.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we could uh bring sort of like the human",
      "offset": 4216.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "in the loop uh type of workflow. This is",
      "offset": 4218.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "where we want to actually maybe bring in",
      "offset": 4220.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "subject matter experts and Carlos",
      "offset": 4222.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "mentioned like uh product managers",
      "offset": 4224.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "maybemes we also have doctors coming",
      "offset": 4226.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "into the platform and actually",
      "offset": 4228.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "evaluating some of this stuff right",
      "offset": 4229.679,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "these are the people that actually",
      "offset": 4231.199,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "understand whether or not that output",
      "offset": 4232.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that's created by that large language",
      "offset": 4233.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "model is valid is good right and this is",
      "offset": 4235.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "a really powerful thing to have as part",
      "offset": 4238.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of the process to to building really",
      "offset": 4240.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "powerful AI applications uh we can catch",
      "offset": 4242.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "hallucinations being able to establish",
      "offset": 4245.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that solid foundation or that ground",
      "offset": 4247.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "truth is oftentimes um you know having",
      "offset": 4249.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that human in the review in the loop",
      "offset": 4252.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "type of person becomes really uh",
      "offset": 4254.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "beneficial here.",
      "offset": 4256.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Uh so why does this matter matter?",
      "offset": 4259.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Excuse me. Uh it's it's really critical",
      "offset": 4261.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "for quality and reliability. Uh like we",
      "offset": 4263.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "were just talking about like with LLMs",
      "offset": 4266.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and being able to trust whether or not",
      "offset": 4268.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "they can do the same thing over and over",
      "offset": 4270,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "again. It's non-deterministic. I",
      "offset": 4271.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "automations can miss that nuance, right?",
      "offset": 4273.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "We want to be able to sort of apply that",
      "offset": 4274.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "that human type of um review to the the",
      "offset": 4277.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "things that we're doing on the AI side",
      "offset": 4280.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "with LM LLM as a judge type scorer. Uh",
      "offset": 4282.239,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "we also want to um help you make sure",
      "offset": 4285.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the final product meets the the actual",
      "offset": 4288.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "expectations of the user, right? So uh",
      "offset": 4290.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the user is going to have a much better",
      "offset": 4294.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "understanding of how to or like what the",
      "offset": 4295.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "uh final output should be. So having",
      "offset": 4298.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that person in the loop to look at those",
      "offset": 4301.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "outputs uh becomes really powerful to",
      "offset": 4303.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "ensuring that you build really uh really",
      "offset": 4306.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "strong genai applications.",
      "offset": 4308.159,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "Uh two types of human in the loop. Uh",
      "offset": 4311.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "there is the uh the human review. Uh so",
      "offset": 4314.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this is where these these people are",
      "offset": 4317.6,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "actually going to go into the brain",
      "offset": 4319.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "trust platform and actually manually",
      "offset": 4320.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "label score and audit those interactions",
      "offset": 4322.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that the uh that the user had with the",
      "offset": 4325.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the AI application. And then there's",
      "offset": 4327.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "actual feedback from the user real time.",
      "offset": 4329.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Uh so this is like you know a thumbs up",
      "offset": 4331.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "thumbs down button within the",
      "offset": 4333.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "application saying hey you did a really",
      "offset": 4335.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "good job you did a really bad job but",
      "offset": 4336.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "now we can you can sort of use these",
      "offset": 4338.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "together as well. I can now create a a",
      "offset": 4340.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "view within brain trust that filters",
      "offset": 4342.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "down to any of my user feedbacks of",
      "offset": 4344.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "zero. So thumbs down and I actually want",
      "offset": 4347.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to review whether or not those things",
      "offset": 4349.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "are bad and if they are bad I can add",
      "offset": 4350.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "those to the data set again creating",
      "offset": 4352.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that that sort of flywheel effect.",
      "offset": 4354.239,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "Um, just really quick here, if if you go",
      "offset": 4358,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "if you look at that application, you're",
      "offset": 4360.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "able to actually uh click one of these,",
      "offset": 4362.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "you know, thumbs up or thumbs down, uh,",
      "offset": 4364.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "create a comment. Really good. And then",
      "offset": 4366.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this is now logged back to the Brain",
      "offset": 4368.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Trust application.",
      "offset": 4371.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "So, if you look back at our logs,",
      "offset": 4372.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "I'll remove our filter. Uh, so we should",
      "offset": 4376.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "have a user feedback score now here of",
      "offset": 4379.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "100%. And then we should have a comment",
      "offset": 4381.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "over here as well. Really good. But then",
      "offset": 4383.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "again, these are the things that we can",
      "offset": 4385.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "now if I open this back up, I can create",
      "offset": 4387.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a filter on my user feedback score. Uh",
      "offset": 4388.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "now I want to understand all of my uh my",
      "offset": 4391.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "logs where user feedback is one or zero",
      "offset": 4393.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and then I can do something from there.",
      "offset": 4395.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "But this is this is done very easily via",
      "offset": 4397.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the log feedback function within within",
      "offset": 4400.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "brain trust. You provide it sort of like",
      "offset": 4402.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the the span that that was already",
      "offset": 4404.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "created within that log and then you",
      "offset": 4407.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "just you log you provide that user",
      "offset": 4409.28,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "feedback to it.",
      "offset": 4411.36,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Um you you can also enter uh really",
      "offset": 4422,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "quickly here in the platform. You can",
      "offset": 4424.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "enter uh human review mode. So this is a",
      "offset": 4426.64,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "way in which um it's sort of hiding away",
      "offset": 4429.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "some of the the different uh you know",
      "offset": 4433.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "some fields that may not be really",
      "offset": 4435.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "relevant for those people that are",
      "offset": 4437.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "coming in and doing human review.",
      "offset": 4438.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Oh, I haven't actually configured any",
      "offset": 4441.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "scores yet. So, you can actually see uh",
      "offset": 4443.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you would come out here and you would",
      "offset": 4444.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "create different uh scores for that",
      "offset": 4446.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "human to go in and do that review.",
      "offset": 4448.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Whether it's uh sort of like an",
      "offset": 4450.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "option-based uh free form input uh",
      "offset": 4452.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "slider, so is this, you know, maybe",
      "offset": 4455.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "thumbs up, thumbs down, sort of like yes",
      "offset": 4457.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "or no, or maybe you could do something a",
      "offset": 4459.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "little bit more verbose, um ABCD,",
      "offset": 4461.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "whatever it is. But this is where you",
      "offset": 4464,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "create these scores. Now, they exist as",
      "offset": 4466.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "part of those logs. those humans can go",
      "offset": 4468,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "in and now look at the input and the",
      "offset": 4470,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "output give their review of it again",
      "offset": 4471.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "adding to uh that that again like that",
      "offset": 4474.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that flywheel effect that we need to",
      "offset": 4476.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "create.\n Yeah, I wanted to add there as",
      "offset": 4478.08,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "well. This is really helpful for evaling",
      "offset": 4481.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "your LM as a judge. Oftent times we see",
      "offset": 4484.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "customers use this process to provide",
      "offset": 4487.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "some ground truth for their data sets",
      "offset": 4490.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and also for the LM as a judge. Right?",
      "offset": 4493.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So you can imagine having a team ofmemes",
      "offset": 4495.199,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "come in, they review uh they do thumbs",
      "offset": 4498.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "up, thumbs down on maybe five different",
      "offset": 4501.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh criteria qualities that they're",
      "offset": 4503.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "measuring and then they provide that",
      "offset": 4505.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "data to uh a playground where the prompt",
      "offset": 4508.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "is the LLM as a judge and they go",
      "offset": 4511.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "through the playground and they test to",
      "offset": 4513.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "make sure that the LM is a judge prompt",
      "offset": 4516,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "matches what the humans thought. So just",
      "offset": 4517.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "something there to think about, but as",
      "offset": 4520.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Doug is saying, it's a great feedback",
      "offset": 4522.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "loop. It's a great uh flywheel effect",
      "offset": 4524.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that can be created when you add this",
      "offset": 4526.159,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "human to to verify and confirm.",
      "offset": 4528.32,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Cool. That that is it for uh the the",
      "offset": 4534,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "workshop. We do have a few minutes left.",
      "offset": 4538,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Could certainly answer a couple more",
      "offset": 4540.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "questions.",
      "offset": 4541.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So for people who are successful with",
      "offset": 4544.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "this, how much time are they going",
      "offset": 4546.64,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "forward",
      "offset": 4550.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "before they get into like more live",
      "offset": 4553.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "testing and then how are they going",
      "offset": 4556.159,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "back? What's the kind of balance",
      "offset": 4557.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "offline versus online? Ev,",
      "offset": 4562.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "how much how much you have to do up",
      "offset": 4565.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "front to really get best results or you",
      "offset": 4567.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "really just put something down, figure",
      "offset": 4569.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "it out later,",
      "offset": 4571.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "get analysis,",
      "offset": 4575.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "right?\n Yeah. The question just to repeat",
      "offset": 4577.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it is how much time do you have to",
      "offset": 4580.159,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "invest upfront to get value? Should you",
      "offset": 4582.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "keep going over over it to try to",
      "offset": 4585.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "optimize or better to just start quickly",
      "offset": 4588.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "with minimal scores, minimal data set",
      "offset": 4590.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and then uh keep improving? And I would",
      "offset": 4592.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "say the latter, right? You don't want to",
      "offset": 4595.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to be fixated on creating a golden data",
      "offset": 4596.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "set or 20 scores. Like if you have one",
      "offset": 4599.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "or two scores and you have 10 rows in a",
      "offset": 4602.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "data set, it's going to be tremendously",
      "offset": 4604.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "helpful. And then from there, it's all",
      "offset": 4606.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "about iteration. So just going back and",
      "offset": 4608.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "improving, adding some more rows, adding",
      "offset": 4610.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "another score, tweaking the scores, but",
      "offset": 4613.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "you really just want to get started",
      "offset": 4615.04,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "quickly.",
      "offset": 4616.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Yeah. Um so um you've mentioned um some",
      "offset": 4619.84,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "elements uh of this u uh scoring. Uh",
      "offset": 4623.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "there's the the function that you want",
      "offset": 4627.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "to test that you have to define the the",
      "offset": 4629.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "test steps if you will. Um one of the",
      "offset": 4631.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "challenges that we are finding is our",
      "offset": 4634.32,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "actual application does change and uh it",
      "offset": 4637.28,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "could change bi-weekly, it could change",
      "offset": 4641.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "monthly.",
      "offset": 4644.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "Is there a way to look at um uh trying",
      "offset": 4645.76,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "to automate changing the actual function",
      "offset": 4649.28,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "that you now need to change to match the",
      "offset": 4652.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "way that your application logic has just",
      "offset": 4655.679,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "changed uh this week from two weeks ago",
      "offset": 4657.679,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "when you say oh go for it\n yeah I guess I",
      "offset": 4661.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "was just going to again clarify to to",
      "offset": 4664.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "make sure I understood so you're saying",
      "offset": 4666.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that the scorer the the actual scoring",
      "offset": 4668.719,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "function is going to",
      "offset": 4672.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "uh stop being useful. It's going to",
      "offset": 4676.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "become obsolete. It's going to become",
      "offset": 4677.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "too old to actually gauge the quality.",
      "offset": 4679.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Not just the scoring function but the",
      "offset": 4682.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "actual steps that you want to uh test.",
      "offset": 4684.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "So you know this week there might be",
      "offset": 4687.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "only two turns just giving a very simple",
      "offset": 4689.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "example and in two weeks in the next",
      "offset": 4691.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "spring there are now five turns in your",
      "offset": 4694.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "app because the logic has changed and",
      "offset": 4697.199,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "now you you have to update of course the",
      "offset": 4700.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I think the function element there's",
      "offset": 4703.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "probably no way around it. I'm just",
      "offset": 4705.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "curious about uh whether uh you guys",
      "offset": 4707.679,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "have uh thoughts about how that could be",
      "offset": 4711.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "improved or or made easier.\n Well, I",
      "offset": 4714.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "think your your task all will will",
      "offset": 4717.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "always change. Right. Right. The thing",
      "offset": 4719.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that we're trying to to build\n Yes.",
      "offset": 4721.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "That's where brain trust helps because",
      "offset": 4723.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we're going to understand when we do go",
      "offset": 4725.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "make that change,\n we actually understand",
      "offset": 4727.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "whether or not that change improved our",
      "offset": 4729.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "application or regressed it. So like",
      "offset": 4731.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we're not going to say stop making",
      "offset": 4733.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "changes to the underlying.\n Yes. No, I I",
      "offset": 4735.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "real I understand that. So it it is",
      "offset": 4738.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "inevitable that the the application is",
      "offset": 4740.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "going to be uh changing.\n Yeah.\n And",
      "offset": 4743.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you're going to have to constantly",
      "offset": 4745.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "update the corres the the test at the",
      "offset": 4746.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "test step the function that you're",
      "offset": 4750.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "actually wanting to mimic in your test.",
      "offset": 4752.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "It's very similar to traditional",
      "offset": 4755.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "software testing. Yeah. You don't want",
      "offset": 4757.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to write a test that lasts for a day or",
      "offset": 4759.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you know a week right you want to think",
      "offset": 4761.76,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "of of robust uh tests that will live on",
      "offset": 4763.44,
      "duration": 8.239
    },
    {
      "lang": "en",
      "text": "for months or years and will actually",
      "offset": 4769.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "measure the underlying quality of the",
      "offset": 4771.679,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "application that will be longived.",
      "offset": 4774.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "So the I think it's more of how do you",
      "offset": 4777.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "optimize the scores to measure qualities",
      "offset": 4780.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that will still be around even if you",
      "offset": 4783.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "add some additional steps in the task.",
      "offset": 4785.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Uh no, it's worse than that because",
      "offset": 4787.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "unless you have those additional steps",
      "offset": 4788.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in your function, you're not mimicking",
      "offset": 4790.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "your your application's uh logic.",
      "offset": 4793.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "You're still using the logic from two,",
      "offset": 4797.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you know, last sprint. So no matter how",
      "offset": 4799.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "good your scoring could be, it's not no",
      "offset": 4802.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it's no longer reflecting what your",
      "offset": 4804.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "application is doing this week or this",
      "offset": 4806.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "sprint. I I think like regardless of",
      "offset": 4809.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like how many steps you have like",
      "offset": 4810.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "there's still an input and there's",
      "offset": 4813.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "there's still an output that we want to",
      "offset": 4814.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "score against.",
      "offset": 4815.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Correct.\n Yes. But um I think one of the",
      "offset": 4817.84,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "things you need to do is to first define",
      "offset": 4821.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "how you're going to arrive at the score.",
      "offset": 4824.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the the input comes in and now maybe you",
      "offset": 4826.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "have three turns",
      "offset": 4829.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and then because you're mimicking your",
      "offset": 4831.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "app and then you get your output from",
      "offset": 4833.12,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "these three turns",
      "offset": 4836,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "your app just got upgraded they're now",
      "offset": 4838.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "seven or five turns or whatever.\n Yeah.",
      "offset": 4840.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "So the when you're writing the evals you",
      "offset": 4843.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "can dynamically call the task. So even",
      "offset": 4846.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "as you're working on your application",
      "offset": 4850,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and it's changing,\n you're still pointing",
      "offset": 4851.6,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "to the the changing app. So the idea is",
      "offset": 4854.56,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "that when you are wanting to merge into",
      "offset": 4859.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "main, you open a PR and then your evals",
      "offset": 4862.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "will run on those new changes. You don't",
      "offset": 4865.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "need to go in and update the eval.ts",
      "offset": 4867.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "files. they will now reference the",
      "offset": 4871.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "updated uh task application that you're",
      "offset": 4873.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "trying to understand the underlying",
      "offset": 4877.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "quality for if that makes sense. So I",
      "offset": 4878.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "think the the question again is are the",
      "offset": 4881.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "scores is the underlying logic something",
      "offset": 4883.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "that you can trust and that will live",
      "offset": 4887.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "on. Um, again, it's not easy and it's",
      "offset": 4889.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "something that is changing. Uh, but",
      "offset": 4891.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that's that's what we're hearing from",
      "offset": 4894.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "customers is investing in that",
      "offset": 4895.52,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "brainsmemes.",
      "offset": 4899.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "What's the name of that role and how are",
      "offset": 4904.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you managing that? Like I'm guessing to",
      "offset": 4907.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "some extent it was originally the team,",
      "offset": 4910.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "right? But that can't scale. So how are",
      "offset": 4912.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you managing that?",
      "offset": 4915.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I think it's like organization specific.",
      "offset": 4918.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I don't know if there's a specific\n I",
      "offset": 4920.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "would say your organization using your",
      "offset": 4921.76,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "own tool. How are you managing yourself?",
      "offset": 4923.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "I don't think we're using anymemes at",
      "offset": 4928.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the moment. We don't we're not a",
      "offset": 4930.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "healthcare company or a legal tech",
      "offset": 4934,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "company where we heavily rely on",
      "offset": 4936.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "specialized knowledge in that degree,",
      "offset": 4939.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you know. Um\n you're not doing human",
      "offset": 4941.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "evals of your own product.",
      "offset": 4944.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "We just now started we just now branched",
      "offset": 4946.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "into having an AI component to our",
      "offset": 4949.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "application. Uh so we haven't needed to",
      "offset": 4951.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to go there just yet. But we, you know,",
      "offset": 4954.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we talk to a lot of customers that are",
      "offset": 4957.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "working in in those specific industries",
      "offset": 4959.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "with those use cases and they will",
      "offset": 4961.76,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "sometimes hire external uh services that",
      "offset": 4965.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "will do the the the annotations for them",
      "offset": 4968.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "or they'll bring them into Brain Trust",
      "offset": 4971.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and you know they'll be using the",
      "offset": 4973.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "platform just to to review. So they have",
      "offset": 4976.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a specific role within Brain Trust and",
      "offset": 4978.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "there's a specific view that they would",
      "offset": 4980.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "operate in that's just for annotation.",
      "offset": 4981.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 4985.199,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "Great. Yeah, another question over here.",
      "offset": 4988.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Uh I just I was just curious that",
      "offset": 4990.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "because we're using out of the box AI",
      "offset": 4992.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "models here and are not really fine",
      "offset": 4994.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "tuning the model as the application",
      "offset": 4996.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "progresses. Are we do we have a way to",
      "offset": 4997.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "like do some few short example prompting",
      "offset": 5000.639,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "from the data set and the eval scores",
      "offset": 5004,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that we are already using. So is there",
      "offset": 5006.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "some feature like that where I can use",
      "offset": 5009.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the data sets or the online logs that",
      "offset": 5011.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are added to the data sets. If the eval",
      "offset": 5013.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "score is good, use it as an example for",
      "offset": 5016,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "future prompts to just make the prompt",
      "offset": 5018.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "better because the models are out of the",
      "offset": 5020.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "box.",
      "offset": 5022.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Yeah. So question around fshot prompting",
      "offset": 5024.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "providing examples to the prompt of the",
      "offset": 5026.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "ideal response. That's something that",
      "offset": 5028.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you can do today with in the data set in",
      "offset": 5029.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the metadata column is where you can",
      "offset": 5032.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "provide the the few shot examples that",
      "offset": 5034.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you want for each row and then when",
      "offset": 5037.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you're running that eval or messing",
      "offset": 5039.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "around in the playground, it'll",
      "offset": 5041.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "reference the few shots in the metadata.",
      "offset": 5042.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Got it. But what about like the online",
      "offset": 5045.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "testing stuff, right? Or the online",
      "offset": 5048.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "logs, whatever you call it. like when",
      "offset": 5050.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "users are actually using the application",
      "offset": 5052.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and it's hitting the prompt there can",
      "offset": 5053.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the prompt real time use those uh",
      "offset": 5055.76,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "examples from the data sets as well\n it",
      "offset": 5059.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "right now it's not it's not not",
      "offset": 5062.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "something that brain trust facilitates",
      "offset": 5064.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "within the SDK and building your own",
      "offset": 5066.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "logic like you could come up with a",
      "offset": 5068.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "workflow like this but natively in the",
      "offset": 5070.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "platform we're not facilitating like",
      "offset": 5072.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "live traffic into few shot examples\n got",
      "offset": 5074.8,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "it makes sense",
      "offset": 5078,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Great. Well, thanks everyone. I know",
      "offset": 5082.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "we're over time. Really great to have",
      "offset": 5084,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you all here for our first workshop of",
      "offset": 5086.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the day. I hope you can walk away with",
      "offset": 5087.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "some ideas of how you can improve your",
      "offset": 5090.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "eval workflow. And you know, our team is",
      "offset": 5092.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "here. We have a booth uh just outside of",
      "offset": 5095.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "this. So, feel free to stop by. We can",
      "offset": 5097.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "answer more questions, have a",
      "offset": 5099.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "conversation. Yeah. Thanks everyone.",
      "offset": 5101.28,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "Thank you all.",
      "offset": 5102.8,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 5106.8,
      "duration": 2.619
    }
  ],
  "cleanText": "[Music]\nHey everyone, thanks for joining us for the Evals Workshop session today. This is the first workshop that we'll be leading. There's another one at 3:30, so you get to be the first people to go through it. Uh, very exciting stuff. If you've gotten a chance to sign up for Braintrust, uh, you know, please do that now. If not, we also have some workshop materials and a Slack channel for you to follow along.\n\nIn the Slack channel, we also sent out a poll if you'd like to respond with a little emoji underneath the message. That'd be great. Uh, in the Slack channel also, there is the workshop guide. So, in case you're not uh able to get the QR code for whatever reason, uh go into the Slack channel and you'll be able to pull up that document.\n\nBefore we jump in, obviously maybe just a quick intro of uh Carlos and I. My name is Doug. I am a solutions engineer at Braintrust. Um have a background in data and finance. Actually, my third week here at Braintrust. Um but but looking forward to kind of leading you all through the platform and giving you a sense for how you can master evals with Braintrust.\n\nYeah, my name's Carlos Essan. I'm also a source engineer helping out with some of our great customers at Braintrust. I'm a little bit more tenured, been here six weeks and before I was in the info world working at HashiCorp doing some uh stuff with Terraform and Vault. Uh but yeah, super exciting to be here today at the AI Engineer World's Fair. Uh we have a lot of exciting things to go over with you.\n\nSo just to go over the high-level agenda, we're going to be alternating between lectures with slides and hands-on activities. So we're going to start with understanding, you know, why even eval, what is an eval task there and then move back into the lecture, talk about the SDK, how you can do the same thing via the SDK. It can be a bit more powerful in certain situations as well. Then go into production like logging. So day two stuff, how are you how are you observing your users interacting with your production app or production feature? And then finally, we're going to be incorporating some human the loop. So trying to uh establish some ground truth for the ideal responses, improve your data sets, and overall improve the performance of your app.\n\nIf you've gotten a chance to check out the poll in the Slack, uh, feel free to submit a response. Really curious to see how everybody is currently evaluating your AI systems.\n\nJust as a, you know, question that I'm out of curiosity, could I ask for a show of hands, how many people have seen Braintrust before, gone to brain.dev, and interacted with it? Cool.\n\nThat's great. So, we have some pupils that have already gone and explored a little bit. That's exciting. And a lot of people that are brand new. So, starting off with just an introduction. What are evals? Uh how do you get started?\n\nFirst, I wanted to just show off some mentions of evals in the public space. Uh you may recognize some of these names. They see the importance of eval which you know may or may not uh point you that this is something important that we should be thinking about when pushing changes into production when we're developing these AI features.\n\nSo why even do eval? Well, they help you answer questions. That's ultimately what they're for. You know, uh what type of model should I use? What's the best cost for my use case? What's going to perform best in all of the edge cases that my users will be interacting with. Is it going to be consistent with my brand? Is it going to uh you know uh talk to the end customer to the end user in the same voice that I would want a human? Am I improving the system over time? Am I able to catch bugs? Am I able to troubleshoot effectively? So all of this can be can be answered with the help of eval which is what we'll be discussing today.\n\nThe best LLMs don't always guarantee consistent performance. So this is why you need to have a testing framework in place. Right? We have hallucinations uh occurring at a pretty high rate. Performance is also degrading when you make changes. It's difficult to guarantee that the change that you're putting through isn't going to regress the application and you know the changing a prompt even if it may seem like it's improving it will actually regress it. So you need to have some scientific empirical way of testing these changes and making sure that your AI feature is performing at the level that your users expect.\n\nSo how do evals help your business? Well, they cut dev time. You'll be able to push changes into production a lot faster. Uh, eval will live at the center of your development life cycle. They will reduce costs as the due to the automated nature of eval. You'll replace manual review. Uh, it will then lead to faster iteration, faster releases. You'll also be able to optimize the model that you're using, make sure that it's the best bang for buck. Your quality will go up and you'll be able to scale your teams. It will enable non-technical users and technical users to also have a say in the prompt choice in the model choice and just the overall um management of the performance in in the production traffic.\n\nThese are some of Braintrust's customer outcomes. So, we've been able to uh help some of these great companies move a lot faster, increase their team productivity, and increase their AI product quality.\n\nSo now moving into some of the core concepts of Braintrust. Uh so we're really targeting three things. Prompt engineering, right? So we're thinking about how we're writing the prompts. What's the best way to provide context on our specific use case to the prompt so that we are optimizing its response. Right? The middle piece evals. Are we measuring improvements? Are we measuring regressions? Is this being done in a statistical way uh that's easy to review, easy to understand? And then finally, AI observability. Are we capturing what's happening in production? Do we know if our users are happy with the outputs, unhappy? Are we able to uh prioritize certain responses so that we can keep iterating, keep improving?\n\nGreat. So now moving uh to the eval section. So what what is an eval? So the definition we've come up with is that it's a structured test that checks how well your AI systems perform. It helps you measure quality, reliability, and correctness across various scenarios. Ideally, you're capturing every scenario that a user will live through when interacting with your AI feature.\n\nWhen it comes to Braintrust and writing evals, there's really three ingredients that you need to understand to be able to to work effectively. The first is a task. So this is the thing that you're testing, right? This is the code or prompt that you want to evaluate. It can be a single prompt or a full agentic workflow. The complexity is really up to you. The one requirement is that it has an input and an output.\n\nThen we have our data set. So this is the set of real-world examples or test cases that we want to uh push through the task to see how it performs. And then the score that's the logic behind uh the evals. So, how are we grading the output of our prompt on our data set? Uh, these can be LLM as a judge scores or they can be full code uh functions and the caveat is that they need to output a score from 0 to one which will then be converted into a percentage.\n\nQuestion.\n Yeah.\n What are the test cases?\nIs it also agent generated?\nIt can be at first. The question was, is the data set synthetic? Can it be synthetic? And the answer is, it's a great way to get started quickly is having an AI generate those initial use cases, but as you progress, as you mature, it's great to ground those in logs so you're capturing the real user traffic, the real interactions that users are having and integrating those into your data sets.\n\nGreat. So now I wanted to talk about offline evals and online eval. So there's two mental models to think through. Offline evals are what you're doing in development, right? So this is the structured testing of the AI model of the prompt uh that you are going to then eventually uh show off to to customers in production. So this is for proactive identification of issues, right? Uh this is what we'll be doing today in the Playground in Braintrust and also via the SDK. Uh but then on the other side is online eval. So this is in production uh real traffic is being captured and being measured. It's being graded just like your offline evals are being graded. And this is going to allow you to diagnose problems, monitor the overall performance and capture user feedback in real time so that you can understand, oh, this edge case isn't included in my data set. This is a weak point in my current AI product. I need to spend some time uh attacking it and improving it.\n\nA big question that we get asked is what should I improve? Right? Uh I have my prompt, I have my eval, you know, how do I know what's wrong? Uh and I think this matrix really helps simplify this question. So if you have a good output from, you know, your own judgment looking at what the LLM is giving you and it's a high score, then great, right? You've verified yourself that the output is is high quality and also the scores the evals have also come to the same conclusion. If you think it's a good output but it's a low score then that's a signal that you need to improve your evals. Maybe the score isn't actually representing what a human would think, right? Uh if it's a bad output but a high score, same thing, right? It doesn't match what a human would think looking at the output. So you need to improve your evals. And then finally, if it's a bad output and a low score, your evals are working correctly. That's good. And now you need to focus on improving your AI app. So, I hope this helps explain how you should be thinking through uh your scores and and what to what to tackle in which moment.\n\nSo, now we're going to zoom into each of those ingredients or components starting off with the task. Uh so, as I mentioned, right, a task is really just an input and an output. It can be a single LM call or a whole agentic workflow. It's really up to you what you want to test. Uh so in this pattern, we're just going to be creating a simple prompt. Uh this is what the activity today is going to encompass and uh you can use dynamic templating with mustache. So you can provide your data set rows as part of the the prompt and that will be tested uh and you'll get to see that in action soon.\n\nWhat if you have more than just a prompt? What if you have a multi-turn chat a whole conversation that you want to evaluate? So you can do that in Braintrust today. You can provide the whole conversation as extra messages. So providing that whole chain of of uh messages back and forth with the user and the assistant. You can include tool calls as well to simulate those tool calls and evaluate that big chunk that context that whole conversation at once.\n\nTools are also something supported in Braintrust. So that's oftentimes something that your AI applications will leverage uh talking to external services or grabbing information from somewhere else. Uh so you can add tools to Braintrust and have the tool available for your prompt uh to use and just to mention that's great for rag use cases. So I know that's a a hot word right now. So if if you have that in you know in mind Braintrust can handle it. We support tools. We support rag um agents another hot word right now. So we also allow you to chain your prompts, right? So you can have three prompts chained together. The output of the first prompt will become the input of the next and so on. Uh so you can start uh testing end to end uh all these prompts back and forth, right? And and do the same thing that you would with with a single prompt.\n\nGreat. Okay, so now moving into data sets. So this is the test cases, right? You're going to keep iterating over time, but initially maybe you're using something synthetic. Um, there are three fields that you need to understand for a data set. Only one of them is required though, and that's the input. So that is the the prompt, the user provided uh use case, the the prompt that would be provided by by the user would be the input. You could think of it that way. And then you have the expected column which is optional which is the anticipated output or the ideal response of that that prompt. And then finally you have your metadata which can allow you to to capture any additional information that you may want to associate to that specific row in the data set.\n\nSome tips for data sets is to start small and iterate. Uh it doesn't need to be the largest data set of all time. It doesn't need to include all of your use cases, right? Just get started. Use synthetic data at first. And the important piece is to keep improving, right? Keep iterating. So if you start logging your real user interactions, you know, even if it's just in staging or internally in your organization, you can start to increase the the scope of the data set and it will start to become closer to the the overall uh domain of of use cases that that users will will interact with. And then finally, you want to start implementing human review. Uh this will allow you to establish ground truth, improve your data set, improve the expected column. um which will be great for for your evals and zooming into scores. So this is uh you have two options here and the type of score that you want to use. LM is a judge. This is great for uh more subjective or contextual feedback. What would a human need to understand when looking at the output? What criteria would they consider? Uh this is more of a qualitative question that you want an answer to, right? Using that LM as a judge. On the code-based score, this is deterministic, right? So, you would want exact or binary conditions. This is more of an objective question. Um, and it's the important piece is to try to use both. So, you want some LLM as a judge scores, but you also would like some code-based scores, and they they'll help you meet in the middle and understand the the quality.\n\nSo, some tips here. uh you know, if you're using an LM as a judge, maybe use a higher quality model, a more expensive model to to grade the the cheaper model. Uh make sure that the LM as a judge has a focus. So don't give it, you know, four or five criteria to consider. Zoom into one specific piece and expand and explain the steps it should think through to to come to its conclusion.\nIf you're writing LM\n\n\nAs a judge, maybe you should eval the judge and make sure that the prompt that you're using is matching what a human would think. So that's another great way of improving your scores. And you know, just make sure that it's confined and you're not overloading it with all the context in the world, right? You want it to be focused on the relevant input and output for consistency.\n\nGreat. Almost at the end here. So there's two things to understand about the Braintrust UI specifically. So there's the playgrounds, and this is for quick iteration of your prompts, agent scores, data sets, right? It's really effective for comparing. You can do AB testing with prompts, you can do AB testing with models, and then you can save a snapshot of the playground to your experiments view, and the experiments is for comparison over time. So you'll be able to track how your scores change over weeks, months, and everything that your team is doing across the UI, across the SDK will also aggregate in the experiments view. So you can analyze everything and understand, okay, this new model came out today, how is it performing to the prompt from two weeks ago.\n\nGreat. So now we've reached the first activity. So if you could please go to the activity document and it will take you through the journey of running your first eval in the UI.\n\nPlease raise your hand if you have any questions or run into any issues. We'll be walking around and just making sure there's no blockers.\n\nYeah, if you check Slack, we'll also go back to the QR code.\n\nSo, did everybody have a chance to get these QR codes?\n\nThe middle one is going to be the most important. And this is where you're going to access the materials for the workshop.\n\nYeah, I'll repeat the question. The question was around extra messages in the prompt, and if you are overseeing agents and you know multiple types of users, multiple different roles are all talking back and forth and you want to distinguish their roles, right? And all being all within the playground UI. Um, so you can right now there is no additional delineation between the assistant, the user, the tool call, and um, I believe that's it. So having the user be branched and it play different roles is something that you would need to rely on the SDK for that additional flexibility.\n\nMy other question, right?\n\nYeah. And and we'll cover the SDK in the next section. I think maybe the biggest takeaway is that there is no limit really on the complexity that you feed to that task, right? The only requirement is that input and that output. Like maybe something is a little bit more tailored to the Braintrust Playground and the UI, where some things are actually a little bit more tailored to that SDK. So that's that's we can jump into that in that next section. Um, maybe it makes sense to like as we're going through like the workshop, I'll kind of walk through this as well just so you can all kind of see me go through it. But feel free to raise your hand. We can walk around and and answer questions.\n\nFor Slack. Are you able to access the document via the... is the slide deck's not public? Is that... it's not... Oh, that... for the slide deck. Is this... Yeah, we... we've just... Yeah, the question was, this is all in the UI, right? Is this... That's the only place we've been thus far, right?\n\nUh, just talking a little bit about the different components of the Braintrust platform.\n\nUm, so let me let me walk through that, right? Give you a sense for what we just kind of showed to you in slides, right? So you can't access the slide deck. We can we can update that.\n\nYeah.\n\nYeah.\n\nUm, just let's kind of walk through this, so we can get a sense for what we're what we're building here. Uh, some of the things that Carlos just walked through. Um, I have a lot of this stuff already installed. I hope that you kind of walked through this, right? We need certain things on our system to actually go and run this. So we have node, we have get. Um, we're going to sign up for a Braintrust account. Creating a Braintrust or already done this, so I'm not going to kind of bore you through that step right there. This project unreleased AI. Um, if you if you don't do that, you'll see two projects in your account, but um, just this is where we're going to actually create our prompts and our scores and our data set from the repo that we're going to clone into our local machine.\n\nPart of this demo requires an OpenAI API key. That's just what we're using under the hood. That's certainly not a limitation of Braintrust. Um, you can use and maybe just to kind of highlight something here. You can uh you can use really any AI provider out there. So if you've gone into Braintrust account, you've probably seen this. You've entered your OpenAI API key here. This is what is going to allow you to to run those prompts in the playground. You can see you have access to many other providers. You have access to cloud providers like uh Bedrock and and so on. And then you can even use your own custom provider. But for this workshop right now we are using OpenAI.\n\nQuestion.\n\nSorry.\n\nAre you able to run locally models?\n\nYes.\n\nYeah. Question was, can you run Braintrust locally using local models? Yeah. Um, we we have uh if you look a little bit further out, there's a section for what we call remote evals. Might not have time to get to it in this particular section, but know that you can go to that and and play with that feature as well.\n\nUm, sorry, coming back down here. Um, so we're going to clone this repo. Right. This is a right, this is the application that we're creating. The idea is to uh give it a GitHub URL and look for most recent commits since the last release and then summarize those right for us as developers. So that's the application that we're going to use. Uh, we're going to create some different API keys locally. So if you've cloned your repo, uh, you'll have a .local file. I'll show you my example. You're going to also input these, your Braintrust API key here and then your OpenAI API key. This is optional down here. Uh, it's just if you don't want to get rate limited by GitHub. Probably not probably not going to create a lot of requests right now. So you probably don't need this.\n\nUm, really important step here. So I'm going to come back into uh Braintrust. So as part of our install, we're actually going to go create some of these resources within our Braintrust project that we just created. So I'm going to run pnpm install. This will actually go push some of these resources and you'll find these in the Braintrust folder, the resources, and we'll jump into that, but just wanted to highlight that. So now if I look back into my project, I should see that unreleased AI and the different things that we've created. We have two different prompts now, right? These are the prompts that we use to generate the change log as well as the test cases, the data set that we'll use as part of this.\n\nYeah.\n\nAll right, let me stop there. Anybody having issues just kind of going through that initial setup phase?\n\nExcuse me.\n\nUh, we haven't made those public yet. I think...\n\nI'm trying to do that now.\n\nYeah.\n\nDo you have a... do you have Slack? Uh, were you able to join the workshop eval channel?\n\nYeah, the Wi-Fi is not working.\n\nOkay. Well, I I'll walk you through.\n\nYeah.\n\nHow are we connecting the repo to the project?\n\nUm, yeah. So, when we ran PNPM install, we ran a script in the background called just Braintrust push. And if I look at that file here, there's different things that we've configured, right? We've configured my change log. So this is actually the Braintrust SDK under the hood. Uh, this is where we're creating that prompt in that project unreleased AI. And so there's a couple things that we can do here from an SDK perspective. This is like you think about version controlling all of these different things and actually pushing them into the Braintrust UI. So there there's a lot of different ways to work with Braintrust. I think we mentioned earlier either via just like the UI or actually via the SDK, but that's that's how a lot of this stuff got created.\n\nCool. Let's let's kind of walk through this first activity. Um, we're going to we're going to access the unreleased AI project. So if we go to the prompts, so this is what we just created.\n\nUh, we created two different prompts, right? This is essentially what we can start to play around with, right? We have this one, and and Carlos mentioned earlier there there's this mustache syntax. We can actually input variables here into our prompts and this is going to actually map to the different data sets that we can actually use as part of this project. So here's our first prompt.\n\nThat's about impossible to see.\n\nThat was impossible.\n\nOkay.\n\nNo, no, that's fine. I appreciate that. Thank you.\n\nAnd the lighting is hard. So...\n\nYeah.\n\nOh, maybe we can change the appearance to light mode or no.\n\nYeah. Where's that? Put the user up here.\n\nHow's that?\n\nThank you. No, I appreciate that.\n\nHow's this?\n\nOkay, cool. Um, so really just reviewing the stuff that we created. We created these two prompts. Here's our data set that we're going to use when we run our evals and our experiments. Uh, you can get a sense for for this. Here's my input. I have a series of commits and then I have a repository URL and I have the when the last release was, that's that since field. So these this is again the thing that we'll use inside of that playground to create uh to create evals and and to use to iterate from. And then the last thing that I'll call out here is the scores. Uh, so we created a few different scores that we'll want to use to actually score these prompts. So we have an accuracy, formatting, and completeness score. And again, this is just in that repo and that resources.ts file. Uh, we have maybe just to to point out, uh, linking a little bit of what Carlos was talking about to the actual code that you're seeing. We have LLM as judge scores. As you can see here, uh, really again trying to pinpoint here the accuracy, right? Uh, we're not overloading a single LLM as a judge score with accuracy, completeness, and formatting. Going to be very uh sort of um detailed or you know, scoped down to that particular thing. And then last one we have a code-based score, right? So this is a little bit more uh binary, right? Is the formatting of this change log that the LLM generated, does it map to what we expect? And so we can use some code to do that. So that's what we created via that script.\n\nYeah.\n\nHow do we get that sandbox project?\n\nYeah. So, if you go back to the lab setup when you run when you run that install. So, I'm using PNPM.\n\nAll right. I don't know if if you use that, if you're using that locally, you can you can also use npm.\n\nSo, what is the key for OpenAI API key?\n\nWhat is the key? Do you not have an OpenAI API key?\n\ninstruction.\n\nOkay. Um, I don't know if we have one to distribute at the moment.\n\nYeah. If you don't have...\n\nI'll do this later. Well, if if you go like the part of the setup here, right, when we go to if you come in here to a playground as an example, right? And we're going to pull in one of those prompts. So, we can pull in both of these prompts to do again like what Carlos was talking about, that that's sort of like AB testing.\n\nIt's going to ask for some OpenAI models, right? It's if you don't configure an OpenAI API key inside of your Braintrust account, you don't have a provider to actually run this this task against.\n\nUm, but this is this is the playground, right? This is what Carlos was talking a little bit about earlier uh about being able to do some AB testing, right? I have uh my two prompts that I've loaded in. The idea here is to to load in those different ingredients, right? Our tasks, our our data set, and then our scores. So you'll you'll look down here. We're going to select that data set and then we're going to select the different scores that we want to score this task against. And I'll load in my accuracy, formatting, and completeness.\n\nCan do a couple things here. I can click run. This will actually in parallel go through that uh that data set and use each task that we've defined and then it will score those. Right? So the idea here is to again, like this provides that sort of rapid rapid iterative uh feedback loop that we often times need to build these types of products. So here are my, you know, like my example rows. Again, these could be synthetic. These could be a a small subset of rows that are coming back from my application. But now I can get a sense for prompt A, prompt B. How are these performing with my scores? Uh, you know, relative to the things that I have here. But uh then I'm able to do a lot of different things here. I I can uh look at maybe a summary layout, get a sense for the scores. So uh at the top, this is sort of like my baseline. Up here is my base task and this is my comparison task. So you can get a very high level uh look at these different scores and how they fared with the different prompts that we've loaded in here. Now the other thing that we can do that Carlos mentioned is experiments. So often times we'll want to capture this these scores over time. So when we do make changes, we understand how those scores fared a week ago, a month ago or or whatever it is. So I can click this experiments button and you'll see the different things that we've loaded up into this playground are already uh here within this uh modal. We'll click create and this will actually create the experiment that you can go to here. And again this will uh if I click maybe one out, this is allow us to track this over time. This is what we can also lay in uh in our CI kind of workflow. So we go make a change to that prompt, make a change to that model. What are the impacts to uh the scores relative to what we had over history?\n\nSorry, can you... what is the completeness score?\n\nWhat is the completeness score?\n\nYeah, we can dig into that a little bit. Um, this is an LLM as a judge score. So the idea, right,\n\n\nWe're just going to give it um instructions.\nThe LLM is going to score the output based on what we've provided in this prompt.\nUh, you'll also note I'm just really pulling in the uh the structure of my data set, right?\nAnd so you obviously can write, and another thing that Carlos mentioned is like scoring the score or evaling the score.\nSo how well is this thing actually doing um based on the output that that we are seeing within our application.\nOkay, that's really activity one, right?\nIt's uh reviewing some of that stuff, and then it's creating that playground and showing you all like the the sort of way that we can uh iterate here within Braintrust to create better AI or genai products, right?\nLike this allows me to now, well, okay, so maybe this isn't the right model.\nMaybe if I do uh maybe I want to see this new GPT model.\nI can run this, and now I can see how the the model changed for that particular score, uh how the scores changed when I changed the underlying model.\nRight?\nBut now I have these, you have like all of these different inputs that could happen to the these applications.\nIt's a way for us to track and understand when I do go and tweak this thing.\nThere's actual data behind it, right?\nThis isn't like vibe check.\nThis isn't um, yep, I think that looks good.\nI looked at this row.\nIt seems like it's better output.\nThis is data behind it, and we can actually understand as we tweak that prompt, tweak that model, how does that impact our uh our scoring, and then again, you can like overlay this within CI and so on.\nQuestion.\nYeah.\nUm, I pulled the project down, and uh I have the Braintrust and everything, just don't know where I do the project that is on my machine to the Yeah.\nOkay.\nSo, you you cloned the repo.\nYeah.\nCan we get a where people are.\nYeah, please.\nWe can back up considerably.\nI, I can, I can just, you know, with you, we can fix this.\nUm, you've cloned the repo, correct?\nYeah.\nOkay.\nIn your enenv.local, do you have aenv.local file?\nI should have.\nUh, there is aenv.local.ample file.\nSo, you can copy that into the env.local.\nThose are the the keys that we want to fill in.\nHave you filled those in?\nNo, I did not.\nOkay.\nSo, if you haven't within your Braintrust or created an API key, am I the only one?\nI'm guessing no.\nYeah, I don't think so.\nI think the internet connection is probably the biggest thing we're fighting here.\nYeah, luckily all this, all the instructions will be available after the workshop.\nSame with the slides.\nI'm about to share the the slide link.\nSo tough to update with the connection, but um at least seeing Doug go through the same process will give you an understanding of, you know, what we were hoping you'd have that hands-on experience doing.\nUm, we don't have too much time to to wait on this specific activity.\nSo I think in a few minutes we'll keep going, and hopefully you'll be able to set up your keys so that you can catch up when when you have some time.\nYeah, just in the interest of time, we'll we'll probably move forward, but just to complete that, if you go into your settings within your project or excuse me, within your uh Braintrust, you'll see API keys.\nThis is where you're going to create this API key.\nYou did that.\nOkay, perfect.\nCreate that.\nYou put that in thatv.local file, and then you run pnpm install.\nSo that that is my in my home directory, right?\nThat's it's wherever you cloned that repo.\nSo at the root of that uh you put the the Braintrust API key in yourv.local file, and you should have a spot for it already if you're using sort of the template from the example.\nAnd then when you run pnpm install again, just to highlight this, it's actually running this command Braintrust push.\nSo, we're taking the the resources that we've configured in our in our project and is pushing it via our API key to the Braintrust or cool.\nAll right.\nUh, maybe going forward a little bit here to talk about uh the flip side of this, right?\nWe've been kind of in the UI for the most part uh in our Playground doing that that iteration, changing prompts, changing models, and so on.\nI think it's important to understand that we can do a lot of this via the SDK as well.\nUh, we also have a Python SDK if that's the kind of uh flavor you're you're most uh used to using or the language, but uh that that top portion here uh is essentially what we did, right?\nIn that that install, that post install script, we defined our assets in code, we defined scores, we defined prompts, um we defined a data set even, and then we push that into our Braintrust or that the benefit here is that again, we can use our repo, right?\nWe can leverage version control to ensure that the things that we want uh to change are actually version controlled alongside of everything else that we're building within that application.\nSo there are really two modes to actually work with with the Braintrust platform.\nIt's its UI or its SDK.\nAgain, there's really no limits that we place on on the on the user of the platform.\nIt's going to cater to maybe a different uh persona, a different use case.\nUh, but you can use kind of both of these different things.\nThe other thing that we haven't done yet uh that we did uh within the Playground when we ran that experiment, we we ran those evals, right?\nWe actually evaluated uh the task against the data set with our scores.\nYou can also define evals in code, right?\nYou can define the eval within your repo and a very similar command Braintrust eval.\nNow we can push that up to the Braintrust platform and essentially run that same thing, track it in that experiments.\nI have now an understanding over time how my uh my evals are performing as I go and change things, all those different things.\nA little bit more uh insight here, you probably saw with some of that code.\nThis is the uh the the command we're going to run Braintrust push.\nAnd you essentially give it either the name of a file that have eval or you can give it the name of a folder uh as long as your uh your files have eval.ts as their naming convention.\nAnd we're just going to go run those evals in that parallel fashion that you saw within the UI.\nA couple things maybe I mentioned earlier, but just important to highlight.\nYou should do this when you want source controlled prompt versioning.\nYou want consistent usage across your your different environments, and you also want to leverage online scoring.\nUh, mentioned this, obviously the the eval.ts, that's essentially what the SDK is looking for uh when we go and run those evals.\nIt makes it really easy to run a a larger subset of those without specifying each single file that you want to go run those eval.\nUm, but you can see, and I'll I'll let me jump into the actual activity.\nYou can see the eval that we've created.\nIt's it's what we've been talking about, right?\nThe three ingredients that we need.\nWe need that task, we need that data set, and then we need at least uh one score there as well.\nHow do you bootstrap a data set like with multiple examples?\nIs there given feedback in loop or is an LLM being used to create?\nYeah, so the question was like, how do you bootstrap a data set?\nUm, I think it's a good question.\nI, I think you could certainly start with synthetic data or you could even start with um, you know, you you release a feature, right?\nYou're going to you're going to start logging that feature.\nThis is another thing that we haven't yet covered, but you can actually use the logs from that to add to the data set.\nSo now you have actual real life data.\nThing to not do is wait until you have 100, 200, like you have this golden data set.\nIf you think back to that matrix that Carlos was showing, there are the different ways in which we could start to think about improving in the application based on what we observe.\nSo start with something small, and again, it could be synthetic, but then you can once you start to evaluate it, then you have a different um, you have different inputs on what you need to do to go and improve an a scorer or improve the application.\nBut it's it's really I think maybe up to you, the the like the best practice or the thing that I would think about is like don't don't stop yourself because you only have a small subset of data.\nOkay.\nYeah.\nSo, for the for the test you're running, if you're using an LLM as a judge, so like for the percent completeness score, you're using GPT41 as a judge.\nThat's subjectively scoring the test that you're running, right?\nSo, like for the for two runs that are that one after the other, you could end up with different scores, right?\nIf you're using like LL as a judge to run.\nUm, so the question was like, would I get different scores?\nUm, because I'm using an LLM to do this, right?\nAnd it's not really deterministic.\nUm, I think that's the the reason why you would use a better model so you don't see something like that.\nUm, I don't know, Carlos, you have any other thoughts there?\nYeah, I mean, I think you're speaking to the nature of an LLM being non-deterministic.\nSo, yes, there may be some variability.\nWhat we see our customers do, especially with the SDK, is do trial evals.\nSo, you will run it maybe five times and then take the average of those.\nSo, there are things that you can do to try to beat that, but it is the nature of the beast, and you have to learn to work with it.\nAnd then the other thing is how are you scoring like a percent completeness if the task of the LLM is like to judge like put it in categories like excellent, good, are you mapping those to scores or yeah, so I think that the question is, and if I come to the score and you look at the completeness, so the LLM here, yeah, it has to decide based again on like the criteria that we give it, um if it comes up with excellent, that maps to one and so on.\nBut again, this the score that it gives has to be between zero and one.\nYeah, it's really helpful when you're using an LLM as a judge to go through the Braintrust logs and read the rationale.\nSo, it will explain why it chose, you know, 100% or 75%, and you can use that to tune the LM as a judge and improve it.\nUm, it likely will not, you know, you don't want it to say 100% for everything, right?\nIf if that's the case, you need to improve your evals.\nAnd even if it's saying, you know, 30% for everything, it doesn't necessarily mean that it's performing horribly.\nUh, what really matters is the baseline that you're comparing it to.\nYou know, how did it perform yesterday on these scores on this data set?\nUh, you shouldn't be comparing just, you know, it needs to be 80%.\nNot necessarily.\nIt matters what happened yesterday, what you're comparing to previously.\nYeah.\nAnd this is where it becomes really beneficial to to be able to actually drill into what happened within that task.\nUh, so being able to not only understand the the calls that the LLM makes, the tools that it invokes, but actually drilling into those scores that we're using, and then like Carlos mentioned, what is the rationale that it gave uh to give it a good score here?\nDoes that make sense?\nThis becomes again another way of like this is the human review portion or part of building a genai app.\nYeah.\nDoes Braintree offer any kind of like optimization?\nSo you have your email down, you got tons of different.\nYeah, that's a good question, and it's something that we're thinking a lot about is how can we add LLM to optimize this process for you.\nNot just for prompt like you mentioned, but also for data sets and for scores.\nWe're one to two weeks away from releasing our first version of this.\nUh, we're going to call it Loop, and it will do exactly what you're saying.\nIt will help you optimize the prompts and improve your emails.\nJust to build on that, imagine like this um this feature Loop.\nIt has access to previous results as it's doing it.\nSo it understands when it makes that change, is it better than it was previously.\nSo it's it's it's sort of like that agentic type workflow where it has access to tools, but it's able to uh to iterate on that prompt and run those experiments uh with you of course, like in the middle to to prompt it to do different things, but it has access to those previous exper experiments and the results of that.\nSo it knows like the general direction it needs to go to get improvements.\nYeah.\nSo how do you build that?\nWe use Braintrust.\nYeah.\nExactly.\nIt's a way of dog fooding.\nSo the question was how do we evalow our AI feature?\nAnd it's you know, of course we have to use Braintrust, and it's it's honestly really cool to to look at the project and see all the logs coming in and and looking at the scores that we've chosen to go with.\nUm yeah, Braintrust is really helping, and it was actually something that Anker, our CEO, has talked about the process of of actually getting to a point where we were excited to release something like this.\nPreviously the models were not performing to the level that we were needing them to perform to.\nSo every few months he would run a new benchmark on this specific use case, and it wasn't until you know the last month that a model finally uh reached that that expectation.\nYeah, they gave me a mic, so hopefully you can hear me.\nUm, cascading off the gentleman in the front's question around um, the subjectivity of using the LLMs as a judge in these types of cases, do you offer any way to gain access to that rationale programmatically such that you could evaluate the thought process of the LLM as it's doing?\nIt's kind of meta like onestep review, but adding in that second layer where you could identify weak spots perhaps if there's something that's hyper workflow oriented or has a very strict process you're looking for the LLM to follow.\nYeah, I mean I think you probably saw when I highlighted here, correct me or thumbs up if you saw this or Yeah.\nOkay.\nThis is all accessible via via API.\nUh, coming back down here.\nSo like the the rationale that the LLM gave, we could certainly build something on on top of uh the these ration uh and then generate, you know, again, like eval type of workflow.\nCool.\nYeah.\nYeah.\nQuestion.\nSo I'm just curious like there's a mic if you'd like it.\nThank you.\nYeah.\nSo, uh yeah, I'm I'm just curious like how should we build our confidence around like, you know, the result of OM as a judge because I mean uh you know, how do we trust the result because is after all the model like evaluate\n\n\nThe data set\nand it's could like we could get like, you know, maybe a good result, but actually it's like maybe large model is just overconfidence or something like that. It's like we need to evaluate or eval results using like humans like at the beginning so that we can build a confidence like yeah, just curious any experience like you have here.\nYeah, definitely that's a great question. So, um, I guess everybody heard, don't need to repeat. Um, I think there's two things that you can do. One that you mentioned, which is involving a human reviewing that LM as a judge and confirming that is it's thinking in the right way. It's outputting the correct score. Um, another approach that you could do as well as the human in the loop is uh using deterministic scores. So full coding functions that are trying to grade the same type of criteria using regular expressions or some other logic that and you can approximate, right? So if the LM is a judge is giving a zero, but the you know, the deterministic code score is giving a way higher score, then you know that there's something that needs attention, needs to be fixed. The matrix as well that we showed at the beginning that pointed to, you know, should you improve your evals or improve your AI app, that's also a great resource.\nYes, thank you.\nUm, how are you guys thinking about the role, if you think there is a role, um, for traditional machine learning models in eval?\nTotally deterministic code and then on the other you have LLM as a judge. Do you think there's kind of a middle ground for things like intent classification models, entity recognition, uh, sentiment classification and clustering and and kind of more traditional machine learning approaches that kind of sit somewhere in between, you know, the the the totally deterministic versus totally non-deterministic spectrum of code versus LLMs. Like do do you think that there's a role for those type of models and what do you think that looks like?\nYeah, that's a good question. Um, I think it's still to be determined how this all shakes out. There are some customers, companies that we talk to that are going full deterministic. They don't use LM as a judge and then there are others that are very much going in the LLM as a judge route. And I think the reason that there's a split is because they both work. Uh, so you know, I don't know, I don't know how this will eventually shake out if we'll reach in the middle or if you know determinism will win. Um, what I can say though is that it's highly dependent on the use case, the problem that you're solving and experiment with both and then you can determine which one is working best.\nI guess those are like also largely code-based, right? The the things that you're talking about and so maybe they lean a little bit more toward towards that.\nYeah, I mean I'd say they're still kind of neural approaches are still entirely deterministic, but thank you.\nYeah. No, I got you. Um, it's it's sort of like that that middle ground. Um, yeah, I don't I don't have a great answer for you. I do think uh using Braintrust, uh, you you do have the ability to at least configure both of these, the LM as a as a judge and then the scorer, and then you can again using the the human review process find the ones that that actually map to the the right output the best and then that's how you start to build your application. But it's it's a really good question.\nThank you.\nHow's the activity going? I know we are getting, you know, last 25 minutes of of the session. We still have two more uh little slide chunks to go through. So maybe in, you know, two minutes or now we could move to the slides and then keep it going. Again, this will all be available after uh so feel free to keep working on it.\nYeah, maybe just really quick, we could run the the eval just from here. You could see um, we're actually going to take the the eval defined here, right? So, we have our our task, we have our scores, then we have our data set, essentially what we just did within the UI pushing that into the Braintrust platform. Uh, and then you can even see like this is where it's running right now.\nSo again, being able to do these in a couple different ways, either via the the SDK or from the the Playground itself.\nI think you explained this already and maybe I was distracted by the Wi-Fi, but how do I think about the difference between the Playground and the experiment?\nYeah, that's that's a great question. Let's see if we can just quickly go back to the slide.\nPlayground you could think of as quick iteration, experiment. So a playground ephemeral, right? Experiments long lived historical analysis, if that helps answer your question. Um, they're becoming more and more the same. You know, historically the experiments had a bit more bells and whistles. So I, you know, typically teams would gravitate towards the experiments, but we found that they really liked the quick iteration, they really liked using the UI, and so we started beefing it up and now they become pretty much the same. So so yeah, playground more ephemeral, quick iteration, you want to save the work that you've done to an experiment so that you can later review it and see the scores update and change over over time.\nI think it throws me off. So when I do an evaluate in my text editor, I should use the UI playground UI for that.\nYes. And remote evals which will allow you to define via the SDK the eval but then expose it into the Playground. So some it's it's like the bonus activity in the document at the very end. So maybe you should check that out and we won't have time in this session, but if you come to the one at 3:30, we will.\nAny other questions?\nOkay, cool. So, moving into lecture three. Uh, so this is, you know, once you've finished development, it's reaching customers. You're in Production, right? Now, what do you do?\nWell, the the important thing is logging. You want some observability. You want to understand what's going on. How are they using it? Are there any gaps? Are they unhappy?\nIt can help you debug uh a lot faster. It can allow you to measure quality instantly on that live traffic. You can turn those production traces, what you're logging, you can turn it into a data set and bring it back into the Playground, keep improving the prompt and you know it allows a lot of nontechnical people to understand what the end user is thinking. So you can close this feedback loop. We have a lot of PMs using Braintrust and going through the logs and and looking at that user feedback to understand what gaps and and improvements may exist.\nSo how do you log into Braintrust? Uh, well this is done via the SDK, right? It needs to plug into your production code. Uh, so these are some of the the steps here or the the tools that you can use. So you know you need you need to initialize a logger that will authenticate into Braintrust. It will connect it to a project. So now your logs will go to a specific project in Braintrust. Um, some great ways to to get started with really one line of code is to wrap your LLM client. So you can use wrap open AI around that LLM client and now any communication will get logged with your prompt response.\nAlso uh metrics. So how many tokens were sent back and forth, the latency, all errors, everything just by adding that that wrap open AI. You can do the same with Versel AI SDK. Uh or you could use OTEL. So we also integrate with OTEL. Um, so if you want to go that route, it's also available.\nIf you want to log and trace arbitrary functions, we also support that. You can just use a trace decorator around the function. um really helpful for keeping track of any uh functions that are helpful to to understand and keep track of. And then if you need to add additional information like metadata, you can use span.log. So it's it's very capable, very flexible, but there's still these, you know, one-line code ways to to get started.\nSo now that you're pushing all of your logs into Braintrust, you're capturing, you're observing real user traffic, now we're going to get into that, you know, online scoring piece as opposed to offline, right? So online is measuring the quality of live traffic. So you can decide how many logs that are coming in will get evaluated and scored. Uh, it could be 100%, it could be 1%, it's up to you. This allows you to set early regression alerts. So if it starts dropping below, you know, 70%, 60%, ultimately it depends on the score that you're using and what you've established as the baseline. But if it starts dropping below a critical amount, you can set up alerts and notify the correct team. Uh, you can also AB test different prompts. You can set up tagging and understand, oh, this trace coming from this user is from prompt A versus this one from prompt B. And you can compare uh the the grades, right? that the score results coming back in.\nSo, this is great for just improving feedback, moving quickly, and understanding if there's been a drop in quality.\nHow do you create an online scoring rule? Well, everything is done via the UI. You go to your project configurations, click on online scoring, and then you can add your rule. This is where you'll define what scores you want to be used on that live traffic. And then uh crucially that sampling rate. So maybe at the beginning you start with a lower sampling rate and then you can increase it once you trust the metrics coming in. You can also choose what span you want this online score to run on. So it defaults to the root span, but you can get more granular and specify, you know, I want this nested child span to be uh scored.\nOnce you start collecting these logs, collecting these online scores, oftentimes teams want to view them in interesting ways and customize the lenses on those logs. So that's where custom views come in. You can apply filters, sorts, you can customize columns on the logs with whatever information you'd like. And now you can start saving those views and making them available for the rest of your team to just come to the logs and select, oh, I want to go to, you know, the logs under 50% view or, you know, their own custom view that they've made that's specific to what they care about. Uh, so it's it's a great way of collaborating and and uh speeding up the process of viewing the important things to you.\nGreat. So, you know, we went through the slides. Now, we would jump back in to the to the activity document. There we can look at the actual uh code, see where the the logging is being captured in our files, spin up the application. So, you can actually view the application in your dev environment, interact with it, and you'll see those prompts and outputs being logged in Braintrust. Uh, so if if you've gotten that far and you have the the dependencies installed, then I would recommend doing a PNPM dev and now you'll have your application up and running, interact with it a few times and uh you'll see that populate in your project logs.\nOnce you do that, then you can go to your online scoring settings, set up a rule, and you can keep interacting with the app. And now you'll see it um populate with that online score that you just enabled.\nMaybe quick example for for those that are still having sort of Wi-Fi issues. Uh, come back down here.\nSo, I'm going to come and uh spin this up. So, as Carlos mentioned, uh PNPMdev, this is going to spin up that server on localhost 3000. Uh you should see something that looks like this.\nUh there's a few things that you can just click these. This is sort of like the easy button to get going. Uh this is the the GitHub URL. It's again it's looking for the the commits that have been made since the last release and it's going to summarize uh again using the the prompts that we've configured here and then start to categorize them.\nBut now the the interesting part of this if you come back into the Braintrust platform is if you look at the logs. So this is what just happened on the Braintrust side. Uh so we sort of have this top level uh trace, the generate change log request and then essentially the the the tool calls you can think of as right, we're getting the commits, we're getting the latest release. We're fetching those commits, we're then loading that prompt. So we're actually loading the prompt from from Braintrust and then you can start to you know you can click through a lot of these and as Carlos mentioned when you use those wrappers you get all of this sort of goodness out of the box, right? So what are the the number of prompt tokens, what is the estimated cost? This is uh this becomes really helpful as you start to monitor over time, right? Uh probably not set up yet because we don't have uh too much going on, but like actually understanding what does that token uh amount look like over time, what does the cost look like over time as we change models uh and so on. But that's how really easy this is. And maybe just to complete that that loop, if you come over to the uh resource, not the resources, the app generate route.ts, you'll start to see some of this stuff. So, I'll just highlight a couple things. We're wrapping this SDK AI SDK model. So, this again is is how we're really getting all of that that that metrics and uh it's it's really allowing for us to log a lot of that information with very little lift from from ourselves as developers.\nBut then you also have the ability to uh configure things in a different way. So maybe we have um different inputs or different outputs that we want to actually log in a particular span or actually we want to log metadata, right? This becomes really powerful when we want to actually go into those views, right? And we can actually start to filter these things down. We can even filter by that metadata. So uh this is where again you can hit the easy button. we're going to wrap our our LLM client with those SDKs or we can actually get a little bit more detailed and start to log uh the particular input and output information that we want metadata and now we can sort of uh set these different things. So if I come out here uh I can even create uh actually when we look when we add in the scores here we can create filters based on those scores. So I want to create a view that says hey anytime my completeness score goes below 50% I want to create a view for this. This is going to enable my human reviewers to go in and actually understand that. And then if you look up here in the top right, we can actually add this span to a data set really easily. So we find this thing, right? That'll actually add a lot of value in that offline eval type of process. Click this. Now we have a net new row in that data set that now adds a lot of value, right? This is sort of like that that feedback loop.\n\n\nRight? We've done that offline eval type of work. We have found the right prompt, the right model, all these different things. It's in production. We are logging it. Now we're sort of understanding that maybe in a human review type of way. Add that span of the data set. This adds again to the offline type of portion. Again, you just see this like this sort of flywheel effect of creating really powerful GenAI apps.\nYes.\nIs there a generated for this online log as well? Like as we ran it, is there an eval created for it? And do we add it to the data set based on if the eval score is good because we don't want bad examples in the data set? That's one way of thinking it. You don't need to run an eval as the user's interacting with it. That's what the online scoring does. So once you set up the online scoring rule, it'll output a score based on the judge that you've chosen as the online scoring rule.\nUm, and exactly what you said, right? You could either filter it and select the good responses, add those to a data set, or vice versa, select the bad responses and understand why are they bad, how do I improve them, make them better.\nJust to complete that and kind of how do we configure this? Right? We, Carlos, walked through like what this would look like. Um, this is, you know, my score, my rule, right? Obviously, you would call that something a little bit better, but I actually want to, you know, add in my scores for those online logs, right? Then you would, you probably wouldn't do 100%, but we're just going to do that for this instance. And now when I come back to my application here, right? And maybe I want to do just do a quick refresh. So now when these logs happen within the Braintrust side, we're actually going to run those scores against that output. So we'll understand based on what happened here, how did it score on formatting, how did it score on correctness.\nUm, and then this also can now layer into, so you can see here we have a 25% on the accuracy on 100% on the completeness. So maybe we have a little bit work to do. But now if I click into this, this is where you can start to now create different things within the view portion here to ensure, like, so this is a filter. So maybe I want to change this to anything that is less than, let's say, 50%.\nNow I can save this as a view, and my human reviewers are able to now come in here, open up this view, and look at for, look, look at all of the logs where my accuracy score is less than 50%. And now we can again create that sort of iterative feedback loop.\nAny questions on this section?\nYeah, maybe a good segue to the human in the loop. Um, this this becomes really, uh, you almost, you almost really, oh, sorry. Um, I had a question about using Braintrust and implementing it on existing projects. Is it something that's easy to do with like a um, something like Langsmith? Can just add like a couple lines and it'll trace everything for you. Is it the same in Braintrust? Is it, or do you have to refactor all your prompts to use like the Braintrust prompts?\nIt's essentially the same thing. So like you have Langsmith has code to wrap an LLM client, right? Or it has decorators to put on functions.\nIt's the exact same thing on the Braintrust side.\nGot it.\nYeah. So then if you do that, is it easy to then use um, like create data sets from those logs?\nYeah, absolutely. As long as you're the logs that you're producing map to the structure of the data set that you've created, then absolutely, it becomes really just you click that button. We're going to add that span to the data set, and it becomes really easy to connect those two things.\nCool. Thanks.\nYeah, of course.\nYeah, good question.\nUm, okay. Yeah. So, let's talk maybe a little bit really quickly about the question over here.\nIs there a way to just override that certain sample set?\nThe way that you would go about that is changing the span that is targeted. So instead of it applying to the root span, you would specify a span that only happens if a certain criteria is met.\nRight?\nSo then it could be 100% or 50% of just when that span appears.\nOkay. So yeah, this is where we could we could bring sort of like the human in the loop type of workflow. This is where we want to actually maybe bring in subject matter experts, and Carlos mentioned like product managers, maybe, we also have doctors coming into the platform and actually evaluating some of this stuff, right? These are the people that actually understand whether or not that output that's created by that large language model is valid, is good, right? And this is a really powerful thing to have as part of the process to building really powerful AI applications. We can catch hallucinations. Being able to establish that solid foundation or that ground truth is oftentimes, um, you know, having that human in the review in the loop type of person becomes really beneficial here.\nUh, so why does this matter? Matter?\nExcuse me. Uh, it's it's really critical for quality and reliability. Uh, like we were just talking about, like, with LLMs and being able to trust whether or not they can do the same thing over and over again. It's non-deterministic. Automations can miss that nuance, right? We want to be able to sort of apply that human type of um review to the things that we're doing on the AI side with LLM as a judge type scorer. Uh, we also want to um help you make sure the final product meets the actual expectations of the user, right? So the user is going to have a much better understanding of how to or like what the final output should be. So having that person in the loop to look at those outputs becomes really powerful to ensuring that you build really, uh, really strong GenAI applications.\nUh, two types of human in the loop. Uh, there is the human review. Uh, so this is where these people are actually going to go into the Braintrust platform and actually manually label, score, and audit those interactions that the user had with the AI application. And then there's actual feedback from the user real time. Uh, so this is like, you know, a thumbs up, thumbs down button within the application saying, hey, you did a really good job, you did a really bad job, but now we can, you can sort of use these together as well. I can now create a view within Braintrust that filters down to any of my user feedbacks of zero. So thumbs down, and I actually want to review whether or not those things are bad, and if they are bad, I can add those to the data set, again, creating that that sort of flywheel effect.\nUm, just really quick here, if you go, if you look at that application, you're able to actually click one of these, you know, thumbs up or thumbs down, create a comment. Really good. And then this is now logged back to the Braintrust application.\nSo, if you look back at our logs, I'll remove our filter. Uh, so we should have a user feedback score now here of 100%. And then we should have a comment over here as well. Really good. But then again, these are the things that we can now, if I open this back up, I can create a filter on my user feedback score. Uh, now I want to understand all of my logs where user feedback is one or zero, and then I can do something from there. But this is this is done very easily via the log feedback function within Braintrust. You provide it sort of like the span that that was already created within that log, and then you just you log, you provide that user feedback to it.\nUm, you can also enter really quickly here in the platform. You can enter human review mode. So this is a way in which um it's sort of hiding away some of the different, uh, you know, some fields that may not be really relevant for those people that are coming in and doing human review.\nOh, I haven't actually configured any scores yet. So, you can actually see, uh, you would come out here and you would create different scores for that human to go in and do that review.\nWhether it's sort of like an option-based, uh, free form input, uh, slider, so is this, you know, maybe thumbs up, thumbs down, sort of like yes or no, or maybe you could do something a little bit more verbose, um, ABCD, whatever it is. But this is where you create these scores. Now, they exist as part of those logs. Those humans can go in and now look at the input and the output, give their review of it, again, adding to that, that again, like that that flywheel effect that we need to create.\nYeah, I wanted to add there as well. This is really helpful for evaling your LLM as a judge. Oftentimes, we see customers use this process to provide some ground truth for their data sets and also for the LLM as a judge, right? So you can imagine having a team of memes come in, they review, uh, they do thumbs up, thumbs down on maybe five different criteria qualities that they're measuring, and then they provide that data to a Playground where the prompt is the LLM as a judge, and they go through the Playground and they test to make sure that the LLM is a judge prompt matches what the humans thought. So just something there to think about, but as Doug is saying, it's a great feedback loop. It's a great flywheel effect that can be created when you add this human to verify and confirm.\nCool. That that is it for the workshop. We do have a few minutes left. Could certainly answer a couple more questions.\nSo for people who are successful with this, how much time are they going forward before they get into like more live testing and then how are they going back? What's the kind of balance offline versus online?\nEvals, how much how much you have to do up front to really get best results, or you really just put something down, figure it out later, get analysis, right?\nYeah. The question, just to repeat it, is how much time do you have to invest upfront to get value? Should you keep going over over it to try to optimize, or better to just start quickly with minimal scores, minimal data set, and then keep improving? And I would say the latter, right? You don't want to be fixated on creating a golden data set or 20 scores. Like if you have one or two scores and you have 10 rows in a data set, it's going to be tremendously helpful. And then from there, it's all about iteration. So just going back and improving, adding some more rows, adding another score, tweaking the scores, but you really just want to get started quickly.\nYeah. Um, so, um, you've mentioned um some elements of this scoring. Uh, there's the function that you want to test, that you have to define the test steps, if you will. Um, one of the challenges that we are finding is our actual application does change, and it could change bi-weekly, it could change monthly.\nIs there a way to look at um trying to automate changing the actual function that you now need to change to match the way that your application logic has just changed this week from two weeks ago when you say, oh, go for it?\nYeah, I guess I was just going to again clarify to make sure I understood. So you're saying that the scorer, the actual scoring function is going to uh stop being useful. It's going to become obsolete. It's going to become too old to actually gauge the quality.\nNot just the scoring function, but the actual steps that you want to test. So, you know, this week there might be only two turns, just giving a very simple example, and in two weeks, in the next sprint, there are now five turns in your app because the logic has changed, and now you you have to update, of course, the I think the function element, there's probably no way around it. I'm just curious about whether you guys have thoughts about how that could be improved or or made easier.\nWell, I think your task all will always change. Right. Right. The thing that we're trying to build\nYes.\nThat's where Braintrust helps because we're going to understand when we do go make that change, we actually understand whether or not that change improved our application or regressed it. So like we're not going to say stop making changes to the underlying.\nYes. No, I I real, I understand that. So it is inevitable that the application is going to be changing.\nYeah.\nAnd you're going to have to constantly update the corres the the test at the test step, the function that you're actually wanting to mimic in your test. It's very similar to traditional software testing. Yeah. You don't want to write a test that lasts for a day or, you know, a week, right? You want to think of robust tests that will live on for months or years and will actually measure the underlying quality of the application that will be long-lived.\nSo the I think it's more of how do you optimize the scores to measure qualities that will still be around even if you add some additional steps in the task.\nUh, no, it's worse than that because unless you have those additional steps in your function, you're not mimicking your application's logic. You're still using the logic from two, you know, last sprint. So no matter how good your scoring could be, it's not, no, it's no longer reflecting what your application is doing this week or this sprint. I I think like regardless of like how many steps you have, like there's still an input and there's there's still an output that we want to score against.\nCorrect.\nYes. But um, I think one of the things you need to do is to first define how you're going to arrive at the score. The input comes in, and now maybe you have three turns, and then because you're mimicking your app, and then you get your output from these three turns, your app just got upgraded, they're now seven or five turns or whatever.\nYeah.\nSo the when you're writing the evals, you can dynamically call the task. So even as you're working on your application and it's changing, you're still pointing to the the changing app. So the idea is that when you are wanting to merge into main, you open a PR, and then your evals will run on those new changes. You don't need to go in and update the eval.ts files. They will now reference the updated task application that you're trying to understand the underlying quality for, if that makes sense. So I think the question again is, are the scores, is the underlying logic something that you can trust and that will live on? Um, again, it's not easy, and it's something that is changing. Uh, but that's that's what we're hearing from customers is investing in that brains.\n\n\nMemes.\n\nWhat's the name of that role, and how are you managing that? Like, I'm guessing to some extent it was originally the team, right? But that can't scale. So how are you managing that?\n\nI think it's organization specific. I don't know if there's a specific. I would say your organization using your own tool. How are you managing yourself?\n\nI don't think we're using any memes at the moment. We're not a healthcare company or a legal tech company where we heavily rely on specialized knowledge in that degree, you know. Um, you're not doing human evals of your own product.\n\nWe just now started, we just now branched into having an AI component to our application. Uh, so we haven't needed to go there just yet. But we, you know, we talk to a lot of customers that are working in those specific industries with those use cases, and they will sometimes hire external services that will do the annotations for them, or they'll bring them into Braintrust, and you know, they'll be using the platform just to review. So they have a specific role within Braintrust, and there's a specific view that they would operate in that's just for annotation.\n\nYeah.\n\nGreat. Yeah, another question over here. Uh, I was just curious that because we're using out-of-the-box AI models here and are not really fine-tuning the model as the application progresses, do we have a way to like do some few-shot example prompting from the data set and the eval scores that we are already using? So is there some feature like that where I can use the data sets or the online logs that are added to the data sets? If the eval score is good, use it as an example for future prompts to just make the prompt better because the models are out of the box.\n\nYeah. So question around few-shot prompting, providing examples to the prompt of the ideal response. That's something that you can do today with in the data set in the metadata column is where you can provide the few-shot examples that you want for each row, and then when you're running that eval or messing around in the Playground, it'll reference the few shots in the metadata.\n\nGot it. But what about like the online testing stuff, right? Or the online logs, whatever you call it. Like when users are actually using the application and it's hitting the prompt, can the prompt real time use those examples from the data sets as well?\n\nRight now it's not. It's not something that Braintrust facilitates within the SDK, and building your own logic like you could come up with a workflow like this, but natively in the platform, we're not facilitating like live traffic into few-shot examples.\n\nGot it. Makes sense.\n\nGreat. Well, thanks everyone. I know we're over time. Really great to have you all here for our first workshop of the day. I hope you can walk away with some ideas of how you can improve your eval workflow. And you know, our team is here. We have a booth just outside of this. So, feel free to stop by. We can answer more questions, have a conversation. Yeah. Thanks everyone.\n\nThank you all.\n\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.478Z"
}