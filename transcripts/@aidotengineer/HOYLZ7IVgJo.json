{
  "episodeId": "HOYLZ7IVgJo",
  "channelSlug": "@aidotengineer",
  "title": "Shipping an Enterprise Voice AI Agent in 100 Days - Peter Bar, Intercom Fin",
  "publishedAt": "2025-07-18T16:00:06.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 0.33,
      "duration": 6.97
    },
    {
      "lang": "en",
      "text": "Today I'm going to be talking about Finn",
      "offset": 15.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Voice. And Finnvoice is a voice agent",
      "offset": 17.279,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "for phone support. Um, and we design it",
      "offset": 20.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "to be a frontline teammate for invone",
      "offset": 23.359,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "calls. So it picks up the phone, answer",
      "offset": 25.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "customers questions, and then escalates",
      "offset": 27.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "to a human agent when needed. And we",
      "offset": 29.119,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "build this experience in about 100 days.",
      "offset": 31.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "So today in this talk, I'll share what",
      "offset": 34.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "it took to get there. And I'll also talk",
      "offset": 36.239,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "about why I believe voice is the next",
      "offset": 39.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "big frontier and AI for customer",
      "offset": 40.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "service.",
      "offset": 42.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So first a little bit of context of my",
      "offset": 45.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "company, Intercom. Um so we're a",
      "offset": 47.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "customer service platform um and also an",
      "offset": 49.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "AI agent company. And you might be",
      "offset": 52.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "familiar with us because of our",
      "offset": 54.079,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "messenger product. Uh seen it, you might",
      "offset": 55.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have seen it in the mobile app or on the",
      "offset": 57.84,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "website. Uh but yeah, it's been our",
      "offset": 59.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "foundation for years. Uh but we evolved",
      "offset": 61.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "over time. We became a complete customer",
      "offset": 63.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "service platform a few years ago. Added",
      "offset": 65.28,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "robust tooling for other channels like",
      "offset": 66.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like email, WhatsApp, and phone.",
      "offset": 68.479,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "And then two years ago, right after the",
      "offset": 71.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "launch of GPD4, we launched Finn uh",
      "offset": 73.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "which is an AI uh uh agent uh via on",
      "offset": 76.64,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "text on chat. And Finn's growth has been",
      "offset": 80.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "incredible. Uh we have over 5,000 of of",
      "offset": 83.439,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "c customers and also in terms of",
      "offset": 85.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "performance is reaching average",
      "offset": 88.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "resolution rate of 56% and for some",
      "offset": 89.84,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "customer to be 70 80%. Um and this is",
      "offset": 92.72,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "defined as a percent of interactions",
      "offset": 96.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "handled by Finn that are resolved",
      "offset": 99.119,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "without human intervention.",
      "offset": 100.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And Finn is also a full system for",
      "offset": 103.759,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "continuous optimization. So it's not",
      "offset": 105.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "just that the agent but also tooling for",
      "offset": 107.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "analyzing conversations, training the",
      "offset": 109.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "behavior of the agent and also testing",
      "offset": 111.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and deploying changes.",
      "offset": 113.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Uh but yeah, up until now we didn't have",
      "offset": 115.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the voice channel and that's what we're",
      "offset": 118.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "changing with Finn voice is the same",
      "offset": 119.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "system but now it can answer phone",
      "offset": 121.119,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "calls.",
      "offset": 122.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "And a few thoughts on uh why voice? Why",
      "offset": 124.479,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "are we investing in this channel? Um so",
      "offset": 127.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "for all of users voice is simply the",
      "offset": 130.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "preferred way to get help. Uh so when an",
      "offset": 133.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "issue is urgent or sensitive uh they",
      "offset": 136.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "don't necessarily want to type they just",
      "offset": 139.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "want to talk and if you look at the uh",
      "offset": 141.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "some of the top level data over 80% of",
      "offset": 144.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "support teams uh still use uh phone",
      "offset": 147.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "support and if you think about all the",
      "offset": 149.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "conversations globally or customer",
      "offset": 151.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "service interactions over onethird of",
      "offset": 153.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "them are uh happening over the phone as",
      "offset": 155.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "well. So it's not a legacy channel",
      "offset": 157.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that's going away it's still widely used",
      "offset": 159.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and it's also quite costly. Uh, if you",
      "offset": 162,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "think about the average cost of handling",
      "offset": 164.56,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "a a phone call in the US with human",
      "offset": 166.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "support, it's about between seven and 12",
      "offset": 169.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "dollars. And with voice AI agents, it",
      "offset": 171.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "can be at least five times cheaper.",
      "offset": 173.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "And a few more benefits of voice AI and",
      "offset": 176.72,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "customer service. Uh, so first",
      "offset": 179.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "availability, 247 support. Uh, you can",
      "offset": 180.879,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "call your bank on the weekend. Uh, no",
      "offset": 184.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "wait time, so instantly available.",
      "offset": 186.239,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "There's no need to be to stay in be on",
      "offset": 188.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "hold in a queue. uh no IVR menus, no",
      "offset": 190.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "need to press one to go to support,",
      "offset": 193.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "press two go to payments uh because all",
      "offset": 195.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is everything is happening via natural",
      "offset": 197.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "speech and also multilingual so uh AI",
      "offset": 199.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "agents can support 30 40 plus languages",
      "offset": 202.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "obviously better for the for the users",
      "offset": 204.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and on the business side major cost",
      "offset": 206.72,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "savings and also scalability as the",
      "offset": 208.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "business grows or when you need to",
      "offset": 210.879,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "handle peak times AI agents were much",
      "offset": 212.799,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "better for that.",
      "offset": 215.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Um so how we built finoice uh over the",
      "offset": 218.239,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "next few minutes um I want to cover uh",
      "offset": 221.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "seven main areas that had the biggest",
      "offset": 224.239,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "impact of how fin came about. I'll try",
      "offset": 226.319,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to be more practical practical focusing",
      "offset": 228.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "on the some of the product decisions we",
      "offset": 230.879,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "made and the challenges we faced. Um and",
      "offset": 232.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and yeah the first one is is the use",
      "offset": 236.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "case. So the starting point for for",
      "offset": 238.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "voice uh then also the scope of our MVP.",
      "offset": 240.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "uh the tech stack behind it, the how we",
      "offset": 243.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "approached the conversation design, how",
      "offset": 246.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we integrated it with the support teams",
      "offset": 248.239,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and also uh how we thought about",
      "offset": 251.04,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "evaluation and pricing.",
      "offset": 252.879,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "So starting with the use case, um if you",
      "offset": 257.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "look at a lot of the voice AI startups",
      "offset": 260.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "in the space, they typically start with",
      "offset": 262.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "a a narrow problem space. I mean like",
      "offset": 265.28,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "scheduling a dentist appointment or uh",
      "offset": 268.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "booking a table at restaurant. And we",
      "offset": 271.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "looked at at some of those options, but",
      "offset": 272.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "eventually decided to go for a more",
      "offset": 275.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "flexible uh knowledgebased agent. So an",
      "offset": 277.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "agent that can answer helps uh her",
      "offset": 280.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "article questions like what are your",
      "offset": 282.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "pricing plans or uh what's your returns",
      "offset": 284.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "policy and why did we decide to go this",
      "offset": 287.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "way? So first uh we had a strong",
      "offset": 290.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "evidence from chat. Finn over chat has",
      "offset": 292.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "been handling those kind of",
      "offset": 295.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "conversations for years and our",
      "offset": 296,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "customers have constantly told us that",
      "offset": 298.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "they seeing the same type of issues over",
      "offset": 299.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the over the phone as they see on chat",
      "offset": 301.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and we also validated this through extra",
      "offset": 304,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "analysis of of call transcripts and this",
      "offset": 306.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "confirmed that a very large percentage",
      "offset": 309.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "of all the queries could be solved with",
      "offset": 310.639,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the with the knowledge base with the",
      "offset": 312.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "help articles content rather than say",
      "offset": 315.039,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "with the API integrations",
      "offset": 317.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and we're also thinking about the",
      "offset": 319.759,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "initial wedge use case so like the",
      "offset": 321.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "what's the lowest possible risk way for",
      "offset": 323.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "companies to integrate voice agents and",
      "offset": 325.199,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we looked at the in office hours and",
      "offset": 327.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "outside of office hours use cases. So we",
      "offset": 329.919,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "pitch out of office hours as initial",
      "offset": 333.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "wedge because essentially it allows the",
      "offset": 334.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "team to not affect the main workflows",
      "offset": 337.28,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and try out this technology build up",
      "offset": 339.52,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "more confidence over time and later",
      "offset": 341.759,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "deploy it for their on their in the main",
      "offset": 343.759,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "office hours. But in out of office hours",
      "offset": 345.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it just replaces their voicemail",
      "offset": 347.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "experience.",
      "offset": 349.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And there's also a few other use cases.",
      "offset": 351.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "We looked at uh authentication, so",
      "offset": 353.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "verifying user identity on another",
      "offset": 355.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "channel. Uh info gathering, so the agent",
      "offset": 357.36,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "getting stuff like uh order ID, account",
      "offset": 360.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "ID, and also uh smart routting to the",
      "offset": 362.479,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "right team. So these use cases are uh",
      "offset": 365.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "still very high leverage because they",
      "offset": 369.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "can save a lot of time for the support",
      "offset": 370.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "team agents. Um uh but they not",
      "offset": 372.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "necessarily solving the issue end to",
      "offset": 374.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "end. So we're still foc focusing on",
      "offset": 376.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "those but they were in say the primary",
      "offset": 378.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "use case for this initial version of the",
      "offset": 380.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "product.",
      "offset": 382,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So now moving on to uh what we shipped",
      "offset": 384.8,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "first. Um",
      "offset": 387.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "uh when we uh when we started started",
      "offset": 389.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "this like the biggest challenge there",
      "offset": 392.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "was to ship something meaningful as soon",
      "offset": 394.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "as possible. We had access to a lot of",
      "offset": 396.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "customers uh because we already had",
      "offset": 398.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "thousands of customers using our native",
      "offset": 401.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "uh phone support product. So it's mostly",
      "offset": 403.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "about how can we test as quickly as",
      "offset": 405.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "possible. So we focus on the three main",
      "offset": 407.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "experiences uh testing, deploying and",
      "offset": 409.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "monitoring the agent behavior. So first",
      "offset": 411.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "test uh this is what we call the fin",
      "offset": 414.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "voice playground. And here was like as a",
      "offset": 416.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "lightweight test environment for the",
      "offset": 419.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "customer service managers to go in and",
      "offset": 421.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "simulate a few sessions uh ask the",
      "offset": 423.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "questions based on the knowledge base,",
      "offset": 426,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "get those answers and get an idea how",
      "offset": 427.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this product actually works. And it was",
      "offset": 428.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "also man we shipped this probably within",
      "offset": 431.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the first four weeks of the project. Uh",
      "offset": 432.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so it was like the the fastest possible",
      "offset": 434.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "way to get get some feedback from",
      "offset": 436.56,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "customer service managers. This was how",
      "offset": 438,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "it's actually performing. So we can do",
      "offset": 439.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "optimization not just based on our",
      "offset": 441.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "internal views but also based on",
      "offset": 442.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "customer feedback.",
      "offset": 444.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Um then the deploy experience this was",
      "offset": 447.199,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to allow um uh customer service managers",
      "offset": 449.199,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to actually deploy it on their phone",
      "offset": 452.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "lines and included some uh basic",
      "offset": 453.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "configuration in terms of agent behavior",
      "offset": 456,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and also how it should interact with the",
      "offset": 458,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "customer service team's workflows",
      "offset": 460.56,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "and lastly observability uh or",
      "offset": 464.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "monitoring uh really want to provide",
      "offset": 466.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "some visibility into into what's",
      "offset": 469.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "actually happening on those calls with",
      "offset": 470.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "an AI agent. Uh so we had those",
      "offset": 472.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "experiences that did show uh the",
      "offset": 475.039,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "transcripts recording and also the",
      "offset": 477.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "transcript summaries and called outcomes",
      "offset": 478.56,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "to customer service agents.",
      "offset": 480.24,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "Cool. And now moving to the text stack.",
      "offset": 486.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "I'm not going to go through the",
      "offset": 488.319,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "technical detail of everything. I'm sure",
      "offset": 489.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "that there's going to be a few more",
      "offset": 491.199,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "talks on this that you might attend as",
      "offset": 492.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "well. Uh but I'll mention some core",
      "offset": 494.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "components and and fin. Uh so there's",
      "offset": 496.56,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "the main uh chained loop for uh for the",
      "offset": 500.16,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "voice agent ST lm tts uh so speech to",
      "offset": 503.759,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "text converting uh speech into text lm",
      "offset": 507.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "for uh actually dating the response and",
      "offset": 510.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "text to speech for converting text back",
      "offset": 513.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "into audio uh but there's also another",
      "offset": 515.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "approach with the voicetovoice models",
      "offset": 517.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "where uh everything is processed",
      "offset": 518.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "directly in audio while skipping the",
      "offset": 521.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "text layer entirely. So uh and the voice",
      "offset": 523.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "voice to voice voice approach has the",
      "offset": 526.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "benefit of uh potentially faster or",
      "offset": 528.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "natural more sounding speech but also",
      "offset": 530.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "gives you less control over the output.",
      "offset": 532.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Uh so in our approach we did start with",
      "offset": 534.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "realtime API by itself from the get-go",
      "offset": 536.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and it allowed us to test very very",
      "offset": 539.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "quickly. Uh but eventually we did evolve",
      "offset": 540.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "our stack but we still using real time",
      "offset": 543.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "API as part of of the core architecture.",
      "offset": 545.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And there are two other components I",
      "offset": 548.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "want to mention rag and telefony. So rag",
      "offset": 550.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "obviously is super critical for a lot of",
      "offset": 553.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "agent experiences. Uh but obviously it",
      "offset": 555.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is important for the uh agent answering",
      "offset": 557.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "questions based on the knowledge base u",
      "offset": 560.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and then telephony. Um uh so actually",
      "offset": 562.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "being able to put the agent on the phone",
      "offset": 566.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "lines and we had a bit of a head start",
      "offset": 568,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "because our agent on chat already had",
      "offset": 570.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the rack set up and we already had a",
      "offset": 572.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "native phone support product. So we got",
      "offset": 575.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "some of those things for free.",
      "offset": 577.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Now once uh uh we have the technical",
      "offset": 581.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "foundation in place uh is a question",
      "offset": 584.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "about how do we actually design the",
      "offset": 586.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "conversations for voice. Um so um",
      "offset": 587.68,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "intercom has a background in chat but we",
      "offset": 592.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "knew from the get-go that the approach",
      "offset": 595.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "for voice will have to be a bit",
      "offset": 596.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "different um and that voice is not",
      "offset": 598.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "necessarily just chat with sound. So",
      "offset": 601.279,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "there are like three key differences I",
      "offset": 603.04,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "want to mention. There's obviously many",
      "offset": 604.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "many more but there's a few that I",
      "offset": 605.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "thought it's worth mentioning. So",
      "offset": 607.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "latency um",
      "offset": 609.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "on chat it's actually I think okay to",
      "offset": 612.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "wait for a few seconds for response at",
      "offset": 614.72,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "least from the user perspective that",
      "offset": 616.079,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "there's a lot of tolerance but obviously",
      "offset": 617.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it doesn't really work on voice if the",
      "offset": 619.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "agent goes silence for for a second or",
      "offset": 622,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "two or maybe longer uh the user might",
      "offset": 623.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "assume that something has gone wrong. So",
      "offset": 626.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "in terms of our approach for the simple",
      "offset": 628,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "queries uh we got it to about 1 second",
      "offset": 629.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so we didn't need to do anything extra",
      "offset": 632.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "uh but for more complex queries when",
      "offset": 634.88,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "they they're running a bit longer three",
      "offset": 636.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "four seconds that we added injected",
      "offset": 637.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "filler words I mean like let let me look",
      "offset": 639.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "into this for you let me look it up uh",
      "offset": 641.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to maintain the conversation flow while",
      "offset": 644,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we generate the answer in the background",
      "offset": 645.92,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "then the second one is answer length uh",
      "offset": 648.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "so again on chat it's actually probably",
      "offset": 651.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "desirable in those agents customer",
      "offset": 653.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "service agents to provide a bit of a",
      "offset": 656,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "longer response to uh provide as much",
      "offset": 657.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "context as possible to the user and it's",
      "offset": 660.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "easy to skim through the answer to find",
      "offset": 662.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "the right information. But again, it",
      "offset": 664.24,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "wouldn't work for on voice. You don't",
      "offset": 666.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "want to wait there for a minute or two",
      "offset": 667.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "listening to to the agent. So for more",
      "offset": 669.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "complex responses or for responses with",
      "offset": 671.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "multiple steps, we're breaking down the",
      "offset": 673.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "answers into multiple chunks and deliver",
      "offset": 675.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "chunk by chunk and after each we ask the",
      "offset": 679.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "user to confirm whether they would like",
      "offset": 681.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to listen to the next step. And this",
      "offset": 682.88,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "works really well for something like",
      "offset": 684.48,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "troubleshooting when you have a few",
      "offset": 685.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "steps uh to follow.",
      "offset": 687.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "And lastly, the user mindset. So, well,",
      "offset": 689.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "something interesting in during our ali",
      "offset": 692.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "testing and real phone calls is uh some",
      "offset": 694.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "customers would interact with invoice",
      "offset": 697.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "like with an old school IBR. So, we just",
      "offset": 698.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "using single words like support uh",
      "offset": 701.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "password reset, yes, no. But then",
      "offset": 704.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "throughout the conversation, I've",
      "offset": 707.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "listened to a lot of those calls is like",
      "offset": 708.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they change their behavior during the",
      "offset": 710.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "call and actually start using full",
      "offset": 712.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "sentences while they hear the agent",
      "offset": 714.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "using full sentences. Um, and one of my",
      "offset": 716.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "colleagues like summed about nicely that",
      "offset": 719.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "crazy how the human speaks more like a",
      "offset": 721.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "bot and the bot speaks more like a",
      "offset": 722.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "human. Uh, and I think this will change",
      "offset": 724.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "over time. uh to some extent train over",
      "offset": 727.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "chats and it's about uh obviously voice",
      "offset": 729.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "getting better and people more being",
      "offset": 733.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "more common as a technology people going",
      "offset": 735.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to get used to it but for now for now",
      "offset": 737.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's on us to make those uh",
      "offset": 739.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "conversations sound as natural as",
      "offset": 741.12,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "possible so we help with this transition",
      "offset": 742.56,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "now thinking about how Finn integrates",
      "offset": 747.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "into support workflows uh so this was",
      "offset": 748.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "super important and definitely",
      "offset": 751.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "surprising for me uh when u when we got",
      "offset": 753.279,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "to this point is that majority of the",
      "offset": 757.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "feedback wasn't about the voice uh about",
      "offset": 760.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the model or about the latency was",
      "offset": 762.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "actually about how does it work with the",
      "offset": 764.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "support team workflows and don't get me",
      "offset": 766.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "wrong I do think that all those core",
      "offset": 768.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "model experience are super important but",
      "offset": 770.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "this actually became a bigger blocker",
      "offset": 772.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for all those teams uh so we put a lot",
      "offset": 773.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "of focus that the integrations points",
      "offset": 776.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "are as smooth as possible so we did a",
      "offset": 778.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "bunch of other things but I to mention",
      "offset": 780.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "two here one is the escalation paths so",
      "offset": 782.24,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "getting the calls um uh getting",
      "offset": 784.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "configurations for how the calls get",
      "offset": 788.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "escalated to the human support team and",
      "offset": 789.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "also the context handoff. So after every",
      "offset": 792,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "AI agent call, we generate a transcript",
      "offset": 795.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "summary that gives a bit more context to",
      "offset": 797.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the human agent that gets the call to",
      "offset": 798.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "what happened on the call. And yeah,",
      "offset": 800.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "these are not like super flashy",
      "offset": 802.56,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "features, but they were absolutely",
      "offset": 803.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "essential to get this from this demo",
      "offset": 805.519,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "stage to the deployment stage for larger",
      "offset": 806.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "customers.",
      "offset": 808.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "And then how do we know it's working? Uh",
      "offset": 812,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so uh there's a few topics I wanted to",
      "offset": 814.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "touch on. So um one is the manual and",
      "offset": 816.16,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "automated evals. Uh so we had a test a",
      "offset": 818.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "set of uh test conversations that will",
      "offset": 822.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be running through on every ma major",
      "offset": 825.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "code uh code change. Initially it was",
      "offset": 827.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "mostly manual just in a spreadsheet but",
      "offset": 829.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "over time we added some automation.",
      "offset": 831.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Number two is internal tooling and this",
      "offset": 833.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "was super critical um h for",
      "offset": 835.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "troubleshooting. So essentially we built",
      "offset": 838,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some internal the streamed web apps to",
      "offset": 839.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "review the logs the the transcripts the",
      "offset": 842.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "recordings. So any any time one of our",
      "offset": 844.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "customer there's an issue we can review",
      "offset": 846.959,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "in detail what happened in the",
      "offset": 848.639,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "conversation actually troubleshoot it",
      "offset": 849.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with with the logs. Um number three",
      "offset": 851.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "resolution rate. So this is our",
      "offset": 854.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "northstar metric which actually tells us",
      "offset": 855.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "whether we're delivering value for our",
      "offset": 858.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "customers. So we define it as um uh",
      "offset": 859.839,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "essentially either the user confirming",
      "offset": 864.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "on the call that the issue was resolved",
      "offset": 866.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "or the user disconnects after hearing at",
      "offset": 868.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "least one answer um and then doesn't",
      "offset": 870.639,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "call back within 24 hours. Um and yeah,",
      "offset": 873.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "this is the main the main metric metric",
      "offset": 876.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "metric we track. Obviously there's more",
      "offset": 878.399,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "in customer service, but this is kind of",
      "offset": 879.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the main success metrics that we have.",
      "offset": 880.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And lastly, LM as a judge. This is more",
      "offset": 883.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "experimental, but we're using another to",
      "offset": 884.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "analyze the call transcripts to help us",
      "offset": 887.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "identify issues or opportunities for",
      "offset": 889.839,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "improvement.",
      "offset": 891.279,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "And lastly, uh how to price it. Uh so",
      "offset": 894.639,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "just want to touch on the cost and uh",
      "offset": 897.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "some of the pricing models. The typical",
      "offset": 900.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "cost ranges between three and 20 cents",
      "offset": 902.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "per minute. And the cost here will",
      "offset": 904.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "depend on the uh the complexity of your",
      "offset": 906.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "queries uh but also on the providers",
      "offset": 908.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that that you choose. And in terms of",
      "offset": 910.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the pricing models, so the the main two",
      "offset": 912.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "dominant on the markets are usage based",
      "offset": 914.399,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "pricing and outcome based pricing.",
      "offset": 916.079,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "Probably usage is still the most",
      "offset": 917.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "dominant right now. Uh so user based",
      "offset": 918.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "pricing very simple. It's like per",
      "offset": 921.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "minute or per call very predictable but",
      "offset": 922.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "it doesn't necessarily capture the",
      "offset": 925.519,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "quality of the agent. Um so uh the",
      "offset": 927.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "incentives are not very well aligned",
      "offset": 931.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "between the providers and the customers.",
      "offset": 932.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "This changes with outcome based pricing",
      "offset": 934.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "because you only charge if you actually",
      "offset": 936.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "resolve something for the customer. So",
      "offset": 937.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it has a lot of benefits but also it",
      "offset": 939.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "also reduces risk because for a very",
      "offset": 942,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "long call and the call that you",
      "offset": 944.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "unresolved the provider actually needs",
      "offset": 945.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "to take the cost. Um so uh so yeah so",
      "offset": 947.68,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "there is risk there but over time I do",
      "offset": 951.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "expect the market will converge toward",
      "offset": 953.519,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "outcome based pricing because those",
      "offset": 954.959,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "incentives are way better aligned.",
      "offset": 956.32,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "Cool. Um and a few final thoughts. Um so",
      "offset": 961.12,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "to recap uh we built a uh voice AI agent",
      "offset": 965.12,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "and shipped it in about 100 days. We got",
      "offset": 970.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "several enterprise customers to use it",
      "offset": 972.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "on on their main phone lines. And when I",
      "offset": 974.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "think about like some of the main",
      "offset": 976.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "takeaways from this experience um is",
      "offset": 977.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "actually like getting to the right",
      "offset": 980.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "performance and those like latency and",
      "offset": 983.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the and the uh resolutions outcomes",
      "offset": 984.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "obviously super important but it also is",
      "offset": 987.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "not just a model problem. It's also a",
      "offset": 990.48,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "product problem. So it's about in the",
      "offset": 991.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right use case, designing for the",
      "offset": 993.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "realities of those phone conversations,",
      "offset": 995.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "building the tools, both your internal",
      "offset": 997.44,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and external, uh, integrating with the",
      "offset": 999.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "supporting workflows and actually",
      "offset": 1001.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "building trust with them because they",
      "offset": 1003.279,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "ultimately going to be decision makers",
      "offset": 1005.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "whether they want to release it. Um, and",
      "offset": 1006.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "yeah, and it's about making it feel",
      "offset": 1008.639,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "effortless even if there's a lot of",
      "offset": 1010.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "complexity behind the scenes. And that's",
      "offset": 1011.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "everything for me. Uh, thank you very",
      "offset": 1014.639,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "much. And yeah, if you're building in",
      "offset": 1016.56,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "this space, would love to chat with you.",
      "offset": 1017.839,
      "duration": 4.36
    }
  ],
  "cleanText": "[Music]\nToday I'm going to be talking about Fin Voice. And Fin Voice is a voice agent for phone support. Um, and we design it to be a frontline teammate for phone calls. So it picks up the phone, answers customers questions, and then escalates to a human agent when needed. And we build this experience in about 100 days. So today in this talk, I'll share what it took to get there. And I'll also talk about why I believe voice is the next big frontier in AI for customer service.\n\nSo first, a little bit of context of my company, Intercom. Um, so we're a customer service platform, um, and also an AI agent company. And you might be familiar with us because of our messenger product. Uh, seen it, you might have seen it in the mobile app or on the website. Uh, but yeah, it's been our foundation for years. Uh, but we evolved over time. We became a complete customer service platform a few years ago. Added robust tooling for other channels like email, WhatsApp, and phone.\n\nAnd then two years ago, right after the launch of GPD4, we launched Fin, uh, which is an AI, uh, agent, uh, via text on chat. And Fin's growth has been incredible. Uh, we have over 5,000 customers, and also in terms of performance, it's reaching an average resolution rate of 56%, and for some customers, it can be 70-80%. Um, and this is defined as a percent of interactions handled by Fin that are resolved without human intervention.\n\nAnd Fin is also a full system for continuous optimization. So it's not just that the agent, but also tooling for analyzing conversations, training the behavior of the agent, and also testing and deploying changes.\n\nUh, but yeah, up until now, we didn't have the voice channel, and that's what we're changing with Fin Voice. It's the same system, but now it can answer phone calls.\n\nAnd a few thoughts on, uh, why voice? Why are we investing in this channel? Um, so for all of our users, voice is simply the preferred way to get help. Uh, so when an issue is urgent or sensitive, uh, they don't necessarily want to type; they just want to talk. And if you look at the, uh, some of the top-level data, over 80% of support teams, uh, still use, uh, phone support. And if you think about all the conversations globally or customer service interactions, over one-third of them are, uh, happening over the phone as well. So it's not a legacy channel that's going away; it's still widely used, and it's also quite costly. Uh, if you think about the average cost of handling a phone call in the US with human support, it's about between seven and 12 dollars. And with voice AI agents, it can be at least five times cheaper.\n\nAnd a few more benefits of voice AI and customer service. Uh, so first, availability, 24/7 support. Uh, you can call your bank on the weekend. Uh, no wait time, so instantly available. There's no need to be on hold in a queue. Uh, no IVR menus, no need to press one to go to support, press two go to payments, uh, because all is everything is happening via natural speech, and also multilingual, so, uh, AI agents can support 30, 40 plus languages, obviously better for the users. And on the business side, major cost savings and also scalability as the business grows or when you need to handle peak times, AI agents were much better for that.\n\nUm, so how we built Fin Voice, uh, over the next few minutes, um, I want to cover, uh, seven main areas that had the biggest impact of how Fin came about. I'll try to be more practical, focusing on some of the product decisions we made and the challenges we faced. Um, and, and yeah, the first one is the use case. So the starting point for voice, uh, then also the scope of our MVP, uh, the tech stack behind it, how we approached the conversation design, how we integrated it with the support teams, and also, uh, how we thought about evaluation and pricing.\n\nSo starting with the use case, um, if you look at a lot of the voice AI startups in the space, they typically start with a narrow problem space. I mean, like scheduling a dentist appointment or, uh, booking a table at a restaurant. And we looked at some of those options, but eventually decided to go for a more flexible, uh, knowledge-based agent. So an agent that can answer help article questions, like what are your pricing plans or, uh, what's your returns policy? And why did we decide to go this way? So first, uh, we had strong evidence from chat. Fin over chat has been handling those kind of conversations for years, and our customers have constantly told us that they're seeing the same type of issues over the phone as they see on chat. And we also validated this through extra analysis of call transcripts, and this confirmed that a very large percentage of all the queries could be solved with the knowledge base, with the help articles content, rather than say with the API integrations.\n\nAnd we're also thinking about the initial wedge use case, so like the what's the lowest possible risk way for companies to integrate voice agents? And we looked at the in-office hours and outside of office hours use cases. So we pitched out of office hours as the initial wedge because essentially it allows the team to not affect the main workflows and try out this technology, build up more confidence over time, and later deploy it for their in the main office hours. But in out of office hours, it just replaces their voicemail experience.\n\nAnd there's also a few other use cases. We looked at, uh, authentication, so verifying user identity on another channel. Uh, info gathering, so the agent getting stuff like, uh, order ID, account ID, and also, uh, smart routing to the right team. So these use cases are, uh, still very high leverage because they can save a lot of time for the support team agents. Um, uh, but they're not necessarily solving the issue end to end. So we're still focusing on those, but they were in, say, the primary use case for this initial version of the product.\n\nSo now moving on to, uh, what we shipped first. Um,\n\nuh, when we, uh, when we started this, like the biggest challenge there was to ship something meaningful as soon as possible. We had access to a lot of customers, uh, because we already had thousands of customers using our native, uh, phone support product. So it's mostly about how can we test as quickly as possible. So we focused on the three main experiences: uh, testing, deploying, and monitoring the agent behavior. So first, test, uh, this is what we call the Fin Voice playground. And here was like a lightweight test environment for the customer service managers to go in and simulate a few sessions, uh, ask the questions based on the knowledge base, get those answers, and get an idea how this product actually works. And it was also, we shipped this probably within the first four weeks of the project. Uh, so it was like the fastest possible way to get some feedback from customer service managers. This was how it's actually performing. So we can do optimization not just based on our internal views, but also based on customer feedback.\n\nUm, then the deploy experience. This was to allow, um, uh, customer service managers to actually deploy it on their phone lines and included some basic configuration in terms of agent behavior and also how it should interact with the customer service team's workflows.\n\nAnd lastly, observability, uh, or monitoring, uh, really want to provide some visibility into what's actually happening on those calls with an AI agent. Uh, so we had those experiences that did show, uh, the transcripts, recording, and also the transcript summaries and called outcomes to customer service agents.\n\nCool. And now moving to the tech stack. I'm not going to go through the technical detail of everything. I'm sure that there's going to be a few more talks on this that you might attend as well. Uh, but I'll mention some core components and Fin. Uh, so there's the main, uh, chained loop for, uh, for the voice agent: STT, LM, TTS, uh, so speech to text, converting, uh, speech into text, LM for, uh, actually dating the response, and text to speech for converting text back into audio. Uh, but there's also another approach with the voice-to-voice models where, uh, everything is processed directly in audio while skipping the text layer entirely. So, uh, and the voice-to-voice approach has the benefit of, uh, potentially faster or natural-sounding speech, but also gives you less control over the output. Uh, so in our approach, we did start with a real-time API by itself from the get-go, and it allowed us to test very, very quickly. Uh, but eventually, we did evolve our stack, but we're still using real-time API as part of the core architecture.\n\nAnd there are two other components I want to mention: RAG and telephony. So RAG obviously is super critical for a lot of agent experiences. Uh, but obviously, it is important for the, uh, agent answering questions based on the knowledge base. Uh, and then telephony. Um, uh, so actually being able to put the agent on the phone lines, and we had a bit of a head start because our agent on chat already had the RAG set up, and we already had a native phone support product. So we got some of those things for free.\n\nNow, once, uh, uh, we have the technical foundation in place, uh, is a question about how do we actually design the conversations for voice. Um, so, um, Intercom has a background in chat, but we knew from the get-go that the approach for voice will have to be a bit different, um, and that voice is not necessarily just chat with sound. So there are like three key differences I want to mention. There's obviously many, many more, but there's a few that I thought it's worth mentioning. So latency, um,\n\non chat, it's actually, I think, okay to wait for a few seconds for a response, at least from the user perspective, that there's a lot of tolerance, but obviously, it doesn't really work on voice. If the agent goes silent for a second or two, or maybe longer, uh, the user might assume that something has gone wrong. So in terms of our approach for the simple queries, uh, we got it to about one second, so we didn't need to do anything extra, uh, but for more complex queries, when they're running a bit longer, three, four seconds, that we added injected filler words. I mean, like, let me look into this for you, let me look it up, uh, to maintain the conversation flow while we generate the answer in the background. Then the second one is answer length. Uh, so again, on chat, it's actually probably desirable in those agents, customer service agents, to provide a bit of a longer response to, uh, provide as much context as possible to the user, and it's easy to skim through the answer to find the right information. But again, it wouldn't work for on voice. You don't want to wait there for a minute or two listening to the agent. So for more complex responses or for responses with multiple steps, we're breaking down the answers into multiple chunks and deliver chunk by chunk, and after each, we ask the user to confirm whether they would like to listen to the next step. And this works really well for something like troubleshooting when you have a few steps, uh, to follow.\n\nAnd lastly, the user mindset. So, well, something interesting in during our early testing and real phone calls is, uh, some customers would interact with Fin Voice like with an old-school IVR. So, we just using single words like support, uh, password reset, yes, no. But then throughout the conversation, I've listened to a lot of those calls, and it's like they change their behavior during the call and actually start using full sentences while they hear the agent using full sentences. Um, and one of my colleagues like summed it up nicely that crazy how the human speaks more like a bot and the bot speaks more like a human. Uh, and I think this will change over time. Uh, to some extent, train over chats, and it's about, uh, obviously voice getting better and people more being more common as a technology, people going to get used to it, but for now, for now, it's on us to make those, uh, conversations sound as natural as possible, so we help with this transition.\n\nNow thinking about how Fin integrates into support workflows, uh, so this was super important and definitely surprising for me, uh, when, uh, when we got to this point is that the majority of the feedback wasn't about the voice, uh, about the model or about the latency, it was actually about how does it work with the support team workflows. And don't get me wrong, I do think that all those core model experiences are super important, but this actually became a bigger blocker for all those teams. Uh, so we put a lot of focus that the integrations points are as smooth as possible. So we did a bunch of other things, but I to mention two here: one is the escalation paths, so getting the calls, um, uh, getting configurations for how the calls get escalated to the human support team, and also the context handoff. So after every AI agent call, we generate a transcript summary that gives a bit more context to the human agent that gets the call to what happened on the call. And yeah, these are not like super flashy features, but they were absolutely essential to get this from this demo stage to the deployment stage for larger customers.\n\nAnd then how do we know it's working? Uh, so, uh, there's a few topics I wanted to touch on. So, um, one is the manual and automated evals. Uh, so we had a test, a set of, uh, test conversations that will be running through on every major code, uh, code change. Initially, it was mostly manual, just in a spreadsheet, but over time, we added some automation.\n\nNumber two is internal tooling, and this was super critical, um, h for troubleshooting. So essentially, we built some internal, the streamed web apps to review the logs, the transcripts, the recordings. So any time one of our customers, there's an issue, we can review in detail what happened in the conversation, actually troubleshoot it with the logs. Um, number three, resolution rate. So this is our northstar metric, which actually tells us whether we're delivering value for our customers. So we define it as, um, uh, essentially either the user confirming on the call that the issue was resolved or the user disconnects after hearing at least one answer, um, and then doesn't call back within 24 hours. Um, and yeah, this is the main, the main metric, metric, metric we track. Obviously, there's more in customer service, but this is kind of the main success metrics that we have.\n\nAnd lastly, LM as a judge. This is more experimental, but we're using another to analyze the call transcripts to help us identify issues or opportunities for improvement.\n\nAnd lastly, uh, how to price it. Uh, so just want to touch on the cost and, uh, some of the pricing models. The typical cost ranges between three and 20 cents per minute. And the cost here will depend on the, uh, the complexity of your queries, uh, but also on the providers that you choose. And in terms of the pricing models, so the main two dominant on the markets are usage-based pricing and outcome-based pricing. Probably usage is still the most dominant right now. Uh, so user-based pricing, very simple. It's like per minute or per call, very predictable, but it doesn't necessarily capture the quality of the agent. Um, so, uh, the incentives are not very well aligned between the providers and the customers. This changes with outcome-based pricing because you only charge if you actually resolve something for the customer. So\n\n\nIt has a lot of benefits, but it also reduces risk because for a very long call and the call that you unresolved, the provider actually needs to take the cost.\nUm, so, uh, so yeah, so there is risk there, but over time, I do expect the market will converge toward outcome-based pricing because those incentives are way better aligned.\nCool.\nUm, and a few final thoughts.\nUm, so to recap, uh, we built a voice AI agent and shipped it in about 100 days.\nWe got several Enterprise customers to use it on their main phone lines.\nAnd when I think about some of the main takeaways from this experience, um, is actually like getting to the right performance and those like latency and the and the uh resolutions outcomes, obviously super important, but it also is not just a model problem.\nIt's also a product problem.\nSo it's about in the right use case, designing for the realities of those phone conversations, building the tools, both your internal and external, uh, integrating with the supporting workflows and actually building trust with them because they're ultimately going to be decision-makers whether they want to release it.\nUm, and yeah, and it's about making it feel effortless even if there's a lot of complexity behind the scenes.\nAnd that's everything for me.\nUh, thank you very much.\nAnd yeah, if you're building in this space, would love to chat with you.\n",
  "dumpedAt": "2025-07-21T18:43:25.768Z"
}