{
  "episodeId": "2vlCqD6igVA",
  "channelSlug": "@aidotengineer",
  "title": "Recsys Keynote: Improving Recommendation Systems & Search in the Age of LLMs - Eugene Yan, Amazon",
  "publishedAt": "2025-07-16T15:00:06.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 0.33,
      "duration": 7.479
    },
    {
      "lang": "en",
      "text": "Hi everyone. Um, thank you for joining",
      "offset": 14.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "us in today's Rexis the inaugural Rexus",
      "offset": 16.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "track at the AI engineer world's fair.",
      "offset": 19.039,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "So today what I want to share about is",
      "offset": 21.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "what the future might look like when we",
      "offset": 24.16,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "use when we try to merge recommendation",
      "offset": 26.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "recommendation systems and language",
      "offset": 29.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "models. So my wife looked at my slides",
      "offset": 30.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and she's like they're so plain. So",
      "offset": 32.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "therefore I'll be giving a talk together",
      "offset": 35.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "with Latte and Mochi. You might have",
      "offset": 36.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "seen Mochi wandering the halls around",
      "offset": 38.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "somewhere but there'll be a lot of",
      "offset": 39.84,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "doggos throughout the slides. I hope you",
      "offset": 41.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "enjoy. First language modeling",
      "offset": 42.559,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "techniques are not new in recommendation",
      "offset": 45.6,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "systems. I mean it started with what in",
      "offset": 48.719,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "2013 we started learning item embeddings",
      "offset": 50.879,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "across um AC from co- occurrences in",
      "offset": 53.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "user interaction sequences and then",
      "offset": 56.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "after that we started using GRU for",
      "offset": 58.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "right I don't know who here remembers",
      "offset": 60.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "recurrent neuronet networks gated",
      "offset": 61.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "recurrent units yeah so those were very",
      "offset": 63.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "short-term and we predict the next item",
      "offset": 65.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "from a short set of sequences then of",
      "offset": 68.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "course um transformers and attention",
      "offset": 70.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "came about and we we we became better on",
      "offset": 72.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "attention on long range dependencies so",
      "offset": 75.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that's where we started that hey you",
      "offset": 78,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "know can we just process on everything",
      "offset": 79.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "in a user sequence hundreds 2,000 item",
      "offset": 81.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "ids long and try to learn from that and",
      "offset": 84.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of course now today in this track I",
      "offset": 86.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "wanted to share with you about three",
      "offset": 89.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "ideas that I think are worth thinking",
      "offset": 90.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "about semantic ids data augmentation and",
      "offset": 92.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "unified models so the first challenge we",
      "offset": 95.04,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "have is hashbased item ids who here",
      "offset": 98.4,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "works on recommendation systems so you",
      "offset": 102,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you you probably know that hashbased",
      "offset": 105.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "item ids actually don't encode put the",
      "offset": 106.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "content of the item itself. And then the",
      "offset": 108.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "problem is that every time you have a",
      "offset": 110.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "new item, you suffer from the cold start",
      "offset": 112.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "problem, which is that all you have to",
      "offset": 114.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "relearn about this item all over again.",
      "offset": 116.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "And and therefore there's also sparsity,",
      "offset": 119.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "right? Whereby you have a long set of",
      "offset": 121.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "tail items that have maybe one or two",
      "offset": 123.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "interactions or even up to 10, but it's",
      "offset": 124.799,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "just not enough to learn. So",
      "offset": 126.32,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "recommendation systems have this issue",
      "offset": 127.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of being very popularity bias and they",
      "offset": 129.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just struggle with coign and sparity. So",
      "offset": 131.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "the solution is semantic ids that may",
      "offset": 134.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "even involve multimodal content. So",
      "offset": 136.959,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "here's an example of trainable",
      "offset": 139.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "multimodal semantic IDs from quaou. So",
      "offset": 141.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "quaisho is kind of like Tik Tok or xongu",
      "offset": 144.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is a short video platform in China. I",
      "offset": 147.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "think it's the number two short video",
      "offset": 149.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "platform. You might have used their text",
      "offset": 150.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "to video model cling which they released",
      "offset": 152.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "sometime last year. So the problem they",
      "offset": 154.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "had, you know, being a short video",
      "offset": 156.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "platform, users upload hundreds of",
      "offset": 158.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "millions of short videos every day and",
      "offset": 160.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it's really hard to learn from this",
      "offset": 162.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "short video. So how can we combine",
      "offset": 164.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "static content embeddings with dynamic",
      "offset": 166.239,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "user behavior?",
      "offset": 169.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Here's how they did it with trainable",
      "offset": 171.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "multimodel semantic IDs. So I'm going to",
      "offset": 172.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "go through each step here. So this is",
      "offset": 175.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the quadial model. It's a standard two",
      "offset": 178.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "tower network. Um on the left this is",
      "offset": 180.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the embedding layer for the user which",
      "offset": 183.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "is a standard sequence uh sequence of",
      "offset": 186.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "ids and the user ID and on the right is",
      "offset": 188.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the embedding layer for the item ids. So",
      "offset": 191.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "these are fairly standard. What what's",
      "offset": 193.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "new here is that they now take in",
      "offset": 195.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "content input. So all of these slides",
      "offset": 198.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "will be available online. Um don't don't",
      "offset": 200.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "worry about it. I'll make it available",
      "offset": 202.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "right media after this. Um and to encode",
      "offset": 204.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "visual they use restnet. To encode video",
      "offset": 207.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "descriptions they use bird and to encode",
      "offset": 210.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "audio they use vGish.",
      "offset": 212.4,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Now the thing about the trick is this",
      "offset": 215.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "when you have this encoder models it's",
      "offset": 216.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "very hard to back propagate and try to",
      "offset": 219.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "update these encoder model embeddings.",
      "offset": 221.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "So what did they do? Well firstly they",
      "offset": 223.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "took all these content embeddings and",
      "offset": 225.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "then they just concatenated them",
      "offset": 227.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "together. I know it sounds crazy right",
      "offset": 229.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but I just concat concat them together.",
      "offset": 231.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Then they learn cluster ids. So I think",
      "offset": 233.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "they shared in the paper they had like a",
      "offset": 236.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "100 million short videos and they",
      "offset": 238.4,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "learned just be via CIN's clustering a",
      "offset": 240.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "thousand cluster ids. So that's what you",
      "offset": 242.959,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "see over there in the model encoder",
      "offset": 245.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "which is in the boxes at at the bottom",
      "offset": 247.439,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "which is the cluster ids. So be above",
      "offset": 249.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the cluster ids you have the",
      "offset": 251.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "non-trainable embeddings below that you",
      "offset": 252.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "have the trainable cluster ids which are",
      "offset": 255.599,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "then all mapped to their own embedding",
      "offset": 257.68,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "table. So the trick here is this. The",
      "offset": 259.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "motor encoder as you train a model, the",
      "offset": 261.759,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "motor encoder learns to map the content",
      "offset": 264.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "space via the cluster ids which are",
      "offset": 266.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "mapped to the embedding table to the",
      "offset": 269.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "behavioral space.",
      "offset": 270.72,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "So the output is this. Um these semantic",
      "offset": 273.36,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "ids not only outperform regular",
      "offset": 276.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "hashbased ids on clicks and likes,",
      "offset": 278.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right? Like that's pretty standard. But",
      "offset": 280.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "what they were able to do was they were",
      "offset": 282.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "able to increase co-start coverage which",
      "offset": 284.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is the of a 100 videos that you share",
      "offset": 286.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "how many of them are new they were able",
      "offset": 289.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to increase it by 3.6%. And also",
      "offset": 291.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "increase co-start velocity which is okay",
      "offset": 293.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "how many new videos were able to hit",
      "offset": 296.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "some threshold of uh views and this they",
      "offset": 298,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "did not they did not share what the",
      "offset": 301.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "threshold was but being able to increase",
      "offset": 302.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "co-start and co-star velocity by these",
      "offset": 304.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "numbers are pretty outstanding.",
      "offset": 306,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "So the long story short, the benefits of",
      "offset": 309.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "semantic ids, you can address coart with",
      "offset": 311.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the semantic ID itself and now your",
      "offset": 313.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "recommendations understand content. So",
      "offset": 315.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "later in the talk, we're going to see",
      "offset": 317.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "some amazing uh sharing from Pinterest",
      "offset": 319.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and YouTube. And in the YouTube one, you",
      "offset": 323.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "see how they actually blend language",
      "offset": 324.8,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "models with semantic ids whereby it can",
      "offset": 327.039,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "actually explain why you might like the",
      "offset": 330.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "semantic ID because it understands the",
      "offset": 332.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "semantic ID and is able to give human",
      "offset": 334.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "readable explanations and vice versa.",
      "offset": 336.4,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Now the next question and I'm sure all",
      "offset": 339.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of this everyone here has this",
      "offset": 341.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "challenge. The lifeblood of machine",
      "offset": 343.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "learning is data. good quality data at",
      "offset": 346.08,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "scale and this is very essential for",
      "offset": 349.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "search and of course recommendation",
      "offset": 352.639,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "systems but search is actually far more",
      "offset": 354.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "important. You you need a lot of",
      "offset": 355.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "metadata. You need a lot of uh query",
      "offset": 356.96,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "expansion, synonyms, uh you need",
      "offset": 359.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "spellchecking, you need um uh all sorts",
      "offset": 361.759,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "of metadata to attach to a search index.",
      "offset": 365.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "But this is very costly and high effort",
      "offset": 367.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "to get. In the past, we used to do with",
      "offset": 368.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "human annotations or maybe you can try",
      "offset": 370.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "to do it automatically. But LMS have",
      "offset": 372,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "been outstanding at this. And I'm sure",
      "offset": 374.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "everyone here is sort of doing this to",
      "offset": 376.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "some extent using LM for synthetic data",
      "offset": 378.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and labels. But I want to share with you",
      "offset": 380.72,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "two examples uh from Spotify and Indeed.",
      "offset": 382.4,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "Now the Indeed paper it's quite out uh I",
      "offset": 388.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "really like it a lot. So the problem",
      "offset": 390.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that they were trying to face is that",
      "offset": 392.639,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "they were sending job recommendations to",
      "offset": 394.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "users via email. But some of these job",
      "offset": 396.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "recommendations were bad. They they were",
      "offset": 399.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "just not a good fit for the user. Right?",
      "offset": 401.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "So they had poor user experience and",
      "offset": 403.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "then users lost trust in the job",
      "offset": 405.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "recommendations. Imagine and how how",
      "offset": 407.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "they would indicate that they lost trust",
      "offset": 408.88,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "was that these job recommendations are",
      "offset": 410.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "not good a good fit for me. I'm just",
      "offset": 411.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "going to unsubscribe. Now the moment a",
      "offset": 413.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "user unsubscribes from your feed or for",
      "offset": 415.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "your newsletter is very very very hard",
      "offset": 417.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to get them back. Almost impossible. So",
      "offset": 419.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "while they had explicit negative",
      "offset": 421.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "feedback, thumbs up and thumbs down,",
      "offset": 423.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this was very sparse. How often would",
      "offset": 425.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "you actually give thumbs down feedback?",
      "offset": 426.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "Very sparse. And implicit feedback is",
      "offset": 428.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "often imprecise. What do I mean? If you",
      "offset": 430.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "if you get some recommendations but you",
      "offset": 432.479,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "actually don't act on it, is it because",
      "offset": 434.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "you didn't like it or is it because it's",
      "offset": 436.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "not the right time or maybe your s your",
      "offset": 437.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "your wife works there and you don't want",
      "offset": 440.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "to work in the same company as your",
      "offset": 442.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "wife. So the solution they had was a",
      "offset": 443.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "lightweight classifier to filter bad",
      "offset": 446.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "racks. And I'll tell you why I really",
      "offset": 448.24,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "like this paper from indeed in the sense",
      "offset": 450.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that they didn't just share their",
      "offset": 451.759,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "successes but they shared the entire",
      "offset": 453.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "process and how they get how they got",
      "offset": 455.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "there and it was fraught with",
      "offset": 456.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "challenges. Well, of course, the first",
      "offset": 458.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "thing that makes me really like it a lot",
      "offset": 460.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "was that they started with emails. So,",
      "offset": 462.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "they had their experts label um job",
      "offset": 464.479,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "recommendations and uh user pairs and",
      "offset": 468.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "from the user you have their resume",
      "offset": 470.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "data, you have their activity data and",
      "offset": 472.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "they try to see hey you know is this",
      "offset": 474.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "recommendation a good fit.",
      "offset": 475.599,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Then they prompted open ALMs uh Mistral",
      "offset": 478.319,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and Lama 2. Unfortunately, the",
      "offset": 481.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "performance is very poor. these models",
      "offset": 483.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "couldn't really pay attention to what",
      "offset": 485.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "was in the resume and what was in the",
      "offset": 487.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "job description even though they had",
      "offset": 489.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "sufficient context length and and the",
      "offset": 491.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "output was just very generic.",
      "offset": 492.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So to get it to work they prompted GB4",
      "offset": 495.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and GB4 worked really well um",
      "offset": 498.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "specifically that GB4 had like 90%",
      "offset": 500.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "precision and recall. However, it was",
      "offset": 502.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "very costly. Um they didn't share actual",
      "offset": 505.199,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "cost but it's too slow. It's 32 seconds.",
      "offset": 507.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Okay, if GBD4 is too slow, what can we",
      "offset": 509.759,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "do? Let's try GBD 3.5. Unfortunately, GP",
      "offset": 512.32,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "GBD3D 3.5 had very poor precision. What",
      "offset": 516.24,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "does this mean? In the sense that of the",
      "offset": 518.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "recommendations that he said were bad,",
      "offset": 523.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "only 63% of them were actually bad. What",
      "offset": 525.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this means is that they were throwing",
      "offset": 528.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "out 37% of recommendations, which is",
      "offset": 529.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "one/ird. And for a company that tries on",
      "offset": 532,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "recommendations and people are rec",
      "offset": 534.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "recruiting through your recommendations,",
      "offset": 536.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "throwing out oneird of them that are",
      "offset": 538.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually good is is is quite a is quite",
      "offset": 540.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a guardrail for them. This was their key",
      "offset": 542.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "metric here and also GB. So what they",
      "offset": 544.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "did then is they fine-tuned GBD 3.5. So",
      "offset": 546.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you can see the the entire journey right",
      "offset": 550,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "open models GBD4 GBD3 now fine-tuning",
      "offset": 551.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "GBD3.5. um GB25 got the precision they",
      "offset": 553.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "wanted 0.3 precision and you know it's",
      "offset": 557.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "one quarter of GBD4's cost and latency",
      "offset": 559.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "right but unfortunately it was too still",
      "offset": 561.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "too slow it was about 6.7 seconds and",
      "offset": 563.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this would not work in an online",
      "offset": 565.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "filtering system so therefore what they",
      "offset": 567.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "did was they distilled a lightweight",
      "offset": 569.519,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "classifier on the fine tetune GBD 2.5",
      "offset": 571.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "labels and this lightweight classifier",
      "offset": 574.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "was able to achieve very high",
      "offset": 576.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "performance uh specifically 0.86 86 AU",
      "offset": 578.399,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "ROC. I mean the numbers may not make",
      "offset": 583.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "sense to you but suffice to say that in",
      "offset": 584.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "an industrial setting this is pretty",
      "offset": 586.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "good. And of course they didn't mention",
      "offset": 587.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the latency but it was good enough for",
      "offset": 589.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "real-time filtering. I think less than",
      "offset": 590.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "200 millconds or something.",
      "offset": 592.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So the outcome of this was that they",
      "offset": 595.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "were able to reduce bad recommendations.",
      "offset": 597.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "They they were able to cut out bad",
      "offset": 599.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "recommendations by about 20%. So",
      "offset": 600.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "initially they had hypothesized that by",
      "offset": 603.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "cutting down recommendations even though",
      "offset": 605.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "they were bad you will get fewer",
      "offset": 607.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "subscriptions. It's just like sending",
      "offset": 609.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "out links, right? You might have links",
      "offset": 610.72,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "that are clickbait, even though they are",
      "offset": 612.08,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "bad, people just click on it. And he",
      "offset": 613.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "thought that even if if we cut down",
      "offset": 614.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "recommendations, even if they were bad,",
      "offset": 616.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "we will get lower application rate. But",
      "offset": 617.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this was not the case. In fact, because",
      "offset": 619.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the recommendations were now better,",
      "offset": 622.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "application rate actually went up by 4%.",
      "offset": 625.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And unsubscribe rate went down by 5%.",
      "offset": 627.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "That that's that's quite a lot. So",
      "offset": 629.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "essentially, what this means is that in",
      "offset": 630.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "recommendations, quantity is not",
      "offset": 632.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "everything. Quality makes a big",
      "offset": 634.399,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "difference. And the quality here moved",
      "offset": 635.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the needle quite a bit by 5%.",
      "offset": 637.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "The next example I want to share with",
      "offset": 640.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "you is Spotify. So who here knows that",
      "offset": 641.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Spotify has podcast and audio books?",
      "offset": 644.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Oh, okay. I guess you you guys are not",
      "offset": 647.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the target target audience in in this",
      "offset": 649.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "use case. So Spotify is really known for",
      "offset": 651.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "song and artists and a lot of their",
      "offset": 653.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "users just search for songs and artists",
      "offset": 655.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and they're very good at that. But when",
      "offset": 657.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "they started introducing podcasts and",
      "offset": 659.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "audio books, how would you help your",
      "offset": 661.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "users know that you know these new items",
      "offset": 663.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "are available? And of course there's a",
      "offset": 665.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "huge ass co-star problem. Now it's not",
      "offset": 667.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "only co-star on it term is now co-star",
      "offset": 669.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "on category. How do you start growing a",
      "offset": 672,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "new category within your service?",
      "offset": 675.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "And of course exploratory search was",
      "offset": 678.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "essential to the business right for",
      "offset": 680.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "going for expanding beyond music.",
      "offset": 681.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Spotify doesn't want to do just do music",
      "offset": 683.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "songs. They just now now they doing",
      "offset": 685.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "audio. So the solution to that is a",
      "offset": 686.959,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "query recommendation system.",
      "offset": 690.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "So how did they recommend how first how",
      "offset": 692.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "did they generate new queries? Well um",
      "offset": 695.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "they have a bunch of ideas which is you",
      "offset": 698.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "know extract it from catalog titles",
      "offset": 700.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "playlist titles you mine it from the",
      "offset": 702.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "search logs you just take the uh you",
      "offset": 703.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "just take the artist and then you just",
      "offset": 705.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "add cover to it and this is what they",
      "offset": 707.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "use from existing data. Now you might be",
      "offset": 709.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "wondering like where's the LM in this?",
      "offset": 712.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Well the LM is used to generate natural",
      "offset": 714.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "language queries. So this might not be",
      "offset": 717.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "sexy, but this works really well, right?",
      "offset": 719.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Take whatever you have with conventional",
      "offset": 722.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "techniques that work really well and use",
      "offset": 723.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the LM to augment it when you need it.",
      "offset": 725.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Don't use the LM for everything at the",
      "offset": 727.519,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "start.",
      "offset": 729.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "So now they have this exploratory",
      "offset": 730.959,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "queries.",
      "offset": 732.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "When you search for something, you still",
      "offset": 735.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "get the you still get the immediate",
      "offset": 736.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "results hit, right? So you take all of",
      "offset": 738.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "this, you add the immediate results, and",
      "offset": 740.959,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "then you rank these new queries. So this",
      "offset": 744.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is why when you do a search, this is the",
      "offset": 747.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "UX that you're probably going to get",
      "offset": 749.6,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "right now. I got this from a paper. It",
      "offset": 750.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "may have changed recently. So you still",
      "offset": 752.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "see the item queries at the bottom, but",
      "offset": 754,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "at the top with the query",
      "offset": 756.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "recommendations, this is how Spotify",
      "offset": 757.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "informs users without having a banner.",
      "offset": 759.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Now we have audio books, now we have",
      "offset": 762.56,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "podcast, right? You search for",
      "offset": 763.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "something, it actually informs you that",
      "offset": 765.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we have these new categories.",
      "offset": 766.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "The benefit here is plus 9% exploratory",
      "offset": 769.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "queries. is essentially onetenth of",
      "offset": 771.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "their users were now exploring their new",
      "offset": 774.639,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "products. So imagine that onetenth every",
      "offset": 778.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "day exploring their new products. How",
      "offset": 780.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "quickly would you be able to grow your",
      "offset": 782.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "your new product category, right? It's",
      "offset": 785.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "actually 1.1 to the^ of n. You will grow",
      "offset": 787.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "pretty fast.",
      "offset": 789.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "Long story short, I don't have to tell",
      "offset": 791.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you about the benefits of LM augmented",
      "offset": 792.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "synthetic data. Richer high quality data",
      "offset": 794.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "at scale on the tail queries, right?",
      "offset": 796.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "even on the tail queries and the tail",
      "offset": 800.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "items and it's far lower cost and effort",
      "offset": 801.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "than is even possible with human",
      "offset": 804.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "annotation. So later we also have a talk",
      "offset": 805.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "from Instacart who will tell us about",
      "offset": 808.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "how they use uh LMS to improve their",
      "offset": 810.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "search and recommend uh their search",
      "offset": 813.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "system.",
      "offset": 814.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Now the last thing I want to share is",
      "offset": 817.12,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "this challenge whereby right now",
      "offset": 819.68,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "in a regular company the system for ads",
      "offset": 824.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "for recommendations for search they're",
      "offset": 827.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "all separate systems and even for",
      "offset": 829.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "recommendations the the model for",
      "offset": 832.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "homepage recommendations the model for",
      "offset": 834.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "item recommendations the model for cut",
      "offset": 836,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "to cut recommendations the model for the",
      "offset": 838.48,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "thank you page recommendations they may",
      "offset": 839.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "all be different models right so you can",
      "offset": 841.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "imagine there you're going to have many",
      "offset": 843.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "many models But you going to have well",
      "offset": 845.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "leadership expects you to keep the same",
      "offset": 848.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "amount of headcount. So then how do you",
      "offset": 850.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "try to get around this right? You have",
      "offset": 852.16,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "duplicative engineering pipelines.",
      "offset": 853.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "There's a lot of maintenance costs and",
      "offset": 854.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "improving one model doesn't naturally",
      "offset": 856.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "transfer to the improvement in another",
      "offset": 859.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "model. So the solution for this is",
      "offset": 861.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "unified models, right? I mean it works",
      "offset": 864.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "for vision, it works for language. So",
      "offset": 866,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "why not recommendation systems? And",
      "offset": 868.079,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "we've been doing this for a while. This",
      "offset": 869.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is not new. And aside maybe the text is",
      "offset": 870.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "too small but this is a tweet from",
      "offset": 874.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "stripe whereby they built a",
      "offset": 876.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "transformerbased payments fraud model",
      "offset": 878.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "right even for payments the sequence of",
      "offset": 881.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "payments you can build a foundation",
      "offset": 883.6,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "model which is transformer based",
      "offset": 885.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so I want to share an example of the",
      "offset": 888.959,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "unified ranker for search and rexis and",
      "offset": 890.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Netflix right the problem I mentioned",
      "offset": 892.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "they have teams they are building",
      "offset": 894.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "bespoke models for search similar item",
      "offset": 896,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "similar video recommendations and",
      "offset": 898.639,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "pre-quyer recommendations like on the",
      "offset": 900.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "search page before you even enter a",
      "offset": 901.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "search query. High operational cost um",
      "offset": 903.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you know missed opportunities from",
      "offset": 905.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "learning throughout. So their solution",
      "offset": 906.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is a unified ranker and they call it a",
      "offset": 908.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "unified contextual ranker which is",
      "offset": 911.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "unicorn. So you can see over here uh at",
      "offset": 912.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the bottom there's the user foundation",
      "offset": 915.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "model and in it you put in the user",
      "offset": 917.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "watch history and then you also have the",
      "offset": 920.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "context and relevance model where where",
      "offset": 922,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you put in the context of the videos and",
      "offset": 924.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "what they've watched.",
      "offset": 925.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Now the thing about this unified model",
      "offset": 927.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is that it takes in unified input right?",
      "offset": 930.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "So now if you are able to find a data",
      "offset": 933.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "schema where all your use cases and all",
      "offset": 935.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "your features can use the same input you",
      "offset": 938.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "can adopt an approach like this which is",
      "offset": 940.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "multi similar to multitask learning. So",
      "offset": 942.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the the user the input will be the user",
      "offset": 945.279,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "ID the item ID you know the video or the",
      "offset": 947.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "drama or the series the search query if",
      "offset": 950.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a search query exists the country and",
      "offset": 952.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the task. So of course they have many",
      "offset": 955.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "different tasks in this example in the",
      "offset": 956.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "paper they have three different tasks uh",
      "offset": 958.56,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "search pre-quyer and more like this. Now",
      "offset": 960.56,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "what they did then was very smart",
      "offset": 963.759,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "imputation of missing items. So for",
      "offset": 965.759,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "example if you are doing an item to item",
      "offset": 968.56,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "recommendation you're just done watching",
      "offset": 972.16,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "this video you want to recommend the",
      "offset": 973.519,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "next video you would have no search",
      "offset": 974.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "query how would you imputee it well you",
      "offset": 975.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just simply use the title of the current",
      "offset": 977.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "item and try to find similar items.",
      "offset": 979.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "The outcome of this is that this unified",
      "offset": 983.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "model was able to match or exceed the",
      "offset": 985.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "metrics of their specialized models on",
      "offset": 987.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "multiple tasks.",
      "offset": 989.92,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Think about it. I mean it doesn't seem",
      "offset": 992.16,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "very impressive, right? It may not seem",
      "offset": 993.759,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "very impressive. Match or exceed. It",
      "offset": 995.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "might seem we did all this work just to",
      "offset": 997.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "match, but imagine unifying all of it",
      "offset": 998.639,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like removing the tech depth and",
      "offset": 1001.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "building a better foundation for your",
      "offset": 1002.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "future iterations. It's going to make",
      "offset": 1005.199,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you iterate faster.",
      "offset": 1006.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "The last example I want to share with",
      "offset": 1008.959,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "you is unified embeddings at Etsy. So",
      "offset": 1010.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you might think that embeddings are not",
      "offset": 1012.16,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "very sexy but this paper from Etsy is",
      "offset": 1013.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "really uh outstanding in what they share",
      "offset": 1015.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "in terms of model architecture as well",
      "offset": 1018.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "as their system. So the problem they had",
      "offset": 1019.44,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "was how can we help users get better",
      "offset": 1022.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "results from very specific queries or",
      "offset": 1025.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "very broad queries and if you know that",
      "offset": 1027.6,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Etsy inventory is constantly changing",
      "offset": 1029.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "they they don't have the the same",
      "offset": 1031.439,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "products all all throughout right it's",
      "offset": 1032.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "very home homegrown so now you might be",
      "offset": 1034.16,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "quering for something like mother's day",
      "offset": 1036.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "gift that would almost match very few",
      "offset": 1037.919,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "items I think very few items would have",
      "offset": 1041.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "mother's day gift in their description",
      "offset": 1043.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "or their title right and you know lexica",
      "offset": 1044.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "embedding the other problem is that",
      "offset": 1046.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "knowledge based embeddings like lexical",
      "offset": 1049.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "embedding retrieval don't account for",
      "offset": 1051.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "user preferences. So how do you try to",
      "offset": 1052.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "address this?",
      "offset": 1054.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "The problem the how do you address this",
      "offset": 1057.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "is with a unified embedding and",
      "offset": 1058.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "retrieval. So if you remember at the",
      "offset": 1059.84,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "start of my presentation I talked about",
      "offset": 1062.559,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "the qu show tower model right there's",
      "offset": 1066.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the user tower and then there's the item",
      "offset": 1068.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "tower. We will see the same pattern",
      "offset": 1070.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "again over here you see the product",
      "offset": 1072.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "tower right this is the product encoder.",
      "offset": 1074.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "So how they encode the product is that",
      "offset": 1076.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "they use T5 models for text embeddings,",
      "offset": 1078.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "right? Text item descriptions as well as",
      "offset": 1080.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a query product log for query",
      "offset": 1083.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "embeddings. What was the query that was",
      "offset": 1085.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "made and what was the product that was",
      "offset": 1087.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "eventually clicked or purchased?",
      "offset": 1088.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "And then over here on the on the left",
      "offset": 1091.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you see the query encoder which is the",
      "offset": 1092.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "search query encoder. And they both",
      "offset": 1094.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "share encoders for the tokens which is",
      "offset": 1096.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "actually a text tokens the product",
      "offset": 1100.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "category which is a token of a token of",
      "offset": 1102.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "itself and the user location. So what",
      "offset": 1104.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this means is that your now your",
      "offset": 1107.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "embedding is able to match user to the",
      "offset": 1108.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "location of the product itself.",
      "offset": 1110.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And then of course to personalize this",
      "offset": 1113.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "they encode the user preferences via the",
      "offset": 1115.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "query user scaler features at the",
      "offset": 1117.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "bottom. Essentially what were the",
      "offset": 1119.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "queries that the user searched for? what",
      "offset": 1120.799,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "what did they buy previously all their",
      "offset": 1122.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "preferences. Now this is they also",
      "offset": 1123.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "shared their system architecture and",
      "offset": 1126.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "over here this is the product encoder",
      "offset": 1128.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "from the previous slide and the query",
      "offset": 1130.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "encoder from the previous slide. But",
      "offset": 1132.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "what's very interesting here is that",
      "offset": 1133.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "they added a quality vector because they",
      "offset": 1135.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "wanted to ensure that whatever was",
      "offset": 1139.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "searched and retrieved was actually of",
      "offset": 1140.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "good quality in terms of ratings uh",
      "offset": 1142.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "freshness and conversion rate. And you",
      "offset": 1144.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "know what they did is they just simply",
      "offset": 1147.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "concatenated this quality vector to the",
      "offset": 1149.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "product embedding vector.",
      "offset": 1150.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "But when you do that for the query",
      "offset": 1153.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "vector, you have to you have to expand",
      "offset": 1155.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the product vector by the same dimension",
      "offset": 1156.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so that you can do a dot product or",
      "offset": 1158.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "cosine similarity. So essentially they",
      "offset": 1160.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "just slapped on a constant vector uh for",
      "offset": 1162.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the query embedding and it just works.",
      "offset": 1164.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "The result 2.6% increase in conversion",
      "offset": 1167.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "across entire site. That's quite crazy.",
      "offset": 1170.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Um and more than 5% increase in search",
      "offset": 1173.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "purchases. If you search for something",
      "offset": 1175.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the purchase rate increases by 5%. Um",
      "offset": 1177.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this is very very these are very very",
      "offset": 1179.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "very good results for e-commerce.",
      "offset": 1181.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Um so the benefits of uh unified models",
      "offset": 1184.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you simplify the system uh you whatever",
      "offset": 1187.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "you build to improve one side of the",
      "offset": 1190.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "tower or improve your model your unified",
      "offset": 1193.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "model also improves other use cases that",
      "offset": 1196.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "use these unified models. That said,",
      "offset": 1198.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "there may also be the alignment text.",
      "offset": 1200.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "You you you may find that when you try",
      "offset": 1202.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to build this, try to compress all 12",
      "offset": 1204,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "use cases into a single unified model,",
      "offset": 1206.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you may need to split it up into maybe",
      "offset": 1207.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "two or three separate unified models",
      "offset": 1209.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "because that's just the alignment text.",
      "offset": 1211.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "We're trying to get better at one task",
      "offset": 1212.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually makes the other task worse. We",
      "offset": 1214,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "have a talk from uh LinkedIn in the in",
      "offset": 1216.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "this afternoon's in this afternoon blog,",
      "offset": 1220.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the last talk of the blog, and then we",
      "offset": 1222.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "also have a talk from Netflix uh which",
      "offset": 1223.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "we'll be sharing about their unified",
      "offset": 1225.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "model at the start of the next blog. All",
      "offset": 1226.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "right, the three takeaways I have for",
      "offset": 1229.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "you, think about it, consider it.",
      "offset": 1230.799,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "Semantic ids, data augmentation, and",
      "offset": 1233.679,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "unified models. Um, and of course do",
      "offset": 1236.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "stay stay tuned for the rest of the",
      "offset": 1240.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "track uh for the rest of the talks in",
      "offset": 1242.48,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "this track. Okay, that's it. Thank you.",
      "offset": 1243.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1252.97,
      "duration": 2.24
    }
  ],
  "cleanText": "[Music]\nHi everyone.\nUm, thank you for joining us in today's RecSys, the inaugural RecSys track at the AI Engineer World's Fair.\nSo today what I want to share about is what the future might look like when we use, when we try to merge recommendation systems and language models.\nSo my wife looked at my slides and she's like, \"They're so plain.\"\nSo therefore I'll be giving a talk together with Latte and Mochi.\nYou might have seen Mochi wandering the halls around somewhere, but there'll be a lot of doggos throughout the slides.\nI hope you enjoy.\nFirst, language modeling techniques are not new in recommendation systems.\nI mean, it started with what, in 2013, we started learning item embeddings across, um, AC from co-occurrences in user interaction sequences, and then after that we started using GRUs for, right?\nI don't know who here remembers recurrent neural networks, gated recurrent units, yeah, so those were very short-term, and we predict the next item from a short set of sequences.\nThen, of course, um, Transformers and attention came about, and we, we, we became better on attention on long-range dependencies, so that's where we started that, \"Hey, you know, can we just process on everything in a user sequence, hundreds, 2,000 item IDs long, and try to learn from that?\"\nAnd of course, now today in this track, I wanted to share with you about three ideas that I think are worth thinking about: semantic IDs, data augmentation, and unified models.\nSo the first challenge we have is hash-based item IDs.\nWho here works on recommendation systems?\nSo you, you, you probably know that hash-based item IDs actually don't encode the content of the item itself.\nAnd then the problem is that every time you have a new item, you suffer from the cold start problem, which is that all you have to relearn about this item all over again.\nAnd, and therefore there's also sparsity, right?\nWhereby you have a long set of tail items that have maybe one or two interactions or even up to 10, but it's just not enough to learn.\nSo recommendation systems have this issue of being very popularity bias, and they just struggle with coign and sparsity.\nSo the solution is semantic IDs that may even involve multimodal content.\nSo here's an example of trainable multimodal semantic IDs from Quaishou.\nSo Quaishou is kind of like TikTok or Xongu is a short video platform in China.\nI think it's the number two short video platform.\nYou might have used their text-to-video model, Cling, which they released sometime last year.\nSo the problem they had, you know, being a short video platform, users upload hundreds of millions of short videos every day, and it's really hard to learn from this short video.\nSo how can we combine static content embeddings with dynamic user behavior?\nHere's how they did it with trainable multimodal semantic IDs.\nSo I'm going to go through each step here.\nSo this is the Quaishou model.\nIt's a standard two-tower network.\nUm, on the left, this is the embedding layer for the user, which is a standard sequence, uh, sequence of IDs and the user ID, and on the right is the embedding layer for the item IDs.\nSo these are fairly standard.\nWhat, what's new here is that they now take in content input.\nSo all of these slides will be available online.\nUm, don't, don't worry about it.\nI'll make it available right media after this.\nUm, and to encode visual, they use ResNet.\nTo encode video descriptions, they use BERT, and to encode audio, they use vGish.\nNow the thing about the trick is this: when you have this encoder models, it's very hard to backpropagate and try to update these encoder model embeddings.\nSo what did they do?\nWell, firstly, they took all these content embeddings and then they just concatenated them together.\nI know it sounds crazy, right?\nBut I just concat, concat them together.\nThen they learn cluster IDs.\nSo I think they shared in the paper they had like a 100 million short videos, and they learned just via CIN's clustering a thousand cluster IDs.\nSo that's what you see over there in the model encoder, which is in the boxes at, at the bottom, which is the cluster IDs.\nSo be above the cluster IDs, you have the non-trainable embeddings, below that you have the trainable cluster IDs, which are then all mapped to their own embedding table.\nSo the trick here is this: the motor encoder, as you train a model, the motor encoder learns to map the content space via the cluster IDs, which are mapped to the embedding table, to the behavioral space.\nSo the output is this.\nUm, these semantic IDs not only outperform regular hash-based IDs on clicks and likes, right?\nLike that's pretty standard.\nBut what they were able to do was they were able to increase cold-start coverage, which is the of a 100 videos that you share, how many of them are new, they were able to increase it by 3.6%.\nAnd also increase cold-start velocity, which is, okay, how many new videos were able to hit some threshold of, uh, views, and this they did not, they did not share what the threshold was, but being able to increase cold-start and cold-start velocity by these numbers are pretty outstanding.\nSo the long story short, the benefits of semantic IDs: you can address cold start with the semantic ID itself, and now your recommendations understand content.\nSo later in the talk, we're going to see some amazing, uh, sharing from Pinterest and YouTube.\nAnd in the YouTube one, you see how they actually blend language models with semantic IDs, whereby it can actually explain why you might like the semantic ID because it understands the semantic ID and is able to give human-readable explanations and vice versa.\nNow the next question, and I'm sure all of this, everyone here has this challenge: the lifeblood of machine learning is data, good quality data at scale, and this is very essential for Search and of course, recommendation systems, but Search is actually far more important.\nYou, you need a lot of metadata.\nYou need a lot of, uh, query expansion, synonyms, uh, you need spellchecking, you need, um, uh, all sorts of metadata to attach to a search index.\nBut this is very costly and high effort to get.\nIn the past, we used to do with human annotations or maybe you can try to do it automatically.\nBut LLMs have been outstanding at this.\nAnd I'm sure everyone here is sort of doing this to some extent using LLMs for synthetic data and labels.\nBut I want to share with you two examples, uh, from Spotify and Indeed.\nNow the Indeed paper, it's quite out, uh, I really like it a lot.\nSo the problem that they were trying to face is that they were sending job recommendations to users via email.\nBut some of these job recommendations were bad.\nThey, they were just not a good fit for the user, right?\nSo they had poor user experience, and then users lost trust in the job recommendations.\nImagine, and how, how they would indicate that they lost trust was that these job recommendations are not good, a good fit for me.\nI'm just going to unsubscribe.\nNow the moment a user unsubscribes from your feed or for your newsletter is very, very, very hard to get them back, almost impossible.\nSo while they had explicit negative feedback, thumbs up and thumbs down, this was very sparse.\nHow often would you actually give thumbs down feedback?\nVery sparse.\nAnd implicit feedback is often imprecise.\nWhat do I mean?\nIf you, if you get some recommendations, but you actually don't act on it, is it because you didn't like it, or is it because it's not the right time, or maybe your, your wife works there and you don't want to work in the same company as your wife?\nSo the solution they had was a lightweight classifier to filter bad racks.\nAnd I'll tell you why I really like this paper from Indeed in the sense that they didn't just share their successes, but they shared the entire process and how they get, how they got there, and it was fraught with challenges.\nWell, of course, the first thing that makes me really like it a lot was that they started with emails.\nSo, they had their experts label, um, job recommendations and, uh, user pairs, and from the user you have their resume data, you have their activity data, and they try to see, \"Hey, you know, is this recommendation a good fit?\"\nThen they prompted open LLMs, uh, Mistral and Lama 2.\nUnfortunately, the performance is very poor.\nThese models couldn't really pay attention to what was in the resume and what was in the job description, even though they had sufficient context length, and, and the output was just very generic.\nSo to get it to work, they prompted GPT4, and GPT4 worked really well, um, specifically that GPT4 had like 90% precision and recall.\nHowever, it was very costly.\nUm, they didn't share actual cost, but it's too slow.\nIt's 32 seconds.\nOkay, if GPT4 is too slow, what can we do?\nLet's try GPT 3.5.\nUnfortunately, GP, GPT 3.5 had very poor precision.\nWhat does this mean?\nIn the sense that of the recommendations that he said were bad, only 63% of them were actually bad.\nWhat this means is that they were throwing out 37% of recommendations, which is one-third.\nAnd for a company that tries on recommendations and people are recruiting through your recommendations, throwing out one-third of them that are actually good is, is, is quite a, is quite a guardrail for them.\nThis was their key metric here and also GPT.\nSo what they did then is they fine-tuned GPT 3.5.\nSo you can see the, the entire journey, right?\nOpen models, GPT4, GPT3, now fine-tuning GPT 3.5.\nUm, GPT 3.5 got the precision they wanted, 0.3 precision, and you know, it's one quarter of GPT4's cost and latency, right?\nBut unfortunately, it was too still too slow, it was about 6.7 seconds, and this would not work in an online filtering system, so therefore what they did was they distilled a lightweight classifier on the fine-tuned GPT 3.5 labels, and this lightweight classifier was able to achieve very high performance, uh, specifically 0.86, 86 AU ROC.\nI mean, the numbers may not make sense to you, but suffice to say that in an industrial setting, this is pretty good.\nAnd of course, they didn't mention the latency, but it was good enough for real-time filtering, I think less than 200 milliseconds or something.\nSo the outcome of this was that they were able to reduce bad recommendations.\nThey, they were able to cut out bad recommendations by about 20%.\nSo initially they had hypothesized that by cutting down recommendations, even though they were bad, you will get fewer subscriptions.\nIt's just like sending out links, right?\nYou might have links that are clickbait, even though they are bad, people just click on it.\nAnd he thought that even if, if we cut down recommendations, even if they were bad, we will get lower application rate.\nBut this was not the case.\nIn fact, because the recommendations were now better, application rate actually went up by 4%.\nAnd unsubscribe rate went down by 5%.\nThat, that's, that's quite a lot.\nSo essentially, what this means is that in recommendations, quantity is not everything.\nQuality makes a big difference.\nAnd the quality here moved the needle quite a bit by 5%.\nThe next example I want to share with you is Spotify.\nSo who here knows that Spotify has podcasts and audio books?\nOh, okay.\nI guess you, you guys are not the target, target audience in, in this use case.\nSo Spotify is really known for song and artists, and a lot of their users just search for songs and artists, and they're very good at that.\nBut when they started introducing podcasts and audio books, how would you help your users know that, you know, these new items are available?\nAnd of course, there's a huge cold-start problem.\nNow it's not only cold-start on item, it's now cold-start on category.\nHow do you start growing a new category within your service?\nAnd of course, exploratory search was essential to the business, right?\nFor going for expanding beyond music.\nSpotify doesn't want to do just do music songs.\nThey just now, now they doing audio.\nSo the solution to that is a query recommendation system.\nSo how did they recommend, how, first, how did they generate new queries?\nWell, um, they have a bunch of ideas, which is, you know, extract it from catalog titles, playlist titles, you mine it from the search logs, you just take the, uh, you just take the artist and then you just add cover to it, and this is what they use from existing data.\nNow you might be wondering like, \"Where's the LM in this?\"\nWell, the LM is used to generate natural language queries.\nSo this might not be sexy, but this works really well, right?\nTake whatever you have with conventional techniques that work really well and use the LM to augment it when you need it.\nDon't use the LM for everything at the start.\nSo now they have this exploratory queries.\nWhen you search for something, you still get the, you still get the immediate results hit, right?\nSo you take all of this, you add the immediate results, and then you rank these new queries.\nSo this is why when you do a search, this is the UX that you're probably going to get right now.\nI got this from a paper.\nIt may have changed recently.\nSo you still see the item queries at the bottom, but at the top with the query recommendations, this is how Spotify informs users without having a banner.\nNow we have audio books, now we have podcasts, right?\nYou search for something, it actually informs you that we have these new categories.\nThe benefit here is plus 9% exploratory queries.\nis essentially one-tenth of their users were now exploring their new products.\nSo imagine that one-tenth every day exploring their new products.\nHow quickly would you be able to grow your, your new product category, right?\nIt's actually 1.1 to the power of n.\nYou will grow pretty fast.\nLong story short, I don't have to tell you about the benefits of LM augmented synthetic data.\nRicher, high-quality data at scale on the tail queries, right?\nEven on the tail queries and the tail items, and it's far lower cost and effort than is even possible with human annotation.\nSo later we also have a talk from Instacart who will tell us about how they use, uh, LLMs to improve their search and recommend, uh, their search system.\nNow the last thing I want to share is this challenge whereby right now in a regular company, the system for ads, for recommendations, for Search, they're all separate systems, and even for recommendations, the, the model for homepage recommendations, the model for item recommendations, the model for cut-to-cut recommendations, the model for the thank you page recommendations, they may all be different models, right?\nSo you can imagine there, you're going to have many, many models.\nBut you going to have, well, leadership expects you to keep the same amount of headcount.\nSo then how do you try to get around this, right?\nYou have duplicative engineering pipelines.\nThere's a lot of maintenance costs, and improving one model doesn't naturally transfer to the improvement in another model.\nSo the solution for this is unified models, right?\nI mean, it works for vision, it works for language.\nSo why not recommendation systems?\nAnd we've been doing this for a\n\n\nWhile this is not new. And aside, maybe the text is too small, but this is a tweet from Stripe whereby they built a Transformer-based payments fraud model, right? Even for payments, the sequence of payments, you can build a foundation model which is Transformer-based.\n\nSo I want to share an example of the unified ranker for Search and Recsys and Netflix, right? The problem I mentioned, they have teams, they are building bespoke models for search, similar item, similar video recommendations, and pre-query recommendations, like on the search page before you even enter a search query. High operational cost, um, you know, missed opportunities from learning throughout. So their solution is a unified ranker, and they call it a unified contextual ranker, which is Unicorn. So you can see over here, uh, at the bottom, there's the user foundation model, and in it, you put in the user watch history, and then you also have the context and relevance model where you put in the context of the videos and what they've watched.\n\nNow, the thing about this unified model is that it takes in unified input, right? So now, if you are able to find a data schema where all your use cases and all your features can use the same input, you can adopt an approach like this, which is multi, similar to multitask learning. So the user, the input will be the user ID, the item ID, you know, the video or the drama or the series, the search query if a search query exists, the country, and the task. So, of course, they have many different tasks. In this example, in the paper, they have three different tasks: uh, search, pre-query, and more like this. Now, what they did then was very smart imputation of missing items. So, for example, if you are doing an item-to-item recommendation, you're just done watching this video, you want to recommend the next video, you would have no search query. How would you impute it? Well, you just simply use the title of the current item and try to find similar items.\n\nThe outcome of this is that this unified model was able to match or exceed the metrics of their specialized models on multiple tasks.\n\nThink about it. I mean, it doesn't seem very impressive, right? It may not seem very impressive. Match or exceed. It might seem we did all this work just to match, but imagine unifying all of it, like removing the tech debt and building a better foundation for your future iterations. It's going to make you iterate faster.\n\nThe last example I want to share with you is unified embeddings at Etsy. So you might think that embeddings are not very sexy, but this paper from Etsy is really uh outstanding in what they share in terms of model architecture as well as their system. So the problem they had was how can we help users get better results from very specific queries or very broad queries, and if you know that Etsy inventory is constantly changing, they don't have the same products all throughout, right? It's very homegrown. So now you might be querying for something like mother's day gift that would almost match very few items. I think very few items would have mother's day gift in their description or their title, right? And you know, lexical embedding, the other problem is that knowledge-based embeddings like lexical embedding retrieval don't account for user preferences. So how do you try to address this?\n\nThe problem, the how do you address this is with a unified embedding and retrieval. So if you remember at the start of my presentation, I talked about the query show tower model, right? There's the user tower and then there's the item tower. We will see the same pattern again over here. You see the product tower, right? This is the product encoder. So how they encode the product is that they use T5 models for text embeddings, right? Text item descriptions as well as a query product log for query embeddings. What was the query that was made and what was the product that was eventually clicked or purchased?\n\nAnd then over here on the on the left, you see the query encoder, which is the search query encoder. And they both share encoders for the tokens, which is actually a text tokens, the product category, which is a token of a token of itself, and the user location. So what this means is that your now your embedding is able to match user to the location of the product itself.\n\nAnd then, of course, to personalize this, they encode the user preferences via the query user scaler features at the bottom. Essentially, what were the queries that the user searched for? What did they buy previously, all their preferences. Now, this is they also shared their system architecture, and over here, this is the product encoder from the previous slide and the query encoder from the previous slide. But what's very interesting here is that they added a quality vector because they wanted to ensure that whatever was searched and retrieved was actually of good quality in terms of ratings, uh, freshness, and conversion rate. And you know what they did is they just simply concatenated this quality vector to the product embedding vector.\n\nBut when you do that for the query vector, you have to you have to expand the product vector by the same dimension so that you can do a dot product or cosine similarity. So essentially, they just slapped on a constant vector uh for the query embedding and it just works.\n\nThe result: 2.6% increase in conversion across the entire site. That's quite crazy. Um, and more than 5% increase in search purchases. If you search for something, the purchase rate increases by 5%. Um, this is very, very, very good results for e-commerce.\n\nUm, so the benefits of uh unified models, you simplify the system, uh, you whatever you build to improve one side of the tower or improve your model, your unified model also improves other use cases that use these unified models. That said, there may also be the alignment text. You you you may find that when you try to build this, try to compress all 12 use cases into a single unified model, you may need to split it up into maybe two or three separate unified models because that's just the alignment text. We're trying to get better at one task actually makes the other task worse. We have a talk from uh LinkedIn in the in this afternoon's in this afternoon blog, the last talk of the blog, and then we also have a talk from Netflix uh which we'll be sharing about their unified model at the start of the next blog. All right, the three takeaways I have for you: think about it, consider it. Semantic IDs, data augmentation, and unified models. Um, and of course, do stay stay tuned for the rest of the track uh for the rest of the talks in this track. Okay, that's it. Thank you.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:24.975Z"
}