{
  "episodeId": "TBjCvB_4mdo",
  "channelSlug": "@machinelearningstreettalk",
  "title": "The Humble Truths Behind Bombastic AI Papers",
  "publishedAt": "2025-07-16T16:04:26.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "In pure mathematics, we have an",
      "offset": 0.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "incredibly brutal aesthetic/formal",
      "offset": 1.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "judgment for when something's true,",
      "offset": 4,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "right? We've got definition and proof.",
      "offset": 5.279,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "But here is a context where you don't",
      "offset": 7.359,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "have that. So instead, you have to",
      "offset": 9.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "concoct something that, you know, will",
      "offset": 10.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "serve a similar role. And Hart argues",
      "offset": 12.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "that this is what benchmarks have been",
      "offset": 15.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "for and what benchmarks have been",
      "offset": 17.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "succeeding for until relatively",
      "offset": 18.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "recently.",
      "offset": 20.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "These deep learning models, as good as",
      "offset": 22.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "they are, they build sand castles. And",
      "offset": 24.08,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "what is a sand castle? It is a structure",
      "offset": 27.039,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "with too many degrees of freedom. You",
      "offset": 29.359,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "prod the sand castle and it just",
      "offset": 31.439,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "collapses because there's no",
      "offset": 33.6,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "undergirling structure in it.",
      "offset": 34.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Okay, but so here's where I think that",
      "offset": 36.079,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "that might not be exactly true.",
      "offset": 37.44,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "Well, is it possible to reconcile",
      "offset": 45.52,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "constructivism and plonism? Right. So so",
      "offset": 48.16,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "many folks really believe that there is",
      "offset": 53.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "actually some generating function of the",
      "offset": 55.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "universe that it appears constructive to",
      "offset": 58.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "us. To us it appears that there's a",
      "offset": 60.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "kaleidoscope that there are pockets of",
      "offset": 62.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "regularities but there's nothing that",
      "offset": 64.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "binds it together but we like to imagine",
      "offset": 66.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that underneath you know behind",
      "offset": 69.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the illusion of teology right",
      "offset": 71.52,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "yeah well it's a bit of a Plato's cave",
      "offset": 73.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "type thing that you know actually if if",
      "offset": 75.439,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "only you know we we see the shadows but",
      "offset": 77.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "actually there is something there which",
      "offset": 79.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "which is universal",
      "offset": 81.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I suspect that the world is in this",
      "offset": 83.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "sense is like fundamentally constructive",
      "offset": 85.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "but it it has enough regularities and",
      "offset": 87.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "like the processes in it like the reason",
      "offset": 90.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "why they work is because they are",
      "offset": 92.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "capable of creating the illusion of an",
      "offset": 94.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "ideal",
      "offset": 97.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like I think that's why stuff works is",
      "offset": 99.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that it's contingent like constructive",
      "offset": 101.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "stuff that is sufficiently regular such",
      "offset": 103.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that like there's a pro like I think",
      "offset": 107.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "there's like a constructive process",
      "offset": 108.64,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "probably like a constructive process of",
      "offset": 110,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "idealization",
      "offset": 111.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "right that goes on like I think people",
      "offset": 113.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "in like the you know theoretical biology",
      "offset": 115.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "world in the 70s were really into like",
      "offset": 118.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "what the hell is a metabolism and like",
      "offset": 121.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "thinking about that as kind of a predict",
      "offset": 124.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like an anticipatory process like a",
      "offset": 125.92,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "metabolism is this chemical cycle that",
      "offset": 128.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "anticipates that stuff is going to keep",
      "offset": 130.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "coming in to run it and so it's it's",
      "offset": 132.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "actually running before it gets the",
      "offset": 134.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "thing",
      "offset": 136.72,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "right so it's that kind of thing so I",
      "offset": 137.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "think my my my vague you know like",
      "offset": 139.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "I just have a feeling kind of feeling",
      "offset": 143.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "about this would be",
      "offset": 145.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "no I'm defin I don't think that there is",
      "offset": 148.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "a a platonic thing here. I think that's",
      "offset": 150.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "wrong. But I think that developing the",
      "offset": 153.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "illusions of platonism is how structure",
      "offset": 156,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "happens in the world.",
      "offset": 159.28,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "But this um metabolism metaphor I think",
      "offset": 160.879,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is beautiful because there are so many",
      "offset": 163.519,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "examples like that. You know like the",
      "offset": 164.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "brain is a prediction machine or the the",
      "offset": 166.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "information metaphor is a wonderful one.",
      "offset": 168.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Biologists talk about you know",
      "offset": 170.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "information as if it has primacy and and",
      "offset": 172.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "explanatory power. And this is clearly a",
      "offset": 174.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "limitation of our cognitive horizon.",
      "offset": 178.08,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Right? So what we do is we we view",
      "offset": 180.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "things locally and we come up with these",
      "offset": 182.959,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "metaphors, these abstractions and and we",
      "offset": 185.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "understand these abstractions because",
      "offset": 188.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "they're quite microscopic. But it feels",
      "offset": 190.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "like there's there's something more to",
      "offset": 192.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "it than that. If only we could have a",
      "offset": 193.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "more global understanding. idealization",
      "offset": 195.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "in this sense is kind and like therefore",
      "offset": 198.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you know like the the plausibility of",
      "offset": 200.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "platonism is like an excellent problem",
      "offset": 202.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "solving strategy",
      "offset": 204.72,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "is like to the invent the thing that's",
      "offset": 206.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not there you know like it's concepts",
      "offset": 208.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "like fitness in biology right I think",
      "offset": 210.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "it's what like I think the the right you",
      "offset": 212.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "know fancy words are apostori or",
      "offset": 214.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "something along those lines right",
      "offset": 216.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "fitness doesn't exist but let's imagine",
      "offset": 217.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that it is and then if we reason about",
      "offset": 220,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it as if fitness exists",
      "offset": 221.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "right it makes the story easier It",
      "offset": 223.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "closes things down. It makes it such",
      "offset": 227.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that there's a a quantity. You can",
      "offset": 229.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "imagine there are being a quantity",
      "offset": 230.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "associated to that notion that happens",
      "offset": 233.44,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "like it doesn't actually exist in the",
      "offset": 235.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "world. But you know and it's this is not",
      "offset": 236.879,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "even like it's you know not even even",
      "offset": 239.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "worse than like quantum mechanics is",
      "offset": 241.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "kind of like this but they're act we",
      "offset": 242.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "sort of do at this point kind of like",
      "offset": 244.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "believe there's some like ontological",
      "offset": 246.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "reality to the distribution not merely",
      "offset": 247.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to the sampling but for other physical",
      "offset": 250.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "processes that are totally macroscopic",
      "offset": 253.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "like I don't believe that there's like",
      "offset": 255.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that the distribution for uh you know a",
      "offset": 257.919,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "species is like essentially real. It's",
      "offset": 261.04,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "like no, the sampling is real, but the",
      "offset": 264.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "abstraction of a distribution like",
      "offset": 266.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "things act as if there were such a",
      "offset": 268.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "thing. There isn't which makes it sort",
      "offset": 270.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 272.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "I mean this is where the plaintists have",
      "offset": 273.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "a point, right? It's like well maybe",
      "offset": 275.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "maybe like what do you mean how real",
      "offset": 277.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "does it have to be to be real man?",
      "offset": 279.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And like maybe that just like well it",
      "offset": 281.28,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "you know walks like a duck why why beat",
      "offset": 283.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "around the bush? Why just why not just",
      "offset": 285.759,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "imagine that there is such a thing? I",
      "offset": 287.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "have no idea you know what is the right",
      "offset": 289.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "answer. I think the like honestly my",
      "offset": 292,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "sensibility with this kind of stuff is",
      "offset": 293.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like you want to know and think about it",
      "offset": 295.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "enough to be electrified by it but not",
      "offset": 297.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "menaced by it.",
      "offset": 299.44,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "I spoke with Maxwell Ramstead a couple",
      "offset": 300.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "of weeks ago and he he quoted a",
      "offset": 301.919,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "philosopher I forget the name but he",
      "offset": 303.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "said reality pushes back",
      "offset": 304.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 307.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "is there something you think about as a",
      "offset": 309.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "category the theorist? I mean what does",
      "offset": 310.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it mean for reality to push back when",
      "offset": 312.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you come up with a model? as someone",
      "offset": 314.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "trained to some extent in category",
      "offset": 316.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "theory and then spending the last year",
      "offset": 318.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and a half working in machine learning.",
      "offset": 320.96,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "Um the way it pushes back is",
      "offset": 323.44,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "I don't know how much of the story how",
      "offset": 329.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "useful it's going to be in the long run.",
      "offset": 331.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I don't I think it brings a bunch of",
      "offset": 333.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "really powerful sensibilities but the",
      "offset": 336.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "naive applications are likely wrong. um",
      "offset": 338.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "for the same reason that I think uh",
      "offset": 341.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "abiding obsessions with architecture are",
      "offset": 343.919,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "likely wrong. I think that you know it",
      "offset": 346.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "could be a very useful tool for indexing",
      "offset": 349.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the experiments that essentially find",
      "offset": 352.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the boundaries between performant and",
      "offset": 354,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "non-performant architectures and that's",
      "offset": 355.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the interesting question",
      "offset": 357.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "right the sensibility I've got like so I",
      "offset": 359.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "can't answer the question you you you",
      "offset": 361.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "gave me precisely instead I can sort of",
      "offset": 362.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "say how do I feel about the whole thing",
      "offset": 364.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "now and what's the way that I'm",
      "offset": 366.24,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "imagining things is",
      "offset": 368.319,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "you know what neural networks are at",
      "offset": 371.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "least as far as they are like you know",
      "offset": 375.039,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "modeled and machines or whatever. Not",
      "offset": 376.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "talking about actual brains or anything.",
      "offset": 378.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "They are an algebra for constructing",
      "offset": 380.319,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "parametric models such that uh a physics",
      "offset": 382.72,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "you know like a stochastic process like",
      "offset": 388.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "SG like will cause it to relax to some",
      "offset": 390.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "lowest energy state and the parameters",
      "offset": 394.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "associated to the that you know there",
      "offset": 396.4,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "may be many choices of parameters",
      "offset": 398.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "associated to that lower energy state",
      "offset": 399.759,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "but the model at that lowest energy",
      "offset": 401.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "state that's the thing that computes",
      "offset": 403.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "sort of gen is capable of approximating",
      "offset": 407.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the data you fed into it, right? And so",
      "offset": 409.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "what's the role for category theory in",
      "offset": 412,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "such a story is well it makes it",
      "offset": 414.8,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "relatively easy to have the idea of what",
      "offset": 416.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "do you what the do you mean by an",
      "offset": 418.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "algebra for constructing these systems",
      "offset": 420.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like that's where it's useful right is",
      "offset": 422.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "this you know because you can imagine",
      "offset": 425.12,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "it's like well I've got various",
      "offset": 426.319,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "different kinds of algebra I've got you",
      "offset": 427.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know wiring diagrams out that's one kind",
      "offset": 429.039,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of algebra you know I've got you know",
      "offset": 430.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "standard algebra I've got that you know",
      "offset": 432.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "I've got various kinds of fl like",
      "offset": 434.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "various other categorical flavors of",
      "offset": 435.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "what is meant by algebra it's like sort",
      "offset": 437.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "of it gives you access to making it easy",
      "offset": 439.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to formalize these things, right? And",
      "offset": 441.919,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "say what it means to like how are you in",
      "offset": 443.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "what ways are you allowed to build",
      "offset": 445.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "things, right? Those are all algebbras",
      "offset": 447.039,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to a category theorist or a co-algebra",
      "offset": 448.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "co-algebbras if people are careful",
      "offset": 451.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "enough, you know, but morally speaking,",
      "offset": 453.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this falls all falls under like what a",
      "offset": 456,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "category theorist would call algebra.",
      "offset": 458.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "And so like that's the interesting theor",
      "offset": 460,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "thing here, I think. Like that's what we",
      "offset": 461.919,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "should be trying to use the category",
      "offset": 464.479,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "theory story for or the tools from it",
      "offset": 467.84,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "for is making it easy to index across a",
      "offset": 470.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "lot of different things to then start",
      "offset": 473.599,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "discovering the performance of those and",
      "offset": 476.319,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sort of running those experiments. You",
      "offset": 479.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "know like most of my last year and a",
      "offset": 480.639,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "half of work has not been um supremely",
      "offset": 482.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "categorical. It's instead sort of been",
      "offset": 486.8,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "like no I would like to be humble. I",
      "offset": 488.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "humbled I would like to learn the",
      "offset": 491.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "accumulated wisdom of you know like the",
      "offset": 493.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "ML world you know this is very much you",
      "offset": 495.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "know a book that I've started reading",
      "offset": 498.319,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "and that I think is at least for me is",
      "offset": 502,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "likely to be one of the most important",
      "offset": 504.879,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "books in my work as like a machine",
      "offset": 506.56,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "learning researcher and this is uh",
      "offset": 509.36,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Moritz Hart's new um emerging science of",
      "offset": 511.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "benchmarks",
      "offset": 514.959,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "and in particular the best thing is",
      "offset": 516,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "because you know this is the this is so",
      "offset": 517.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "I'll say this is the book that if I",
      "offset": 519.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "really felt like I still had the energy",
      "offset": 521.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and like the real drive to truly",
      "offset": 523.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "understand not merely a scientific",
      "offset": 525.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "question but like the political and",
      "offset": 527.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "philosophical contingencies from which",
      "offset": 530,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it arises like all of those this and",
      "offset": 532,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then and I wanted to do that story about",
      "offset": 534.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "machine learning I think this is the",
      "offset": 536.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "book that I would imagine I would want",
      "offset": 538,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to write but you know the first chapter",
      "offset": 539.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "says you know machine learning is",
      "offset": 541.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "essentially uh a synthesis of Fiora",
      "offset": 544,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Ben's uh notion of anything goes right",
      "offset": 547.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So farend whether I'm pronouncing the",
      "offset": 549.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "name right or wrong is immaterial you",
      "offset": 551.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "know contemporary of and popper and",
      "offset": 554,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that sort of like mid 20th century you",
      "offset": 556.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "know",
      "offset": 560,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "big deal about okay what is the",
      "offset": 561.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "scientific method it's definitely not",
      "offset": 563.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "what we idealized it to be in the late",
      "offset": 564.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "1800s you know with compton like those",
      "offset": 566.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "folks it's something else what is it you",
      "offset": 569.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "know and comes up with this notion",
      "offset": 571.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "of you know paradigms and revolutions",
      "offset": 572.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "and all of these things a bunch of",
      "offset": 574.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "people who are all operating on like",
      "offset": 576.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "superficially compatible metaphors, but",
      "offset": 578.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually having deeply intuitive and",
      "offset": 581.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "kind of wild sensibilities",
      "offset": 582.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "about what's going on. But the only",
      "offset": 585.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thing that allows you to judge what",
      "offset": 587.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "science looks like is seriousness. And",
      "offset": 588.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "then he describes what like the",
      "offset": 592.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "character of that notion of seriousness",
      "offset": 594.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is. But sort of you can describe this as",
      "offset": 596.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like how science really works is well",
      "offset": 598.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "actually anything goes. And in some",
      "offset": 599.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "sense machine learning has taken that",
      "offset": 601.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "mantle up seriously. It's like I don't",
      "offset": 602.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "know what like the right aesthetic",
      "offset": 605.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "sensibility to have about what a good",
      "offset": 608.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "enough description for machine learning",
      "offset": 609.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "is. I don't think anybody does. And",
      "offset": 611.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "indeed people are like actually that's",
      "offset": 614.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the wrong question. We just don't know",
      "offset": 616.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "enough to even try that. And so in this",
      "offset": 617.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "sense anything goes. But then you also",
      "offset": 620,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "need well well how do you estab if you",
      "offset": 621.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "have no aesthetic judgment for when",
      "offset": 623.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "something is true right? I'll say this",
      "offset": 625.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that you know um in pure mathematics we",
      "offset": 627.519,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "have an incredibly brutal like",
      "offset": 630.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "aesthetic/formal judgment for when",
      "offset": 632.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "something's true right we've got",
      "offset": 634.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "definition and proof but here is a",
      "offset": 635.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "context where you don't have that so",
      "offset": 637.92,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "instead you have to concoct something",
      "offset": 639.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that you know will you know serve a",
      "offset": 641.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "similar role and Hart's Hart argues that",
      "offset": 643.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this is what benchmarks have been for",
      "offset": 646.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and what benchmarks have been seating",
      "offset": 648.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "succeeding for until relatively recently",
      "offset": 650.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you know the example is well you take",
      "offset": 653.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "you know one particular family of models",
      "offset": 655.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "all for similar task and you take a",
      "offset": 657.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "bunch of benchmarks for that same kind",
      "offset": 659.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of task and while the actual numbers you",
      "offset": 661.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "get in terms of performance are",
      "offset": 664.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "different the ranking is was preserved",
      "offset": 665.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "across all of those benchmarks but",
      "offset": 667.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that's not the case anymore",
      "offset": 669.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "right and so this is either indication",
      "offset": 671.68,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "that uh you know like all of the easy",
      "offset": 674.959,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "stuff with models is gone and therefore",
      "offset": 679.12,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "we simply shouldn't expect there to be a",
      "offset": 682.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "preservation of relative capability",
      "offset": 684.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "across benchmarks and that's just like",
      "offset": 687.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the wrong thing to think about. Either",
      "offset": 689.68,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "that or the entire approach of",
      "offset": 691.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "everything is benchmarks is broken. I",
      "offset": 695.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "haven't read deep enough into the book,",
      "offset": 698.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "but I suspect Hart's premise is our",
      "offset": 699.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "heart is going to go for I think only",
      "offset": 701.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "going on benchmarks is broken. And we",
      "offset": 704.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "need to sort of establish at least",
      "offset": 706.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "higher order aesthetic sensibilities of",
      "offset": 709.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "what an adequate argument for why",
      "offset": 711.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "something works is",
      "offset": 712.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like that's the kind of place where I",
      "offset": 714.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I'm hoping to like be operating in this",
      "offset": 715.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "space. Like the thing that you know I've",
      "offset": 719.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "learned over the last year and a half,",
      "offset": 720.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the thing that motivates me is telling a",
      "offset": 722.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "story about why that I believe, right?",
      "offset": 724.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "not merely telling a compelling story",
      "offset": 727.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "about why, but like I just wanted to",
      "offset": 728.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "like I I've tried to be motivated by",
      "offset": 730.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "lots of different things. You know, I'm",
      "offset": 733.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "in a business research environment and",
      "offset": 735.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "so I can I try to get really excited",
      "offset": 738.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "about what this thing would do for our",
      "offset": 740.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "market position, all of that stuff. I",
      "offset": 742,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "can't maintain the excitement. The thing",
      "offset": 744.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that I can maintain excitement for is",
      "offset": 745.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "providing an elegant and intuitive",
      "offset": 748.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "description of why something works.",
      "offset": 750.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Like that's the interesting thing. And",
      "offset": 752.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "so that's kind of the thing that uh the",
      "offset": 754.959,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "part of the sort of new era of machine",
      "offset": 757.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "learning that I think we have to go to",
      "offset": 760.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that I want to like be part of is like m",
      "offset": 762.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "like figuring out",
      "offset": 764.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what even is an adequate explanation",
      "offset": 767.12,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "here.",
      "offset": 769.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Yeah. I mean this comes back to um when",
      "offset": 770.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "I spoke with Chsky he was he was joking",
      "offset": 772.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "with us. He said you know I' I've got a",
      "offset": 774.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "scientific theory and it's anything",
      "offset": 776.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "goes.",
      "offset": 779.68,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah, you know, because he said",
      "offset": 780.639,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "machine learning, you know, it's it's",
      "offset": 781.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's it's like a bulldozer. It's great",
      "offset": 783.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "for clearing the snow, but it's not a",
      "offset": 785.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "theory of science. And and when I spoke",
      "offset": 787.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "with Dennit, you know, I was I was",
      "offset": 789.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "asking him, he was a philosopher of",
      "offset": 791.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "science. I said, \"What what is science",
      "offset": 793.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "all about?\" And he said, \"It's creating",
      "offset": 794.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "explanations. You know, it's it's",
      "offset": 796.959,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "generating the wise. The wise are",
      "offset": 798.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "important.\" And these these deep",
      "offset": 800.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "learning models, as good as they are,",
      "offset": 802.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "they build sand castles. And what is a",
      "offset": 804.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "sand castle? It is it is a a structure",
      "offset": 807.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "with too many degrees of freedom. You",
      "offset": 810.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "you prod the sand castle and it just",
      "offset": 812.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "collapses because there's no",
      "offset": 814.639,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "undergirling structure. And it it's not",
      "offset": 816.079,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "even that there's no undergirling",
      "offset": 817.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "structure. The problem is there is every",
      "offset": 818.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "structure, right? Any possible structure",
      "offset": 820.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "is represented in the sand and humans",
      "offset": 822.959,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "can can guide these models to make them",
      "offset": 826.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "behave as if they have structure. But",
      "offset": 828.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "actually having every structure is the",
      "offset": 830.959,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "same as having no structure.",
      "offset": 832.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "Okay. But so here's where I think that",
      "offset": 833.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that might not be exactly true, right?",
      "offset": 835.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "And so I'm not uh this is again a domain",
      "offset": 837.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "of the machine learning world in which I",
      "offset": 841.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "am not yet an expert very much a neoight",
      "offset": 843.199,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "but so I was just at a structural AI",
      "offset": 846.16,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "conference at in Shanghai hosted by the",
      "offset": 849.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Shanghai Chi Institute but he gave a",
      "offset": 852.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "talk sort of providing a statistical",
      "offset": 855.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "learning theory analysis of why chain of",
      "offset": 857.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "thought works and in particular I was so",
      "offset": 859.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "compelled by the story because first of",
      "offset": 862.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "all just in terms of what kind of thing",
      "offset": 864.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 866.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "He didn't give me or he didn't give the",
      "offset": 867.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "audience a humanist or an",
      "offset": 869.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "anthropomorphized explanation for this",
      "offset": 871.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that says it's like oh well we think",
      "offset": 873.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "more clearly some kind of you know bogus",
      "offset": 875.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "appeal to system 2. I don't believe in",
      "offset": 877.199,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "system two. I think we talked about that",
      "offset": 878.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "last time. I think it's garbage",
      "offset": 880.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "but we'll get to that later.",
      "offset": 882.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 884.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "But he was like",
      "offset": 885.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "actually I think that chain of thought",
      "offset": 888.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "works for the same reason that",
      "offset": 890.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "transformers work for",
      "offset": 893.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "natural language corpora which is the",
      "offset": 897.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "following. Um",
      "offset": 900,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "transformers beat the worst case",
      "offset": 903.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "predictions for how long it is going to",
      "offset": 905.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "take a particular model after a process",
      "offset": 908.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "of updating to model a distribution of",
      "offset": 910.8,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "sequences. It beats that time by a lot",
      "offset": 914.56,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "on natural language data. Why is because",
      "offset": 918.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "with relatively high probability you can",
      "offset": 922.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "predict the next thing in a sequence of",
      "offset": 924.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "natural language data from relatively",
      "offset": 926.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "few things that precede it. Right? So",
      "offset": 929.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "the rel the sparse dependency that is a",
      "offset": 931.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "character of natural language",
      "offset": 933.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "transformers are adapted to that to",
      "offset": 936.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "quickly identifying it and that's why",
      "offset": 938.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "they beat you know the worst case",
      "offset": 940.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "predictions for for how long it would",
      "offset": 942.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "take a model trained in this fashion to",
      "offset": 943.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "learn the distribution. Mhm.",
      "offset": 946,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "He says that you know and you know",
      "offset": 948.639,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "interview him he might disagree with my",
      "offset": 951.279,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "characterization of his talk that's",
      "offset": 952.959,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "entirely likely but uh he then says I",
      "offset": 954.399,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "think this is why chain of thought works",
      "offset": 958,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "which is because chain of thought for",
      "offset": 960.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "reasoning similarly sparsifies the",
      "offset": 961.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "dependency of the formal like of the",
      "offset": 964.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "mathematical statement that you're",
      "offset": 966.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "trying to make is each little thing",
      "offset": 968,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "instead of having the final answer",
      "offset": 971.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "depend in on all of the things that had",
      "offset": 973.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to be brought",
      "offset": 975.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "So you have each one submputations done",
      "offset": 976.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "separately. They're more secure. They're",
      "offset": 979.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "more certain, etc. So it sparsifies the",
      "offset": 981.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "dependencies to do the computation. But",
      "offset": 983.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I was profoundly dissatisfied at the",
      "offset": 985.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "time with the epistemology of sort of",
      "offset": 987.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "complex systems work. I figured I needed",
      "offset": 989.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to go do something like harder and more",
      "offset": 991.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "certain. And so I was like, I've got an",
      "offset": 993.04,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "idea. I'll go become an algebra",
      "offset": 994.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "geometer. Um that didn't entirely work",
      "offset": 995.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "out largely because this incredible",
      "offset": 998.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "thing called homotopi type theory",
      "offset": 999.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "happened during my mathematics PhD.",
      "offset": 1001.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "eventually finished up doing a PhD",
      "offset": 1004.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "really in sort of cominatorial models of",
      "offset": 1006.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "stable homotopic theory which is the",
      "offset": 1007.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "notion of homotopic theory that",
      "offset": 1009.839,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "corresponds to the idea that instead of",
      "offset": 1011.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "having de dimension valued in the",
      "offset": 1012.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "natural numbers instead it's valued in",
      "offset": 1014.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the integers you know academia has a",
      "offset": 1016.079,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "long history of provocative provocative",
      "offset": 1018.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "titles to papers you know so I wouldn't",
      "offset": 1020.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and it seems like maybe clickbait",
      "offset": 1023.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "culture is kind of uh infiltrating into",
      "offset": 1024.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "into paper titles um even more I mean",
      "offset": 1028,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like a classic example is is you know",
      "offset": 1030.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "goto considered harmful by Dystra,",
      "offset": 1032.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "right? That was that was back in like '",
      "offset": 1034.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "68. But you got fun things like that.",
      "offset": 1036.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. No, I think that the world",
      "offset": 1037.839,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "of pure mathematics is um significantly",
      "offset": 1040.319,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "more I one could either call it straight",
      "offset": 1043.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "laced or paranoid paranoid that anyone",
      "offset": 1045.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "will ever uh say something wrong. And so",
      "offset": 1048.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the fear of saying something wrong or",
      "offset": 1051.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "overstating your case is so intense that",
      "offset": 1052.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "people have these astoundingly boring",
      "offset": 1056.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "titles",
      "offset": 1057.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "for like incredible results. And so",
      "offset": 1059.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that's really the difference is, you",
      "offset": 1061.36,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "know, you got these amazing titles that",
      "offset": 1062.559,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "really clear sound like you're going to",
      "offset": 1064.16,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "drop a bomb on this thing. And",
      "offset": 1065.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the reality is it's like this is an",
      "offset": 1067.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "incremental improvement that might in",
      "offset": 1069.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "fact be wrong and you've just asserted",
      "offset": 1070.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like grand authority with the title,",
      "offset": 1072.4,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "right?",
      "offset": 1074.72,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "You know, so this is just part of the",
      "offset": 1075.039,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "learning process of moving to a new",
      "offset": 1076.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "discipline and you know, you come in of",
      "offset": 1077.919,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "course with the the the ideas like oh",
      "offset": 1081.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "these people are clearly doing it wrong",
      "offset": 1083.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and then within 6 months you're like",
      "offset": 1085.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "okay I kind of understand 90% of this. I",
      "offset": 1086.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "think you're doing 10% of it wrong and",
      "offset": 1088.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then after another year you're like",
      "offset": 1090.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "mostly you're doing it right but you",
      "offset": 1092.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "could slow down here here and here and",
      "offset": 1094.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "maybe that's where some stuff can come",
      "offset": 1096.88,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "in.",
      "offset": 1098.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah, most of our viewers could could",
      "offset": 1098.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "understand intuitively that something",
      "offset": 1101.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like an RNN, right, is applying the same",
      "offset": 1102.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the literal same calculation over and",
      "offset": 1105.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "over and over again in sequence. And",
      "offset": 1107.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with with CNN's, you know, it's almost",
      "offset": 1110.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "like you got to think of it as a",
      "offset": 1111.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "flashlight just being shined over the",
      "offset": 1113.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "kind of XY, you know, axes of an image.",
      "offset": 1115.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "That's sort of the the little",
      "offset": 1118.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "convolution kernel there. So those types",
      "offset": 1119.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of um formal constraints are simple to",
      "offset": 1121.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to see and imagine and we could think",
      "offset": 1124.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "about, you know, kind of building them",
      "offset": 1126.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "into the code. But what about I mean",
      "offset": 1128.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just things that are much harder for for",
      "offset": 1130.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "humans to constrain that way. I mean",
      "offset": 1133.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "what chance is there of really let's say",
      "offset": 1136.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "programmatically automatically",
      "offset": 1139.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "discovering so having a set of data and",
      "offset": 1141.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "then statistically seeing you know what",
      "offset": 1144.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "there seems to be something that should",
      "offset": 1146.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "be a formal constraint and then kind of",
      "offset": 1148.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "factoring that out and then enforcing it",
      "offset": 1150.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "is a formal constraint like do you think",
      "offset": 1152.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "category theory offers tooling there",
      "offset": 1154.799,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "I mean it gives you a nice way to phrase",
      "offset": 1158.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the problem I don't in general know how",
      "offset": 1160.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to do it but I think it's a good",
      "offset": 1163.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "question Um, you know, the analogy I",
      "offset": 1164.88,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "might make here is in a very handwavy",
      "offset": 1168.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "sense that I don't know how to",
      "offset": 1171.919,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "formalize, but I'm sure more",
      "offset": 1173.6,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "sophisticated members of the machine",
      "offset": 1175.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "learning community can do a better job",
      "offset": 1176.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "about this than I. What should you think",
      "offset": 1178.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of a causally masked transformer doing?",
      "offset": 1180.559,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "What does it really do? Is it's a pro",
      "offset": 1183.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "it's doing a parallelized approximation",
      "offset": 1187.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "only n layers deep of an RNN,",
      "offset": 1190.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "right? That's really what you're doing.",
      "offset": 1193.36,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "That's an interesting. Has that analogy",
      "offset": 1195.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "or that that correspondence been pointed",
      "offset": 1196.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "out before? I like it. I just saw this",
      "offset": 1199.039,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "one.",
      "offset": 1201.039,
      "duration": 1.921
    },
    {
      "lang": "en",
      "text": "I I don't know if it's been pointed out,",
      "offset": 1201.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "but my inspiration for thinking about it",
      "offset": 1202.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like this is really looking at like uh I",
      "offset": 1204.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "can't remember the guy's name. Um",
      "offset": 1208.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but I think it's probably",
      "offset": 1210.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "I'm guessing. No. No.",
      "offset": 1213.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Uh I don't no I I don't know a damn",
      "offset": 1214,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thing about Schmidt Hooper's work, but",
      "offset": 1216.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "he was probably was right in the 90s.",
      "offset": 1217.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Like I'll say that. I kind of suspect",
      "offset": 1219.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that that's true. Um no but uh so I",
      "offset": 1220.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "learned this or developed this",
      "offset": 1223.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sensibility after watching uh Peter",
      "offset": 1225.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "actually had this really excellent talk",
      "offset": 1227.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "at the uh graph learning workshop",
      "offset": 1230.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "uh a couple of months ago and like I I",
      "offset": 1233.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just absolutely loved it. And I told my",
      "offset": 1234.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "boss, I told everyone at the company",
      "offset": 1236.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like this is exactly what really good",
      "offset": 1238.159,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "foundational ML research in a business",
      "offset": 1241.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "environment looks like",
      "offset": 1245.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "because you know started started with",
      "offset": 1247.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the question you know and so I can",
      "offset": 1249.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "explain like I mean ask ask Peter",
      "offset": 1250.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "exactly what was in that talk that I",
      "offset": 1253.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "thought was so good but",
      "offset": 1255.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "we'd love to hear your perspective.",
      "offset": 1258.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Yeah. No, I will. So the perspective",
      "offset": 1259.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "here is that uh you know transformers",
      "offset": 1261.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "are graph neural networks",
      "offset": 1264.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and so and that's that's where you get",
      "offset": 1268,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the sense where it's like well the",
      "offset": 1270,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "causal masking it just tells you which",
      "offset": 1271.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "nodes can update which other nodes and",
      "offset": 1273.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "what is the pattern of their thing",
      "offset": 1275.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "they're allowed to only do this they're",
      "offset": 1277.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "allowed to just bump things up. So",
      "offset": 1278.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "essentially you're restricting the kind",
      "offset": 1280.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of functions that the RNN is allowed to",
      "offset": 1281.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interpolate. If you say that your RNN is",
      "offset": 1283.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "only allowed to shift things as opposed",
      "offset": 1285.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to like bend your space in a complex",
      "offset": 1287.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "way, you make that hypothesis. So none",
      "offset": 1289.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "of this is formalized yet. And I'm not I",
      "offset": 1292.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "might say some something wrong here and",
      "offset": 1293.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "I'll just like wave my hands with that",
      "offset": 1295.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "appropriate caveat. But it is in this",
      "offset": 1297.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "sense the way that in a transformer",
      "offset": 1300.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "previous things can update later on",
      "offset": 1303.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "things that is a and that's and just",
      "offset": 1305.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "exclusively just by bumping them a",
      "offset": 1307.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "little bit. that is very much analogous",
      "offset": 1310.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to the way that previous steps in",
      "offset": 1312.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "recursion update this thing. But",
      "offset": 1314.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "essentially, you're just saying instead",
      "offset": 1317.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of remembering the actual function that",
      "offset": 1318.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "moves things around, you're just",
      "offset": 1320.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "remembering a bunch of displacements and",
      "offset": 1321.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you're passing displacements up the",
      "offset": 1323.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "chain. And the reason why this works for",
      "offset": 1324.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sort of sparse dependencies and things",
      "offset": 1327.039,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "like that is because you actually only",
      "offset": 1328.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "need to bump it like have the cascade of",
      "offset": 1329.84,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "bumpings go all the way to the front a",
      "offset": 1333.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "few times before you've gotten enough of",
      "offset": 1336.72,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "it,",
      "offset": 1338.559,
      "duration": 1.761
    },
    {
      "lang": "en",
      "text": "right? Right. And that's really critical",
      "offset": 1339.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "is the is the finite depth because",
      "offset": 1340.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that's that's really the key that allows",
      "offset": 1342.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the parallelized, you know, training.",
      "offset": 1344.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "Exactly.",
      "offset": 1346.64,
      "duration": 1.2
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1347.2,
      "duration": 0.959
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1347.84,
      "duration": 0.8
    },
    {
      "lang": "en",
      "text": "Go ahead.",
      "offset": 1348.159,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Yeah. Isn't that um a little bit of an",
      "offset": 1348.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "idealization as well because many of",
      "offset": 1350.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "these models we can think of them as as",
      "offset": 1353.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "graph neural networks because GNN's are",
      "offset": 1355.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "quite intuitive in the sense that we can",
      "offset": 1357.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "think of them as information diffusers.",
      "offset": 1359.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "But Peta has done some interesting work",
      "offset": 1363.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "with some other folks that I interviewed",
      "offset": 1364.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "at Europs talking about information",
      "offset": 1366.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "squashing. So even though um a",
      "offset": 1368.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "transformer might be an idealized",
      "offset": 1371.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "version of an RNN as Keith was saying",
      "offset": 1373.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with this with this fixed depth, it's",
      "offset": 1375.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it's not really the thing that we want",
      "offset": 1377.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "because you have this like information",
      "offset": 1380.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "squashing problem. So RNN's for example",
      "offset": 1383.28,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "um when you you know roll them auto",
      "offset": 1385.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "reggressively the information gets",
      "offset": 1388.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "slowly forgotten and transformers and",
      "offset": 1390.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "GNN's have a similar form of this",
      "offset": 1392.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "problem where all of the the information",
      "offset": 1394.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "pathways get compressed so that it's not",
      "offset": 1396.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "actually paying attention to many of the",
      "offset": 1399.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "tokens in the context.",
      "offset": 1401.039,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Yeah. No, right in this. So, Peter",
      "offset": 1402.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "actually in this graph learning workshop",
      "offset": 1404.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "gave a really nice uh discussion of this",
      "offset": 1405.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "talks about you think that argmax is",
      "offset": 1408.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "going to save you from the overs",
      "offset": 1412.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "squashing problem. But the problem is",
      "offset": 1414.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the context length and context window",
      "offset": 1416.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "gets so large argmax actually doesn't",
      "offset": 1418.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "sharpen your distribution enough to save",
      "offset": 1421.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you and in fact doesn't matter what",
      "offset": 1423.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "temperature you put on the argmax. It's",
      "offset": 1425.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "simply not going to do it well enough.",
      "offset": 1427.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "This is where they sort of optimize",
      "offset": 1428.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "expander graphs instead of having you",
      "offset": 1430.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "know anything can attend to everything",
      "offset": 1432,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "before it they you like actually that's",
      "offset": 1433.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "too much let's spify this thing in such",
      "offset": 1436.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a way that reduces the overs squashing",
      "offset": 1439.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of the signal",
      "offset": 1441.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and like that's I think that's that's",
      "offset": 1442.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "like exceptional work because it it's",
      "offset": 1444.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "incredibly pragmatic. It diagnoses",
      "offset": 1447.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "exactly what the question is what the",
      "offset": 1449.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "business utility is is that these things",
      "offset": 1451.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "are going towards bigger context length.",
      "offset": 1452.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "How do you make bigger context length",
      "offset": 1454.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "work? and you actually just force it in",
      "offset": 1456.159,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "like a random way to attend to less,",
      "offset": 1459.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "right? We might think that you want",
      "offset": 1462.559,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "might want some like repeating pattern",
      "offset": 1463.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of how you cut things away. And instead,",
      "offset": 1465.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I think it's actually much more that you",
      "offset": 1467.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "want a very random one that doesn't",
      "offset": 1468.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "constrain the optimization problem. And",
      "offset": 1472,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "they were saying there there are some",
      "offset": 1474.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "advantages to having a specific like um",
      "offset": 1475.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you know to to to narrowing the cone of",
      "offset": 1479.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "attention because with reasoning you",
      "offset": 1481.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually want to have the correct answer",
      "offset": 1483.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and and sometimes you don't. I think the",
      "offset": 1485.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interest in them comes from the signal",
      "offset": 1487.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "processing world where they try and",
      "offset": 1489.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "figure out how do you have you know",
      "offset": 1491.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "graphs such that you know the heat",
      "offset": 1493.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "equation on them behaves well or not",
      "offset": 1495.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "actually the heat equation but sort of",
      "offset": 1497.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "message passing on them behaves well as",
      "offset": 1498.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "opposed to behaves poorly. And the",
      "offset": 1500.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "premise sort of the nice thing about",
      "offset": 1503.679,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "their work says okay well this is",
      "offset": 1505.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually the right perspective to have",
      "offset": 1507.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "about what is happening in you know the",
      "offset": 1508.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "sequence of updates along of of a",
      "offset": 1511.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "transformer yada yada yada. So you want",
      "offset": 1514.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to worry about specifically about this",
      "offset": 1516,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like signal processing perspective on",
      "offset": 1517.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the thing and so you you simply say well",
      "offset": 1518.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "these kinds of graphs that we know are",
      "offset": 1521.279,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "better for message passing than",
      "offset": 1524.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "attending to everything previously. you",
      "offset": 1526.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just substitute those in and that you",
      "offset": 1529.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "get better performance in the large",
      "offset": 1530.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "context window situation specifically",
      "offset": 1533.919,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "because you've removed over part of the",
      "offset": 1536.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "oversquashing problem by simply snipping",
      "offset": 1539.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "connections in a way that staggers",
      "offset": 1541.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "signal.",
      "offset": 1543.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "So Tim you've inspired a new title that",
      "offset": 1544.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I hope somebody writes one day which is",
      "offset": 1546.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "attention considered harmful like we",
      "offset": 1548.96,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "need",
      "offset": 1551.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "the critical difference between the",
      "offset": 1555.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "unified representations and the",
      "offset": 1557.6,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "fractured entangled ones is that the",
      "offset": 1561.36,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "unified ones um have kind of a sort of",
      "offset": 1564.64,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "coherence with the with um many more",
      "offset": 1567.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "representations in their set. So like",
      "offset": 1571.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "you can take you can take these two",
      "offset": 1573.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "unified representations and combine them",
      "offset": 1576.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and get something you know meaningful or",
      "offset": 1578.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "interesting or these two or these two",
      "offset": 1580.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like they're very composable like across",
      "offset": 1582.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the entire set you know uh like almost",
      "offset": 1585.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "pairwise whereas the whereas the",
      "offset": 1587.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "fractured ones like if I take this one I",
      "offset": 1589.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "might get something but this pair",
      "offset": 1591.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "produce garbage produce noise that sort",
      "offset": 1592.48,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "of thing.",
      "offset": 1594.799,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "Yeah. No. So yeah, exactly. This is what",
      "offset": 1595.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "I was what I'm saying is like it's",
      "offset": 1597.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "essentially like an optimal al like an",
      "offset": 1598.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "optimized algebra for constructing",
      "offset": 1600.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "images,",
      "offset": 1602.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "right? That's that's the story that I",
      "offset": 1604.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "think is right. You know, that like like",
      "offset": 1606.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "universal algebra from a category",
      "offset": 1608.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "theorist perspective kind of makes",
      "offset": 1609.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "things make sense is you've got a very",
      "offset": 1611.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "general vibe of what it means to have",
      "offset": 1613.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "like uh algebra for constructing things,",
      "offset": 1616,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "right? But you've got a very So it",
      "offset": 1619.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "becomes relatively easy to even say the",
      "offset": 1621.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "phrase. It's just like okay I'm",
      "offset": 1623.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "optimizing over a space of cominatorial",
      "offset": 1624.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "representations of things.",
      "offset": 1628.159,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "Mhm.",
      "offset": 1629.919,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "And like specifically I'm optimizing to",
      "offset": 1630.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "make it really easy to draw pictures of",
      "offset": 1632.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the kinds of things that I originally",
      "offset": 1635.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "say are important but in frankly like",
      "offset": 1638.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "what the things that people think are",
      "offset": 1640.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "important are exactly the kind of things",
      "offset": 1642.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that they find easy to draw.",
      "offset": 1643.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "something that's like fascinating that I",
      "offset": 1646.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "would love to connect this work with is",
      "offset": 1647.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so back when I was doing like complex",
      "offset": 1649.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "systems stuff I was reading a lot of",
      "offset": 1650.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like uh Stannis loss dehane so this will",
      "offset": 1653.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "date me or whatever so this was like",
      "offset": 1655.919,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "what 2000 no probably 2010 2011 this",
      "offset": 1657.36,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "kind of thing but uh Stannis",
      "offset": 1662.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the global workspace theory guy",
      "offset": 1664.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "he might be now I remember reading a",
      "offset": 1666.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "book about reading in the brain",
      "offset": 1668.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and it was about like what like how do",
      "offset": 1670.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the pre-existing structure of the brain,",
      "offset": 1674.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "how do we learn to read with them? And",
      "offset": 1676.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sort of that kind. So that was cool. And",
      "offset": 1678.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "then there's also like a work by like",
      "offset": 1680.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Changy in the '9s uh trying to sort of",
      "offset": 1682.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "see like what kinds of things are in",
      "offset": 1685.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "across writing systems of the world.",
      "offset": 1688.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "What kinds of intersections of lines or",
      "offset": 1690.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like bumping into of lines are allowed",
      "offset": 1692.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to be discriminatory between this symbol",
      "offset": 1694.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and this one versus which ones are not?",
      "offset": 1696.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "That seems very much like the same kind",
      "offset": 1699.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of question. And so the point is we have",
      "offset": 1701.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "evidence that writing systems have been",
      "offset": 1702.88,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "adapted to being distinguished only by",
      "offset": 1705.279,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "certain kinds of things and that the",
      "offset": 1708.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "rules for distinguishing things are like",
      "offset": 1711.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "quite common like it's unacceptable to",
      "offset": 1713.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "have this and this be different whereas",
      "offset": 1715.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "it's very acceptable to have this and th",
      "offset": 1719.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "this be different.",
      "offset": 1721.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "I like that story. I mean something else",
      "offset": 1722.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "you might add to it is that you know the",
      "offset": 1724.159,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "human being is not you know learning for",
      "offset": 1727.279,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the same task all the time right you're",
      "offset": 1729.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "alternating between your between things",
      "offset": 1732.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that you're thinking about a lot and",
      "offset": 1734.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "learning for. So I wonder if there's",
      "offset": 1737.679,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "something of that story in there too. Is",
      "offset": 1739.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "that actually part of the failure of",
      "offset": 1741.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "machine learning models as they are? Is",
      "offset": 1743.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "this legacy sensibility that they're for",
      "offset": 1747.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "one thing? Because we think about them",
      "offset": 1749.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like machines. And in fact, the",
      "offset": 1750.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "optimization process works better if",
      "offset": 1752.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "you're not thinking about it like it's",
      "offset": 1754.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "for one thing.",
      "offset": 1755.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "So, I was just going to I was just going",
      "offset": 1758,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to say this this really resonates with",
      "offset": 1759.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "me about uh so many of the problems that",
      "offset": 1761.12,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "um you work on definitely in in high",
      "offset": 1764,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "school and even undergraduate things",
      "offset": 1767.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "like that. They're really contrived.",
      "offset": 1769.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "It's like every every problem in your",
      "offset": 1771.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "math book has a solution. You know, you",
      "offset": 1773.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "get it right or wrong when you solve it.",
      "offset": 1776.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "And I got uh you know, it was a big",
      "offset": 1778,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "shocker to me because when I started",
      "offset": 1779.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "doing graduate work Okay.",
      "offset": 1781.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Yeah. literally the first interesting",
      "offset": 1782.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "physical system that I modeled and and",
      "offset": 1785.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "then went to try and solve like had no",
      "offset": 1789.039,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "closed form solutions, you know? Yeah.",
      "offset": 1790.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "And I'm just like, what the heck? I",
      "offset": 1793.279,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "mean, how come I all these years",
      "offset": 1794.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "everything had solutions and like now",
      "offset": 1796.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "all of a sudden my god,",
      "offset": 1798.799,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you know, like for example, you know,",
      "offset": 1799.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "I'll give you one specific example. Um,",
      "offset": 1802.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "I had this uh, you know, this series I",
      "offset": 1804.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "was trying to sum, okay? and I couldn't",
      "offset": 1807.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "figure out how to do it and Mathematica",
      "offset": 1809.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "couldn't figure out how to do it. And I",
      "offset": 1811.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "really didn't want to do it numerically.",
      "offset": 1813.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "So I I went to the library where they",
      "offset": 1814.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "had those those like Russian books, you",
      "offset": 1817.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "know, from like prior to I mean from",
      "offset": 1819.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "like World War II era or whatever when",
      "offset": 1821.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they didn't have a lot of computers. And",
      "offset": 1823.679,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 1825.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "yeah, just like books of sequences.",
      "offset": 1825.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Exactly. Like these massive tomes of",
      "offset": 1828.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "integral series sequences, all this kind",
      "offset": 1830.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "of stuff, right? So I'm flipping around",
      "offset": 1833.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "eventually in like to seven or",
      "offset": 1835.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "something. I find I find my my series.",
      "offset": 1837.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Okay. And it says,",
      "offset": 1839.919,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "\"Yeah,",
      "offset": 1841.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that's the Sterling series of the second",
      "offset": 1842.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "kind. Go to this other book on this page",
      "offset": 1844.64,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "for more details.\" Okay. Awesome. Yes.",
      "offset": 1847.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "So, I go get the other book. I flip to",
      "offset": 1851.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that page. The Sterling series of the",
      "offset": 1853.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "second kind is defined by the following",
      "offset": 1855.279,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "series. The end.",
      "offset": 1857.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Nothing. There's like no more. There's",
      "offset": 1860.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "just the definition of the series,",
      "offset": 1862.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "right? So I go, okay, well, I couldn't",
      "offset": 1864.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "solve it. Mathematica can't solve it.",
      "offset": 1867.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "The Russians from the 50s couldn't solve",
      "offset": 1870.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it. Like I that's hopeless. They can't I",
      "offset": 1872.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "can't solve it, you know? So the only",
      "offset": 1874.399,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thing I can do is just numerically",
      "offset": 1875.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "calculate, approximate, whatever. And",
      "offset": 1877.919,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "I'm like, how come we didn't spend more",
      "offset": 1879.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "time in any of my prior schooling",
      "offset": 1881.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "learning about these techniques that you",
      "offset": 1883.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "use when you don't have closed form",
      "offset": 1886.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "solutions? you know approximations,",
      "offset": 1889.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "numerical uh methods, uh apply like",
      "offset": 1891.6,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "applied mathematics basically, right?",
      "offset": 1894.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Yeah. No, I mean it's uh what is it",
      "offset": 1898.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "curriculum? That's that's the question",
      "offset": 1900.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "is like you give people the problems",
      "offset": 1902.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that they can solve such that when they",
      "offset": 1904.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "finally find all the problems they",
      "offset": 1906.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "can't, they're not like math is stupid.",
      "offset": 1909.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Instead, they're like math is just hard",
      "offset": 1911.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and I'm comfortable with that.",
      "offset": 1913.919,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "I see.",
      "offset": 1916.24,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "Right. That's that's that I think that's",
      "offset": 1916.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "really the reason why is you want to",
      "offset": 1918.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "give I mean",
      "offset": 1920.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you know if you're reinforced",
      "offset": 1922.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "if you're not think from a reinforcement",
      "offset": 1924.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "learning perspective why is theorem",
      "offset": 1926.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "proving hard you know why has no one",
      "offset": 1928.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "successfully made like an alpha zero",
      "offset": 1930.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "style theorem prover that's suddenly",
      "offset": 1932.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "better than humans it's well",
      "offset": 1934.559,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "any like alpha go alpha zero works for",
      "offset": 1937.76,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "go why because one version of you is",
      "offset": 1941.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "always winning",
      "offset": 1943.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "right but If you try and do the",
      "offset": 1946.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "analogous thing for proof,",
      "offset": 1947.44,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "you're almost always losing. So, you",
      "offset": 1950.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "have to get relatively good at even",
      "offset": 1953.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "thinking about the question before you",
      "offset": 1955.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "try and tackle the harder things because",
      "offset": 1957.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "otherwise you're just going to get",
      "offset": 1959.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "exclusively ne negative stimulus and get",
      "offset": 1960.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "scrambled.",
      "offset": 1962.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Beautiful.",
      "offset": 1964.799,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "No, and I really like that you bring up",
      "offset": 1966.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this this particular example of like all",
      "offset": 1967.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of the math problems I did as a young",
      "offset": 1969.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "person, they all had nice solutions. And",
      "offset": 1971.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it's this, you know, I remember walking",
      "offset": 1974,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "home from I it was either in I was in",
      "offset": 1976,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Vermont, it was in Burlington, it was my",
      "offset": 1979.039,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "master's program. I was walking home",
      "offset": 1980.72,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "from the bar from my with my friend",
      "offset": 1981.919,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "Michael Lambert who's now like done a",
      "offset": 1983.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "lot of really great um double category",
      "offset": 1984.72,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "work. Uh shout out to Michael. But um we",
      "offset": 1987.279,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "were walking home from the bar and I",
      "offset": 1991.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "think this is the first time I had",
      "offset": 1993.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "taught a class and I was talking to him",
      "offset": 1994.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "about just how easy it is to",
      "offset": 1997.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "make an unsolvable problem if you're",
      "offset": 1999.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "just like randomly generating problems",
      "offset": 2002.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "for your students. And he responds",
      "offset": 2003.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "cannon to the left of me and cannon to",
      "offset": 2006.799,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "the right.",
      "offset": 2009.039,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "The point is it's like no there's just",
      "offset": 2012.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like you're walking you are walking this",
      "offset": 2014.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "incredibly narrow path. You know, you as",
      "offset": 2016,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you as you work in mathematics, as you",
      "offset": 2018.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "become more and more expert, you",
      "offset": 2020.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "realize, oh, I thought I was learning",
      "offset": 2022,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "all of this stuff for the first time.",
      "offset": 2023.84,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "It's like, no, this is what everyone",
      "offset": 2025.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "already knows, has known for hundreds of",
      "offset": 2026.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "years. This is the walled garden where",
      "offset": 2028.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "everything is clean and like you learn",
      "offset": 2031.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "how to operate and be sophisticated in",
      "offset": 2034,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "this place. Then you go to the bigger",
      "offset": 2036.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "place where you can keep all of the",
      "offset": 2037.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "knowledge you have without without it",
      "offset": 2039.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "totally getting lost, right? Um, one of",
      "offset": 2041.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "the most one of the like I'm not a an",
      "offset": 2044.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "expert on Pavlov's work or anything like",
      "offset": 2047.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "this, but I love this folksy version of",
      "offset": 2049.679,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the definition of Pavlov's anxiet or",
      "offset": 2051.679,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Pavlovian anxiety when the subject does",
      "offset": 2053.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "not know whether they will be rewarded",
      "offset": 2055.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "or punished for a given action.",
      "offset": 2057.44,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2059.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "Right. It's exactly that is that's the",
      "offset": 2060.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "problem. Like as an educator, you can't",
      "offset": 2062.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "teach students when they don't know when",
      "offset": 2064.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "they have no sense of whether the next",
      "offset": 2067.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thing they do is going to get them",
      "offset": 2069.599,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "punished or not, right? they have no",
      "offset": 2070.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "idea. That's when they can't learn. And",
      "offset": 2072.879,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that's when you have to back up until",
      "offset": 2074.72,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "they are until they know enough such",
      "offset": 2076.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that they have a really good guess for",
      "offset": 2078.879,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "whether they're going to be punished or",
      "offset": 2080.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "not. Right? You that's why you have to",
      "offset": 2082.32,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "that's why you have to go back to where",
      "offset": 2084.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "where it begins before things go off the",
      "offset": 2085.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "rails where people are trying to learn",
      "offset": 2087.599,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "things that they're not ready to learn.",
      "offset": 2089.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "I suspect that the same thing is true",
      "offset": 2090.879,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "for models. I was just going to say um I",
      "offset": 2092.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "really appreciate um when you when you",
      "offset": 2095.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "said you you finally learned what",
      "offset": 2097.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "category theory is because I know I",
      "offset": 2099.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "don't know like what what category",
      "offset": 2101.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "theory is but I think um I think both",
      "offset": 2103.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Tim and I certainly me I'm very",
      "offset": 2105.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "fascinated by it and uh I always I",
      "offset": 2107.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "always think it it has to have some",
      "offset": 2109.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "extremely valuable insights you know to",
      "offset": 2111.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "offer for AI and machine learning. So,",
      "offset": 2114,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's always a joy to talk to you because",
      "offset": 2116.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "I'd love to know, you know, what you're",
      "offset": 2118,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "up to lately and what you're thinking",
      "offset": 2119.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "about the application of category theory",
      "offset": 2121.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "to, you know, our field. Really,",
      "offset": 2124.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "realistically, what most of what I've",
      "offset": 2126.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "been doing for the last year and a half",
      "offset": 2127.599,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "is not so much uh",
      "offset": 2129.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "figuring out how to apply category",
      "offset": 2134,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "theory, but honestly trying to catch up",
      "offset": 2135.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to what the abiding sensibilities, folk",
      "offset": 2137.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "wisdom, and accumulated knowledge of",
      "offset": 2140.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "machine learning is. you know I come",
      "offset": 2142,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "came from radically outside the",
      "offset": 2144.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "discipline and I really saw it as the",
      "offset": 2146.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "challen like you know so I you know",
      "offset": 2148.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "along with Ganovich and you know Dudzik",
      "offset": 2150,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "and Vonain and Arojo you know we wrote",
      "offset": 2153.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the position paper that was sort of a",
      "offset": 2156.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "moderately big deal last year that's",
      "offset": 2158.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "sort of how we got introduced um last",
      "offset": 2160.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "year we wrote that but at the time I was",
      "offset": 2163.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "like still very much at like a straight",
      "offset": 2164.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "ahead category theorist just trying to",
      "offset": 2166.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "come to grips with what uh machine",
      "offset": 2168,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "learning is actually doing just sort of",
      "offset": 2170.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "really just playing around and sort of",
      "offset": 2172,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "trying to understand what it means to",
      "offset": 2173.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "have you know categories of parametric",
      "offset": 2174.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "functions i.e models",
      "offset": 2176.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and how to sort of just begin to work",
      "offset": 2179.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and think with those but really I've put",
      "offset": 2181.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "most of the last year and a half into I",
      "offset": 2182.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "mean not really year and a half more",
      "offset": 2185.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like year into just trying to come up to",
      "offset": 2187.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "speed as fast as possible with you know",
      "offset": 2190.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the accumulated wisdom of machine",
      "offset": 2192.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "learning folks.",
      "offset": 2194.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "So that that's fascinating. Can you",
      "offset": 2195.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "share with us I'm curious what some of",
      "offset": 2197.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the top few things um you learned that",
      "offset": 2199.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "surprised you are.",
      "offset": 2202.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "The thing that say so that working a",
      "offset": 2203.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "little bit on automated theorem proving",
      "offset": 2206.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "has taught me and just really thinking",
      "offset": 2208.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "about it and going to a couple",
      "offset": 2210.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "conferences talking to people the thing",
      "offset": 2211.359,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "that's really thought about is like um I",
      "offset": 2212.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "find a renewed appreciation for my intu",
      "offset": 2216.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "intuitionist sensibilities. You talk to",
      "offset": 2219.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "a lot of people who are interested in",
      "offset": 2222.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "things like structure such as I am and",
      "offset": 2225.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "there's this abiding presumption that if",
      "offset": 2227.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "you only trained on that one kind of",
      "offset": 2229.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "structure things would perform better",
      "offset": 2231.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "whereas we know experimentally that they",
      "offset": 2233.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "perform worse right we know that if you",
      "offset": 2235.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "train a code model exclusively on one",
      "offset": 2238.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "language it is almost guaranteed to",
      "offset": 2240.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "perform worse than if you train one on",
      "offset": 2242.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "various languages exactly why is unclear",
      "offset": 2244.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "but you know it's a completely",
      "offset": 2248.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "reasonable hypothesis is to assume that",
      "offset": 2249.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there's something about like higher",
      "offset": 2251.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "order regularities and sort of trimming",
      "offset": 2253.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "off the specificity the you know",
      "offset": 2255.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "probabilistic accumulation of a couple",
      "offset": 2258.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of syntactic things that are confused by",
      "offset": 2260.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "for semantics by the model those are",
      "offset": 2262.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "stripped away by operating in various",
      "offset": 2264.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "different languages. So instead the",
      "offset": 2266.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "latent representation corresponds to",
      "offset": 2267.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "something closer to like a higher level",
      "offset": 2270.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "description of what's going on in code",
      "offset": 2272.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "than you know just the a",
      "offset": 2274.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "there has to be a connection there there",
      "offset": 2276.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "has to be a connection here to category",
      "offset": 2278,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "theory help me help me see it",
      "offset": 2279.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "one thing that I would say is training",
      "offset": 2281.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "on multiple languages is essentially the",
      "offset": 2282.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "same is an analog to what you do to",
      "offset": 2284.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "train vision transformers right where",
      "offset": 2287.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you know vision transformers are not",
      "offset": 2290.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "explicitly equivariant but what you do",
      "offset": 2291.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "is you do this operation of data",
      "offset": 2293.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "saturation you just like Well, I'm just",
      "offset": 2295.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "going to soup up my data set with every",
      "offset": 2297.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "single rotation translation and",
      "offset": 2299.2,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "transformation that I want to be",
      "offset": 2300.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "equariant with respect to. You know, I",
      "offset": 2301.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "modify the data set and then I train the",
      "offset": 2303.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "transformer on it. And that's where you",
      "offset": 2306,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "get this like transformers can be",
      "offset": 2307.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "trained to be more equariant than actual",
      "offset": 2309.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "convolutional neural networks. Why is",
      "offset": 2311.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "well it's kind of disingenuous because",
      "offset": 2313.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "what you actually did is you know you",
      "offset": 2315.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "boosted the scale of the data set and",
      "offset": 2317.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "you forced the data set to have um a",
      "offset": 2319.839,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "distribution level version of the",
      "offset": 2323.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "equariance story in it",
      "offset": 2326.8,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 2329.2,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "and then you're just leveraging the fact",
      "offset": 2329.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "that the transformer is particularly",
      "offset": 2330.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "good at sequence prediction",
      "offset": 2332.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you know so that that kind of thing so",
      "offset": 2335.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that's the story so the analog here as I",
      "offset": 2336.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "would say is training on multiple",
      "offset": 2338.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "languages is very much analogous to um",
      "offset": 2339.76,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "saturating a data set with respect to a",
      "offset": 2345.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "group action,",
      "offset": 2347.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? And so, and let me ask you this",
      "offset": 2348.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "because and again like folks out there,",
      "offset": 2350.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "please understand, I'm a complete newbie",
      "offset": 2353.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "with category theory. So, I'm trying to",
      "offset": 2355.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "learn more about it. Wish I knew more",
      "offset": 2357.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "about it. So, if my questions are",
      "offset": 2359.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "totally off the mark, feel free to uh",
      "offset": 2360.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "feel free to correct them. But I I've",
      "offset": 2362.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "been hoping or thinking that category",
      "offset": 2365.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "theory should help us to enforce in the",
      "offset": 2367.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in the structure of neural networks",
      "offset": 2371.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "certain equivariances",
      "offset": 2372.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "um that we that we know should be there.",
      "offset": 2375.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "So for example, if we know there should",
      "offset": 2377.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "be a rotation of variance or some other",
      "offset": 2379.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "aphine transformation, you know, is",
      "offset": 2381.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "there a way to structure the neural",
      "offset": 2383.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "network where that's kind of built in as",
      "offset": 2384.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "an inductive bias? Yeah, I mean this is",
      "offset": 2386.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "I mean it's well actually as a as a",
      "offset": 2389.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "formal constraint not merely like an",
      "offset": 2391.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "inductive bias where something",
      "offset": 2392.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "preferentially finds solution that have",
      "offset": 2393.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that property you can completely",
      "offset": 2395.599,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "restrict it to have that property.",
      "offset": 2396.88,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 2398.56,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "Right. And we've done that like and",
      "offset": 2398.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "that's what you know that's what",
      "offset": 2400.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "geometric deep learning is about and",
      "offset": 2401.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "indeed you know the content of the paper",
      "offset": 2403.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "where we made a subtle mistake that I",
      "offset": 2405.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "can sort of explain you know over the",
      "offset": 2407.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "next 20 minutes or whatever. uh that's",
      "offset": 2408.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you know RNN's are an example of this",
      "offset": 2411.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "right is that you've got a formal",
      "offset": 2413.359,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "constraint which is that you're",
      "offset": 2414.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "repeating exactly the same unit again",
      "offset": 2415.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and again so the the the point being is",
      "offset": 2417.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "that the weight time that corresponds to",
      "offset": 2419.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "convolutional neural networks is exactly",
      "offset": 2421.119,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "the same kind of weight time that",
      "offset": 2422.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "corresponds to recursion in RNN's and",
      "offset": 2424.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "all of it is algebra for monads right",
      "offset": 2428.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that was sort of the story that we told",
      "offset": 2430,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "in the paper the thing that we made a",
      "offset": 2431.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "mistake about in the paper largely",
      "offset": 2433.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "because you know I'm a pure",
      "offset": 2435.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "mathematician I'm used to having a like",
      "offset": 2436.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "a long time to think about something and",
      "offset": 2438.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "machine learning publications are more",
      "offset": 2441.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like you lock yourself in a room for",
      "offset": 2443.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "three weeks and you hail Mary. Um",
      "offset": 2445.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 2449.28,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "remind us about the paper.",
      "offset": 2449.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Oh yeah. No, so the paper is this paper",
      "offset": 2451.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "called position categorical deep",
      "offset": 2453.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "learning is a theory of all",
      "offset": 2455.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "architectures. Uh I confess I objected",
      "offset": 2456.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "to the title being so hubristic in a",
      "offset": 2460,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "sense of theory of all architectures but",
      "offset": 2462.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the editors uh were like no this is not",
      "offset": 2464.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "forceful enough. you need to you need to",
      "offset": 2467.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "pick something that's sufficiently",
      "offset": 2468.56,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "bombastic and it's like okay I guess",
      "offset": 2469.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it's the standard of the discipline I'm",
      "offset": 2471.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "moving to a stochastic universe I can",
      "offset": 2473.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "accept that maybe it's a bit bit",
      "offset": 2475.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "overblown but yeah and so in that paper",
      "offset": 2477.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "what we talked about is how you know the",
      "offset": 2480.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "notion of monads the notion of algebbras",
      "offset": 2483.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "therefore and demonstrating that",
      "offset": 2486,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "convolutional layers are",
      "offset": 2488,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "morphisms of algebbras for monads and",
      "offset": 2491.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "likewise rnn's can be seen as morphisms",
      "offset": 2494,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "of algebra for monads. The fact that",
      "offset": 2496.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "those monads are actually the free",
      "offset": 2499.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "monads on polinomial endo functors we",
      "offset": 2500.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "made a little bit of big deal about you",
      "offset": 2503.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "know yada yada yada that kind of thing",
      "offset": 2504.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but turns out that's not the easiest",
      "offset": 2506.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "story and indeed that's where some of",
      "offset": 2509.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the mis the subtle mistake came from. I",
      "offset": 2510.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "think back at the end also through the",
      "offset": 2512.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "editing process we claimed oh these",
      "offset": 2514.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "things are uh lax algebbras for these",
      "offset": 2516.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "things or or like you know lax natural",
      "offset": 2519.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "transformations between these structures",
      "offset": 2522.319,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "and I think that's wrong you actually",
      "offset": 2523.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "don't need laxness at all the illusion",
      "offset": 2525.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of needing laxness is because that the",
      "offset": 2527.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "paraconstruction is something I I don't",
      "offset": 2529.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "put much faith in anymore I think it's a",
      "offset": 2532.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "it's a accidental rigidity and instead",
      "offset": 2535.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you know some work that people at",
      "offset": 2538.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "symbolica solo clingman Mitch Buckley",
      "offset": 2539.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and Talier and Bayon have done. you",
      "offset": 2542.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "know, they sort of they filled in the",
      "offset": 2544.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "funal semantic story for inductive",
      "offset": 2546.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "families in a neat way that made it",
      "offset": 2549.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "clear that RNN's just fall out in",
      "offset": 2551.119,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "exactly the same way as you know",
      "offset": 2553.68,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "functors on BG where BG is the",
      "offset": 2558.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "classifying category of a group valued",
      "offset": 2560.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in so funers",
      "offset": 2562.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "valued in set from BG where BG is the",
      "offset": 2564.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "classifying category of a group that",
      "offset": 2567.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "kind of story gives you sets together",
      "offset": 2569.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "with a group action similar story where",
      "offset": 2571.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "some flavor of funure valued in some",
      "offset": 2574.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "other category gives you things like",
      "offset": 2576.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "RNN's.",
      "offset": 2578.56,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 2580.56,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "Yeah. So that's not the that's not the",
      "offset": 2580.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "best description, but the point is we",
      "offset": 2582.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "made a subtle claim and you actually",
      "offset": 2583.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "don't need laxmness at all. And it's",
      "offset": 2585.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "actually a much more elementary story.",
      "offset": 2587.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "And the reason why it was hard at the",
      "offset": 2588.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "time of writing the paper is because we",
      "offset": 2591.04,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "were working with this para",
      "offset": 2592.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "construction. And in particular, it",
      "offset": 2593.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "makes these things called co-limits not",
      "offset": 2595.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "work or not necessarily work very well.",
      "offset": 2597.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "And co-limits are the sort of general",
      "offset": 2600.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "theory of gluing things together. And as",
      "offset": 2602.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you can believe, gluing things together",
      "offset": 2605.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "is hard. And in particular, it's very",
      "offset": 2606.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "hard if the uh presentation of things",
      "offset": 2608.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you've got to work with is overly",
      "offset": 2610.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "complicated. And I think that's what was",
      "offset": 2612.16,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "wrong with PAR,",
      "offset": 2613.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "right? What are you most excited about",
      "offset": 2615.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and interested in? So, you know, maybe",
      "offset": 2617.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "what you're working on right now or what",
      "offset": 2619.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you see in the field happening that",
      "offset": 2620.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "you're really excited about um promising",
      "offset": 2622.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "for the future, that type of thing.",
      "offset": 2625.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "Oh, the thing that I'm just really",
      "offset": 2627.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "enjoying right now is, you know, my new",
      "offset": 2628.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "my new feeling about what am I doing?",
      "offset": 2631.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Like, what am I doing as a machine",
      "offset": 2633.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "learning researcher? What are we doing",
      "offset": 2634.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "with these systems overall is the",
      "offset": 2636.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "following sensibility. The thing that is",
      "offset": 2638.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the most compelling is that every single",
      "offset": 2640.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "story in machine learning is something",
      "offset": 2642.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "about stochastic processes. And what we",
      "offset": 2644.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "are doing is we are you know when neural",
      "offset": 2645.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "what neural networks are is they are an",
      "offset": 2650.16,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "algebra for constructing a machine whose",
      "offset": 2651.92,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "or essentially essentially like a fake",
      "offset": 2657.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "physics whose energy relaxation process",
      "offset": 2659.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "computes the distribution of data we fed",
      "offset": 2663.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "it like and just fe like that that",
      "offset": 2665.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "feeling that's that's that's what is",
      "offset": 2667.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "what is",
      "offset": 2669.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that's the feeling that's the vague",
      "offset": 2671.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "sensibility I have that I'm most excited",
      "offset": 2672.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "about",
      "offset": 2674.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you know like I have you know I come",
      "offset": 2676,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "from this world where I've got sort of",
      "offset": 2677.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "like all of this formalism now I'm",
      "offset": 2678.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "really trying to sort of just dig deep",
      "offset": 2680.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "into just like no everything is",
      "offset": 2681.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "probabilistic here what does that mean",
      "offset": 2683.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "what does it mean to take that as",
      "offset": 2684.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "seriously as possible what does it take",
      "offset": 2686.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "what does it mean to take both diffusion",
      "offset": 2688.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "process as an SGD all of these things",
      "offset": 2690.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "are stochastic processes like all of",
      "offset": 2692.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "these things are like there's this is",
      "offset": 2694.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "really a statistical physics story at",
      "offset": 2696.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "some level and sort of just trying to",
      "offset": 2698.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like accept that as like the most",
      "offset": 2700.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "radical premise and just see Like what",
      "offset": 2702.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we are doing is designing incredibly",
      "offset": 2705.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "complicated versions of like the single",
      "offset": 2707.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "electron of the hydrogen atom and",
      "offset": 2710.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "bombarding it in some particular context",
      "offset": 2711.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and figuring out what distribution of",
      "offset": 2713.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "energy states that single electron is",
      "offset": 2715.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "in. It's analogous to that except it's",
      "offset": 2717.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "an extraordinarily complicated system",
      "offset": 2719.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "but nonetheless the training process is",
      "offset": 2721.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "exactly that bombarding it with you know",
      "offset": 2723.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some amount of energy in various forms",
      "offset": 2726.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then seeing what the distribution of",
      "offset": 2728.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "stuff is at the end.",
      "offset": 2729.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Like so really thinking about like the",
      "offset": 2732.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "task of machine learning and the sort of",
      "offset": 2734.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "task of a machine learning researcher if",
      "offset": 2735.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you're interested in foundational",
      "offset": 2737.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "questions of how this works is figuring",
      "offset": 2738.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "out what an efficient language for",
      "offset": 2740.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "specifying",
      "offset": 2743.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "uh parametric families of models such",
      "offset": 2745.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that these stoastic processes like",
      "offset": 2747.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "compute quickly and come up with good",
      "offset": 2749.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "stories like that that's the that's sort",
      "offset": 2752.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of the vague sensibility I have that I'm",
      "offset": 2755.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "finding exciting now.",
      "offset": 2756.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "No thank you. I mean, I think um on that",
      "offset": 2758.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "on that beautiful and intriguing note, I",
      "offset": 2761.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "think we'll leave our uh leave our",
      "offset": 2763.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "listeners to ponder that and I would",
      "offset": 2765.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "encourage them encourage them to do so.",
      "offset": 2767.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So, listen, Paul, it's been an absolute",
      "offset": 2769.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pleasure um talking with you again today",
      "offset": 2771.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and I really appreciate you uh taking",
      "offset": 2773.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the time to hang out with us.",
      "offset": 2776.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "No, indeed. It's been great. It's good",
      "offset": 2778.24,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "to see you again. It's good to see both",
      "offset": 2779.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of you again. Wonderful. Thank you so",
      "offset": 2781.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "much for coming, Opo. I really",
      "offset": 2782.8,
      "duration": 1.519
    },
    {
      "lang": "en",
      "text": "appreciate it.",
      "offset": 2783.839,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "Yeah, absolutely.",
      "offset": 2784.319,
      "duration": 3.321
    }
  ],
  "cleanText": "In pure mathematics, we have an incredibly brutal aesthetic/formal judgment for when something's true, right? We've got definition and proof. But here is a context where you don't have that. So instead, you have to concoct something that, you know, will serve a similar role. And Hart argues that this is what benchmarks have been for and what benchmarks have been succeeding for until relatively recently.\n\nThese deep learning models, as good as they are, they build sand castles. And what is a sand castle? It is a structure with too many degrees of freedom. You prod the sand castle and it just collapses because there's no undergirling structure in it.\n\nOkay, but so here's where I think that that might not be exactly true.\n\nWell, is it possible to reconcile constructivism and platonism? Right. So many folks really believe that there is actually some generating function of the universe that it appears constructive to us. To us it appears that there's a kaleidoscope, that there are pockets of regularities, but there's nothing that binds it together, but we like to imagine that underneath, you know, behind the illusion of teology, right?\n\nYeah, well, it's a bit of a Plato's cave type thing, that you know, actually, if only, you know, we see the shadows, but actually there is something there which is universal.\n\nI suspect that the world is in this sense is like fundamentally constructive, but it has enough regularities and like the processes in it, like the reason why they work, is because they are capable of creating the illusion of an ideal.\n\nLike I think that's why stuff works is that it's contingent, like constructive stuff that is sufficiently regular such that like there's a pro, like I think there's like a constructive process, probably like a constructive process of idealization, right? That goes on. Like I think people in like the, you know, theoretical biology world in the 70s were really into like what the hell is a metabolism and like thinking about that as kind of a predict, like an anticipatory process. Like a metabolism is this chemical cycle that anticipates that stuff is going to keep coming in to run it, and so it's actually running before it gets the thing, right? So it's that kind of thing. So I think my, my vague, you know, like, I just have a feeling kind of feeling about this would be,\n\nno, I'm defin, I don't think that there is a platonic thing here. I think that's wrong. But I think that developing the illusions of platonism is how structure happens in the world.\n\nBut this um metabolism metaphor I think is beautiful because there are so many examples like that. You know, like the brain is a prediction machine or the the information metaphor is a wonderful one. Biologists talk about, you know, information as if it has primacy and and explanatory power. And this is clearly a limitation of our cognitive horizon. Right? So what we do is we we view things locally and we come up with these metaphors, these abstractions, and and we understand these abstractions because they're quite microscopic. But it feels like there's there's something more to it than that. If only we could have a more global understanding. Idealization in this sense is kind and like therefore, you know, like the the plausibility of platonism is like an excellent problem-solving strategy.\n\nIs like to the invent the thing that's not there, you know, like it's concepts like fitness in biology, right? I think it's what like I think the the right, you know, fancy words are apostori or something along those lines, right? Fitness doesn't exist, but let's imagine that it is and then if we reason about it as if fitness exists, right? It makes the story easier. It closes things down. It makes it such that there's a a quantity. You can imagine there are being a quantity associated to that notion that happens. Like it doesn't actually exist in the world. But you know, and it's this is not even like it's you know, not even even worse than like quantum mechanics is kind of like this, but they're act, we sort of do at this point kind of like believe there's some like ontological reality to the distribution, not merely to the sampling, but for other physical processes that are totally macroscopic. Like I don't believe that there's like that the distribution for uh, you know, a species is like essentially real. It's like no, the sampling is real, but the abstraction of a distribution, like things act as if there were such a thing. There isn't, which makes it sort of,\n\nI mean, this is where the plaintists have a point, right? It's like, well, maybe, maybe like what do you mean how real does it have to be to be real, man? And like maybe that just like, well, it, you know, walks like a duck, why, why beat around the bush? Why just, why not just imagine that there is such a thing? I have no idea, you know, what is the right answer. I think the like, honestly, my sensibility with this kind of stuff is like, you want to know and think about it enough to be electrified by it, but not menaced by it.\n\nI spoke with Maxwell Ramstead a couple of weeks ago and he he quoted a philosopher, I forget the name, but he said reality pushes back.\n\nAnd is there something you think about as a category theorist? I mean, what does it mean for reality to push back when you come up with a model? As someone trained to some extent in category theory and then spending the last year and a half working in machine learning.\n\nUm, the way it pushes back is, I don't know how much of the story, how useful it's going to be in the long run. I don't, I think it brings a bunch of really powerful sensibilities, but the naive applications are likely wrong. Um, for the same reason that I think uh, abiding obsessions with architecture are likely wrong. I think that, you know, it could be a very useful tool for indexing the experiments that essentially find the boundaries between performant and non-performant architectures, and that's the interesting question.\n\nRight, the sensibility I've got, like, so I can't answer the question you you you gave me precisely. Instead, I can sort of say how do I feel about the whole thing now and what's the way that I'm imagining things is, you know, what neural networks are, at least as far as they are like, you know, modeled and machines or whatever. Not talking about actual brains or anything. They are an algebra for constructing parametric models such that uh, a physics, you know, like a stochastic process like SG, like will cause it to relax to some lowest energy state and the parameters associated to the that, you know, there may be many choices of parameters associated to that lower energy state, but the model at that lowest energy state, that's the thing that computes sort of gen, is capable of approximating the data you fed into it, right? And so what's the role for category theory in such a story is, well, it makes it relatively easy to have the idea of what do you, what do you mean by an algebra for constructing these systems? Like that's where it's useful, right? Is this, you know, because you can imagine it's like, well, I've got various different kinds of algebra. I've got, you know, wiring diagrams out, that's one kind of algebra, you know, I've got, you know, standard algebra, I've got that, you know, I've got various kinds of fl, like various other categorical flavors of what is meant by algebra. It's like sort of it gives you access to making it easy to formalize these things, right? And say what it means to like, how are you in what ways are you allowed to build things, right? Those are all algebras to a category theorist or a co-algebra, co-algebbras if people are careful enough, you know, but morally speaking, this falls all falls under like what a category theorist would call algebra.\n\nAnd so like that's the interesting theor thing here, I think. Like that's what we should be trying to use the category theory story for or the tools from it for is making it easy to index across a lot of different things to then start discovering the performance of those and sort of running those experiments. You know, like most of my last year and a half of work has not been um supremely categorical. It's instead sort of been like no, I would like to be humble. I humbled, I would like to learn the accumulated wisdom of, you know, like the ML world, you know, this is very much, you know, a book that I've started reading and that I think is at least for me is likely to be one of the most important books in my work as like a machine learning researcher, and this is uh, Moritz Hart's new um, emerging science of benchmarks.\n\nAnd in particular, the best thing is because, you know, this is the, this is so, I'll say this is the book that if I really felt like I still had the energy and like the real drive to truly understand not merely a scientific question, but like the political and philosophical contingencies from which it arises, like all of those this and then and I wanted to do that story about machine learning, I think this is the book that I would imagine I would want to write, but you know, the first chapter says, you know, machine learning is essentially uh, a synthesis of Fiora Ben's uh, notion of anything goes, right? So, Farend, whether I'm pronouncing the name right or wrong is immaterial, you know, contemporary of and Popper and that sort of like mid 20th century, you know, big deal about, okay, what is the scientific method? It's definitely not what we idealized it to be in the late 1800s, you know, with Compton, like those folks, it's something else, what is it, you know, and comes up with this notion of, you know, paradigms and revolutions and all of these things, a bunch of people who are all operating on like superficially compatible metaphors, but actually having deeply intuitive and kind of wild sensibilities about what's going on. But the only thing that allows you to judge what science looks like is seriousness. And then he describes what like the character of that notion of seriousness is. But sort of you can describe this as like how science really works is, well, actually anything goes. And in some sense, machine learning has taken that mantle up seriously. It's like I don't know what like the right aesthetic sensibility to have about what a good enough description for machine learning is. I don't think anybody does. And indeed people are like, actually, that's the wrong question. We just don't know enough to even try that. And so in this sense, anything goes. But then you also need, well, well, how do you estab, if you have no aesthetic judgment for when something is true, right? I'll say this that, you know, um, in pure mathematics, we have an incredibly brutal like aesthetic/formal judgment for when something's true, right? We've got definition and proof, but here is a context where you don't have that. So instead, you have to concoct something that, you know, will, you know, serve a similar role, and Hart's Hart argues that this is what benchmarks have been for and what benchmarks have been seeding, succeeding for until relatively recently. You know, the example is, well, you take, you know, one particular family of models, all for similar task, and you take a bunch of benchmarks for that same kind of task, and while the actual numbers you get in terms of performance are different, the ranking is was preserved across all of those benchmarks, but that's not the case anymore, right? And so this is either indication that uh, you know, like all of the easy stuff with models is gone and therefore we simply shouldn't expect there to be a preservation of relative capability across benchmarks, and that's just like the wrong thing to think about. Either that or the entire approach of everything is benchmarks is broken. I haven't read deep enough into the book, but I suspect Hart's premise is our heart is going to go for, I think only going on benchmarks is broken. And we need to sort of establish at least higher order aesthetic sensibilities of what an adequate argument for why something works is.\n\nLike that's the kind of place where I, I'm hoping to like be operating in this space. Like the thing that you know, I've learned over the last year and a half, the thing that motivates me is telling a story about why that I believe, right? Not merely telling a compelling story about why, but like I just wanted to like, I, I've tried to be motivated by lots of different things. You know, I'm in a business research environment and so I can, I try to get really excited about what this thing would do for our market position, all of that stuff. I can't maintain the excitement. The thing that I can maintain excitement for is providing an elegant and intuitive description of why something works. Like that's the interesting thing. And so that's kind of the thing that uh, the part of the sort of new era of machine learning that I think we have to go to, that I want to like be part of is like m, like figuring out what even is an adequate explanation here.\n\nYeah. I mean, this comes back to um, when I spoke with Chsky, he was, he was joking with us. He said, you know, I, I've got a scientific theory and it's anything goes.\n\nYeah. Yeah, you know, because he said machine learning, you know, it's, it's, it's like a bulldozer. It's great for clearing the snow, but it's not a theory of science. And and when I spoke with Dennit, you know, I was I was asking him, he was a philosopher of science. I said, \"What, what is science all about?\" And he said, \"It's creating explanations. You know, it's, it's generating the wise. The wise are important.\" And these these deep learning models, as good as they are, they build sand castles. And what is a sand castle? It is it is a a structure with too many degrees of freedom. You you prod the sand castle and it just collapses because there's no undergirling structure. And it it's not even that there's no undergirling structure. The problem is there is every structure, right? Any possible structure is represented in the sand and humans can can guide these models to make them behave as if they have structure. But actually having every structure is the same as having no structure.\n\nOkay. But so here's where I think that that might not be exactly true, right? And so I'm not uh, this is again a domain of the machine learning world in which I am not yet an expert, very much a neoight, but so I was just at a structural AI conference at in Shanghai, hosted by the Shanghai Chi Institute, but he gave a talk sort of providing a statistical learning theory analysis of why chain of thought works and in particular, I was so compelled by the story because first of all, just in terms of what kind of thing it,\n\nHe didn't give me or he didn't give the audience a humanist or an anthropomorphized explanation for this that says it's like, oh well, we think more clearly, some kind of you know, bogus appeal to system 2. I don't believe in system two. I think we talked about that last time. I think it's garbage.\n\nBut we'll get to that later.\n\nYeah.\n\nBut he was like, actually, I think that chain of thought works for the same reason that transformers work for natural language corpora, which is the following. Um, transformers beat the worst case predictions for how long it is going to take a particular model after a process of updating to model a distribution of sequences. It beats that time by a lot on natural language data. Why is because\n\n\nWith relatively high probability, you can predict the next thing in a sequence of natural language data from relatively few things that precede it, right? So the sparse dependency that is a character of natural language, transformers are adapted to that to quickly identifying it, and that's why they beat, you know, the worst-case predictions for how long it would take a model trained in this fashion to learn the distribution. Mhm.\n\nHe says that, you know, and you know, interview him, he might disagree with my characterization of his talk. That's entirely likely, but uh, he then says, I think this is why chain of thought works, which is because chain of thought for reasoning similarly sparsifies the dependency of the formal, like, of the mathematical statement that you're trying to make. Each little thing, instead of having the final answer depend on all of the things that had to be brought...\n\nSo you have each one sub-computations done separately. They're more secure. They're more certain, etc. So it sparsifies the dependencies to do the computation. But I was profoundly dissatisfied at the time with the epistemology of sort of complex systems work. I figured I needed to go do something like harder and more certain. And so I was like, I've got an idea. I'll go become an algebra geometer. Um, that didn't entirely work out, largely because this incredible thing called homotopy type theory happened during my mathematics PhD.\n\nI eventually finished up doing a PhD really in sort of combinatorial models of stable homotopic theory, which is the notion of homotopic theory that corresponds to the idea that instead of having dimensions valued in the natural numbers, instead, it's valued in the integers. You know, academia has a long history of provocative, provocative titles to papers, you know, so I wouldn't... and it seems like maybe clickbait culture is kind of uh infiltrating into paper titles um even more. I mean, like a classic example is, is, you know, \"goto considered harmful\" by Dijkstra, right? That was back in like '68. But you got fun things like that.\n\nYeah. Yeah. No, I think that the world of pure mathematics is um significantly more... I, one could either call it straight-laced or paranoid, paranoid that anyone will ever uh say something wrong. And so the fear of saying something wrong or overstating your case is so intense that people have these astoundingly boring titles for like incredible results. And so that's really the difference is, you know, you got these amazing titles that really clear sound like you're going to drop a bomb on this thing. And the reality is it's like this is an incremental improvement that might in fact be wrong, and you've just asserted like grand authority with the title, right?\n\nYou know, so this is just part of the learning process of moving to a new discipline, and you know, you come in, of course, with the the the ideas like, oh, these people are clearly doing it wrong, and then within 6 months, you're like, okay, I kind of understand 90% of this. I think you're doing 10% of it wrong, and then after another year, you're like, mostly you're doing it right, but you could slow down here, here, and here, and maybe that's where some stuff can come in.\n\nYeah, most of our viewers could understand intuitively that something like an RNN, right, is applying the same, the literal same calculation over and over and over again in sequence. And with CNN's, you know, it's almost like you got to think of it as a flashlight just being shined over the kind of XY, you know, axes of an image. That's sort of the the little convolution kernel there. So those types of um formal constraints are simple to to see and imagine, and we could think about, you know, kind of building them into the code. But what about, I mean, just things that are much harder for for humans to constrain that way? I mean, what chance is there of really, let's say, programmatically automatically discovering, so having a set of data and then statistically seeing, you know, what there seems to be something that should be a formal constraint, and then kind of factoring that out and then enforcing it as a formal constraint? Like, do you think category theory offers tooling there?\n\nI mean, it gives you a nice way to phrase the problem. I don't in general know how to do it, but I think it's a good question. Um, you know, the analogy I might make here is, in a very handwavy sense that I don't know how to formalize, but I'm sure more sophisticated members of the machine learning community can do a better job about this than I. What should you think of a causally masked transformer doing? What does it really do? Is it's a pro... it's doing a parallelized approximation only n layers deep of an RNN, right? That's really what you're doing.\n\nThat's an interesting... Has that analogy or that correspondence been pointed out before? I like it. I just saw this one.\n\nI... I don't know if it's been pointed out, but my inspiration for thinking about it like this is really looking at like uh... I can't remember the guy's name. Um... but I think it's probably... I'm guessing. No. No.\n\nUh, I don't... no, I... I don't know a damn thing about Schmidt Hooper's work, but he was probably... was right in the 90s. Like, I'll say that. I kind of suspect that that's true. Um, no, but uh, so I learned this or developed this sensibility after watching uh, Peter actually had this really excellent talk at the uh, graph learning workshop uh, a couple of months ago, and like, I... I just absolutely loved it. And I told my boss, I told everyone at the company, like, this is exactly what really good foundational ML research in a business environment looks like because, you know, started... started with the question, you know, and so I can explain, like, I mean, ask Peter exactly what was in that talk that I thought was so good, but...\n\nWe'd love to hear your perspective.\n\nYeah. No, I will. So the perspective here is that uh, you know, transformers are graph neural networks, and so... and that's that's where you get the sense where it's like, well, the causal masking, it just tells you which nodes can update which other nodes and what is the pattern of their thing. They're allowed to only do this, they're allowed to just bump things up. So essentially, you're restricting the kind of functions that the RNN is allowed to interpolate. If you say that your RNN is only allowed to shift things as opposed to like bend your space in a complex way, you make that hypothesis. So none of this is formalized yet, and I'm not... I might say some something wrong here, and I'll just like wave my hands with that appropriate caveat. But it is in this sense the way that in a transformer, previous things can update later on things that is a... and that's... and just exclusively just by bumping them a little bit. That is very much analogous to the way that previous steps in recursion update this thing. But essentially, you're just saying, instead of remembering the actual function that moves things around, you're just remembering a bunch of displacements, and you're passing displacements up the chain. And the reason why this works for sort of sparse dependencies and things like that is because you actually only need to bump it, like, have the cascade of bumpings go all the way to the front a few times before you've gotten enough of it, right? Right. And that's really critical is the is the finite depth, because that's that's really the key that allows the parallelized, you know, training.\n\nExactly.\n\nYeah.\n\nYeah.\n\nGo ahead.\n\nYeah. Isn't that um, a little bit of an idealization as well, because many of these models, we can think of them as as graph neural networks, because GNN's are quite intuitive in the sense that we can think of them as information diffusers. But Petar Velikovi has done some interesting work with some other folks that I interviewed at NeurIPS talking about information squashing. So even though um, a transformer might be an idealized version of an RNN, as Keith Duggar was saying, with this with this fixed depth, it's it's not really the thing that we want, because you have this like information squashing problem. So RNN's, for example, um, when you you know, roll them auto-regressively, the information gets slowly forgotten, and transformers and GNN's have a similar form of this problem, where all of the the information pathways get compressed so that it's not actually paying attention to many of the tokens in the context.\n\nYeah. No, right in this. So, Peter actually in this graph learning workshop gave a really nice uh discussion of this, talks about you think that argmax is going to save you from the over-squashing problem. But the problem is the context length and context window gets so large, argmax actually doesn't sharpen your distribution enough to save you, and in fact, doesn't matter what temperature you put on the argmax. It's simply not going to do it well enough. This is where they sort of optimize expander graphs instead of having you know, anything can attend to everything before it. They you like, actually, that's too much. Let's spify this thing in such a way that reduces the over-squashing of the signal. And like that's I think that's that's like exceptional work, because it it's incredibly pragmatic. It diagnoses exactly what the question is, what the business utility is, is that these things are going towards bigger context length. How do you make bigger context length work? And you actually just force it in like a random way to attend to less, right? We might think that you want might want some like repeating pattern of how you cut things away. And instead, I think it's actually much more that you want a very random one that doesn't constrain the optimization problem. And they were saying there, there are some advantages to having a specific like um, you know, to to to narrowing the cone of attention, because with reasoning, you actually want to have the correct answer, and and sometimes you don't. I think the interest in them comes from the signal processing world, where they try and figure out how do you have, you know, graphs such that, you know, the heat equation on them behaves well or not actually the heat equation, but sort of message passing on them behaves well as opposed to behaves poorly. And the premise sort of the nice thing about their work says, okay, well, this is actually the right perspective to have about what is happening in, you know, the sequence of updates along of of a transformer, yada yada yada. So you want to worry about specifically about this like signal processing perspective on the thing, and so you you simply say, well, these kinds of graphs that we know are better for message passing than attending to everything previously, you just substitute those in, and that you get better performance in the large context window situation specifically, because you've removed over part of the over-squashing problem by simply snipping connections in a way that staggers signal.\n\nSo Tim Scarfe, you've inspired a new title that I hope somebody writes one day, which is \"Attention Considered Harmful,\" like we need...\n\nThe critical difference between the unified representations and the fractured entangled ones is that the unified ones um have kind of a sort of coherence with the with um many more representations in their set. So like you can take you can take these two unified representations and combine them and get something, you know, meaningful or interesting, or these two or these two, like they're very composable, like across the entire set, you know, uh, like almost pairwise, whereas the whereas the fractured ones, like if I take this one, I might get something, but this pair produce garbage, produce noise, that sort of thing.\n\nYeah. No. So yeah, exactly. This is what I was what I'm saying is like, it's essentially like an optimal al... like an optimized algebra for constructing images, right? That's that's the story that I think is right. You know, that like, like universal algebra from a category theorist perspective kind of makes things make sense is you've got a very general vibe of what it means to have like uh, algebra for constructing things, right? But you've got a very... So it becomes relatively easy to even say the phrase. It's just like, okay, I'm optimizing over a space of combinatorial representations of things.\n\nMhm.\n\nAnd like specifically, I'm optimizing to make it really easy to draw pictures of the kinds of things that I originally say are important, but in frankly, like what the things that people think are important are exactly the kind of things that they find easy to draw.\n\nSomething that's like fascinating that I would love to connect this work with is, so back when I was doing like complex systems stuff, I was reading a lot of like uh, Stanislas Dehaene, so this will date me or whatever, so this was like what, 2000... no, probably 2010, 2011, this kind of thing, but uh, Stanislas... the global workspace theory guy... he might be... now I remember reading a book about reading in the brain, and it was about like what, like how do the pre-existing structure of the brain, how do we learn to read with them? And sort of that kind. So that was cool. And then there's also like a work by like Changy in the '90s uh, trying to sort of see like what kinds of things are in across writing systems of the world. What kinds of intersections of lines or like bumping into of lines are allowed to be discriminatory between this symbol and this one versus which ones are not? That seems very much like the same kind of question. And so the point is, we have evidence that writing systems have been adapted to being distinguished only by certain kinds of things, and that the rules for distinguishing things are like quite common, like it's unacceptable to have this and this be different, whereas it's very acceptable to have this and this be different.\n\nI like that story. I mean, something else you might add to it is that you know, the human being is not, you know, learning for the same task all the time, right? You're alternating between your between things that you're thinking about a lot and learning for. So I wonder if there's something of that story in there too. Is that actually part of the failure of machine learning models as they are? Is this legacy sensibility that they're for one thing? Because we think about them like machines. And in fact, the optimization process works better if you're not thinking about it like it's for one thing.\n\nSo, I was just going to... I was just going to say this this really resonates with me about uh, so many of the problems that um, you work on, definitely in in high school and even undergraduate things like that. They're really contrived. It's like every every problem in your math book has a solution. You know, you get it right or wrong when you solve it. And I got uh, you know, it was a big shocker to me, because when I started doing graduate work... Okay.\n\nYeah. Literally the first interesting physical system that I modeled and and then went to try and solve, like, had no closed-form solutions, you know? Yeah. And I'm just like, what the heck? I mean, how come I all these years, everything had solutions, and like, now all of a sudden, my god, you know, like, for example, you know, I'll give you one specific example. Um, I had this uh, you know, this series I was trying to sum, okay? And I couldn't figure out how to do it, and Mathematica couldn't figure out how to do it.\n\n\nAnd I really didn't want to do it numerically. So I, I went to the library where they had those, those like Russian books, you know, from like prior to, I mean, from like World War II era or whatever when they didn't have a lot of computers. And so, yeah, just like books of sequences. Exactly. Like these massive tomes of integral series sequences, all this kind of stuff, right? So I'm flipping around eventually in like to seven or something. I find, I find my, my series. Okay. And it says, \"Yeah, that's the Sterling series of the second kind. Go to this other book on this page for more details.\" Okay. Awesome. Yes. So, I go get the other book. I flip to that page. The Sterling series of the second kind is defined by the following series. The end. Nothing. There's like no more. There's just the definition of the series, right? So I go, okay, well, I couldn't solve it. Mathematica can't solve it. The Russians from the 50s couldn't solve it. Like I, that's hopeless. They can't, I can't solve it, you know? So the only thing I can do is just numerically calculate, approximate, whatever. And I'm like, how come we didn't spend more time in any of my prior schooling learning about these techniques that you use when you don't have closed form solutions? You know, approximations, numerical uh methods, uh apply like applied mathematics basically, right? Yeah. No, I mean it's uh what is it curriculum? That's, that's the question is like you give people the problems that they can solve such that when they finally find all the problems they can't, they're not like math is stupid. Instead, they're like math is just hard and I'm comfortable with that. I see. Right. That's, that's that, I think that's really the reason why is you want to give, I mean, you know, if you're reinforced, if you're not, think from a reinforcement learning perspective, why is theorem proving hard, you know, why has no one successfully made like an alpha zero style theorem prover that's suddenly better than humans? It's well, any like AlphaGo, AlphaZero works for go, why? Because one version of you is always winning, right? But if you try and do the analogous thing for proof, you're almost always losing. So, you have to get relatively good at even thinking about the question before you try and tackle the harder things because otherwise you're just going to get exclusively negative stimulus and get scrambled. Beautiful. No, and I really like that you bring up this, this particular example of like all of the math problems I did as a young person, they all had nice solutions. And it's this, you know, I remember walking home from, I, it was either in, I was in Vermont, it was in Burlington, it was my master's program. I was walking home from the bar from my with my friend Michael Lambert, who's now like done a lot of really great um double category work. Uh shout out to Michael. But um we were walking home from the bar and I think this is the first time I had taught a class and I was talking to him about just how easy it is to make an unsolvable problem if you're just like randomly generating problems for your students. And he responds, cannon to the left of me and cannon to the right. The point is, it's like no, there's just like you're walking, you are walking this incredibly narrow path. You know, you as you as you work in mathematics, as you become more and more expert, you realize, oh, I thought I was learning all of this stuff for the first time. It's like, no, this is what everyone already knows, has known for hundreds of years. This is the walled garden where everything is clean and like you learn how to operate and be sophisticated in this place. Then you go to the bigger place where you can keep all of the knowledge you have without without it totally getting lost, right? Um, one of the most, one of the like, I'm not a an expert on Pavlov's work or anything like this, but I love this folksy version of the definition of Pavlov's anxiety or Pavlovian anxiety when the subject does not know whether they will be rewarded or punished for a given action. Yeah. Right. It's exactly that, is that's the problem. Like as an educator, you can't teach students when they don't know when they have no sense of whether the next thing they do is going to get them punished or not, right? They have no idea. That's when they can't learn. And that's when you have to back up until they are until they know enough such that they have a really good guess for whether they're going to be punished or not. Right? You, that's why you have to, that's why you have to go back to where where it begins before things go off the rails where people are trying to learn things that they're not ready to learn. I suspect that the same thing is true for models. I was just going to say, um, I really appreciate um when you when you said you, you finally learned what category theory is because I know, I don't know like what what category theory is, but I think um I think both Tim and I, certainly me, I'm very fascinated by it and uh I always, I always think it, it has to have some extremely valuable insights, you know, to offer for AI and machine learning. So, it's always a joy to talk to you because I'd love to know, you know, what you're up to lately and what you're thinking about the application of category theory to, you know, our field. Really, realistically, what most of what I've been doing for the last year and a half is not so much uh figuring out how to apply category theory, but honestly trying to catch up to what the abiding sensibilities, folk wisdom, and accumulated knowledge of machine learning is. You know, I came from radically outside the discipline and I really saw it as the challenge, like, you know, so I, you know, along with Bruno Gavranovi and you know, Andrew Dudzik and Tamara von Glehn and Joo G. M. Arajo, you know, we wrote the position paper that was sort of a moderately big deal last year, that's sort of how we got introduced. Um last year we wrote that, but at the time I was like still very much at like a straight ahead category theorist just trying to come to grips with what uh machine learning is actually doing, just sort of really just playing around and sort of trying to understand what it means to have, you know, categories of parametric functions, i.e. models and how to sort of just begin to work and think with those, but really I've put most of the last year and a half into, I mean, not really a year and a half, more like a year into just trying to come up to speed as fast as possible with, you know, the accumulated wisdom of machine learning folks. So that, that's fascinating. Can you share with us, I'm curious what some of the top few things um you learned that surprised you are. The thing that say, so that working a little bit on automated theorem proving has taught me and just really thinking about it and going to a couple conferences, talking to people, the thing that's really thought about is like um I find a renewed appreciation for my intuitionist sensibilities. You talk to a lot of people who are interested in things like structure such as I am and there's this abiding presumption that if you only trained on that one kind of structure, things would perform better, whereas we know experimentally that they perform worse, right? We know that if you train a code model exclusively on one language, it is almost guaranteed to perform worse than if you train one on various languages. Exactly why is unclear, but you know, it's a completely reasonable hypothesis is to assume that there's something about like higher order regularities and sort of trimming off the specificity, the you know, probabilistic accumulation of a couple of syntactic things that are confused by for semantics by the model, those are stripped away by operating in various different languages. So instead, the latent representation corresponds to something closer to like a higher level description of what's going on in code than you know, just the a, there has to be a connection there, there has to be a connection here to category theory, help me, help me see it. One thing that I would say is training on multiple languages is essentially the same is an analog to what you do to train vision transformers, right? Where you know, vision transformers are not explicitly equivariant, but what you do is you do this operation of data saturation, you just like, well, I'm just going to soup up my data set with every single rotation, translation, and transformation that I want to be equivariant with respect to. You know, I modify the data set and then I train the transformer on it. And that's where you get this like transformers can be trained to be more equivariant than actual convolutional neural networks. Why is, well, it's kind of disingenuous because what you actually did is, you know, you boosted the scale of the data set and you forced the data set to have um a distribution level version of the equivariance story in it, right? And then you're just leveraging the fact that the transformer is particularly good at sequence prediction, you know, so that that kind of thing. So that's the story. So the analog here as I would say is training on multiple languages is very much analogous to um saturating a data set with respect to a group action, right? And so, and let me ask you this because and again, like folks out there, please understand, I'm a complete newbie with category theory. So, I'm trying to learn more about it. Wish I knew more about it. So, if my questions are totally off the mark, feel free to uh feel free to correct them. But I, I've been hoping or thinking that category theory should help us to enforce in the in the structure of neural networks certain equivariances um that we that we know should be there. So for example, if we know there should be a rotation of variance or some other affine transformation, you know, is there a way to structure the neural network where that's kind of built in as an inductive bias? Yeah, I mean this is, I mean it's well actually as a as a formal constraint, not merely like an inductive bias where something preferentially finds solutions that have that property, you can completely restrict it to have that property. Okay. Right. And we've done that, like and that's what you know, that's what geometric deep learning is about and indeed, you know, the content of the paper where we made a subtle mistake that I can sort of explain, you know, over the next 20 minutes or whatever. Uh that's you know, RNN's are an example of this, right? Is that you've got a formal constraint, which is that you're repeating exactly the same unit again and again. So the the the point being is that the weight time that corresponds to convolutional neural networks is exactly the same kind of weight time that corresponds to recursion in RNN's and all of it is algebra for monads, right? That was sort of the story that we told in the paper. The thing that we made a mistake about in the paper, largely because, you know, I'm a pure mathematician, I'm used to having a like a long time to think about something and machine learning publications are more like you lock yourself in a room for three weeks and you hail Mary. Um and so, remind us about the paper. Oh yeah. No, so the paper is this paper called Position: Categorical Deep Learning is an Algebraic Theory of All Architectures. Uh I confess I objected to the title being so hubristic in a sense of theory of all architectures, but the editors uh were like, no, this is not forceful enough. You need to, you need to pick something that's sufficiently bombastic and it's like, okay, I guess it's the standard of the discipline. I'm moving to a stochastic universe, I can accept that. Maybe it's a bit bit overblown, but yeah. And so in that paper, what we talked about is how, you know, the notion of monads, the notion of algebras therefore, and demonstrating that convolutional layers are morphisms of algebras for monads and likewise RNN's can be seen as morphisms of algebra for monads. The fact that those monads are actually the free monads on polynomial endo functors, we made a little bit of big deal about, you know, yada yada yada, that kind of thing, but turns out that's not the easiest story and indeed that's where some of the mis, the subtle mistake came from. I think back at the end also through the editing process, we claimed, oh, these things are uh lax algebras for these things or or like, you know, lax natural transformations between these structures and I think that's wrong. You actually don't need laxness at all. The illusion of needing laxness is because that the paraconstruction is something I, I don't put much faith in anymore. I think it's a, it's a accidental rigidity and instead, you know, some work that people at Symbolica, Solo Clingman, Mitch Buckley, and Talier and Bayon have done. You know, they sort of, they filled in the funal semantic story for inductive families in a neat way that made it clear that RNN's just fall out in exactly the same way as, you know, functors on BG where BG is the classifying category of a group valued in so funers valued in set from BG where BG is the classifying category of a group that kind of story gives you sets together with a group action, similar story where some flavor of funure valued in some other category gives you things like RNN's. Okay. Yeah. So that's not the, that's not the best description, but the point is we made a subtle claim and you actually don't need laxmness at all. And it's actually a much more elementary story. And the reason why it was hard at the time of writing the paper is because we were working with this para construction. And in particular, it makes these things called co-limits not work or not necessarily work very well. And co-limits are the sort of general theory of gluing things together. And as you can believe, gluing things together is hard. And in particular, it's very hard if the uh presentation of things you've got to work with is overly complicated. And I think that's what was wrong with PAR, right? What are you most excited about and interested in? So, you know, maybe what you're working on right now or what you see in the field happening that you're really excited about um promising for the future, that type of thing. Oh, the thing that I'm just really enjoying right now is, you know, my new, my new feeling about what am I doing? Like, what am I doing as a machine learning researcher? What are we doing with these systems overall is the following sensibility. The thing that is the most compelling is that every single story in machine learning is something about stochastic processes. And what we are doing is we are, you know, when neural, what neural networks are is they are an algebra for constructing a machine whose or essentially, essentially like a fake physics whose energy relaxation process computes the distribution of data we fed it, like and just fe, like that, that feeling, that's, that's, that's what is, what is, that's the\n\n\nThat's the vague sensibility I have that I'm most excited about.\n\nYou know, like I have, you know, I come from this world where I've got sort of like all of this formalism. Now I'm really trying to sort of just dig deep into just like, no, everything is probabilistic here. What does that mean? What does it mean to take that as seriously as possible? What does it take? What does it mean to take both diffusion process as an SGD? All of these things are stochastic processes. Like all of these things are like there's this is really a statistical physics story at some level and sort of just trying to like accept that as like the most radical premise and just see. Like what we are doing is designing incredibly complicated versions of like the single electron of the hydrogen atom and bombarding it in some particular context and figuring out what distribution of energy states that single electron is in. It's analogous to that, except it's an extraordinarily complicated system, but nonetheless the training process is exactly that, bombarding it with, you know, some amount of energy in various forms and then seeing what the distribution of stuff is at the end.\n\nLike, so really thinking about like the task of machine learning and the sort of task of a machine learning researcher, if you're interested in foundational questions of how this works, is figuring out what an efficient language for specifying uh parametric families of models such that these stoastic processes like compute quickly and come up with good stories. Like that, that's the that's sort of the vague sensibility I have that I'm finding exciting now.\n\nNo, thank you. I mean, I think um on that on that beautiful and intriguing note, I think we'll leave our uh leave our listeners to ponder that and I would encourage them encourage them to do so.\n\nSo, listen, Paul, it's been an absolute pleasure um talking with you again today and I really appreciate you uh taking the time to hang out with us.\n\nNo, indeed. It's been great. It's good to see you again. It's good to see both of you again. Wonderful. Thank you so much for coming, Opo. I really appreciate it.\n\nYeah, absolutely.\n",
  "dumpedAt": "2025-07-21T18:43:24.957Z"
}