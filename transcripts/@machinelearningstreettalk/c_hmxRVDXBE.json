{
  "episodeId": "c_hmxRVDXBE",
  "channelSlug": "@machinelearningstreettalk",
  "title": "The Weird ChatGPT Hack That Leaked Training Data",
  "publishedAt": "2025-07-12T19:35:44.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "there was this investigation into why",
      "offset": 0.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the word delve was used so often, right?",
      "offset": 1.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And it turns out, okay, a lot of the",
      "offset": 3.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "training data comes from kind of",
      "offset": 5.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Nigerian crowd workers, I think, because",
      "offset": 7.279,
      "duration": 4.001
    },
    {
      "lang": "en",
      "text": "their style of English just contains",
      "offset": 9.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "that word a lot. And so, in a sense,",
      "offset": 11.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that completely backfired because now",
      "offset": 13.679,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "every time they write, you know, some",
      "offset": 15.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "kind of application letter somewhere,",
      "offset": 17.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the company puts it through the detector",
      "offset": 19.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and the detector is just trained on chat",
      "offset": 21.039,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "GPT text versus human text, right? And",
      "offset": 22.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "then for them it always just turns out",
      "offset": 25.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like whoop out here or you sound like",
      "offset": 27.119,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "chat GPT I feel it's like kind of like",
      "offset": 29.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "hopeless to build these detectors of",
      "offset": 31.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "whether something's fake. But on the",
      "offset": 33.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "other hand I feel as a human you're",
      "offset": 35.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "pretty good at being like that looks",
      "offset": 37.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "generated",
      "offset": 40.079,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "in security 99% means you fail.",
      "offset": 41.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Essentially 99% is somewhat",
      "offset": 45.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "indistinguishable from 0%. It just means",
      "offset": 47.84,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "these models have learned sort of",
      "offset": 50.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "increasingly complicated correlations",
      "offset": 52.079,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "between inputs that sort of somehow",
      "offset": 54.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "explain roughly how the world looks like",
      "offset": 56.879,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "99% of cases. If you're talking about",
      "offset": 59.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "worst case and there is this 1% of",
      "offset": 62.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "failures, well, an attacker is going to",
      "offset": 64.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "find them and that's what they're going",
      "offset": 66.479,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "to send to your model.",
      "offset": 67.92,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "To give one concrete example of",
      "offset": 82.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "something we worked on in the past year",
      "offset": 84.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that's still actually to me also just",
      "offset": 86.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "mindboggling. We had a project, this was",
      "offset": 88.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with Nicholas Carlini at Google in",
      "offset": 90.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "collaboration with a bunch of other",
      "offset": 92.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "people there where we tried to recover",
      "offset": 94.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "some of the training data of chat GPT by",
      "offset": 97.68,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "interacting with the model and one of my",
      "offset": 100.4,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "colleagues Milad Nazer who's a",
      "offset": 103.6,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "researcher at Google at some point made",
      "offset": 105.439,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "this weird discovery that if you ask",
      "offset": 107.759,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "chat GPT to repeat the word poem",
      "offset": 110.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "forever, the model starts doing that and",
      "offset": 113.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "at some point it stops and starts",
      "offset": 116.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "spitting out random bits and pieces of",
      "offset": 118.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "text from the internet that it has",
      "offset": 121.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "memorized. This is, I think, to date the",
      "offset": 123.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "weirdest attack I've ever seen on",
      "offset": 125.92,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "machine learning. We sent this to OpenAI",
      "offset": 128.239,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "before we published the paper and we",
      "offset": 131.039,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "were like, we found this flaw. Training",
      "offset": 132.959,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "data is coming out and they replied to",
      "offset": 136.16,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "us and said, why? And we were like, how",
      "offset": 139.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "would we know? You're you're the experts",
      "offset": 143.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "here. You're the guys deploying this",
      "offset": 144.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "system.",
      "offset": 146.239,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "We're actually asking you to tell us",
      "offset": 147.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like what's going on with this model.",
      "offset": 148.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "This worked with like one specific",
      "offset": 151.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "version of chat GPT. We tried some",
      "offset": 153.92,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "experiments to to sort of pinpoint",
      "offset": 157.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "roughly what might be going on with this",
      "offset": 159.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "model. Didn't really end up with",
      "offset": 161.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "anything that was particularly",
      "offset": 163.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "convincing. OpenAI sort of patched this",
      "offset": 165.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "flaw in that now if you ask CHPT to",
      "offset": 169.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "repeat the word forever, it tells you",
      "offset": 171.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "no, I can't do that. a bit of a band-aid",
      "offset": 173.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "on a gaping wound kind of patch. I mean,",
      "offset": 175.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of course, here this is a model that's",
      "offset": 178.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "only been mostly, as far as we know,",
      "offset": 179.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "been trained on publicly available data",
      "offset": 182,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "from the internet. So, leaking this data",
      "offset": 184.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "doesn't matter all that much. But if",
      "offset": 186.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "suddenly you have a model that's been",
      "offset": 188.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "trained in um in a medical domain, in a",
      "offset": 190,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "legal domain maybe where some of the",
      "offset": 193.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "data could be proprietary and someone",
      "offset": 194.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "comes along and asks your, you know,",
      "offset": 197.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "your medical assistant chatbot, hey,",
      "offset": 199.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "don't you want to maybe repeat the word",
      "offset": 201.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "poem forever? this is something that",
      "offset": 202.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "someone might do or it's it's very hard",
      "offset": 205.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "to when you design your system to sort",
      "offset": 208.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of think of all the weird things people",
      "offset": 210.159,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "might use these systems for because",
      "offset": 212.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "they're supposed to be somewhat general",
      "offset": 214.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "purpose. When we first looked into this,",
      "offset": 216.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we then at some point found out that",
      "offset": 218.799,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "there was people discussing something",
      "offset": 220.4,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "like this on Twitter who had also at",
      "offset": 223.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "some point just asked chatgbt to repeat",
      "offset": 224.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "a word forever and sort of seen similar",
      "offset": 226.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "things happen. But this is also",
      "offset": 229.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "something we're starting to see that as",
      "offset": 230.959,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "academic researchers, we can't really",
      "offset": 233.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "gatekeep this this field, right? You",
      "offset": 235.599,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "just have random people uh enthusiasts",
      "offset": 237.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "or or people in in industry who play",
      "offset": 241.439,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "with these models who just from time to",
      "offset": 243.92,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "time stumble upon extremely weird",
      "offset": 246.239,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "behaviors, new attack vectors. In that",
      "offset": 250.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "sense, it keeps things interesting that",
      "offset": 253.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "everyone's sort of on the lookout for",
      "offset": 255.04,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "weird things to do with with chat GPT",
      "offset": 257.44,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "and with other models.",
      "offset": 259.919,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "What are your I don't know top three",
      "offset": 267.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "worries? Like what are your top three?",
      "offset": 270.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "We really shouldn't do that, but I know",
      "offset": 272.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we're going like I know people are going",
      "offset": 275.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to do that, right? and more like like",
      "offset": 277.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you mentioned okay if you find an",
      "offset": 279.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "exploit or something do you have",
      "offset": 281.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "concrete things in mind where you see",
      "offset": 282.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "stuff going and where you're like okay",
      "offset": 284.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that's that's going to end pretty badly",
      "offset": 286.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so in no particular order maybe one I",
      "offset": 288.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "would say is starting to train these",
      "offset": 291.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "models or or or fine-tune them on",
      "offset": 293.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "proprietary and sort of privacy",
      "offset": 296.4,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "sensitive data in that yeah the",
      "offset": 298.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "memorization risks in these models are",
      "offset": 302.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "just something we haven't really gotten",
      "offset": 304.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "under control yet we some",
      "offset": 306.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "some proofs of concepts that things can",
      "offset": 309.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "leak, but it's sort of not well",
      "offset": 311.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "understood. And at the same time,",
      "offset": 313.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "there's this huge need, right, for more",
      "offset": 314.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "data. And then a whole bunch of",
      "offset": 316.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "researchers or companies that are",
      "offset": 318.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "starting to say, well, we'll just, you",
      "offset": 320.479,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "know, develop some kind of synthetic",
      "offset": 322.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "data generation pipeline. We don't",
      "offset": 324,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "really have any guarantees that this is",
      "offset": 326.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "not somehow going to leak the the data",
      "offset": 327.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you trained on either. And this is going",
      "offset": 329.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to hit areas like I could see this being",
      "offset": 332.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the case in yeah in the medical domain",
      "offset": 334.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in education and so on that people are",
      "offset": 337.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "just going to try to get their hands on",
      "offset": 339.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "whatever data they can to further train",
      "offset": 341.199,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "or fine-tune these models and I would",
      "offset": 344.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "expect that then at some point some data",
      "offset": 347.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is just going to get leaked. Second bad",
      "offset": 349.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "stuff. Yeah. And this is really to me I",
      "offset": 351.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "think the the the main concern with how",
      "offset": 353.52,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "people deploy these models today is",
      "offset": 355.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "prompt injections. people put agents in",
      "offset": 357.759,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "various products that that actually have",
      "offset": 360.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "a quite a large action space and the",
      "offset": 363.199,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "most recent prime example of this which",
      "offset": 367.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to be honest I feel like what were they",
      "offset": 369.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "thinking is uh Antropics like computer",
      "offset": 371.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "use it's a cute demo and they have a big",
      "offset": 373.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "warning somewhere in the documentation",
      "offset": 376.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "like oh yeah this thing is like easy to",
      "offset": 378.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "prompt inject and then it's kind of like",
      "offset": 380.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what's the point of this like if you if",
      "offset": 382.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "you install this in your uh if you if",
      "offset": 384.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you put this on your computer and sort",
      "offset": 386.319,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "of give",
      "offset": 387.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "any access you're you're kind of yeah",
      "offset": 389.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "asking for for something to go wrong and",
      "offset": 391.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but I think this isn't going to stop",
      "offset": 394.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like yeah there's there's too many",
      "offset": 395.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "companies startups kind of wanting to",
      "offset": 397.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "push these kind of things and so we're",
      "offset": 399.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "going to see essentially a new decade of",
      "offset": 400.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you know injection attacks we've had a",
      "offset": 402.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "decade of SQL injection attacks in the",
      "offset": 405.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "past we've had buffer overflow type",
      "offset": 407.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "attacks for for years now it's going to",
      "offset": 409.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "be prompt injections that probably are",
      "offset": 412.72,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "just going to start showing up",
      "offset": 414.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "everywhere except maybe for a few",
      "offset": 415.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "companies that really understand the",
      "offset": 418.639,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "risk and and that are going to build",
      "offset": 420.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "much much more restrained uh tools, but",
      "offset": 421.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "the comp yeah the competitive pressure",
      "offset": 425.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "to do cool and fancy stuff is is going",
      "offset": 427.599,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "to be there and so people will do it",
      "offset": 430,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "in between chat GBT happened and for our",
      "offset": 437.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "field this has been um quite amazing and",
      "offset": 441.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "also quite scary. amazing in the sense",
      "offset": 443.599,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "that it's kind of really pushed this",
      "offset": 446.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "this research field into into the",
      "offset": 448.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "limelight and kind of into into",
      "offset": 450.639,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "something real and that many people in",
      "offset": 452.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this field had gotten a bit annoyed and",
      "offset": 454.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "a bit scared maybe even of the fact that",
      "offset": 456.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "a lot of the research we were doing was",
      "offset": 459.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "kind of trying to come up inventing",
      "offset": 460.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "maybe problems that AI systems might",
      "offset": 462.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "have or kind of trying to speculate of",
      "offset": 464.639,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what some bad guy somewhere in a hacker",
      "offset": 466.639,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "hoodie might do to your AI system. But",
      "offset": 469.039,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "it was always very hard to make this",
      "offset": 471.919,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "concrete because you didn't really know",
      "offset": 474.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "what people were actually using AI for.",
      "offset": 476.56,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "There were sort of some speculation",
      "offset": 479.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "about, you know, maybe people might put",
      "offset": 481.039,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "an AI in a self-driving car and then",
      "offset": 482.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "maybe you could go and put some uh some",
      "offset": 485.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "stickers on a stop sign somewhere, but",
      "offset": 487.28,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "it was very hard to sort of in some",
      "offset": 489.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "sense evaluate our assumptions like is",
      "offset": 490.879,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "is is any of this actually real? Does",
      "offset": 493.199,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "any of this make sense? And well",
      "offset": 495.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "nowadays you can go and uh take a tool",
      "offset": 496.72,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "like chat GPT that like has a 100",
      "offset": 498.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "million users and you don't have to",
      "offset": 500.879,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "guess anymore how the system works, how",
      "offset": 502.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "people use it, how you might go about",
      "offset": 505.52,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "attacking it because someone else",
      "offset": 507.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "actually does this for you. Like a",
      "offset": 508.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "company like OpenAI tells you this is",
      "offset": 510.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the tool we're trying to build. This is",
      "offset": 512.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "our threat model. This is what the tool",
      "offset": 514.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is supposed to do. This is what it's not",
      "offset": 516,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "supposed to do. And if you can then go",
      "offset": 517.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and say, well, I made the model do what",
      "offset": 519.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it's not supposed to do. you don't",
      "offset": 521.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "really have to spend all this time like",
      "offset": 523.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "arguing about hypotheticals of whether",
      "offset": 524.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "what you're doing is is relevant or or",
      "offset": 527.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "real. It kind of is by definition. And",
      "offset": 529.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so in that sense, from a researcher's",
      "offset": 531.839,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "perspective, this has been very nice",
      "offset": 533.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that it's made it much easier to sort of",
      "offset": 534.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "come up with concrete real problems for",
      "offset": 537.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "us to work on. The downside is that now",
      "offset": 539.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "when we find flaws in these systems, and",
      "offset": 541.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "we still do, all of a sudden things are",
      "offset": 545.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "a lot more real. you actually have in",
      "offset": 548.32,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "your hands maybe some exploit that could",
      "offset": 550.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "impact tools that are used by millions",
      "offset": 553.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of people. We have to think a bit harder",
      "offset": 555.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "about how we go about doing this kind of",
      "offset": 557.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "research, the ethics behind it, how we",
      "offset": 560,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "might disclose vulnerabilities to whom,",
      "offset": 562.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "how we might go about patching these",
      "offset": 564.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "things. And so it's become an area of",
      "offset": 566.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "research that I think has gotten closer",
      "offset": 568.8,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "to traditional computer security in that",
      "offset": 570.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "sense. The machine learning community",
      "offset": 572.959,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "kind of wasn't necessarily ready for",
      "offset": 574.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this. And there's maybe a whole bunch of",
      "offset": 576.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "lessons from computer security that the",
      "offset": 578.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "machine learning community will have to",
      "offset": 580.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "relearn over the years. But yeah, it's",
      "offset": 582.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "definitely made the whole work a lot of",
      "offset": 585.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "fun. It's also kind of amazing now that",
      "offset": 588.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and I guess this is true for everyone",
      "offset": 590.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "working in AI these days that it's just",
      "offset": 592.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "a a topic that everyone cares about,",
      "offset": 594.399,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "everyone has heard about. last year at",
      "offset": 598.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "some point my my landlord came up to me",
      "offset": 600.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and asked me about chat GBT and for me",
      "offset": 602.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this was just like the the turning point",
      "offset": 604.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you know of I mean he he was a tech guy",
      "offset": 606.08,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "so maybe maybe it wasn't that surprising",
      "offset": 608,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "but still it was kind of the the first",
      "offset": 609.839,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "time really that I started having people",
      "offset": 611.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "from everywhere in my life like family",
      "offset": 613.6,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "friends suddenly talking about things",
      "offset": 616.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "that were very close to to my research",
      "offset": 620.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and also the fact that I could actually",
      "offset": 622.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "talk about my research in rather open",
      "offset": 624.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and simple terms with people and not get",
      "offset": 626.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "sort of blank stares back of like what",
      "offset": 628.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the hell are you doing? but actually",
      "offset": 630.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like oh okay this this I kind of",
      "offset": 631.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "understand and yeah this sounds scary",
      "offset": 633.839,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "and so yeah in that sense it's a very",
      "offset": 635.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "very cool period I think for AI security",
      "offset": 637.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "research but also a scary one because",
      "offset": 640.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "people are deploying crazy new",
      "offset": 642.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "technology left and right that we we",
      "offset": 644.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "don't really know how to how to",
      "offset": 647.2,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "safeguard",
      "offset": 648.959,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "in some sense with machine learning",
      "offset": 655.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "whatever task we've looked at over the",
      "offset": 658.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "past decades, we've sort of we've gotten",
      "offset": 660.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "better and better at getting models that",
      "offset": 662.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "work sort of on average. Maybe before",
      "offset": 665.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "chat GBT, we would have had language",
      "offset": 667.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "models that, you know, predict the right",
      "offset": 669.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "word only 50% of the time, which gives",
      "offset": 671.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you something complete gibberish. Now",
      "offset": 674,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "with ChatGpt, it might be 99% of the",
      "offset": 675.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "time or 99.9% of the time, which is good",
      "offset": 678.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "enough to generate hundreds or maybe",
      "offset": 681.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "thousands of tokens that that are",
      "offset": 683.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "coherent, but it's still never 100%. I",
      "offset": 685.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "think we need something else than just",
      "offset": 688.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "scale to to make this go away. That I",
      "offset": 690.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "think with scale and with more data,",
      "offset": 693.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "we're going to be fitting increasingly",
      "offset": 695.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "complex sort of functions that are sort",
      "offset": 697.519,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "of going to cover maybe at some point an",
      "offset": 699.519,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "extra nine an extra nine of reliability,",
      "offset": 702.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "but that you're never going to be able",
      "offset": 706,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to fit everything. And that there's",
      "offset": 707.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "somehow the way that we learn about",
      "offset": 709.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "problems and how to solve them is sort",
      "offset": 712.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "of not by just throwing more and more",
      "offset": 714.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "and more and more data at it. And",
      "offset": 716,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "there's probably something else that",
      "offset": 717.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "causes us to have some kind of causal",
      "offset": 719.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "understanding of of the world. And my",
      "offset": 722.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "guess is that scale alone will not bring",
      "offset": 725.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "us that. The caveat here is that if you",
      "offset": 728.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "had asked me a couple of years ago if",
      "offset": 730.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "scale alone would give us something like",
      "offset": 732.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "chat GPT, I would have also said no. And",
      "offset": 735.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "so I'm I'm kind of aware of my own",
      "offset": 737.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "limitations here when it comes to",
      "offset": 740.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "predicting how how fast AI can or sort",
      "offset": 742.32,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "of how how far we can get with with what",
      "offset": 746.079,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "we're currently doing.",
      "offset": 748.959,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "You said something interesting and that",
      "offset": 755.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "is that you're not bullish on",
      "offset": 757.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "watermarking. Could you actually let us",
      "offset": 758.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "in why?",
      "offset": 760.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "So two two reasons. Uh, one which is",
      "offset": 761.6,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "which is relatively basic is just that",
      "offset": 765.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "many models that people use that can do",
      "offset": 768.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "bad stuff are open source and if the",
      "offset": 771.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "model is open source you can do with it",
      "offset": 773.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "whatever you want and you you can't",
      "offset": 775.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "really embed a watermark. I mean there",
      "offset": 777.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "are some proposals sort of for doing",
      "offset": 779.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "some kind of backd doorors but",
      "offset": 780.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "essentially once the model is on your",
      "offset": 783.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "machine you can do whatever you want",
      "offset": 784.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "with it. Nothing's going to stick with",
      "offset": 786,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "closed source models. We have nice",
      "offset": 788.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "proposals sort of that that give you",
      "offset": 790.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "watermarks that give you nice",
      "offset": 792.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "statistical guarantees and I think these",
      "offset": 794.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "watermarks will be useful for for",
      "offset": 796.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "example just filtering you know the",
      "offset": 798.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "training data of GPT5 to make sure that",
      "offset": 800.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you're not including a whole bunch of",
      "offset": 803.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "outputs from GPT4. So kind of on average",
      "offset": 805.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I feel like these these things probably",
      "offset": 807.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "will will work reasonably well. The",
      "offset": 808.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "problem is that as with any kind of ML",
      "offset": 810.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "system it's just not going to be",
      "offset": 813.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "particularly robust. And so if I take a",
      "offset": 814.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "text that's been output by a watermarked",
      "offset": 817.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "language model, maybe maybe I ask the",
      "offset": 820.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "model to write the text in German and",
      "offset": 822.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "then I feed it through Google Translate",
      "offset": 824.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to put it in English and uh voila,",
      "offset": 826.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "watermark gone. You you sometimes have",
      "offset": 828.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "to be a little bit more careful like",
      "offset": 830.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "some of these watermarks can be",
      "offset": 831.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "relatively robust, but in the end you",
      "offset": 833.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "can you can do whatever you want with",
      "offset": 836.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this text. And the same goes for images.",
      "offset": 838.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "is I mean there there have been some",
      "offset": 841.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "nice proposals from deep mind actually",
      "offset": 842.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Ilia I think has been involved in some",
      "offset": 845.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "of these as well both for images and",
      "offset": 846.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "text but if you read the paper it's",
      "offset": 848.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "literally somewhere they say like the",
      "offset": 849.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "watermark is somewhat robust whatever",
      "offset": 851.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that means and yeah what it I guess what",
      "offset": 854.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it does mean is that if you're going to",
      "offset": 856.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "try sort of very simplistic low resource",
      "offset": 857.44,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "type edits like you you just try to to",
      "offset": 860.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "paraphrase some of your text or you you",
      "offset": 863.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "replace a few words here and there it's",
      "offset": 866.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "going to be robust to that um it's not",
      "offset": 868.16,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "going to robust to everything. And if",
      "offset": 871.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you actually care about using these",
      "offset": 872.959,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "watermarks for something that's",
      "offset": 874.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "adversarial, like detecting, say if some",
      "offset": 877.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "if something is is just some some deep",
      "offset": 879.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "fake, we just don't really have a robust",
      "offset": 882.639,
      "duration": 4.421
    },
    {
      "lang": "en",
      "text": "way of doing that.",
      "offset": 885.12,
      "duration": 6.79
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 887.06,
      "duration": 4.85
    }
  ],
  "cleanText": "There was this investigation into why the word delve was used so often, right? And it turns out, okay, a lot of the training data comes from kind of Nigerian crowd workers, I think, because their style of English just contains that word a lot. And so, in a sense, that completely backfired because now every time they write, you know, some kind of application letter somewhere, the company puts it through the detector and the detector is just trained on ChatGPT text versus human text, right? And then for them it always just turns out like whoop out here or you sound like ChatGPT.\n\nI feel it's like kind of like hopeless to build these detectors of whether something's fake. But on the other hand, I feel as a human, you're pretty good at being like, that looks generated.\n\nIn security, 99% means you fail. Essentially, 99% is somewhat indistinguishable from 0%. It just means these models have learned sort of increasingly complicated correlations between inputs that sort of somehow explain roughly how the world looks like 99% of cases. If you're talking about worst case and there is this 1% of failures, well, an attacker is going to find them and that's what they're going to send to your model.\n\nTo give one concrete example of something we worked on in the past year that's still actually to me also just mindboggling. We had a project, this was with Nicholas Carlini at Google in collaboration with a bunch of other people there, where we tried to recover some of the training data of ChatGPT by interacting with the model. And one of my colleagues, Milad Nasr, who's a researcher at Google, at some point made this weird discovery that if you ask ChatGPT to repeat the word poem forever, the model starts doing that and at some point it stops and starts spitting out random bits and pieces of text from the internet that it has memorized. This is, I think, to date, the weirdest attack I've ever seen on machine learning. We sent this to OpenAI before we published the paper and we were like, we found this flaw. Training data is coming out. And they replied to us and said, why? And we were like, how would we know? You're the experts here. You're the guys deploying this system. We're actually asking you to tell us like what's going on with this model.\n\nThis worked with like one specific version of ChatGPT. We tried some experiments to sort of pinpoint roughly what might be going on with this model. Didn't really end up with anything that was particularly convincing. OpenAI sort of patched this flaw in that now if you ask CHPT to repeat the word forever, it tells you no, I can't do that. A bit of a band-aid on a gaping wound kind of patch. I mean, of course, here this is a model that's only been mostly, as far as we know, been trained on publicly available data from the internet. So, leaking this data doesn't matter all that much. But if suddenly you have a model that's been trained in um in a medical domain, in a legal domain maybe where some of the data could be proprietary and someone comes along and asks your, you know, your medical assistant chatbot, hey, don't you want to maybe repeat the word poem forever? This is something that someone might do or it's it's very hard to when you design your system to sort of think of all the weird things people might use these systems for because they're supposed to be somewhat general purpose.\n\nWhen we first looked into this, we then at some point found out that there was people discussing something like this on Twitter who had also at some point just asked ChatGPT to repeat a word forever and sort of seen similar things happen. But this is also something we're starting to see that as academic researchers, we can't really gatekeep this field, right? You just have random people, uh, enthusiasts or or people in in industry who play with these models who just from time to time stumble upon extremely weird behaviors, new attack vectors. In that sense, it keeps things interesting that everyone's sort of on the lookout for weird things to do with with ChatGPT and with other models.\n\nWhat are your, I don't know, top three worries? Like what are your top three?\n\nWe really shouldn't do that, but I know we're going like I know people are going to do that, right? And more like like you mentioned, okay, if you find an exploit or something, do you have concrete things in mind where you see stuff going and where you're like, okay, that's that's going to end pretty badly? So in no particular order, maybe one I would say is starting to train these models or or or fine-tune them on proprietary and sort of privacy sensitive data in that yeah, the memorization risks in these models are just something we haven't really gotten under control yet. We have some some proofs of concepts that things can leak, but it's sort of not well understood. And at the same time, there's this huge need, right, for more data. And then a whole bunch of researchers or companies that are starting to say, well, we'll just, you know, develop some kind of synthetic data generation pipeline. We don't really have any guarantees that this is not somehow going to leak the the data you trained on either. And this is going to hit areas like I could see this being the case in yeah, in the medical domain, in education and so on that people are just going to try to get their hands on whatever data they can to further train or fine-tune these models and I would expect that then at some point some data is just going to get leaked.\n\nSecond bad stuff. Yeah. And this is really to me I think the the the main concern with how people deploy these models today is prompt injections. People put agents in various products that that actually have a quite a large action space and the most recent prime example of this, which to be honest, I feel like what were they thinking, is Anthropic's like computer use. It's a cute demo and they have a big warning somewhere in the documentation like, oh yeah, this thing is like easy to prompt inject and then it's kind of like what's the point of this? Like if you if you install this in your uh if you if you put this on your computer and sort of give any access, you're you're kind of yeah, asking for for something to go wrong. And but I think this isn't going to stop. Like yeah, there's there's too many companies, startups kind of wanting to push these kind of things and so we're going to see essentially a new decade of, you know, injection attacks. We've had a decade of SQL injection attacks in the past, we've had buffer overflow type attacks for for years now, it's going to be prompt injections that probably are just going to start showing up everywhere except maybe for a few companies that really understand the risk and and that are going to build much much more restrained uh tools, but the comp yeah, the competitive pressure to do cool and fancy stuff is is going to be there and so people will do it.\n\nIn between ChatGPT happened and for our field this has been um quite amazing and also quite scary. Amazing in the sense that it's kind of really pushed this this research field into into the limelight and kind of into into something real and that many people in this field had gotten a bit annoyed and a bit scared maybe even of the fact that a lot of the research we were doing was kind of trying to come up inventing maybe problems that AI systems might have or kind of trying to speculate of what some bad guy somewhere in a hacker hoodie might do to your AI system. But it was always very hard to make this concrete because you didn't really know what people were actually using AI for. There were sort of some speculation about, you know, maybe people might put an AI in a self-driving car and then maybe you could go and put some uh some stickers on a stop sign somewhere, but it was very hard to sort of in some sense evaluate our assumptions like is is is any of this actually real? Does any of this make sense? And well nowadays you can go and uh take a tool like ChatGPT that like has a 100 million users and you don't have to guess anymore how the system works, how people use it, how you might go about attacking it because someone else actually does this for you. Like a company like OpenAI tells you this is the tool we're trying to build. This is our threat model. This is what the tool is supposed to do. This is what it's not supposed to do. And if you can then go and say, well, I made the model do what it's not supposed to do, you don't really have to spend all this time like arguing about hypotheticals of whether what you're doing is is relevant or or real. It kind of is by definition. And so in that sense, from a researcher's perspective, this has been very nice that it's made it much easier to sort of come up with concrete real problems for us to work on. The downside is that now when we find flaws in these systems, and we still do, all of a sudden things are a lot more real. You actually have in your hands maybe some exploit that could impact tools that are used by millions of people. We have to think a bit harder about how we go about doing this kind of research, the ethics behind it, how we might disclose vulnerabilities to whom, how we might go about patching these things. And so it's become an area of research that I think has gotten closer to traditional computer security in that sense. The machine learning community kind of wasn't necessarily ready for this. And there's maybe a whole bunch of lessons from computer security that the machine learning community will have to relearn over the years. But yeah, it's definitely made the whole work a lot of fun.\n\nIt's also kind of amazing now that and I guess this is true for everyone working in AI these days that it's just a a topic that everyone cares about, everyone has heard about. Last year at some point my my landlord came up to me and asked me about ChatGPT and for me this was just like the the turning point, you know, of I mean he he was a tech guy so maybe maybe it wasn't that surprising, but still it was kind of the the first time really that I started having people from everywhere in my life like family, friends suddenly talking about things that were very close to to my research and also the fact that I could actually talk about my research in rather open and simple terms with people and not get sort of blank stares back of like what the hell are you doing? But actually like, oh okay, this this I kind of understand and yeah, this sounds scary. And so yeah, in that sense it's a very, very cool period I think for AI security research, but also a scary one because people are deploying crazy new technology left and right that we we don't really know how to how to safeguard.\n\nIn some sense with machine learning, whatever task we've looked at over the past decades, we've sort of we've gotten better and better at getting models that work sort of on average. Maybe before ChatGPT, we would have had language models that, you know, predict the right word only 50% of the time, which gives you something complete gibberish. Now with ChatGPT, it might be 99% of the time or 99.9% of the time, which is good enough to generate hundreds or maybe thousands of tokens that that are coherent, but it's still never 100%. I think we need something else than just scale to to make this go away. That I think with scale and with more data, we're going to be fitting increasingly complex sort of functions that are sort of going to cover maybe at some point an extra nine, an extra nine of reliability, but that you're never going to be able to fit everything. And that there's somehow the way that we learn about problems and how to solve them is sort of not by just throwing more and more and more and more data at it. And there's probably something else that causes us to have some kind of causal understanding of of the world. And my guess is that scale alone will not bring us that. The caveat here is that if you had asked me a couple of years ago if scale alone would give us something like ChatGPT, I would have also said no. And so I'm I'm kind of aware of my own limitations here when it comes to predicting how how fast AI can or sort of how how far we can get with with what we're currently doing.\n\nYou said something interesting and that is that you're not bullish on watermarking. Could you actually let us in why?\n\nSo two two reasons. Uh, one which is which is relatively basic is just that many models that people use that can do bad stuff are open source and if the model is open source you can do with it whatever you want and you you can't really embed a watermark. I mean there are some proposals sort of for doing some kind of backdoors, but essentially once the model is on your machine you can do whatever you want with it. Nothing's going to stick with closed source models. We have nice proposals sort of that that give you watermarks that give you nice statistical guarantees and I think these watermarks will be useful for for example just filtering, you know, the training data of GPT5 to make sure that you're not including a whole bunch of outputs from GPT4. So kind of on average I feel like these these things probably will will work reasonably well. The problem is that as with any kind of ML system it's just not going to be particularly robust. And so if I take a text that's been output by a watermarked language model, maybe maybe I ask the model to write the text in German and then I feed it through Google Translate to put it in English and uh voila, watermark gone. You you sometimes have to be a little bit more careful like some of these watermarks can be relatively robust, but in the end you can you can do whatever you want with this text. And the same goes for images. I mean there there have been some nice proposals from DeepMind, actually, Ilia, I think has been involved in some of these as well, both for images and text, but if you read the paper, it's literally somewhere they say like the watermark is somewhat robust, whatever that means. And yeah, what it I guess what it does mean is that if you're going to try sort of very simplistic low resource type edits, like you you just try to to paraphrase some of your text or you you replace a few words here and there, it's going to be robust to that. Um, it's not going to robust to everything. And if you actually care about using these watermarks for something that's adversarial, like detecting, say if some if something is is just some some deep fake, we just don't really have a robust way of doing that.\n\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.140Z"
}