{
  "episodeId": "9W_OhS38rIE",
  "channelSlug": "@machinelearningstreettalk",
  "title": "We have a problem with ChatBot Arena.",
  "publishedAt": "2025-06-08T00:05:34.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "We have a serious problem with chatbot",
      "offset": 0.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "arena. For those of you at the coal",
      "offset": 2.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "phase thinking about how good language",
      "offset": 4.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "models really are compared to each",
      "offset": 6.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "other, you've almost certainly heard of",
      "offset": 7.839,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 9.84,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "arena. At this point, indisputably the",
      "offset": 13.08,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "supreme king of rankings and widely",
      "offset": 16.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "perceived as an accurate reflection of",
      "offset": 19.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "model capabilities. As the Wall Street",
      "offset": 21.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Journal described it, it's the AI",
      "offset": 23.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "industry's obsession. a place where, as",
      "offset": 25.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Bloomberg observed, vast sums of money",
      "offset": 28.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "appear contingent on a model's sudden",
      "offset": 30.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "climb in the",
      "offset": 33.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "rankings. It's possible that today I",
      "offset": 35,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "might just disabuse you of any notion of",
      "offset": 37.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "fairness, especially when you hear about",
      "offset": 40.48,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the explosive paper just released, which",
      "offset": 42.399,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "proved it's nothing of the sort. We'll",
      "offset": 44.879,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "also tell you about the recent debacle",
      "offset": 47.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "over Llama 4 and how blatantly",
      "offset": 48.879,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Zuckerberg admitted to hacking chatbot",
      "offset": 51.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "arena. Google IO just wrapped and they",
      "offset": 53.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "headlined with their solidified position",
      "offset": 56.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "at the top of the leaderboard for Gemini",
      "offset": 58.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "2.5 Pro and Flash. Gemini genuinely is",
      "offset": 60.719,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "an amazing model, but did they achieve",
      "offset": 63.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "these results fairly? Yesterday, Chatbot",
      "offset": 65.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Arena secured a $100 million investment",
      "offset": 68.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "from Andre Horovitz and UC Investments,",
      "offset": 71.28,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "taking them to a valuation of $600",
      "offset": 74,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "million. There's a lot at stake in this",
      "offset": 77.56,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "industry. billions of dollars, greed,",
      "offset": 79.759,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "power, and deception in this cutthroat",
      "offset": 82.4,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "race for AI",
      "offset": 85.28,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "dominance. It's interesting, isn't it,",
      "offset": 90.759,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "that sometimes chatting with a language",
      "offset": 92.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "model about some specific thing for 5",
      "offset": 94.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "minutes gives you more information than",
      "offset": 96.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "benchmarks do about real world",
      "offset": 98.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "performance, certainly for things that",
      "offset": 100.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you're interested in. benchmarks are",
      "offset": 102.32,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "saturating and in many cases don't seem",
      "offset": 104.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to actually capture the essence of the",
      "offset": 107.439,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "thing that they're purporting to",
      "offset": 109.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "capture. Why is that? Mark Zuckerberg",
      "offset": 110.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "was recently on the Dvaresh podcast and",
      "offset": 113.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "he admitted completely that they had",
      "offset": 115.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "hacked chatbot Arena by creating lots of",
      "offset": 118.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "private models, selecting the best ones,",
      "offset": 122,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and even fine-tuning those models on the",
      "offset": 124.32,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "arena data. This is what Mark Zuckerberg",
      "offset": 126.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "said like Sonnet 37. It's like a great",
      "offset": 128.879,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model, right? and it's it's like not",
      "offset": 131.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "near the top. Um, and it was relatively",
      "offset": 132.879,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "easy for our team to tune a version of",
      "offset": 135.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Llama 4 Maverick. Um, that basically was",
      "offset": 137.84,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "way at the top. Um, whereas the one that",
      "offset": 140.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we released that's the the kind of the",
      "offset": 142.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "pure model actually has no tuning for",
      "offset": 145.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that at all. So, it so it's further",
      "offset": 147.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "down. And sometimes these things just",
      "offset": 148.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "don't quite line up. And and I think",
      "offset": 149.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that a lot of them are are quite easily",
      "offset": 151.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "um gameable. they're often skewed for a",
      "offset": 153.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "either a very specific um set of use",
      "offset": 156.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "cases which are often not actually what",
      "offset": 159.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "any normal person does in in in your",
      "offset": 161.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "product. He said plainly that we were",
      "offset": 163.92,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "fine-tuning for the arena benchmark,",
      "offset": 166.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "right? They had no intention of",
      "offset": 169.519,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "publishing that model. They were simply",
      "offset": 171.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "gaming the score. Why was that allowed",
      "offset": 173.599,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "to happen? So, um just imagine how Coher",
      "offset": 176,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "must have felt, right? They're a",
      "offset": 180.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "frontier model company and they noted",
      "offset": 181.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that their models weren't doing very",
      "offset": 183.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "well on chatbot arena. This is Nick",
      "offset": 185.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Frost. I do think that a person's",
      "offset": 188,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "ability to look at a model and say this",
      "offset": 189.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "one's better than that one based on my",
      "offset": 191.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "brief conversations with it. And we've",
      "offset": 193.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "seen I mean a bunch of research has come",
      "offset": 195.92,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "out on this recently showing the effect",
      "offset": 197.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the preamles have on ELO ranking showing",
      "offset": 198.879,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the the like slight formatting changes",
      "offset": 201.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "make a huge difference in people's",
      "offset": 203.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "perception of how good the model is. The",
      "offset": 205.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "headline of this paper essentially is",
      "offset": 207.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that chatbot arena itself fell victim to",
      "offset": 208.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "good heart's law which meant that many",
      "offset": 211.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "of the models are optimized to do well",
      "offset": 213.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "on the arena and not actually be capable",
      "offset": 215.599,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "AI",
      "offset": 218.239,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "models. Weirdly seeing a model",
      "offset": 220.36,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "understand some specific thing that you",
      "offset": 222.799,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "ask it is the best test of",
      "offset": 224.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "generalization for you. This is known as",
      "offset": 226.28,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "passing the vibes test.",
      "offset": 229.12,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "This is related to the reason why humans",
      "offset": 234.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are naturally creative when we write and",
      "offset": 236.56,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "speak because there's a very good chance",
      "offset": 239.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that any thought that comes into your",
      "offset": 240.879,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "mind while you're uniquely situated is",
      "offset": 242.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "genuinely novel in at least some small",
      "offset": 245.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and important way. Andre Kapathy",
      "offset": 247.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "recently put out a tweet talking about",
      "offset": 250,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "his own experience with the GPT series",
      "offset": 251.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of models going all the way back to GPT2",
      "offset": 253.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and he said that the differences as we",
      "offset": 256.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "went up in model versions became more",
      "offset": 258.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and more subtle. This is what he said.",
      "offset": 260.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Now, recall that GPT1 barely generated",
      "offset": 263.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "coherent text. GPT2 was a confused toy.",
      "offset": 266.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "GPT 2.5 was skipped straight on to GPT3,",
      "offset": 269.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "which was even more interesting. GPT3.5",
      "offset": 272.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "crossed the threshold where it was",
      "offset": 275.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "enough to actually ship as a product and",
      "offset": 277.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sparked the OpenAI's chat GPT moment.",
      "offset": 278.8,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "And GPT4 in turn also felt better, but",
      "offset": 281.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I'll say or him saying this of course",
      "offset": 284.479,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that it definitely felt more subtle. He",
      "offset": 286.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "said he remembers being part of a",
      "offset": 288.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "hackathon trying to find concrete",
      "offset": 289.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "prompts for GPT4 which clearly",
      "offset": 291.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "outperformed 3.5. He said they",
      "offset": 292.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "definitely existed but clear and",
      "offset": 295.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "concrete slam dunk examples were",
      "offset": 297.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "difficult to find. It's that everything",
      "offset": 299.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is just a little bit better but in a",
      "offset": 301.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "diffused way. The word choice was a bit",
      "offset": 303.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "more creative. Understanding of nuance",
      "offset": 306,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "in the prompt was improved. Analogies",
      "offset": 308.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "made a bit more sense. The model was a",
      "offset": 311.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "little bit funnier. World knowledge and",
      "offset": 313.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "understanding was improved at the edges",
      "offset": 315.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of rare domains. Hallucinations were a",
      "offset": 317.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "bit less frequent. The vibes were just a",
      "offset": 320.479,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "little bit better. End quote. He was",
      "offset": 323.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "testing GPT 4.5 and he said he expected",
      "offset": 326.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "or perhaps he wanted the performance to",
      "offset": 329.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "be better. It wasn't, by the way. I",
      "offset": 331.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "mean, it had to be right. It it was 10",
      "offset": 333.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "times bigger. He put a test together of",
      "offset": 336,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "examples a bit like a mini chatbot arena",
      "offset": 338.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and he invited his followers to vote on",
      "offset": 341.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Twitter. He was horrified to discover",
      "offset": 343.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that in four out of five cases, people",
      "offset": 344.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "thought the older, smaller four row",
      "offset": 347.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "model was better. His response was even",
      "offset": 349.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "more interesting. He said, &quot;To be",
      "offset": 351.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "honest, I found this a bit surprising as",
      "offset": 353.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "he had personally found GPT4.5 responses",
      "offset": 355.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to be better in all cases.&quot; He said",
      "offset": 358.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "maybe he's just in quotes a high taste",
      "offset": 360.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tester. He said the thing to look for is",
      "offset": 363.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that GPT4 more often says stuff that on",
      "offset": 365.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the face of it looks fine, but type",
      "offset": 368.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "checks as making sense. But if you",
      "offset": 370.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "really think about it longer and more",
      "offset": 371.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "carefully, you'll more often catch it",
      "offset": 373.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "saying things that are just a bit of an",
      "offset": 375.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "odd thing to say or a little bit too",
      "offset": 376.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "formulaic, a little bit too basic, a",
      "offset": 378.72,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "little bit too cringe, or a little bit",
      "offset": 380.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "too tropey. End quote. So, in not so",
      "offset": 383.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "many words, Kapathy argued that he was",
      "offset": 385.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "smart enough to recognize that the model",
      "offset": 387.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "was better. I ran my own subjective",
      "offset": 389.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "evaluation of all of this with the",
      "offset": 391.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Frontier models at the time in response.",
      "offset": 392.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "And while GPT 4.5 was slightly less",
      "offset": 395.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "sloppy than four row, it was still very",
      "offset": 397.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "sloppy indeed. In April 2023, two",
      "offset": 399.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Berkeley PhD students, Anastasios",
      "offset": 402.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Angelopoulos and Waying Chiang, couldn't",
      "offset": 404.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "get a straight answer from an existing",
      "offset": 407.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "LLM benchmark. So, in a single week,",
      "offset": 409.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "they hacked together a website where any",
      "offset": 412.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "visitor could pit two anonymous chat",
      "offset": 414.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "bots against each other and swipe right",
      "offset": 416.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "or left on their favorite answer.",
      "offset": 418.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Basically, Tinder for chat bots, at",
      "offset": 421.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "least from a front-end perspective. I'm",
      "offset": 423.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "glossing a little bit here, but roughly",
      "offset": 425.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "speaking, the user visits the site and",
      "offset": 427.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is blindly matched with two large",
      "offset": 429.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "language models which are selected via a",
      "offset": 430.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "selection process, which we'll get to a",
      "offset": 432.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "bit later. The user types any prompt",
      "offset": 435.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "they want into the text box via, you",
      "offset": 437.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "know, it could be a question about chaos",
      "offset": 439.919,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "theory and predicting weather cycles or",
      "offset": 441.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "whether Schmid Huba really did invent",
      "offset": 443.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the transformer in the 1990s. They see",
      "offset": 445.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "both of the responses streamed out",
      "offset": 448.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "either side of the screen and they make",
      "offset": 449.599,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "their choice about which is best. The",
      "offset": 451.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "models don't know they're being compared",
      "offset": 453.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and the users don't know what the models",
      "offset": 454.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "are. Although the latter part of that",
      "offset": 456.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is, yeah, a little bit iffy. When you",
      "offset": 458.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "sit there watching the token slowly",
      "offset": 461.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "dribble out like it's 1998, you tend to",
      "offset": 463.28,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "notice things. The speed for one. Some",
      "offset": 466.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "models have a really distinctive turgid",
      "offset": 468.479,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "voluable idiosyncratic language",
      "offset": 471.52,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "construction output. You know, the Open",
      "offset": 473.919,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "AI ones in particular produce grotesque",
      "offset": 475.759,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "brickarages of slop. And I don't just",
      "offset": 478.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "mean superficial things like delve or em",
      "offset": 480.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "dashes. There's just so many subliminal",
      "offset": 482.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "stylistic features like the the",
      "offset": 484.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "verbosity, the vagueness, the heading",
      "offset": 486.72,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "structure. So when the user executes the",
      "offset": 489.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "metaphorical Tinder swipe, you know, one",
      "offset": 491.56,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "more match is logged into a giant",
      "offset": 493.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "ratings ladder. The arena treats every",
      "offset": 495.759,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "battle like a chess game. The winner",
      "offset": 498.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "gains elo points, the loser loses them.",
      "offset": 500.479,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "A stronger model beating a weaker model",
      "offset": 503.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "yields only a very small bump. An upset",
      "offset": 505.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "yields a big bump. There was a boon",
      "offset": 508.4,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "right on the surface at least. It was",
      "offset": 510.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "untrableled by the pitfalls of",
      "offset": 512.399,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "traditional benchmarks like MMLU and it",
      "offset": 514.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "amassed a cult following an active",
      "offset": 517.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "community. In the early days it was",
      "offset": 518.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "mostly sort of like open source models",
      "offset": 520.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you've never heard of like Vuna 13B",
      "offset": 523.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "which quickly topped the leaderboard",
      "offset": 525.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "with um 1167 ELO points I think it was",
      "offset": 527.6,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "after 4,700 blind votes. MMLU as a",
      "offset": 532.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "reference point was multiplechoice",
      "offset": 535.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "questions across 57 subjects which all",
      "offset": 537.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of the models had obviously been trained",
      "offset": 540.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "on. You know, everything from high",
      "offset": 542.08,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "school biology and US history and",
      "offset": 543.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "electrical engineering, even law. By",
      "offset": 545.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "July of that year, the little side",
      "offset": 548.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "project was hosting 22 models and uh",
      "offset": 550,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "Claude 1 and GBT 3.5 were snuck in and",
      "offset": 553.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "there were 53,000 votes in the system.",
      "offset": 556.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "They even uploaded the raw conversation",
      "offset": 560,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and data uh to hugging face for research",
      "offset": 561.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "purposes. That might have ended up being",
      "offset": 564.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "a bad idea for reasons which need not",
      "offset": 566.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "detain us, but we'll get to that. On the",
      "offset": 568.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "subject of the perils of chatbot Arena,",
      "offset": 571.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you may well work in a large enterprise",
      "offset": 573.839,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and want to build something similar to",
      "offset": 575.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "chatbot Arena yourself. I mean, let's",
      "offset": 576.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "face it, the magic is access to a",
      "offset": 578.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "diverse pool of humans for annotation.",
      "offset": 580.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Scaling this up often turns into a",
      "offset": 584.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "nightmare, not to mention all the",
      "offset": 585.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "control headaches. We are sponsored by",
      "offset": 587.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Prolific. We're proud to be sponsored by",
      "offset": 589.519,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "them and they have built an API",
      "offset": 591.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "precisely for this. You can automate",
      "offset": 594.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "task distributions. You can manage",
      "offset": 596.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "participant pools and you can maintain",
      "offset": 599.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "data quality standards as your project",
      "offset": 601.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "grows without having to build all the",
      "offset": 603.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "infrastructure yourself. Get details at",
      "offset": 606,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "prolific.com.",
      "offset": 608.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "The ELO rating has been used to rank",
      "offset": 610.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "chess players by the international",
      "offset": 612.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "community for over 60 years and they can",
      "offset": 613.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "be computed online and asynchronously",
      "offset": 616.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "from results of chess games played",
      "offset": 618.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "around the world. And it doesn't assume",
      "offset": 621.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that the skill of each player is fixed.",
      "offset": 623.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "ELO is really simple. It just takes the",
      "offset": 625.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "difference in scores between players",
      "offset": 628.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "after a match. It runs it through the",
      "offset": 630.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "sigmoid function which converts it into",
      "offset": 632.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a smooth probability between not and",
      "offset": 634.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "one. you subtract the actual result",
      "offset": 636,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "which is represented as 1 0.5 or not and",
      "offset": 638.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "then you add that onto the scores",
      "offset": 641.519,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "multiplied by a learning rate K. In the",
      "offset": 643.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "real world of chess the games just",
      "offset": 646.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "naturally arise. In something more",
      "offset": 648.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "contrived like chatbot Arena the",
      "offset": 650.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "sampling strategy has a big impact. We",
      "offset": 652.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "get to the end of 2023 and now the arena",
      "offset": 655.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "has registered more than 130,000 votes.",
      "offset": 657.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Users started complaining that the",
      "offset": 660.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "scores were unstable. They were wobbling",
      "offset": 662.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "around all over the place which made",
      "offset": 664.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "them seem sketchy at best. Sarah Hooker",
      "offset": 666.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and Mazia Faradia's team at Coher had",
      "offset": 669.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "previously pointed out this exact issue",
      "offset": 672.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to the arena in a paper saying that the",
      "offset": 674.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "order in which you compare the models",
      "offset": 676.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "can significantly change the final ELO",
      "offset": 678.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "scores. They basically said, and I'm",
      "offset": 681.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "paraphrasing, ELO was designed for",
      "offset": 683.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "things like chess where their skills",
      "offset": 685.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "change over time. LLMs once trained have",
      "offset": 688,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "a fixed skill level. So, are we using",
      "offset": 690.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "ELO correctly and are the rankings",
      "offset": 692.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "reliable? They noted that transitivity",
      "offset": 695.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "wasn't guaranteed, which basically means",
      "offset": 697.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "assuming that the rankings are a",
      "offset": 699.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "consistent way of making inferences",
      "offset": 701.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "about model strength, especially when",
      "offset": 703.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "models were close in performance. They",
      "offset": 705.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "also said that the ELO algorithm had a",
      "offset": 707.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "weird sensitivity to its learning rate,",
      "offset": 709.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "which could skew the results a fair bit.",
      "offset": 711.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Cohhere argued for adapting the learning",
      "offset": 713.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "rate dynamically and shuffling the order",
      "offset": 716,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of comparisons. Coher were trying to",
      "offset": 718.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "keep the arena honest. And keep this in",
      "offset": 720.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "mind as it will become important later",
      "offset": 722.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in the story. To improve the quality of",
      "offset": 724.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the arena's rankings and confidence",
      "offset": 726.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "estimates, they adopted another widely",
      "offset": 728,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "used rating system which is called the",
      "offset": 730.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "Bradley Terry model. Like the ELO",
      "offset": 732,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "rating, the Bradley Terry model uses",
      "offset": 734.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "pair-wise comparisons to derive ratings",
      "offset": 735.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and estimate win rates between players.",
      "offset": 738.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "The main difference is that it assumes",
      "offset": 740.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "player performance remains constant,",
      "offset": 742.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "meaning that the game order doesn't",
      "offset": 744.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "matter and it performs calculations all",
      "offset": 746,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "at once. Many are excited about models",
      "offset": 748.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "gaining in capabilities. In fact,",
      "offset": 750.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there's an old adage called the Mccord",
      "offset": 752.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "effect which says that every time a new",
      "offset": 754.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "capability is reached, there's a chorus",
      "offset": 756.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of people that exclaim, &quot;This isn't",
      "offset": 758.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "really thinking.&quot; Almost as if the",
      "offset": 760.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "goalposts keep getting moved. When a",
      "offset": 762.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "capability becomes disappointingly",
      "offset": 764.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "simple to mechanize, our perception",
      "offset": 766.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "widens as if it has to be",
      "offset": 768.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "indistinguishable from magic. But I",
      "offset": 770,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "would like to propose the David Shapiro",
      "offset": 772.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "effect, which is that when a model takes",
      "offset": 774.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "the leadership position on chatbot",
      "offset": 776.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Arena, many of course including David,",
      "offset": 777.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "sorry, David, will exclaim AGI is here",
      "offset": 780.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "or nearly here. You know, we might as",
      "offset": 783.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "well just pack up our bags and prepare",
      "offset": 784.8,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "for the post labor",
      "offset": 786.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "economy. This brings us to Goodart's",
      "offset": 788.76,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "law. There was a principle articulated",
      "offset": 791.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "decades ago by the economist Charles",
      "offset": 793.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Goodhart. I'm sure you've heard of him,",
      "offset": 795.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "by the way, because we constantly",
      "offset": 797.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "reference it on almost every single",
      "offset": 798.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "show. He said, &quot;Any observed statistical",
      "offset": 800.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "regularity will tend to collapse once",
      "offset": 802.72,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "pressure is placed upon it for control",
      "offset": 804.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "purposes.&quot; Or more simply, when a",
      "offset": 807.24,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "measure becomes a target, it ceases to",
      "offset": 809.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "be a good measure. Goodart's law, while",
      "offset": 812.079,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "simple on the surface, it rears its ugly",
      "offset": 815.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "head in almost every facet of machine",
      "offset": 817.12,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "learning. When we pursue a complex goal",
      "offset": 819.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "such as providing quality education or",
      "offset": 822.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "developing truly general artificial",
      "offset": 824.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "intelligence, direct measurement is",
      "offset": 827.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "often impossible. So what do we do?",
      "offset": 829.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Right? We use a proxy, a measurable",
      "offset": 832.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "metric which we hope accurately reflects",
      "offset": 834.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the desired goal. Standardized test",
      "offset": 836.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "scores act as a proxy for education.",
      "offset": 839.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Chatbot arena ELO scores act as a proxy",
      "offset": 841.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "for AI capability. Initially, progress",
      "offset": 844.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "on the proxy correlates with progress",
      "offset": 846.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "towards the actual goal. Rising test",
      "offset": 848.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "scores might genuinely signify better",
      "offset": 851.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "educational outcomes. But once the proxy",
      "offset": 853.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "itself becomes the primary target, the",
      "offset": 855.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "metric everyone is incentivized to",
      "offset": 858.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "maximize, it inevitably loses its",
      "offset": 860.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "reliability as any indication of the",
      "offset": 863.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "original objective. If schools face",
      "offset": 865.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "intense pressure based on test results,",
      "offset": 868.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what's going to happen, right? Kids are",
      "offset": 870.88,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "just going to learn the tests and",
      "offset": 872.399,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "they're not going to develop any",
      "offset": 873.6,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "critical analysis or creative",
      "offset": 874.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "exploration. The proxy, which is the",
      "offset": 877.24,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "test scores, demonstrabably improve",
      "offset": 879.6,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "while the intended goal at best",
      "offset": 882,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "stagnates, at worst",
      "offset": 884.6,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "regresses. Quick pause. Getting language",
      "offset": 887.399,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "models to the upper etchelins of chatbot",
      "offset": 890.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "arena the old-fashioned way, not using",
      "offset": 893.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the Zuckerberg strategy is really,",
      "offset": 895.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "really difficult. And that's why",
      "offset": 897.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Benjamin Kruier at Two For AI Labs is",
      "offset": 899.519,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "hiring cracked MO engineers in Zurich",
      "offset": 901.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "and in San Francisco. So if that sounds",
      "offset": 904.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "like you and you didn't work on the",
      "offset": 907.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Llama 4 team, like slightly half joking,",
      "offset": 909.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "uh get in touch with Benjamin Kruier, go",
      "offset": 912.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "to twoferabs.ai.",
      "offset": 914,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Okay, so researchers at cohhere just",
      "offset": 916.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "released this explosive paper called the",
      "offset": 918,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "leaderboard illusion about chatbot arena",
      "offset": 920.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "of course uh lead author Shivalika Singh",
      "offset": 923.279,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and also Mazia Fadia and Sarah Hooker",
      "offset": 925.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and a whole bunch of other folks were",
      "offset": 928.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "involved um you'll recognize Sai Kapoor",
      "offset": 930.16,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of course he was on the show recently on",
      "offset": 932.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "page three there's an interesting plot",
      "offset": 934.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "here and it shows the relationship",
      "offset": 936.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "between the number of models submitted",
      "offset": 938.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "on the arena and the maximum score and",
      "offset": 940.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the size of the bubbles is how many",
      "offset": 943.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "battles have been generated. So some",
      "offset": 945.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interesting correlations of course you",
      "offset": 948,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "can see that the more uh you know models",
      "offset": 949.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we test the higher the score and the",
      "offset": 952.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "more battles the higher the score. Now",
      "offset": 954.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the reason why you get a higher score",
      "offset": 956.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "with more battles cohhere argue is",
      "offset": 958.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "because every time a battle is generated",
      "offset": 960.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you get juicy data and you can fine-tune",
      "offset": 963.36,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "on that data to be even better at",
      "offset": 966.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "chatbot arena going forwards. Another",
      "offset": 969.279,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "stark observation they found straight",
      "offset": 971.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "away, they had access to a few months of",
      "offset": 973.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "arena data, was that proprietary models",
      "offset": 975.04,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "are sampled significantly more than",
      "offset": 978,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "open-source and open weights models. An",
      "offset": 980.959,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "absolutely huge disparity. I mean, in",
      "offset": 983.44,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "quarter 1 of 2025, nearly 70% of the of",
      "offset": 985.12,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "the battles were going to proprietary",
      "offset": 989.839,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "models and only 23.1 and 9.2 to open",
      "offset": 992.32,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "weights and open- source models. So",
      "offset": 997.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that's an absolutely massive disparity.",
      "offset": 999.279,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "Why is",
      "offset": 1001.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that? Let's get to their main",
      "offset": 1002.92,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "conclusions here. So number one,",
      "offset": 1005.199,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "preferential treatment around private",
      "offset": 1006.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "testing and attraction. So they said",
      "offset": 1008.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that the arena has an unstated policy of",
      "offset": 1010.48,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "allowing select providers to test many",
      "offset": 1013.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "submissions privately and in parallel.",
      "offset": 1015.88,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "They said that certain model developers,",
      "offset": 1019.12,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "notably Meta, uh, Google, OpenAI, and",
      "offset": 1021.279,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "Amazon have benefited from extensive",
      "offset": 1024.959,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "private testing. In a single month, they",
      "offset": 1027.36,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "observed as many as 27 models from Meta.",
      "offset": 1030,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "27 models uh, being tested privately on",
      "offset": 1034.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "Chatbot Arena. Uh, in the leadup to the",
      "offset": 1037.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Llama 4 release, which was an absolute",
      "offset": 1040.079,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "debacle. Incredibly, Chatbot Arena, even",
      "offset": 1042.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "if you're testing a private model, they",
      "offset": 1045.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "do not require that you publish the",
      "offset": 1048,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "private models. The providers can simply",
      "offset": 1050.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "retract models that they don't like and",
      "offset": 1053.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "only choose to publish the best models",
      "offset": 1055.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "from the private pool. So, not only are",
      "offset": 1057.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "they testing more models, they can",
      "offset": 1059.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "selectively publish the best ones, they",
      "offset": 1061.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "can grab as much data from the arena",
      "offset": 1064.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "users as possible and fine-tune on it.",
      "offset": 1066.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "This is incredibly unfair. And they also",
      "offset": 1068.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "point out in the background that the",
      "offset": 1071.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "private providers are just even without",
      "offset": 1072.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "private testing being generated more",
      "offset": 1074.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "battles than the open source and the",
      "offset": 1076.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "open weights models. Does that sound",
      "offset": 1078.559,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "fair to",
      "offset": 1080.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you? So they also say number two far",
      "offset": 1081.32,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "more data is released to proprietary",
      "offset": 1084,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "model providers. They say chatbot arena",
      "offset": 1085.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is a communitydriven leaderboard that",
      "offset": 1088.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "benefits from free crowdsourced feedback",
      "offset": 1089.679,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "provided by everyday users. However,",
      "offset": 1092,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "proprietary models uh collect",
      "offset": 1094.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "significantly more test prompts and",
      "offset": 1096.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "model battle outcomes than others. But",
      "offset": 1098.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "coher is showing an example here of you",
      "offset": 1100.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "could have model family A and B and",
      "offset": 1102.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "model family A might have a lower",
      "offset": 1105.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "average than B yet if you you know",
      "offset": 1106.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "choose a sample which is on one of the",
      "offset": 1109.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "tails of the distribution you can",
      "offset": 1111.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "actually make A or B better just having",
      "offset": 1114,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "access to arena data to fine-tune on is",
      "offset": 1116.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "really important and they actually had",
      "offset": 1118.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "access to a load of arena data because",
      "offset": 1120.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "if you think about it they run the API",
      "offset": 1122.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "endpoint for cohhere so hundreds of",
      "offset": 1124.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "thousands of people had been hitting",
      "offset": 1126.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "their API endpoint which means they",
      "offset": 1127.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "could take that data, they could",
      "offset": 1129.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "fine-tune on it and they could just see",
      "offset": 1130.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "how much of an of an uplift does it give",
      "offset": 1132.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "you if you fine-tune on that data. The",
      "offset": 1134.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "answer is a huge uplift. So just the",
      "offset": 1137.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "model against itself on, you know,",
      "offset": 1140.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "trained or fine-tuned with the arena",
      "offset": 1142.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "mix, it went from 50% win rate win rate",
      "offset": 1144.16,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "up to",
      "offset": 1147.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "79.2 win rate um with a 70% arena mix in",
      "offset": 1148.2,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "in the in the finetune, which is",
      "offset": 1152.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "massive. They also say number three,",
      "offset": 1154.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "chatbot arena data access drives",
      "offset": 1157.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "significant performance gains. So they",
      "offset": 1159.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "said that they estimate that by training",
      "offset": 1162.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on chatbot arena data model ranking can",
      "offset": 1163.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "be improved significantly. And in a",
      "offset": 1166.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "controlled experiment setting they",
      "offset": 1168.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "observed that increasing the arena",
      "offset": 1169.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "training data mix from naugh to 70% more",
      "offset": 1171.28,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "than doubles the win rates from 23.5% to",
      "offset": 1174.32,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "49.9% which is an absolutely massive",
      "offset": 1178.679,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "difference. And finally, they say that",
      "offset": 1181.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "deprecations can result in unreliable",
      "offset": 1183.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model ranking. And they have a list of",
      "offset": 1185.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "officially deprecated models. So these",
      "offset": 1188,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "are models that we know won't be",
      "offset": 1189.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "generating any more battles, but 205",
      "offset": 1191.28,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "have been silently deprecated. That",
      "offset": 1194.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "means they're still officially on the",
      "offset": 1196.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "leaderboard, but no samples or no",
      "offset": 1198.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "battles are being generated for those",
      "offset": 1200.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "models. And this actually has some very",
      "offset": 1202.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "serious ramifications because on the",
      "offset": 1204.96,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "Bradley Terry model, it requires",
      "offset": 1206.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "transitivity, right? which means that",
      "offset": 1209.559,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "the entire comparison graph needs to be",
      "offset": 1211.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "densely sampled. If you sparify that",
      "offset": 1214.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "graph, you get islands, right? And that",
      "offset": 1216.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "means it just doesn't work anymore",
      "offset": 1219.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "because if you think about it, the data",
      "offset": 1221.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "distribution is changing. Like the the",
      "offset": 1223.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "kind of prompts that users put into the",
      "offset": 1226.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "models are changing and if you're still",
      "offset": 1228.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "making assumptions on the model's",
      "offset": 1230.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "performance based on data that you had",
      "offset": 1232.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "many many months ago, you're going to",
      "offset": 1234.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "create an inconsistent skill ranking.",
      "offset": 1236.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And that's a big problem. They also show",
      "offset": 1238.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "here the maximum observe observed sample",
      "offset": 1240.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "rates for models for from different",
      "offset": 1243.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "providers. And you can see here that",
      "offset": 1245.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "Google and Open AI are overwhelmingly",
      "offset": 1246.88,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "sampled more than any of the other",
      "offset": 1249.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "providers. It's it's stark. Why should",
      "offset": 1252.2,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "that be the case? Yeah, it's almost it's",
      "offset": 1255.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "almost like a a comedy sketch. You know,",
      "offset": 1258.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you can imagine showing up for a",
      "offset": 1260,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "competition to win a million dollars by",
      "offset": 1262.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "making a uh a basketball shot from from",
      "offset": 1264.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the three-point line on the other side",
      "offset": 1268.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of the court. And you you take the shot",
      "offset": 1270.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and you brick and you're like, &quot;Ah,",
      "offset": 1272.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "shoot, man. Oh, well, it was worth a",
      "offset": 1273.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "try.&quot; And then and then Tim walks up",
      "offset": 1275.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "there, he shoots, misses, they throw him",
      "offset": 1277.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "another ball, you'll be like, &quot;Huh,",
      "offset": 1279.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "dude, what's going on?&quot; And then they",
      "offset": 1281.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "give him 18 more. You know, what do you",
      "offset": 1283.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "do? Do you just uniformly sample pairs",
      "offset": 1285.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of matches? That's actually quite a dumb",
      "offset": 1287.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thing to do. What you should do is",
      "offset": 1289.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "sample based on uncertainty will",
      "offset": 1291.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "minimize the expected difference in the",
      "offset": 1294.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "confidence interval. Now the fascinating",
      "offset": 1296.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "thing is that the the the people who",
      "offset": 1299.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "created chatbot Arena, they know about",
      "offset": 1302.24,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "this. They actually wrote a paper in",
      "offset": 1304.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "2024 describing a sampling strategy",
      "offset": 1307.24,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "which does pretty much this. Yet they",
      "offset": 1310.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "don't implement it themselves. They have",
      "offset": 1313.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "some weird hard-coded rule and their",
      "offset": 1315.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "explanation for this is because the",
      "offset": 1318.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "users of chatbot arena just really like",
      "offset": 1320.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "playing with the latest and greatest",
      "offset": 1323.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "shiny toys and the big frontier models.",
      "offset": 1325.039,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So, let's just generate more battles for",
      "offset": 1327.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the frontier models. But that",
      "offset": 1330.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "unfortunately has a huge, you know,",
      "offset": 1331.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "confers a huge advantage for those uh",
      "offset": 1333.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "providers. So, what are their findings?",
      "offset": 1336.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Well, I think this is very reasonable to",
      "offset": 1338.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be honest. Number one is prohibit",
      "offset": 1340.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "retraction of scores after submission.",
      "offset": 1343.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "And this applies to just normal models",
      "offset": 1345.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that are on chatbot arena and the",
      "offset": 1347.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "private variants. If you submit a model",
      "offset": 1349.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "on there and it achieves a certain",
      "offset": 1351.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "score, you're not allowed to selectively",
      "offset": 1353.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "delete ones that you don't like. The",
      "offset": 1355.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "record needs to stay intact. Number two,",
      "offset": 1357.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "uh establish transparent limits on the",
      "offset": 1361.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "number of private models per provider.",
      "offset": 1362.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "So Meta doing 27 models in March. That",
      "offset": 1364.799,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "is absolutely ridiculous. It should be",
      "offset": 1367.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "something like three or maybe five. It",
      "offset": 1370.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "should be the same for everyone and it",
      "offset": 1372.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "should be transparent. They also say",
      "offset": 1374.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that um model removals are applied",
      "offset": 1376.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "equally to proprietary open weights and",
      "offset": 1379.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "open source models. Implement fair",
      "offset": 1381.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "sampling. So this is what I was saying",
      "offset": 1383.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "about the the the chess thing and how",
      "offset": 1384.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that what they should do is sample",
      "offset": 1386.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "models based on minimizing uncertainty",
      "offset": 1388.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "or or generating more matches between",
      "offset": 1391.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "models that have a similar rank. not",
      "offset": 1394,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "doing what they are doing which is just",
      "offset": 1396.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "generating battles for meta and Google",
      "offset": 1398.159,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "and XAI and the folks at the very top",
      "offset": 1402,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and finally providing transparent info",
      "offset": 1405.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "into which models are being removed from",
      "offset": 1407.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the leaderboard. So this is the",
      "offset": 1409.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "deprecation thing. Loads and loads of",
      "offset": 1410.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the models are basically dead which is",
      "offset": 1412.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "sparsifying this connectivity graph",
      "offset": 1414.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "between the models. They say that they",
      "offset": 1416.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "should be transparent about this.",
      "offset": 1418.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Completely reasonable. And then there's",
      "offset": 1421.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the question of how useful is it that we",
      "offset": 1423.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "are training on arena data? Because if",
      "offset": 1426.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you think about it, the whole reason why",
      "offset": 1428.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "arena is supposedly a good thing",
      "offset": 1429.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "compared to static benchmarks is that",
      "offset": 1431.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the data distribution is non-stationary.",
      "offset": 1433.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "It's changing all the time, uh, you",
      "offset": 1435.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "know, different people are using it,",
      "offset": 1437.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "people think of different prompts.",
      "offset": 1438.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "That's in theory. And it's kind of true",
      "offset": 1440.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and it's not at the same time. They did",
      "offset": 1442.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "some analysis here and and you can see",
      "offset": 1444.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "some macro trends certainly between the",
      "offset": 1446.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "languages that people are using and they",
      "offset": 1448.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "did some similarity analysis between",
      "offset": 1450.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "subsequent months of um chat you know",
      "offset": 1453.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "chatbot arena battle data. They have",
      "offset": 1455.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "access to this through various sources.",
      "offset": 1458.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Of course they they have it from the",
      "offset": 1460.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "cohhere endpoint and there are data sets",
      "offset": 1461.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "released and whatnot. And what they",
      "offset": 1463.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "found was that even though there were a",
      "offset": 1465.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "lot of differences, there was also a",
      "offset": 1467.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "startling amount of similarity. And",
      "offset": 1469.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "amazingly um something like between 25",
      "offset": 1471.76,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "and 33% of the prompts were very high",
      "offset": 1475.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "similarity. And this is given by if you",
      "offset": 1480,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "take the cosine of the embeddings and if",
      "offset": 1482.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it's greater than 0.95, it's high",
      "offset": 1483.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "similarity. And then um in some cases",
      "offset": 1485.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "I'd say roughly between 16% and 20% or",
      "offset": 1488.32,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "even 26.5 in in March um the prompts",
      "offset": 1492.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "were identical. And I've always found",
      "offset": 1495.919,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "this very curious. You know certainly",
      "offset": 1497.919,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "when you interview people when you talk",
      "offset": 1499.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "with your friends people are not that",
      "offset": 1500.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "creative. They just tend to be",
      "offset": 1503.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "interested in the same things and ask",
      "offset": 1504.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the same questions. And they said in the",
      "offset": 1506.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "paper people were asking questions about",
      "offset": 1508.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Star Trek and you know like the same",
      "offset": 1509.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "kind of logical puzzles and and stuff",
      "offset": 1512.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "like that. So surprisingly, you know,",
      "offset": 1514.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "roughly 50% of the data seems to be the",
      "offset": 1517.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "same or nearly the same every single",
      "offset": 1520.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "month. So if you could fine-tune on",
      "offset": 1522.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that, if you knew what that was, it",
      "offset": 1524.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "would confer an absolutely massive",
      "offset": 1526.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "advantage. Maybe the folks at Coher have",
      "offset": 1527.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "an agenda. Okay, I'm that's true. They",
      "offset": 1530.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have an agenda. The problem is so does",
      "offset": 1532.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "everybody else. So everybody has these",
      "offset": 1534.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "agendas and it's completely fair to call",
      "offset": 1537.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "out things like this happening and to",
      "offset": 1540.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "allow critical thinking to to take over,",
      "offset": 1543.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "right? And what I'm wondering is what",
      "offset": 1546.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "went on behind the scenes at the folks",
      "offset": 1548.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "behind LLM Arena to remotely ever think",
      "offset": 1550.64,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "this was a good idea. So there you have",
      "offset": 1554.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it. That has been the story of Chatbot",
      "offset": 1557.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Arena. It is an incredible benchmark. I",
      "offset": 1559.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "don't want to sound too negative. I",
      "offset": 1562.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "still think that it has quite a lot of",
      "offset": 1564.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "alpha as a benchmark, but they have some",
      "offset": 1565.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "serious problems. After um Sarah",
      "offset": 1568.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Hooker's team released that paper, um",
      "offset": 1570.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "she laid it all out on Twitter and she",
      "offset": 1573.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "went through all of the findings and the",
      "offset": 1575.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "way that the arena responded was a",
      "offset": 1577.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "little bit weird. They didn't really",
      "offset": 1579.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "pick up on a lot of the feedback from",
      "offset": 1581.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the paper and they were being a bit",
      "offset": 1583.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "evasive and then one week later the",
      "offset": 1585.279,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "investment was announced",
      "offset": 1588.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "which was a little bit weird. Um, I",
      "offset": 1590.919,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "think they did concede a couple of",
      "offset": 1594.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Sarah's points, but the blog post in",
      "offset": 1595.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "response really glossed over it quite a",
      "offset": 1597.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "bit. So, I very much hope that they will",
      "offset": 1600.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "take this feedback on board and make the",
      "offset": 1602,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "changes that the cohhere folks are",
      "offset": 1605.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "arguing for. Hope you've enjoyed the",
      "offset": 1607.279,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "show.",
      "offset": 1609.36,
      "duration": 3
    }
  ],
  "cleanText": "We have a serious problem with ChatBot Arena. For those of you at the coal phase thinking about how good language models really are compared to each other, you've almost certainly heard of the arena. At this point, indisputably the supreme king of rankings and widely perceived as an accurate reflection of model capabilities. As the Wall Street Journal described it, it's the AI industry's obsession, a place where, as Bloomberg observed, vast sums of money appear contingent on a model's sudden climb in the rankings. It's possible that today I might just disabuse you of any notion of fairness, especially when you hear about the explosive paper just released, which proved it's nothing of the sort. We'll also tell you about the recent debacle over Llama 4 and how blatantly Zuckerberg admitted to hacking ChatBot Arena. Google IO just wrapped and they headlined with their solidified position at the top of the leaderboard for Gemini 2.5 Pro and Flash. Gemini genuinely is an amazing model, but did they achieve these results fairly? Yesterday, ChatBot Arena secured a $100 million investment from Andre Horovitz and UC Investments, taking them to a valuation of $600 million. There's a lot at stake in this industry: billions of dollars, greed, power, and deception in this cutthroat race for AI dominance. It's interesting, isn't it, that sometimes chatting with a language model about some specific thing for 5 minutes gives you more information than benchmarks do about real-world performance, certainly for things that you're interested in. Benchmarks are saturating and in many cases don't seem to actually capture the essence of the thing that they're purporting to capture. Why is that? Mark Zuckerberg was recently on the Dvaresh podcast and he admitted completely that they had hacked ChatBot Arena by creating lots of private models, selecting the best ones, and even fine-tuning those models on the arena data. This is what Mark Zuckerberg said: \"Like Sonnet 37. It's like a great model, right? And it's, it's like not near the top. Um, and it was relatively easy for our team to tune a version of Llama 4 Maverick. Um, that basically was way at the top. Um, whereas the one that we released, that's the, the kind of the pure model actually has no tuning for that at all. So, it, so it's further down. And sometimes these things just don't quite line up. And, and I think that a lot of them are, are quite easily, um, gameable. They're often skewed for a either a very specific, um, set of use cases which are often not actually what any normal person does in, in, in your product.\" He said plainly that we were fine-tuning for the arena benchmark, right? They had no intention of publishing that model. They were simply gaming the score. Why was that allowed to happen? So, um, just imagine how Cohere must have felt, right? They're a frontier model company and they noted that their models weren't doing very well on ChatBot Arena. This is Nick Frost. I do think that a person's ability to look at a model and say this one's better than that one based on my brief conversations with it. And we've seen, I mean, a bunch of research has come out on this recently showing the effect the preambles have on ELO ranking, showing the, the like slight formatting changes make a huge difference in people's perception of how good the model is. The headline of this paper essentially is that ChatBot Arena itself fell victim to Goodhart's Law, which meant that many of the models are optimized to do well on the arena and not actually be capable AI models. Weirdly, seeing a model understand some specific thing that you ask it is the best test of generalization for you. This is known as passing the vibes test.\nThis is related to the reason why humans are naturally creative when we write and speak because there's a very good chance that any thought that comes into your mind while you're uniquely situated is genuinely novel in at least some small and important way. Andre Karpathy recently put out a tweet talking about his own experience with the GPT series of models going all the way back to GPT2 and he said that the differences as we went up in model versions became more and more subtle. This is what he said: \"Now, recall that GPT1 barely generated coherent text. GPT2 was a confused toy. GPT 2.5 was skipped straight on to GPT3, which was even more interesting. GPT3.5 crossed the threshold where it was enough to actually ship as a product and sparked the OpenAI's chat GPT moment. And GPT4 in turn also felt better, but I'll say, or him saying this of course, that it definitely felt more subtle. He said he remembers being part of a hackathon trying to find concrete prompts for GPT4 which clearly outperformed 3.5. He said they definitely existed but clear and concrete slam dunk examples were difficult to find. It's that everything is just a little bit better but in a diffused way. The word choice was a bit more creative. Understanding of nuance in the prompt was improved. Analogies made a bit more sense. The model was a little bit funnier. World knowledge and understanding was improved at the edges of rare domains. Hallucinations were a bit less frequent. The vibes were just a little bit better.\" End quote. He was testing GPT 4.5 and he said he expected or perhaps he wanted the performance to be better. It wasn't, by the way. I mean, it had to be right. It, it was 10 times bigger. He put a test together of examples a bit like a mini ChatBot Arena and he invited his followers to vote on Twitter. He was horrified to discover that in four out of five cases, people thought the older, smaller four row model was better. His response was even more interesting. He said, \"To be honest, I found this a bit surprising as he had personally found GPT4.5 responses to be better in all cases.\" He said maybe he's just in quotes a high taste tester. He said the thing to look for is that GPT4 more often says stuff that on the face of it looks fine, but type checks as making sense. But if you really think about it longer and more carefully, you'll more often catch it saying things that are just a bit of an odd thing to say or a little bit too formulaic, a little bit too basic, a little bit too cringe, or a little bit too tropey.\" End quote. So, in not so many words, Kapathy argued that he was smart enough to recognize that the model was better. I ran my own subjective evaluation of all of this with the Frontier models at the time in response. And while GPT 4.5 was slightly less sloppy than four row, it was still very sloppy indeed. In April 2023, two Berkeley PhD students, Anastasios Angelopoulos and Wei-Lin Chiang, couldn't get a straight answer from an existing LLM benchmark. So, in a single week, they hacked together a website where any visitor could pit two anonymous chat bots against each other and swipe right or left on their favorite answer. Basically, Tinder for chat bots, at least from a front-end perspective. I'm glossing a little bit here, but roughly speaking, the user visits the site and is blindly matched with two large language models which are selected via a selection process, which we'll get to a bit later. The user types any prompt they want into the text box via, you know, it could be a question about chaos theory and predicting weather cycles or whether Schmid Huba really did invent the transformer in the 1990s. They see both of the responses streamed out either side of the screen and they make their choice about which is best. The models don't know they're being compared and the users don't know what the models are. Although the latter part of that is, yeah, a little bit iffy. When you sit there watching the token slowly dribble out like it's 1998, you tend to notice things. The speed for one. Some models have a really distinctive turgid voluable idiosyncratic language construction output. You know, the Open AI ones in particular produce grotesque brickarages of slop. And I don't just mean superficial things like delve or em dashes. There's just so many subliminal stylistic features like the verbosity, the vagueness, the heading structure. So when the user executes the metaphorical Tinder swipe, you know, one more match is logged into a giant ratings ladder. The arena treats every battle like a chess game. The winner gains ELO points, the loser loses them. A stronger model beating a weaker model yields only a very small bump. An upset yields a big bump. There was a boon right on the surface at least. It was untrableled by the pitfalls of traditional benchmarks like MMLU and it amassed a cult following an active community. In the early days it was mostly sort of like open source models you've never heard of like Vuna 13B which quickly topped the leaderboard with um 1167 ELO points I think it was after 4,700 blind votes. MMLU as a reference point was multiple-choice questions across 57 subjects which all of the models had obviously been trained on. You know, everything from high school biology and US history and electrical engineering, even law. By July of that year, the little side project was hosting 22 models and uh Claude 1 and GBT 3.5 were snuck in and there were 53,000 votes in the system. They even uploaded the raw conversation and data uh to hugging face for research purposes. That might have ended up being a bad idea for reasons which need not detain us, but we'll get to that. On the subject of the perils of ChatBot Arena, you may well work in a large enterprise and want to build something similar to ChatBot Arena yourself. I mean, let's face it, the magic is access to a diverse pool of humans for annotation. Scaling this up often turns into a nightmare, not to mention all the control headaches. We are sponsored by Prolific. We're proud to be sponsored by them and they have built an API precisely for this. You can automate task distributions. You can manage participant pools and you can maintain data quality standards as your project grows without having to build all the infrastructure yourself. Get details at prolific.com.\nThe ELO rating has been used to rank chess players by the international community for over 60 years and they can be computed online and asynchronously from results of chess games played around the world. And it doesn't assume that the skill of each player is fixed. ELO is really simple. It just takes the difference in scores between players after a match. It runs it through the sigmoid function which converts it into a smooth probability between not and one. You subtract the actual result which is represented as 1, 0.5 or not and then you add that onto the scores multiplied by a learning rate K. In the real world of chess the games just naturally arise. In something more contrived like ChatBot Arena the sampling strategy has a big impact. We get to the end of 2023 and now the arena has registered more than 130,000 votes. Users started complaining that the scores were unstable. They were wobbling around all over the place which made them seem sketchy at best. Sara Hooker and Marzieh Fadaee's team at Cohere had previously pointed out this exact issue to the arena in a paper saying that the order in which you compare the models can significantly change the final ELO scores. They basically said, and I'm paraphrasing, ELO was designed for things like chess where their skills change over time. LLMs once trained have a fixed skill level. So, are we using ELO correctly and are the rankings reliable? They noted that transitivity wasn't guaranteed, which basically means assuming that the rankings are a consistent way of making inferences about model strength, especially when models were close in performance. They also said that the ELO algorithm had a weird sensitivity to its learning rate, which could skew the results a fair bit. Cohere argued for adapting the learning rate dynamically and shuffling the order of comparisons. Cohere were trying to keep the arena honest. And keep this in mind as it will become important later in the story. To improve the quality of the arena's rankings and confidence estimates, they adopted another widely used rating system which is called the Bradley Terry model. Like the ELO rating, the Bradley Terry model uses pair-wise comparisons to derive ratings and estimate win rates between players. The main difference is that it assumes player performance remains constant, meaning that the game order doesn't matter and it performs calculations all at once. Many are excited about models gaining in capabilities. In fact, there's an old adage called the Mccord effect which says that every time a new capability is reached, there's a chorus of people that exclaim, \"This isn't really thinking.\" Almost as if the goalposts keep getting moved. When a capability becomes disappointingly simple to mechanize, our perception widens as if it has to be indistinguishable from magic. But I would like to propose the David Shapiro effect, which is that when a model takes the leadership position on ChatBot Arena, many of course including David, sorry, David, will exclaim AGI is here or nearly here. You know, we might as well just pack up our bags and prepare for the post labor economy. This brings us to Goodhart's law. There was a principle articulated decades ago by the economist Charles Goodhart. I'm sure you've heard of him, by the way, because we constantly reference it on almost every single show. He said, \"Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.\" Or more simply, when a measure becomes a target, it ceases to be a good measure. Goodhart's law, while simple on the surface, it rears its ugly head in almost every facet of machine learning. When we pursue a complex goal such as providing quality education or developing truly general artificial intelligence, direct measurement is often impossible. So what do we do? Right? We use a proxy, a measurable metric which we hope accurately reflects the desired goal. Standardized test scores act as a proxy for education. ChatBot Arena ELO scores act as a proxy for AI capability. Initially, progress on the proxy correlates with progress towards the actual goal. Rising test scores might genuinely signify better educational outcomes. But once the proxy itself becomes the primary target, the metric everyone is incentivized to maximize, it inevitably loses its reliability as any indication of the original objective. If schools face intense pressure based on test results, what's going to happen, right? Kids are just going to learn the tests and they're not going to develop any critical analysis or creative exploration. The proxy, which is the test scores, demonstrably improve while the intended goal at best stagnates, at worst regresses. Quick pause. Getting language models to the upper etchelins of ChatBot Arena the old-fashioned way, not using the Zuckerberg strategy is really, really difficult.\n\n\nAnd that's why Benjamin Kruier at Tufa AI Labs is hiring cracked ML engineers in Zurich and in SF.\nSo if that sounds like you and you didn't work on the Llama 4 team, like slightly half joking, uh get in touch with Benjamin Kruier, go to twoferabs.ai.\nOkay, so researchers at Cohere just released this explosive paper called The Leaderboard Illusion about ChatBot Arena.\nOf course, uh lead author Shivalika Singh and also Marzieh Fadaee and Sara Hooker and a whole bunch of other folks were involved.\nUm, you'll recognize Sayash Kapoor, of course he was on the show recently.\nOn page three, there's an interesting plot here and it shows the relationship between the number of models submitted on the arena and the maximum score and the size of the bubbles is how many battles have been generated.\nSo some interesting correlations, of course you can see that the more uh you know models we test, the higher the score and the more battles the higher the score.\nNow the reason why you get a higher score with more battles Cohere argue is because every time a battle is generated you get juicy data and you can fine-tune on that data to be even better at ChatBot Arena going forwards.\nAnother stark observation they found straight away, they had access to a few months of arena data, was that proprietary models are sampled significantly more than open-source and open weights models.\nAn absolutely huge disparity.\nI mean, in quarter 1 of 2025, nearly 70% of the battles were going to proprietary models and only 23.1 and 9.2 to open weights and open- source models.\nSo that's an absolutely massive disparity.\nWhy is that?\nLet's get to their main conclusions here.\nSo number one, preferential treatment around private testing and attraction.\nSo they said that the arena has an unstated policy of allowing select providers to test many submissions privately and in parallel.\nThey said that certain model developers, notably Meta, uh, Google, OpenAI, and Amazon have benefited from extensive private testing.\nIn a single month, they observed as many as 27 models from Meta, 27 models uh, being tested privately on ChatBot Arena.\nUh, in the leadup to the Llama 4 release, which was an absolute debacle.\nIncredibly, ChatBot Arena, even if you're testing a private model, they do not require that you publish the private models.\nThe providers can simply retract models that they don't like and only choose to publish the best models from the private pool.\nSo, not only are they testing more models, they can selectively publish the best ones, they can grab as much data from the arena users as possible and fine-tune on it.\nThis is incredibly unfair.\nAnd they also point out in the background that the private providers are just even without private testing being generated more battles than the open source and the open weights models.\nDoes that sound fair to you?\nSo they also say number two, far more data is released to proprietary model providers.\nThey say ChatBot Arena is a communitydriven leaderboard that benefits from free crowdsourced feedback provided by everyday users.\nHowever, proprietary models uh collect significantly more test prompts and model battle outcomes than others.\nBut Cohere is showing an example here of you could have model family A and B and model family A might have a lower average than B yet if you you know choose a sample which is on one of the tails of the distribution you can actually make A or B better just having access to arena data to fine-tune on is really important and they actually had access to a load of arena data because if you think about it they run the API endpoint for Cohere so hundreds of thousands of people had been hitting their API endpoint which means they could take that data, they could fine-tune on it and they could just see how much of an of an uplift does it give you if you fine-tune on that data.\nThe answer is a huge uplift.\nSo just the model against itself on, you know, trained or fine-tuned with the arena mix, it went from 50% win rate win rate up to 79.2 win rate um with a 70% arena mix in in the in the finetune, which is massive.\nThey also say number three, ChatBot Arena data access drives significant performance gains.\nSo they said that they estimate that by training on ChatBot Arena data model ranking can be improved significantly.\nAnd in a controlled experiment setting they observed that increasing the arena training data mix from naugh to 70% more than doubles the win rates from 23.5% to 49.9% which is an absolutely massive difference.\nAnd finally, they say that deprecations can result in unreliable model ranking.\nAnd they have a list of officially deprecated models.\nSo these are models that we know won't be generating any more battles, but 205 have been silently deprecated.\nThat means they're still officially on the leaderboard, but no samples or no battles are being generated for those models.\nAnd this actually has some very serious ramifications because on the Bradley Terry model, it requires transitivity, right?\nWhich means that the entire comparison graph needs to be densely sampled.\nIf you sparify that graph, you get islands, right?\nAnd that means it just doesn't work anymore because if you think about it, the data distribution is changing.\nLike the the kind of prompts that users put into the models are changing and if you're still making assumptions on the model's performance based on data that you had many many months ago, you're going to create an inconsistent skill ranking.\nAnd that's a big problem.\nThey also show here the maximum observe observed sample rates for models for from different providers.\nAnd you can see here that Google and OpenAI are overwhelmingly sampled more than any of the other providers.\nIt's it's stark.\nWhy should that be the case?\nYeah, it's almost it's almost like a a comedy sketch.\nYou know, you can imagine showing up for a competition to win a million dollars by making a uh a basketball shot from from the three-point line on the other side of the court.\nAnd you you take the shot and you brick and you're like, \"Ah, shoot, man.\nOh, well, it was worth a try.\"\nAnd then and then Tim walks up there, he shoots, misses, they throw him another ball, you'll be like, \"Huh, dude, what's going on?\"\nAnd then they give him 18 more.\nYou know, what do you do?\nDo you just uniformly sample pairs of matches?\nThat's actually quite a dumb thing to do.\nWhat you should do is sample based on uncertainty will minimize the expected difference in the confidence interval.\nNow the fascinating thing is that the the the people who created ChatBot Arena, they know about this.\nThey actually wrote a paper in 2024 describing a sampling strategy which does pretty much this.\nYet they don't implement it themselves.\nThey have some weird hard-coded rule and their explanation for this is because the users of ChatBot Arena just really like playing with the latest and greatest shiny toys and the big frontier models.\nSo, let's just generate more battles for the frontier models.\nBut that unfortunately has a huge, you know, confers a huge advantage for those uh providers.\nSo, what are their findings?\nWell, I think this is very reasonable to be honest.\nNumber one is prohibit retraction of scores after submission.\nAnd this applies to just normal models that are on ChatBot Arena and the private variants.\nIf you submit a model on there and it achieves a certain score, you're not allowed to selectively delete ones that you don't like.\nThe record needs to stay intact.\nNumber two, uh establish transparent limits on the number of private models per provider.\nSo Meta doing 27 models in March.\nThat is absolutely ridiculous.\nIt should be something like three or maybe five.\nIt should be the same for everyone and it should be transparent.\nThey also say that um model removals are applied equally to proprietary open weights and open source models.\nImplement fair sampling.\nSo this is what I was saying about the the the chess thing and how that what they should do is sample models based on minimizing uncertainty or or generating more matches between models that have a similar rank.\nNot doing what they are doing which is just generating battles for Meta and Google and XAI and the folks at the very top and finally providing transparent info into which models are being removed from the leaderboard.\nSo this is the deprecation thing.\nLoads and loads of the models are basically dead which is sparsifying this connectivity graph between the models.\nThey say that they should be transparent about this.\nCompletely reasonable.\nAnd then there's the question of how useful is it that we are training on arena data?\nBecause if you think about it, the whole reason why arena is supposedly a good thing compared to static benchmarks is that the data distribution is non-stationary.\nIt's changing all the time, uh, you know, different people are using it, people think of different prompts.\nThat's in theory.\nAnd it's kind of true and it's not at the same time.\nThey did some analysis here and and you can see some macro trends certainly between the languages that people are using and they did some similarity analysis between subsequent months of um chat you know chatbot arena battle data.\nThey have access to this through various sources.\nOf course they they have it from the Cohere endpoint and there are data sets released and whatnot.\nAnd what they found was that even though there were a lot of differences, there was also a startling amount of similarity.\nAnd amazingly um something like between 25 and 33% of the prompts were very high similarity.\nAnd this is given by if you take the cosine of the embeddings and if it's greater than 0.95, it's high similarity.\nAnd then um in some cases I'd say roughly between 16% and 20% or even 26.5 in in March um the prompts were identical.\nAnd I've always found this very curious.\nYou know certainly when you interview people when you talk with your friends people are not that creative.\nThey just tend to be interested in the same things and ask the same questions.\nAnd they said in the paper people were asking questions about Star Trek and you know like the same kind of logical puzzles and and stuff like that.\nSo surprisingly, you know, roughly 50% of the data seems to be the same or nearly the same every single month.\nSo if you could fine-tune on that, if you knew what that was, it would confer an absolutely massive advantage.\nMaybe the folks at Cohere have an agenda.\nOkay, I'm that's true.\nThey have an agenda.\nThe problem is so does everybody else.\nSo everybody has these agendas and it's completely fair to call out things like this happening and to allow critical thinking to to take over, right?\nAnd what I'm wondering is what went on behind the scenes at the folks behind LM Arena to remotely ever think this was a good idea.\nSo there you have it.\nThat has been the story of ChatBot Arena.\nIt is an incredible benchmark.\nI don't want to sound too negative.\nI still think that it has quite a lot of alpha as a benchmark, but they have some serious problems.\nAfter um Sara Hooker's team released that paper, um she laid it all out on Twitter and she went through all of the findings and the way that the arena responded was a little bit weird.\nThey didn't really pick up on a lot of the feedback from the paper and they were being a bit evasive and then one week later the investment was announced which was a little bit weird.\nUm, I think they did concede a couple of Sarah's points, but the blog post in response really glossed over it quite a bit.\nSo, I very much hope that they will take this feedback on board and make the changes that the Cohere folks are arguing for.\nHope you've enjoyed the show.\n",
  "dumpedAt": "2025-07-21T18:43:24.628Z"
}