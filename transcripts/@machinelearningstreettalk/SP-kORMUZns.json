{
  "episodeId": "SP-kORMUZns",
  "channelSlug": "@machinelearningstreettalk",
  "title": "Build Specialist LLMs Like Itâ€™s 2019 (Randall Balestriero)",
  "publishedAt": "2025-04-23T14:09:32.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "We just launched this experiment and",
      "offset": 0.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "then we are very surprised to see that",
      "offset": 2.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the hugely overparameterized model and",
      "offset": 3.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "not only train out of the box like you",
      "offset": 5.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "have very nice training curves but also",
      "offset": 8.559,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "they don't overfeit aggressively at all.",
      "offset": 10.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "And what we found empirically is that we",
      "offset": 12.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "just out of the box use typical like",
      "offset": 15.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "supervised training. We don't have to",
      "offset": 17.84,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "play with hyperparameter optimizer and",
      "offset": 19.279,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "you have very very stable training.",
      "offset": 21.439,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "So this brings also the question is it",
      "offset": 26.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "worth it to spend so much money together",
      "offset": 28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "gigantic pre-training data set spend",
      "offset": 30.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like months on many GPUs to produce",
      "offset": 32.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "those models but at least for some",
      "offset": 35.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "application it seems to not be much",
      "offset": 37.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "better than",
      "offset": 39.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "random MLST is sponsored by twofer AI",
      "offset": 44.68,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "labs now they are the deepseek based in",
      "offset": 47.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Switzerland they have an amazing team",
      "offset": 50,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you've seen many of the folks on the",
      "offset": 52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "team they acquired at Mind's AI, of",
      "offset": 53.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "course, they did a lot of great work on",
      "offset": 55.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "ARC. They're now working on 01 style",
      "offset": 56.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "models and reasoning and thinking and",
      "offset": 58.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "test time computation. The reason you",
      "offset": 60.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "want to work for them is you get loads",
      "offset": 62.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of autonomy. You get visibility. You can",
      "offset": 64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "publish your research and also they are",
      "offset": 65.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "hiring as well as ML engineers, they're",
      "offset": 68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "hiring a chief scientist. They really",
      "offset": 70,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "really want to find the best possible",
      "offset": 72.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "person for this role and they're",
      "offset": 74.64,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "prepared to pay top dollar as as a",
      "offset": 76.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "joining bonus. So if you're interested",
      "offset": 78.479,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "in working for them as an MO engineer or",
      "offset": 80.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "their chief scientist, get in touch with",
      "offset": 82.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Benjamin Cruzia, go to twoferabs.ai and",
      "offset": 84.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "uh see what happens. Originally the main",
      "offset": 87.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "motivation was to see okay how much",
      "offset": 90.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "information you gain by doing uh",
      "offset": 92.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "pre-training right and is this next",
      "offset": 93.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "token prediction really making your",
      "offset": 95.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "network learn something about language",
      "offset": 97.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and uh reasoning. And so then we are",
      "offset": 100.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "saying okay one way to compare this at",
      "offset": 102.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "least empirically is to just take a",
      "offset": 104.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "randomly initialized model train it from",
      "offset": 106.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "scratch on a supervised task like",
      "offset": 109.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "sentiment uh prediction sentiment",
      "offset": 111.28,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "analysis and then in theory because we",
      "offset": 114.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "have a very very small training data",
      "offset": 117.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "let's say like 20,000 samples and",
      "offset": 119.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "because those model have like 7 billion",
      "offset": 121.439,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "parameters uh the pre-trained one will",
      "offset": 124.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "perform very nicely with a little bit of",
      "offset": 126.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Laura fine tuning because it already",
      "offset": 128.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "knows to reason about the formed, right?",
      "offset": 130.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "So maybe you just adjust a little bit to",
      "offset": 132.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the specific task that you want, but",
      "offset": 134.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "since you have so many prior knowledge,",
      "offset": 136.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you will uh solve the task very easily,",
      "offset": 138.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "but the random one either will overfeit",
      "offset": 140.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "completely because you have like 7",
      "offset": 143.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "billion parameters and only 20,000",
      "offset": 145.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "training samples or maybe it will not",
      "offset": 147.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "learn at all because you know training",
      "offset": 149.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "dynamics will be completely chaotic. And",
      "offset": 150.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "so we just launched this uh experiment",
      "offset": 153.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then we're very surprised to see",
      "offset": 155.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that the seven billion or like the",
      "offset": 157.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "hugely overparameterized model uh not",
      "offset": 159.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "only train out of the box like you have",
      "offset": 162.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "very nice training curves almost like",
      "offset": 164.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you train amnest but also they don't",
      "offset": 166.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "overfeit aggressively at all like they",
      "offset": 168.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "overfeit less than if you just train MLP",
      "offset": 171.04,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "on MNEST basically. uh and this is very",
      "offset": 173.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "uh surprising and so basically from this",
      "offset": 176.879,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "we said okay actually maybe there is a",
      "offset": 178.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "deeper question we could which could be",
      "offset": 180.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "how much uh implicit bias you have in",
      "offset": 183.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "those language model because already we",
      "offset": 185.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "knew from computer vision that for",
      "offset": 188.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "example imaget you can have a 50 million",
      "offset": 189.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "model on a 1 million data set so you",
      "offset": 191.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "have like this 50 to1 ratio and you have",
      "offset": 194,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the implicit bias that prevents you from",
      "offset": 196.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "overfeitting and just solving the task",
      "offset": 197.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "right but still it's 50 to1 so this may",
      "offset": 200.159,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "sound a lot for you know",
      "offset": 203.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "statistician but uh now it's like 7",
      "offset": 205.159,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "billion to 20,000 so like the ratio is",
      "offset": 208.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "like gigantic right and uh yeah I don't",
      "offset": 211.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "know to me it was very surprising that",
      "offset": 214.08,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the scale uh like the size of this ratio",
      "offset": 216.64,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "uh still allows you to learn something",
      "offset": 220.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that does not overfeit and this is very",
      "offset": 222.959,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "surprising because in vision for example",
      "offset": 225.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "transformers are known to overfeit more",
      "offset": 227.64,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "easily than resnet so they seem at least",
      "offset": 230.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "in vision to have actually less",
      "offset": 232.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "implicit bias or implicit regularization",
      "offset": 235.4,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "but at least with this type of next",
      "offset": 238.56,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "token causal architecture LLM uh yeah",
      "offset": 241.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you don't seem to fit easily to your",
      "offset": 245.28,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "data so this was a quite surprising yeah",
      "offset": 247.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and and we should bring in the name so",
      "offset": 249.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "this was your your workshop paper at the",
      "offset": 250.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "self-supervised learning workshop here",
      "offset": 253.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "at News and it's called for perception",
      "offset": 254.4,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "tasks is LLM pre-training by next token",
      "offset": 256.239,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "prediction worth the cost so this is",
      "offset": 258.959,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "absolutely fascinating right so so we",
      "offset": 261.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we've been given this this belief that",
      "offset": 263.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "we need to have these huge pre-trained",
      "offset": 266.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "models. They're trained on all the data",
      "offset": 269.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "on on on the internet. And it turns out",
      "offset": 270.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that certainly for discrimination tasks,",
      "offset": 272.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "so things like classification rather",
      "offset": 275.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "than generation actually you can just",
      "offset": 276.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "start from scratch with a with a with a",
      "offset": 279.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "fairly small model and you get sometimes",
      "offset": 281.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "even better results. Yeah. Yeah. And uh",
      "offset": 283.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "even small or even large uh model like",
      "offset": 285.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "you just start from scratch. You do this",
      "offset": 288.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "very simple supervised uh classification",
      "offset": 291.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "task right. Okay, given this prompt, is",
      "offset": 294.24,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "it uh good or a bad sentiment or what",
      "offset": 296.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "type of job uh is the prompt describing?",
      "offset": 299.759,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "You know, this type of we not call it",
      "offset": 301.919,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "reasoning, but yeah, more semantic",
      "offset": 303.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "classification and turns out that yeah,",
      "offset": 306.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "you start from random. Even if you have",
      "offset": 308.32,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "a small training data set, you will have",
      "offset": 310.56,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "performances that are uh sometimes as",
      "offset": 312.479,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "good pre-trained model. So this yeah",
      "offset": 315.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "brings also the question is it worth it",
      "offset": 318,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to spend so much money to gather a",
      "offset": 319.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "gigantic pre-training data set spend",
      "offset": 322.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like months on many GPUs to produce uh",
      "offset": 324.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "those models uh and for some cases for",
      "offset": 327.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "generation all right there is no",
      "offset": 330.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "question this is what you need to do uh",
      "offset": 331.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "you have your next token prediction you",
      "offset": 334.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "learn how to generate samples but at",
      "offset": 335.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "least for some application it seems to",
      "offset": 337.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "not be much better than random so it's",
      "offset": 340.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "quite quite interesting so what are the",
      "offset": 342.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "differences in the learned",
      "offset": 344.88,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "representations",
      "offset": 345.919,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "So that's something we did not really",
      "offset": 346.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "look at like low dimensional",
      "offset": 348.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "representation of what you learn. Uh",
      "offset": 351.08,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "it's possible. So some work try to look",
      "offset": 354.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "at the attention entropy and the like",
      "offset": 356.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you know those mechanistic",
      "offset": 360,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "interpretability viewpoint of LLM. So it",
      "offset": 361.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "will be interesting to see if you have",
      "offset": 363.759,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "this sort of uh you know neural collapse",
      "offset": 366.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "thing that happen. So even if you're",
      "offset": 368.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like a 7 billion parameter maybe you end",
      "offset": 370.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "up learning a very very very simple sub",
      "offset": 372.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "network that does the task you know bit",
      "offset": 374.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "like lottery ticket hypothesis as well",
      "offset": 376.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and that naturally emerge from the",
      "offset": 379.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "training dynamics or is it really",
      "offset": 380.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "exploiting all the parameters I think",
      "offset": 382.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "that's one thing so to extend the",
      "offset": 384.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "workshop paper to conference we want to",
      "offset": 386.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "probe into more like what are the useful",
      "offset": 388.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "parameters what did they learn are each",
      "offset": 390.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "layer actually learning something or",
      "offset": 393.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "maybe the first layers don't really",
      "offset": 395.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "learn anything just the last few ones uh",
      "offset": 397.039,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "are learning something. So yeah, there",
      "offset": 399.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "is like lots of open questions for this.",
      "offset": 400.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "What does it tell us about the nature of",
      "offset": 403.039,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "understanding and maybe even",
      "offset": 404.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "intelligence? Because we we think that",
      "offset": 405.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the reason why these things understand",
      "offset": 407.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "is because they're they they just have",
      "offset": 409.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "all of these representations to all of",
      "offset": 411.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "these different you know things in in in",
      "offset": 412.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "their experience and and now we can",
      "offset": 415.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "shortcut to to you know to one of a",
      "offset": 418.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "better word. What does that tell us?",
      "offset": 420.56,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Yeah, I think that's a good question. So",
      "offset": 422.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in this case we must look at very",
      "offset": 423.759,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "specific classification tasks. So for",
      "offset": 426.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "example you have a description of a job",
      "offset": 428.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what job it is it like a good or bad",
      "offset": 429.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "sentiment uh and this you are able to",
      "offset": 432.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "solve it good but you are not able to go",
      "offset": 434.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "out of distribution to solve a new uh",
      "offset": 436.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "type of question for example for this",
      "offset": 439.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "job description then you cannot answer",
      "offset": 441.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "okay is this job paying you more than",
      "offset": 443.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this job because this was not present in",
      "offset": 444.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the training data right so I think you",
      "offset": 446.639,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "get very good models cheaply quickly",
      "offset": 449.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "from random initialization but they will",
      "offset": 452.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "be very specialized and I think the",
      "offset": 454.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "benefit of having maybe the pre-training",
      "offset": 456.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "may come if you want to do more of like",
      "offset": 458.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "open-ended uh classification or",
      "offset": 460.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "reasoning. So I think it really depend",
      "offset": 464.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "on the type of application you want to",
      "offset": 466.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "solve what's your downstream task and",
      "offset": 468.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "how much you want to generalize to new",
      "offset": 470.16,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "scenarios but at least now it shows that",
      "offset": 471.919,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "yeah it's not just pre-training with",
      "offset": 473.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "next to prediction is better uh for",
      "offset": 475.759,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "everything. So I mean going back five",
      "offset": 478,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "years, data scientists used to build",
      "offset": 480.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "specific classification models for doing",
      "offset": 482.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "everything. And now we're in this regime",
      "offset": 484.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "of we need these really big models and",
      "offset": 485.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we do in context learning and maybe even",
      "offset": 488.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "some fine-tuning and and we get them to",
      "offset": 490.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "do fairly specific discriminative tasks.",
      "offset": 493.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "But now you're saying we should almost",
      "offset": 495.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "go back to where we were 5 years ago and",
      "offset": 496.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "start building specialized models again.",
      "offset": 498.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "only now rather than building",
      "offset": 500.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "classification models, we're actually",
      "offset": 502.319,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we're still using the transformers and",
      "offset": 503.84,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and the LLMs, but we're we're making",
      "offset": 505.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "them do specific tasks. Yeah. Yeah.",
      "offset": 507.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Exactly. I think if you only want to",
      "offset": 509.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "solve a few specific task, uh use this",
      "offset": 511.199,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "prior knowledge to have a nice",
      "offset": 514.159,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "architecture supervised data set for",
      "offset": 516.279,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "that and just do that from scratch. Uh",
      "offset": 518.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "this is something that's going to",
      "offset": 520.719,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "probably work much better. But again you",
      "offset": 522.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "need to make sure that uh the downstream",
      "offset": 524.399,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "application will never go too out of",
      "offset": 527.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "distribution. So that's why it really",
      "offset": 530.6,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "depend on the application and the type",
      "offset": 532.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "of use cases that you have. But I think",
      "offset": 534.959,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "at least here it shows that there exists",
      "offset": 538.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "some task where next to prediction is",
      "offset": 540.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "not the answer and in fact it's not just",
      "offset": 543.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not the answer but it's not better than",
      "offset": 546.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "random initialization which is really",
      "offset": 547.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "sort of the worst case scenario.",
      "offset": 549.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Interesting. I mean from a fairness and",
      "offset": 551.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "bias point of view a lot of people say",
      "offset": 552.959,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that you know large language models are",
      "offset": 554.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "bad in a way because there's the",
      "offset": 555.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "dominance of North American cultures and",
      "offset": 557.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and so on. Um but you could also argue",
      "offset": 559.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the converse which is that the good",
      "offset": 561.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "thing about them is that they do have",
      "offset": 563.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "some awareness of value you know so we",
      "offset": 564.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "can fine-tune them to um have guard",
      "offset": 567.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "rails and to sort of say the right thing",
      "offset": 570.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and so on. Is that harder to do with",
      "offset": 571.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this approach? Yeah. So here because you",
      "offset": 574.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "are in a fully uh supervised setting uh",
      "offset": 575.68,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you don't have as much flexibility to",
      "offset": 578.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "let's say change the behavior of your",
      "offset": 581.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "model where it will have to take the",
      "offset": 584,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "form of supervised fine-tuning but",
      "offset": 585.44,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "because you don't have a generative",
      "offset": 587.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "capability it's certainly restrict the",
      "offset": 590.04,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "type of interaction you have with the",
      "offset": 592.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "model and you can improve it right uh",
      "offset": 594.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because the output is just okay is it a",
      "offset": 596.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "good or bad sentiment it's not something",
      "offset": 598.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that gives you a full answer that then",
      "offset": 600.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you can try to argue against and",
      "offset": 602.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "generate a fine-tuning data set from is",
      "offset": 604.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "just okay good bad and that's it.",
      "offset": 607.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Another thing is training strategy. So",
      "offset": 609.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you know like the big players building",
      "offset": 610.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "these LLMs they have lots of",
      "offset": 612,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "internalized knowledge around um you",
      "offset": 613.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "know even the order in which you train",
      "offset": 616.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "the language models everything is",
      "offset": 618.079,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "important you know certainly in the old",
      "offset": 619.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "days of like basic models you know you",
      "offset": 620.959,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "just stick a load of data in there no",
      "offset": 622.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "one no one really cares. So you know now",
      "offset": 623.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "do people need to be sort of thinking",
      "offset": 626.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "about the specialized knowledge maybe",
      "offset": 627.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "thinking about curriculum learning and",
      "offset": 629.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "all of this kind of stuff. Yeah so this",
      "offset": 630.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is a good point. So uh we did a paper",
      "offset": 632.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "recently called the fair language model",
      "offset": 634.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "paradox where we show that when you do",
      "offset": 636.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "this next token uh prediction because",
      "offset": 638.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you have some tokens that are very low",
      "offset": 640.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "frequency it's very hard to train on",
      "offset": 642.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "them and it takes a very long training",
      "offset": 644.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so it's very wasteful right and the",
      "offset": 646.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "problem is that because you do this next",
      "offset": 648.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "token prediction you need to really",
      "offset": 649.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "capture all your distribution of tokens",
      "offset": 651.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and so you spend a lot of time but in",
      "offset": 653.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this case if the low frequency tokens",
      "offset": 655.44,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "are not useful to solve your uh task you",
      "offset": 658.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "actually don't need to capture it at So",
      "offset": 661.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "in ter of training dynamics this is",
      "offset": 663.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "actually a much uh simpler problem in",
      "offset": 665.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "many cases and what we found empirically",
      "offset": 667.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is that we just out of the box use",
      "offset": 670.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "typical like supervised training we",
      "offset": 673.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "don't have to play with hyperparameter",
      "offset": 674.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "optimizer and you have very very stable",
      "offset": 677.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "uh training. So that's one thing that",
      "offset": 679.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "could be also interesting for uh future",
      "offset": 681.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "work is to see is this something that is",
      "offset": 683.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "easier to optimize and maybe that's why",
      "offset": 686.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "those like 7 billion parameter model can",
      "offset": 688.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "learn and not overfeit on like 10,000",
      "offset": 690.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "samples and then it's also bring other",
      "offset": 692.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "things that maybe this on its own could",
      "offset": 694.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "be a better initialization for next",
      "offset": 697.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "token prediction as well. So this is",
      "offset": 700.079,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "like very open up in the air. But maybe",
      "offset": 702.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you could think of a simpler uh",
      "offset": 705.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "supervised objective that would be a",
      "offset": 708.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "better pre-training uh solution that",
      "offset": 711.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "then you can use for like next token",
      "offset": 713.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "prediction if you wanted to. But at",
      "offset": 715.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "least this will be a better starting",
      "offset": 717.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "point from random. So you almost reverse",
      "offset": 718.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the the trend. So we've spoken about two",
      "offset": 721.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "extremes. So on the one extreme we have",
      "offset": 723.519,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "pre-training and and you can like use it",
      "offset": 725.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "for any downstream task. And on the",
      "offset": 726.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "other extreme we have you know you start",
      "offset": 728.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "from scratch just with one task. Is",
      "offset": 730.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "there an intermediate solution? So what",
      "offset": 732.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "if I did this new approach but for",
      "offset": 734.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "multitask let's say for five tasks.",
      "offset": 736,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. So that's a great question.",
      "offset": 737.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "So if you really think about it in the",
      "offset": 739.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "limit you could formulate a next token",
      "offset": 741.519,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "prediction as a multitask where you want",
      "offset": 743.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "to each task is the next token uh this",
      "offset": 746.399,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "one or not. So in the extreme uh case",
      "offset": 750,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "you could just recover an ex token",
      "offset": 753.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "prediction on one end and on the other",
      "offset": 755.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "end you have what we have here. So just",
      "offset": 757.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "one task very course eye level predict",
      "offset": 759.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "if it's a good or bad sentiment or",
      "offset": 762.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "whatever. So in between you have a huge",
      "offset": 763.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "spectrum that you can exploit and if you",
      "offset": 766.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "can find as you said maybe five very",
      "offset": 768.959,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "different representative task this you",
      "offset": 771.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "should be enough to or could be enough",
      "offset": 774.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to learn a representation that is as",
      "offset": 776,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "general as possible and then you can use",
      "offset": 777.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "this for maybe new task that come on the",
      "offset": 779.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "go. So I think the research question is",
      "offset": 783.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "how to design the minimum amount of task",
      "offset": 785.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so that you have as diverse",
      "offset": 788.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "representation as possible and of course",
      "offset": 790.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we don't want to go to the extreme of",
      "offset": 792.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "just doing again next token prediction.",
      "offset": 794.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Uh but this this is a very very nice",
      "offset": 797.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "research question because if you have",
      "offset": 799.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this spectrum and you can control where",
      "offset": 801.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you want to be then you can really have",
      "offset": 803.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a per use case choice. So it's not okay",
      "offset": 805.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you're always here or always here. Tell",
      "offset": 807.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "me what you want to do, how much new",
      "offset": 810,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "task you expect your model to be exposed",
      "offset": 812.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to and I tell you where you need to be",
      "offset": 814.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "in this spectrum. So this could be like",
      "offset": 816,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "very interesting as well. Very cool.",
      "offset": 817.6,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Very cool. It does make me think though",
      "offset": 819.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that these models understand through",
      "offset": 820.959,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "naive statistical alignment and is it",
      "offset": 823.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "possible that the benchmarks we use just",
      "offset": 826.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "don't cap, you know, they the gap of",
      "offset": 828.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "understanding that we've lost from",
      "offset": 831.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "moving from the pre-trained models isn't",
      "offset": 833.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "being captured. Yeah, I think uh because",
      "offset": 835.279,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "especially in the recent years we focus",
      "offset": 837.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "a lot on uh generative uh decoder only",
      "offset": 839.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "methods all the evaluation and the type",
      "offset": 842.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "of objectives we put on ourselves in",
      "offset": 844.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "really is really about good generation",
      "offset": 846.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "right uh even if you want to answer a",
      "offset": 849.199,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "question you need to generate a good",
      "offset": 851.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "explanation you need to uh understand",
      "offset": 853.16,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "what are the intermediate steps and I",
      "offset": 855.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "think the fact that we focus on",
      "offset": 858.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "generative models means that we",
      "offset": 859.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "completely bias the evaluation and the",
      "offset": 861.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "way we approach uh this and maybe you",
      "offset": 863.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "could have still knowledge that is learn",
      "offset": 866.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "without being able to generate uh",
      "offset": 869.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "anything. So I think this is also",
      "offset": 871.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "something that could be interesting to",
      "offset": 873.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "look at or at least keep in mind uh when",
      "offset": 874.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we explore those models. But",
      "offset": 877.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "philosophically though isn't generation",
      "offset": 879.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "analogous to thinking in some sense. So",
      "offset": 881.6,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "don't don't models that generate aren't",
      "offset": 883.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "they smarter in some deep way? probably",
      "offset": 884.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "what you want to do is maybe imagine",
      "offset": 887.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "what could be uh but I don't think you",
      "offset": 889.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "want to do generation is with very",
      "offset": 892.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "granular details like next token",
      "offset": 894.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "generation because if you think about it",
      "offset": 896.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "even just in term of like uh",
      "offset": 898.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "classification task you have a lot of",
      "offset": 900.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "different uncertainty depending on the",
      "offset": 903.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "token if I start the sentence okay I saw",
      "offset": 904.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this movie for minutes there is no way",
      "offset": 906.959,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "you can tell what was the next token for",
      "offset": 910,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "after four right so this means that you",
      "offset": 912.72,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "No, a it would be like a time component,",
      "offset": 916.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "right? Maybe it's like 1 hour, 10",
      "offset": 919.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "minutes, 2 hours. But do you really need",
      "offset": 920.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "to be able to generate the I don't know",
      "offset": 922.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "52 minutes or whatever the answer was to",
      "offset": 924.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "actually understand that I was seeing a",
      "offset": 926.639,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "movie, therefore I was staying in a",
      "offset": 928.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "place for at least more than 5 seconds,",
      "offset": 930.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "right? So I think token is way too",
      "offset": 931.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "granular. Uh and if you had maybe like",
      "offset": 934.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "concept token, that's where you could",
      "offset": 936.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "start seeing, okay, this is meaningful",
      "offset": 938.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because that's closer to maybe uh what",
      "offset": 940.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "we do. But right now we are very very",
      "offset": 942.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "very low level because tokenization is a",
      "offset": 944.079,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "lossless compression right so this is",
      "offset": 946.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "too close to the uh raw data and yet we",
      "offset": 949.279,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "have the life easy compared to computer",
      "offset": 952.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "vision because already you work in",
      "offset": 954.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "language which is very compressed",
      "offset": 956.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "representation of knowledge uh but still",
      "offset": 958.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "token is probably too low level still",
      "offset": 961.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "well that was a fascinating paper let's",
      "offset": 964.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "move on to your next one so the birth of",
      "offset": 966.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "self-supervised learning as supervised",
      "offset": 967.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "theory and that was with Yan Lun and um",
      "offset": 969.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "yeah basically You said that the",
      "offset": 972.399,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "observed differences between",
      "offset": 973.6,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "self-supervised learning and supervised",
      "offset": 974.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "learning are not due to the loss",
      "offset": 976.24,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "function themselves but rather the",
      "offset": 978.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "labeling of the data set used in",
      "offset": 979.759,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "training. Give us the elevator pitch.",
      "offset": 981.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Yeah. So basically what we show in this",
      "offset": 982.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "paper is that you can have a supervised",
      "offset": 984.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "objective like let's say le squares to",
      "offset": 987.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "make it simple. Uh so you have the",
      "offset": 988.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "inputs, you have your network's",
      "offset": 991.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "prediction and you have the labels and",
      "offset": 992.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you can turn this objective which tries",
      "offset": 994.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to predict sample XN to prediction YN",
      "offset": 996.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "into a self-supervised learning",
      "offset": 1000.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "objective which try which tries to",
      "offset": 1002,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "compare samples with each other. So",
      "offset": 1003.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "basically you go from saying okay uh",
      "offset": 1005.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this image is a car or a dog to saying",
      "offset": 1007.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "are those two images the same or not",
      "offset": 1009.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "which is like the self supervised uh",
      "offset": 1012.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "type of jointing world. uh and so you",
      "offset": 1014,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "can show that if you have labels or you",
      "offset": 1017.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "have knowledge of this pair wise",
      "offset": 1020.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "relationship they actually learning the",
      "offset": 1022.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "same representation up to some symmetry",
      "offset": 1024.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that is irrelevant if you do linear",
      "offset": 1026.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "probing. So the loss function in itself",
      "offset": 1028.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the SSL one or the supervised one try to",
      "offset": 1031.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "do the same thing they just operate on a",
      "offset": 1033.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "different view of the labeling whether",
      "offset": 1036.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "this image is that or are those two",
      "offset": 1038.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "images or two samples uh representing",
      "offset": 1040.64,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the same thing. So given that then the",
      "offset": 1043.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "next question is okcom self-supervised",
      "offset": 1045.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "learning is able to generalize better",
      "offset": 1048.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "than supervised and from this",
      "offset": 1050.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "perspective what you can say is that",
      "offset": 1052.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it's because it's as if they were",
      "offset": 1054.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "solving a supervised task where the",
      "offset": 1055.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "labels are not about predicting all the",
      "offset": 1058.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "cars to cars but are very very very fine",
      "offset": 1060.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "grain label where in the limit each",
      "offset": 1063.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "image is its own class basically. So if",
      "offset": 1064.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you think about supervised learning in",
      "offset": 1067.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this uh extreme setting, you also don't",
      "offset": 1069.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "overfeit to the task because you don't",
      "offset": 1072.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "collapse any image to another one and so",
      "offset": 1073.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "theoretically speaking you can solve",
      "offset": 1076.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "many downstream task uh as you want. So",
      "offset": 1078.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this equivalence of floss at least",
      "offset": 1080.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "brings a slight new perspective on the",
      "offset": 1083.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "fact that it's not really about the",
      "offset": 1085.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "objective. It's more about oh you design",
      "offset": 1086.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the SSL pipeline or you say okay with",
      "offset": 1089.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this sample is related to this sample",
      "offset": 1091.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but it's not the objective that makes",
      "offset": 1092.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you learn a better representation. Okay.",
      "offset": 1094.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "And in the paper you were talking about",
      "offset": 1096.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "how SSL can maximize the worst case",
      "offset": 1098.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "downstream task performance. Can you",
      "offset": 1100.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sketch that? Yeah. So basically if you",
      "offset": 1102,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "think about all the possible realization",
      "offset": 1104,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "of downstream task, you could have some",
      "offset": 1106.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "very core scale ones. You have maybe",
      "offset": 1107.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "different pictures of cars and buses and",
      "offset": 1109.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you just want to say it's a car or a",
      "offset": 1111.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "bus. So no details need to be encoded to",
      "offset": 1113.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "solve this. But then you can have",
      "offset": 1115.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "downstream task where you want to say",
      "offset": 1116.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "okay which brand of car is it or which",
      "offset": 1118.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "color of car is it. So you have a",
      "offset": 1119.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "distribution of downstream task right.",
      "offset": 1121.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "And so the point now is that you want to",
      "offset": 1123.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "learn a representation so that if you",
      "offset": 1124.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "look at the distribution of downstream",
      "offset": 1127.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "task performance, you are able to be as",
      "offset": 1128.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "good as possible on most of them. Right?",
      "offset": 1131.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So you don't want to be very good on",
      "offset": 1133.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some and then in the tail you are very",
      "offset": 1135.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "bad on the majority of them. And so then",
      "offset": 1138.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "from this you can try to say okay what",
      "offset": 1140.559,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "will be the labeling that tries to make",
      "offset": 1142.559,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "your worst case as good as possible. And",
      "offset": 1146.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "from this you can say okay this is",
      "offset": 1148.799,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "actually the labeling that self-s",
      "offset": 1150.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "supervisor actually implicitly uh doing.",
      "offset": 1151.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "How does the class the class balance",
      "offset": 1154.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "affect the the difference in the losses?",
      "offset": 1156.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Oh yeah. So this is a very good point",
      "offset": 1158.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually in a follow-up paper we're",
      "offset": 1160.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "doing right now. We show that current",
      "offset": 1162.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "SSL objective assume class balanceness.",
      "offset": 1164.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And this is something we already",
      "offset": 1167.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "highlighted quickly in this uh SL super",
      "offset": 1169.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "learning as a uniform cluster prior",
      "offset": 1172.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "paper we did a couple years ago. And we",
      "offset": 1174.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "show that current SSL objectives assume",
      "offset": 1176.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "balanced representation of classes or",
      "offset": 1180,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "concepts. And this means that if you",
      "offset": 1181.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "train on imaget things work out very",
      "offset": 1183.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "well because concepts are sort of",
      "offset": 1185.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "equally represented. But then if you go",
      "offset": 1187.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to other data set like I naturalist",
      "offset": 1189.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "which are very heavy tail then you have",
      "offset": 1191.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a huge bias in your representation. So",
      "offset": 1193.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "until now people did not really know how",
      "offset": 1196.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "to solve this and so one way uh people",
      "offset": 1198.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "approach this is through data curation",
      "offset": 1201.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "and they say okay I'm just going to",
      "offset": 1203.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "remove the over sampled concepts to try",
      "offset": 1204.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "to make it more uniform and then I do",
      "offset": 1208.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "self-supervised learning on this. But",
      "offset": 1209.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because now we have this theoretical",
      "offset": 1211.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "formulation and this equivalence of",
      "offset": 1213.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "losses, we can use the exact same",
      "offset": 1215.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "setting that people used in supervised",
      "offset": 1218,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "learning to rewe depending on the",
      "offset": 1220.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "frequency of classes. We can use that to",
      "offset": 1222.16,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "come up with a new self-supervised",
      "offset": 1224.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "learning loss that takes this imbalance",
      "offset": 1225.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "into account. So this type of thing is",
      "offset": 1228.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "enabled from this mathematical",
      "offset": 1230.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "formulation and it's principle. So the",
      "offset": 1232.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "way we do this waiting you can prove",
      "offset": 1234.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that it's the right way to do it from",
      "offset": 1236.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "this supervised theory. And so this is",
      "offset": 1238.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "really nice because suddenly from this",
      "offset": 1240.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "seemingly naive connection you can now",
      "offset": 1243,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "come up with new generation of",
      "offset": 1245.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "self-supervised learning models where",
      "offset": 1247.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you can actually match what the real",
      "offset": 1248.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "world data data distribution is like. So",
      "offset": 1250.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "uh non-uniform distribution of classes",
      "offset": 1253.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "maybe even if you have some samples that",
      "offset": 1255.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "are more noisy than others you can",
      "offset": 1257.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "include that information as part of the",
      "offset": 1259.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "SSL objective as well. So suddenly you",
      "offset": 1260.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "have a world new world of possibilities",
      "offset": 1262.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that comes and because there is this",
      "offset": 1265.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "connection you can actually prove okay",
      "offset": 1267.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this is the right way to do it at least",
      "offset": 1268.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "from this supervised Siri viewpoint. You",
      "offset": 1270.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "also pointed out a connection to VCrag.",
      "offset": 1272.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Exactly. So basically uh what we do in",
      "offset": 1275.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the paper is that we show if you have a",
      "offset": 1277.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "le square supervised type of objective",
      "offset": 1279.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and you turn it into a SSL one you",
      "offset": 1282.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "obtain is basically vrag. So then you",
      "offset": 1284.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "have a few variation it could be vrag or",
      "offset": 1286.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "wms depending on how you do this uh from",
      "offset": 1289.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "supervised to SSL but you can show that",
      "offset": 1292.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "depending on the type of supervised loss",
      "offset": 1294.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "you will cover different type of SSL",
      "offset": 1296.799,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "ones. If you look maybe more at cross",
      "offset": 1298.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "entropy, supervised learning is going to",
      "offset": 1300.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "be more like SIM clear type of loss, but",
      "offset": 1302.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "you have this onetoone correspondence.",
      "offset": 1304.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "And this is also very nice because in",
      "offset": 1306.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "supervised learning at least you know",
      "offset": 1308.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "when one loss maybe uh preferred",
      "offset": 1310.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "compared to another one and this has",
      "offset": 1312.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "been studied for a long time, right?",
      "offset": 1314.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Because supervised learning has been",
      "offset": 1316.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "around forever and so now we can reuse",
      "offset": 1318.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "those insights for self-supervised",
      "offset": 1320.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "learning. So this to me is also a very",
      "offset": 1322.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "very strong benefit of this thing is",
      "offset": 1324.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that suddenly all the theory and like",
      "offset": 1326.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "the thousands of papers that have been",
      "offset": 1329.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "done in supervised learning we can just",
      "offset": 1331.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "take it and apply it in SSL. Uh another",
      "offset": 1333.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "example is a neural collapse for example",
      "offset": 1336.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that has been proven in supervised",
      "offset": 1338.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "setting. Now it applies like in five",
      "offset": 1340.24,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "lines in a SSL setting as well. So this",
      "offset": 1342.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "connection is really beyond just trying",
      "offset": 1345.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to say okay it's not the objective that",
      "offset": 1348.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "make SSL better. It's really tying those",
      "offset": 1350.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "two uh huge communities together towards",
      "offset": 1353.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "a goal where you have a single unified",
      "offset": 1356.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "objective to learn representation. And",
      "offset": 1359.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this is nice too because if you speak to",
      "offset": 1361.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "people they will think okay you have",
      "offset": 1363.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "supervised learning on one side SSL on",
      "offset": 1364.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the other side and basically you are",
      "offset": 1367.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "either in one camp or the other but now",
      "offset": 1369.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "what we show is that you actually SSL is",
      "offset": 1372.159,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "pretty much everything in representation",
      "offset": 1375.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "learning and supervis is just one",
      "offset": 1377.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "realization of SSL. Then uh V Craig",
      "offset": 1379.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "without label is another one. Then this",
      "offset": 1382.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "one is another one. So you really have a",
      "offset": 1384.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "better understanding of this",
      "offset": 1387.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "relationship and what learning is trying",
      "offset": 1388.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to do. Galaxy brain question incoming.",
      "offset": 1391.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Could you combine SSL and supervised",
      "offset": 1393.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "objectives in in some way to improve",
      "offset": 1395.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "generalization? Yes. Yes. So uh there is",
      "offset": 1397.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "one paper which is supervised",
      "offset": 1399.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "contrastive learning. So the way they do",
      "offset": 1401.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it is that they use the labels within a",
      "offset": 1403.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "sim clear framework to try to basically",
      "offset": 1406.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "do fully supervised learning but with a",
      "offset": 1408.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "sim clear objective. So first of all we",
      "offset": 1410.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "can show that indeed this makes sense",
      "offset": 1412.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and that basically we can explain the",
      "offset": 1414.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "empirical result that they had but",
      "offset": 1417.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "actually we can do a little bit more",
      "offset": 1418.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "than that. So if you are in a",
      "offset": 1421.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "semi-supervised setting for example it",
      "offset": 1422.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "may not be clear how to combine those",
      "offset": 1424.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "two losses anymore or maybe you could",
      "offset": 1426.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "say okay I have the two and I have a",
      "offset": 1427.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "coefficient to weight them then you need",
      "offset": 1428.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "to do cross validation and so on but now",
      "offset": 1430.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "from this perspective you can combine",
      "offset": 1432.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "them in a very principled way and you",
      "offset": 1434.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "can understand which waiting makes sense",
      "offset": 1437.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "depending on how much sample you have in",
      "offset": 1439.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "one or the other and you can use all the",
      "offset": 1441.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "literature again from like supervised",
      "offset": 1443.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "learning uh for this setting as well. So",
      "offset": 1445.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "this is something can do very easily",
      "offset": 1448.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "with this formulation as well. Okay. So",
      "offset": 1450.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "if SSL and supervised are two sides of",
      "offset": 1452.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the same coin. I mean of course we we",
      "offset": 1454.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "can use this theoretical framework to",
      "offset": 1456.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "design new forms of SSL framework but",
      "offset": 1457.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "does it you know is the distinction",
      "offset": 1460.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "relevant if they are the same thing? I",
      "offset": 1462.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "think it's not just two side of the same",
      "offset": 1464.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "coin. SSL is more general than",
      "offset": 1466.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "supervised learning. So it's really SSL",
      "offset": 1468.48,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "could be the more general objective to",
      "offset": 1471.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "learn representation. The more prior",
      "offset": 1475.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "knowledge you have, the more you know",
      "offset": 1477.279,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "about your downstream task, the more you",
      "offset": 1478.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "know about your labels and then SSL like",
      "offset": 1480.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "slowly becomes supervised learning",
      "offset": 1482.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "through the label that you use for the",
      "offset": 1484.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "SSL objective. uh but then because as",
      "offset": 1486.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you said you have this hierarchy now it",
      "offset": 1490.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "does not really make sense to say you",
      "offset": 1492.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "have either supervised learning or SSL",
      "offset": 1493.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "rather what makes sense is to say okay",
      "offset": 1495.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what this uh relation matrix what this",
      "offset": 1497.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "pair wise matrix if you build it from",
      "offset": 1499.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "labels it's supervised learning if you",
      "offset": 1501.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "build it from other a priority knowledge",
      "offset": 1503.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "for example two consecutive frame in a",
      "offset": 1505.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "video basically have the same class then",
      "offset": 1507.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you are more in a unsupervised SSL",
      "offset": 1510.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "setting but it's all about how do you",
      "offset": 1512.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "build this per wise relation matrix",
      "offset": 1514.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that's the main question Very cool.",
      "offset": 1516.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Right, let's move on to our next paper.",
      "offset": 1518.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So, no location left behind, measuring",
      "offset": 1520.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and improving the fairness of implicit",
      "offset": 1522.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "representations for earth data. So,",
      "offset": 1524,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "there's loads and loads of modeling",
      "offset": 1526.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "frameworks now that do these implicit",
      "offset": 1527.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "neural representations of geospatial",
      "offset": 1529.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "earth data. So, things like climate",
      "offset": 1530.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "modeling, resource allocation,",
      "offset": 1532.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "environmental modeling. I was actually",
      "offset": 1534.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "interviewing Johannes from NXAI",
      "offset": 1535.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "yesterday. I don't know if you know him,",
      "offset": 1537.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "but he's working on on similar stuff.",
      "offset": 1538.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "The problem is you've studied this and",
      "offset": 1540.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you found that there's loads of like um",
      "offset": 1542,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "biases and fairness problems. Yeah,",
      "offset": 1543.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "exactly. So basically what we show is",
      "offset": 1545.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that uh when you want to model for",
      "offset": 1547.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "example let's say temperature or",
      "offset": 1549.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "precipitation to make it simple uh and",
      "offset": 1550.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you want to learn for example implicit",
      "offset": 1552.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "representation it means that you want a",
      "offset": 1554.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "model so that if you give a location and",
      "offset": 1556.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "a date for example it can predict what",
      "offset": 1559.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "was the temperature there. So if you",
      "offset": 1562.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have this type of implicit",
      "offset": 1564.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "representation it's very good because if",
      "offset": 1566,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you learn a nice model then you can",
      "offset": 1567.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "actually interpolate those values. So",
      "offset": 1569.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "maybe estimate what the temperature was",
      "offset": 1571.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "in this part of the globe where you did",
      "offset": 1573.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "not have a sensor. But you can also do",
      "offset": 1576.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "extraction as well. If you assume you",
      "offset": 1578.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "really learn the true physical model of",
      "offset": 1580.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the world, you could start saying okay",
      "offset": 1582.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "what the temperature will be two years",
      "offset": 1583.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "from now. Right? So this is very nice to",
      "offset": 1585.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "have this type of model for all sort of",
      "offset": 1587.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "applications. The thing is that when you",
      "offset": 1590.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "do this nowadays depending on the",
      "offset": 1592.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "architecture and the different design",
      "offset": 1595.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "choices that you do you will maybe a",
      "offset": 1596.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "very good prediction on average. So when",
      "offset": 1598.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you look at the rate performance around",
      "offset": 1600.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the world globe but actually if you look",
      "offset": 1602.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "for example around islands or coastal",
      "offset": 1604.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "area your prediction is going to be very",
      "offset": 1606.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "bad almost random. So this is something",
      "offset": 1609.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that can be very concerning because if",
      "offset": 1611.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "you use this type of model to decide",
      "offset": 1613.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about a policy that will affect a",
      "offset": 1615.279,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "specific island using this model",
      "offset": 1617.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "prediction is as good as using like",
      "offset": 1619.52,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "random guesses. So it can be very",
      "offset": 1621.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "detrimental and people need to be aware",
      "offset": 1624.679,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "of those biases. So what we found is",
      "offset": 1627.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that for example for this type of",
      "offset": 1630,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "climate data islands are often",
      "offset": 1631.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "disregarded coastal area basically",
      "offset": 1633.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "region where you have a big gradient in",
      "offset": 1635.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "the type of uh data that you try to",
      "offset": 1638.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "model. How much of a a responsibility do",
      "offset": 1640.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "modelers have you know to detect these",
      "offset": 1643.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "kinds of biases in the data? So I think",
      "offset": 1646,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there is like two components as you",
      "offset": 1648.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "said. So one could be that just the",
      "offset": 1650.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "dynamic of the data you're trying to",
      "offset": 1652.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "model is harder uh near island or maybe",
      "offset": 1654.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it's even unpredictable because you",
      "offset": 1656.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "don't have enough observations to do",
      "offset": 1658.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that. So you have some uh uncertainty",
      "offset": 1660.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that probably you can never recover from",
      "offset": 1662.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "good design. But still what we found",
      "offset": 1664.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "here is that a lot of the biases now",
      "offset": 1666.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "comes from the architecture and all you",
      "offset": 1669.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "want to do to encode those position the",
      "offset": 1671.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "type of basis you use to do the",
      "offset": 1674.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "prediction. So right now it seems that a",
      "offset": 1676,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "big chunk of the bias come from the",
      "offset": 1678.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "architecture but I totally agree that I",
      "offset": 1680.64,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "don't think we can remove the bias",
      "offset": 1683.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "entirely uh because there is maybe just",
      "offset": 1684.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "different type of uncertainty at",
      "offset": 1687.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "different part of the planet as well. I",
      "offset": 1688.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "mean the world is a very very",
      "offset": 1691.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "complicated place. I mean realistically",
      "offset": 1692.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to what extent can we mathematically",
      "offset": 1694.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "model it? Yeah. So that's a good",
      "offset": 1696.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "question. So I think it depends the type",
      "offset": 1697.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of horizon that you have and the type of",
      "offset": 1699.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "data that you want to model. If you have",
      "offset": 1701.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a system that is much more chaotic or",
      "offset": 1703.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "can vary very quickly without much",
      "offset": 1705.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "changes in the past observation, that's",
      "offset": 1707.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "something that current models are having",
      "offset": 1709.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a very hard time with. If you want to",
      "offset": 1712.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "predict something else, for example,",
      "offset": 1714.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "temperature uh in uh North America uh",
      "offset": 1715.679,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "not near the coastal area, so really",
      "offset": 1720.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "inland maybe that's why you have less",
      "offset": 1722.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "gradient dynamics, things are a bit more",
      "offset": 1724.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "stationary, especially and through time.",
      "offset": 1726.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "So then it can become much better. uh",
      "offset": 1728.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "but I think at this point we don't have",
      "offset": 1730.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "an architecture that is really able to",
      "offset": 1732.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "understand that you have different",
      "offset": 1734.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "physics different dynamics models at",
      "offset": 1736.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "different part of the globe uh and so",
      "offset": 1739.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "because of this you just see what's the",
      "offset": 1741.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "best on average and it means you miss",
      "offset": 1743.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "out a lot of details can you tell us",
      "offset": 1744.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "about some of the the technical",
      "offset": 1746.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "framework so uh one thing we showed for",
      "offset": 1748.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "example at least for this type of globe",
      "offset": 1750.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "data representation is that people use a",
      "offset": 1752.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "4year basis to model the prediction and",
      "offset": 1755.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this is something that is better than",
      "offset": 1758,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "not using any basis at all. But what it",
      "offset": 1760.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "means that you imply the type of signal",
      "offset": 1762.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you're predicting is very stationary and",
      "offset": 1764.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "not localized at all. And this is a very",
      "offset": 1766.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "strong prior right. So this may be true",
      "offset": 1769.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "for some things but for other things",
      "offset": 1771.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "like precipitation or temperature where",
      "offset": 1773.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you have localized very high gradients",
      "offset": 1776.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "then it's a strong bias. And if you come",
      "offset": 1779.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "from signal processing community you",
      "offset": 1782,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know very well that to have better",
      "offset": 1784.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "localization you go from 4 to wavelets.",
      "offset": 1786,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "And so that's one thing we did uh in",
      "offset": 1788.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "this paper and we showed that using",
      "offset": 1790.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "wavel basis to encode those uh data",
      "offset": 1792.159,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "allows you to have better localization",
      "offset": 1796,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and this removes uh some of the biases",
      "offset": 1798.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and here it's more of a proof of concept",
      "offset": 1801.279,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that different design choices give you",
      "offset": 1803.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "different type of bias trade-off is not",
      "offset": 1805.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the answer answer to everything right",
      "offset": 1808.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but I think the next step is to really",
      "offset": 1810.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "be able to encode less and less apparis",
      "offset": 1813.039,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "to use and let the model learn from the",
      "offset": 1816.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "data on its own and we are not yet at",
      "offset": 1819.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this point at least for this type of",
      "offset": 1821.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "climate data. How could it handle noisy",
      "offset": 1823.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "or missing data? This depends really on",
      "offset": 1825.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the type of model you use. So for",
      "offset": 1827.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "example, if you have INR, then you will",
      "offset": 1828.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "not use the missing data as part of your",
      "offset": 1831.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "training pipeline and that's one of the",
      "offset": 1833.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "benefit of them. So if one of your",
      "offset": 1834.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "sensor stop recording during some years,",
      "offset": 1837.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you just don't use that as part of your",
      "offset": 1839.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "training data because you really control",
      "offset": 1841.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "where do you have the data and when you",
      "offset": 1843.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have it? What the prediction uh should",
      "offset": 1845.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "be? So these earth models, they are now",
      "offset": 1847.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "informing policy around the world. Who",
      "offset": 1849.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "should we hold accountable? I mean is it",
      "offset": 1852.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the technology? Is it the the scientists",
      "offset": 1854.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "who design the models? Is it the policy",
      "offset": 1856.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "makers who interpret the results? I",
      "offset": 1858.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "think it's uh very hard for the person",
      "offset": 1861.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "who designs the model to know probably",
      "offset": 1863.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "what it's going to be used for. So I",
      "offset": 1865.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "think it's more downstream when you know",
      "offset": 1868.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "clearly what you want to do with it. You",
      "offset": 1870.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "should first set up a nice evaluation",
      "offset": 1872.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "pipeline to make sure that it's",
      "offset": 1874.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "something you can actually use to make",
      "offset": 1876.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "those decisions and then you can report",
      "offset": 1878,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "any type of failure mode you observe for",
      "offset": 1880.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "people to improve on the design. But a",
      "offset": 1883.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "priori it's very hard to imagine what",
      "offset": 1885.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this model will be used for. So in an",
      "offset": 1888.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "ideal setting you wish that there would",
      "offset": 1890.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "be no bias at all but in practice uh the",
      "offset": 1892.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "world of possibilities being so large uh",
      "offset": 1895.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it needs to be more of a feedback loop",
      "offset": 1897.44,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "uh and then iterate until you have",
      "offset": 1900.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "something that you can really trust and",
      "offset": 1903.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "then you can act on it. Earth um",
      "offset": 1904.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "modeling data is very anthropocentric",
      "offset": 1906.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "right so you know we we focus on on",
      "offset": 1908.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "human populations and so on. Should we",
      "offset": 1910.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "also focus on you know like just",
      "offset": 1912.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "ecosystems and places that have got",
      "offset": 1914.08,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "nothing to do with humans? Oh yeah,",
      "offset": 1915.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "that's a a great question and in fact",
      "offset": 1916.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that's one of the big issue with a lot",
      "offset": 1918.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of the data set which is crowdsourced",
      "offset": 1920.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "because by definition the amount of data",
      "offset": 1923.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that you get is proportional to the",
      "offset": 1925.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "number of users you have depending on",
      "offset": 1927.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the location and this means you have a",
      "offset": 1929.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "huge bias in what your model is learning",
      "offset": 1931.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and what your model is focusing on which",
      "offset": 1934.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "means you miss out on a lot of things.",
      "offset": 1936.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "So I think that's also one thing that",
      "offset": 1938.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "okay crowd sourcing can give you a lot",
      "offset": 1940.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of data quickly but it's very biased",
      "offset": 1942.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "data. So then the question is how much",
      "offset": 1944.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of this bias data versus maybe paying a",
      "offset": 1946.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "lot more and capturing other part of the",
      "offset": 1949.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "globe how much of the two you should",
      "offset": 1951.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have and maybe you could be able to show",
      "offset": 1953.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that under some specific condition just",
      "offset": 1955.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "having 10% of the data which is high",
      "offset": 1957.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "quality uniformly sample and then 90%",
      "offset": 1960.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "which is crowd sources you can try to",
      "offset": 1963.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "use those 10% to anchor your",
      "offset": 1965.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "representation and then use all that",
      "offset": 1966.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "data together but there is a huge amount",
      "offset": 1968.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of research question in that uh because",
      "offset": 1970.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that's a very big uh source of bias. And",
      "offset": 1973.36,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "this a bit of a policy question but we",
      "offset": 1976.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "are using these things you know to do",
      "offset": 1978.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "resource allocation right you know so um",
      "offset": 1980.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "giving more resources to to some",
      "offset": 1983.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "populations might be taking it away from",
      "offset": 1986.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "others and then there's the fairness",
      "offset": 1987.84,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "over time thing as well which is that",
      "offset": 1989.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "what is fair like now might not be fair",
      "offset": 1990.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in 100 years time so how should we think",
      "offset": 1992.799,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "about this yeah that's a good question I",
      "offset": 1994.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "think this is also very uh application",
      "offset": 1995.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "specific so for example if you want to",
      "offset": 1998.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "uh predict where to build a house to uh",
      "offset": 2000.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "solve some specific problem Maybe you",
      "offset": 2003.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "don't really mind having bad prediction",
      "offset": 2005.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "where there is no population anyway",
      "offset": 2007.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "because you're not going to build out",
      "offset": 2008.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "there. So in this case, maybe the crowd",
      "offset": 2010.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sourcing type of data is actually good.",
      "offset": 2012,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "But this could really be dependent on",
      "offset": 2014.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the type of application. And just one",
      "offset": 2016.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "thing I will say regarding the point you",
      "offset": 2018.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "made before this type of bias actually",
      "offset": 2020.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "is something that you have in computer",
      "offset": 2022.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "vision. So there is like a very nice uh",
      "offset": 2024.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "paper done by Mark Bra. Basically they",
      "offset": 2027.279,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "showed that most of the data we have",
      "offset": 2030.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like from imaget is from North America",
      "offset": 2031.919,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "and so maybe you reach like 90%",
      "offset": 2034.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "state-of-the-art performance to predict",
      "offset": 2036.2,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "for example type of chairs cars but only",
      "offset": 2038.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for North American models and when you",
      "offset": 2040.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "start looking at type of cars or chairs",
      "offset": 2042.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "in central Africa or East Asia suddenly",
      "offset": 2044.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the model performance is extremely bad",
      "offset": 2048.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "so this type of problem is something you",
      "offset": 2050.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "have across modalities and that's",
      "offset": 2052.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "something that's a very big issue",
      "offset": 2055.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Randall it's always It's a pleasure and",
      "offset": 2057.839,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "an honor to have you on the show. Thank",
      "offset": 2059.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you so much. Likewise. Likewise. Thank",
      "offset": 2060.639,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "you so much.",
      "offset": 2062.32,
      "duration": 3.319
    }
  ],
  "cleanText": "We just launched this experiment, and then we are very surprised to see that the hugely overparameterized model not only trains out of the box, like you have very nice training curves, but also they don't overfit aggressively at all. And what we found empirically is that we just out of the box use typical, like supervised training. We don't have to play with hyperparameter optimizer, and you have very, very stable training. So this brings also the question: is it worth it to spend so much money together, gigantic pre-training data set, spend like months on many GPUs to produce those models? But at least for some application, it seems to not be much better than.\n\nTufa AI Labs is sponsored by Tufa AI Labs. Now they are the Deepseek based in Zurich. They have an amazing team. You've seen many of the folks on the team they acquired at Mind's AI. Of course, they did a lot of great work on ARC. They're now working on 01 style models and reasoning and thinking and test time computation. The reason you want to work for them is you get loads of autonomy. You get visibility. You can publish your research, and also they are hiring as well as ML engineers. They're hiring a Chief Engineer. They really, really want to find the best possible person for this role, and they're prepared to pay top dollar as a joining bonus. So if you're interested in working for them as an ML engineer or their Chief Engineer, get in touch with Benjamin Crouzier, go to https://tufalabs.ai/ and see what happens.\n\nOriginally, the main motivation was to see, okay, how much information you gain by doing pre-training, right? And is this next token prediction really making your network learn something about language and reasoning? And so then we are saying, okay, one way to compare this, at least empirically, is to just take a randomly initialized model, train it from scratch on a supervised task like sentiment prediction, sentiment analysis. And then in theory, because we have a very, very small training data, let's say like 20,000 samples, and because those models have like 7 billion parameters, the pre-trained one will perform very nicely with a little bit of Laura fine-tuning because it already knows to reason about the formed, right? So maybe you just adjust a little bit to the specific task that you want, but since you have so many prior knowledge, you will solve the task very easily, but the random one either will overfit completely because you have like 7 billion parameters and only 20,000 training samples, or maybe it will not learn at all because, you know, training dynamics will be completely chaotic. And so we just launched this experiment, and then we're very surprised to see that the seven billion or like the hugely overparameterized model not only trains out of the box, like you have very nice training curves, almost like you train MNIST, but also they don't overfit aggressively at all, like they overfit less than if you just train MLP on MNIST, basically. And this is very surprising. And so, basically, from this, we said, okay, actually, maybe there is a deeper question we could, which could be how much implicit bias you have in those language models because already we knew from computer vision that, for example, ImageNet, you can have a 50 million model on a 1 million data set, so you have like this 50 to 1 ratio, and you have the implicit bias that prevents you from overfitting and just solving the task, right? But still, it's 50 to 1, so this may sound a lot for, you know, statistician, but now it's like 7 billion to 20,000, so like the ratio is like gigantic, right? And yeah, I don't know, to me it was very surprising that the scale, like the size of this ratio, still allows you to learn something that does not overfit, and this is very surprising because in vision, for example, transformers are known to overfit more easily than ResNet, so they seem at least in vision to have actually less implicit bias or implicit regularization, but at least with this type of next token causal architecture LLM, yeah, you don't seem to fit easily to your data. So this was a quite surprising, yeah. And and we should bring in the name, so this was your workshop paper at the self-supervised learning workshop here at NeurIPS, and it's called \"For Perception Tasks, Is LLM Pre-training by Next Token Prediction Worth the Cost?\" So this is absolutely fascinating, right? So we've been given this belief that we need to have these huge pre-trained models. They're trained on all the data on the internet. And it turns out that certainly for discrimination tasks, so things like classification rather than generation, actually, you can just start from scratch with a with a with a fairly small model, and you get sometimes even better results. Yeah. Yeah. And even small or even large model, like you just start from scratch. You do this very simple supervised classification task, right? Okay, given this prompt, is it good or a bad sentiment, or what type of job is the prompt describing? You know, this type of, we not call it reasoning, but yeah, more semantic classification, and turns out that, yeah, you start from random. Even if you have a small training data set, you will have performances that are sometimes as good as pre-trained model. So this, yeah, brings also the question: is it worth it to spend so much money to gather a gigantic pre-training data set, spend like months on many GPUs to produce those models? And for some cases, for generation, all right, there is no question, this is what you need to do. You have your next token prediction, you learn how to generate samples, but at least for some application, it seems to not be much better than random, so it's quite, quite interesting. So what are the differences in the learned representations?\n\nSo that's something we did not really look at, like low dimensional representation of what you learn. It's possible. So some work try to look at the attention entropy and the like, you know, those mechanistic interpretability viewpoint of LLM. So it will be interesting to see if you have this sort of, you know, neural collapse thing that happen. So even if you're like a 7 billion parameter, maybe you end up learning a very, very, very simple sub network that does the task, you know, bit like lottery ticket hypothesis as well, and that naturally emerge from the training dynamics, or is it really exploiting all the parameters? I think that's one thing. So to extend the workshop paper to conference, we want to probe into more, like, what are the useful parameters, what did they learn? Are each layer actually learning something, or maybe the first layers don't really learn anything, just the last few ones are learning something? So yeah, there is like lots of open questions for this.\n\nWhat does it tell us about the nature of understanding and maybe even intelligence? Because we we think that the reason why these things understand is because they're they they just have all of these representations to all of these different, you know, things in in in their experience, and and now we can shortcut to to, you know, to one of a better word. What does that tell us?\n\nYeah, I think that's a good question. So in this case, we must look at very specific classification tasks. So for example, you have a description of a job, what job it is, is it like a good or bad sentiment, and this you are able to solve it good, but you are not able to go out of distribution to solve a new type of question, for example, for this job description, then you cannot answer, okay, is this job paying you more than this job, because this was not present in the training data, right? So I think you get very good models cheaply, quickly from random initialization, but they will be very specialized, and I think the benefit of having maybe the pre-training may come if you want to do more of like open-ended classification or reasoning. So I think it really depends on the type of application you want to solve, what's your downstream task, and how much you want to generalize to new scenarios, but at least now it shows that, yeah, it's not just pre-training with next to prediction is better for everything. So I mean, going back five years, data scientists used to build specific classification models for doing everything. And now we're in this regime of we need these really big models, and we do in context learning and maybe even some fine-tuning, and and we get them to do fairly specific discriminative tasks. But now you're saying we should almost go back to where we were 5 years ago and start building specialized models again. Only now, rather than building classification models, we're actually we're still using the transformers and and the LLMs, but we're we're making them do specific tasks. Yeah. Yeah. Exactly. I think if you only want to solve a few specific tasks, use this prior knowledge to have a nice architecture, supervised data set for that, and just do that from scratch. This is something that's going to probably work much better. But again, you need to make sure that the downstream application will never go too out of distribution. So that's why it really depends on the application and the type of use cases that you have. But I think at least here it shows that there exists some task where next to prediction is not the answer, and in fact, it's not just not the answer, but it's not better than random initialization, which is really sort of the worst case scenario.\n\nInteresting. I mean, from a fairness and bias point of view, a lot of people say that, you know, large language models are bad in a way because there's the dominance of North American cultures and and so on. Um, but you could also argue the converse, which is that the good thing about them is that they do have some awareness of value, you know, so we can fine-tune them to um have guard rails and to sort of say the right thing and so on. Is that harder to do with this approach? Yeah. So here, because you are in a fully supervised setting, you don't have as much flexibility to, let's say, change the behavior of your model, where it will have to take the form of supervised fine-tuning, but because you don't have a generative capability, it's certainly restrict the type of interaction you have with the model, and you can improve it, right? Because the output is just, okay, is it a good or bad sentiment? It's not something that gives you a full answer that then you can try to argue against and generate a fine-tuning data set from. It's just, okay, good, bad, and that's it.\n\nAnother thing is training strategy. So, you know, like the big players building these LLMs, they have lots of internalized knowledge around um, you know, even the order in which you train the language models, everything is important, you know, certainly in the old days of like basic models, you know, you just stick a load of data in there, no one, no one really cares. So, you know, now do people need to be sort of thinking about the specialized knowledge, maybe thinking about curriculum learning and all of this kind of stuff? Yeah, so this is a good point. So we did a paper recently called \"The Fair Language Model Paradox,\" where we show that when you do this next token prediction, because you have some tokens that are very low frequency, it's very hard to train on them, and it takes a very long training, so it's very wasteful, right? And the problem is that because you do this next token prediction, you need to really capture all your distribution of tokens, and so you spend a lot of time, but in this case, if the low frequency tokens are not useful to solve your task, you actually don't need to capture it. So in terms of training dynamics, this is actually a much simpler problem in many cases, and what we found empirically is that we just out of the box use typical, like supervised training. We don't have to play with hyperparameter optimizer, and you have very, very stable training. So that's one thing that could be also interesting for future work is to see is this something that is easier to optimize, and maybe that's why those like 7 billion parameter model can learn and not overfit on like 10,000 samples, and then it's also bring other things that maybe this on its own could be a better initialization for next token prediction as well. So this is like very open up in the air. But maybe you could think of a simpler supervised objective that would be a better pre-training solution that then you can use for like next token prediction if you wanted to. But at least this will be a better starting point from random.\n\nSo you almost reverse the the trend. So we've spoken about two extremes. So on the one extreme, we have pre-training, and and you can like use it for any downstream task. And on the other extreme, we have, you know, you start from scratch just with one task. Is there an intermediate solution? So what if I did this new approach, but for multitask, let's say for five tasks? Yeah. Yeah. So that's a great question. So if you really think about it, in the limit, you could formulate a next token prediction as a multitask where you want to each task is the next token this one or not. So in the extreme case, you could just recover an ex token prediction on one end, and on the other end, you have what we have here. So just one task, very course eye level, predict if it's a good or bad sentiment or whatever. So in between, you have a huge spectrum that you can exploit, and if you can find, as you said, maybe five very different representative tasks, this you should be enough to, or could be enough to learn a representation that is as general as possible, and then you can use this for maybe new tasks that come on the go. So I think the research question is how to design the minimum amount of tasks so that you have as diverse representation as possible, and of course, we don't want to go to the extreme of just doing again next token prediction. But this this is a very, very nice research question because if you have this spectrum and you can control where you want to be, then you can really have a per use case choice. So it's not, okay, you're always here or always here. Tell me what you want to do, how much new task you expect your model to be exposed to, and I tell you where you need to be in this spectrum. So this could be like very interesting as well. Very cool. Very cool. It does make me think though that these models understand through naive statistical alignment, and is it possible that the benchmarks we use just don't cap, you know, they the gap of understanding that we've lost from moving from the pre-trained models isn't being captured? Yeah, I think because especially in the recent years, we focus a lot on generative decoder only methods, all the evaluation and the type of objectives we put on ourselves in really is really about good generation, right? Even if you want to answer a question, you need to generate a good explanation, you need to understand what are the intermediate steps, and I think the fact that we focus on generative models means that we completely bias the evaluation and the way we approach this, and maybe you could have still knowledge that is learned without being able to generate anything. So I think this is also something that could be interesting to look at or at least keep in mind when we explore those models. But philosophically though, isn't generation analogous to thinking in some sense? So don't don't models that generate, aren't they smarter in some deep way? Probably what you want to do is maybe imagine what could be, but I don't think you want.\n\n\nTo do generation is with very granular details like next token generation, because if you think about it, even just in terms of, like, uh, classification task, you have a lot of different uncertainty depending on the token. If I start the sentence, \"Okay, I saw this movie for minutes,\" there is no way you can tell what was the next token for after four, right? So this means that you... No, it would be like a time component, right? Maybe it's like 1 hour, 10 minutes, 2 hours. But do you really need to be able to generate the, I don't know, 52 minutes or whatever the answer was to actually understand that I was seeing a movie, therefore I was staying in a place for at least more than 5 seconds, right? So I think token is way too granular. Uh, and if you had maybe like concept token, that's where you could start seeing, \"Okay, this is meaningful,\" because that's closer to maybe, uh, what we do. But right now, we are very, very, very low level, because tokenization is a lossless compression, right? So this is too close to the uh raw data, and yet we have the life easy compared to computer vision, because already you work in language, which is very compressed representation of knowledge, uh, but still token is probably too low level.\n\nStill, well, that was a fascinating paper. Let's move on to your next one. So, \"The Birth of Self-Supervised Learning: A Supervised Theory,\" and that was with Yann LeCun, and um, yeah, basically, you said that the observed differences between self-supervised learning and supervised learning are not due to the loss function themselves, but rather the labeling of the data set used in training. Give us the elevator pitch.\n\nYeah. So, basically, what we show in this paper is that you can have a supervised objective, like, let's say, least squares, to make it simple. Uh, so you have the inputs, you have your network's prediction, and you have the labels, and you can turn this objective, which tries to predict sample XN to prediction YN, into a self-supervised learning objective, which tries which tries to compare samples with each other. So, basically, you go from saying, \"Okay, uh, this image is a car or a dog,\" to saying, \"Are those two images the same or not?\" which is like the self-supervised uh type of jointing world. Uh, and so you can show that if you have labels or you have knowledge of this pairwise relationship, they actually learning the same representation up to some symmetry that is irrelevant if you do linear probing. So, the loss function in itself, the SSL one or the supervised one, try to do the same thing. They just operate on a different view of the labeling, whether this image is that, or are those two images or two samples uh representing the same thing. So, given that, then the next question is, \"Okay, how can self-supervised learning is able to generalize better than supervised?\" And from this perspective, what you can say is that it's because it's as if they were solving a supervised task where the labels are not about predicting all the cars to cars, but are very, very, very fine-grain label, where in the limit, each image is its own class, basically. So, if you think about supervised learning in this uh extreme setting, you also don't overfit to the task, because you don't collapse any image to another one, and so theoretically speaking, you can solve many downstream tasks uh as you want. So, this equivalence of losses at least brings a slight new perspective on the fact that it's not really about the objective. It's more about, \"Oh, you design the SSL pipeline,\" or you say, \"Okay, with this sample is related to this sample,\" but it's not the objective that makes you learn a better representation. Okay.\n\nAnd in the paper, you were talking about how SSL can maximize the worst-case downstream task performance. Can you sketch that?\n\nYeah. So, basically, if you think about all the possible realization of downstream tasks, you could have some very core scale ones. You have maybe different pictures of cars and buses, and you just want to say it's a car or a bus. So, no details need to be encoded to solve this. But then you can have downstream tasks where you want to say, \"Okay, which brand of car is it, or which color of car is it?\" So, you have a distribution of downstream tasks, right? And so the point now is that you want to learn a representation so that if you look at the distribution of downstream task performance, you are able to be as good as possible on most of them, right? So, you don't want to be very good on some, and then in the tail, you are very bad on the majority of them. And so then from this, you can try to say, \"Okay, what will be the labeling that tries to make your worst case as good as possible?\" And from this, you can say, \"Okay, this is actually the labeling that self-supervisor actually implicitly uh doing.\"\n\nHow does the class the class balance affect the the difference in the losses?\n\nOh, yeah. So, this is a very good point, actually, in a follow-up paper we're doing right now. We show that current SSL objectives assume class balanceness. And this is something we already highlighted quickly in this uh SL super learning as a uniform cluster prior paper we did a couple years ago. And we show that current SSL objectives assume balanced representation of classes or concepts. And this means that if you train on image, things work out very well, because concepts are sort of equally represented. But then if you go to other data sets like iNaturalist, which are very heavy tail, then you have a huge bias in your representation. So, until now, people did not really know how to solve this, and so one way uh people approach this is through data curation, and they say, \"Okay, I'm just going to remove the over-sampled concepts to try to make it more uniform, and then I do self-supervised learning on this.\" But because now we have this theoretical formulation and this equivalence of losses, we can use the exact same setting that people used in supervised learning to reweigh depending on the frequency of classes. We can use that to come up with a new self-supervised learning loss that takes this imbalance into account. So, this type of thing is enabled from this mathematical formulation, and it's principle. So, the way we do this waiting, you can prove that it's the right way to do it from this supervised theory. And so this is really nice, because suddenly from this seemingly naive connection, you can now come up with new generation of self-supervised learning models where you can actually match what the real-world data data distribution is like. So, uh, non-uniform distribution of classes, maybe even if you have some samples that are more noisy than others, you can include that information as part of the SSL objective as well. So, suddenly you have a world new world of possibilities that comes, and because there is this connection, you can actually prove, \"Okay, this is the right way to do it,\" at least from this supervised Siri viewpoint.\n\nYou also pointed out a connection to VICReg.\n\nExactly. So, basically, uh, what we do in the paper is that we show if you have a least square supervised type of objective, and you turn it into a SSL one, you obtain is basically VICReg. So, then you have a few variation, it could be VICReg or WMS, depending on how you do this uh from supervised to SSL, but you can show that depending on the type of supervised loss, you will cover different type of SSL ones. If you look maybe more at cross-entropy, supervised learning is going to be more like SimCLR type of loss, but you have this one-to-one correspondence. And this is also very nice, because in supervised learning, at least you know when one loss maybe uh preferred compared to another one, and this has been studied for a long time, right? Because supervised learning has been around forever, and so now we can reuse those insights for self-supervised learning. So, this to me is also a very, very strong benefit of this thing is that suddenly all the theory and like the thousands of papers that have been done in supervised learning, we can just take it and apply it in SSL. Uh, another example is a neural collapse, for example, that has been proven in supervised setting. Now it applies like in five lines in a SSL setting as well. So, this connection is really beyond just trying to say, \"Okay, it's not the objective that make SSL better.\" It's really tying those two uh huge communities together towards a goal where you have a single unified objective to learn representation. And this is nice, too, because if you speak to people, they will think, \"Okay, you have supervised learning on one side, SSL on the other side,\" and basically you are either in one camp or the other, but now what we show is that you actually SSL is pretty much everything in representation learning, and supervis is just one realization of SSL. Then uh VICReg without label is another one. Then this one is another one. So, you really have a better understanding of this relationship and what learning is trying to do.\n\nGalaxy brain question incoming. Could you combine SSL and supervised objectives in in some way to improve generalization?\n\nYes. Yes. So, uh, there is one paper which is supervised contrastive learning. So, the way they do it is that they use the labels within a SimCLR framework to try to basically do fully supervised learning, but with a SimCLR objective. So, first of all, we can show that indeed this makes sense, and that basically we can explain the empirical result that they had, but actually we can do a little bit more than that. So, if you are in a semi-supervised setting, for example, it may not be clear how to combine those two losses anymore, or maybe you could say, \"Okay, I have the two, and I have a coefficient to weight them,\" then you need to do cross-validation and so on, but now from this perspective, you can combine them in a very principled way, and you can understand which weighting makes sense depending on how much sample you have in one or the other, and you can use all the literature again from like supervised learning uh for this setting as well. So, this is something can do very easily with this formulation as well.\n\nOkay. So, if SSL and supervised are two sides of the same coin. I mean, of course, we we can use this theoretical framework to design new forms of SSL framework, but does it, you know, is the distinction relevant if they are the same thing?\n\nI think it's not just two sides of the same coin. SSL is more general than supervised learning. So, it's really SSL could be the more general objective to learn representation. The more prior knowledge you have, the more you know about your downstream task, the more you know about your labels, and then SSL like slowly becomes supervised learning through the label that you use for the SSL objective. Uh, but then because as you said, you have this hierarchy, now it does not really make sense to say you have either supervised learning or SSL, rather what makes sense is to say, \"Okay, what this uh relation matrix, what this pairwise matrix, if you build it from labels, it's supervised learning. If you build it from other a priori knowledge, for example, two consecutive frames in a video basically have the same class, then you are more in a unsupervised SSL setting, but it's all about how do you build this pairwise relation matrix, that's the main question.\"\n\nVery cool. Right, let's move on to our next paper. So, \"No Location Left Behind: Measuring and Improving the Fairness of Implicit Representations for Earth Data.\" So, there's loads and loads of modeling frameworks now that do these implicit neural representations of geospatial earth data. So, things like climate modeling, resource allocation, environmental modeling. I was actually interviewing Johannes from NXAI yesterday. I don't know if you know him, but he's working on on similar stuff. The problem is you've studied this and you found that there's loads of like um biases and fairness problems.\n\nYeah, exactly. So, basically, what we show is that uh when you want to model, for example, let's say temperature or precipitation, to make it simple, uh, and you want to learn, for example, implicit representation, it means that you want a model so that if you give a location and a date, for example, it can predict what was the temperature there. So, if you have this type of implicit representation, it's very good, because if you learn a nice model, then you can actually interpolate those values. So, maybe estimate what the temperature was in this part of the globe where you did not have a sensor. But you can also do extraction as well. If you assume you really learn the true physical model of the world, you could start saying, \"Okay, what the temperature will be two years from now,\" right? So, this is very nice to have this type of model for all sort of applications. The thing is that when you do this nowadays, depending on the architecture and the different design choices that you do, you will maybe a very good prediction on average. So, when you look at the rate performance around the world globe, but actually if you look, for example, around islands or coastal area, your prediction is going to be very bad, almost random. So, this is something that can be very concerning, because if you use this type of model to decide about a policy that will affect a specific island, using this model prediction is as good as using like random guesses. So, it can be very detrimental, and people need to be aware of those biases. So, what we found is that, for example, for this type of climate data, islands are often disregarded, coastal area, basically region where you have a big gradient in the type of uh data that you try to model.\n\nHow much of a a responsibility do modelers have, you know, to detect these kinds of biases in the data?\n\nSo, I think there is like two components as you said. So, one could be that just the dynamic of the data you're trying to model is harder uh near island, or maybe it's even unpredictable, because you don't have enough observations to do that. So, you have some uh uncertainty that probably you can never recover from good design. But still, what we found here is that a lot of the biases now comes from the architecture and all you want to do to encode those position, the type of basis you use to do the prediction. So, right now, it seems that a big chunk of the bias come from the architecture, but I totally agree that I don't think we can remove the bias entirely uh because there is maybe just different type of uncertainty at different part of the planet as well. I mean, the world is a very, very complicated place. I mean, realistically, to what extent can we mathematically model it?\n\nYeah. So, that's a good question. So, I think it depends the type of horizon that you have and the type of data that you want to model. If you have a system that is much more chaotic or can vary very quickly without much changes in the past observation, that's something that current models are having a very hard time with. If you want to predict something else, for example, temperature uh in uh North America uh not near the coastal area, so really inland, maybe that's why you have less gradient dynamics, things are a bit more stationary, especially and through time. So, then it can become much better. Uh, but I think at this point, we don't have an architecture that is really able to understand that you have different physics, different dynamics models at different part of the globe, uh, and so because of this, you just see what's the best on average, and it means you miss out a lot of details.\n\nCan you tell us about some of the the technical framework?\n\nSo, uh, one thing we showed, for example, at least for this type of globe data representation, is that people use a 4-year basis to model the prediction, and this is something that is better than not using any basis at all. But what it means that you imply the type of signal you're predicting is very stationary and\n\n\nNot localized at all.\nAnd this is a very strong prior, right?\nSo this may be true for some things, but for other things, like precipitation or temperature, where you have localized very high gradients, then it's a strong bias.\nAnd if you come from signal processing community, you know very well that to have better localization, you go from 4 to wavelets.\nAnd so that's one thing we did in this paper, and we showed that using wavelet basis to encode those data allows you to have better localization, and this removes some of the biases.\nAnd here it's more of a proof of concept that different design choices give you different type of bias trade-off.\nIt's not the answer to everything, right?\nBut I think the next step is to really be able to encode less and less apparatus to use and let the model learn from the data on its own, and we are not yet at this point, at least for this type of climate data.\nHow could it handle noisy or missing data?\nThis depends really on the type of model you use.\nSo, for example, if you have INR, then you will not use the missing data as part of your training pipeline, and that's one of the benefits of them.\nSo if one of your sensor stops recording during some years, you just don't use that as part of your training data because you really control where do you have the data and when you have it?\nWhat the prediction should be?\nSo these earth models, they are now informing policy around the world.\nWho should we hold accountable?\nI mean, is it the technology?\nIs it the scientists who design the models?\nIs it the policy makers who interpret the results?\nI think it's very hard for the person who designs the model to know probably what it's going to be used for.\nSo I think it's more downstream when you know clearly what you want to do with it.\nYou should first set up a nice evaluation pipeline to make sure that it's something you can actually use to make those decisions, and then you can report any type of failure mode you observe for people to improve on the design.\nBut a priori, it's very hard to imagine what this model will be used for.\nSo in an ideal setting, you wish that there would be no bias at all, but in practice, the world of possibilities being so large, it needs to be more of a feedback loop and then iterate until you have something that you can really trust, and then you can act on it.\nEarth modeling data is very anthropocentric, right?\nSo you know, we focus on human populations and so on.\nShould we also focus on, you know, like just ecosystems and places that have got nothing to do with humans?\nOh yeah, that's a great question, and in fact, that's one of the big issues with a lot of the data set which is crowdsourced because by definition the amount of data that you get is proportional to the number of users you have depending on the location, and this means you have a huge bias in what your model is learning and what your model is focusing on, which means you miss out on a lot of things.\nSo I think that's also one thing that okay, crowd sourcing can give you a lot of data quickly, but it's very biased data.\nSo then the question is how much of this bias data versus maybe paying a lot more and capturing other part of the globe, how much of the two you should have, and maybe you could be able to show that under some specific condition, just having 10% of the data which is high quality uniformly sample and then 90% which is crowd sources, you can try to use those 10% to anchor your representation and then use all that data together, but there is a huge amount of research question in that because that's a very big source of bias.\nAnd this a bit of a policy question, but we are using these things, you know, to do resource allocation, right?\nYou know, so um, giving more resources to some populations might be taking it away from others, and then there's the fairness over time thing as well, which is that what is fair like now might not be fair in 100 years time, so how should we think about this?\nYeah, that's a good question.\nI think this is also very application specific.\nSo, for example, if you want to predict where to build a house to solve some specific problem, maybe you don't really mind having bad prediction where there is no population anyway because you're not going to build out there.\nSo in this case, maybe the crowd sourcing type of data is actually good.\nBut this could really be dependent on the type of application.\nAnd just one thing I will say regarding the point you made before, this type of bias actually is something that you have in computer vision.\nSo there is like a very nice paper done by Mark Ibrahim.\nBasically, they showed that most of the data we have, like from ImageNet, is from North America, and so maybe you reach like 90% state-of-the-art performance to predict, for example, type of chairs, cars, but only for North American models, and when you start looking at type of cars or chairs in central Africa or East Asia, suddenly the model performance is extremely bad.\nSo this type of problem is something you have across modalities, and that's something that's a very big issue.\nRandall, it's always It's a pleasure and an honor to have you on the show.\nThank you so much.\nLikewise.\nLikewise.\nThank you so much.\n",
  "dumpedAt": "2025-07-21T18:43:25.227Z"
}