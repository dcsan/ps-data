{
  "episodeId": "j13ySJLvdOc",
  "channelSlug": "@machinelearningstreettalk",
  "title": "Three Red Lines We're About to Cross Toward AGI",
  "publishedAt": "2025-06-24T04:49:17.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Okay, take one.\n All right, here we are",
      "offset": 0.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "saving humanity. Take one.\n Having, say,",
      "offset": 2.72,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "for instance, the United States disrupt",
      "offset": 5.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "China's ability to develop a super",
      "offset": 8.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "intelligence. The main way in which they",
      "offset": 11.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "would develop it is if they get the",
      "offset": 12.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "ability to automate AI research and",
      "offset": 14.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "development fully and take the human out",
      "offset": 16.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of the loop. Then you go from human",
      "offset": 18.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "speed to machine speed. We've had people",
      "offset": 20.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "such as Daario, for instance, it's",
      "offset": 22.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "gioanthropic, talk about how such a",
      "offset": 24.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "recursive process like that could lead",
      "offset": 26.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to an intelligence explosion and that",
      "offset": 28.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "would lead to a durable edge where",
      "offset": 30.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "nobody will be able to catch up. And",
      "offset": 33.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "last week, Sam Alman discussed how this",
      "offset": 35.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "process could telescope a decade's worth",
      "offset": 37.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of AI development in a year or",
      "offset": 40.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "potentially a month. There's been this",
      "offset": 42.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "very seductive argument that has",
      "offset": 44.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "appealed to all of these people, which",
      "offset": 46.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "is basically, well, it's probably going",
      "offset": 47.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to happen anyway. If we don't do it,",
      "offset": 49.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "someone else will. Basically, all of",
      "offset": 50.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "these people sort of trust themselves",
      "offset": 52.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "more than they trust everyone else and",
      "offset": 53.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have therefore convinced themselves that",
      "offset": 56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "even though these risks are real, the",
      "offset": 57.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "best way to deal with them is for them",
      "offset": 59.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "to go as fast as possible and win the",
      "offset": 60.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "race. Why did they make OpenAI? Well,",
      "offset": 62.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "they were worried that they didn't trust",
      "offset": 64.799,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "Demis to handle all that power",
      "offset": 66.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "responsibly when he was in charge of the",
      "offset": 67.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "AI project and all the fate of the world",
      "offset": 70,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "rested in his hand. So they wanted to",
      "offset": 71.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "create OpenAI to be this counterveiling",
      "offset": 72.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "force that could do it right and",
      "offset": 75.04,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "distribute it to everybody and not",
      "offset": 77.119,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "concentrate power so much in Demis'",
      "offset": 78.479,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "hands. And in fact, the emails that came",
      "offset": 80.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "up in the lawsuit, they were talking",
      "offset": 81.84,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "about how they were worried that Demis",
      "offset": 82.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "would become dictator using AI.",
      "offset": 84.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "On the protoi things, I think that they",
      "offset": 87.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "follow the instru instructions fairly",
      "offset": 89.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "reasonably. There are some other parts",
      "offset": 92.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of it.\n Yeah, that scares the out of",
      "offset": 95.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "me, right? Fairly reasonably. you know,",
      "offset": 96.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "when these things are still in our hands",
      "offset": 99.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "kind of that's okay. But if you have",
      "offset": 101.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "them guiding weapons or something like",
      "offset": 102.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that, there are many circumstances where",
      "offset": 104.4,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "fairly reasonably is not good enough.",
      "offset": 106.56,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "This episode of MLST is sponsored by",
      "offset": 116.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "Tufa AI Labs. It is a research lab which",
      "offset": 118.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "is headquartered in Zurich. They're",
      "offset": 121.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "moving to San Francisco as well. These",
      "offset": 124,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "guys are number one of the ARK v2",
      "offset": 126.159,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "leaderboard. They're genuinely",
      "offset": 128,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "fascinated in building the next",
      "offset": 129.599,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "generation of technology, the next",
      "offset": 131.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "innovation which will take large",
      "offset": 133.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "language models to the next stage. If",
      "offset": 135.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that sounds like you, please get in",
      "offset": 137.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "touch with Benjamin Kruier. Go to",
      "offset": 139.68,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "twoferabs.ai.",
      "offset": 141.599,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "This podcast is supported by Google. Hey",
      "offset": 146.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "everyone, David here, one of the product",
      "offset": 149.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "leads for Google Gemini. If you dream it",
      "offset": 151.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and describe it, V3 and Gemini can help",
      "offset": 153.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you bring it to life as a video. Now",
      "offset": 156.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "with incredible sound effects,",
      "offset": 158.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "background noise, and even dialogue. Try",
      "offset": 160.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it with a Google AI Pro plan or get the",
      "offset": 163.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "highest access with the Ultra plan. Sign",
      "offset": 165.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "up at gemini.google to get started and",
      "offset": 167.68,
      "duration": 5.42
    },
    {
      "lang": "en",
      "text": "show us what you create.",
      "offset": 170.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 173.1,
      "duration": 4.34
    },
    {
      "lang": "en",
      "text": "All right, so I'm Gary Marcus. I'm a",
      "offset": 175.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "cognitive scientist, uh an entrepreneur.",
      "offset": 177.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Um, I've written six books, most",
      "offset": 180.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "recently Taming Silicon Valley. I think",
      "offset": 181.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that uh and here we are in the heart of",
      "offset": 184.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Silicon Valley as we record this in San",
      "offset": 185.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Francisco. Um, and I think we all want",
      "offset": 188.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "what is best for humanity and and hope",
      "offset": 190.8,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that that AI can be positive for",
      "offset": 193.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "humanity and not negative. I think the",
      "offset": 194.959,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "three of us are maybe not known for",
      "offset": 197.2,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "agreeing on everything, but we actually",
      "offset": 198.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "have a lot of shared values around that",
      "offset": 200.159,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "and I'm looking forward to the",
      "offset": 201.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "conversation.\n Thanks, Gary. Uh, my name",
      "offset": 202.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is Daniel Cocatello. I'm the executive",
      "offset": 205.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "director of the AI Futures Project. Uh",
      "offset": 207.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we are the people who made AI 2027 which",
      "offset": 209.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is a comprehensive detailed scenario",
      "offset": 212.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "forecast of the future of AI.\n I'm Dan.",
      "offset": 214.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I'm the director of the center for AI",
      "offset": 218.08,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "safety. I also advise scale AI and XAI.",
      "offset": 219.84,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "I also have done research uh in machine",
      "offset": 224.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "learning like I coined the the Jello and",
      "offset": 227.36,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the Celu a common activation function.",
      "offset": 230.08,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "Uh more recently I've done evaluations",
      "offset": 232.959,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "to evaluate capabilities such as MMLU",
      "offset": 236.159,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and the math benchmark and humanity's",
      "offset": 239.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "last exam. Uh and I've been focusing on",
      "offset": 241.599,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "trying to uh measure aspects of",
      "offset": 244.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "intelligence and also AI systems safety",
      "offset": 246.959,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "properties for uh mostly the course of",
      "offset": 249.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "my whole career.\n The common things that",
      "offset": 252.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "we care about are what is going to be",
      "offset": 255.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the outcome when AGI comes? Um how soon",
      "offset": 257.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "is it going to come? How do we forecast",
      "offset": 261.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "these things methodologically? Um, one",
      "offset": 263.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "person wrote in a question that maybe we",
      "offset": 266.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "could actually start, which is like what",
      "offset": 268.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is the positive view? Like I I take it",
      "offset": 270.639,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "that all three of us in the room are",
      "offset": 273.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "united in thinking we think AGI could be",
      "offset": 277.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a good thing. None of us is sort of like",
      "offset": 279.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "standing, you know, down like people",
      "offset": 282.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "would in front of a um construction",
      "offset": 284.479,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "machine saying stop it all period. we I",
      "offset": 286.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "think we all think there's at least a",
      "offset": 289.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "possibility of a positive outcome, but",
      "offset": 291.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you can stop me if I'm wrong. Um, and",
      "offset": 293.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that we're hoping to steer towards that",
      "offset": 294.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "positive outcome. So, our first question",
      "offset": 296.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "is what could be the upside here? And",
      "offset": 298.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "why aren't you just saying let's just",
      "offset": 301.199,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "forget about AI altogether?\n I can start",
      "offset": 302.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with that. Yeah. So, so first of all, I",
      "offset": 305.759,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "actually think it's quite reasonable to",
      "offset": 307.28,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "basically stand in front of the",
      "offset": 308.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "bulldozer and say stop. I think that",
      "offset": 309.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "eventually we want to build AI because",
      "offset": 311.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it is possible to get it right and it is",
      "offset": 313.759,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "possible to make it in such a way that's",
      "offset": 315.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "you know beneficial to everyone and",
      "offset": 317.759,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "massively beneficial to everyone in",
      "offset": 319.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "fact. Um but uh if we are currently not",
      "offset": 320.72,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "on a track to getting it right and we're",
      "offset": 324.56,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "currently on a track to make it in a way",
      "offset": 326.479,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "that's going to be horrible then it",
      "offset": 327.759,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "makes sense to just sort of stop until",
      "offset": 329.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we figure out a better way. Um but uh",
      "offset": 330.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "yeah, as for what the benefits could",
      "offset": 333.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "look like, well, uh AI 2027 has the",
      "offset": 336.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "slowdown ending in which things go",
      "offset": 340.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "really well for almost everybody. And",
      "offset": 342.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "since since not everybody read I think",
      "offset": 345.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "more people probably read the um darker",
      "offset": 348.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "scenario than the positive scenario. Um",
      "offset": 350.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "lay out uh for people who might not have",
      "offset": 352.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "read the positive scenario a little bit",
      "offset": 354.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "about what the spirit of that is.\n Sure.",
      "offset": 356.16,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "So in the in the slowdown ending of AI",
      "offset": 358.16,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "2027 uh they manage to devote just",
      "offset": 362.479,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "barely enough effort and time and",
      "offset": 366.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "resources towards the technical",
      "offset": 368.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "alignment problem that they solve it uh",
      "offset": 370.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "just barely in time so that they can",
      "offset": 372.88,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "still uh beat China as they have wanted",
      "offset": 375.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to do. By they I mean you know the",
      "offset": 378.479,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "leader of the of the leading AI company",
      "offset": 380.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and the president and maybe some other",
      "offset": 383.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "companies right that group of people.",
      "offset": 385.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Um, so",
      "offset": 387.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you know, the slowdown ending is not our",
      "offset": 389.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "recommendation for what we should",
      "offset": 391.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "actually aim for. We think it's an to to",
      "offset": 392.639,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "aim for this sort of scenario would be",
      "offset": 395.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to expose the world to incredible",
      "offset": 397.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "amounts of risk of various types. So we",
      "offset": 399.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "don't think it's our recommendation, but",
      "offset": 401.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "nevertheless, it's a it's a coherent",
      "offset": 403.52,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "plausible scenario for how things could",
      "offset": 406.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sort of muddle through and end up pretty",
      "offset": 407.759,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "well for for everybody.\n And pretty well",
      "offset": 410.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is I mean like it's the Peter Peter",
      "offset": 412.319,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Demandis abundance scenario for example.",
      "offset": 414.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Is that what you have in mind?\n I haven't",
      "offset": 417.759,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "actually read that but but I would say",
      "offset": 419.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "when you get the AIs that are super",
      "offset": 422.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "intelligent so what what I mean by that",
      "offset": 424.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is better than the best humans at",
      "offset": 426.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "everything and also faster and cheaper.",
      "offset": 428.16,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Uh you can just completely transform the",
      "offset": 430.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "economy. you know, super intelligent",
      "offset": 433.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "designed robot factories constructed in",
      "offset": 435.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "record speed, producing all these",
      "offset": 437.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "amazing robots and all these amazing new",
      "offset": 439.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "industrial equipment which then are used",
      "offset": 441.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to construct new types of factories and",
      "offset": 443.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "new types of laboratories to do all",
      "offset": 444.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "sorts of experiments to build all sorts",
      "offset": 446.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "of new technologies and blah blah blah",
      "offset": 447.599,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "blah blah blah. Eventually, you get and",
      "offset": 448.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "by eventually I mean in only a few years",
      "offset": 450.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "uh you get to a completely automated",
      "offset": 452.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "wonderful economy of all sorts of new",
      "offset": 455.919,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "technologies that have been iterated on",
      "offset": 458.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and designed by super intelligences.",
      "offset": 459.759,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "material needs are basically just met",
      "offset": 462.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "for everybody. You know, there's just an",
      "offset": 464.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "incredible abundance of wealth uh to",
      "offset": 466.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "distribute and uh you know things like",
      "offset": 468.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "curing all sorts of diseases uh putting",
      "offset": 471.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "up new new settlements on Mars and so",
      "offset": 474.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "forth all that stuff becomes possible.",
      "offset": 477.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Um so that's the sort of like potential",
      "offset": 479.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "potential upside and then of course",
      "offset": 483.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "there's the question of uh can can we",
      "offset": 485.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "actually achieve that right and and if",
      "offset": 487.919,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "we do have the technology to achieve",
      "offset": 490.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that who's in control of the technology",
      "offset": 491.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and do they actually use their power",
      "offset": 493.36,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "over uh the army of super intelligences",
      "offset": 496.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to make that sort of broadly distributed",
      "offset": 499.599,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "good future for everybody or do they do",
      "offset": 502,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "something that's more dystopian you know",
      "offset": 503.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "so I I think we\n well first of We'll come",
      "offset": 506.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "back to the bulldozers and whether you",
      "offset": 509.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "think we're actually at that point um or",
      "offset": 511.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "not. But I think we can agree probably",
      "offset": 513.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and and we'll let you chime in too. Um",
      "offset": 516.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that there is a technical alignment",
      "offset": 518.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "problem and we should talk about that",
      "offset": 520.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and a political if it's not alignment",
      "offset": 521.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "problem maybe that's too close pairing",
      "offset": 524,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of words but a political issue about if",
      "offset": 525.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "this technology exists who controls it",
      "offset": 528.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that's deeply important. Yeah.\n And so we",
      "offset": 530.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "should talk about both sides of that",
      "offset": 532.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "sort of\n political control and technical",
      "offset": 534.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "alignment. Um let me get your answer",
      "offset": 536.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "though on on that first question. You",
      "offset": 538.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "know, do you think we need to stand in",
      "offset": 540.56,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "front of a bulldozer now? And if you",
      "offset": 541.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "don't, you know, what do you think is",
      "offset": 543.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the positive?\n Yeah. So I I think there",
      "offset": 545.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "are two concepts. There's there's AGI",
      "offset": 547.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and there's ASI the artificial general",
      "offset": 549.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "intelligence and artificial super",
      "offset": 551.92,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "intelligence. I think AGI doesn't have a",
      "offset": 553.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "clear definition. So it's difficult for",
      "offset": 555.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "me to say for all definitions of that",
      "offset": 556.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that would be worth standing in front",
      "offset": 558.959,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "of. Some people would say we have AGI",
      "offset": 560.24,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "already.\n Let's just start with what we",
      "offset": 561.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have now. Yeah. You know, there could be",
      "offset": 562.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "an argument that right now we should",
      "offset": 565.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just stop the train. Sorry, I've moved",
      "offset": 567.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "from bulldozers to trains. Um, you know,",
      "offset": 569.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "there'd be an argument. Um, I'm not",
      "offset": 571.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "making this argument, but some people",
      "offset": 573.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "have made it. The argument would be",
      "offset": 574.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we're already so far along this path,",
      "offset": 576.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and if we don't resist it massively",
      "offset": 578.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "right now, it's going to be bad, and",
      "offset": 580.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "there isn't a good outcome here, a good",
      "offset": 582.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "enough outcome to offset what seems",
      "offset": 584.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "inevitable. I'm not making that",
      "offset": 586.48,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "argument, but I've heard people make it.",
      "offset": 587.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And so, partly, you know, grounding",
      "offset": 589.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "exercise, where are we on that? So, you",
      "offset": 591.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "know, one argument would be it. We",
      "offset": 593.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we've already screwed up. We need to put",
      "offset": 596.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have every person on the planet get in",
      "offset": 598.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "front of this train or bulldozer and and",
      "offset": 600.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "say just stop it because there isn't a",
      "offset": 602.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "positive enough outcome to warrant it.",
      "offset": 605.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Another position would be there's",
      "offset": 607.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually a really positive outcome here",
      "offset": 609.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and if we can guide it in the right way,",
      "offset": 611.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "we can get to that positive outcome and",
      "offset": 613.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "here's why I think it's positive. That's",
      "offset": 615.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "the kind of set of issues I wanted you",
      "offset": 616.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "to speak to first. So I primarily think",
      "offset": 618.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "about stopping uh super intelligence. Uh",
      "offset": 621.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "the main reason for that is because that",
      "offset": 624.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "is much more uh implementable",
      "offset": 627.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "geopolitically compatible with existing",
      "offset": 631.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "incentives and and so on. So it's it's",
      "offset": 633.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "something that I can actually foresee. I",
      "offset": 636.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "think things stopping tomorrow is not",
      "offset": 638.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "something I can as easily foresee. So I",
      "offset": 640.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "don't really think about that. Uh the",
      "offset": 642.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "reason that super\n But is that just a",
      "offset": 644.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "fatalism then? I mean like if you're",
      "offset": 646.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "worried about super intelligence and you",
      "offset": 648.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "might think if we move even towards AGI",
      "offset": 651.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "which presumably is a closer in time",
      "offset": 653.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that that's a bad idea because we won't",
      "offset": 656.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "be able to stop the super intelligence",
      "offset": 658.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "like maybe we don't want to risk going",
      "offset": 659.839,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "further.\n I mean I I couldn't conceive of",
      "offset": 661.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "how we would coordinate you know now to",
      "offset": 665.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "do that. I I think that I think the asks",
      "offset": 667.92,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "are instead uh having um say for",
      "offset": 670.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "instance the United States disrupt",
      "offset": 674.079,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "China's ability to develop a super",
      "offset": 676.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "intelligence. The main way in which they",
      "offset": 679.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "would develop it is if they get uh the",
      "offset": 680.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "ability to automate AI research and",
      "offset": 683.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "development fully and take the human out",
      "offset": 685.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of the loop. Then you go from machine",
      "offset": 687.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "speed or from human speed to machine",
      "offset": 689.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "speed. And we've had people such as",
      "offset": 691.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Daario for instance uh the CEO talk",
      "offset": 693.2,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "about how that will give the US a or",
      "offset": 696.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "such a such a um an recursive process",
      "offset": 698.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "like that uh could lead to an",
      "offset": 702,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "intelligence explosion and that would",
      "offset": 703.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "lead to a durable edge where nobody will",
      "offset": 705.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "be able to catch up. And last week Sam",
      "offset": 708.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Alman discussed how this process could",
      "offset": 710.959,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "telescope a decade's worth of AI",
      "offset": 714.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "development in a year or potentially a",
      "offset": 716.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "month. And I think that's",
      "offset": 718.8,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "extraordinarily destabilizing for two",
      "offset": 720.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "reasons. One, if they control it, then",
      "offset": 722.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "or if a state controls it such as say",
      "offset": 725.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "China, uh then uh all the other",
      "offset": 727.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "countries are at substantial risk uh",
      "offset": 730.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "because that super intelligence could be",
      "offset": 732.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "weaponized and used to crush other",
      "offset": 734.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "countries. And uh if they don't control",
      "offset": 736.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "it, which I think would be fairly likely",
      "offset": 739.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "by a nearly unsupervised, extremely",
      "offset": 741.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "fast-moving process, um uh uh then",
      "offset": 743.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "everybody's survival is also threatened.",
      "offset": 747.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "So either way, this uh very fast",
      "offset": 749.279,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "automated AI R&amp;D loop is is quite uh",
      "offset": 751.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "destabilizing whether a state controls",
      "offset": 754.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "it or not. And so I think it makes sense",
      "offset": 756.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh not just because you know AI is is is",
      "offset": 759.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "scary in some vague sense but I think",
      "offset": 762.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that there are very strong geopolitical",
      "offset": 764.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "incentives for estate self-preservation.",
      "offset": 766.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "I got that I want to come back to the",
      "offset": 768.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "geopolitical aspect of it and I read um",
      "offset": 770.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "I skimmed the the paper that you just",
      "offset": 772.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "did with with Alexander and and Eric. Um",
      "offset": 774.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "but I don't think I got an answer for",
      "offset": 777.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the first part which is the upside. I",
      "offset": 779.44,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "don't think I need you to lay out the",
      "offset": 781.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "upside.\n Yeah. Yeah. Yeah. So for for",
      "offset": 782.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "upside um",
      "offset": 784.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "I think people um generally think that",
      "offset": 787.44,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "uh if we have AI it will necessarily",
      "offset": 791.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "hollow out all other all values or that",
      "offset": 793.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "we'll have an extreme pressure toward",
      "offset": 796.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "one of them. Uh that was we'll be zoo",
      "offset": 798.24,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "animals or we will um uh or we'll be all",
      "offset": 800.48,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "pleasured out and just you know in our",
      "offset": 806.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "VR experiences constantly and have have",
      "offset": 808.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "superficial experiences. I I I think you",
      "offset": 810.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "could set a society up so that people",
      "offset": 813.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have the autonomy to choose between",
      "offset": 815.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "different ways that they would want to",
      "offset": 817.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "live their life given the resources that",
      "offset": 819.279,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "uh uh AI increasing GDP could provide.",
      "offset": 822,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "So I think that there's a way you could",
      "offset": 825.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "have u a a list of objective goods being",
      "offset": 827.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "met uh in society. So I I don't think",
      "offset": 830.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "there's um no equilibria where things",
      "offset": 832.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "can work out. I think there's a way of",
      "offset": 836.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "achieving a variety of goods and still",
      "offset": 838.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "preserving human autonomy uh and all",
      "offset": 839.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that like sort of what we have now.",
      "offset": 841.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Imagine we had things now um but then or",
      "offset": 842.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "we have the autonomy to choose how we",
      "offset": 845.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "want to live our lives now. Um and",
      "offset": 847.279,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "there'll be more resources um uh\n some of",
      "offset": 849.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "us in the west have\n Yes, that's right.",
      "offset": 853.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "That's right. That's right. And there's",
      "offset": 855.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there's other sorts of longer term",
      "offset": 857.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "questions about how you might do that",
      "offset": 859.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like how are you going to distribute",
      "offset": 860.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "power? Maybe that would be with uh",
      "offset": 861.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "compute slices that people would rent",
      "offset": 863.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "out for instance where they would have",
      "offset": 865.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the unique cryptographic key uh to to",
      "offset": 867.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "activating that compute slice so that",
      "offset": 870.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you're not just distributing wealth but",
      "offset": 872.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you're also distributing um ways of of",
      "offset": 873.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "generating that wealth. You know,",
      "offset": 876.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "there's there's a lot in this in this",
      "offset": 878,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "deep future that one could uh pull out.",
      "offset": 879.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "But I I I think there are some uh there",
      "offset": 881.279,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "there are some uh paths uh where things",
      "offset": 883.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "work out well.\n I like your term",
      "offset": 886.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "equilibria. Um, I think we probably all",
      "offset": 888.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "agree that there's multiple equilibria",
      "offset": 890.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "here and that we're all trying to steer",
      "offset": 892.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "towards positive equilibria. And I think",
      "offset": 895.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we would all also all agree that there's",
      "offset": 897.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "lots of different ways to get to both",
      "offset": 899.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the positive and the negative. We might",
      "offset": 901.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "disagree about some of the paths, some",
      "offset": 902.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of the likelihoods and so forth. But I",
      "offset": 904.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think we both we're all three operating",
      "offset": 906.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "under that general assumption that there",
      "offset": 908.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "there's multiple ways this could go and",
      "offset": 910.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "some of them are good and some of them",
      "offset": 913.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are bad. I'm a little darker than I used",
      "offset": 914.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "to be even a couple years ago because of",
      "offset": 917.44,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "um the economic and equality sorts of",
      "offset": 920.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "issues. So, a couple years ago when I",
      "offset": 924.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "trusted Sam a lot more than I trust him",
      "offset": 927.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "now, um I heard him talking about",
      "offset": 929.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "universal basic income and um you know,",
      "offset": 932.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I've always thought that that was an",
      "offset": 936.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "important part of the equation. And it",
      "offset": 937.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "seems to me that the more that data has",
      "offset": 939.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "driven things, the more inquisitive the",
      "offset": 942.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "companies, and it's not just open AI,",
      "offset": 945.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "have been, the less realistic it has",
      "offset": 947.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "seemed to me that we're going to have",
      "offset": 949.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "anything like a universal basic income.",
      "offset": 951.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "I think there is some scenario under",
      "offset": 953.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "which there's so much wealth that",
      "offset": 955.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "everybody's subsistence gets met. I",
      "offset": 956.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "don't expect that the wealthy people are",
      "offset": 959.519,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "going to give up the beachfront property",
      "offset": 961.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "under any circumstance or their power.",
      "offset": 962.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And I think the dynamics that we've seen",
      "offset": 964.959,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "in Washington lately have made me,",
      "offset": 966.639,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "though I know our politics may not be",
      "offset": 968.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the same, have made me less optimistic",
      "offset": 969.759,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "about any kind of relatively equal",
      "offset": 972.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "distribution or even just not a",
      "offset": 975.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "relatively extreme distribution. I think",
      "offset": 977.199,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that the positive outcomes do depend on",
      "offset": 979.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "getting some answers there to what would",
      "offset": 982.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "be the incentives, the mechanisms, the",
      "offset": 984.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "dynamics such that things are well",
      "offset": 986.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "distributed. And I guess I'm sort of",
      "offset": 989.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "making this up as I go, but I think",
      "offset": 991.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "there's an argument here for stopping",
      "offset": 993.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the train if we can't see any solution",
      "offset": 995.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "to the political thing. Most of my own",
      "offset": 996.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "personal concerns are really about",
      "offset": 1000.079,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "technical alignment, which we'll talk",
      "offset": 1001.68,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "about soon enough. Um, but if we don't",
      "offset": 1002.959,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "have an even envisionable political",
      "offset": 1005.759,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "solution, that does make me nervous. And",
      "offset": 1009.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "I could see an argument for let's stop",
      "offset": 1011.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "the train now because we just don't see",
      "offset": 1012.959,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "how to do it. And going back to",
      "offset": 1014.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "something you said at the very beginning",
      "offset": 1016.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "there there's an interesting argument",
      "offset": 1018.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "about delaying the train versus stopping",
      "offset": 1020.24,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "the train. And um like I think all of us",
      "offset": 1022.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "signed the pause letter. Maybe you",
      "offset": 1025.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "didn't sign the pause letter.\n Okay. Only",
      "offset": 1027.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "I signed the pause letter. That's",
      "offset": 1029.28,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "interesting. I think that that didn't",
      "offset": 1030.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "call that. Um that's fine. whether or",
      "offset": 1031.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "not\n the the the part of the pause letter",
      "offset": 1034.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that made me sign it that I like the",
      "offset": 1036.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "best was it said let's pause this",
      "offset": 1038.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "particular thing that we know is",
      "offset": 1041.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "problematic in certain ways. I mean it",
      "offset": 1043.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "was really about delaying the",
      "offset": 1045.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "development of GPT5 which ironically",
      "offset": 1047.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "still doesn't exist two and a half years",
      "offset": 1050,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "after some of us signed the pause",
      "offset": 1051.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "letter. Um but the notion was we would",
      "offset": 1053.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "pause the development of GPT5 because we",
      "offset": 1056.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "knew that GPT4 had certain kinds of",
      "offset": 1058.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "problems um around alignment a and that",
      "offset": 1060.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we would spend that time instead working",
      "offset": 1063.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "on safety. So it was it was explicitly",
      "offset": 1064.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "constructed as a delaying tactic. It",
      "offset": 1067.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "wasn't saying never build AI. It wasn't",
      "offset": 1069.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "saying do any don't do any more AI",
      "offset": 1071.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "research. It was in fact saying do more",
      "offset": 1073.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "AI safety research and wait. And that",
      "offset": 1075.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "does still seem like maybe not a",
      "offset": 1078.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "politically viable thing in this moment,",
      "offset": 1081.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but at least a strategy one might",
      "offset": 1083.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "consider is, you know, maybe we should",
      "offset": 1084.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "be constantly updating our estimates on,",
      "offset": 1086.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you know, how likely are you to get the",
      "offset": 1089.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "positive outcomes versus the negative",
      "offset": 1090.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "outcomes and how much would that change",
      "offset": 1092.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "as a function, for example, of putting",
      "offset": 1094.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "more resources into safety research as",
      "offset": 1096.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "opposed to capabilities research, etc.",
      "offset": 1098.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Um, and what's your take on what I just",
      "offset": 1100.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "said?\n Totally agree. Like I think um",
      "offset": 1103.12,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "yeah like I I I like the pause AI",
      "offset": 1107.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "framing more than the stop AI framing",
      "offset": 1110.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "for what for what you for the reasons",
      "offset": 1111.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you just mentioned and then there's a",
      "offset": 1113.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "lot more to say about more more uh",
      "offset": 1115.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "nuanced proposals for what is to be",
      "offset": 1117.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "done. Um I do think we should maybe save",
      "offset": 1119.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "a lot of this for the later part of our",
      "offset": 1122.799,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "discussion after we've gotten through",
      "offset": 1124.32,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "the technical stuff like timelines",
      "offset": 1125.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "alignment etc. But you're the moderator,",
      "offset": 1126.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so\n I'm I'm gonna use moderator's",
      "offset": 1128.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "prerogative for now, but you can push me",
      "offset": 1130.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "a little bit later if we don't we don't",
      "offset": 1132.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "get there. I'm trying to find some sort",
      "offset": 1133.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "of common ground.\n I think we have lots",
      "offset": 1135.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "of common ground.\n And yeah,\n um and then",
      "offset": 1136.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we'll get we get to the differences. Do",
      "offset": 1139.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "do you buy that notion that also this",
      "offset": 1140.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "should be on the table of some kind of",
      "offset": 1143.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "clause or no?\n I I I I think um uh making",
      "offset": 1144.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "time for technical research I I'm not",
      "offset": 1148,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "expecting much of a return on investment",
      "offset": 1150.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "from that. Um I don't think um most",
      "offset": 1151.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "success strategies uh part um in the",
      "offset": 1155.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "foreseeable future particularly go",
      "offset": 1157.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "through uh um uh AI being fully",
      "offset": 1159.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "controllable. So I'm I I do technical",
      "offset": 1162.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "research. I'll give you a comeback on",
      "offset": 1164.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that like an extreme version of the",
      "offset": 1166.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "argument I just placed comes from the",
      "offset": 1168.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "cognitive psychologist and evolutionary",
      "offset": 1170.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "psychologist Jeffrey Miller um who had a",
      "offset": 1171.84,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "a tweet that I really liked which was um",
      "offset": 1174.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "if it you know we should wait until we",
      "offset": 1177.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "can build this stuff safely even if that",
      "offset": 1180.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "takes I forget what he said I'll say 250",
      "offset": 1182.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "years um and it was an interesting",
      "offset": 1184.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "framing because like everybody's",
      "offset": 1186.72,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "thinking like what should we do next",
      "offset": 1187.919,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "week or should we sign this pause letter",
      "offset": 1189.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and it was kind of deliberately extreme",
      "offset": 1190.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like I think it was maybe even 500 like",
      "offset": 1192.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "if it takes us 500 years we should just",
      "offset": 1195.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "wait and",
      "offset": 1197.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "you know I could see an argument for",
      "offset": 1199.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "that and in fact I wonder what the",
      "offset": 1201.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "counterarguments are.\n So I I think that",
      "offset": 1202.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the process that I described earlier",
      "offset": 1205.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that sort of recursive loop I guess you",
      "offset": 1207.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "could call it an intelligence recursion",
      "offset": 1210.4,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "which if it goes fast enough is an",
      "offset": 1211.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "intelligence explosion. Uh that is not",
      "offset": 1212.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "something you can research your way out",
      "offset": 1215.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of. Uh you can't just you know write an",
      "offset": 1217.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "eight-page paper and then we've we've",
      "offset": 1219.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "solved it you know in the news tomorrow.",
      "offset": 1220.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "It's we we figured out how to um pull",
      "offset": 1222.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "fully derisk some um extremely",
      "offset": 1225.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "fastmoving process that we've never done",
      "offset": 1227.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "before. Um uh and all of its unknown",
      "offset": 1229.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "unknowns have been anticipated.\n Well, I",
      "offset": 1231.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "mean we we could politically I mean it",
      "offset": 1233.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "would take a lot of willpower and it's",
      "offset": 1236.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "probably not likely, but we could, you",
      "offset": 1237.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "know, try to have a global treaty. Don't",
      "offset": 1239.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "go there. Don't work on these kinds of",
      "offset": 1241.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "things. Let's report it. If you do,",
      "offset": 1243.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "right? Right. I mean, you could at least",
      "offset": 1245.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "imagine that kind of scenario. I don't",
      "offset": 1247.039,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "see any technical way of coping",
      "offset": 1249.36,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "necessarily with that set of problems",
      "offset": 1254,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "right now. But I if we were I mean not",
      "offset": 1255.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just the three of us in the room, but if",
      "offset": 1258.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we as a society were convinced that",
      "offset": 1259.919,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "let's say that was a red line, we you",
      "offset": 1263.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "know that you suggested one red line,",
      "offset": 1265.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there are others. So we could decide as",
      "offset": 1266.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a society there are certain red lines.",
      "offset": 1269.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Maybe recursive self-improvement might",
      "offset": 1271.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "be a reasonable one to consider. um that",
      "offset": 1272.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we can say we won't cross any of the red",
      "offset": 1275.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "lines. Um we will make treaties around",
      "offset": 1277.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it and we just shouldn't do it until we",
      "offset": 1279.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "have answers to them.\n So I think it",
      "offset": 1281.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "would be worth states clarifying that we",
      "offset": 1284.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "don't want anybody doing that um type of",
      "offset": 1287.12,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "a fully automated intelligence recursion",
      "offset": 1290.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "uh because it'll be destabilizing",
      "offset": 1293.919,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "and um now that doesn't necessarily",
      "offset": 1296.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "immediately take the form of a treaty.",
      "offset": 1300,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So initially you have them exchanging",
      "offset": 1302.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "words about that and articulating their",
      "offset": 1305.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "preferences",
      "offset": 1307.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "uh potentially explicitly or potentially",
      "offset": 1308.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "in internal policy at CIA and other um",
      "offset": 1310.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and things like that and the other",
      "offset": 1313.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "parties come to learn it uh um through",
      "offset": 1314.88,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "leaks or directly. Um and you eventually",
      "offset": 1317.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "want to gain more confidence that",
      "offset": 1320.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "nobody's trying to trigger something",
      "offset": 1322.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like this. And um this this process is",
      "offset": 1324.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "multiple stages. It may involve things",
      "offset": 1326.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "even like skirm it requires a",
      "offset": 1329.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "conversation. And it may even require a",
      "offset": 1330.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "skirmish uh for for people to think okay",
      "offset": 1332.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "we need to do a verification now and",
      "offset": 1335.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "then maybe you get some type of of of",
      "offset": 1338.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "treaty but you can still uh have various",
      "offset": 1340.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "forms of coordination through deterrence",
      "offset": 1342.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh without anything like a treaty um",
      "offset": 1345.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just as uh we've had strategic stability",
      "offset": 1347.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "on multiple issues without treaties",
      "offset": 1350.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "necessarily. So that's something that",
      "offset": 1352.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "when is is potentially a later stage",
      "offset": 1353.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "thing but you have to have the",
      "offset": 1356.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "conversation advance far further.",
      "offset": 1358.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Daniel's g giving me a look on the\n No,",
      "offset": 1360.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I'm just looking back and forth.\n You're",
      "offset": 1362.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "just looking back and forth. Why am I in",
      "offset": 1363.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "this?\n You have to like rub your neck",
      "offset": 1365.52,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "because I'm right in the middle.\n Yeah,",
      "offset": 1367.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "in the middle. Okay. Um, so let's",
      "offset": 1367.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "separate for a moment the deterrent",
      "offset": 1371.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "stuff, although I know you're keen to",
      "offset": 1372.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "talk about and know more about it than I",
      "offset": 1374.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "do. Um, and let's try to get to it. Um,",
      "offset": 1376,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you could sort of separate what are the",
      "offset": 1379.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "dynamics for which we would form",
      "offset": 1381.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "treaties. Maybe we need some disturbance",
      "offset": 1383.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and skirmishes as you just talked about,",
      "offset": 1385.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "but from the question of like would the",
      "offset": 1387.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "rational thing for civilization to do",
      "offset": 1389.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "right now be in fact to sign these",
      "offset": 1391.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "treaties because we're basically pretty",
      "offset": 1393.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "close. I maybe take a longer timeline",
      "offset": 1396.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "than you, but we're reasonably close to",
      "offset": 1398.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "recursive self-improvements of at least",
      "offset": 1400.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "some sort. And so like maybe we don't",
      "offset": 1402.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "want to go there. Maybe that's what the",
      "offset": 1404.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "rational thing to do be right now to",
      "offset": 1406.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "make those treaties. Or even if like we",
      "offset": 1408.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "thought that was 25 years away because",
      "offset": 1410.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we know treaties take 8 10 years like",
      "offset": 1413.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "maybe we should be putting all our you",
      "offset": 1415.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "know intellectual capital or political",
      "offset": 1417.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "capital or whatever into doing that",
      "offset": 1419.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "right now. What do you think?\n Yes. So I",
      "offset": 1420.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "think three three red lines would be no",
      "offset": 1422.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "recursion where that's a fully automated",
      "offset": 1424.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "not just some AI assisted one but uh one",
      "offset": 1426.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that has that explosive uh potential. uh",
      "offset": 1428.559,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "no AI agents with expert level verology",
      "offset": 1432.24,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "skills or cyber offensive skills uh made",
      "offset": 1435.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "accessible without some safeguards. Um",
      "offset": 1438.159,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "uh and uh model weights need to have",
      "offset": 1441.2,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "model weights past some capability level",
      "offset": 1445.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "need to have some good information",
      "offset": 1447.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "security for containing them uh and",
      "offset": 1449.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "making sure that they're not uh uh",
      "offset": 1452,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "exfiltrated or stolen by rogue actors.",
      "offset": 1453.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "In a minute, I'm going to ask you how",
      "offset": 1456.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "close we are on these various things.",
      "offset": 1457.679,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "But what's what's your take on what he",
      "offset": 1459.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just said? Is that those same\n with the",
      "offset": 1460.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "first I don't know about the second two",
      "offset": 1462.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "the other two, but definitely the first",
      "offset": 1464.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "one. I think that if somehow we could",
      "offset": 1465.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "coordinate on that, that'd be great. I'm",
      "offset": 1467.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "not sure if that's the best red line to",
      "offset": 1468.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "coordinate on, but it's a excellent",
      "offset": 1470.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "place to start at least. Do you want to",
      "offset": 1472.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "throw in any others now or you can later",
      "offset": 1474.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in the conversation or\n I think that the",
      "offset": 1476.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the type of thing that I'm probably",
      "offset": 1478.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "going to end up advocating for is going",
      "offset": 1480.559,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "to be um more of a like rather than like",
      "offset": 1482.32,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "here's a line that we're all not going",
      "offset": 1487.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to cross something more like we are",
      "offset": 1488.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "going to gradually develop AIS with",
      "offset": 1491.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "these capabilities but we're going to do",
      "offset": 1493.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it in a way that's like mutually",
      "offset": 1495.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "transparent to each other and that",
      "offset": 1496.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "proceeds sort of slowly and cautiously",
      "offset": 1498.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "where we all like debate whether it's",
      "offset": 1500.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "safe to go to the next level and then",
      "offset": 1502.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "after we get there we study it for a",
      "offset": 1504.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "little bit and then debate whether it's",
      "offset": 1506.08,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "safe to go to the next level and so",
      "offset": 1507.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "forth. So so it the sort of thing that",
      "offset": 1508.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we're probably going to end up",
      "offset": 1511.279,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "advocating for is going to look",
      "offset": 1512.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "something more like that rather than uh",
      "offset": 1513.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "but but yeah in term in terms of like",
      "offset": 1516.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the thing that you really need to like",
      "offset": 1518.159,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "stop from happening in the short term.",
      "offset": 1519.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "uh that sort of recursive",
      "offset": 1521.279,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "self-improvement thing is I would say",
      "offset": 1522.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the number one thing\n right so um",
      "offset": 1524.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "this conversation is a little bit",
      "offset": 1527.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "depressing in the sense that many of the",
      "offset": 1528.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "things that we seem to be worried about",
      "offset": 1531.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "actually seem fairly close um maybe the",
      "offset": 1534.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "person in the room who's most",
      "offset": 1537.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "pessimistic if that's the word let's not",
      "offset": 1539.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "the most extended in time on this is me",
      "offset": 1541.679,
      "duration": 8.401
    },
    {
      "lang": "en",
      "text": "um but all of us I would say think that",
      "offset": 1545.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "well no let Let me rephrase this",
      "offset": 1550.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "question. Um, it's a dark set of answers",
      "offset": 1551.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "relative to the reality right now. I",
      "offset": 1555.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "think in the following sense, even if",
      "offset": 1557.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you think it's going to take a while to",
      "offset": 1559.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "get to AGI or ASI or something like",
      "offset": 1560.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that, and we'll talk about that in a",
      "offset": 1563.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "little bit. Um, the the things that are",
      "offset": 1565.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "red lines, and I like your red lines.",
      "offset": 1568.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Um, people are already pushing against",
      "offset": 1570.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "them. They may not be breaking through",
      "offset": 1573.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "them. Like depending on your definition",
      "offset": 1575.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of recursive self-improvement, I might",
      "offset": 1577.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "give you different estimates. Mine might",
      "offset": 1580.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "be a little longer than yours, but",
      "offset": 1581.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "people are already trying to do that,",
      "offset": 1583.039,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "right? And transparency is like 1990s",
      "offset": 1585.039,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "talk. Like it's like it's in the rear",
      "offset": 1590.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I'm exaggerating a little bit, but it's",
      "offset": 1592.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it's in the rearview mirror. I mean like",
      "offset": 1593.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "open AI was open originally. It is not",
      "offset": 1596.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "open anymore. So, you know, there are",
      "offset": 1598.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "elements still pushing for transparency,",
      "offset": 1601.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "but there are certainly elements pushing",
      "offset": 1604.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "against.\n So, I'm I'm somewhat more",
      "offset": 1605.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "optimistic about uh um increased",
      "offset": 1608,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "transparency and more situational",
      "offset": 1610.799,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "awareness from from governments. The a a",
      "offset": 1612.799,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "a reason for that is I think it's",
      "offset": 1616.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "incentive compatible for the US to be",
      "offset": 1619.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "more transparent about what's going on",
      "offset": 1621.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "at the frontier because China already",
      "offset": 1623.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "knows. Meanwhile, for PLA or People's",
      "offset": 1625.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Liberation Army developments, there's",
      "offset": 1627.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "somewhat less uh transparency there. So,",
      "offset": 1629.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "uh the people to gain more from",
      "offset": 1632.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "information would probably be the rest",
      "offset": 1634.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "of the world. Uh so, so China has",
      "offset": 1636.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "somewhat more of an advantage there. And",
      "offset": 1638.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this sort of levels things and that um",
      "offset": 1639.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "having high transparency into the",
      "offset": 1642.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "frontier is very useful for making more",
      "offset": 1644.24,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "credible uh deterrence. Um because then",
      "offset": 1647.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "yeah, interrupt for a second about a",
      "offset": 1651.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "time frame question. I think the notion",
      "offset": 1653.76,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "of a durable advantage in LLMs, if",
      "offset": 1656.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "that's what the technology is, is a",
      "offset": 1660.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "myth. Like if we stay on LLMs, nothing's",
      "offset": 1663.039,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "going to be durable. Um, but I think you",
      "offset": 1666.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "think about these things in a little bit",
      "offset": 1669.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "different way than I do.\n I I I think",
      "offset": 1670.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "with the current um paradigm, uh, I am",
      "offset": 1672.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "expecting them to continue to leaprog",
      "offset": 1676.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "each other. And\n so roughly parody is",
      "offset": 1678,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "what I'm seeing, right? So there could",
      "offset": 1680,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be a different technology. you know, my",
      "offset": 1681.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "personal favorite would be neurosymbolic",
      "offset": 1684,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "AI that somebody gets a durable",
      "offset": 1685.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "advantage of. Maybe, but you know,",
      "offset": 1687.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "there's going to be espionage. People",
      "offset": 1689.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "are going to share ideas and whatever.",
      "offset": 1691.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Um, but if the paradigm stays roughly",
      "offset": 1693.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like it is, I don't see anybody getting",
      "offset": 1696.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "a durable advantage. So, that's part one",
      "offset": 1698,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is do you agree disagree with that?\n I",
      "offset": 1699.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "think I it depends on what you mean by",
      "offset": 1702,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "paradigm. So, I would say like LMS are",
      "offset": 1703.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "just a part of this or a subset of this",
      "offset": 1707.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "overall like AI research. you know,",
      "offset": 1709.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "they're not the only possible AI design",
      "offset": 1711.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and uh progress is going to continue",
      "offset": 1713.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "being made. I mean, I would say like in",
      "offset": 1715.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "some sense, arguably we're already",
      "offset": 1717.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "seeing a move away from LMS with things",
      "offset": 1718.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like these so-called reasoning agents",
      "offset": 1720.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that have access to tools and can write",
      "offset": 1722.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "code and so forth. So, so there's going",
      "offset": 1724.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "to be this continuous shift uh towards I",
      "offset": 1726.399,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "would say rather than like discrete",
      "offset": 1730.559,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "paradigm shifts, it'll be more like a",
      "offset": 1731.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "continuous paradigm shift and the the",
      "offset": 1733.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "AIS of 2027 are just going to be like",
      "offset": 1735.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "quite different from the AI of 2023, you",
      "offset": 1737.6,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "know. Um, but taken as a whole, the AIS,",
      "offset": 1739.76,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "um, I do think that, uh, the United",
      "offset": 1744.799,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "States could potentially end up having a",
      "offset": 1748.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "durable advantage over, uh, China and",
      "offset": 1750.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "one particular tech company within the",
      "offset": 1753.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "United States could end up having a",
      "offset": 1755.039,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "durable advantage.\n I see this as",
      "offset": 1756.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "possible. I see that as unlikely unless",
      "offset": 1758.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "somebody approaches the problem in a",
      "offset": 1761.6,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "pretty different way. Like, I can",
      "offset": 1763.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "imagine a neurosyolic approach. it would",
      "offset": 1764.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "be just so different from what anybody",
      "offset": 1766.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "else has that I could see at least for a",
      "offset": 1768.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "while an advantage in the current way of",
      "offset": 1770.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "people are doing things. I don't even",
      "offset": 1773.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "know what that would look\n perhaps I",
      "offset": 1775.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "should clarify what by durable advantage",
      "offset": 1777.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "I don't mean um they never figure out",
      "offset": 1778.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "what you already figured out. I mean",
      "offset": 1781.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "that by the time they catch up, you've",
      "offset": 1782.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "already moved ahead.\n So that you,\n it's",
      "offset": 1784.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "like, you know, you keep running as fast",
      "offset": 1786.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "as they're running so that even though",
      "offset": 1788.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "they're only a six months behind,\n they",
      "offset": 1790.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "can never quite catch up because by the",
      "offset": 1792.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "time they do, you're six months ahead",
      "offset": 1794.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "again.\n And that was part of the other",
      "offset": 1795.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "part of that question I wanted to get",
      "offset": 1797.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to, which is like, does six months",
      "offset": 1798.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "matter? Is that enough to change the",
      "offset": 1800.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "world or not? Right now it doesn't seem",
      "offset": 1802.64,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "huge but uh um if you are the first to",
      "offset": 1806,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "trigger a recursion yeah\n that can matter",
      "offset": 1809.44,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "a lot be the moment where it matters and",
      "offset": 1813.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that goes back to the other question",
      "offset": 1815.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "which is on the recursion thing the fact",
      "offset": 1816.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is people are trying it as far as I",
      "offset": 1818.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "understand it like\n it's the plan\n it is",
      "offset": 1820.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it's the plan yeah it's still highly AI",
      "offset": 1822.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "assisted and doesn't there's nobody",
      "offset": 1824.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "could try and spin this off spin this up",
      "offset": 1826.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "right now and have it be foreseeably",
      "offset": 1829.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "going to to lead to something uh",
      "offset": 1830.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "substantial But uh yeah,\n I mean what I",
      "offset": 1833.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "see right now is like you could do your",
      "offset": 1835.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "hyperparameter search faster or",
      "offset": 1837.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "something like that and you could call",
      "offset": 1839.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that recursion if you want but that's",
      "offset": 1841.52,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "not really what we're talking about",
      "offset": 1843.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "here. We're talking about like a system",
      "offset": 1844.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "finds a fundamentally new idea that year",
      "offset": 1846.799,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "24 would only come from humans and that",
      "offset": 1849.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "leverage",
      "offset": 1852.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "of of not only coming up with the ideas",
      "offset": 1854.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "but testing them validating them working",
      "offset": 1856.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "out the tweaks implementing them",
      "offset": 1858.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "scaling. I think that's exactly what",
      "offset": 1860.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "people like Sam and Dario are now",
      "offset": 1862.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "hinting that they're going to be able to",
      "offset": 1864.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "do soon. I'm not really buying those",
      "offset": 1866.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "claims, but I think that's what they",
      "offset": 1868.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "want to do now.\n 100%.\n Yep.\n Yeah. And",
      "offset": 1870,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "that is destabilizing\n and scary and",
      "offset": 1873.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "dangerous.\n Let's say potentially",
      "offset": 1875.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "destabilizing. I mean, if it's just hype",
      "offset": 1877.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and they can't really do it, it's not",
      "offset": 1879.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "destabilizing. But if one of them",
      "offset": 1880.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "achieves it,\n if if it were technically",
      "offset": 1882.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "feasible, that would be destabilizing.",
      "offset": 1884.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "And I mean, I think we can agree it",
      "offset": 1886.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "probably is technically achievable. It's",
      "offset": 1887.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "just a question of whether you can do it",
      "offset": 1890,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "with current technology or later how far",
      "offset": 1891.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "in the future there's no you know",
      "offset": 1893.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "surirly an argument that this can't be",
      "offset": 1896.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "done that you need right I mean I think",
      "offset": 1898.159,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we all think that's going to happen\n and",
      "offset": 1900.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and so one advantage for transparency",
      "offset": 1902.399,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "then is if there were very high",
      "offset": 1905.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "visibility as to what's going on at the",
      "offset": 1907.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "frontier if they're basically triggering",
      "offset": 1909.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this sort of process and the public is",
      "offset": 1912,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "kept reasonably informed as it's",
      "offset": 1913.919,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "happening I think the world would be",
      "offset": 1915.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "very much freaking out. Yeah. Uh so I I",
      "offset": 1916.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "I think that possibly sunlight might um",
      "offset": 1918.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "be a way of of providing substantial",
      "offset": 1921.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "pressure to prevent that.\n Freaking out",
      "offset": 1923.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "might be too strong, but it's worth",
      "offset": 1925.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "mentioning that like polls of the",
      "offset": 1926.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "American people, not necessarily in",
      "offset": 1928.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Asia, but American people are pretty",
      "offset": 1929.679,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "worried. maybe not at the top of their",
      "offset": 1932.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "set of worries which might be economic",
      "offset": 1935.519,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "or um and so forth but you know you look",
      "offset": 1937.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "at these polls and like 75% of the",
      "offset": 1939.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "American public is already worried\n and I",
      "offset": 1941.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "expect that that worry will increase not",
      "offset": 1944.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "decrease at least I don't see any active",
      "offset": 1946.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "thing that is going to decrease it in",
      "offset": 1948.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the thing that might decrease it is um",
      "offset": 1950.559,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "uh propaganda from the tech companies\n I",
      "offset": 1952.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "think the tech companies seem to like",
      "offset": 1955.919,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "scaring people as far as I can tell\n oh I",
      "offset": 1959.039,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "disagree I I think that um",
      "offset": 1961.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "I mean there's probably some of that",
      "offset": 1965.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "there to some extent for sure but um at",
      "offset": 1967.12,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "least my experience at OpenAI was that",
      "offset": 1970.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "uh there was more pressure to like not",
      "offset": 1973.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "talk about the risks in public and to",
      "offset": 1975.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like you know sort of downplay that sort",
      "offset": 1977.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of thing than pre there was no pressure",
      "offset": 1979.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "to play it up at least not as far as I",
      "offset": 1981.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "could tell. Um, and you know, if you",
      "offset": 1983.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "look at the the public messaging by Sam",
      "offset": 1985.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "uh over the years, he certainly started",
      "offset": 1988.399,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to like talk about it less and less and",
      "offset": 1990.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "less uh as\n well when he he I mean partly",
      "offset": 1991.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "because I pushed him, but when we talked",
      "offset": 1994.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "at the Senate, he's um you he didn't",
      "offset": 1996.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "answer the question, what were we most",
      "offset": 1998.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "worried about? I mean, I made Blumenthal",
      "offset": 2000.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "come back, but Blumenthal says it's jobs",
      "offset": 2002,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and and and he Sam gives his explanation",
      "offset": 2004.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "why he's not that worried about jobs.",
      "offset": 2007.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "And I say to Blumenthal, you should",
      "offset": 2008.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "really ask him a question. and and Sam",
      "offset": 2010.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "there said my biggest worry is that we",
      "offset": 2012.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do I can't remember the exact words but",
      "offset": 2015.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "substantial harm to humanity and when he",
      "offset": 2016.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "was back at the Senate a few weeks ago",
      "offset": 2018.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "his biggest harm was like in so many",
      "offset": 2020.96,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "words like we don't make all the money",
      "offset": 2023.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and extract all the value that we could",
      "offset": 2024.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "or something like that right so he I",
      "offset": 2026,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "mean I think it's true that he he used",
      "offset": 2028.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to talk more about this\n kind of risk",
      "offset": 2030,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "stuff and talks less now\n so I mean",
      "offset": 2032.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that's interesting data point is around",
      "offset": 2035.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the time that I did my Senate appearance",
      "offset": 2038.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "which was May 2023. So reconstructing",
      "offset": 2040.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "timelines a little bit after that was a",
      "offset": 2042.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "second letter related to the pause",
      "offset": 2044.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "letter which I did not sign. Um the\n that",
      "offset": 2046,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "was the one about um being and maybe you",
      "offset": 2049.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "wrote I don't know can letter\n the",
      "offset": 2051.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "extinction letter.\n Yeah I signed that",
      "offset": 2054,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "one. You made\n that's what we signed.\n So",
      "offset": 2055.679,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "so I didn't sign that one and we should",
      "offset": 2059.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "soon get to extinction risk but um but",
      "offset": 2062.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "we we'll come back to it in a minute. Um",
      "offset": 2065.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "Sam did I think sign that right? Um so",
      "offset": 2067.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "at that time that was maybe June 23 or",
      "offset": 2070.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "July 23 or something like that. Um it",
      "offset": 2073.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "was after the Senate appearance. I knew",
      "offset": 2076.399,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that. Um at that time it was still",
      "offset": 2078.24,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "popular among CEOs to express concern",
      "offset": 2081.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "about this and maybe it's true that they",
      "offset": 2084.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "do a little bit less. Dario still",
      "offset": 2086.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "alludes to this. Right. Right. Yeah.\n So",
      "offset": 2088.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "a lot of people that I have spoken to",
      "offset": 2091.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "think that it's a deliberate mechanism",
      "offset": 2094.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "to try to hype the product to talk about",
      "offset": 2096.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the risk like look how great our stuff",
      "offset": 2098.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is we it might kill us.\n Yeah. So I have",
      "offset": 2100.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "lots of\n scientists sign that",
      "offset": 2102.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "I didn't but yeah I understand.\n Yeah. So",
      "offset": 2105.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "so first of all yeah loads of people",
      "offset": 2108.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "have these views who aren't conflicted",
      "offset": 2111.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and trying to hype up the companies you",
      "offset": 2113.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "know.\n Yeah I agreed agreed about that",
      "offset": 2115.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "too. But then but then to your point",
      "offset": 2116.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "about the hype, you know, my take on",
      "offset": 2118.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "what's been happening is that",
      "offset": 2119.839,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "Deep Mind, OpenAI and Enthropic",
      "offset": 2122.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "have been full of people who were",
      "offset": 2126.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thinking about super intelligence from",
      "offset": 2128.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the very beginning",
      "offset": 2130.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "um at the highest levels of leadership",
      "offset": 2132.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and therefore they have been considering",
      "offset": 2135.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "at least somewhat both the loss of",
      "offset": 2138.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "control risk and the concentration of",
      "offset": 2140.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "power risk. you know, who gets the",
      "offset": 2143.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "control of the AIS from the beginning.",
      "offset": 2144.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And this is documented in all sorts of",
      "offset": 2146.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "ways. You can look at their old writings",
      "offset": 2148.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and so forth, some of the leaked emails.",
      "offset": 2149.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Um, and uh, then you might ask, well,",
      "offset": 2151.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "why are they building it if if they've",
      "offset": 2154.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "been taking these if they've been",
      "offset": 2157.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "thinking about these risks? Can they see",
      "offset": 2158.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "loss of control?\n Loss of control,",
      "offset": 2160.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "concentration of power. So, loss of",
      "offset": 2161.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "control is what if we don't solve the",
      "offset": 2163.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "alignment problem in time and the AI",
      "offset": 2164.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "take over. And then concentration of",
      "offset": 2166.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "power is if we do solve the alignment",
      "offset": 2168.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "problem, like who who controls the AIs?",
      "offset": 2169.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "what goals do we put in them? You know,",
      "offset": 2173.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "do we risk becoming a dictatorship or",
      "offset": 2175.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "some sort of crazy oligarchy or",
      "offset": 2177.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "whatever? Um, so, so, so like you can",
      "offset": 2179.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "find writings from people at these",
      "offset": 2181.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "companies, both senior researchers and",
      "offset": 2183.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "also the CEOs and so forth going back",
      "offset": 2185.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "decades talking about these things.\n Um,",
      "offset": 2187.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and then you might ask, well, why are",
      "offset": 2190.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "they doing this? Why are they racing to",
      "offset": 2191.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to build these things if if they were\n I",
      "offset": 2193.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "mean, Dario seems like the most extreme",
      "offset": 2195.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "version of that question. like he seems",
      "offset": 2197.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "still very much uh publicly saying that",
      "offset": 2199.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "there's very serious risk and he seems",
      "offset": 2202.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "very much publicly pushing the models",
      "offset": 2205.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "forward like I I I understand though I",
      "offset": 2207.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "didn't see it myself that at some point",
      "offset": 2210.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "he was saying we won't build frontier",
      "offset": 2211.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "models because of these risks and now",
      "offset": 2213.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "he's obviously building frontier models",
      "offset": 2215.2,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "and and so the the the key here is um I",
      "offset": 2216.64,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "guess I would say in a single word it",
      "offset": 2221.119,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "would be rationalization so um there's",
      "offset": 2223.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "been this very seduct ive argument that",
      "offset": 2227.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "has appealed to all of these people,",
      "offset": 2229.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which is basically, well, uh, it's",
      "offset": 2230.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "probably going to happen anyway. If we",
      "offset": 2233.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "don't do it, someone else will, and it's",
      "offset": 2235.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "better for us to do it first than for",
      "offset": 2236.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "someone else to do it because we're the",
      "offset": 2238.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "responsible good guys who will wisely",
      "offset": 2240.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "solve all the safety issues and then",
      "offset": 2243.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "also beneficently, you know, uh, give",
      "offset": 2245.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "UBI or whatever to to make sure that",
      "offset": 2248.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "everything works out well. So, so",
      "offset": 2250.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "basically all of these people sort of",
      "offset": 2251.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "trust themselves more than they trust",
      "offset": 2252.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "everyone else and have therefore",
      "offset": 2254.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "convinced themselves that even though",
      "offset": 2256.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "these risks are real, the best way to",
      "offset": 2258.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "deal with them is for them to go as fast",
      "offset": 2260.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "as possible and win the race. And this",
      "offset": 2261.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is, you know, Deep Mind's plan, so to",
      "offset": 2264.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "speak. Demesis Savis, his plan was",
      "offset": 2266.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "basically be there, get there first with",
      "offset": 2268.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "this big corporation, Google. And then,",
      "offset": 2271.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "uh, because you have such a lead over",
      "offset": 2275.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "everybody else, you can sort of slow",
      "offset": 2277.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "down and get all the safety stuff right",
      "offset": 2278.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and sort of make sure that everything",
      "offset": 2281.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "goes well before everybody else catches",
      "offset": 2283.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "up.\n That seems naive. That plan was",
      "offset": 2285.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "torpedoed when you know Elon and Sam and",
      "offset": 2287.52,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Ilia made open AAI. Uh why did they make",
      "offset": 2290.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "open AAI? Well, they were worried that",
      "offset": 2294.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they didn't trust Demis to to handle all",
      "offset": 2296.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that power responsibly when he was in",
      "offset": 2298.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "charge of the AI project. You know, the",
      "offset": 2301.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "only, you know, when all the fate of the",
      "offset": 2302.72,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "world rested in his hands. So, they",
      "offset": 2304.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "wanted to create OpenAI to be this",
      "offset": 2305.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "counterveilling force that could do it",
      "offset": 2306.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "right and make it, you know, distributed",
      "offset": 2309.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to everybody and not concentrate power",
      "offset": 2311.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so much in Demis' hands. Um, and in fact",
      "offset": 2313.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "they were the leaked emails or the",
      "offset": 2315.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "emails that came up in the lawsuit, they",
      "offset": 2317.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "were talking about how they were worried",
      "offset": 2319.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that Demis would become dictator using",
      "offset": 2320.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "AGI. Um,",
      "offset": 2322.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and uh, well, we can see how well that's",
      "offset": 2324.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "worked out. You know, uh, all the",
      "offset": 2326.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "anthropic people basically split off",
      "offset": 2328.56,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "from OpenAI because they didn't think",
      "offset": 2330,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "OpenAI was going to handle the safety",
      "offset": 2331.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "stuff responsibly. So then they're",
      "offset": 2332.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "claiming that like they have the",
      "offset": 2334.64,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "technical talent and they'll be able to",
      "offset": 2335.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "like sort out the alignment issues",
      "offset": 2337.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "better than everyone else, you know? Uh,",
      "offset": 2338.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it's a mess.\n Yeah.\n You have anything to",
      "offset": 2341.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "add? I agree that it's a mess.\n Not to",
      "offset": 2343.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "that it's a mess.\n And I mean, I guess I",
      "offset": 2346.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "don't know if you may not want to answer",
      "offset": 2349.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this on camera, but um do you feel",
      "offset": 2350.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "confident that we should trust any of",
      "offset": 2353.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "these particular?\n We definitely should",
      "offset": 2355.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "not.",
      "offset": 2356.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "I think um uh pushing for things like",
      "offset": 2358.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "transparency as well as the government",
      "offset": 2361.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "having people whose job it is to be",
      "offset": 2364,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "keeping track of these, coming up with",
      "offset": 2366.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "contingency plans, interviewing these",
      "offset": 2367.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "labs for uh their plans and coming up",
      "offset": 2369.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "with an internal assessments, their",
      "offset": 2371.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "probability of success. All of that uh",
      "offset": 2372.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "seems useful and I think um at least uh",
      "offset": 2375.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "um some of the players here would",
      "offset": 2378.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "probably be willing to push for that",
      "offset": 2380.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "type of stuff and I think others",
      "offset": 2382.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "wouldn't. Uh so I think it's um are they",
      "offset": 2383.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "willing to help solve these collective",
      "offset": 2386.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "action problems or are they going to",
      "offset": 2388.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "continue defecting and uh um\n it's been a",
      "offset": 2390,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "lot of defection",
      "offset": 2393.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to get back to your I realized I never",
      "offset": 2395.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "actually answered why this ties into",
      "offset": 2397.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "your question. So early on when these",
      "offset": 2398.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "companies were fresh and young and",
      "offset": 2401.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "idealistic, their their founding",
      "offset": 2402.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "mythology was basically, &quot;Yes, the risks",
      "offset": 2404.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are real and that's why you should come",
      "offset": 2407.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "work at our company because we're the",
      "offset": 2408.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "good guys.&quot; And so when that was still",
      "offset": 2410.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "fresh and still like, you know, the main",
      "offset": 2412.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "thing they were saying, they were",
      "offset": 2414.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "talking about it a lot. Uh but then now",
      "offset": 2416.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "when their their sort of like founding",
      "offset": 2418.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "myth is sort of like kind of laughable",
      "offset": 2420.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and it's like very much not uh something",
      "offset": 2422.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "they can say with a strafe. It's it's",
      "offset": 2425.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "eroded some. not something they can",
      "offset": 2426.72,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "really say with a straight face so much",
      "offset": 2427.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "anymore and also they're lots of under",
      "offset": 2429.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "lots of political pressure you know uh",
      "offset": 2430.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to get investors and to ward off",
      "offset": 2432.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "regulation and stuff like that. So, so",
      "offset": 2434.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "um the rationalization wheels are",
      "offset": 2437.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "continuing to turn and they're coming up",
      "offset": 2439.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "with new narratives to justify what",
      "offset": 2441.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "they're doing, you know,\n and there's",
      "offset": 2442.8,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "also a lot more players at the table",
      "offset": 2443.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "both within the US industry, which is",
      "offset": 2445.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "most of what we were talking about, but",
      "offset": 2447.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "also in China. Um then we have the whole",
      "offset": 2448.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I don't know if we have time to go into",
      "offset": 2451.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "it, but they'll maybe we will maybe",
      "offset": 2452.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "won't the um mechanisms of open sourcing",
      "offset": 2454,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "or at least open weight models, which",
      "offset": 2456.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "means that you know essentially anybody",
      "offset": 2458.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "can get in this game to some degree. Um",
      "offset": 2460.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "I disagree with that to some.\n Oh, go",
      "offset": 2462.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "ahead. I mean to some degree sure but\n no",
      "offset": 2464.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "walk me through the disagreement.\n Well",
      "offset": 2466.16,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "um like hypothetically suppose that we",
      "offset": 2467.76,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "suppose that we like achieved AGI",
      "offset": 2472.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "someone achieved AGI and immediately",
      "offset": 2475.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "opened the weights for everybody.",
      "offset": 2477.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "It's not going to happen like that. But",
      "offset": 2481.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "suppose it did happen like that. Then",
      "offset": 2482.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "for a brief glorious moment anybody with",
      "offset": 2484.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "enough GPUs could be able to run their",
      "offset": 2487.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "own AGI right at the frontier of",
      "offset": 2489.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "capability. However, um, AGI isn't like,",
      "offset": 2490.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you know, it's g there's going to be AGI",
      "offset": 2494.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "plus and AGI++ and so forth, and",
      "offset": 2496.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "whoever's going to get to AGI plus is",
      "offset": 2498.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "going to be the one who had the most GPU",
      "offset": 2499.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so they can run the AGIs to do the",
      "offset": 2501.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "research fastest, you know. So, so even",
      "offset": 2502.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "if you gave everybody exactly the same",
      "offset": 2505.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "starting point at the same level of",
      "offset": 2507.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "capability, the people who had more GPUs",
      "offset": 2509.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "would pull ahead slowly but surely over",
      "offset": 2512.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "everybody else, right? Um, so, so I",
      "offset": 2514.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "think I think that there's this",
      "offset": 2517.52,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "unfortunate\n around that, but go ahead.",
      "offset": 2518.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "So there there's this unfortunate sort",
      "offset": 2519.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of inherent uh I I don't know if I want",
      "offset": 2521.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "to say winner or take all effect, but",
      "offset": 2525.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "there's like this return to scale sort",
      "offset": 2526.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of thing inherent in the dynamics of an",
      "offset": 2528.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "intelligence explosion. Um and that's",
      "offset": 2530.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "part of what makes this so scary. Um",
      "offset": 2532.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "yeah,\n all the actors are incented to",
      "offset": 2536.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "push it as fast as possible and to to",
      "offset": 2538.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "have the maximal resources in order to",
      "offset": 2541.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "do so.\n And they're also incentivized not",
      "offset": 2543.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to open it up and, you know, to not be",
      "offset": 2544.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "transparent about it and so forth.",
      "offset": 2547.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Unfortunately.\n Yeah. I I I think we have",
      "offset": 2548.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a little disagreement there about",
      "offset": 2551.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "transparency and how much we might",
      "offset": 2552.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "expect. I think you're a little bit more",
      "offset": 2554.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "optimistic. Um and I'm I'm",
      "offset": 2555.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "uh Daniel and being a little bit less",
      "offset": 2559.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "optimistic about transparency. I'm",
      "offset": 2561.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "especially less optimistic about",
      "offset": 2562.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "transparency as we get closer to having",
      "offset": 2564.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "actual AGI or let's say differentiated",
      "offset": 2567.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "AI. So right now I would say that what's",
      "offset": 2570.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there is not very differentiated.",
      "offset": 2572.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Everybody's kind of using LLMs with",
      "offset": 2574.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "reasoning models and so forth. Um",
      "offset": 2576.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there's some differentiation in how",
      "offset": 2578.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "people do their RL and what their data",
      "offset": 2580.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "sets are, but um I think there's not so",
      "offset": 2582.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "much value in transparency until",
      "offset": 2586.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "somebody has something that really is",
      "offset": 2588.96,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "unique that they think other people",
      "offset": 2590.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "aren't going to just reconstruct very",
      "offset": 2591.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "rapidly. Um and as one gets to that",
      "offset": 2593.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "point of having some unique piece of",
      "offset": 2596.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "intellectual property, which I think",
      "offset": 2598.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "will happen, um there's even more reason",
      "offset": 2599.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "to be less transparent. So I I I think",
      "offset": 2602.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "I'd be optimistic about being",
      "offset": 2605.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "transparent about the numbers of the",
      "offset": 2608,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "best models internally. Not necessarily",
      "offset": 2610.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the methods that were used to create",
      "offset": 2612.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "them or the weights for them, but at",
      "offset": 2614.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "least the public knowing uh uh what's",
      "offset": 2617.04,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "going on um what's the peak capabilities",
      "offset": 2620.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "that the models are exhibiting and then",
      "offset": 2623.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "being aware of that.\n Are you being",
      "offset": 2625.68,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "optimistic that they'll do this by",
      "offset": 2626.72,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "default or that this would be good?",
      "offset": 2627.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Obviously, it'd be good. Um I I think",
      "offset": 2629.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it's good and that there's more",
      "offset": 2630.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "tractability for this than a lot of",
      "offset": 2632.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "other asks.\n I agree.\n More tractability",
      "offset": 2633.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like people might agree to do it.\n Yeah.",
      "offset": 2636.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. I agree with that. That's",
      "offset": 2637.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "why I've been making these asks. Yeah.",
      "offset": 2639.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "But I'm just saying that they're not",
      "offset": 2640.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "going to do it by default. Like if",
      "offset": 2641.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "nobody asks them to be transparent about",
      "offset": 2643.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "these things.\n No, I agree.\n So I mean we",
      "offset": 2644.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "can agree that like voluntary reg",
      "offset": 2647.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "self-regulation is probably not enough",
      "offset": 2649.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to get to that.\n Yeah. Yeah.\n Um",
      "offset": 2650.96,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "yeah. Um I I wouldn't bank on it. Yeah.",
      "offset": 2654.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "All right. Let's talk a little bit about",
      "offset": 2657.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "time and forecasting and scenarios.",
      "offset": 2659.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "We've talked a bunch about strategy,",
      "offset": 2661.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "what we should do uh going forward, what",
      "offset": 2663.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "policy should be. Some of that depends",
      "offset": 2665.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "on timelines. So, if we thought that we",
      "offset": 2667.92,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "had a thousand years before AGI, then we",
      "offset": 2671.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "might make different choices than if we",
      "offset": 2674.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thought we had six months. Um, if we",
      "offset": 2676.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "thought that LLMs were the answer to",
      "offset": 2678.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "AGI, we might make one set of choices.",
      "offset": 2680.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "if we thought they were definitely not",
      "offset": 2683.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the answer to AGI, we might make a",
      "offset": 2684.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "different set of choices, maybe focusing",
      "offset": 2686.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "more on research. Um, so let's talk",
      "offset": 2688.64,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "about our forecasts around that and also",
      "offset": 2691.839,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "maybe I'll throw in there our forecasts",
      "offset": 2696.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "around when we might sort out the safety",
      "offset": 2698.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "problems, the alignment problems. Um, I",
      "offset": 2700.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "will make the argument that we've made",
      "offset": 2703.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "some progress towards AGI and very",
      "offset": 2704.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "little towards alignment and we can see",
      "offset": 2707.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "if if we agree on that. So",
      "offset": 2709.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I want to start by using a notion that",
      "offset": 2712.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "many in the audience will know but not",
      "offset": 2715.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "all. Uh which is a a distribution of",
      "offset": 2716.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "probability mass which is to say that",
      "offset": 2719.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you could make a simple prediction. You",
      "offset": 2722.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "could say I think AGI will be here in",
      "offset": 2724.16,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "2027 or 2039 or whatever. But I think we",
      "offset": 2727.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "all understand that that's the",
      "offset": 2731.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "unsophisticated thing to do. You can",
      "offset": 2732.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "certainly say your best guess for which",
      "offset": 2735.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "particular year you think it might come.",
      "offset": 2736.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "But as people who are either scientists",
      "offset": 2738.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "or know something about science, we know",
      "offset": 2741.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "there's what we call confidence interval",
      "offset": 2742.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "around that. So, you know, it might be",
      "offset": 2744.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this plus or minus that. The most",
      "offset": 2746.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "sophisticated thing to do is to actually",
      "offset": 2748.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "draw out a curve and say, you know, I",
      "offset": 2750.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "think some of the probability mass will",
      "offset": 2752.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "come, you know, before 2027 and some of",
      "offset": 2754.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "it will come before 2037 and some of it",
      "offset": 2756.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "will come after. in your I guess it's",
      "offset": 2758.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "sort of an appendex to your AI 2027. You",
      "offset": 2761.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "go through this in a fair amount of",
      "offset": 2764.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "detail. I'm not sure how many people got",
      "offset": 2766.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "to the appendix. I think it was maybe a",
      "offset": 2767.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "little hidden to find it, but it was",
      "offset": 2769.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "there and it did a good job of that and",
      "offset": 2770.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it gave the forecast for four different",
      "offset": 2773.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "people in a sort of qualitative way. I",
      "offset": 2775.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "don't know if it showed the full curves,",
      "offset": 2777.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "but it said like this is the chance that",
      "offset": 2778.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "they think it will come before 2027. And",
      "offset": 2779.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "I think for three of the four",
      "offset": 2783.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "forecasters or something, you remember",
      "offset": 2784.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "better than I. Um, you know, some of the",
      "offset": 2786.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "probability mass was like after 2040 or",
      "offset": 2788.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "something like that. So maybe I'll start",
      "offset": 2791.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "with you because I think you have maybe",
      "offset": 2793.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "put the most work into trying to get",
      "offset": 2795.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "detailed probability mass distributions",
      "offset": 2797.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and maybe you can talk about what your",
      "offset": 2800.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "own are and different techniques people",
      "offset": 2802.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "have used and where you're where you are",
      "offset": 2803.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "on that.\n Thank you for that uh excellent",
      "offset": 2806,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "uh launch to this. Feel free to stop me",
      "offset": 2809.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if I ramble too long. Uh because it's",
      "offset": 2811.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "huge topic, lots to talk about. Um okay,",
      "offset": 2813.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "so first the exciting part, the actual",
      "offset": 2816.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "numbers. Um when we were writing AI",
      "offset": 2818.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "2027, we had our different medians or",
      "offset": 2821.119,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "50% marks for uh AGI or I think we were",
      "offset": 2824.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you we divide up we didn't use the word",
      "offset": 2828.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "AGI, we used different milestones. So",
      "offset": 2830,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "superhuman coder uh full automation of",
      "offset": 2831.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "AI research uh super intelligence. Um",
      "offset": 2833.599,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "but uh for those things uh let's say for",
      "offset": 2836.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "super intelligence I was thinking 50%",
      "offset": 2840.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "chance by the end of 2027. Um and that's",
      "offset": 2842.079,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "why AI 2027 depicts it happening at the",
      "offset": 2845.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "end of 2027 is because I was sort of",
      "offset": 2847.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "illustrating my median projection. Now",
      "offset": 2850,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the other people at AI futures project",
      "offset": 2851.52,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "tended to be somewhat more optimistic",
      "offset": 2852.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "than me. They tended to think it would",
      "offset": 2854.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "take longer uh to get to super",
      "offset": 2855.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "intelligence.\n Let's clarify optimistic",
      "offset": 2856.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "is that flips depending on how you think",
      "offset": 2859.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "is sure. Yeah. So the other\n they thought",
      "offset": 2861.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "it would take longer like 2029 2031",
      "offset": 2862.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "something like\n let's say more",
      "offset": 2864.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "conservative\n few more years. Yeah that's",
      "offset": 2866,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "right. Um and uh but you know I was the",
      "offset": 2867.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "boss so we went with my my timelines but",
      "offset": 2870.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "happily uh by the time we actually",
      "offset": 2872.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "published it I had lengthened my",
      "offset": 2874.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "timeline somewhat. So so these days I",
      "offset": 2875.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "would say 50% by end of 2028. Um but uh",
      "offset": 2877.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "yeah\n doesn't give me that much comfort.",
      "offset": 2881.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "I mean, I I think you're wrong, but if",
      "offset": 2883.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "we have an extra 12 months, like I'm not",
      "offset": 2886.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "sure that's enough to handle all the",
      "offset": 2888.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "still quite scary to prepare.\n But um in",
      "offset": 2890.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "terms of what the shape of our",
      "offset": 2892.88,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "distributions looks like, they tend to",
      "offset": 2893.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "have a sort of hump in the next five",
      "offset": 2894.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "years and then like a long tail. And the",
      "offset": 2896.8,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "reason for that is because um\n well, the",
      "offset": 2899.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "pace of AI progress has been quite fast",
      "offset": 2902.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "over the last 15 years. And we",
      "offset": 2904.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "understand something about the reasons",
      "offset": 2907.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "for why it's been so fast. And basically",
      "offset": 2908.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the reason is scale. So they've been",
      "offset": 2910,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "scaling up compute, they've been scaling",
      "offset": 2912,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "up data, they've been drawing in ever",
      "offset": 2913.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "more researchers into the field,",
      "offset": 2915.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "especially compute is the probably the",
      "offset": 2916.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "main the most important input that",
      "offset": 2918.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "they're scaling up. Um, and that's sort",
      "offset": 2919.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of turbocharged progress, but uh they",
      "offset": 2922.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "simply won't they the companies simply",
      "offset": 2924.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "won't be able to scale things up at the",
      "offset": 2926.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "same pace after a couple years.\n Is that",
      "offset": 2929.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a function of power or is it a function",
      "offset": 2931.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "like electrical power? What what is the",
      "offset": 2933.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "function? What's the rate limiting step",
      "offset": 2935.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "by which you think that kind of scaling",
      "offset": 2937.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "won't continue? It won't be like a sharp",
      "offset": 2939.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "cut off, but it'll be a couple things.",
      "offset": 2940.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "So, so partly it'll be, you know, power",
      "offset": 2942.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "supplies. Partly it'll just be um uh",
      "offset": 2944.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "compute production like much of the",
      "offset": 2948.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "world's even after building new fabs and",
      "offset": 2950.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "even after converting much of the",
      "offset": 2952.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "world's chip production into uh AI",
      "offset": 2953.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "chips. Uh they won't they'll have to",
      "offset": 2955.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like produce 10 times more fabs in order",
      "offset": 2958.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to scale up by 10 times, right? Whereas",
      "offset": 2960.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "previously they could just take chips",
      "offset": 2962.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "designed for gaming and you repurpose",
      "offset": 2964.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "them for AI. So, so in a bunch of little",
      "offset": 2966.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "ways that are going to add up, there's",
      "offset": 2968.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "going to be all these frictions that's",
      "offset": 2969.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "going to start to bite that will make it",
      "offset": 2971.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "harder for them to continue the crazy",
      "offset": 2973.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "exponential rate of scale to\n I think",
      "offset": 2975.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we're already seeing that with data. I",
      "offset": 2977.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "think I don't even know the actual",
      "offset": 2978.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "numbers, but um let's say that GPT2 used",
      "offset": 2980.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "maybe 10% of the internet or something",
      "offset": 2984.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like that or 5% or something like that.",
      "offset": 2986.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "U maybe you guys know the actual numbers",
      "offset": 2988.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and GPT3, you know, used a significantly",
      "offset": 2989.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "larger fraction. GPT4 used like most of",
      "offset": 2992.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the internet including transcriptions of",
      "offset": 2995.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "of videos and stuff like that. And so",
      "offset": 2997.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "you can't just keep 100xing that because",
      "offset": 2999.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "there just isn't enough data. There's",
      "offset": 3002.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "new data generated every day, you know,",
      "offset": 3004.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "so um you can always ek out a little",
      "offset": 3006.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "more and people are turning to augmented",
      "offset": 3009.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "data um as well. They should, but that's",
      "offset": 3010.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "not a kind of universal solvent and it",
      "offset": 3013.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "works better for things like math where",
      "offset": 3015.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you can verify that the augmented data",
      "offset": 3016.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "are better. And so I think we're already",
      "offset": 3018.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "running against that. um kind of like",
      "offset": 3020.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "bottleneck on one of the let's say raw",
      "offset": 3023.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "resources that go in at least into the",
      "offset": 3025.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "current approaches.\n And another thing I",
      "offset": 3027.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "would add is that um you can also just",
      "offset": 3029.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "think about money which is which you can",
      "offset": 3031.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "use to buy many of these things and you",
      "offset": 3033.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "can say well they've been scaling up the",
      "offset": 3035.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "amount of money that they're spending on",
      "offset": 3036.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "AI research and on training runs in",
      "offset": 3037.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "particular um over the last decades but",
      "offset": 3039.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "they it'll be hard for them to continue",
      "offset": 3042.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "scaling at the same pace. You know they",
      "offset": 3044.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "they're probably already doing something",
      "offset": 3046.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "like billion dollar training runs but",
      "offset": 3047.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you know if it goes at the same p Uh",
      "offset": 3050.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "what was the biggest training run in in",
      "offset": 3052.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in 2020 was like what like $3 million",
      "offset": 3053.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "something like that you know four $5",
      "offset": 3056.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "million. So so they they've gone up by",
      "offset": 3058.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like two and a half orders of magnitude",
      "offset": 3060.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "in 5 years. If it's another two and a",
      "offset": 3062,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "half orders of magnitude we're doing a",
      "offset": 3063.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "$500 billion training run in 2030. Like",
      "offset": 3064.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there starts to be not enough.\n Yeah.",
      "offset": 3067.92,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "There's just not enough money in the",
      "offset": 3069.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "world, you know, like the the tech",
      "offset": 3070.559,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "companies just won't be able to afford",
      "offset": 3071.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it. Even if they've grown bigger than",
      "offset": 3073.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "they are today, the economy just won't",
      "offset": 3074.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "be able to afford it. And so, so that's",
      "offset": 3076.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "why we predict that like if you don't",
      "offset": 3078.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "get to some sort of radical",
      "offset": 3081.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "transformation, if you don't get to some",
      "offset": 3082.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sort of crazy AI powered automation of",
      "offset": 3083.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the economy by the end of this decade,",
      "offset": 3086.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "then there's going to be a bit of an AI",
      "offset": 3088.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "winter. There's going to be at the very",
      "offset": 3089.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "least a sort of tapering off of the pace",
      "offset": 3091.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of progress. Um, and then that sort of",
      "offset": 3093.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "stretches out a lot of probability mass",
      "offset": 3096.24,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "into the future because that's a very",
      "offset": 3097.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "different world. It's like, you know,",
      "offset": 3098.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "could take forever. Well, not forever,",
      "offset": 3100.559,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "it could take a quite a long time to uh",
      "offset": 3101.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to get to AGI once you're in that",
      "offset": 3103.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "regime. I'm gonna ask you one or two",
      "offset": 3105.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "more questions. I'm gonna insert mine",
      "offset": 3107.28,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "and then I'm going to come to Dan.",
      "offset": 3108.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Although if you want to\n I mean one one",
      "offset": 3109.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "note on data was yeah we sort of ran",
      "offset": 3111.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "into that bottleneck I think maybe two",
      "offset": 3114,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "or so years ago. Um and",
      "offset": 3115.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "the main things that have been",
      "offset": 3119.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "continuing the pace um uh would be this",
      "offset": 3121.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "you know thinking mode type of stuff uh",
      "offset": 3124,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "um outside of the the trends that were",
      "offset": 3126.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "existing previously. So I think it's",
      "offset": 3128.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "sort of picking up slack in some ways",
      "offset": 3130.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "for the the the fact that most of the",
      "offset": 3131.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "internet has been trained on.\n Um the",
      "offset": 3133.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "methodology by which you came up with",
      "offset": 3136.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "these curves, can you just tell us a",
      "offset": 3139.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "little bit about them?\n Yeah. So again,",
      "offset": 3140.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "these curves represent our subjective",
      "offset": 3142.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "judgment which is very uncertain. It's",
      "offset": 3143.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "just our opinions, you know, but the the",
      "offset": 3145.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "way that",
      "offset": 3148.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I guess the the way I would like to to",
      "offset": 3151.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "say it should be done is um rather than",
      "offset": 3152.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "just sort of uh pulling a number out of",
      "offset": 3155.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "your ass. so to speak. You should come",
      "offset": 3158.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "up with models and look at trends and",
      "offset": 3160.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "then, you know, have have have uh little",
      "offset": 3162.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "calculations that attempt to give",
      "offset": 3166.319,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "numbers and then you should stare at all",
      "offset": 3167.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "of that and then pull a number out of",
      "offset": 3169.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "your ass based on based on all that",
      "offset": 3171.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "stuff that you've just looked at, you",
      "offset": 3173.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "know. Um, and so and so that's what we",
      "offset": 3174.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "did. And the the the main arguments and",
      "offset": 3177.119,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "pieces of evidence that we found uh",
      "offset": 3179.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "moving to to inform our overall",
      "offset": 3182.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "estimates were uh what we would call the",
      "offset": 3184.8,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "benchmark and plus gaps argument. Um",
      "offset": 3187.68,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "perhaps I should also go into the like",
      "offset": 3191.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "um compute-based forecast. Have you",
      "offset": 3194.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "heard of the bioankers uh framework by",
      "offset": 3196.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "AJotra?\n I don't think I have.\n Okay.",
      "offset": 3198.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Well, we can I'll I'll briefly mention",
      "offset": 3200.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that before getting into the benchmarks",
      "offset": 3202.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "plus gaps thing. So, so the the",
      "offset": 3204.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "bioanchers framework, it's called",
      "offset": 3206.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "bioanchors because it references the",
      "offset": 3208.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "human brain. And I think that part's",
      "offset": 3210.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "actually the less exciting and plausible",
      "offset": 3212.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "part of it. The part that I think is is",
      "offset": 3213.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "more robust and more worth using is this",
      "offset": 3216.559,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "core idea that you can you can think of",
      "offset": 3219.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this trade-off between more time to come",
      "offset": 3221.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "up with new ideas and do AI research and",
      "offset": 3224.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "uh more compute with which to do the AI",
      "offset": 3227.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "research. And you can sort of think you",
      "offset": 3230.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "can you can make a big two two",
      "offset": 3232.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "dimensional plot and you can imagine",
      "offset": 3234,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "okay 10 more years 20 more years 30 more",
      "offset": 3236.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "years what's how does the probability",
      "offset": 3239.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that we get to AGI go up with more time",
      "offset": 3241.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "but you can also imagine uh not more",
      "offset": 3244,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "time but just more compute could we get",
      "offset": 3247.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to AGI today if we had you know five",
      "offset": 3249.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "orders of magnitude more compute 10",
      "offset": 3252.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "orders of magnitude more compute 30",
      "offset": 3253.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "orders of magnitude more compute right",
      "offset": 3255.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and the the insight there is that the",
      "offset": 3257.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "answer is yeah probably like like for",
      "offset": 3259.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "example if you had 10^ the 45",
      "offset": 3262,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "floatingoint operations, you could do a",
      "offset": 3264.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a training run that's basically just",
      "offset": 3267.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "simulating the entire planet Earth and",
      "offset": 3269.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "all life evolving on it for a billion",
      "offset": 3271.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "years, you know, with that amount of",
      "offset": 3273.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "compute. And and and the thought there",
      "offset": 3275.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is that you don't really need to",
      "offset": 3277.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "understand how intelligence works at all",
      "offset": 3278.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "if you're building it with that type of",
      "offset": 3280.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "training run because there's no insight",
      "offset": 3282.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "coming from you. It's you're just sort",
      "offset": 3284.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "of letting nature do its thing and",
      "offset": 3285.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "letting evolution do take its course.",
      "offset": 3286.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "And so so the thought is that we can",
      "offset": 3289.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "make a not guaranteed but like a soft",
      "offset": 3290.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "upper bound at something like 10^ the 45",
      "offset": 3292.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and then you can make other sort of soft",
      "offset": 3295.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "upper bounds. You can think well what",
      "offset": 3297.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "could we do with 10^ the 36 and you can",
      "offset": 3299.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "lay I I wrote a blog post about this in",
      "offset": 3301.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "2021. You know, suppose we had 10 to the",
      "offset": 3302.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "36 flop. What are some like really huge",
      "offset": 3304.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "types of training runs we could do? And",
      "offset": 3307.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "then like what's our guess as to how",
      "offset": 3309.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "likely that is to work? And and what you",
      "offset": 3311.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "can do is you can sort of you can start",
      "offset": 3313.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "to smear out your probability mass over",
      "offset": 3315.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "this dimension of compute. And so you",
      "offset": 3317.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "sort of have a soft upper bound and then",
      "offset": 3321.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you have of course a lower bound which",
      "offset": 3322.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "is the amount that we already have done.",
      "offset": 3324.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "You we clearly haven't done it right now",
      "offset": 3325.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "with this amount of compute. Uh and so",
      "offset": 3327.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that gives you this smeared probability",
      "offset": 3329.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "distribution over compute. And then you",
      "offset": 3331.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "think, okay, but now we're also going to",
      "offset": 3332.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "get new ideas. And so as new ideas come",
      "offset": 3334.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "along, we're going to be able to come up",
      "offset": 3336.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "with more efficient methods that allow",
      "offset": 3338.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "us to train it with less compute. So you",
      "offset": 3340,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "can think of your probability",
      "offset": 3341.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "distribution as shifting downwards while",
      "offset": 3342.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "also the amount of actual compute",
      "offset": 3344.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "increases. And then that gets you your",
      "offset": 3346.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "actual distribution over years. Um and",
      "offset": 3348.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "uh I think this is the right sort of",
      "offset": 3351.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like basic framework for for calculating",
      "offset": 3353.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "these sorts of timelines. Um, but it's a",
      "offset": 3355.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "sort of relatively abstract like low",
      "offset": 3358.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "information framework that doesn't",
      "offset": 3361.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "really look at the details of the",
      "offset": 3362.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "technology today and the details of the",
      "offset": 3363.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "benchmarks. So I think it's like a good",
      "offset": 3365.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "way to get your prior so to speak. Uh,",
      "offset": 3367.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "but then you should update based on",
      "offset": 3369.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "actual trends on the benchmarks and so",
      "offset": 3371.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "forth, which is what I'm about to get",
      "offset": 3373.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to. But the the reason why I mentioned",
      "offset": 3374.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "this prior process is that 10 to the 45",
      "offset": 3376.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "floatingoint operations isn't actually",
      "offset": 3379.92,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "that far away from where we are right",
      "offset": 3381.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "now. Um, right now we're at like what",
      "offset": 3382.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like 10^ the 26 or something for for",
      "offset": 3385.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "training runs and we're going to be",
      "offset": 3387.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "crossing a few orders of magnitude uh in",
      "offset": 3389.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the next couple years. And so even if",
      "offset": 3391.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you just had like a an in even if you",
      "offset": 3394.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just smeared out your probability mass",
      "offset": 3396.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "with maximum uncertainty across the like",
      "offset": 3397.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "orders of magnitude from where we are",
      "offset": 3401.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "now to 10^ the 45, there'd be like you",
      "offset": 3403.359,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "know a non negligible amount that it's",
      "offset": 3406.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "going to happen in the next few years.",
      "offset": 3408.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "uh and so like even on priors you should",
      "offset": 3410.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "think it like decently plausible that it",
      "offset": 3412,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "could happen by the end of the decade",
      "offset": 3413.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "and then you should update your prior",
      "offset": 3415.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "based on the actual evidence which I'll",
      "offset": 3417.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "now get to. So the actual evidence I",
      "offset": 3418.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "would say let's look at agentic coding",
      "offset": 3420.319,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "benchmarks that seems to me to be um the",
      "offset": 3422.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "most informative thing to look at and",
      "offset": 3426.559,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "the reason for that is because I don't",
      "offset": 3428.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "think that the fastest way to get to",
      "offset": 3429.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "super intelligence is in a single leap",
      "offset": 3432.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "where humans come up with the new",
      "offset": 3433.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "paradigms uh in their own brains. I",
      "offset": 3435.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "think rather it's going to be this more",
      "offset": 3437.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "gradual process where humans automate",
      "offset": 3438.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "more of the AI research process and then",
      "offset": 3440.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that gets us to the new paradigms fast.",
      "offset": 3442.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "And so I think that the the the lowest",
      "offset": 3444.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "hanging fruit as far as the AI research",
      "offset": 3447.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "process is concerned that's going to",
      "offset": 3449.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "automate first is the coding. So I'm",
      "offset": 3450.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "looking to see when will we get to the",
      "offset": 3452.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "point where the the coding is basically",
      "offset": 3454.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "all handled by LLM like AI assistants.",
      "offset": 3456.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Um and we have benchmarks for that sort",
      "offset": 3460.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of like you know com places like meter",
      "offset": 3462.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "MER have been uh building these little",
      "offset": 3464.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "coding environments doing all these",
      "offset": 3468.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "coding tasks. The companies themselves",
      "offset": 3469.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "have been doing this of course because",
      "offset": 3471.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "they are racing as fast as they can to",
      "offset": 3472.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "get to this automated coder milestone",
      "offset": 3474.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and um",
      "offset": 3476.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "so we look at those and we extrapolate",
      "offset": 3478.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "trends on them and we we forecast that",
      "offset": 3480.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "well in the next couple years they're",
      "offset": 3482.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "basically going to saturate. you know,",
      "offset": 3483.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we're gonna have AIs that can just crush",
      "offset": 3485.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "all of these coding tasks. And they're",
      "offset": 3487.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "relatively, you know, they're they're",
      "offset": 3489.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "not something to scoff at. They're not",
      "offset": 3490.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "just multiple choice questions. They're",
      "offset": 3492.799,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "like the sort of task that would take a",
      "offset": 3494.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "human like four hours to do or eight",
      "offset": 3495.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "hours to do. Um uh but that's not the",
      "offset": 3498.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "same thing as completely automated",
      "offset": 3501.44,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "coding. So first we extrapolate to when",
      "offset": 3502.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "they saturate the benchmarks and then we",
      "offset": 3504.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "try to make our guess as to what the gap",
      "offset": 3506.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is between the first system that can",
      "offset": 3508.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "completely saturate these benchmarks and",
      "offset": 3510.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the first system that can actually",
      "offset": 3512.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "automate the coding. And that's probably",
      "offset": 3514.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "the more speculative part. Uh but you",
      "offset": 3516.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "know we do our best to reason about well",
      "offset": 3519.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "now we're ready for our first full-on",
      "offset": 3521.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "disagreement of the day. Um but I",
      "offset": 3524,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "understand I understand your logic",
      "offset": 3526.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "there. I think it's well thought through",
      "offset": 3528.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but it's missing the cognitive science",
      "offset": 3531.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "for me and my approach to this is more",
      "offset": 3532.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "from the cognitive science. I see a set",
      "offset": 3535.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "of problems that a cognitive creature",
      "offset": 3538.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "must must solve many of which I wrote in",
      "offset": 3540.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "my 2001 book the algebraic mind and I",
      "offset": 3543.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "don't feel like we've solved any of",
      "offset": 3546.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "those problems despite the quantitative",
      "offset": 3547.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "progress that we've made and those",
      "offset": 3550.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "include generalizing outside the",
      "offset": 3552.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "distribution with which I think still",
      "offset": 3554.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "remains a huge problem. I think the",
      "offset": 3556.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Apple paper was a um there actually two",
      "offset": 3558.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Apple papers I discovered today. Um but",
      "offset": 3561.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the Apple paper with the Tower of Hanoi",
      "offset": 3563.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "stuff I think is an example of problems",
      "offset": 3565.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "with distribution shift. I think we've",
      "offset": 3567.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "seen many of them over the years. We see",
      "offset": 3569.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that these systems have trouble multi",
      "offset": 3571.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "doing multiplication with large numbers",
      "offset": 3573.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "unless they call on tools um and so",
      "offset": 3575.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "forth. I think there's lots of evidence",
      "offset": 3578.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "for that. I think that there's a problem",
      "offset": 3580.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "of distinguishing types and tokens that",
      "offset": 3582.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "leads to bleedth through when you're",
      "offset": 3585.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "representing multiple individuals from",
      "offset": 3587.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some category that leads to",
      "offset": 3589.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "hallucinations. So I wrote an essay",
      "offset": 3591.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "recently about the hallucinations that",
      "offset": 3593.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "um I think it was Chat GPT made about my",
      "offset": 3595.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "friend Harry Sheer who's a pretty",
      "offset": 3598.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "well-known actor um and it misnamed the",
      "offset": 3600.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "roles of characters that he played in",
      "offset": 3602.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the movie Spinal Tap and said that he",
      "offset": 3604.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "was British when he's American and so",
      "offset": 3606.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "forth. I think this blurring together",
      "offset": 3608.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that we see of hallucinations remains a",
      "offset": 3610.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "problem. And I could go on with a list",
      "offset": 3612.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "of others. I think there are several",
      "offset": 3614.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "having to do with reasoning, planning,",
      "offset": 3615.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "etc. And the way I look at things, which",
      "offset": 3617.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is not to say that there isn't some",
      "offset": 3620.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "value in what you're doing, um, is more",
      "offset": 3621.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "on these cognitive tasks. And so what I",
      "offset": 3623.68,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "say to myself is what would AI look like",
      "offset": 3626.799,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "two years before we achieved AGI or ASI",
      "offset": 3630.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "or something like that. Um certainly two",
      "offset": 3633.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "years before we achieved ASI we would",
      "offset": 3635.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have full solutions to all of those",
      "offset": 3637.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "things. If we specified if we specified",
      "offset": 3639.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "an algorithm for something we would",
      "offset": 3642.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "expect the system to be able to follow",
      "offset": 3644.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it. Current systems can't even play",
      "offset": 3645.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "chess reliably according to the rules.",
      "offset": 3648,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "So you know 03 will not uh sorry 03 will",
      "offset": 3650.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "sometimes make illegal moves. It can't",
      "offset": 3655.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "avoid illegal moves. Um, another thing I",
      "offset": 3657.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "would expect is that current systems",
      "offset": 3660.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "when we're close would be basically the",
      "offset": 3662.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "equivalent of their domain specific uh",
      "offset": 3665.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "counterparts or at least be close.",
      "offset": 3667.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Right? AGI means artificial general",
      "offset": 3669.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "intelligence. And I would say the",
      "offset": 3671.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "reality is that domain specific systems",
      "offset": 3672.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "are actually much better than the",
      "offset": 3675.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "general ones. Right? Now the only",
      "offset": 3676.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "general ones we have are LLM based. Um",
      "offset": 3678,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "but for example, Alphafold is a very",
      "offset": 3681.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "carefully engineered hybrid",
      "offset": 3684,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "neurosymbolic system that far",
      "offset": 3686.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "outperforms what you could get from a",
      "offset": 3688.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "pure chatbot or something like that. Um",
      "offset": 3690.319,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "somebody just showed that an Atari 2600",
      "offset": 3693.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "beat I think it was 03 and chess. Um so",
      "offset": 3695.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "even you know sometimes very old uh",
      "offset": 3698.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "systems will will beat the modern domain",
      "offset": 3701.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "general ones. on Tower of Hanoi. Uh Herb",
      "offset": 3703.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Simon solved it in 1957 with a classical",
      "offset": 3706.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "technique that generalizes to arbitrary",
      "offset": 3709.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "length whereas the LLMs do not",
      "offset": 3711.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "generalize to arbitrary length and face",
      "offset": 3713.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "problems. And so I could go through more",
      "offset": 3715.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "but the gist of it is I don't see the",
      "offset": 3717.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "qualitative problems that I think need",
      "offset": 3721.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to be solved. Um and I'll just go to",
      "offset": 3723.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "your 1045 10 to the 45th because it's",
      "offset": 3726,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "really interesting to me. Um I wrote a",
      "offset": 3728.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "piece once with Kristoff Ko. I don't",
      "offset": 3730.16,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "know if you guys know him, the",
      "offset": 3731.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "neuroscientist. Um, we wrote a science",
      "offset": 3732.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "fiction essay. It's the only published",
      "offset": 3735.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "science fiction essay I've ever written.",
      "offset": 3736.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "And it was in a book that I wrote called",
      "offset": 3738.319,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "The Future of the Brain, which I guess",
      "offset": 3739.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we wrote in 2015. I was the editor. And",
      "offset": 3741.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we wrote the epilogue to it in I'll call",
      "offset": 3744,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "it 2015. And so it was said in something",
      "offset": 3746.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like 2015",
      "offset": 3748.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "or I think it was 2045. And the notion",
      "offset": 3751.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "was there's a book about neuroscience",
      "offset": 3753.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that by that point we would actually",
      "offset": 3755.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "have created an entire simulation of the",
      "offset": 3757.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "brain but it would run slower than the",
      "offset": 3760.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "human brain and would still not have",
      "offset": 3762.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "taught us anything about how the brain",
      "offset": 3765.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "really works. So we'd have this",
      "offset": 3767.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "simulation but we wouldn't understand",
      "offset": 3768.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the principles of it. you know, we'd",
      "offset": 3770.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have a neuron by neuron simulation,",
      "offset": 3772.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "maybe even a protein by protein",
      "offset": 3774.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "simulation, but we could find ourselves",
      "offset": 3776.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "in the place where we'd replicated the",
      "offset": 3778,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "whole thing without really understanding",
      "offset": 3780,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "where it worked. And I do wonder with",
      "offset": 3781.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the 10 to 45, even if you sort of",
      "offset": 3783.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "trained on everything, would you have",
      "offset": 3785.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "solved the distribution shift? And would",
      "offset": 3787.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you have abstracted principles that",
      "offset": 3789.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "allow you to run efficiently and",
      "offset": 3791.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "effectively and and usefully in new new",
      "offset": 3793.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "domains and so forth? I think it's a",
      "offset": 3796.16,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "really interesting question. I had never",
      "offset": 3797.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "thought of the the 1045 even though I I",
      "offset": 3799.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "read at least some of your paper. I",
      "offset": 3801.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "admit I didn't get to that level of",
      "offset": 3802.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "detail.\n Well, this part wasn't in 2020.",
      "offset": 3804.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "This is sort of\n so it wasn't in that",
      "offset": 3806.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "appendix.\n I admit I didn't read the",
      "offset": 3808.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "whole appendix. I read some of it. Um I",
      "offset": 3810.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "think it's a really fascinating thought",
      "offset": 3812.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "experiment. So maybe I've said enough.",
      "offset": 3814.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So just to lay it out like there's one",
      "offset": 3816.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "way to extrapolate on basis of things",
      "offset": 3819.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like compute and I think you've done a",
      "offset": 3821.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "masterful job of doing that as well as",
      "offset": 3823.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "can be done and acknowledging that",
      "offset": 3824.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "there's still an element of pulling",
      "offset": 3826.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "things out of one's um behind which is",
      "offset": 3828,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "true on you know any any account like",
      "offset": 3830,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "nobody can really do this in a closed",
      "offset": 3832.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "form way. Um and then I have a different",
      "offset": 3833.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "slice on it which is like where are the",
      "offset": 3837.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "qualitative things that I want to have",
      "offset": 3839.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "solved. Um I think Yan Lun who I often",
      "offset": 3841.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "disagree about um many things with",
      "offset": 3844.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually would be closer to mind. He",
      "offset": 3846.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "would probably give a different set of",
      "offset": 3848,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "litmus tests that he's looking for. We",
      "offset": 3849.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "would both emphasize world models. We",
      "offset": 3851.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "have slightly different ideas about",
      "offset": 3853.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "world models. But he would say I don't",
      "offset": 3854.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think we're close because we don't",
      "offset": 3856.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "really have world models and neither of",
      "offset": 3857.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "us I think are satisfied with the",
      "offset": 3860.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "current thing that some people call",
      "offset": 3862.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "reasoning but neither of us think is you",
      "offset": 3863.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "know robust enough. And so I think he",
      "offset": 3865.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and I both take an architectural",
      "offset": 3867.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "approach or a cognitive approach.\n Great.",
      "offset": 3869.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Uh so saying I'll list a bunch of bullet",
      "offset": 3872.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "points of things we could discuss and",
      "offset": 3874.4,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "then hopefully we can get through them",
      "offset": 3875.52,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "each and if we miss some well at least I",
      "offset": 3876.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "put them out. So in reverse order um 10",
      "offset": 3878.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "45 scenario where you just sort of brute",
      "offset": 3881.039,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "force evolve intelligence. Indeed you",
      "offset": 3882.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "would not understand how it works at",
      "offset": 3884.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "all. Um, but nevertheless, you would",
      "offset": 3885.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "have it and you could sort of take those",
      "offset": 3887.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "evolved creatures out of their simulated",
      "offset": 3889.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "environment and then start, you know,",
      "offset": 3890.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "plugging them into like chat products",
      "offset": 3893.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and stuff and using them in your",
      "offset": 3895.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "economy. Um, and they would be smart,",
      "offset": 3897.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you know, they evolved. They built their",
      "offset": 3900,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "own civilization in there, you know, so",
      "offset": 3901.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so they're pretty smart and they're",
      "offset": 3903.039,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "pretty good at generalizing and so",
      "offset": 3904,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "forth. Um, so you wouldn't understand",
      "offset": 3905.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "how it works, but you'd still",
      "offset": 3906.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "nevertheless have the AI system. And and",
      "offset": 3907.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "indeed that's kind of what I think is",
      "offset": 3910.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "happening today for us. It's like we",
      "offset": 3911.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "don't understand how these AI systems",
      "offset": 3913.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "work very well. Uh we're sort of just",
      "offset": 3916.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "throwing giant blobs of compute at giant",
      "offset": 3918.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "data sets and training environments and",
      "offset": 3920.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "then toying out playing around with them",
      "offset": 3922.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "afterwards and seeing what they're good",
      "offset": 3924.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "at, you know. Um\n that's part of my",
      "offset": 3926.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "darkness about it is I think there's too",
      "offset": 3928.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "much alchemy and not enough principles.",
      "offset": 3930.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "100%. And this is and this is part of",
      "offset": 3932.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "why the like alignment issue feels so",
      "offset": 3934,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "looming to me is that like we don't even",
      "offset": 3936.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "know what we're doing. How are we",
      "offset": 3938.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "supposed to how are we supposed to craft",
      "offset": 3939.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a mind that has you know the right",
      "offset": 3941.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "virtues and the right principles and so",
      "offset": 3943.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "forth when we don't even like right",
      "offset": 3945.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "anyhow so so so there's that uh next",
      "offset": 3947.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "thing so you you mentioned um",
      "offset": 3949.76,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "uh tower of Hanoi math problems these",
      "offset": 3953.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "these sort of ways in which the current",
      "offset": 3956.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "AI systems seem uh limited in in",
      "offset": 3958.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "particular limited and fragile and and",
      "offset": 3961.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "hallucinations in ways that you you",
      "offset": 3963.2,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "think that we're not really making",
      "offset": 3964.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "progress on so there I would say Well,",
      "offset": 3965.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "um, you know, I also can't solve the",
      "offset": 3968.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "tower of Henoi and I also can't do large",
      "offset": 3970.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "math problems in my head. I need tools.",
      "offset": 3973.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I need to be able to like program a",
      "offset": 3975.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "little bit or like do use\n I'm going to",
      "offset": 3976.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "come in on that briefly. Okay.\n One is",
      "offset": 3978.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that, you know, relatively young",
      "offset": 3980.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "children can actually do towers of annoy",
      "offset": 3983.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "really well if they care about it. So,",
      "offset": 3985.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "um,\n even really big ones,\n even pretty",
      "offset": 3986.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "big ones. Um, I mean, there's a video I",
      "offset": 3989.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "think of of a kid doing seven discs",
      "offset": 3991.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "lightly lightning fast like in two",
      "offset": 3993.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "minutes on YouTube. um a kid who enjoys",
      "offset": 3995.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it. He's probably 12 or 15 or whatever.",
      "offset": 3998.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Um I'm sure he can do eight discs if he",
      "offset": 4000.16,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "wants to because it's a recursive",
      "offset": 4001.839,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "algorithm. I'm sure he's learned it. And",
      "offset": 4003.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so I think some humans can do a problem",
      "offset": 4004.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like that. Some humans if they want to",
      "offset": 4006.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "can do arithmetic. We humans do um get",
      "offset": 4008.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "into memory limitations. But it you",
      "offset": 4011.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "shouldn't expect that AGI should um",
      "offset": 4014.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "there is actually like a",
      "offset": 4016.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "um we should pause for a moment on",
      "offset": 4019.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "definition of AGI, right? It it has had",
      "offset": 4021.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "varied definitions. The one that I",
      "offset": 4023.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "always imagine is it should be at least",
      "offset": 4025.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "as good as humans in a bunch of things",
      "offset": 4028.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and better in certain ones. So I would",
      "offset": 4029.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "not be satisfied with an AGI that can't",
      "offset": 4031.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "do arithmetic. I'd be like this, you",
      "offset": 4033.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "know, yes, okay, it's equivalent to",
      "offset": 4036,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "people in in this respect, but I would",
      "offset": 4037.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "actually expect more. And especially if",
      "offset": 4040.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "we're talking about the risks that we're",
      "offset": 4042.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "um concerning, I think we're really",
      "offset": 4046,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "talking about a form of AGI that",
      "offset": 4048.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "probably can do all the short-term",
      "offset": 4050.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "memory things that people can't and has",
      "offset": 4051.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the versatility and flexibility of",
      "offset": 4053.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "humans. You can argue about that and and",
      "offset": 4055.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "you know, I I would say that like the",
      "offset": 4058.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "weakest AGI would be it's as good as Joe",
      "offset": 4060.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Sixpack, who's really not very good at",
      "offset": 4063.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "reasoning, has is full of confirmation",
      "offset": 4065.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "bias, has not gone to graduate school,",
      "offset": 4067.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "doesn't know how, you know, critical",
      "offset": 4069.599,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "reasoning. and you'd say, &quot;Okay, that's",
      "offset": 4070.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "AGI because it's it does what Joe",
      "offset": 4071.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Sixbacks does.&quot; I think most of the",
      "offset": 4073.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "safety arguments are really around AGI",
      "offset": 4076.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "that's at least as smart as like\n sure,",
      "offset": 4078.88,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "you know, most smart people. Um,",
      "offset": 4081.119,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "smart people can in fact do things like",
      "offset": 4085.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Tower of Hanoi. And certainly, you know,",
      "offset": 4087.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "another example I gave you was chess,",
      "offset": 4089.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "right? Six-year-olds can learn to follow",
      "offset": 4090.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the rules of chess. 03 will do things",
      "offset": 4092.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like have a queen jump over a knight,",
      "offset": 4094.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "which is just not possible. And so, you",
      "offset": 4095.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "know, the failures there are quite",
      "offset": 4098,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "striking and not something you need even",
      "offset": 4099.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "an expert to do.\n Yeah. So, I guess I",
      "offset": 4102.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "would say it feels to me like we're",
      "offset": 4104.96,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "making progress on these things or at",
      "offset": 4106.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "least that these these barriers are not",
      "offset": 4107.759,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "going to be uh barriers for long. Uh I",
      "offset": 4109.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "think for example that maybe the\n that's",
      "offset": 4112.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "assuming your conclusion that last",
      "offset": 4115.359,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "little piece.\n Well, let me tell you",
      "offset": 4116.88,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "more. Let me tell you why why I think",
      "offset": 4118.08,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "this. So, so I think that yeah, maybe",
      "offset": 4119.279,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like the transformer by itself might",
      "offset": 4121.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "have trouble doing a lot of this, but",
      "offset": 4123.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "you should think about the transformer",
      "offset": 4124.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "plus the system of tools and plugins",
      "offset": 4126.08,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "that you can build around it. Claude",
      "offset": 4127.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "plus code interpreter and things like",
      "offset": 4129.359,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that. Um, uh, and I think that that",
      "offset": 4131.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "system could solve tower of things like",
      "offset": 4133.759,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that because Claude might be smart",
      "offset": 4135.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "enough to to look up the algorithm and",
      "offset": 4136.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "then implement the algorithm and then,",
      "offset": 4139.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you know, do it. Um, and so so I",
      "offset": 4141.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "actually I'm curious for your immediate",
      "offset": 4144.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reaction to that point. I mean I look I",
      "offset": 4145.759,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "I've always advocated neurosymbolic AI",
      "offset": 4148.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and I think that that is actually a",
      "offset": 4150.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "species of neurosymbolic AI. I don't",
      "offset": 4152.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think it's the right one but the point",
      "offset": 4154.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of the neurosymbolic AI the arguments",
      "offset": 4156.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "that I made going back to 2001 was that",
      "offset": 4158.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you need symbols in the system to do",
      "offset": 4161.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "abstract operations over variables.",
      "offset": 4162.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "There were several other arguments but",
      "offset": 4164.719,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "that was the core argument. And what",
      "offset": 4166.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you're doing when you have claude colon",
      "offset": 4167.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "interpreter is you are doing operations",
      "offset": 4170.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "over variables in the Python that it",
      "offset": 4172.239,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "creates. And so you're you know the",
      "offset": 4174.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you're moving it to a different part of",
      "offset": 4177.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the system. So the pure neural networks",
      "offset": 4179.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "don't have operations over variables and",
      "offset": 4183.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "fail on all of these things. Symbolic",
      "offset": 4184.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "systems can do them fine. And here",
      "offset": 4186.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "you're using the neural network to",
      "offset": 4188.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "create the symbolic system that you need",
      "offset": 4190.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "in order to solve that particular",
      "offset": 4192.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "problem. So it's absolutely a",
      "offset": 4193.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "neurosymbolic solution. I think that the",
      "offset": 4195.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "rate limiting step is that they don't",
      "offset": 4198.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "always call the right code that they",
      "offset": 4199.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "need. If if you could make that solid",
      "offset": 4202.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "enough, that would be great. Okay. You",
      "offset": 4205.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you could think I'll just say a couple",
      "offset": 4206.719,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "more sentences there. Um one of the",
      "offset": 4207.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "first attempts at this strategy was to",
      "offset": 4209.52,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "put an LLM into will from Alpha as as",
      "offset": 4213.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the which is a totally symbolic system,",
      "offset": 4216.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "right? Um and not will from Alpha into",
      "offset": 4218.96,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Mathematica. Um and the results were",
      "offset": 4221.92,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "were you know hit or miss, right? And",
      "offset": 4225.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the problem was on the interface getting",
      "offset": 4229.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to the tools. If you can really reliably",
      "offset": 4231.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "get to the tools, then you have a",
      "offset": 4233.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "neurosy symbolic system that work,",
      "offset": 4235.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "right? If you can have the neural",
      "offset": 4236.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "networks reliably call the tools that",
      "offset": 4238.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "they want um or or that they should be",
      "offset": 4240.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "calling relative to the problem, I",
      "offset": 4243.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "should say, um then you're golden. I",
      "offset": 4244.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "think empirically it is bit hard to get",
      "offset": 4247.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the tools to work reliably.\n So, so that",
      "offset": 4250.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that helpful because I think that sort",
      "offset": 4252.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of collapses a lot of your um your",
      "offset": 4254.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "barriers into one barrier which is",
      "offset": 4256.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "reliability. Uh because perhaps you",
      "offset": 4258.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "would agree that like if we can get them",
      "offset": 4260.48,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "to\n we should come back to the world",
      "offset": 4262.08,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "models piece of it, but yeah, go ahead.",
      "offset": 4263.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Go run with that.\n Well, yeah. So, so,",
      "offset": 4264.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "so, so there I would say it does seem to",
      "offset": 4266.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "me like the AIS have been getting more",
      "offset": 4269.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "reliable over the last couple years. Um,",
      "offset": 4270.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and one piece of evidence I would point",
      "offset": 4273.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to is the horizon length graph from",
      "offset": 4274.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "meter, which you've probably seen me",
      "offset": 4277.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "talk.\n I wrote a whole paper about why I",
      "offset": 4278.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "hate it.\n Oh, okay. Interesting. Yeah.",
      "offset": 4279.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So, so\n in my subst. So, so the way I",
      "offset": 4281.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "would interpret this graph, and I'm sure",
      "offset": 4283.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "many of our audience has already seen",
      "offset": 4285.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this, but they have this, you know, they",
      "offset": 4287.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "they have this suite of agentic coding",
      "offset": 4289.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "tasks, uh, that are sort of organized",
      "offset": 4291.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "from, uh, from shortest to longest in",
      "offset": 4294.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "terms of like how long it takes human",
      "offset": 4297.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "beings to complete the tasks. And then",
      "offset": 4298.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "they note that, you know, the AIS of",
      "offset": 4300.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "2023 could generally speaking do the",
      "offset": 4302.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "tasks from here to here, but not do the",
      "offset": 4304.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "tasks above this length. But then each",
      "offset": 4306.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "year like the crossover point has been",
      "offset": 4308.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "lengthening. And a very natural",
      "offset": 4311.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "interpretation of what's going on here",
      "offset": 4312.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "is that the AIS are getting more",
      "offset": 4314.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "reliable. You know, if if they have a if",
      "offset": 4315.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they have a a chance of getting into",
      "offset": 4317.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "some sort of catastrophic error at any",
      "offset": 4318.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "given point, um then you know if it's",
      "offset": 4321.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "like a 1% chance per second, then after",
      "offset": 4324.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "like 50 seconds they're going to get",
      "offset": 4327.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "into an error. Uh but then if that goes",
      "offset": 4329.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "down by an order of magnitude, then they",
      "offset": 4332,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "can go for more seconds and so forth.",
      "offset": 4333.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "And so so the thought here I would say",
      "offset": 4335.199,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "this is evidence that they are just",
      "offset": 4336.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "getting generally speaking more reliable",
      "offset": 4337.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "better at not only not making mistakes",
      "offset": 4339.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "but recovering from the mistakes they",
      "offset": 4342,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "make. Not infinitely better. They're",
      "offset": 4343.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "still less reliable than humans but but",
      "offset": 4345.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there's substantial progress being made",
      "offset": 4347.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "year-over-year.\n I see that your argument",
      "offset": 4349.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "you're making um with the particular",
      "offset": 4351.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "graph I think there's a lot of problems",
      "offset": 4353.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "and some are actually relevant to this",
      "offset": 4354.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "argument. Um one is it was all relative",
      "offset": 4356,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "to coding tasks. It wasn't tasks in",
      "offset": 4358.719,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "general. um the coding tasks. I'm very",
      "offset": 4361.199,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "concerned from a scientific perspective",
      "offset": 4365.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that we don't know how much data",
      "offset": 4367.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "contamination there is. We don't know",
      "offset": 4369.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "how much augmentation is done relative",
      "offset": 4371.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to those benchmarks and so forth. Which",
      "offset": 4373.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "leads me to my next point about it,",
      "offset": 4375.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "which is um uh",
      "offset": 4376.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I can't I'm blanking on the guy's name",
      "offset": 4379.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "from 80,000 hours posted something on on",
      "offset": 4381.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Twitter yesterday which was a parallel",
      "offset": 4384.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to that graph. I think it was also by",
      "offset": 4386.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "meter uh looking at agents and so the",
      "offset": 4388.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "y-axis for the coding thing was sort of",
      "offset": 4391.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "minutes to hours to days or whatever and",
      "offset": 4393.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "for agents it was like seconds to tens",
      "offset": 4396.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of seconds or something like that. So it",
      "offset": 4399.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "was a totally different y-axis.\n What",
      "offset": 4400.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "types of agents\n like it\n I thought these",
      "offset": 4402.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "with the coding thing it was like coding",
      "offset": 4406.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "agents\n this was some more maybe like web",
      "offset": 4408,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "browsing agents or something like that.",
      "offset": 4411.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Um and so and and so the argument was",
      "offset": 4413.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that you're getting the same kind of",
      "offset": 4415.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "curve but the scaling was completely",
      "offset": 4417.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "different on the on the y- axis. Um so",
      "offset": 4419.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there's there's something there for both",
      "offset": 4422.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of us, right? So so what's there for you",
      "offset": 4423.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is is a general, you know, curve of",
      "offset": 4426,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "reliability over time. Um what's there",
      "offset": 4428.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "for me is the performance is still, you",
      "offset": 4430.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "know, pretty poor on on those agent",
      "offset": 4432.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "things. And more generally, I don't like",
      "offset": 4435.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the axis at all. I I think that it's",
      "offset": 4436.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "very arbitrary. um in in my critique we",
      "offset": 4439.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we give a bunch of examples but like how",
      "offset": 4442,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "long does it take this task and also um",
      "offset": 4444,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "it's all to the point of 50%",
      "offset": 4446.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "performance. It's just a very weird um",
      "offset": 4448.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "measure and you can find many things",
      "offset": 4451.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that you know a human can actually do in",
      "offset": 4453.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "3 seconds that according to the graph if",
      "offset": 4455.6,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "you look at the graph um you know should",
      "offset": 4457.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "have been done by 2023 model here's one",
      "offset": 4460.719,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "just off the top of my head which is",
      "offset": 4463.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "choose a legal move on a chessboard",
      "offset": 4464.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "right you know this takes a grandmaster",
      "offset": 4466.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I don't know, 100 milliseconds or",
      "offset": 4470.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "something like that. But 03 still can't",
      "offset": 4471.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "do that task with 100% reliability. It",
      "offset": 4473.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "can do 80% reliability or something like",
      "offset": 4476.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "that. So it's just it's weird to read",
      "offset": 4478.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "anything off of this graph. I'll I'll",
      "offset": 4481.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "send you the substack essay on it.\n Yeah.",
      "offset": 4483.28,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "Thanks. One one other thing I was going",
      "offset": 4484.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "to mention which is a new topic sort of",
      "offset": 4486.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "is that I think in a lot of these cases",
      "offset": 4488.239,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "my defense of the AI is that well they",
      "offset": 4491.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "never were trained on this task and\n but",
      "offset": 4493.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that's I mean that's the essence to me",
      "offset": 4495.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "right is is\n I understand they fail on",
      "offset": 4497.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "things that they're not trained on. I",
      "offset": 4499.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "mean that's an oversimplification but if",
      "offset": 4501.199,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "we're talking about AI risk and\n I I",
      "offset": 4503.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "understand that the the really dangerous",
      "offset": 4507.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "AIs of the future have to be able to do",
      "offset": 4508.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "stuff without having trained on it. uh",
      "offset": 4510.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "how at least I would say to the same",
      "offset": 4513.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "extent that humans can do things without",
      "offset": 4515.6,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "having training. So to to take your",
      "offset": 4517.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "grandmaster example, grandmasters can do",
      "offset": 4518.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that task in less than 3 seconds with",
      "offset": 4520.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "greater than 80% reliability, but also",
      "offset": 4521.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "100% reliability.\n They've trained on it",
      "offset": 4523.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "a lot like they they've done lots of and",
      "offset": 4525.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "whereas 03 possib I don't know that we",
      "offset": 4526.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "don't know the details of 03's training",
      "offset": 4529.04,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "process, but it's possible that there",
      "offset": 4530.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "was not a single playing of a game of",
      "offset": 4531.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "chess at all in the training process for",
      "offset": 4534,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah. So I don't think that's very",
      "offset": 4536.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "plausible. So there's a\n seen transcript",
      "offset": 4537.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of games of chess.\n They've seen",
      "offset": 4539.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "transcripts. They've seen the explicit",
      "offset": 4541.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "rules in wiki. I know they've been",
      "offset": 4543.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "trained on Wikipedia. They've probably",
      "offset": 4544.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "read like Bobby Fischer played chess,",
      "offset": 4546.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "which is how I learned to play chess.",
      "offset": 4547.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "So, they have books on chess. And so,",
      "offset": 4549.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "they've read everything related to",
      "offset": 4551.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "chess.\n They read read everything in what",
      "offset": 4552.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is the famous phrase publicly and",
      "offset": 4555.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "privately available sources or whatever.",
      "offset": 4557.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um, uh, that wasn't quite the famous",
      "offset": 4559.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "phrase, but um, you get the idea. I So,",
      "offset": 4561.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I think they probably actually have a",
      "offset": 4563.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "lot of data on chess. And just as a side",
      "offset": 4565.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "note on transparency from the scientific",
      "offset": 4567.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "perspective, it's very hard to really",
      "offset": 4569.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "evaluate these systems because we don't",
      "offset": 4570.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "know what's in the training set. The",
      "offset": 4572.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "core question going back to my work in",
      "offset": 4574,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "1998 is how do they generalize beyond",
      "offset": 4576.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "what they've been trained on and we just",
      "offset": 4578.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "don't have transparency on that.\n So, so",
      "offset": 4580.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "then rel so first of all I totally agree",
      "offset": 4582,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "about the transparency thing. Back to",
      "offset": 4583.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the training thing. Um I want to talk",
      "offset": 4585.199,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "about cloud plays Pokemon and I want to",
      "offset": 4588.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "again talk about the chess thing. So I",
      "offset": 4591.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "wasn't I wasn't aware of this 80%",
      "offset": 4592.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "statistic for 03, but I'm curious what",
      "offset": 4594.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the statistic\n I don't know the exact",
      "offset": 4597.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "number. I mean that that's a that's a a",
      "offset": 4598.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "butt fact is\n I haven't heard about",
      "offset": 4600.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "something I I I had a friend look into",
      "offset": 4602.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this and he's very easily able to do it.",
      "offset": 4604.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "I can give you some other example. Let",
      "offset": 4606.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "me just put one other example on the",
      "offset": 4608,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "table similar flavor which is I played",
      "offset": 4609.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "grock in tic-tac-toe and I came up with",
      "offset": 4611.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "some variation and then um I might write",
      "offset": 4613.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "this up this weekend and so I'm asked",
      "offset": 4616.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "some variation. We played it and then it",
      "offset": 4619.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "offered another variation which was",
      "offset": 4621.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "let's only play tic-tac-toe where you",
      "offset": 4623.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "can let's play tic-tac-toe where you can",
      "offset": 4625.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "only win if your three in a row is on",
      "offset": 4627.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the edges. And I said fine. We had some",
      "offset": 4628.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "moves back and forth and then it",
      "offset": 4630.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "suggested some moves for me at a point",
      "offset": 4632.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "where I could in fact make a three in a",
      "offset": 4634.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "row. So it had failed to develop the",
      "offset": 4636.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "right strategy. Um I hadn't played this",
      "offset": 4638.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "variation either before but it was",
      "offset": 4640.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "obvious what to do. So he failed to",
      "offset": 4642.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "identify what three in a row was which I",
      "offset": 4644.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "would say goes to a conceptual weakness",
      "offset": 4646.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like you should have had enough data to",
      "offset": 4648.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "understand what the three in row was and",
      "offset": 4650,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so it suggested other moves didn't",
      "offset": 4651.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "recognize it. It's a pretty bad failure.",
      "offset": 4653.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "And then we played again after I",
      "offset": 4655.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "corrected it. You know how you get these",
      "offset": 4657.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "sickopantic answers? I'm sorry, you",
      "offset": 4658.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "know. So so we played again and it lost",
      "offset": 4660.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to me again and then a third time. Um I",
      "offset": 4662.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "posted all of this on Twitter a couple",
      "offset": 4665.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "weeks ago. Um, so if you move these",
      "offset": 4666.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "things off the typical problem, they're",
      "offset": 4669.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "even worse. If they had a robust",
      "offset": 4671.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "understanding of a domain like",
      "offset": 4674.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tic-tac-toe or chess, it wouldn't be too",
      "offset": 4675.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "hard. I posted another on Twitter which",
      "offset": 4677.679,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "was like I had some variation on chess",
      "offset": 4680.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like um give me a a board state where",
      "offset": 4682.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "there are three queens on white side but",
      "offset": 4685.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "black um can immediately win and",
      "offset": 4688.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "performance was not great. So like",
      "offset": 4691.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you've you take knowledge that for an",
      "offset": 4693.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "expert would be out of what they've",
      "offset": 4696.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "specifically practiced but they",
      "offset": 4698.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "understand the domain well enough and",
      "offset": 4700.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "experts will have no trouble at all and",
      "offset": 4702.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these systems still do. So I I totally",
      "offset": 4704.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "agree with those limitations about",
      "offset": 4706.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "current systems but the thing that I",
      "offset": 4707.679,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "would say is that it seems like the",
      "offset": 4708.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "trends are you know going up. So I would",
      "offset": 4710.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "predict that if you measured this sort",
      "offset": 4713.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of thing for the last five years, even",
      "offset": 4715.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "though the current systems might still",
      "offset": 4718.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "have weaknesses, they would be less weak",
      "offset": 4719.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "than the systems of two years ago, which",
      "offset": 4721.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "are less weak.\n Well, chess is an example",
      "offset": 4722.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and then I'm going to go to you. I was",
      "offset": 4724.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "just say last thing, which is I first",
      "offset": 4726,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "noted the problem with chess, I think",
      "offset": 4727.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "two years ago, the illegal move problem.",
      "offset": 4730.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "It really I think Matthew Akre was first",
      "offset": 4733.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "to um Arer was first first to point it",
      "offset": 4735.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "out and I spread it on Twitter and said,",
      "offset": 4738.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "&quot;Look, this is a serious problem.&quot; and",
      "offset": 4740.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "it persists. There are a lot of problems",
      "offset": 4741.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that I feel like have persisted but your",
      "offset": 4743.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "turn. You've been fine.\n Yeah. Yeah.\n Uh",
      "offset": 4744.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "one one thing is on benchmark based",
      "offset": 4746.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "forecasting and I think that has",
      "offset": 4749.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "limitations in particular street light",
      "offset": 4751.199,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "effect where well when it gets 100% that",
      "offset": 4753.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "will suggest that it has the task or",
      "offset": 4756.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that suggests it's it's solved the task.",
      "offset": 4758.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "The issue is that it they often have",
      "offset": 4760.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "some structural defects that are only",
      "offset": 4763.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "obvious later in the uh or when you go",
      "offset": 4764.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "far fairly far out into the curve. So",
      "offset": 4767.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "for instance in video understanding you",
      "offset": 4770.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "think wow if you looked at the",
      "offset": 4773.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "benchmarks from a few years ago they're",
      "offset": 4774.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "they're totally at the top of them but",
      "offset": 4776.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "then you can come up a year later with",
      "offset": 4777.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "several other sorts of benchmarks that",
      "offset": 4780.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "uh uh challenge them. So I I I I think",
      "offset": 4782.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that there tends to be a gravitation",
      "offset": 4785.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "toward what's very tractable for AI",
      "offset": 4787.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "systems and where some interesting",
      "offset": 4789.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "action is happening. That's a selection",
      "offset": 4791.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "pressure on the sort of benchmarks. If",
      "offset": 4793.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "one's looking at cognitive tasks such as",
      "offset": 4795.679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "those that you would give kids if you're",
      "offset": 4798.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "testing their intelligence, for",
      "offset": 4800.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "instance, if you randomly sample many of",
      "offset": 4802,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "those, the the models don't do that",
      "offset": 4804.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "well. Um uh maybe on you could it'd be a",
      "offset": 4806.56,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "double-digit percentage, maybe be almost",
      "offset": 4810.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "on half of them, they don't do that",
      "offset": 4812.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "well. For instance, count the number of",
      "offset": 4814.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "faces in this photograph. 03 can't do",
      "offset": 4816.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that very well. Uh or connect the dots",
      "offset": 4818.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "or fill in fill in the colors in this in",
      "offset": 4820.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "this picture. uh just an example for for",
      "offset": 4822.88,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "visual um ability. So uh there's um I I",
      "offset": 4825.199,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "don't think when Gary's pointing these",
      "offset": 4830.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "out, it's it's just he's just running",
      "offset": 4832.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the cherry-picking program and what he's",
      "offset": 4834.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "going to do is he's going to keep just",
      "offset": 4836.64,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "he's just going to keep cherrypicking",
      "offset": 4837.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and doing god of the gaps thing um for",
      "offset": 4838.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "AI until basically is is AGI. Uh I I I",
      "offset": 4841.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "do think that there is a non-adversarial",
      "offset": 4845.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "distribution a different sort from the",
      "offset": 4846.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "cognitive science angle. uh you would",
      "offset": 4848.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "see many of these these um uh sorts of",
      "offset": 4850.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "issues. Um and I I think that there's",
      "offset": 4853.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "potentially some speaking past each",
      "offset": 4856.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "other in part because he's not viewing",
      "offset": 4858.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "intelligence as uh some undidnimensional",
      "offset": 4861.6,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "thing entirely. There's can uh a lot",
      "offset": 4864.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "might be correlated together, but there",
      "offset": 4867.679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "are various other uh mental faculties",
      "offset": 4869.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "that are important that aren't um that",
      "offset": 4872.719,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "aren't uh really online that much. uh um",
      "offset": 4875.679,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "we we saw with GPT um with the GPT",
      "offset": 4879.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "series that by pre-training on a lot of",
      "offset": 4882.719,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "uh text we got some subcomponents of",
      "offset": 4885.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "intelligence we got reading writing",
      "offset": 4888.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "ability and we got a lot of crystallized",
      "offset": 4889.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "intelligence or acquired knowledge from",
      "offset": 4892.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "that um uh and that process took several",
      "offset": 4893.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "years um so it's it's not the case",
      "offset": 4897.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "there's often people will sort of point",
      "offset": 4900.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "out once it gets traction on it then it",
      "offset": 4901.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "will solve it immediately or solve it",
      "offset": 4903.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "very shortly thereafter a lot for a lot",
      "offset": 4904.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of these core cognitive abilities. They",
      "offset": 4906.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "took multiple years. Mathematical",
      "offset": 4908.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "reasoning would be a recent example with",
      "offset": 4910.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Manurva. Google's Manurva system got 50%",
      "offset": 4912.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "on the math benchmark. Some benchmarking",
      "offset": 4915.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "I made some a while ago. Um uh in 2022",
      "offset": 4917.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "um and I think we've only recently",
      "offset": 4921.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "crushed it in 2025. So it took a good",
      "offset": 4924.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "three years and I think the reading",
      "offset": 4926.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "writing ability took um got to an",
      "offset": 4928.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "interesting state and now relatively",
      "offset": 4930.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "complete uh four years later. I think",
      "offset": 4933.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "crystallized intelligence as well is now",
      "offset": 4935.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "relatively complete. However, there's",
      "offset": 4937.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "still a variety of others. I wouldn't",
      "offset": 4939.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "expect visual processing ability",
      "offset": 4941.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "including for video to be taken care of",
      "offset": 4943.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in a year or two. Maybe audio audio",
      "offset": 4945.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "ability would be taken care of in a year",
      "offset": 4948.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "or two though. Uh we don't see almost",
      "offset": 4950.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "any progress on long-term memory. um",
      "offset": 4953.04,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "basically uh I think that uh they can't",
      "offset": 4956.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "really that meaningfully maintain um a",
      "offset": 4958.719,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "state across long periods of times over",
      "offset": 4962.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "complex interactions. Uh and so once we",
      "offset": 4964.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "start to get traction on that then we",
      "offset": 4968.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "can start to forecast that out. The",
      "offset": 4969.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "reason this is um and and for fluid",
      "offset": 4971.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "intelligence which is what we see in the",
      "offset": 4974.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "ARGI thing and Raven's progressive",
      "offset": 4975.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "matrices test that still seems very",
      "offset": 4977.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "deficient as well. So I think when",
      "offset": 4979.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "thinking about these it's it's it's",
      "offset": 4981.6,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "important to um split up the uh um uh uh",
      "offset": 4983.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "one's one's notion of cognition because",
      "offset": 4988.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "if there is a severe limitation on any",
      "offset": 4990.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of these dimensions then the models will",
      "offset": 4992.719,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "be uh fairly defective economically or",
      "offset": 4996,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "at least for many tasks. For instance,",
      "offset": 4998.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "if a person doesn't have good long-term",
      "offset": 5000.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "memory, it will be very diffic diff",
      "offset": 5002.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "difficult to inculturate them and and",
      "offset": 5004.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "teach them how to be productive in a in",
      "offset": 5006.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "a working environment and load all that",
      "offset": 5008.719,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "context. Um, if they uh have very slow",
      "offset": 5010.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "reaction time, that's also a problem. Or",
      "offset": 5014.639,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "if they have low fluid intelligence,",
      "offset": 5016.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "that will limit their ability to",
      "offset": 5017.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "generalize substantially. So, um, I",
      "offset": 5018.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "think the number is the numbers are",
      "offset": 5020.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "going up substantially or continually on",
      "offset": 5022.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "many of these axes um for these",
      "offset": 5025.199,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "benchmarks. Uh so but at the same time",
      "offset": 5027.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "when those benchmarks are at their",
      "offset": 5031.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "conclusion there might be another peak",
      "offset": 5032.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "for those and there's many of these and",
      "offset": 5034.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for it to be an AGI you're going to need",
      "offset": 5036.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "all of them as well and for some of them",
      "offset": 5038,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "they're fairly early on in their uh uh",
      "offset": 5040.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "capabilities such as with uh long-term",
      "offset": 5043.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "memory. Uh so I I think um if one's",
      "offset": 5045.679,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "doing forecasting I think understanding",
      "offset": 5049.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "intelligence somewhat more and having a",
      "offset": 5052.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "uh uh um uh a more sophisticated account",
      "offset": 5055.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "such as one would find in cognitive",
      "offset": 5058.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "science um can help um foresee",
      "offset": 5060.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "bottlenecks that will actually be fairly",
      "offset": 5063.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "action relevant.\n I mean I completely",
      "offset": 5064.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "agree with you and I'll just mention uh",
      "offset": 5067.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "physical intelligence and visual",
      "offset": 5069.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "intelligence. You you mentioned visual",
      "offset": 5071.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "we left out I skip physical physical",
      "offset": 5073.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "physical and spatial intelligence are",
      "offset": 5075.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "really very serious limits right now. So",
      "offset": 5077.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "if you ask 03 to label a diagram for",
      "offset": 5078.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "example it would be quite poor at it. If",
      "offset": 5081.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you ask it to reason about an",
      "offset": 5082.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "environment it's going to be poor\n amaz",
      "offset": 5084.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "So I mean I agree with like you made it",
      "offset": 5087.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "sound like I think intelligence is a",
      "offset": 5089.679,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "single dimension. I don't I don't know",
      "offset": 5091.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "where you got that impression. Um I",
      "offset": 5092.639,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "agree that there's all these",
      "offset": 5094.4,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "limitations.\n Well you said reliability",
      "offset": 5095.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "in all things considered type of sense.",
      "offset": 5096.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "I think your forecasts don't make a lot",
      "offset": 5098.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of contact with the kind of stuff that",
      "offset": 5101.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the two of us just talked about. This",
      "offset": 5102.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "became two two against one in a way that",
      "offset": 5104.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I entirely did not\n bring it on.\n I can",
      "offset": 5106.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "usually a bridging type.\n But anyway, um",
      "offset": 5108.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I I understand that like this is why we",
      "offset": 5111.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "call it the benchmarks plus gaps is that",
      "offset": 5113.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we understand that just because\n explain",
      "offset": 5115.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "that phrase. Can you just explain",
      "offset": 5117.12,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "benchmarks plus\n benchmark? So yeah, the",
      "offset": 5118.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "benchmarks is you take all the",
      "offset": 5119.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "benchmarks that you like and you",
      "offset": 5121.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "extrapolate them and you try to see when",
      "offset": 5122.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "they saturate.\n Mh. And then the gaps is",
      "offset": 5124.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "thinking about all the stuff that you",
      "offset": 5127.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "just mentioned and thinking about how",
      "offset": 5128.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you know just because you've knocked",
      "offset": 5130.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "down these benchmarks doesn't mean that",
      "offset": 5131.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you've already reached AGI. There's all",
      "offset": 5133.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "this other stuff that the benchmarks",
      "offset": 5135.04,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "might not be measuring. You know you",
      "offset": 5136.08,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "have to sort of try to reason about",
      "offset": 5137.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that.\n Another way of putting my argument",
      "offset": 5138.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "is I identified a set of gaps in 2001",
      "offset": 5140.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "rightly or wrongly but I identified",
      "offset": 5144.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "them. I think that they were real in",
      "offset": 5145.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "2001. I mean those were applying to",
      "offset": 5147.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "multi-layer perceptions not to",
      "offset": 5150.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "transformers which hadn't been invented",
      "offset": 5151.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "yet. But I still see exactly the same",
      "offset": 5153.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "gaps and I see some quantitative",
      "offset": 5155.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "improvement but no principled solution",
      "offset": 5158.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "to any of the gaps. So there were three",
      "offset": 5160.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "chap core chapters in that book. One of",
      "offset": 5162.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "them was about operations over",
      "offset": 5164.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "variables. One was about structured",
      "offset": 5166.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "representations and one was about types",
      "offset": 5167.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and tokens or kinds and individuals. And",
      "offset": 5169.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I just don't see that the things that I",
      "offset": 5172.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "described then have been solved. I see",
      "offset": 5174.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "certain cases where they can be solved",
      "offset": 5176.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "but all of the problems that I see seem",
      "offset": 5178.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to be reflexes of those same core",
      "offset": 5180,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "problems. And so if I seem like a grumpy",
      "offset": 5182.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "old man, it's because for 20ome years,",
      "offset": 5184.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and really I pointed these things out in",
      "offset": 5186.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "1998. 27 years, I have not really seen",
      "offset": 5187.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "them. And so the notion that we're going",
      "offset": 5191.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to solve those gaps all in three years",
      "offset": 5192.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "seems weird. Like I thought your",
      "offset": 5195.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "estimates of solving a few of the things",
      "offset": 5197.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you just mentioned were generous. You",
      "offset": 5199.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "say, you know, we solve reading and",
      "offset": 5201.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "writing in three years.\n Video is much",
      "offset": 5202.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "more certain.\n Video but but let me",
      "offset": 5204.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "finish the sentence. Um but the kinds of",
      "offset": 5207.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "numbers that you gave were like three or",
      "offset": 5209.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "four for this one, three or four for",
      "offset": 5211.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that one. And I think like really, you",
      "offset": 5213.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "know, my estimate is really 10 years is",
      "offset": 5217.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you most of the distribution is past 10",
      "offset": 5219.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "years. It's because I see several",
      "offset": 5221.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "problems that like we'd be really lucky",
      "offset": 5223.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "if we solved, let's say, the visual",
      "offset": 5225.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "intelligence problems in three or four",
      "offset": 5227.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "years. We'd be really lucky if we solved",
      "offset": 5229.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the um the video kind of visual",
      "offset": 5231.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "intelligence over time, let's call it,",
      "offset": 5233.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "problems in three or four years. I just",
      "offset": 5235.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "see too many problems, too many gaps to",
      "offset": 5236.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "use your phrase to think we'd get all of",
      "offset": 5238.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "those at once. Like it's in the movie",
      "offset": 5241.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Everywhere every what is it? Everything",
      "offset": 5243.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "everywhere all at once. To me, to get to",
      "offset": 5244.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "2027 would be everything everywhere all",
      "offset": 5247.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "at once for a set of things that I've",
      "offset": 5249.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "been worried about for 25 years.\n So, so,",
      "offset": 5251.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "uh, excellent. First of all, yeah, fun",
      "offset": 5255.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "point. You were, it sounded like you",
      "offset": 5257.92,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "were disagreeing with me, but then you",
      "offset": 5259.04,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "were saying like, yeah, three years,",
      "offset": 5260.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "three to\n in terms of the in terms of the",
      "offset": 5261.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the overall basically",
      "offset": 5263.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "There's there's maybe a caricature which",
      "offset": 5266.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "maybe isn't exa exactly you but the",
      "offset": 5268.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "caric caricature would be things will",
      "offset": 5270,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "keep scaling that will automatically",
      "offset": 5272.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "solve the problems. That's what's been",
      "offset": 5274,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "happening. You point out an issue.\n It's",
      "offset": 5275.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "the word automatically that I bustle",
      "offset": 5276.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "out.\n Um\n well I'm just interested in the",
      "offset": 5278.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "bottom line numbers for the years. I",
      "offset": 5280.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like like we had this potential dispute",
      "offset": 5282.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "about whether it's just LMS or whether",
      "offset": 5285.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "it's neurosymbolic. I don't want to",
      "offset": 5286.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "fight you about that. Like we can say",
      "offset": 5288.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "it's neurosymbolic if you like. We can",
      "offset": 5289.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "say it's not automatic but rather",
      "offset": 5291.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "involves some new ideas. But the point",
      "offset": 5292.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "is I think it's going to happen in like",
      "offset": 5294,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "three or four years.\n Yeah. So, okay,",
      "offset": 5295.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "great. So, so I just think new ideas are",
      "offset": 5297.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "hard to project. I'll give you my",
      "offset": 5300.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "favorite example is in the early 20th",
      "offset": 5302,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "century, everybody thought genes were",
      "offset": 5304.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "made of proteins. Somebody even won a",
      "offset": 5305.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "Nobel Prize on that premise. They were",
      "offset": 5307.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "all wrong. And it took a while for",
      "offset": 5309.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "people to figure out that this is wrong.",
      "offset": 5311.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "And Oswald Avery did these experiments",
      "offset": 5312.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "in the 1940s which really ruled it out",
      "offset": 5314.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "and you know discovered that by process",
      "offset": 5317.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "of elimination that it was DNA. And it",
      "offset": 5319.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "didn't take that long to move very fast",
      "offset": 5321.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "in molecular biology once people got rid",
      "offset": 5324.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "of the bad assumption. I think we're",
      "offset": 5326.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "making some bad assumptions, but it's",
      "offset": 5328.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "hard to predict when people will move",
      "offset": 5330.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "past an EDA fix, which is where I think",
      "offset": 5332.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we are now. I mean, proving that is very",
      "offset": 5334.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "hard. I can give a lot of, you know,",
      "offset": 5336.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "qualitative evidence that point in that",
      "offset": 5338.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "direction, but there's at least a",
      "offset": 5339.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "possibility that I'm right about that.",
      "offset": 5341.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "If we don't have the right set of ideas,",
      "offset": 5343.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it depends on when we come up with new",
      "offset": 5345.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "ideas. It could be that we automate the",
      "offset": 5347.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "discovery of new ideas, but most of what",
      "offset": 5348.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "LLM have done is not discovery of new",
      "offset": 5350.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "ideas. It's really exploiting existing",
      "offset": 5353.12,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "ideas.\n So I I I would claim that uh uh",
      "offset": 5355.76,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "the timelines there are many paths to it",
      "offset": 5360.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "being shorter before 2030 for instance.",
      "offset": 5363.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "But the the picture that you point out",
      "offset": 5366.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of there are these various other",
      "offset": 5368.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "cognitive abilities that are long",
      "offset": 5369.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "unressed and this isn't a cherrypicked",
      "offset": 5371.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "distribution. This is actually um uh um",
      "offset": 5373.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "what you would do if you're um inspired",
      "offset": 5376.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "by cognitive science or psychometrics.",
      "offset": 5379.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Uh I think basically both positions are",
      "offset": 5381.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "legitimate, but I think if you'd",
      "offset": 5384.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "integrate them, there could be a bullish",
      "offset": 5385.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "case that you'll be able to knock off",
      "offset": 5387.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "some of those core um uh core uh",
      "offset": 5389.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "cognitive uh abilities uh by the end of",
      "offset": 5392.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the decade. It's not totally certain.",
      "offset": 5395.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "It's funny you say it. I think that the",
      "offset": 5397.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "absolute most bullish, if we want to use",
      "offset": 5399.04,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "that word, um, case is 2030. Like in the",
      "offset": 5400.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "very fastest case, it'd be 2030. Would",
      "offset": 5404.639,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "require solving multiple problems that I",
      "offset": 5407.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "think we're just not positioned quite",
      "offset": 5410.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "yet. I don't think is very likely, but I",
      "offset": 5412,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "think that that's the maximum possible.",
      "offset": 5414.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I just can't get my mind around 2027. It",
      "offset": 5416.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "just does not seem plausible because of",
      "offset": 5419.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the number problems. And I think also",
      "offset": 5421.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "what you said is right. we want to",
      "offset": 5423.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "integrate these two approaches to make",
      "offset": 5424.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the the forecast and nobody's really",
      "offset": 5426.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "quite done that. Maybe that's a a um",
      "offset": 5428.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "adversarial collaborative uh",
      "offset": 5431.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "collaboration that that we could do. I",
      "offset": 5433.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "haven't really seen uh something dig",
      "offset": 5435.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "into that side of it of sort of new",
      "offset": 5437.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "discoveries. Maybe your gaps does it a",
      "offset": 5439.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "little bit.\n So let's talk about the",
      "offset": 5442,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "superhuman coder milestone and let's\n the",
      "offset": 5443.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "superhuman coder milestone, the",
      "offset": 5446.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "automating encoding aspects which is not",
      "offset": 5447.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "all of a R&amp;D but just part of it. Um, I",
      "offset": 5449.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "think the I I'm excited to try to drill",
      "offset": 5452.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "down into that and for us to try to make",
      "offset": 5455.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "bets or predictions of like what the",
      "offset": 5457.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "next few years are going to look like",
      "offset": 5460.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "basically. So from from my perspective,",
      "offset": 5461.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it feels like",
      "offset": 5463.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "you have often over the last couple",
      "offset": 5465.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "years been pointing to limitations of",
      "offset": 5467.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "LLMs that were then like overcome in the",
      "offset": 5471.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "next year or two\n which I mean there are",
      "offset": 5474,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "specific examples.\n Yes.\n Right. So, so",
      "offset": 5476.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like for example particular well chess",
      "offset": 5479.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hasn't been sol. So let me clarify what",
      "offset": 5481.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I mean by specific examples and I'll",
      "offset": 5483.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "come back to you which is I I give you",
      "offset": 5485.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "know on Twitter here here's a sentence",
      "offset": 5487.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "where you know here's a prompt and it",
      "offset": 5490.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "gets a weird answer. Those are always",
      "offset": 5491.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "remedied. I was told I don't know if",
      "offset": 5494.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "it's true that that people in open AI",
      "offset": 5496.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "actually look on social media for",
      "offset": 5498.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "examples including mine um and you know",
      "offset": 5500.159,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "people patch them up. Uh the specific",
      "offset": 5503.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "examples in that sense like literally",
      "offset": 5506.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "this prompt gets this weird answer. A",
      "offset": 5508.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "lot of them have been solved. These more",
      "offset": 5510.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "abstract things like playing chess",
      "offset": 5512.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "actually have not been solved. And the",
      "offset": 5513.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "more abstract problem of can I give you",
      "offset": 5515.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "a game with variations on the rules. I",
      "offset": 5517.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "talked about that in rebooting AI in",
      "offset": 5520.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "2019 in the first chapter. Um that",
      "offset": 5522.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "hasn't been solved. Are you then saying",
      "offset": 5525.04,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "that like for all you know six months",
      "offset": 5527.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "from now the AIS will be able to solve",
      "offset": 5530.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the the legal move in chess thing or are",
      "offset": 5533.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you saying\n I don't think LLMs will solve",
      "offset": 5535.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I mean I think someone could come up",
      "offset": 5537.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "with a scheme to get LLMs to uh to play",
      "offset": 5538.639,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "chess without making illegal moves by",
      "offset": 5543.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "calling a tool. I think that's possible",
      "offset": 5545.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "like like\n without calling a tool I don't",
      "offset": 5547.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think it'll be possible in the next six",
      "offset": 5550.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "months. I'd be very surprised. Um and",
      "offset": 5551.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "you can ridicule me in six months if it",
      "offset": 5553.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "happens. Um and that's fine. Um but also",
      "offset": 5555.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "let me add a twist to it which is what",
      "offset": 5557.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "we talked about in rebooting AI was like",
      "offset": 5559.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "for example if you train a particular",
      "offset": 5561.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "system on I think we use go as an",
      "offset": 5562.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "example but if you train it on a 15 by",
      "offset": 5564.239,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "what is go is it uh sorry 21x 21 if you",
      "offset": 5566.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "train on ordinary 21x 21 board will be",
      "offset": 5569.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "able to play on a rectangle instead of a",
      "offset": 5573.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "square will you know different",
      "offset": 5575.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "variations. I had a conversation with a",
      "offset": 5576.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "guy at DeepMind who did chess in a",
      "offset": 5578.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "really interesting way without using",
      "offset": 5581.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Monte Carlo tree simulation which is",
      "offset": 5582.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "what um the alpha zeros and so forth do",
      "offset": 5584.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and he trained it on an 8 by8 board and",
      "offset": 5587.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I said if you just had to do 7 by seven",
      "offset": 5589.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "would it work and he said no right so",
      "offset": 5592.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you know he did a version where there",
      "offset": 5594.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "was no tree search so it's not",
      "offset": 5596.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "neurosymbolic the tree search is one of",
      "offset": 5598.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the symbol processes or at least is much",
      "offset": 5600.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "less neurosyic and you know",
      "offset": 5602.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "astonishingly it played pretty well",
      "offset": 5604.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "trained on you know billion games or",
      "offset": 5606.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "something like that. Um but even then it",
      "offset": 5608.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "was sterile knowledge in the sense that",
      "offset": 5610.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it was purposeuilt for this but it",
      "offset": 5612.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "wasn't general knowledge of chess. If",
      "offset": 5614.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you asked it to do my put three queens",
      "offset": 5616.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "on a board and but still three white",
      "offset": 5618.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "queens but make sure that black wins on",
      "offset": 5621.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the next move. It wouldn't be able to do",
      "offset": 5623.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that at all. Right? the the there's a",
      "offset": 5624.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "flexibility to human expert knowledge",
      "offset": 5626.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that we emphasized in rebooting AI that",
      "offset": 5628.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "I see no evidence of of prog or maybe a",
      "offset": 5631.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "little evidence of progress but you know",
      "offset": 5634.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "relatively little progress on that my",
      "offset": 5636.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tic-tac-toe example is just like that\n so",
      "offset": 5638.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "can you give me more things of can you",
      "offset": 5640.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "give me more examples of things that a",
      "offset": 5642.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "year from now I won't be able to get an",
      "offset": 5643.679,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "off-the-shelf model to do without giving",
      "offset": 5646.239,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "it tools\n I I think distri I'll put it",
      "offset": 5648.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this way there'll be many many examples",
      "offset": 5651.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I can come up with distribution ution",
      "offset": 5654,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "shift all of what I was just doing.",
      "offset": 5655.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "We'll still find failures on uh next",
      "offset": 5657.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "year. I mean\n you'll be able to come up",
      "offset": 5660,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "with new examples but you don't you",
      "offset": 5661.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "can't come up with any example now that",
      "offset": 5663.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "will still be true a year from now. If I",
      "offset": 5664.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "make it in a slightly more generic",
      "offset": 5666.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "fashion, then yes, I I am sure that I",
      "offset": 5667.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "will become able to come up with",
      "offset": 5670.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "variations on chess um that are not",
      "offset": 5671.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "orthodox variations, but maybe there's",
      "offset": 5675.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "some already known um chess actually has",
      "offset": 5677.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "lots of variations um or chess problems",
      "offset": 5680.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and things like that that these systems",
      "offset": 5683.92,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "won't be able to do. I'll probably be",
      "offset": 5685.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "able to come up with variations on",
      "offset": 5686.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "tic-tac-toe. So 5x5 board, but you can",
      "offset": 5688,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "only win on the edges. There will be",
      "offset": 5690.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tons of problems like that. They're all",
      "offset": 5692.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "kind of outlier-ish. Um, but I think",
      "offset": 5694.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that a year from now, these systems will",
      "offset": 5697.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "still be very vulnerable to those",
      "offset": 5699.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "outliers.\n I think that you'll probably",
      "offset": 5701.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "still be able to come up with some",
      "offset": 5704.719,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "things like this a year from now, but",
      "offset": 5705.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you'll it'll be harder and then it'll be",
      "offset": 5707.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "even harder the next year and so forth.",
      "offset": 5709.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Um, we'll\n it should be get monotonic, of",
      "offset": 5711.52,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "course, but\n I mean, here's here's",
      "offset": 5714.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "another way to put it is the AGI that I",
      "offset": 5717.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "think we're afraid might be",
      "offset": 5720.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "unconstrained or whatever. Um there",
      "offset": 5722.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "really shouldn't be a lot of edge cases",
      "offset": 5724.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like that.\n Yeah.\n Right. I mean it\n that's",
      "offset": 5726.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "why I focus on the superhuman coder",
      "offset": 5729.36,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "milestone. Like I I don't again I don't",
      "offset": 5730.56,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "think we just leap straight.\n Do you know",
      "offset": 5731.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "about the live code pro benchmark that",
      "offset": 5732.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just came out?\n Uh not much. Tell me",
      "offset": 5735.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "about it.\n So So I don't know what the",
      "offset": 5736.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "human data on it but I know that the",
      "offset": 5738.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "machine data on it was 0%. And what was",
      "offset": 5740.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "interesting about the Live Code Pro",
      "offset": 5743.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "benchmark, Live Code Bench Pro, excuse",
      "offset": 5745.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "me, um was that they were all brand new",
      "offset": 5748.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "problems and so they ruled out data",
      "offset": 5751.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "contamination. And I've seen two careful",
      "offset": 5753.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "studies of ruling out data",
      "offset": 5755.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "contamination. The other was with the US",
      "offset": 5756.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "AMO um US math Olympiad problems six",
      "offset": 5759.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "hours after testing. Performance was",
      "offset": 5762.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "terrible on them. Um, I think if you",
      "offset": 5764.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "rule out data contamination, the",
      "offset": 5766.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "performance on",
      "offset": 5768.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "problems that are new is not good. And",
      "offset": 5770.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "for programming, especially programming",
      "offset": 5772.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "AGI, that's super relevant. You know, if",
      "offset": 5774.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "you're using these things to code up a",
      "offset": 5776.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "website, that's not new. There's so many",
      "offset": 5779.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "examples. But if you're using it to do",
      "offset": 5781.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "something that's new, you get out of",
      "offset": 5783.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "distribution, there's still a lot of",
      "offset": 5785.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "problems.\n What about Frontier Math?",
      "offset": 5786.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Wasn't that also\n there's some",
      "offset": 5788.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "contamination issues there? Um, you",
      "offset": 5789.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "know, OpenAI had access to the problems.",
      "offset": 5792.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "They said they didn't train on it",
      "offset": 5794,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "though.\n I know they did. But what",
      "offset": 5795.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "augmentation did they did? What did they",
      "offset": 5797.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do that's relevant to those problems,",
      "offset": 5799.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right? Open AI is just not I mean you",
      "offset": 5800.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "can agree with me on this is not an",
      "offset": 5803.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "entirely forthcoming place about what",
      "offset": 5804.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "they did.\n Sure. But aren't there like",
      "offset": 5806.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "aren't doesn't like Claude and Gemini",
      "offset": 5808.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "now have like okay scores on Frontier",
      "offset": 5810.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Math? I haven't actually checked, but",
      "offset": 5812.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like\n I think they have okay, but",
      "offset": 5814,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "everybody is is teaching to the test",
      "offset": 5815.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "right now and they're all doing",
      "offset": 5818.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "augmentation. We don't know what the",
      "offset": 5820.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "augmentation is. The point is if you",
      "offset": 5821.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "have AGI, it's not just about teaching",
      "offset": 5823.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to the test anymore and you need to be",
      "offset": 5825.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "able to solve things that are original.",
      "offset": 5827.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I mean, we're lucky if AI never can do",
      "offset": 5829.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that because then it gives an Achilles",
      "offset": 5831.92,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "heels.\n The or I guess you're pointing at",
      "offset": 5833.6,
      "duration": 8.559
    },
    {
      "lang": "en",
      "text": "that uh superhuman coding may still be",
      "offset": 5838.48,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "around the corner if we um extrapolate",
      "offset": 5842.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "out the benchmarks.\n I mean, I also just",
      "offset": 5844.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "I do for the record, I disagree with you",
      "offset": 5846.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "guys. like I think that progress is",
      "offset": 5848.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "being made in this sort of um hard to",
      "offset": 5849.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "measure uh dimension that you're",
      "offset": 5852.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "pointing to. Uh it's hard to measure the",
      "offset": 5854.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the best ways we have to measure it are",
      "offset": 5857.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for you to come up with examples. Uh and",
      "offset": 5859.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "let me give you a parallel\n progress is",
      "offset": 5861.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "being made.\n Let me give you a parallel",
      "offset": 5864.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and then we'll come back to\n I mean sorry",
      "offset": 5865.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "let me just insert very quickly. Um\n the",
      "offset": 5868,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "parallel is to driverless cars. Um, in",
      "offset": 5870.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "driverless cars, we know there's been",
      "offset": 5873.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "progress made every year. Absolutely.",
      "offset": 5874.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "But the distribution shift problem still",
      "offset": 5876.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "remains. So, you know, even Whimo, who I",
      "offset": 5878.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "think is maybe ahead in this, like they",
      "offset": 5881.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "just announced New York City, but when",
      "offset": 5883.119,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "they go to New York City, they're going",
      "offset": 5884.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "to have a safety driver there. They're",
      "offset": 5885.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "going to have geo fencing and so forth.",
      "offset": 5886.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "The general form of driving solution",
      "offset": 5888.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "would be level five. There'd be no geo",
      "offset": 5890.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "fencing. You wouldn't need specialized",
      "offset": 5892.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "maps, etc. So, there's been progress for",
      "offset": 5893.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "40 years every year in driverless cars.",
      "offset": 5896.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "But the distribution shift problem is",
      "offset": 5899.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "really what is still hobbling that from",
      "offset": 5901.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "being a thing everywhere as opposed to a",
      "offset": 5904,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thing in San Francisco.\n Well, we're also",
      "offset": 5906,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "getting better at distribution like like",
      "offset": 5907.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we have gotten better",
      "offset": 5909.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to yesterday. It was, you know, it was",
      "offset": 5911.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "great, but I took it here in San",
      "offset": 5913.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Francisco.\n There's there's still the the",
      "offset": 5915.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "problem of if it's getting better,",
      "offset": 5917.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "there's a question of is it able to do",
      "offset": 5918.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "full automation, which is more the key",
      "offset": 5921.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "thing. And unfortunately for other",
      "offset": 5923.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "abilities that probably the ability has",
      "offset": 5926.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "most um or well it's two main abilities",
      "offset": 5928.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "would be crystallized intelligence",
      "offset": 5930.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "acquired knowledge and reading writing",
      "offset": 5932.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "ability. But even for reading writing",
      "offset": 5933.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "ability the models and when you ask them",
      "offset": 5935.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to write an essay if you um score them",
      "offset": 5937.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "on like the GRE score out of six they",
      "offset": 5940.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "get like 4.5 or so. They're not",
      "offset": 5942.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "particularly great writers and we can't",
      "offset": 5943.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you know you can't automate uh writers",
      "offset": 5946,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that well with um if if they're doing",
      "offset": 5947.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "somewhat complicated writing. you still",
      "offset": 5950.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "people still need to be doing that and",
      "offset": 5951.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that's somewhat surprising because",
      "offset": 5953.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "they've had so much data uh they've had",
      "offset": 5955.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "all the data in the world basically and",
      "offset": 5957.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "likewise for coding they have had GitHub",
      "offset": 5959.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "so that at least there's it potentially",
      "offset": 5961.76,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "um the case that as you extrapolate them",
      "offset": 5965.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "out they'll get a lot of the lowhanging",
      "offset": 5968.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "fruit but it crossing some sort of",
      "offset": 5969.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "threshold for doing more automation or",
      "offset": 5971.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "full automation that could actually",
      "offset": 5974.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "require resolving some of these other",
      "offset": 5976.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "bottlenecks that are um other other",
      "offset": 5978.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "cognitive abil bottlenecks that uh uh",
      "offset": 5980.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that that Gary's alluding to.\n I'm going",
      "offset": 5982.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to um put a hold on this discussion. I",
      "offset": 5985.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "think we did a good job. We're not going",
      "offset": 5988.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to convince each other, but we laid out",
      "offset": 5990,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the issues and that's great. I want to",
      "offset": 5991.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "ask at least one other question because",
      "offset": 5993.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we've somewhat limited time. Um do you",
      "offset": 5995.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "think that we've made any progress on",
      "offset": 5998.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "alignment? Do you think there's a",
      "offset": 6000.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "conceivable solution to technical",
      "offset": 6002.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "alignment? Are we close to it? Like",
      "offset": 6004.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "let's talk about the technical alignment",
      "offset": 6006.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "problem. And I'll just put one thing out",
      "offset": 6008.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there which is I think even though I",
      "offset": 6010.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "don't think there's been as much",
      "offset": 6012.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "progress as you do on capabilities,",
      "offset": 6013.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "there's obviously been some like there",
      "offset": 6016.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "there's no argument there. I would say",
      "offset": 6018.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that a lot of it is sort of",
      "offset": 6020.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "interpolation rather than extrapolation",
      "offset": 6021.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and we haven't solved the extrapolation",
      "offset": 6023.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "problem but interpolation we've made",
      "offset": 6024.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "huge progress mostly just in virtue of",
      "offset": 6026,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "having more data and more compute. But",
      "offset": 6028.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you know there's no question that new",
      "offset": 6030.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "systems are much better at interpolating",
      "offset": 6031.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "um than previous systems and that has",
      "offset": 6034,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "lots of practical consequences for",
      "offset": 6035.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "labor. for example, there's a whole",
      "offset": 6037.6,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "bunch of things that you can use these",
      "offset": 6038.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "systems to do that you couldn't use them",
      "offset": 6040.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "before that's already affecting labor",
      "offset": 6042.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "markets. Whereas my intuitive sense is",
      "offset": 6044.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "on alignment, all we have is maybe eight",
      "offset": 6047.119,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "uh human uh reinforcement learning um",
      "offset": 6050.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "helps a little bit. So that like if you",
      "offset": 6054.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "ask these systems the most obvious",
      "offset": 6056.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "question like how do I build a",
      "offset": 6058.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "biological weapon, they'll decline. And",
      "offset": 6060,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that's a little bit of progress, but",
      "offset": 6062,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like we all know that those are things",
      "offset": 6063.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "are easily jailbroken like my view and",
      "offset": 6065.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "you can agree or disagree or whatever is",
      "offset": 6068.639,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like in alignment systems still don't",
      "offset": 6071.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "really do what we want them to do.",
      "offset": 6073.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "There's still kind of sorcerers",
      "offset": 6075.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "apprentice style problems. There are",
      "offset": 6076.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "kind of problems of just like they don't",
      "offset": 6078.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "quite fit, they don't do what we want.",
      "offset": 6081.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "We have system prompts that say don't",
      "offset": 6083.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "produce copyrighted material and they",
      "offset": 6085.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "still do. Don't hallucinate, they still",
      "offset": 6087.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "do. like they you know they can",
      "offset": 6089.119,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "approximate what we ask for them but",
      "offset": 6091.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like they don't really do what we ask",
      "offset": 6093.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "them. I take I take even like don't make",
      "offset": 6095.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "illegal moves in chess to be a form of",
      "offset": 6098.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the alignment problem like a very simple",
      "offset": 6100.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "microcosm of the alignment problem and",
      "offset": 6102.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "they're still struggling with that.",
      "offset": 6104.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "That's my view. How about you guys? Why",
      "offset": 6105.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "don't we start with you because you said",
      "offset": 6107.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "this?\n Yeah. Um so there's some different",
      "offset": 6108.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "notions of alignment or alignability and",
      "offset": 6111.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "uh I I I would distinguish between",
      "offset": 6114.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "aligning proto um uh super intelligences",
      "offset": 6116.8,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "and aligning a sort of recursion that",
      "offset": 6120.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "gives rise to super intelligence. Those",
      "offset": 6123.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "are very qualitatively different. One is",
      "offset": 6125.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "more mo model level and one is more uh",
      "offset": 6126.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "process level. So I don't think you're",
      "offset": 6130.719,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "going to solve that process level um one",
      "offset": 6133.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of doing a recursion and fully",
      "offset": 6136.48,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "de-risking that anticipating all the",
      "offset": 6137.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "unknown unknowns and solving a wicked",
      "offset": 6139.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "problem in a nice a clean way beforehand",
      "offset": 6141.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "which points to the necessity for",
      "offset": 6143.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "resolving geopolitical competitive",
      "offset": 6146.639,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "pressures and giving them an out to to",
      "offset": 6148.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "proceed with a recursion more slowly or",
      "offset": 6151.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "substantially for stall that on the",
      "offset": 6153.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "protoi things I think that they follow",
      "offset": 6155.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the instru instructions fairly",
      "offset": 6158.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "reasonably. There are some other parts",
      "offset": 6161.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "of\n that scares the out of me like",
      "offset": 6163.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "fairly reasonably like I mean as you",
      "offset": 6165.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "know when these things are still in our",
      "offset": 6168.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "hands kind of that's okay but like if",
      "offset": 6170.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you have them guiding weapons or",
      "offset": 6172.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "something like that there there are many",
      "offset": 6173.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "circumstances where fairly reasonably is",
      "offset": 6175.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "not good enough.\n Yeah. No, I I agree.",
      "offset": 6177.76,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "They're definitely safety critical",
      "offset": 6179.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "domains for it. And uh I I think for for",
      "offset": 6180.639,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "instance in bio I think for refusal for",
      "offset": 6184.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "instance in in some high stakes context",
      "offset": 6187.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "or given some high stakes queries it it",
      "offset": 6189.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "depends. There are some cases where I",
      "offset": 6192.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "think you can actually get multiple",
      "offset": 6194.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "nines of reliability such as with",
      "offset": 6196.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "bioweapons refusal. However, for other",
      "offset": 6198.32,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "types of refusal like don't um cause any",
      "offset": 6201.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "criminal or don't take a criminal or",
      "offset": 6204.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "torchious action, that's a lot fuzzier",
      "offset": 6206.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and it's I don't think\n on the first one",
      "offset": 6209.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "though, can I rewind? You probably saw",
      "offset": 6211.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Adam Gleav's postings a couple weeks",
      "offset": 6213.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "ago. I can't remember who he was leaning",
      "offset": 6215.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "on, but he he described someone else's",
      "offset": 6216.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "work saying that there was a very easy",
      "offset": 6218.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "jailbreak. I think it was with Claude.",
      "offset": 6220.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Yeah. So in production they're not using",
      "offset": 6222.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "they're not using the techniques that",
      "offset": 6224.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "are as adversarily robust because they",
      "offset": 6225.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "come at a cost of maybe a percent or two",
      "offset": 6227.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "in MMLU and uh so um so they're not",
      "offset": 6230,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "doing\n the epetap for humanity if they",
      "offset": 6233.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "hadn't squeezed out that last bit of MLU",
      "offset": 6236.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we would have been okay. uh but but",
      "offset": 6238.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually so I think there are some types",
      "offset": 6240.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of there are some types of solutions for",
      "offset": 6242.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "um some types of malicious use where you",
      "offset": 6245.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "can have some nines of liability but for",
      "offset": 6247.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "for the general problem of of refusal um",
      "offset": 6249.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "including for everyday uh uh criminal",
      "offset": 6252.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and torches actions which would be",
      "offset": 6254.639,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "extremely relevant for for agents and",
      "offset": 6255.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "making them feasible I don't see",
      "offset": 6257.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "substantial progress I mean think about",
      "offset": 6259.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "asthma's law by the way which were",
      "offset": 6261.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "written what in the 40s or something",
      "offset": 6263.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like that and you know number one was",
      "offset": 6264.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "was don't let harm come to humans or",
      "offset": 6266.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "whatever Right. And and the cleaned up",
      "offset": 6268.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "version law is actually the law kind of",
      "offset": 6270.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "cleanses it up and saying don't cause",
      "offset": 6272.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "foreseeable harm because harm is way too",
      "offset": 6274.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "strict, but foreseeable harm is what we",
      "offset": 6276.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "demand of humans. And so\n that one we're",
      "offset": 6277.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "not doing that well.\n Yeah. Yeah. That",
      "offset": 6279.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "one we're not Yeah. Yeah. And so so I",
      "offset": 6280.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "think for most of these um uh",
      "offset": 6283.84,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "reliability issues and safety issues,",
      "offset": 6286.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we'll keep seeing new symptoms crop up",
      "offset": 6289.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and we'll have specific solutions that",
      "offset": 6292,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "partly target them if there's",
      "offset": 6294.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "willingness.\n Cyber crime, a kind of",
      "offset": 6295.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "constant cat.\n That's right. and and I I",
      "offset": 6297.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "I expect that um uh this game will keep",
      "offset": 6299.199,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "continuing um and we won't get to a",
      "offset": 6303.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "state where that is basically mostly",
      "offset": 6306.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "managed in time um because the risk",
      "offset": 6309.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "surface will keep evolving with agents",
      "offset": 6312.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "that'll present new things and we'll",
      "offset": 6313.76,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "have to deal with those current cases",
      "offset": 6314.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "that will create a substantial backlog",
      "offset": 6315.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and we just won't have the adaptive",
      "offset": 6317.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "capacity and so consequently on both",
      "offset": 6318.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "fronts for aligning recursion and",
      "offset": 6321.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "aligning protois",
      "offset": 6323.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "um uh we the geopolitical competitive",
      "offset": 6325.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "pressures make it such that we're",
      "offset": 6330,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "probably not going to solve either",
      "offset": 6331.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "problem.\n So this is dark and going back",
      "offset": 6334.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to the beginning of our conversation.",
      "offset": 6336.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "It's a reason to stand in front of the",
      "offset": 6338.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "train especially a particular train. So",
      "offset": 6339.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "let's say that there's one train that's",
      "offset": 6342.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "about chat bots and people having fun",
      "offset": 6344,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "with chat bots and using them for",
      "offset": 6345.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "brainstorming and whatever that are not",
      "offset": 6347.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "mission critical and safety critical",
      "offset": 6348.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "maybe it's fine we just let people do",
      "offset": 6350.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that and there are already some risks",
      "offset": 6352,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like around delusions that that Kashmir",
      "offset": 6353.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Hill wrote about in the New York Times",
      "offset": 6355.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the other day you probably saw um but",
      "offset": 6357.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the really safety critical things or",
      "offset": 6359.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "maximally safety critical things if if",
      "offset": 6361.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "things are as dark as you said that is a",
      "offset": 6363.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "reason to stand in front of that train",
      "offset": 6365.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "right now and say look if we can't do",
      "offset": 6366.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "alignment well on the refusal",
      "offset": 6369.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "etc. Um, and the cause to harm causing",
      "offset": 6372.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "harm to humans, foreseeable harm even",
      "offset": 6374.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "even to humans. That's a reason to say,",
      "offset": 6377.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "&quot;Hey, we got to wait until we have a",
      "offset": 6379.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "better solution here. If that takes 500",
      "offset": 6381.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "years, if it takes 5 years, like in the",
      "offset": 6384,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "safety critical stuff, isn't that a",
      "offset": 6386.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "reason to to, you know, slow things down",
      "offset": 6388.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "a bit?&quot;\n I think that that's that's a",
      "offset": 6390.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "direction or an interpretation of those",
      "offset": 6393.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "facts that that is reasonable. But",
      "offset": 6394.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "there's there's a question of incentives",
      "offset": 6398.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and what you know there might be a",
      "offset": 6399.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "somewhat different aspect better\n you",
      "offset": 6401.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "could go at it by incentives.\n Yeah.",
      "offset": 6403.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah. So so that that's why in in super",
      "offset": 6405.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "intelligence strategy will um suggest",
      "offset": 6407.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "some different things like u making the",
      "offset": 6409.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "threat of preeemption if you're crossing",
      "offset": 6411.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "these sorts of lines uh as opposed to um",
      "offset": 6413.36,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "stopping today because that is is a lot",
      "offset": 6417.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "harder to\n I'll weaken my claim and say",
      "offset": 6419.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "is that a reason to",
      "offset": 6421.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to intervene I think is the word.\n Yeah.",
      "offset": 6424.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "That would be a reason to get\n Yeah, of",
      "offset": 6426.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "course.\n Right.\n And and and yeah, like to",
      "offset": 6428.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "continue your your metaphor with",
      "offset": 6430.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "stopping the train, like right now, if I",
      "offset": 6432.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "were to step in front of the train, it",
      "offset": 6434.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "would just sort of like run me over. Uh",
      "offset": 6436.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and you need to have like quite a lot of",
      "offset": 6438.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "people in front of the train for the",
      "offset": 6440,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "weight of the bodies to like really slow",
      "offset": 6441.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it down. Um I've been kind of trying on",
      "offset": 6443.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Twitter and it's, you know, so\n there's",
      "offset": 6445.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "some push back when I try to hold the",
      "offset": 6448.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "train back.\n Yeah. Yeah. So I mean I'm",
      "offset": 6449.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "not I'm not actually advocating holding",
      "offset": 6451.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the train per se but on these safety",
      "offset": 6453.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "critical things I really could see an",
      "offset": 6455.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "argument for some kind of intervention",
      "offset": 6457.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "as as you note it could be um incentives",
      "offset": 6459.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "rather than than mortorium but my view",
      "offset": 6461.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "is that we are not going to solve the",
      "offset": 6464.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "technical alignment the sort of nearer",
      "offset": 6466.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "term technical alignment problem with",
      "offset": 6468.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "let me finish the sentence um with LLMs",
      "offset": 6470.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that they're just not up to the task",
      "offset": 6472.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "maybe with neurosymbolic we might have a",
      "offset": 6475.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "chance so neurosymbolic gives you a",
      "offset": 6477.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "chance to state explicit constraints",
      "offset": 6478.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that is very difficult in pure LLMs and",
      "offset": 6481.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so I think there's reason to explore",
      "offset": 6483.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that avenue as a way I don't know that",
      "offset": 6485.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it helps with super intelligence but at",
      "offset": 6487.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "least with the near-term use of these",
      "offset": 6489.199,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "systems for safety critical measures",
      "offset": 6491.6,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "there might be some some mileage to be",
      "offset": 6494.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "gotten there\n yeah one one uh last point",
      "offset": 6497.119,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "on on this for Daniel is uh I think in",
      "offset": 6499.679,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "both cases I I I think it's a problem to",
      "offset": 6504.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "use the phrase solve like that because",
      "offset": 6507.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it's not something you can sort of do",
      "offset": 6508.8,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "beforehand. In both cases, you want",
      "offset": 6510.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "adaptive capacity and slack in the",
      "offset": 6511.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "system, some sort of safety budget, some",
      "offset": 6513.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "people who can put out the fires faster",
      "offset": 6515.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "than they're emerging. And because as",
      "offset": 6517.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "AIs continue to develop, they'll keep",
      "offset": 6520.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "being new problems as they evolve. And",
      "offset": 6522.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "likewise with recursion, we just see",
      "offset": 6524.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that, but on on steroids,\n faster than",
      "offset": 6525.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "emerging.\n So with both of it's a",
      "offset": 6528.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "resource thing, and I think it's less of",
      "offset": 6530.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "a technical thing. the technical things",
      "offset": 6532,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "can increase the capacity to deal with",
      "offset": 6533.6,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "these problems or uh have more efficient",
      "offset": 6536.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "solutions for for some of these",
      "offset": 6539.119,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "particular symptoms um or these these",
      "offset": 6540.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "new failure modes that crop up. But I",
      "offset": 6543.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "I'm not expecting a total um monolithic",
      "offset": 6544.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "uh solution um uh that a pause would",
      "offset": 6548.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "necessarily give. I think you have to",
      "offset": 6551.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "have the the background context in both",
      "offset": 6553.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "cases be that you're able to proceed um",
      "offset": 6555.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "under some ris proceed with development",
      "offset": 6558.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "under some uh risk tolerance that's much",
      "offset": 6560.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "lower than what there is today.",
      "offset": 6562.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Uh",
      "offset": 6564.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "I mean I agree we're totally not on",
      "offset": 6566.719,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "track to have figured out the alignment",
      "offset": 6568.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "stuff in time. Uh I can pontificate more",
      "offset": 6569.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "on that but I've talked a lot so\n so I",
      "offset": 6572.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "mean we should probably actually wrap",
      "offset": 6574.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "up. So may maybe um some final words.",
      "offset": 6575.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I'll start with some final words. Maybe",
      "offset": 6578.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I'll make some very last words. Um, I",
      "offset": 6579.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "think we actually agreed on a lot here.",
      "offset": 6582.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "You know, our clearest disagreement is",
      "offset": 6584.88,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "on forecasting. Even there, we're not",
      "offset": 6587.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "probably as far apart as maybe people",
      "offset": 6591.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "thought that we were, right? So, you",
      "offset": 6593.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "know, I I push all of my probability",
      "offset": 6595.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "mass five years out and have, you know,",
      "offset": 6598.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "basically none before and you've got",
      "offset": 6601.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some at three years out or two years out",
      "offset": 6603.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that I don't. Um, we both have some out",
      "offset": 6605.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "at 2045. I have some even past then and",
      "offset": 6608.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you may or may not. Um we have slightly",
      "offset": 6611.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "different methodologies that we have",
      "offset": 6614.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "talked about. You were surprisingly",
      "offset": 6616,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "coming to my aid. Um which I love. Um",
      "offset": 6617.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "happy moment for me. Um but you know",
      "offset": 6620.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we're not hugely apart and I think we've",
      "offset": 6623.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "acknowledged the value in some of the",
      "offset": 6625.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "forecasting techniques that the other",
      "offset": 6627.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "has used even if we don't. Um so we're",
      "offset": 6628.8,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "not hugely apart there. I think we're",
      "offset": 6631.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "completely agreed that we're not doing a",
      "offset": 6633.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "great job on the alignment problem and",
      "offset": 6636.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that we need to do much better and that",
      "offset": 6638.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "there's a temporal dimension to that as",
      "offset": 6641.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you were just saying which is like you",
      "offset": 6643.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "know it's not great for humanity if we",
      "offset": 6645.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "solve that problem in 200 years and we",
      "offset": 6647.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "we have AGI or ASI you know in the next",
      "offset": 6650.159,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "decade or two. Um and I think we agree",
      "offset": 6653.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "also that the current companies are not",
      "offset": 6658,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "entirely trustworthy. Um those some of",
      "offset": 6660.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "those are some of the things that we",
      "offset": 6662.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "agree or don't disagree so much on like",
      "offset": 6664.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the broader picture will be remarkably",
      "offset": 6667.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "aligned.\n Yep. I would agree with that.",
      "offset": 6669.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Also, I just realized I forgot to ask",
      "offset": 6671.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you my big question about scenarios.\n Uh",
      "offset": 6672.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "do you still have time for a brief foray",
      "offset": 6675.52,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "into that?\n You guys can sit for a few",
      "offset": 6677.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "minutes. I'll I'll do that.\n So So here's",
      "offset": 6678.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the way I would pose the question. You",
      "offset": 6681.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "know, you've read AI 2027. Thank you for",
      "offset": 6682.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reading it. Um, if you were to write",
      "offset": 6684.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "your own scenario of the future, what",
      "offset": 6686.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "would it look like? And perhaps where",
      "offset": 6688.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "would it start to branch off for me at",
      "offset": 6690.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "2027, for example? Would it look",
      "offset": 6692,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "basically the same until 2027 or would",
      "offset": 6693.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it branch off earlier than that? And",
      "offset": 6696.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "when it does branch off, can you sort of",
      "offset": 6698.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like\n say what that looks like, you know?",
      "offset": 6700.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Yeah. So, there's a couple different",
      "offset": 6702.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "pieces. So, um, I think the the speed at",
      "offset": 6703.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "which things unfold in AI 2027 is not",
      "offset": 6707.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "plausible to me. I think by the end of",
      "offset": 6710.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the year we will already be less far",
      "offset": 6712,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "ahead on agents as as you hypothesize",
      "offset": 6713.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "there like I think the first couple",
      "offset": 6716.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "months we're actually kind of in",
      "offset": 6718.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "agreement but you know there's a",
      "offset": 6719.92,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "divergence in speed um for sure. Uh I",
      "offset": 6721.679,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "would say that\n so like take a 2027 and",
      "offset": 6726.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "then like double the length of",
      "offset": 6729.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "everything maybe or triple it or like",
      "offset": 6731.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "what would what would you say?\n Well I",
      "offset": 6733.119,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "mean",
      "offset": 6735.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "or at least Yeah. I think that the level",
      "offset": 6736.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "of intelligence that the m that you",
      "offset": 6739.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "attribute to the machines towards the",
      "offset": 6741.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "end of the essay remember this some of",
      "offset": 6743.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "that actually happens after the year",
      "offset": 6745.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "2027 I think um\n you know I don't really",
      "offset": 6746.719,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "think is very likely in the next decade",
      "offset": 6750.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and I don't think is super likely in the",
      "offset": 6753.679,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "decade after that but it's certainly",
      "offset": 6756.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "possible.\n What about the superhuman",
      "offset": 6757.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "coder milestone? Do you think that that",
      "offset": 6759.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "won't be happening in the next decade?",
      "offset": 6760.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Probably.\n I don't remember how you",
      "offset": 6762.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "define them milestone. Basically, think",
      "offset": 6763.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "about like these coding agents like",
      "offset": 6766.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Claude and so forth, but imagine that",
      "offset": 6768.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "they just like actually work to the",
      "offset": 6770.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "point where you can just treat them like",
      "offset": 6772.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a software engineer and chat with them",
      "offset": 6774.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and give them high level instructions",
      "offset": 6776.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and they'll just do as good a job as a",
      "offset": 6778.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "very professional excellent software",
      "offset": 6780.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "engineer would have done.\n So I think",
      "offset": 6782.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "apprentice engineer we're actually close",
      "offset": 6784.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "already.\n Well, I mean top I mean\n but top",
      "offset": 6786.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "software engineer I don't think we're",
      "offset": 6789.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "close. I I think that that requires an",
      "offset": 6791.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "understanding of the problem, what",
      "offset": 6794.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "humans want solved by the problem. It un",
      "offset": 6796.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "requires a deep understanding of various",
      "offset": 6799.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "domains. I just don't think that we're",
      "offset": 6801.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that close to that. So I I do think",
      "offset": 6803.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "these things will continue to improve",
      "offset": 6805.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "regularly. Um we will get more and more",
      "offset": 6807.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "value out of them. They will improve pro",
      "offset": 6809.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "programmer productivity with some",
      "offset": 6811.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "asterisks around how secure is the code,",
      "offset": 6814,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "how maintainable is the code, etc. Um",
      "offset": 6816,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but I don't think that they're going to",
      "offset": 6818.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "replace the best coders. You're not",
      "offset": 6819.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "going to get a machine that is Jeff Dean",
      "offset": 6821.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "anytime in the next decade. Like I will",
      "offset": 6824,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I will be really surprised if you get a",
      "offset": 6826,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Jeff Dean level coder, right? You",
      "offset": 6827.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "probably know, you know, some of the",
      "offset": 6829.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "stories that the Chuck Norris stories",
      "offset": 6831.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that aren't really true, but um, you",
      "offset": 6833.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "know, he was able to look at problems",
      "offset": 6835.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "that nobody had seen before about the",
      "offset": 6836.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "distribution of, you know, these",
      "offset": 6839.599,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "searches and coming up with",
      "offset": 6841.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "advertisements over, you know, enormous",
      "offset": 6843.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "scale that nobody had ever done before",
      "offset": 6845.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and pretty rapidly prototype and then",
      "offset": 6847.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "make, you know, maybe with some help",
      "offset": 6850.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "some production level solutions. Um, so",
      "offset": 6851.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "take Jeff Dean as kind of, you know, our",
      "offset": 6854.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "our example. He he deserves to be our",
      "offset": 6856.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "example in this. I don't see Jeff Dean",
      "offset": 6858.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "coming out of these systems soon. I just",
      "offset": 6860.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "don't.",
      "offset": 6862.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Um and then a little bit more on",
      "offset": 6865.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "scenario. So um I think that the human",
      "offset": 6866.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "mind sucks in light of scenarios that it",
      "offset": 6870.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "takes them very seriously. The vivid",
      "offset": 6872.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "details I mean there's lots of",
      "offset": 6874.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "psychological literature on this",
      "offset": 6875.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "overwhelm people's ability to see",
      "offset": 6877.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "things. What I would like to see",
      "offset": 6880.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually would be a distribution of",
      "offset": 6881.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "scenarios. So when you put Scott",
      "offset": 6883.36,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Alexander, who's a brilliant writer or",
      "offset": 6885.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "at least a very, you know, compelling",
      "offset": 6887.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "writer of a certain sort, um, into",
      "offset": 6889.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "making one scenario vivid, everybody",
      "offset": 6892.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "goes home and thinks that that scenario",
      "offset": 6894.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "is real. But you and I know that that",
      "offset": 6896.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "was one scenario of many. There's",
      "offset": 6898.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "reasons to consider that scenario. And",
      "offset": 6900.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it's sort of, you know, the darker one",
      "offset": 6902.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is is a very vivid version of the dark",
      "offset": 6903.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "scenario. Um, but really we want to",
      "offset": 6906.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "understand the distribution of scenario.",
      "offset": 6909.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "And that's a lot more work. I'm sure it",
      "offset": 6911.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "took you what a few person years or",
      "offset": 6913.679,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "something like that to put together that",
      "offset": 6916.08,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "report. There were multiple people",
      "offset": 6917.599,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "involved. You probably worked on it for",
      "offset": 6918.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "a while. And so it's a big ask, but what",
      "offset": 6920,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I would like to see is really a",
      "offset": 6921.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "distribution of scenarios.\n We're working",
      "offset": 6923.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "on it. I I agree with the problem you're",
      "offset": 6925.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "pointing out. Um, we we currently have a",
      "offset": 6926.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "project to make a a good ending, so to",
      "offset": 6929.199,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "speak, at a similar level of detail to",
      "offset": 6931.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "what we already have, and then also a",
      "offset": 6933.599,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "mini project to make a like more scrappy",
      "offset": 6935.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "spread of possible scenarios",
      "offset": 6938.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "illustrating different stuff at lower",
      "offset": 6940.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "levels of detail, like just like a a few",
      "offset": 6942.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "pages.\n I I think that that would be",
      "offset": 6943.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "helpful. My own personal scenario is",
      "offset": 6946,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "like in three or four years, neurosyic",
      "offset": 6948.88,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "AI starts to take off. like it's I",
      "offset": 6953.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "already see signs of this. Alpha Fold",
      "offset": 6955.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just won a Nobel Prize. That's a nice",
      "offset": 6957.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "thing for neurosymbolic AI. The",
      "offset": 6958.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "conferences for it are getting bigger",
      "offset": 6960.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and and and so forth. Um and I think",
      "offset": 6962.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "eventually there will be a state change.",
      "offset": 6965.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I find it very hard to know when there",
      "offset": 6966.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "will be a state change, but I think in",
      "offset": 6968.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "2035 we will look at LLMs and be like",
      "offset": 6970.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "nice try. We still use them for some",
      "offset": 6973.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "things, but that wasn't really the",
      "offset": 6976,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "answer. Um I think Yan Lun would say the",
      "offset": 6977.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "same thing. Again, you despite our",
      "offset": 6980,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "differences, I think we both think LOMs",
      "offset": 6981.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "are not really the route to AGI and that",
      "offset": 6984.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "when we get there, it's going to look",
      "offset": 6986.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "pretty different. It might use LOMs.",
      "offset": 6987.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "They're great at kind of distributional",
      "offset": 6990.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "learning. It might replace them because",
      "offset": 6991.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "they're very inefficient in terms of",
      "offset": 6993.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "energy and data. So, somebody might find",
      "offset": 6995.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a better way to do the same kind of",
      "offset": 6997.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "thing of of learning the models of",
      "offset": 7000.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "distributions of things, which is a",
      "offset": 7002.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "super helpful cognitive skill. It's not",
      "offset": 7004,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the only one, but it's super helpful.",
      "offset": 7005.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "But we'll have much better ways of doing",
      "offset": 7007.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "reasoning and planning. We'll have much",
      "offset": 7008.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "more stable world models. I think it",
      "offset": 7010.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "will take five or 10 years to develop",
      "offset": 7012.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that. I think the semantics that the",
      "offset": 7014.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "current models have is very superficial.",
      "offset": 7016.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "It's really about distributions of",
      "offset": 7018.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "words. Um, and we need a deeper one.",
      "offset": 7020,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Like if you talk about three in a row,",
      "offset": 7022.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you should understand what a three in a",
      "offset": 7023.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "row is. And I think we're missing",
      "offset": 7025.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "something to get that. We will get it.",
      "offset": 7026.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Like I don't think it's\n and you don't",
      "offset": 7028.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "think we'll get it after automating a",
      "offset": 7030.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "R&amp;D. We'll get it in like humans will",
      "offset": 7032.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "come up with the ideas that get us",
      "offset": 7034.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "there. I mean I guess it relates so for",
      "offset": 7036,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "me automating R&amp;D like there's a",
      "offset": 7038.719,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "plausible version and a much more distal",
      "offset": 7041.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "version.\n So the near-term version is",
      "offset": 7043.119,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like a lot of people do a lot of",
      "offset": 7045.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "experiments on LLMs and I think you can",
      "offset": 7046.639,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "automate a bunch of that. Um but having",
      "offset": 7048.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "genuinely new ideas has not been the",
      "offset": 7052.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "forte of this. Somebody just did a",
      "offset": 7054.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "paper, I'll try to dig up the reference,",
      "offset": 7056.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in which they looked at whether these",
      "offset": 7058.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "systems develop new or are able to infer",
      "offset": 7060.56,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "causal laws and they're not that good at",
      "offset": 7064.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "it. Like I don't expect that an LLM is",
      "offset": 7067.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "going to do Einstein level look at a",
      "offset": 7070.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "problem, come up with a completely",
      "offset": 7072.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "different solution anytime soon. All the",
      "offset": 7073.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "solutions they have seem to me to be",
      "offset": 7076.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "kind of inside the box and I think that",
      "offset": 7078.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "getting to AGI is going to require",
      "offset": 7081.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "outside the box solutions. They might",
      "offset": 7083.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "automate the stuff inside the box and",
      "offset": 7085.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "inside the box they may even do better",
      "offset": 7087.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "than people like the famous was it move",
      "offset": 7090,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "37 or web from one of the go",
      "offset": 7092.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "championships. It's kind of inside the",
      "offset": 7094.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "box. It was still within the realm of",
      "offset": 7096.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "things that you know it's still within",
      "offset": 7097.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "go you know it's not outside the box the",
      "offset": 7099.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "way that thinking about relativity like",
      "offset": 7102.32,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "there's a whole different way to think",
      "offset": 7104.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "about physics than we thought before. I",
      "offset": 7105.599,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "think that we will need some Einstein",
      "offset": 7107.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "level innovations in order to get to",
      "offset": 7110.159,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "AGI. Um and certainly to get to ASI and",
      "offset": 7112.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "I don't expect that at least automating",
      "offset": 7116.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the current machines will do that.",
      "offset": 7118.639,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Okay. When we do get to AGI, how fast do",
      "offset": 7121.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you think the takeoff will be\n to to ASI?",
      "offset": 7124.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Yeah. Like for example, you just",
      "offset": 7127.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "mentioned now that like the current",
      "offset": 7129.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "systems are very inefficient in terms of",
      "offset": 7130.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "how much energy they need. Uh that's a",
      "offset": 7132.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "bit scary if you if you really think",
      "offset": 7135.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that in the next you know 10 years human",
      "offset": 7136.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "scientists doing neurosymbolic uh",
      "offset": 7139.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "research will come up with much more",
      "offset": 7141.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "efficient systems that are also better",
      "offset": 7143.36,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "at generalizing. Uh holy cow that's",
      "offset": 7145.199,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that's like going to be orders of",
      "offset": 7149.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "magnitude better in various dimensions",
      "offset": 7151.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "than what we currently I think you know",
      "offset": 7153.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "whether I'm right about neurosyolic AI",
      "offset": 7155.36,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "or not I think that\n there are orders of",
      "offset": 7158.88,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "magnitudes more uh data efficiency to be",
      "offset": 7162.159,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "carved out than than we have. I mean you",
      "offset": 7166.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "just look at human children they don't",
      "offset": 7168.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "need that much data.\n Okay. So you're",
      "offset": 7169.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "saying it'd be orders of magnitude more",
      "offset": 7172.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "data efficient but still only about as",
      "offset": 7173.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "data efficient as humans or\n possible you",
      "offset": 7175.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "could find better. I mean, humans are",
      "offset": 7178.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "pretty good data efficiency wise, but I",
      "offset": 7180.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "doubt that they're at the like",
      "offset": 7182.719,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "theoretical limits. There are some",
      "offset": 7183.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "things in psychometrics where people",
      "offset": 7185.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "really are at the theoretical limits.",
      "offset": 7187.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "So, like we can notice the presence or",
      "offset": 7189.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "absence, I think, of a photon. Like you",
      "offset": 7192.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "can't do better than that. So, um there",
      "offset": 7193.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "are things where we're at the",
      "offset": 7196,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "theoretical limits. There are things",
      "offset": 7196.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "where we're not. Like we are constrained",
      "offset": 7198.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "very much, I argued in my book, Cluge,",
      "offset": 7200,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "by a lack of location addressable",
      "offset": 7201.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "memory. And so like you know my daughter",
      "offset": 7203.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just memorized 105 digits of pi which I",
      "offset": 7205.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "could never do but it's still nothing",
      "offset": 7208.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "compared to what a computer could do",
      "offset": 7209.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "memorized billions of digits of pi. And",
      "offset": 7211.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so you know there are some advantages to",
      "offset": 7214.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "machines and places where they should",
      "offset": 7215.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "really be doing better than people if we",
      "offset": 7217.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "had the right way of writing the",
      "offset": 7219.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "software. Um so at least we should be",
      "offset": 7220.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "able to get to human levels like humans",
      "offset": 7223.679,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "are existence proof of data efficiency",
      "offset": 7225.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and humans are fabulously data efficient",
      "offset": 7227.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "on many problems not all. And then we",
      "offset": 7229.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "have problems like we have cognitive",
      "offset": 7231.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "impairments such that we have",
      "offset": 7233.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "confirmation bias and motivated",
      "offset": 7235.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "reasoning. Motivated reasoning is like",
      "offset": 7236.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "um I you know want this argument to be",
      "offset": 7239.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "true and so I I kind of you know play",
      "offset": 7242.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "little games and we all do this. I mean",
      "offset": 7244.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "scientists are better because we",
      "offset": 7246.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "recognize the behavior in ourselves and",
      "offset": 7248.32,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "try to self-correct. But scientists do",
      "offset": 7249.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "it too. Um machines shouldn't need that.",
      "offset": 7251.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Like some of our things are to use",
      "offset": 7254.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Freudian technology that terminology",
      "offset": 7256.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "even though I'm not a Freudian. Um we",
      "offset": 7258.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "have ego protective ways of reasoning.",
      "offset": 7260.719,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Machines should not need that right like",
      "offset": 7262.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I believe in my political party and so",
      "offset": 7265.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "when my political party does dumb thing",
      "offset": 7267.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "then I go and try to you know come up",
      "offset": 7269.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "with a rationale for it. Machines",
      "offset": 7271.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "shouldn't need to do that kind of stuff.",
      "offset": 7273.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "And so in that way, you know, certainly",
      "offset": 7274.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "the upper bound is going to be way",
      "offset": 7276.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "beyond we are. I I did a panel once with",
      "offset": 7278.239,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Daniel Conorman and he's he well he's",
      "offset": 7280.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "not with us anymore but he was very fond",
      "offset": 7283.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "of these studies that showed that in",
      "offset": 7284.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "certain domains machines were already",
      "offset": 7286.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "better than people and these are",
      "offset": 7288.08,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "problems basically of multiple",
      "offset": 7289.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "regression weighing multiple factors and",
      "offset": 7290.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "he was right and I think I came back",
      "offset": 7292.239,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "with some you know where the machines",
      "offset": 7293.84,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "were not very good and people were",
      "offset": 7295.119,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "better and in the end he said something",
      "offset": 7296.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like you know humans are a really low",
      "offset": 7298.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "bar and you know his whole research well",
      "offset": 7299.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "not his he had many research but one of",
      "offset": 7303.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "his whole research lines was showing",
      "offset": 7305.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that humans in fact were pretty bad at",
      "offset": 7306.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "all kinds of reasoning. So he says,",
      "offset": 7308.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "&quot;Humans are a low bar. We're doing this",
      "offset": 7310,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "panel.&quot; I said, &quot;Yeah, and machines",
      "offset": 7311.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "still haven't met them. They will",
      "offset": 7313.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "someday. They will exceed them. There's",
      "offset": 7314.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "no question about that in my mind. It's",
      "offset": 7316.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "a question of when and how and and so",
      "offset": 7318.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "forth, but we really are a low bar",
      "offset": 7320.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "because of the all of the kind of",
      "offset": 7323.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "cognitive biases and illusions, the",
      "offset": 7325.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "problems with memory. My book, Cluge,",
      "offset": 7327.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "was all about this kind of stuff.",
      "offset": 7329.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "There's absolutely room to do better.",
      "offset": 7330.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And yes, it could happen in 10 years. I",
      "offset": 7332.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "don't, you know, probably it will happen",
      "offset": 7334.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "on some of those dimensions and not",
      "offset": 7336.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "others. it already did on like math, you",
      "offset": 7338.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "know, 60 years ago or 80 years ago. Um,",
      "offset": 7340.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "it will happen, you know, dimension by",
      "offset": 7343.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "dimension, maybe several all at once",
      "offset": 7345.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "when there's a breakthrough or something",
      "offset": 7347.599,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like that. But it will happen and it",
      "offset": 7348.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "could happen in 10 years. Again, I don't",
      "offset": 7350.239,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "think it can happen in two. I think",
      "offset": 7351.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "we're missing some ideas right now.",
      "offset": 7352.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "We're missing some critical ideas, but",
      "offset": 7354.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "when we get those critical ideas, they",
      "offset": 7356.719,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "could go fast. Just like molecular",
      "offset": 7358.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "biology, once Watson and Crick figured",
      "offset": 7359.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "out, you know, DNA, things move pretty",
      "offset": 7361.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "fast in 40 years, you know, now we can",
      "offset": 7363.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "do crisper and stuff like that. or not",
      "offset": 7365.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "40 years but se 70 years you know in",
      "offset": 7367.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "remarkable progress. There will be I",
      "offset": 7370.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "think periods of AI progress that exceed",
      "offset": 7372.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the last few years. I know that the last",
      "offset": 7374.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "few years feel like a lot to a lot of",
      "offset": 7376.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "people but I think in hindsight you know",
      "offset": 7378.239,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "30 years from now AI will be enormously",
      "offset": 7381.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "ahead of where we are now. I mean almost",
      "offset": 7384.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "on any of these kinds of projections,",
      "offset": 7387.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "right? and we will be like, &quot;Yeah, a",
      "offset": 7388.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "bunch of stuff happened and they were",
      "offset": 7391.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "really proud of themselves, but you",
      "offset": 7393.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know, the way that we look back at like",
      "offset": 7394.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "flip phones are like, &quot;Yeah, those were",
      "offset": 7396.56,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "kind of cool, but they didn't know about",
      "offset": 7398,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "smartphones.&quot; I I agree with everything",
      "offset": 7399.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you said about what we agree on. Uh we",
      "offset": 7401.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "continue to disagree about what the next",
      "offset": 7403.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "couple years are going to look like. I",
      "offset": 7405.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "think that um well I think it's going to",
      "offset": 7406.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "look more like A27 where the rather than",
      "offset": 7408.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sort of like tapering off and running",
      "offset": 7411.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "into sort of rather than the the the",
      "offset": 7413.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "limitations that you're talking about",
      "offset": 7416.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "becoming bottlenecks that the companies",
      "offset": 7418.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "can't work around. I think that they're",
      "offset": 7420.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "going to be more like a series of road",
      "offset": 7422.239,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "bumps that the companies sort of like",
      "offset": 7423.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "bash through.\n I mean that is that is the",
      "offset": 7425.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "question. So we we should have a second",
      "offset": 7427.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "edition of this two years from the day",
      "offset": 7429.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and see how this goes on.\n Yeah. Yeah.",
      "offset": 7431.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Okay. So, so one thing worth clarifying",
      "offset": 7433.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "since some people may be confused. I'm",
      "offset": 7435.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "sort of in the middle of thinking",
      "offset": 7436.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "through AI timelines and largely because",
      "offset": 7438,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "I'm sort of trying to reflect on uh what",
      "offset": 7440.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "intelligence is in a sort of",
      "offset": 7442.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "multi-dimensional way. We maybe got a",
      "offset": 7444.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "little bit spoiled with reading writing",
      "offset": 7446.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "ability and crystallized intelligence. I",
      "offset": 7448.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "mean progress over the past two years or",
      "offset": 7450.239,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "so has been in mathematical ability and",
      "offset": 7452.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "uh it's its short-term memory is also",
      "offset": 7455.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "better. Uh but um so I'm still wrapping",
      "offset": 7459.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "my head around that since hence I'm",
      "offset": 7461.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "being more non-committal about some of",
      "offset": 7463.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "these different forecasts where I think",
      "offset": 7465.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "maybe still seems very plausible um uh",
      "offset": 7466.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "more than plausible than by 2030",
      "offset": 7469.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "something that is has the cognitive",
      "offset": 7472.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "abilities of a t typical human um some",
      "offset": 7475.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "system like that u there's some",
      "offset": 7478.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "differences",
      "offset": 7481.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "um in how things might play out at a",
      "offset": 7482.96,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "technical level um in AI 2026 of it. I I",
      "offset": 7486.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "I don't think much really goes through",
      "offset": 7490.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "techn mechanistic interpretability or",
      "offset": 7492.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "technical solutions really solving much",
      "offset": 7494.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "of it. I think you need uh to really",
      "offset": 7496.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "ease the geopolitical competitive",
      "offset": 7498.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "pressures. I think the main dynamics",
      "offset": 7500.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that make way for that are uh",
      "offset": 7502.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "transparency and the espion and how easy",
      "offset": 7505.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it is to do espionage as well as the",
      "offset": 7508.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "sabotageability of that which I think",
      "offset": 7510.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "are very important dynamics that aren't",
      "offset": 7512.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "uh that are in some ways reflected in",
      "offset": 7514.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there but not totally captured on on",
      "offset": 7516.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "sabotageability for instance if China",
      "offset": 7518.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "were interested in stopping the US they",
      "offset": 7520.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "could do some sort of cyber attack on",
      "offset": 7523.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "some power utilities but say that that",
      "offset": 7525.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "doesn't work they can also there's",
      "offset": 7526.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "there's just there's basically a lot of",
      "offset": 7528.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "vulnerabilities uh that they can",
      "offset": 7530.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "exploit. For instance, they could they",
      "offset": 7532,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "could from a few miles away snipe the",
      "offset": 7533.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "power plants transformers and that would",
      "offset": 7536,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "take down the the data center. So I",
      "offset": 7538.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "think that that and there's lower",
      "offset": 7540.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "attributability. Was it Russia? Was it",
      "offset": 7542.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Iran? Was it China? Was it some uh US",
      "offset": 7544.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "citizen as an example? So I think that",
      "offset": 7546.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "affects the strategic dynamics where I",
      "offset": 7549.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "speak about that in in super",
      "offset": 7551.119,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "intelligence strategy as well as I think",
      "offset": 7552.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the transparency that China has to the",
      "offset": 7554.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "US would be relatively high. Right now",
      "offset": 7556.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it's a matter of just hacking Slack and",
      "offset": 7558.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "then you can see anthropic Slack, you",
      "offset": 7559.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "can see OpenAI Slack, you can see XAI",
      "offset": 7561.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "slack, uh Google Deep Mind Slack. So you",
      "offset": 7564.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "can have very high transparency there",
      "offset": 7566,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and hack the phones of of top leadership",
      "offset": 7567.84,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "as well. Um so uh so this this paints a",
      "offset": 7570.88,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "you know in some ways a different",
      "offset": 7575.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "picture, but I think we would agree that",
      "offset": 7576.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "we want to um uh work toward a",
      "offset": 7578.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "verification regime so as to have red",
      "offset": 7581.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "lines around things like intelligence",
      "offset": 7583.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "explosions and and things like that. I",
      "offset": 7585.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "won't really say anything more",
      "offset": 7587.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "substantive, but I will say this. I",
      "offset": 7588.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "thought this was a fantastic",
      "offset": 7590.719,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "conversation. Um, I hope that it won't",
      "offset": 7591.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "be cut too much. Um, because it was",
      "offset": 7594.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "really interesting and I I salute",
      "offset": 7596.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "anybody who made it through watching the",
      "offset": 7598.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "entire thing. Um, we we got pretty",
      "offset": 7600.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "technical at times and really laid out I",
      "offset": 7602.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "think where the state of play is today,",
      "offset": 7605.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which was my fondest hope. I I think we",
      "offset": 7607.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "did a great job with that.\n Thank you,",
      "offset": 7609.52,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "gentlemen. Shake hands for the camera.",
      "offset": 7611.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "All right.\n All right.",
      "offset": 7617.28,
      "duration": 3.64
    }
  ],
  "cleanText": "Okay, take one.\n All right, here we are saving humanity. Take one.\n Having, say, for instance, the United States disrupt China's ability to develop a super intelligence. The main way in which they would develop it is if they get the ability to automate AI research and development fully and take the human out of the loop. Then you go from human speed to machine speed. We've had people such as Daario, for instance, it's gioanthropic, talk about how such a recursive process like that could lead to an intelligence explosion and that would lead to a durable edge where nobody will be able to catch up. And last week, Sam Altman discussed how this process could telescope a decade's worth of AI development in a year or potentially a month. There's been this very seductive argument that has appealed to all of these people, which is basically, well, it's probably going to happen anyway. If we don't do it, someone else will. Basically, all of these people sort of trust themselves more than they trust everyone else and have therefore convinced themselves that even though these risks are real, the best way to deal with them is for them to go as fast as possible and win the race. Why did they make OpenAI? Well, they were worried that they didn't trust Demis to handle all that power responsibly when he was in charge of the AI project and all the fate of the world rested in his hand. So they wanted to create OpenAI to be this counterveiling force that could do it right and distribute it to everybody and not concentrate power so much in Demis' hands. And in fact, the emails that came up in the lawsuit, they were talking about how they were worried that Demis would become dictator using AI.\nOn the protoi things, I think that they follow the instructions fairly reasonably. There are some other parts of it.\n Yeah, that scares the out of me, right? Fairly reasonably. You know, when these things are still in our hands kind of that's okay. But if you have them guiding weapons or something like that, there are many circumstances where fairly reasonably is not good enough.\nThis episode of MLST is sponsored by Tufa AI Labs. It is a research lab which is headquartered in Zurich. They're moving to San Francisco as well. These guys are number one of the ARCv2 leaderboard. They're genuinely fascinated in building the next generation of technology, the next innovation which will take large language models to the next stage. If that sounds like you, please get in touch with Benjamin Kruier. Go to twoferabs.ai.\nThis podcast is supported by Google. Hey everyone, David here, one of the product leads for Google Gemini. If you dream it and describe it, Veo3 and Gemini can help you bring it to life as a video. Now with incredible sound effects, background noise, and even dialogue. Try it with a Google AI Pro plan or get the highest access with the Ultra plan. Sign up at gemini.google to get started and show us what you create.\n[Music]\nAll right, so I'm Gary Marcus. I'm a cognitive scientist, uh an entrepreneur. Um, I've written six books, most recently Taming Silicon Valley. I think that uh and here we are in the heart of Silicon Valley as we record this in San Francisco. Um, and I think we all want what is best for humanity and and hope that that AI can be positive for humanity and not negative. I think the three of us are maybe not known for agreeing on everything, but we actually have a lot of shared values around that and I'm looking forward to the conversation.\n Thanks, Gary. Uh, my name is Daniel Kokotajlo. I'm the executive director of the AI Futures Project. Uh we are the people who made AI 2027 which is a comprehensive detailed scenario forecast of the future of AI.\n I'm Dan.\nI'm the director of the Center for AI Safety. I also advise scale AI and XAI. I also have done research uh in machine learning like I coined the Jello and the Celu a common activation function. Uh more recently I've done evaluations to evaluate capabilities such as MMLU and the MATH benchmark and humanity's last exam. Uh and I've been focusing on trying to uh measure aspects of intelligence and also AI systems safety properties for uh mostly the course of my whole career.\n The common things that we care about are what is going to be the outcome when AGI comes? Um how soon is it going to come? How do we forecast these things methodologically? Um, one person wrote in a question that maybe we could actually start, which is like what is the positive view? Like I I take it that all three of us in the room are united in thinking we think AGI could be a good thing. None of us is sort of like standing, you know, down like people would in front of a um construction machine saying stop it all period. We I think we all think there's at least a possibility of a positive outcome, but you can stop me if I'm wrong. Um, and that we're hoping to steer towards that positive outcome. So, our first question is what could be the upside here? And why aren't you just saying let's just forget about AI altogether?\n I can start with that. Yeah. So, so first of all, I actually think it's quite reasonable to basically stand in front of the bulldozer and say stop. I think that eventually we want to build AI because it is possible to get it right and it is possible to make it in such a way that's you know beneficial to everyone and massively beneficial to everyone in fact. Um but uh if we are currently not on a track to getting it right and we're currently on a track to make it in a way that's going to be horrible then it makes sense to just sort of stop until we figure out a better way. Um but uh yeah, as for what the benefits could look like, well, uh AI 2027 has the slowdown ending in which things go really well for almost everybody. And since since not everybody read I think more people probably read the um darker scenario than the positive scenario. Um lay out uh for people who might not have read the positive scenario a little bit about what the spirit of that is.\n Sure.\nSo in the in the slowdown ending of AI 2027 uh they manage to devote just barely enough effort and time and resources towards the technical alignment problem that they solve it uh just barely in time so that they can still uh beat China as they have wanted to do. By they I mean you know the leader of the of the leading AI company and the president and maybe some other companies right that group of people.\nUm, so you know, the slowdown ending is not our recommendation for what we should actually aim for. We think it's an to to aim for this sort of scenario would be to expose the world to incredible amounts of risk of various types. So we don't think it's our recommendation, but nevertheless, it's a it's a coherent plausible scenario for how things could sort of muddle through and end up pretty well for for everybody.\n And pretty well is I mean like it's the Peter Peter Demandi's abundance scenario for example. Is that what you have in mind?\n I haven't actually read that but but I would say when you get the AIs that are super intelligent so what what I mean by that is better than the best humans at everything and also faster and cheaper. Uh you can just completely transform the economy. You know, super intelligent designed robot factories constructed in record speed, producing all these amazing robots and all these amazing new industrial equipment which then are used to construct new types of factories and new types of laboratories to do all sorts of experiments to build all sorts of new technologies and blah blah blah blah blah blah. Eventually, you get and by eventually I mean in only a few years uh you get to a completely automated wonderful economy of all sorts of new technologies that have been iterated on and designed by super intelligences. material needs are basically just met for everybody. You know, there's just an incredible abundance of wealth uh to distribute and uh you know things like curing all sorts of diseases uh putting up new new settlements on Mars and so forth all that stuff becomes possible.\nUm so that's the sort of like potential potential upside and then of course there's the question of uh can can we actually achieve that right and and if we do have the technology to achieve that who's in control of the technology and do they actually use their power over uh the army of super intelligences to make that sort of broadly distributed good future for everybody or do they do something that's more dystopian you know so I I think we\n well first of We'll come back to the bulldozers and whether you think we're actually at that point um or not. But I think we can agree probably and and we'll let you chime in too. Um that there is a technical alignment problem and we should talk about that and a political if it's not alignment problem maybe that's too close pairing of words but a political issue about if this technology exists who controls it that's deeply important. Yeah.\n And so we should talk about both sides of that sort of political control and technical alignment. Um let me get your answer though on on that first question. You know, do you think we need to stand in front of a bulldozer now? And if you don't, you know, what do you think is the positive?\n Yeah. So I I think there are two concepts. There's AGI and there's ASI the artificial general intelligence and artificial super intelligence. I think AGI doesn't have a clear definition. So it's difficult for me to say for all definitions of that that would be worth standing in front of. Some people would say we have AGI already.\n Let's just start with what we have now. Yeah. You know, there could be an argument that right now we should just stop the train. Sorry, I've moved from bulldozers to trains. Um, you know, there'd be an argument. Um, I'm not making this argument, but some people have made it. The argument would be we're already so far along this path, and if we don't resist it massively right now, it's going to be bad, and there isn't a good outcome here, a good enough outcome to offset what seems inevitable. I'm not making that argument, but I've heard people make it. And so, partly, you know, grounding exercise, where are we on that? So, you know, one argument would be it. We we've already screwed up. We need to put have every person on the planet get in front of this train or bulldozer and and say just stop it because there isn't a positive enough outcome to warrant it.\nAnother position would be there's actually a really positive outcome here and if we can guide it in the right way, we can get to that positive outcome and here's why I think it's positive. That's the kind of set of issues I wanted you to speak to first. So I primarily think about stopping uh super intelligence. Uh the main reason for that is because that is much more uh implementable geopolitically compatible with existing incentives and and so on. So it's it's something that I can actually foresee. I think things stopping tomorrow is not something I can as easily foresee. So I don't really think about that. Uh the reason that super\n But is that just a fatalism then? I mean like if you're worried about super intelligence and you might think if we move even towards AGI which presumably is a closer in time that that's a bad idea because we won't be able to stop the super intelligence like maybe we don't want to risk going further.\n I mean I I couldn't conceive of how we would coordinate you know now to do that. I I think that I think the asks are instead uh having um say for instance the United States disrupt China's ability to develop a super intelligence. The main way in which they would develop it is if they get uh the ability to automate AI research and development fully and take the human out of the loop. Then you go from machine speed or from human speed to machine speed. And we've had people such as Daario for instance uh the CEO talk about how that will give the US a or such a such a um an recursive process like that uh could lead to an intelligence explosion and that would lead to a durable edge where nobody will be able to catch up. And last week Sam Altman discussed how this process could telescope a decade's worth of AI development in a year or potentially a month. And I think that's extraordinarily destabilizing for two reasons. One, if they control it, then or if a state controls it such as say China, uh then uh all the other countries are at substantial risk uh because that super intelligence could be weaponized and used to crush other countries. And uh if they don't control it, which I think would be fairly likely by a nearly unsupervised, extremely fast-moving process, um uh uh then everybody's survival is also threatened. So either way, this uh very fast automated AI R&amp;D loop is is quite uh destabilizing whether a state controls it or not. And so I think it makes sense uh not just because you know AI is is is scary in some vague sense but I think that there are very strong geopolitical incentives for estate self-preservation.\nI got that I want to come back to the geopolitical aspect of it and I read um I skimmed the the paper that you just did with with Alexander and and Eric. Um but I don't think I got an answer for the first part which is the upside. I don't think I need you to lay out the upside.\n Yeah. Yeah. Yeah. So for for upside um\nI think people um generally think that uh if we have AI it will necessarily hollow out all other all values or that we'll have an extreme pressure toward one of them. Uh that was we'll be zoo animals or we will um uh or we'll be all pleasured out and just you know in our VR experiences constantly and have have superficial experiences. I I I think you could set a society up so that people have the autonomy to choose between different ways that they would want to live their life given the resources that uh uh AI increasing GDP could provide. So I think that there's a way you could have u a a list of objective goods being met uh in society. So I I don't think there's um no equilibria where things can work out. I think there's a way of achieving a variety of goods and still preserving human autonomy uh and all that like sort of what we have now. Imagine we had things now um but then or we have the autonomy to choose how we want to live our lives now. Um and there'll be more resources um uh\n some of us in the west have\n Yes, that's right.\nThat's right. That's right. And there's there's other sorts of longer term questions about how you might do that like how are you going to distribute power? Maybe that would be with uh compute slices that people would rent out for instance where they would have the unique cryptographic key uh to to activating that compute slice so that you're not just distributing wealth but you're also distributing um ways of of generating that wealth. You know, there's there's a lot in this\n\n\nIn this deep future that one could pull out. But I, I, I think there are some paths where things work out well. I like your term equilibria. Um, I think we probably all agree that there's multiple equilibria here and that we're all trying to steer towards positive equilibria. And I think we would all also agree that there's lots of different ways to get to both the positive and the negative. We might disagree about some of the paths, some of the likelihoods and so forth. But I think we both, we're all three operating under that general assumption that there, there's multiple ways this could go and some of them are good and some of them are bad. I'm a little darker than I used to be even a couple years ago because of um the economic and inequality sorts of issues. So, a couple years ago when I trusted Sam a lot more than I trust him now, um I heard him talking about universal basic income and um you know, I've always thought that that was an important part of the equation. And it seems to me that the more that data has driven things, the more inquisitive the companies, and it's not just OpenAI, have been, the less realistic it has seemed to me that we're going to have anything like a universal basic income. I think there is some scenario under which there's so much wealth that everybody's subsistence gets met. I don't expect that the wealthy people are going to give up the beachfront property under any circumstance or their power. And I think the dynamics that we've seen in Washington lately have made me, though I know our politics may not be the same, have made me less optimistic about any kind of relatively equal distribution or even just not a relatively extreme distribution. I think that the positive outcomes do depend on getting some answers there to what would be the incentives, the mechanisms, the dynamics such that things are well distributed. And I guess I'm sort of making this up as I go, but I think there's an argument here for stopping the train if we can't see any solution to the political thing. Most of my own personal concerns are really about technical alignment, which we'll talk about soon enough. Um, but if we don't have an even envisionable political solution, that does make me nervous. And I could see an argument for let's stop the train now because we just don't see how to do it. And going back to something you said at the very beginning, there's an interesting argument about delaying the train versus stopping the train. And um like I think all of us signed the pause letter. Maybe you didn't sign the pause letter.\n\nOkay. Only I signed the pause letter. That's interesting. I think that that didn't call that. Um that's fine. Whether or not the part of the pause letter that made me sign it that I like the best was it said let's pause this particular thing that we know is problematic in certain ways. I mean it was really about delaying the development of GPT5, which ironically still doesn't exist two and a half years after some of us signed the pause letter. Um but the notion was we would pause the development of GPT5 because we knew that GPT4 had certain kinds of problems um around alignment and that we would spend that time instead working on safety. So it was explicitly constructed as a delaying tactic. It wasn't saying never build AI. It wasn't saying don't do any more AI research. It was in fact saying do more AI safety research and wait. And that does still seem like maybe not a politically viable thing in this moment, but at least a strategy one might consider is, you know, maybe we should be constantly updating our estimates on, you know, how likely are you to get the positive outcomes versus the negative outcomes and how much would that change as a function, for example, of putting more resources into safety research as opposed to capabilities research, etc. Um, and what's your take on what I just said?\n\nTotally agree. Like I think um yeah like I, I, I like the pause AI framing more than the stop AI framing for what, for what you, for the reasons you just mentioned and then there's a lot more to say about more, more nuanced proposals for what is to be done. Um I do think we should maybe save a lot of this for the later part of our discussion after we've gotten through the technical stuff like timelines, alignment, etc. But you're the moderator, so I'm, I'm gonna use moderator's prerogative for now, but you can push me a little bit later if we don't, we don't get there. I'm trying to find some sort of common ground. I think we have lots of common ground. And yeah, um and then we'll get, we get to the differences. Do, do you buy that notion that also this should be on the table of some kind of clause or no?\n\nI, I, I think um uh making time for technical research, I, I'm not expecting much of a return on investment from that. Um I don't think um most success strategies part um in the foreseeable future particularly go through uh um uh AI being fully controllable. So I'm, I do technical research. I'll give you a comeback on that like an extreme version of the argument I just placed comes from the cognitive psychologist and evolutionary psychologist Gary Marcus, um who had a tweet that I really liked which was um if it, you know, we should wait until we can build this stuff safely even if that takes, I forget what he said, I'll say 250 years, um and it was an interesting framing because like everybody's thinking like what should we do next week or should we sign this pause letter and it was kind of deliberately extreme like I think it was maybe even 500, like if it takes us 500 years we should just wait and you know I could see an argument for that and in fact I wonder what the counterarguments are.\n\nSo I, I think that the process that I described earlier, that sort of recursive loop, I guess you could call it an intelligence recursion, which if it goes fast enough is an intelligence explosion. Uh that is not something you can research your way out of. Uh you can't just, you know, write an eight-page paper and then we've, we've solved it, you know, in the news tomorrow. It's we, we figured out how to um pull fully derisk some um extremely fast-moving process that we've never done before. Um uh and all of its unknown unknowns have been anticipated.\n\nWell, I mean we, we could politically, I mean it would take a lot of willpower and it's probably not likely, but we could, you know, try to have a global treaty. Don't go there. Don't work on these kinds of things. Let's report it. If you do, right? Right. I mean, you could at least imagine that kind of scenario. I don't see any technical way of coping necessarily with that set of problems right now. But I, if we were, I mean not just the three of us in the room, but if we as a society were convinced that let's say that was a red line, we, you know that you suggested one red line, there are others. So we could decide as a society there are certain red lines. Maybe recursive self-improvement might be a reasonable one to consider. um that we can say we won't cross any of the red lines. Um we will make treaties around it and we just shouldn't do it until we have answers to them.\n\nSo I think it would be worth states clarifying that we don't want anybody doing that um type of a fully automated intelligence recursion uh because it'll be destabilizing and um now that doesn't necessarily immediately take the form of a treaty. So initially you have them exchanging words about that and articulating their preferences uh potentially explicitly or potentially in internal policy at CIA and other um and things like that and the other parties come to learn it uh um through leaks or directly. Um and you eventually want to gain more confidence that nobody's trying to trigger something like this. And um this, this process is multiple stages. It may involve things even like skirm, it requires a conversation. And it may even require a skirmish uh for, for people to think okay we need to do a verification now and then maybe you get some type of of of treaty but you can still uh have various forms of coordination through deterrence uh without anything like a treaty um just as uh we've had strategic stability on multiple issues without treaties necessarily. So that's something that when is is potentially a later stage thing but you have to have the conversation advance far further.\n\nDaniel's giving me a look on the.\n\nNo, I'm just looking back and forth.\n\nYou're just looking back and forth. Why am I in this?\n\nYou have to like rub your neck because I'm right in the middle.\n\nYeah, in the middle. Okay. Um, so let's separate for a moment the deterrent stuff, although I know you're keen to talk about and know more about it than I do. Um, and let's try to get to it. Um, you could sort of separate what are the dynamics for which we would form treaties. Maybe we need some disturbance and skirmishes as you just talked about, but from the question of like would the rational thing for civilization to do right now be in fact to sign these treaties because we're basically pretty close. I maybe take a longer timeline than you, but we're reasonably close to recursive self-improvements of at least some sort. And so like maybe we don't want to go there. Maybe that's what the rational thing to do be right now to make those treaties. Or even if like we thought that was 25 years away because we know treaties take 8, 10 years, like maybe we should be putting all our, you know, intellectual capital or political capital or whatever into doing that right now. What do you think?\n\nYes. So I think three, three red lines would be no recursion where that's a fully automated, not just some AI-assisted one, but uh one that has that explosive uh potential. uh no AI agents with expert-level verology skills or cyber offensive skills uh made accessible without some safeguards. Um uh and uh model weights need to have model weights past some capability level need to have some good information security for containing them uh and making sure that they're not uh uh exfiltrated or stolen by rogue actors.\n\nIn a minute, I'm going to ask you how close we are on these various things. But what's what's your take on what he just said? Is that those same.\n\nWith the first, I don't know about the second two, the other two, but definitely the first one. I think that if somehow we could coordinate on that, that'd be great. I'm not sure if that's the best red line to coordinate on, but it's a excellent place to start at least. Do you want to throw in any others now or you can later in the conversation or.\n\nI think that the type of thing that I'm probably going to end up advocating for is going to be um more of a like rather than like here's a line that we're all not going to cross, something more like we are going to gradually develop AIS with these capabilities, but we're going to do it in a way that's like mutually transparent to each other and that proceeds sort of slowly and cautiously where we all like debate whether it's safe to go to the next level and then after we get there we study it for a little bit and then debate whether it's safe to go to the next level and so forth. So, so it, the sort of thing that we're probably going to end up advocating for is going to look something more like that rather than uh but but yeah in term in terms of like the thing that you really need to like stop from happening in the short term. uh that sort of recursive self-improvement thing is I would say the number one thing.\n\nRight, so um this conversation is a little bit depressing in the sense that many of the things that we seem to be worried about actually seem fairly close. Um maybe the person in the room who's most pessimistic, if that's the word, let's not the most extended in time on this is me, um but all of us I would say think that, well no, let, let me rephrase this question. Um, it's a dark set of answers relative to the reality right now. I think in the following sense, even if you think it's going to take a while to get to AGI or ASI or something like that, and we'll talk about that in a little bit. Um, the the things that are red lines, and I like your red lines. Um, people are already pushing against them. They may not be breaking through them. Like depending on your definition of recursive self-improvement, I might give you different estimates. Mine might be a little longer than yours, but people are already trying to do that, right? And transparency is like 1990s talk. Like it's like it's in the rear, I'm exaggerating a little bit, but it's it's in the rearview mirror. I mean like OpenAI was open originally. It is not open anymore. So, you know, there are elements still pushing for transparency, but there are certainly elements pushing against.\n\nSo, I'm I'm somewhat more optimistic about uh um increased transparency and more situational awareness from from governments. The a a reason for that is I think it's incentive compatible for the US to be more transparent about what's going on at the frontier because China already knows. Meanwhile, for PLA or People's Liberation Army developments, there's somewhat less uh transparency there. So, uh the people to gain more from information would probably be the rest of the world. Uh so, so China has somewhat more of an advantage there. And this sort of levels things and that um having high transparency into the frontier is very useful for making more credible uh deterrence. Um because then yeah, interrupt for a second about a time frame question. I think the notion of a durable advantage in LLMs, if that's what the technology is, is a myth. Like if we stay on LLMs, nothing's going to be durable. Um, but I think you think about these things in a little bit different way than I do.\n\nI, I, I think with the current um paradigm, uh, I am expecting them to continue to leapfrog each other. And so roughly parody is what I'm seeing, right? So there could be a different technology. you know, my personal favorite would be neurosymbolic AI that somebody gets a durable advantage of. Maybe, but you know, there's going to be espionage. People are going to share ideas and whatever. Um, but if the paradigm stays roughly like it is, I don't see anybody getting a durable advantage. So, that's part one is do you agree, disagree with that?\n\nI think I, it depends on what you mean by paradigm. So, I would say like LLMs are just a part of this or a subset of this overall like AI research. you know, they're not the only possible AI design and uh progress is going to continue being made. I mean, I would say like in some sense, arguably we're already seeing a move away from LLMs with things like these so-called reasoning agents that have access to tools and can write code and so forth. So, so there's going to be this continuous shift uh towards I\n\n\nI would say, rather than like discrete paradigm shifts, it'll be more like a continuous paradigm shift. The AIs of 2027 are just going to be quite different from the AI of 2023, you know. Um, but taken as a whole, the AIs, um, I do think that, uh, the United States could potentially end up having a durable advantage over, uh, China, and one particular tech company within the United States could end up having a durable advantage. I see this as possible. I see that as unlikely unless somebody approaches the problem in a pretty different way. Like, I can imagine a neurosyolic approach. It would be just so different from what anybody else has that I could see, at least for a while, an advantage in the current way of people are doing things. I don't even know what that would look like. Perhaps I should clarify what I mean by durable advantage. I don't mean, um, they never figure out what you already figured out. I mean that by the time they catch up, you've already moved ahead. So that you, it's like, you know, you keep running as fast as they're running, so that even though they're only six months behind, they can never quite catch up because by the time they do, you're six months ahead again. And that was part of the other part of that question I wanted to get to, which is like, does six months matter? Is that enough to change the world or not? Right now, it doesn't seem huge, but, uh, um, if you are the first to trigger a recursion, yeah, that can matter a lot. Be the moment where it matters, and that goes back to the other question, which is on the recursion thing. The fact is, people are trying it, as far as I understand it. Like, it's the plan. It is the plan, yeah. It's still highly AI-assisted, and there's nobody who could try and spin this off, spin this up right now and have it be foreseeably going to lead to something substantial. But, uh, yeah, I mean, what I see right now is like, you could do your hyperparameter search faster or something like that, and you could call that recursion if you want, but that's not really what we're talking about here. We're talking about like a system finds a fundamentally new idea that year 24 would only come from humans and that leverage of not only coming up with the ideas, but testing them, validating them, working out the tweaks, implementing them, scaling. I think that's exactly what people like Sam and Dario are now hinting that they're going to be able to do soon. I'm not really buying those claims, but I think that's what they want to do now. 100%. Yep. Yeah. And that is destabilizing and scary and dangerous. Let's say potentially destabilizing. I mean, if it's just hype and they can't really do it, it's not destabilizing. But if one of them achieves it, if it were technically feasible, that would be destabilizing. And I mean, I think we can agree it probably is technically achievable. It's just a question of whether you can do it with current technology or later, how far in the future. There's no, you know, surely an argument that this can't be done, that you need, right? I mean, I think we all think that's going to happen. And so one advantage for transparency then is if there were very high visibility as to what's going on at the frontier, if they're basically triggering this sort of process and the public is kept reasonably informed as it's happening, I think the world would be very much freaking out. Yeah. Uh, so I, I think that possibly sunlight might, um, be a way of providing substantial pressure to prevent that. Freaking out might be too strong, but it's worth mentioning that like polls of the American people, not necessarily in Asia, but American people are pretty worried, maybe not at the top of their set of worries, which might be economic or, um, and so forth, but you know, you look at these polls and like 75% of the American public is already worried. And I expect that that worry will increase, not decrease. At least, I don't see any active thing that is going to decrease it. The thing that might decrease it is, um, uh, propaganda from the tech companies. I think the tech companies seem to like scaring people, as far as I can tell. Oh, I disagree. I think that, um, I mean, there's probably some of that there to some extent, for sure, but, um, at least my experience at OpenAI was that, uh, there was more pressure to like not talk about the risks in public and to like, you know, sort of downplay that sort of thing than pre. There was no pressure to play it up, at least not as far as I could tell. Um, and you know, if you look at the public messaging by Sam over the years, he certainly started to like talk about it less and less and less, uh, as well. When he, he, I mean, partly because I pushed him, but when we talked at the Senate, he's, um, you, he didn't answer the question, what were we most worried about? I mean, I made Blumenthal come back, but Blumenthal says it's jobs, and and and he, Sam gives his explanation why he's not that worried about jobs. And I say to Blumenthal, you should really ask him a question. And and Sam there said, my biggest worry is that we do, I can't remember the exact words, but substantial harm to humanity. And when he was back at the Senate a few weeks ago, his biggest harm was like, in so many words, like, we don't make all the money and extract all the value that we could or something like that, right? So he, I mean, I think it's true that he, he used to talk more about this kind of risk stuff and talks less now. So I mean, that's interesting data point is around the time that I did my Senate appearance, which was May 2023. So reconstructing timelines a little bit after that was a second letter related to the pause letter, which I did not sign. Um, that was the one about, um, being, and maybe you wrote, I don't know, can letter, the extinction letter. Yeah, I signed that one. You made, that's what we signed. So, so I didn't sign that one, and we should soon get to extinction risk, but, um, but we'll come back to it in a minute. Um, Sam did, I think, sign that, right? Um, so at that time, that was maybe June 23 or July 23 or something like that. Um, it was after the Senate appearance. I knew that. Um, at that time, it was still popular among CEOs to express concern about this, and maybe it's true that they do a little bit less. Dario still alludes to this. Right. Right. Yeah. So a lot of people that I have spoken to think that it's a deliberate mechanism to try to hype the product, to talk about the risk, like, look how great our stuff is, we, it might kill us. Yeah. So I have lots of scientists sign that. I didn't, but yeah, I understand. Yeah. So, so first of all, yeah, loads of people have these views who aren't conflicted and trying to hype up the companies, you know. Yeah, I agreed, agreed about that too. But then, but then to your point about the hype, you know, my take on what's been happening is that DeepMind, OpenAI, and Anthropic have been full of people who were thinking about super intelligence from the very beginning, um, at the highest levels of leadership, and therefore they have been considering at least somewhat both the loss of control risk and the concentration of power risk, you know, who gets the control of the AIs from the beginning. And this is documented in all sorts of ways. You can look at their old writings and so forth, some of the leaked emails. Um, and, uh, then you might ask, well, why are they building it if if they've been taking these, if they've been thinking about these risks? Can they see loss of control? Loss of control, concentration of power. So, loss of control is what if we don't solve the alignment problem in time and the AI take over. And then concentration of power is if we do solve the alignment problem, like who who controls the AIs? What goals do we put in them? You know, do we risk becoming a dictatorship or some sort of crazy oligarchy or whatever? Um, so, so, so like you can find writings from people at these companies, both senior researchers and also the CEOs and so forth, going back decades talking about these things. Um, and then you might ask, well, why are they doing this? Why are they racing to to build these things if if they were, I mean, Dario seems like the most extreme version of that question. Like he seems still very much, uh, publicly saying that there's very serious risk, and he seems very much publicly pushing the models forward. Like I, I, I understand, though I didn't see it myself, that at some point he was saying we won't build frontier models because of these risks, and now he's obviously building frontier models. And and so the the the key here is, um, I guess I would say in a single word, it would be rationalization. So, um, there's been this very seductive argument that has appealed to all of these people, which is basically, well, uh, it's probably going to happen anyway. If we don't do it, someone else will, and it's better for us to do it first than for someone else to do it because we're the responsible good guys who will wisely solve all the safety issues and then also beneficently, you know, uh, give UBI or whatever to to make sure that everything works out well. So, so basically all of these people sort of trust themselves more than they trust everyone else and have therefore convinced themselves that even though these risks are real, the best way to deal with them is for them to go as fast as possible and win the race. And this is, you know, DeepMind's plan, so to speak. Demis Hassabis, his plan was basically be there, get there first with this big corporation, Google. And then, uh, because you have such a lead over everybody else, you can sort of slow down and get all the safety stuff right and sort of make sure that everything goes well before everybody else catches up. That seems naive. That plan was torpedoed when, you know, Elon and Sam and Ilia made OpenAI. Uh, why did they make OpenAI? Well, they were worried that they didn't trust Demis to to handle all that power responsibly when he was in charge of the AI project. You know, the only, you know, when all the fate of the world rested in his hands. So, they wanted to create OpenAI to be this counterveilling force that could do it right and make it, you know, distributed to everybody and not concentrate power so much in Demis' hands. Um, and in fact, they were the leaked emails or the emails that came up in the lawsuit, they were talking about how they were worried that Demis would become dictator using AGI. Um, and uh, well, we can see how well that's worked out. You know, uh, all the Anthropic people basically split off from OpenAI because they didn't think OpenAI was going to handle the safety stuff responsibly. So then they're claiming that like they have the technical talent and they'll be able to like sort out the alignment issues better than everyone else, you know? Uh, it's a mess. Yeah. You have anything to add? I agree that it's a mess. Not to that it's a mess. And I mean, I guess I don't know if you may not want to answer this on camera, but um, do you feel confident that we should trust any of these particular? We definitely should not. I think, um, uh, pushing for things like transparency as well as the government having people whose job it is to be keeping track of these, coming up with contingency plans, interviewing these labs for, uh, their plans and coming up with an internal assessments, their probability of success. All of that, uh, seems useful, and I think, um, at least, uh, um, some of the players here would probably be willing to push for that type of stuff, and I think others wouldn't. Uh, so I think it's, um, are they willing to help solve these collective action problems or are they going to continue defecting and, uh, um, it's been a lot of defection. To get back to your, I realized I never actually answered why this ties into your question. So early on when these companies were fresh and young and idealistic, their their founding mythology was basically, \"Yes, the risks are real, and that's why you should come work at our company because we're the good guys.\" And so when that was still fresh and still like, you know, the main thing they were saying, they were talking about it a lot. Uh, but then now when their their sort of like founding myth is sort of like kind of laughable, and it's like very much not, uh, something they can say with a strafe. It's it's eroded some. Not something they can really say with a straight face so much anymore, and also they're lots of under, lots of political pressure, you know, uh, to get investors and to ward off regulation and stuff like that. So, so, um, the rationalization wheels are continuing to turn, and they're coming up with new narratives to justify what they're doing, you know. And there's also a lot more players at the table, both within the US industry, which is most of what we were talking about, but also in China. Um, then we have the whole, I don't know if we have time to go into it, but they'll, maybe we will, maybe won't, the, um, mechanisms of open sourcing or at least open weight models, which means that, you know, essentially anybody can get in this game to some degree. Um, I disagree with that to some. Oh, go ahead. I mean, to some degree, sure, but no, walk me through the disagreement. Well, um, like, hypothetically, suppose that we, suppose that we like achieved AGI, someone achieved AGI and immediately opened the weights for everybody. It's not going to happen like that. But suppose it did happen like that. Then for a brief, glorious moment, anybody with enough GPUs could be able to run their own AGI right at the frontier of capability. However, um, AGI isn't like, you know, it's g, there's going to be AGI plus and AGI++ and so forth, and whoever's going to get to AGI plus is going to be the one who had the most GPU, so they can run the AGIs to do the research fastest, you know. So, so even if you gave everybody exactly the same starting point at the same level of capability, the people who had more GPUs would pull ahead slowly but surely over everybody else, right? Um, so, so I think, I think that there's this unfortunate, around that, but go ahead. So there, there's this unfortunate sort of inherent, uh, I, I don't know if I want to say winner or take all effect, but there's like this return to scale sort of thing inherent in the dynamics of an intelligence explosion. Um, and that's part of what makes this so scary. Um, yeah, all the actors are incented to push it as fast as possible and to to have the maximal resources in order to\n\n\nAnd they're also incentivized not to open it up and, you know, to not be transparent about it and so forth. Unfortunately.\n\nYeah. I, I, I think we have a little disagreement there about transparency and how much we might expect. I think you're a little bit more optimistic. Um, and I'm, I'm, uh, Daniel, and being a little bit less optimistic about transparency. I'm especially less optimistic about transparency as we get closer to having actual AGI or let's say differentiated AI. So right now I would say that what's there is not very differentiated. Everybody's kind of using LLMs with reasoning models and so forth. Um, there's some differentiation in how people do their RL and what their data sets are, but, um, I think there's not so much value in transparency until somebody has something that really is unique that they think other people aren't going to just reconstruct very rapidly. Um, and as one gets to that point of having some unique piece of intellectual property, which I think will happen, um, there's even more reason to be less transparent. So I, I, I think I'd be optimistic about being transparent about the numbers of the best models internally, not necessarily the methods that were used to create them or the weights for them, but at least the public knowing, uh, uh, what's going on, um, what's the peak capabilities that the models are exhibiting and then being aware of that.\n\nAre you being optimistic that they'll do this by default or that this would be good? Obviously, it'd be good. Um, I, I think it's good and that there's more tractability for this than a lot of other asks.\n\nI agree.\n\nMore tractability like people might agree to do it.\n\nYeah.\n\nYeah. Yeah. I agree with that. That's why I've been making these asks. Yeah. But I'm just saying that they're not going to do it by default. Like if nobody asks them to be transparent about these things.\n\nNo, I agree.\n\nSo I mean we can agree that like voluntary self-regulation is probably not enough to get to that.\n\nYeah. Yeah.\n\nUm, yeah. Um, I, I wouldn't bank on it. Yeah. All right. Let's talk a little bit about time and forecasting and scenarios. We've talked a bunch about strategy, what we should do, uh, going forward, what policy should be. Some of that depends on timelines. So, if we thought that we had a thousand years before AGI, then we might make different choices than if we thought we had six months. Um, if we thought that LLMs were the answer to AGI, we might make one set of choices. If we thought they were definitely not the answer to AGI, we might make a different set of choices, maybe focusing more on research. Um, so let's talk about our forecasts around that and also maybe I'll throw in there our forecasts around when we might sort out the safety problems, the alignment problems. Um, I will make the argument that we've made some progress towards AGI and very little towards alignment, and we can see if if we agree on that.\n\nSo I want to start by using a notion that many in the audience will know but not all, uh, which is a distribution of probability mass, which is to say that you could make a simple prediction. You could say I think AGI will be here in 2027 or 2039 or whatever. But I think we all understand that that's the unsophisticated thing to do. You can certainly say your best guess for which particular year you think it might come. But as people who are either scientists or know something about science, we know there's what we call confidence interval around that. So, you know, it might be this plus or minus that. The most sophisticated thing to do is to actually draw out a curve and say, you know, I think some of the probability mass will come, you know, before 2027 and some of it will come before 2037 and some of it will come after. In your, I guess it's sort of an appendix to your AI 2027, you go through this in a fair amount of detail. I'm not sure how many people got to the appendix. I think it was maybe a little hidden to find it, but it was there and it did a good job of that, and it gave the forecast for four different people in a sort of qualitative way. I don't know if it showed the full curves, but it said like this is the chance that they think it will come before 2027. And I think for three of the four forecasters or something, you remember better than I. Um, you know, some of the probability mass was like after 2040 or something like that. So maybe I'll start with you because I think you have maybe put the most work into trying to get detailed probability mass distributions and maybe you can talk about what your own are and different techniques people have used and where you're where you are on that.\n\nThank you for that, uh, excellent, uh, launch to this. Feel free to stop me if I ramble too long. Uh, because it's huge topic, lots to talk about. Um, okay, so first the exciting part, the actual numbers. Um, when we were writing AI 2027, we had our different medians or 50% marks for, uh, AGI, or I think we were, we divide up, we didn't use the word AGI, we used different milestones. So superhuman coder, uh, full automation of AI research, uh, super intelligence. Um, but, uh, for those things, uh, let's say for super intelligence, I was thinking 50% chance by the end of 2027. Um, and that's why AI 2027 depicts it happening at the end of 2027 is because I was sort of illustrating my median projection. Now the other people at AI futures project tended to be somewhat more optimistic than me. They tended to think it would take longer, uh, to get to super intelligence.\n\nLet's clarify optimistic is that flips depending on how you think is sure. Yeah. So the other, they thought it would take longer, like 2029, 2031, something like,\n\nlet's say more conservative, a few more years. Yeah, that's right. Um, and, uh, but you know, I was the boss, so we went with my, my timelines, but happily, uh, by the time we actually published it, I had lengthened my timeline somewhat. So, so these days I would say 50% by end of 2028. Um, but, uh, yeah,\n\ndoesn't give me that much comfort. I mean, I, I think you're wrong, but if we have an extra 12 months, like I'm not sure that's enough to handle all the, still quite scary to prepare.\n\nBut, um, in terms of what the shape of our distributions looks like, they tend to have a sort of hump in the next five years and then like a long tail. And the reason for that is because, um, well, the pace of AI progress has been quite fast over the last 15 years. And we understand something about the reasons for why it's been so fast. And basically the reason is scale. So they've been scaling up compute, they've been scaling up data, they've been drawing in ever more researchers into the field, especially compute is the probably the main, the most important input that they're scaling up. Um, and that's sort of turbocharged progress, but, uh, they simply won't, they, the companies simply won't be able to scale things up at the same pace after a couple years.\n\nIs that a function of power or is it a function like electrical power? What, what is the function? What's the rate limiting step by which you think that kind of scaling won't continue? It won't be like a sharp cut off, but it'll be a couple things. So, so partly it'll be, you know, power supplies. Partly it'll just be, um, uh, compute production, like much of the world's, even after building new fabs and even after converting much of the world's chip production into, uh, AI chips. Uh, they won't, they'll have to like produce 10 times more fabs in order to scale up by 10 times, right? Whereas previously they could just take chips designed for gaming and you repurpose them for AI. So, so in a bunch of little ways that are going to add up, there's going to be all these frictions that's going to start to bite that will make it harder for them to continue the crazy exponential rate of scale to,\n\nI think we're already seeing that with data. I think I don't even know the actual numbers, but, um, let's say that GPT2 used maybe 10% of the internet or something like that or 5% or something like that. Um, maybe you guys know the actual numbers, and GPT3, you know, used a significantly larger fraction. GPT4 used like most of the internet including transcriptions of videos and stuff like that. And so you can't just keep 100xing that because there just isn't enough data. There's new data generated every day, you know, so, um, you can always ek out a little more and people are turning to augmented data, um, as well. They should, but that's not a kind of universal solvent and it works better for things like math where you can verify that the augmented data are better. And so I think we're already running against that. Um, kind of like bottleneck on one of the, let's say, raw resources that go in at least into the current approaches.\n\nAnd another thing I would add is that, um, you can also just think about money, which is which you can use to buy many of these things, and you can say, well, they've been scaling up the amount of money that they're spending on AI research and on training runs in particular, um, over the last decades, but they, it'll be hard for them to continue scaling at the same pace. You know, they, they're probably already doing something like billion dollar training runs, but, you know, if it goes at the same p, uh, what was the biggest training run in in in 2020 was like what, like $3 million, something like that, you know, four, $5 million. So, so they, they've gone up by like two and a half orders of magnitude in 5 years. If it's another two and a half orders of magnitude, we're doing a $500 billion training run in 2030. Like there starts to be not enough.\n\nYeah.\n\nThere's just not enough money in the world, you know, like the the tech companies just won't be able to afford it. Even if they've grown bigger than they are today, the economy just won't be able to afford it. And so, so that's why we predict that like if you don't get to some sort of radical transformation, if you don't get to some sort of crazy AI powered automation of the economy by the end of this decade, then there's going to be a bit of an AI winter. There's going to be at the very least a sort of tapering off of the pace of progress. Um, and then that sort of stretches out a lot of probability mass into the future because that's a very different world. It's like, you know, could take forever. Well, not forever, it could take a quite a long time to, uh, to get to AGI once you're in that regime. I'm gonna ask you one or two more questions. I'm gonna insert mine and then I'm going to come to Dan. Although if you want to,\n\nI mean, one, one note on data was, yeah, we sort of ran into that bottleneck, I think maybe two or so years ago. Um, and the main things that have been continuing the pace, um, uh, would be this, you know, thinking mode type of stuff, uh, um, outside of the the trends that were existing previously. So I think it's sort of picking up slack in some ways for the the the fact that most of the internet has been trained on.\n\nUm, the methodology by which you came up with these curves, can you just tell us a little bit about them?\n\nYeah. So again, these curves represent our subjective judgment, which is very uncertain. It's just our opinions, you know, but the the way that, I guess the the way I would like to to say it should be done is, um, rather than just sort of, uh, pulling a number out of your ass, so to speak. You should come up with models and look at trends and then, you know, have, have, have, uh, little calculations that attempt to give numbers, and then you should stare at all of that and then pull a number out of your ass based on based on all that stuff that you've just looked at, you know. Um, and so, and so that's what we did. And the the the main arguments and pieces of evidence that we found, uh, moving to to inform our overall estimates were, uh, what we would call the benchmark and plus gaps argument. Um, perhaps I should also go into the like, um, compute-based forecast. Have you heard of the bioanchors, uh, framework by Ajeya Cotra?\n\nI don't think I have.\n\nOkay. Well, we can, I'll, I'll briefly mention that before getting into the benchmarks plus gaps thing. So, so the bioanchors framework, it's called bioanchors because it references the human brain. And I think that part's actually the less exciting and plausible part of it. The part that I think is is more robust and more worth using is this core idea that you can, you can think of this trade-off between more time to come up with new ideas and do AI research and, uh, more compute with which to do the AI research. And you can sort of think, you can, you can make a big two, two-dimensional plot, and you can imagine, okay, 10 more years, 20 more years, 30 more years, what's, how does the probability that we get to AGI go up with more time, but you can also imagine, uh, not more time, but just more compute, could we get to AGI today if we had, you know, five orders of magnitude more compute, 10 orders of magnitude more compute, 30 orders of magnitude more compute, right? And the the insight there is that the answer is yeah, probably, like, like, for example, if you had 10^ the 45 floating point operations, you could do a training run that's basically just simulating the entire planet Earth and all life evolving on it for a billion years, you know, with that amount of compute. And, and, and the thought there is that you don't really need to understand how intelligence works at all if you're building it with that type of training run because there's no insight coming from you. It's you're just sort of letting nature do its thing and letting evolution do take its course. And so, so the thought is that we can make a not guaranteed, but like a soft upper bound at something like 10^ the 45, and then you can make other sort of soft upper bounds. You can think, well, what could we do with 10^ the 36, and you can lay, I, I wrote a blog post about this in 2021. You know, suppose we had 10 to the 36 flop. What are some like really huge types of training runs we could do? And then like, what's our guess as to how likely that is to work? And, and what you can do is you can sort of, you can start to smear out your probability mass over this dimension of compute. And so you sort of have a soft upper bound, and then you have, of course, a lower bound, which is the amount that we already have done. You, we clearly haven't done it right now with this amount of compute. Uh, and so that gives you this smeared probability distribution over compute. And then you think, okay, but now we're also going to get new ideas. And so as new ideas,\n\n\nCome along, we're going to be able to come up with more efficient methods that allow us to train it with less compute.\nSo you can think of your probability distribution as shifting downwards while also the amount of actual compute increases.\nAnd then that gets you your actual distribution over years.\nUm, and uh, I think this is the right sort of like basic framework for calculating these sorts of timelines.\nUm, but it's a sort of relatively abstract, like low information framework that doesn't really look at the details of the technology today and the details of the benchmarks.\nSo I think it's like a good way to get your prior, so to speak.\nUh, but then you should update based on actual trends on the benchmarks and so forth, which is what I'm about to get to.\nBut the reason why I mentioned this prior process is that 10 to the 45 floating-point operations isn't actually that far away from where we are right now.\nUm, right now we're at like what, like 10^ the 26 or something for training runs, and we're going to be crossing a few orders of magnitude in the next couple years.\nAnd so even if you just had like an in, even if you just smeared out your probability mass with maximum uncertainty across the like orders of magnitude from where we are now to 10^ the 45, there'd be like, you know, a non-negligible amount that it's going to happen in the next few years.\nUh, and so like, even on priors, you should think it's like decently plausible that it could happen by the end of the decade, and then you should update your prior based on the actual evidence, which I'll now get to.\nSo the actual evidence, I would say, let's look at agentic coding benchmarks.\nThat seems to me to be um, the most informative thing to look at, and the reason for that is because I don't think that the fastest way to get to super intelligence is in a single leap where humans come up with the new paradigms uh in their own brains.\nI think rather it's going to be this more gradual process where humans automate more of the AI research process, and then that gets us to the new paradigms fast.\nAnd so I think that the the lowest hanging fruit as far as the AI research process is concerned that's going to automate first is the coding.\nSo I'm looking to see when will we get to the point where the coding is basically all handled by LLM like AI assistants.\nUm, and we have benchmarks for that sort of like, you know, companies like Meter, MER have been uh, building these little coding environments, doing all these coding tasks.\nThe companies themselves have been doing this, of course, because they are racing as fast as they can to get to this automated coder milestone.\nAnd um, so we look at those and we extrapolate trends on them, and we we forecast that well, in the next couple years, they're basically going to saturate.\nYou know, we're gonna have AIs that can just crush all of these coding tasks.\nAnd they're relatively, you know, they're not something to scoff at.\nThey're not just multiple-choice questions.\nThey're like the sort of task that would take a human like four hours to do or eight hours to do.\nUm, uh, but that's not the same thing as completely automated coding.\nSo first, we extrapolate to when they saturate the benchmarks, and then we try to make our guess as to what the gap is between the first system that can completely saturate these benchmarks and the first system that can actually automate the coding.\nAnd that's probably the more speculative part.\nUh, but you know, we do our best to reason about.\nWell, now we're ready for our first full-on disagreement of the day.\nUm, but I understand, I understand your logic there.\nI think it's well thought through, but it's missing the cognitive science for me, and my approach to this is more from the cognitive science.\nI see a set of problems that a cognitive creature must must solve, many of which I wrote in my 2001 book, *The Algebraic Mind*, and I don't feel like we've solved any of those problems despite the quantitative progress that we've made.\nAnd those include generalizing outside the distribution, with which I think still remains a huge problem.\nI think the Apple paper was a um, there are actually two Apple papers I discovered today.\nUm, but the Apple paper with the Tower of Hanoi stuff, I think is an example of problems with distribution shift.\nI think we've seen many of them over the years.\nWe see that these systems have trouble multi doing multiplication with large numbers unless they call on tools, um, and so forth.\nI think there's lots of evidence for that.\nI think that there's a problem of distinguishing types and tokens that leads to bleed-through when you're representing multiple individuals from some category that leads to hallucinations.\nSo I wrote an essay recently about the hallucinations that um, I think it was Chat GPT made about my friend Harry Shearer, who's a pretty well-known actor, um, and it misnamed the roles of characters that he played in the movie *Spinal Tap* and said that he was British when he's American and so forth.\nI think this blurring together that we see of hallucinations remains a problem.\nAnd I could go on with a list of others.\nI think there are several having to do with reasoning, planning, etc.\nAnd the way I look at things, which is not to say that there isn't some value in what you're doing, um, is more on these cognitive tasks.\nAnd so what I say to myself is what would AI look like two years before we achieved AGI or ASI or something like that.\nUm, certainly two years before we achieved ASI, we would have full solutions to all of those things.\nIf we specified if we specified an algorithm for something, we would expect the system to be able to follow it.\nCurrent systems can't even play chess reliably according to the rules.\nSo, you know, 03 will not uh, sorry, 03 will sometimes make illegal moves.\nIt can't avoid illegal moves.\nUm, another thing I would expect is that current systems when we're close would be basically the equivalent of their domain-specific uh, counterparts or at least be close.\nRight?\nAGI means artificial general intelligence.\nAnd I would say the reality is that domain-specific systems are actually much better than the general ones.\nRight?\nNow the only general ones we have are LLM based.\nUm, but for example, AlphaFold is a very carefully engineered hybrid neurosymbolic system that far outperforms what you could get from a pure chatbot or something like that.\nUm, somebody just showed that an Atari 2600 beat, I think it was 03 and chess.\nUm, so even, you know, sometimes very old uh, systems will will beat the modern domain general ones.\nOn Tower of Hanoi, Herb Simon solved it in 1957 with a classical technique that generalizes to arbitrary length, whereas the LLMs do not generalize to arbitrary length and face problems.\nAnd so I could go through more, but the gist of it is I don't see the qualitative problems that I think need to be solved.\nUm, and I'll just go to your 1045, 10 to the 45th, because it's really interesting to me.\nUm, I wrote a piece once with Kristoff Ko.\nI don't know if you guys know him, the neuroscientist.\nUm, we wrote a science fiction essay.\nIt's the only published science fiction essay I've ever written.\nAnd it was in a book that I wrote called *The Future of the Brain*, which I guess we wrote in 2015.\nI was the editor.\nAnd we wrote the epilogue to it in, I'll call it 2015.\nAnd so it was said in something like 2015 or I think it was 2045.\nAnd the notion was there's a book about neuroscience that by that point we would actually have created an entire simulation of the brain, but it would run slower than the human brain and would still not have taught us anything about how the brain really works.\nSo we'd have this simulation, but we wouldn't understand the principles of it.\nYou know, we'd have a neuron by neuron simulation, maybe even a protein by protein simulation, but we could find ourselves in the place where we'd replicated the whole thing without really understanding where it worked.\nAnd I do wonder with the 10 to 45, even if you sort of trained on everything, would you have solved the distribution shift?\nAnd would you have abstracted principles that allow you to run efficiently and effectively and and usefully in new new domains and so forth?\nI think it's a really interesting question.\nI had never thought of the the 1045, even though I I read at least some of your paper.\nI admit I didn't get to that level of detail.\nWell, this part wasn't in 2020.\nThis is sort of, so it wasn't in that appendix.\nI admit I didn't read the whole appendix.\nI read some of it.\nUm, I think it's a really fascinating thought experiment.\nSo maybe I've said enough.\nSo just to lay it out, like there's one way to extrapolate on the basis of things like compute, and I think you've done a masterful job of doing that as well as can be done and acknowledging that there's still an element of pulling things out of one's um, behind, which is true on, you know, any any account, like nobody can really do this in a closed form way.\nUm, and then I have a different slice on it, which is like, where are the qualitative things that I want to have solved?\nUm, I think Yan LeCun, who I often disagree about um, many things with, actually would be closer to mind.\nHe would probably give a different set of litmus tests that he's looking for.\nWe would both emphasize world models.\nWe have slightly different ideas about world models.\nBut he would say, I don't think we're close because we don't really have world models, and neither of us, I think, are satisfied with the current thing that some people call reasoning, but neither of us think is, you know, robust enough.\nAnd so I think he and I both take an architectural approach or a cognitive approach.\nGreat.\nUh, so saying I'll list a bunch of bullet points of things we could discuss, and then hopefully we can get through them each, and if we miss some, well, at least I put them out.\nSo in reverse order, um, 10 45 scenario where you just sort of brute force evolve intelligence.\nIndeed, you would not understand how it works at all.\nUm, but nevertheless, you would have it, and you could sort of take those evolved creatures out of their simulated environment and then start, you know, plugging them into like chat products and stuff and using them in your economy.\nUm, and they would be smart, you know, they evolved.\nThey built their own civilization in there, you know, so so they're pretty smart and they're pretty good at generalizing and so forth.\nUm, so you wouldn't understand how it works, but you'd still nevertheless have the AI system.\nAnd and indeed, that's kind of what I think is happening today for us.\nIt's like we don't understand how these AI systems work very well.\nUh, we're sort of just throwing giant blobs of compute at giant data sets and training environments and then toying out, playing around with them afterwards and seeing what they're good at, you know.\nUm, that's part of my darkness about it is I think there's too much alchemy and not enough principles.\n100%.\nAnd this is and this is part of why the like alignment issue feels so looming to me is that like we don't even know what we're doing.\nHow are we supposed to, how are we supposed to craft a mind that has, you know, the right virtues and the right principles and so forth when we don't even like, right?\nAnyhow, so so so there's that.\nUh, next thing, so you you mentioned um, uh, Tower of Hanoi math problems, these these sort of ways in which the current AI systems seem uh, limited in in particular, limited and fragile and and hallucinations in ways that you you think that we're not really making progress on.\nSo there I would say, well, um, you know, I also can't solve the Tower of Hanoi, and I also can't do large math problems in my head.\nI need tools.\nI need to be able to like program a little bit or like do use.\nI'm going to come in on that briefly.\nOkay.\nOne is that, you know, relatively young children can actually do Towers of Hanoi really well if they care about it.\nSo, um, even really big ones, even pretty big ones.\nUm, I mean, there's a video I think of a kid doing seven discs lightly, lightning fast, like in two minutes on YouTube.\nUm, a kid who enjoys it.\nHe's probably 12 or 15 or whatever.\nUm, I'm sure he can do eight discs if he wants to because it's a recursive algorithm.\nI'm sure he's learned it.\nAnd so I think some humans can do a problem like that.\nSome humans, if they want to, can do arithmetic.\nWe humans do um, get into memory limitations.\nBut it, you shouldn't expect that AGI should um, there is actually like a um, we should pause for a moment on definition of AGI, right?\nIt has had varied definitions.\nThe one that I always imagine is it should be at least as good as humans in a bunch of things and better in certain ones.\nSo I would not be satisfied with an AGI that can't do arithmetic.\nI'd be like this, you know, yes, okay, it's equivalent to people in in this respect, but I would actually expect more.\nAnd especially if we're talking about the risks that we're um, concerning, I think we're really talking about a form of AGI that probably can do all the short-term memory things that people can't and has the versatility and flexibility of humans.\nYou can argue about that, and and you know, I I would say that like the weakest AGI would be it's as good as Joe Sixpack, who's really not very good at reasoning, has is full of confirmation bias, has not gone to graduate school, doesn't know how, you know, critical reasoning.\nAnd you'd say, \"Okay, that's AGI because it's it does what Joe Sixbacks does.\"\nI think most of the safety arguments are really around AGI that's at least as smart as like, sure, you know, most smart people.\nUm, smart people can in fact do things like Tower of Hanoi.\nAnd certainly, you know, another example I gave you was chess, right?\nSix-year-olds can learn to follow the rules of chess.\n03 will do things like have a queen jump over a knight, which is just not possible.\nAnd so, you know, the failures there are quite striking and not something you need even an expert to do.\nYeah.\nSo, I guess I would say it feels to me like we're making progress on these things or at least that these these barriers are not going to be uh, barriers for long.\nUh, I think for example that maybe the, that's assuming your conclusion that last little piece.\nWell, let me tell you more.\nLet me tell you why why I think this.\nSo, so I think that yeah, maybe like the transformer by itself might have trouble doing a lot of this, but you should think about the transformer plus the system of tools and plugins that you can build around it, Claude plus code interpreter and things like that.\nUm, uh, and I think that that system could solve tower of things like that because Claude might be smart enough to to look up the algorithm and then implement the algorithm and then, you know,\n\n\nDo it. Um, and so, so I actually, I'm curious for your immediate reaction to that point. I mean, I look, I've always advocated neurosymbolic AI, and I think that that is actually a species of neurosymbolic AI. I don't think it's the right one, but the point of the neurosymbolic AI, the arguments that I made going back to 2001, was that you need symbols in the system to do abstract operations over variables. There were several other arguments, but that was the core argument. And what you're doing when you have Claude colon interpreter is you are doing operations over variables in the Python that it creates. And so you're, you know, you're moving it to a different part of the system. So the pure neural networks don't have operations over variables and fail on all of these things. Symbolic systems can do them fine. And here you're using the neural network to create the symbolic system that you need in order to solve that particular problem. So it's absolutely a neurosymbolic solution. I think that the rate-limiting step is that they don't always call the right code that they need. If, if you could make that solid enough, that would be great. Okay. You, you could think, I'll just say a couple more sentences there. Um, one of the first attempts at this strategy was to put an LLM into Wolfram Alpha as the, which is a totally symbolic system, right? Um, and not Wolfram Alpha into Mathematica. Um, and the results were, were, you know, hit or miss, right? And the problem was on the interface getting to the tools. If you can really reliably get to the tools, then you have a neurosymbolic system that works, right? If you can have the neural networks reliably call the tools that they want, um, or, or that they should be calling relative to the problem, I should say, um, then you're golden. I think empirically, it is a bit hard to get the tools to work reliably.\n\nSo, so that that's helpful because I think that sort of collapses a lot of your, um, your barriers into one barrier, which is reliability. Uh, because perhaps you would agree that, like, if we can get them to... We should come back to the world models piece of it, but yeah, go ahead. Go run with that.\n\nWell, yeah. So, so, so, so there, I would say it does seem to me like the AIs have been getting more reliable over the last couple years. Um, and one piece of evidence I would point to is the horizon length graph from Meter, which you've probably seen me talk about.\n\nI wrote a whole paper about why I hate it.\n\nOh, okay. Interesting. Yeah. So, so, in my substack. So, so the way I would interpret this graph, and I'm sure many of our audience has already seen this, but they have this, you know, they have this suite of agentic coding tasks, uh, that are sort of organized from, uh, from shortest to longest in terms of, like, how long it takes human beings to complete the tasks. And then they note that, you know, the AIs of 2023 could, generally speaking, do the tasks from here to here, but not do the tasks above this length. But then each year, like, the crossover point has been lengthening. And a very natural interpretation of what's going on here is that the AIs are getting more reliable. You know, if, if they have a, if they have a chance of getting into some sort of catastrophic error at any given point, um, then, you know, if it's like a 1% chance per second, then after, like, 50 seconds, they're going to get into an error. Uh, but then if that goes down by an order of magnitude, then they can go for more seconds and so forth. And so, so the thought here, I would say, this is evidence that they are just getting, generally speaking, more reliable, better at not only not making mistakes, but recovering from the mistakes they make. Not infinitely better. They're still less reliable than humans, but, but there's substantial progress being made year-over-year.\n\nI see that your argument you're making, um, with the particular graph, I think there's a lot of problems, and some are actually relevant to this argument. Um, one is, it was all relative to coding tasks. It wasn't tasks in general. Um, the coding tasks. I'm very concerned from a scientific perspective that we don't know how much data contamination there is. We don't know how much augmentation is done relative to those benchmarks and so forth. Which leads me to my next point about it, which is, um, uh... I can't, I'm blanking on the guy's name from 80,000 Hours posted something on Twitter yesterday, which was a parallel to that graph. I think it was also by Meter, uh, looking at agents, and so the y-axis for the coding thing was sort of minutes to hours to days or whatever, and for agents, it was like seconds to tens of seconds or something like that. So it was a totally different y-axis.\n\nWhat types of agents?\n\nLike it... I thought these... with the coding thing, it was like coding agents...\n\nThis was some more, maybe like web browsing agents or something like that. Um, and so, and, and so the argument was that you're getting the same kind of curve, but the scaling was completely different on the, on the y-axis. Um, so there's, there's something there for both of us, right? So, so what's there for you is, is a general, you know, curve of reliability over time. Um, what's there for me is the performance is still, you know, pretty poor on, on those agent things. And more generally, I don't like the axis at all. I, I think that it's very arbitrary. Um, in, in my critique, we, we give a bunch of examples, but, like, how long does it take this task, and also, um, it's all to the point of 50% performance. It's just a very weird, um, measure, and you can find many things that, you know, a human can actually do in 3 seconds that, according to the graph, if you look at the graph, um, you know, should have been done by 2023 model. Here's one just off the top of my head, which is choose a legal move on a chessboard, right? You know, this takes a grandmaster, I don't know, 100 milliseconds or something like that. But 03 still can't do that task with 100% reliability. It can do 80% reliability or something like that. So it's just, it's weird to read anything off of this graph. I'll, I'll send you the substack essay on it.\n\nYeah. Thanks. One, one other thing I was going to mention, which is a new topic, sort of, is that I think in a lot of these cases, my defense of the AI is that, well, they never were trained on this task, and... But, that's, I mean, that's the essence to me, right? Is, is... I understand they fail on things that they're not trained on. I mean, that's an oversimplification, but if we're talking about AI risk, and... I, I understand that the, the really dangerous AIs of the future have to be able to do stuff without having trained on it. Uh, how, at least I would say, to the same extent that humans can do things without having training. So, to, to take your grandmaster example, grandmasters can do that task in less than 3 seconds with greater than 80% reliability, but also 100% reliability.\n\nThey've trained on it a lot, like, they, they've done lots of... And whereas 03, possib... I don't know that we don't know the details of 03's training process, but it's possible that there was not a single playing of a game of chess at all in the training process for...\n\nYeah. So I don't think that's very plausible. So there's a...\n\nseen transcript of games of chess.\n\nThey've seen transcripts. They've seen the explicit rules in wiki. I know they've been trained on Wikipedia. They've probably read, like, Bobby Fischer played chess, which is how I learned to play chess. So, they have books on chess. And so, they've read everything related to chess. They read, read everything in what is the famous phrase, publicly and privately available sources or whatever. Um, uh, that wasn't quite the famous phrase, but, um, you get the idea. I... So, I think they probably actually have a lot of data on chess. And just as a side note on transparency from the scientific perspective, it's very hard to really evaluate these systems because we don't know what's in the training set. The core question, going back to my work in 1998, is how do they generalize beyond what they've been trained on, and we just don't have transparency on that.\n\nSo, so then, rel... So, first of all, I totally agree about the transparency thing. Back to the training thing. Um, I want to talk about Claude plays Pokemon, and I want to again talk about the chess thing. So I wasn't, I wasn't aware of this 80% statistic for 03, but I'm curious what the statistic...\n\nI don't know the exact number. I mean, that, that's a, that's a butt fact is...\n\nI haven't heard about something. I, I, I had a friend look into this, and he's very easily able to do it. I can give you some other example. Let me just put one other example on the table, similar flavor, which is I played Grok in tic-tac-toe, and I came up with some variation, and then, um, I might write this up this weekend, and so I'm asked some variation. We played it, and then it offered another variation, which was, let's only play tic-tac-toe where you can, let's play tic-tac-toe where you can only win if your three in a row is on the edges. And I said, fine. We had some moves back and forth, and then it suggested some moves for me at a point where I could, in fact, make a three in a row. So it had failed to develop the right strategy. Um, I hadn't played this variation either before, but it was obvious what to do. So he failed to identify what three in a row was, which I would say goes to a conceptual weakness, like you should have had enough data to understand what the three in row was, and so it suggested other moves, didn't recognize it. It's a pretty bad failure. And then we played again after I corrected it. You know how you get these sickopantic answers? I'm sorry, you know. So, so we played again, and it lost to me again, and then a third time. Um, I posted all of this on Twitter a couple weeks ago. Um, so if you move these things off the typical problem, they're even worse. If they had a robust understanding of a domain like tic-tac-toe or chess, it wouldn't be too hard. I posted another on Twitter, which was like, I had some variation on chess, like, um, give me a board state where there are three queens on white side, but black, um, can immediately win, and performance was not great. So, like, you've, you take knowledge that for an expert would be out of what they've specifically practiced, but they understand the domain well enough, and experts will have no trouble at all, and these systems still do. So I, I totally agree with those limitations about current systems, but the thing that I would say is that it seems like the trends are, you know, going up. So I would predict that if you measured this sort of thing for the last five years, even though the current systems might still have weaknesses, they would be less weak than the systems of two years ago, which are less weak.\n\nWell, chess is an example, and then I'm going to go to you. I was just say last thing, which is I first noted the problem with chess, I think two years ago, the illegal move problem. It really, I think Matthew Akre was first to... Arer was first first to point it out, and I spread it on Twitter and said, \"Look, this is a serious problem,\" and it persists. There are a lot of problems that I feel like have persisted, but your turn. You've been fine.\n\nYeah. Yeah. Uh, one, one thing is on benchmark-based forecasting, and I think that has limitations, in particular, street light effect, where, well, when it gets 100%, that will suggest that it has the task, or that suggests it's, it's solved the task. The issue is that it, they often have some structural defects that are only obvious later in the, uh, or when you go fairly far out into the curve. So, for instance, in video understanding, you think, wow, if you looked at the benchmarks from a few years ago, they're, they're totally at the top of them, but then you can come up a year later with several other sorts of benchmarks that, uh, uh, challenge them. So I, I, I think that there tends to be a gravitation toward what's very tractable for AI systems and where some interesting action is happening. That's a selection pressure on the sort of benchmarks. If one's looking at cognitive tasks, such as those that you would give kids if you're testing their intelligence, for instance, if you randomly sample many of those, the, the models don't do that well. Um, uh, maybe on, you could, it'd be a double-digit percentage, maybe be almost on half of them, they don't do that well. For instance, count the number of faces in this photograph. 03 can't do that very well. Uh, or connect the dots or fill in, fill in the colors in this in this picture. Uh, just an example for, for visual, um, ability. So, uh, there's, um, I, I don't think when Gary's pointing these out, it's, it's just, he's just running the cherry-picking program, and what he's going to do is, he's going to keep just, he's just going to keep cherry-picking and doing god of the gaps thing, um, for AI until basically is, is AGI. Uh, I, I do think that there is a non-adversarial distribution, a different sort from the cognitive science angle. Uh, you would see many of these, these, um, uh, sorts of issues. Um, and I, I think that there's potentially some speaking past each other, in part because he's not viewing intelligence as, uh, some undidnimensional thing entirely. There's can, uh, a lot might be correlated together, but there are various other, uh, mental faculties that are important that aren't, um, that aren't, uh, really online that much. Uh, um, we, we saw with GPT, um, with the GPT series, that by pre-training on a lot of, uh, text, we got some subcomponents of intelligence. We got reading, writing ability, and we got a lot of crystallized intelligence or acquired knowledge from that, um, uh, and that process took several years. Um, so it's, it's not the case, there's often people will sort of point out, once it gets traction on it, then it will solve it immediately or solve it very shortly thereafter, a lot for a lot of these core cognitive abilities. They took multiple years. Mathematical reasoning would be a recent example with Minerva. Google's Minerva system got 50% on the math benchmark. Some benchmarking I made some a while ago. Um, uh, in 2022, um, and I think we've only recently crushed it in 2025. So it took a good three years, and I think the reading, writing ability took, um, got to an interesting state, and now relatively complete, uh, four years later. I think crystallized intelligence as well is now relatively complete. However, there's still a variety of others. I wouldn't expect visual processing ability, including for video, to be taken care of in...\n\n\nA year or two. Maybe audio ability would be taken care of in a year or two though. Uh, we don't see almost any progress on long-term memory. Um, basically, uh, I think that, uh, they can't really that meaningfully maintain, um, a state across long periods of times over complex interactions. Uh, and so once we start to get traction on that, then we can start to forecast that out. The reason this is, um, and and for fluid intelligence, which is what we see in the AGI thing and Raven's progressive matrices test, that still seems very deficient as well. So I think when thinking about these, it's it's it's important to, um, split up the, uh, um, uh, uh, one's, one's notion of cognition because if there is a severe limitation on any of these dimensions, then the models will be, uh, fairly defective economically or at least for many tasks. For instance, if a person doesn't have good long-term memory, it will be very difficult to inculturate them and teach them how to be productive in a working environment and load all that context. Um, if they, uh, have very slow reaction time, that's also a problem. Or if they have low fluid intelligence, that will limit their ability to generalize substantially. So, um, I think the numbers are going up substantially or continually on many of these axes, um, for these benchmarks. Uh, so, but at the same time, when those benchmarks are at their conclusion, there might be another peak for those, and there's many of these, and for it to be an AGI, you're going to need all of them as well, and for some of them, they're fairly early on in their, uh, uh, capabilities, such as with, uh, long-term memory. Uh, so I, I think, um, if one's doing forecasting, I think understanding intelligence somewhat more and having a, uh, uh, um, uh, a more sophisticated account, such as one would find in cognitive science, um, can help, um, foresee bottlenecks that will actually be fairly action relevant.\n\nI mean, I completely agree with you, and I'll just mention, uh, physical intelligence and visual intelligence. You mentioned visual, we left out, I skip physical, physical, physical, and spatial intelligence are really very serious limits right now. So if you ask Veo3 to label a diagram, for example, it would be quite poor at it. If you ask it to reason about an environment, it's going to be poor.\n\nSo I mean, I agree with, like, you made it sound like I think intelligence is a single dimension. I don't, I don't know where you got that impression. Um, I agree that there's all these limitations.\n\nWell, you said reliability in all things considered type of sense. I think your forecasts don't make a lot of contact with the kind of stuff that the two of us just talked about. This became two, two against one in a way that I entirely did not bring it on. I can usually a bridging type. But anyway, um, I, I understand that, like, this is why we call it the benchmarks plus gaps is that we understand that just because.\n\nCan you just explain that phrase? Can you just explain benchmark? So yeah, the benchmarks is you take all the benchmarks that you like and you extrapolate them, and you try to see when they saturate.\n\nMh. And then the gaps is thinking about all the stuff that you just mentioned and thinking about how, you know, just because you've knocked down these benchmarks doesn't mean that you've already reached AGI. There's all this other stuff that the benchmarks might not be measuring. You know, you have to sort of try to reason about that.\n\nAnother way of putting my argument is I identified a set of gaps in 2001, rightly or wrongly, but I identified them. I think that they were real in 2001. I mean, those were applying to multi-layer perceptions, not to transformers, which hadn't been invented yet. But I still see exactly the same gaps, and I see some quantitative improvement, but no principled solution to any of the gaps. So there were three core chapters in that book. One of them was about operations over variables. One was about structured representations, and one was about types and tokens or kinds and individuals. And I just don't see that the things that I described then have been solved. I see certain cases where they can be solved, but all of the problems that I see seem to be reflexes of those same core problems. And so if I seem like a grumpy old man, it's because for 20-some years, and really I pointed these things out in 1998, 27 years, I have not really seen them. And so the notion that we're going to solve those gaps all in three years seems weird. Like I thought your estimates of solving a few of the things you just mentioned were generous. You say, you know, we solve reading and writing in three years.\n\nVideo is much more certain.\n\nVideo, but let me finish the sentence. Um, but the kinds of numbers that you gave were like three or four for this one, three or four for that one. And I think, like, really, you know, my estimate is really 10 years is you most of the distribution is past 10 years. It's because I see several problems that, like, we'd be really lucky if we solved, let's say, the visual intelligence problems in three or four years. We'd be really lucky if we solved the, um, the video kind of visual intelligence over time, let's call it, problems in three or four years. I just see too many problems, too many gaps, to use your phrase, to think we'd get all of those at once. Like it's in the movie Everywhere, every, what is it? Everything everywhere all at once. To me, to get to 2027 would be everything everywhere all at once for a set of things that I've been worried about for 25 years.\n\nSo, so, uh, excellent. First of all, yeah, fun point. You were, it sounded like you were disagreeing with me, but then you were saying, like, yeah, three years, three to, in terms of the, in terms of the overall, basically.\n\nThere's, there's maybe a caricature, which maybe isn't exactly you, but the caricature would be things will keep scaling that will automatically solve the problems. That's what's been happening. You point out an issue.\n\nIt's the word automatically that I bustle out.\n\nUm, well, I'm just interested in the bottom line numbers for the years. I like, like, we had this potential dispute about whether it's just LMS or whether it's neurosymbolic. I don't want to fight you about that. Like, we can say it's neurosymbolic if you like. We can say it's not automatic, but rather involves some new ideas. But the point is, I think it's going to happen in like three or four years.\n\nYeah. So, okay, great. So, so I just think new ideas are hard to project. I'll give you my favorite example is in the early 20th century, everybody thought genes were made of proteins. Somebody even won a Nobel Prize on that premise. They were all wrong. And it took a while for people to figure out that this is wrong. And Oswald Avery did these experiments in the 1940s, which really ruled it out, and you know, discovered that by process of elimination that it was DNA. And it didn't take that long to move very fast in molecular biology once people got rid of the bad assumption. I think we're making some bad assumptions, but it's hard to predict when people will move past an EDA fix, which is where I think we are now. I mean, proving that is very hard. I can give a lot of, you know, qualitative evidence that point in that direction, but there's at least a possibility that I'm right about that. If we don't have the right set of ideas, it depends on when we come up with new ideas. It could be that we automate the discovery of new ideas, but most of what LLMs have done is not discovery of new ideas. It's really exploiting existing ideas.\n\nSo I, I, I would claim that, uh, uh, the timelines, there are many paths to it being shorter before 2030, for instance. But the, the picture that you point out of there are these various other cognitive abilities that are long unaddressed, and this isn't a cherry-picked distribution. This is actually, um, uh, um, what you would do if you're, um, inspired by cognitive science or psychometrics. Uh, I think basically both positions are legitimate, but I think if you'd integrate them, there could be a bullish case that you'll be able to knock off some of those core, um, uh, core, uh, cognitive, uh, abilities, uh, by the end of the decade. It's not totally certain.\n\nIt's funny you say it. I think that the absolute most bullish, if we want to use that word, um, case is 2030. Like in the very fastest case, it'd be 2030. Would require solving multiple problems that I think we're just not positioned quite yet. I don't think is very likely, but I think that that's the maximum possible. I just can't get my mind around 2027. It just does not seem plausible because of the number problems. And I think also what you said is right. We want to integrate these two approaches to make the forecast, and nobody's really quite done that. Maybe that's a, a, um, adversarial collaborative, uh, collaboration that that we could do. I haven't really seen, uh, something dig into that side of it of sort of new discoveries. Maybe your gaps does it a little bit.\n\nSo let's talk about the superhuman coder milestone and let's, the superhuman coder milestone, the automating encoding aspects, which is not all of R&D, but just part of it. Um, I think the, I, I'm excited to try to drill down into that and for us to try to make bets or predictions of, like, what the next few years are going to look like, basically. So from, from my perspective, it feels like.\n\nYou have often over the last couple years been pointing to limitations of LLMs that were then, like, overcome in the next year or two, which I mean, there are specific examples.\n\nYes.\n\nRight. So, so, like, for example, particular, well, chess hasn't been solved. So let me clarify what I mean by specific examples, and I'll come back to you, which is I, I give, you know, on Twitter, here's a sentence where, you know, here's a prompt, and it gets a weird answer. Those are always remedied. I was told, I don't know if it's true, that that people in OpenAI actually look on social media for examples, including mine, um, and you know, people patch them up. Uh, the specific examples in that sense, like, literally this prompt gets this weird answer. A lot of them have been solved. These more abstract things like playing chess actually have not been solved. And the more abstract problem of can I give you a game with variations on the rules. I talked about that in Rebooting AI in 2019 in the first chapter. Um, that hasn't been solved. Are you then saying that, like, for all you know, six months from now, the AIs will be able to solve the legal move in chess thing, or are you saying.\n\nI don't think LLMs will solve.\n\nI mean, I think someone could come up with a scheme to get LLMs to, uh, to play chess without making illegal moves by calling a tool. I think that's possible, like, like.\n\nWithout calling a tool, I don't think it'll be possible in the next six months. I'd be very surprised. Um, and you can ridicule me in six months if it happens. Um, and that's fine. Um, but also let me add a twist to it, which is what we talked about in Rebooting AI was, like, for example, if you train a particular system on, I think we use go as an example, but if you train it on a 15 by, what is go, is it, uh, sorry, 21x 21, if you train on ordinary 21x 21 board, will be able to play on a rectangle instead of a square, will you know, different variations. I had a conversation with a guy at DeepMind who did chess in a really interesting way without using Monte Carlo tree simulation, which is what, um, the alpha zeros and so forth do, and he trained it on an 8 by 8 board, and I said if you just had to do 7 by seven, would it work, and he said no, right? So, you know, he did a version where there was no tree search, so it's not neurosymbolic, the tree search is one of the symbol processes, or at least is much less neurosymbolic, and you know, astonishingly, it played pretty well, trained on, you know, billion games or something like that. Um, but even then, it was sterile knowledge in the sense that it was purpose-built for this, but it wasn't general knowledge of chess. If you asked it to do, my put three queens on a board and but still three white queens, but make sure that black wins on the next move, it wouldn't be able to do that at all. Right? The, the, there's a flexibility to human expert knowledge that we emphasized in Rebooting AI that I see no evidence of, of prog, or maybe a little evidence of progress, but you know, relatively little progress on that. My tic-tac-toe example is just like that.\n\nSo can you give me more things of, can you give me more examples of things that a year from now I won't be able to get an off-the-shelf model to do without giving it tools?\n\nI, I think, distri, I'll put it this way, there'll be many, many examples. I can come up with distribution shift, all of what I was just doing. We'll still find failures on, uh, next year. I mean.\n\nYou'll be able to come up with new examples, but you don't, you can't come up with any example now that will still be true a year from now. If I make it in a slightly more generic fashion, then yes, I, I am sure that I will become able to come up with variations on chess, um, that are not orthodox variations, but maybe there's some already known, um, chess actually has lots of variations, um, or chess problems and things like that that these systems won't be able to do. I'll probably be able to come up with variations on tic-tac-toe. So 5x5 board, but you can only win on the edges. There will be tons of problems like that. They're all kind of outlier-ish. Um, but I think that a year from now, these systems will still be very vulnerable to those outliers.\n\nI think that you'll probably still be able to come up with some things like this a year from now, but you'll, it'll be harder, and then it'll be even harder the next year and so forth. Um, we'll, it should be get monotonic, of course, but.\n\nI mean, here's, here's another way to put it is the AGI that I think we're afraid might be unconstrained or whatever. Um, there really shouldn't be a lot of edge cases like that.\n\nYeah.\n\nRight. I mean, it, that's why I focus on the superhuman coder milestone. Like, I, I don't, again, I don't think we just leap straight.\n\nDo you know about the Live Code Pro benchmark that just came out?\n\nUh, not much. Tell me about it.\n\nSo, so I don't know what the human data on it, but I know that the machine data on it was 0%. And what was interesting about the Live Code Pro benchmark, Live Code Bench Pro, excuse me, um, was that they were all brand new problems, and so they ruled out.\n\n\nData contamination. And I've seen two careful studies of ruling out data contamination. The other was with the US AMO, um, US math Olympiad problems, six hours after testing. Performance was terrible on them. Um, I think if you rule out data contamination, the performance on problems that are new is not good. And for programming, especially programming AGI, that's super relevant. You know, if you're using these things to code up a website, that's not new. There's so many examples. But if you're using it to do something that's new, you get out of distribution, there's still a lot of problems.\n\nWhat about Frontier Math? Wasn't that also, there's some contamination issues there? Um, you know, OpenAI had access to the problems. They said they didn't train on it though.\n\nI know they did. But what augmentation did they do? What did they do that's relevant to those problems, right? OpenAI is just not, I mean, you can agree with me on this, is not an entirely forthcoming place about what they did.\n\nSure. But aren't there, like, aren't, doesn't, like, Claude and Gemini now have, like, okay scores on Frontier Math? I haven't actually checked, but, like,\n\nI think they have okay, but everybody is teaching to the test right now, and they're all doing augmentation. We don't know what the augmentation is. The point is, if you have AGI, it's not just about teaching to the test anymore, and you need to be able to solve things that are original. I mean, we're lucky if AI never can do that because then it gives an Achilles heel.\n\nThe, or I guess you're pointing at that, uh, superhuman coding may still be around the corner if we, um, extrapolate out the benchmarks.\n\nI mean, I also just, I do, for the record, I disagree with you guys. Like, I think that progress is being made in this sort of, um, hard to measure, uh, dimension that you're pointing to. Uh, it's hard to measure. The best ways we have to measure it are for you to come up with examples. Uh, and let me give you a parallel, progress is being made.\n\nLet me give you a parallel, and then we'll come back to, I mean, sorry, let me just insert very quickly. Um, the parallel is to driverless cars. Um, in driverless cars, we know there's been progress made every year. Absolutely. But the distribution shift problem still remains. So, you know, even Whimo, who I think is maybe ahead in this, like, they just announced New York City, but when they go to New York City, they're going to have a safety driver there. They're going to have geo-fencing and so forth. The general form of driving solution would be level five. There'd be no geo-fencing. You wouldn't need specialized maps, etc. So, there's been progress for 40 years every year in driverless cars. But the distribution shift problem is really what is still hobbling that from being a thing everywhere as opposed to a thing in San Francisco.\n\nWell, we're also getting better at distribution, like, like we have gotten better.\n\nTo yesterday. It was, you know, it was great, but I took it here in San Francisco.\n\nThere's, there's still the, the problem of if it's getting better, there's a question of is it able to do full automation, which is more the key thing. And unfortunately, for other abilities that probably the ability has most, um, or, well, it's two main abilities would be crystallized intelligence, acquired knowledge, and reading writing ability. But even for reading writing ability, the models, and when you ask them to write an essay, if you, um, score them on, like, the GRE score out of six, they get, like, 4.5 or so. They're not particularly great writers, and we can't, you know, you can't automate, uh, writers that well with, um, if, if they're doing somewhat complicated writing. You still, people still need to be doing that, and that's somewhat surprising because they've had so much data, uh, they've had all the data in the world, basically. And likewise, for coding, they have had GitHub, so that at least there's it potentially, um, the case that as you extrapolate them out, they'll get a lot of the low-hanging fruit, but it crossing some sort of threshold for doing more automation or full automation that could actually require resolving some of these other bottlenecks that are, um, other, other cognitive abil bottlenecks that, uh, uh, that, that Gary's alluding to.\n\nI'm going to, um, put a hold on this discussion. I think we did a good job. We're not going to convince each other, but we laid out the issues, and that's great. I want to ask at least one other question because we've somewhat limited time. Um, do you think that we've made any progress on alignment? Do you think there's a conceivable solution to technical alignment? Are we close to it? Like, let's talk about the technical alignment problem. And I'll just put one thing out there, which is, I think, even though I don't think there's been as much progress as you do on capabilities, there's obviously been some, like, there, there's no argument there. I would say that a lot of it is sort of interpolation rather than extrapolation, and we haven't solved the extrapolation problem, but interpolation, we've made huge progress, mostly just in virtue of having more data and more compute. But, you know, there's no question that new systems are much better at interpolating, um, than previous systems, and that has lots of practical consequences for labor. For example, there's a whole bunch of things that you can use these systems to do that you couldn't use them before, that's already affecting labor markets. Whereas my intuitive sense is, on alignment, all we have is maybe eight, uh, human, uh, reinforcement learning, um, helps a little bit. So that, like, if you ask these systems the most obvious question, like, how do I build a biological weapon, they'll decline. And that's a little bit of progress, but, like, we all know that those are things are easily jailbroken, like, my view, and you can agree or disagree or whatever, is, like, in alignment, systems still don't really do what we want them to do. There's still kind of sorcerer's apprentice style problems. There are kind of problems of just, like, they don't quite fit, they don't do what we want. We have system prompts that say, don't produce copyrighted material, and they still do. Don't hallucinate, they still do. Like, they, you know, they can approximate what we ask for them, but, like, they don't really do what we ask them. I take, I take even, like, don't make illegal moves in chess to be a form of the alignment problem, like, a very simple microcosm of the alignment problem, and they're still struggling with that. That's my view. How about you guys? Why don't we start with you because you said this?\n\nYeah. Um, so there's some different notions of alignment or alignability, and, uh, I, I, I would distinguish between aligning proto, um, uh, super intelligences and aligning a sort of recursion that gives rise to super intelligence. Those are very qualitatively different. One is more model level, and one is more, uh, process level. So I don't think you're going to solve that process level, um, one of doing a recursion and fully de-risking that, anticipating all the unknown unknowns and solving a wicked problem in a nice, a clean way beforehand, which points to the necessity for resolving geopolitical competitive pressures and giving them an out to to proceed with a recursion more slowly or substantially for stall that. On the protoi things, I think that they follow the instru instructions fairly reasonably. There are some other parts of\n\nthat scares the out of me, like, fairly reasonably. Like, I mean, as you know, when these things are still in our hands, kind of, that's okay, but, like, if you have them guiding weapons or something like that, there, there are many circumstances where fairly reasonably is not good enough.\n\nYeah. No, I, I agree. They're definitely safety critical domains for it. And, uh, I, I think for, for instance, in bio, I think for refusal, for instance, in, in some high stakes context or given some high stakes queries, it, it depends. There are some cases where I think you can actually get multiple nines of reliability, such as with bioweapons refusal. However, for other types of refusal, like, don't, um, cause any criminal or don't take a criminal or torchious action, that's a lot fuzzier, and it's, I don't think\n\non the first one though, can I rewind? You probably saw Adam Gleav's postings a couple weeks ago. I can't remember who he was leaning on, but he, he described someone else's work saying that there was a very easy jailbreak. I think it was with Claude.\n\nYeah. So in production, they're not using, they're not using the techniques that are as adversarily robust because they come at a cost of maybe a percent or two in MMLU, and, uh, so, um, so they're not doing\n\nthe epetap for humanity if they hadn't squeezed out that last bit of MLU, we would have been okay. Uh, but, but actually, so I think there are some types of, there are some types of solutions for, um, some types of malicious use where you can have some nines of liability, but for, for the general problem of, of refusal, um, including for everyday, uh, uh, criminal and torches actions, which would be extremely relevant for, for agents and making them feasible, I don't see substantial progress. I mean, think about Asimov's law, by the way, which were written what, in the 40s or something like that, and you know, number one was, was don't let harm come to humans or whatever. Right. And, and the cleaned up version law is actually the law kind of cleanses it up and saying, don't cause foreseeable harm, because harm is way too strict, but foreseeable harm is what we demand of humans. And so\n\nthat one we're not doing that well.\n\nYeah. Yeah. That one we're not. Yeah. Yeah. And so, so I think for most of these, um, uh, reliability issues and safety issues, we'll keep seeing new symptoms crop up, and we'll have specific solutions that partly target them if there's willingness.\n\nCyber crime, a kind of constant cat.\n\nThat's right. And, and I, I expect that, um, uh, this game will keep continuing, um, and we won't get to a state where that is basically mostly managed in time, um, because the risk surface will keep evolving with agents that'll present new things, and we'll have to deal with those current cases that will create a substantial backlog, and we just won't have the adaptive capacity. And so, consequently, on both fronts, for aligning recursion and aligning protois, um, uh, we, the geopolitical competitive pressures make it such that we're probably not going to solve either problem.\n\nSo this is dark, and going back to the beginning of our conversation, it's a reason to stand in front of the train, especially a particular train. So let's say that there's one train that's about chatbots and people having fun with chatbots and using them for brainstorming and whatever that are not mission critical and safety critical, maybe it's fine, we just let people do that. And there are already some risks, like, around delusions that Kashmir Hill wrote about in the New York Times the other day, you probably saw, um, but the really safety critical things or maximally safety critical things, if, if things are as dark as you said, that is a reason to stand in front of that train right now and say, \"Look, if we can't do alignment well on the refusal, etc. Um, and the cause to harm, causing harm to humans, foreseeable harm, even, even to humans. That's a reason to say, 'Hey, we got to wait until we have a better solution here. If that takes 500 years, if it takes 5 years, like, in the safety critical stuff, isn't that a reason to, to, you know, slow things down a bit?'\"\n\nI think that that's, that's a direction or an interpretation of those facts that, that is reasonable. But there's, there's a question of incentives and what, you know, there might be a somewhat different aspect, better, you could go at it by incentives.\n\nYeah.\n\nYeah. So, so that, that's why in, in super intelligence strategy, we'll, um, suggest some different things, like, u, making the threat of preemption if you're crossing these sorts of lines, uh, as opposed to, um, stopping today because that is is a lot harder to\n\nI'll weaken my claim and say, is that a reason to, to intervene, I think, is the word.\n\nYeah.\n\nThat would be a reason to get.\n\nYeah, of course.\n\nRight. And, and, and yeah, like, to continue your, your metaphor with stopping the train, like, right now, if I were to step in front of the train, it would just sort of, like, run me over. Uh, and you need to have, like, quite a lot of people in front of the train for the weight of the bodies to, like, really slow it down. Um, I've been kind of trying on Twitter, and it's, you know, so,\n\nthere's some push back when I try to hold the train back.\n\nYeah. Yeah. So I mean, I'm not, I'm not actually advocating holding the train per se, but on these safety critical things, I really could see an argument for some kind of intervention, as, as you note, it could be, um, incentives rather than than moratorium, but my view is that we are not going to solve the technical alignment, the sort of nearer term technical alignment problem with, let me finish the sentence, um, with LLMs, that they're just not up to the task, maybe with neurosymbolic, we might have a chance. So neurosymbolic gives you a chance to state explicit constraints, that is very difficult in pure LLMs, and so I think there's reason to explore that avenue as a way, I don't know that it helps with super intelligence, but at least with the near-term use of these systems for safety critical measures, there might be some, some mileage to be gotten there.\n\nYeah, one, one, uh, last point on, on this for Daniel is, uh, I think in both cases, I, I, I think it's a problem to use the phrase solve like that because it's not something you can sort of do beforehand. In both cases, you want adaptive capacity and slack in the system, some sort of safety budget, some people who can put out the fires faster than they're emerging. And because as AIs continue to develop, they'll keep being new problems as they evolve. And likewise with recursion, we just see that, but on, on steroids, faster than emerging.\n\nSo with both of it's a resource thing, and I think it's less of a technical thing. The technical things can increase the capacity to deal with these problems or, uh, have more efficient solutions for, for some of these particular symptoms, um, or these, these new failure modes that crop up. But I, I'm not expecting a total, um, monolithic, uh, solution, um, uh, that a pause would necessarily give. I think you have to have the, the background context in both cases be that you're able to proceed, um, under some ris, proceed with development under some, uh, risk tolerance that's much lower than what there is today.\n\nUh, I mean, I agree, we're totally not on track to have figured out the alignment stuff in time. Uh, I can pontificate more on that, but I've talked a lot, so,\n\nso I mean, we should probably actually wrap up. So may, maybe, um, some final words. I'll start with some final words. Maybe I'll\n\n\nI think we actually agreed on a lot here. You know, our clearest disagreement is on forecasting. Even there, we're not probably as far apart as maybe people thought that we were, right? So, you know, I I push all of my probability mass five years out and have, you know, basically none before, and you've got some at three years out or two years out that I don't. Um, we both have some out at 2045. I have some even past then, and you may or may not. Um, we have slightly different methodologies that we have talked about. You were surprisingly coming to my aid, which I love. Happy moment for me. But, you know, we're not hugely apart, and I think we've acknowledged the value in some of the forecasting techniques that the other has used, even if we don't. So we're not hugely apart there. I think we're completely agreed that we're not doing a great job on the alignment problem and that we need to do much better and that there's a temporal dimension to that, as you were just saying, which is like, you know, it's not great for humanity if we solve that problem in 200 years and we have AGI or ASI, you know, in the next decade or two. Um, and I think we agree also that the current companies are not entirely trustworthy. Those are some of the things that we agree or don't disagree so much on. Like the broader picture will be remarkably aligned.\nYep, I would agree with that. Also, I just realized I forgot to ask you my big question about scenarios. Uh, do you still have time for a brief foray into that?\nYou guys can sit for a few minutes. I'll do that.\nSo, here's the way I would pose the question. You know, you've read *AI 2027*. Thank you for reading it. Um, if you were to write your own scenario of the future, what would it look like? And perhaps where would it start to branch off for me at 2027, for example? Would it look basically the same until 2027, or would it branch off earlier than that? And when it does branch off, can you sort of say what that looks like, you know?\nYeah. So, there's a couple different pieces. So, um, I think the speed at which things unfold in *AI 2027* is not plausible to me. I think by the end of the year, we will already be less far ahead on agents as you hypothesize there. Like, I think the first couple months, we're actually kind of in agreement, but, you know, there's a divergence in speed, um, for sure. Uh, I would say that... so, like, take a 2027 and then, like, double the length of everything, maybe, or triple it, or like, what would... what would you say?\nWell, I mean... or at least... Yeah. I think that the level of intelligence that the machines that you attribute to the machines towards the end of the essay, remember this, some of that actually happens after the year 2027, I think. Um, you know, I don't really think is very likely in the next decade, and I don't think is super likely in the decade after that, but it's certainly possible.\nWhat about the superhuman coder milestone? Do you think that that won't be happening in the next decade?\nProbably.\nI don't remember how you define that milestone. Basically, think about like these coding agents like Claude and so forth, but imagine that they just like actually work to the point where you can just treat them like a software engineer and chat with them and give them high-level instructions, and they'll just do as good a job as a very professional, excellent software engineer would have done.\nSo, I think apprentice engineer, we're actually close already.\nWell, I mean, top... I mean... but top software engineer, I don't think we're close. I I think that that requires an understanding of the problem, what humans want solved by the problem. It requires a deep understanding of various domains. I just don't think that we're that close to that. So, I do think these things will continue to improve regularly. Um, we will get more and more value out of them. They will improve programmer productivity with some asterisks around how secure is the code, how maintainable is the code, etc. Um, but I don't think that they're going to replace the best coders. You're not going to get a machine that is Jeff Dean anytime in the next decade. Like, I will be really surprised if you get a Jeff Dean level coder, right? You probably know, you know, some of the stories that the Chuck Norris stories that aren't really true, but, um, you know, he was able to look at problems that nobody had seen before about the distribution of, you know, these searches and coming up with advertisements over, you know, enormous scale that nobody had ever done before and pretty rapidly prototype and then make, you know, maybe with some help, some production-level solutions. Um, so take Jeff Dean as kind of, you know, our example. He deserves to be our example in this. I don't see Jeff Dean coming out of these systems soon. I just don't.\nUm, and then a little bit more on scenario. So, um, I think that the human mind sucks in light of scenarios that it takes them very seriously. The vivid details, I mean, there's lots of psychological literature on this, overwhelm people's ability to see things. What I would like to see, actually, would be a distribution of scenarios. So when you put Scott Alexander, who's a brilliant writer or at least a very, you know, compelling writer of a certain sort, um, into making one scenario vivid, everybody goes home and thinks that that scenario is real. But you and I know that that was one scenario of many. There's reasons to consider that scenario, and it's sort of, you know, the darker one is a very vivid version of the dark scenario. Um, but really, we want to understand the distribution of scenarios. And that's a lot more work. I'm sure it took you what, a few person-years or something like that to put together that report. There were multiple people involved. You probably worked on it for a while. And so it's a big ask, but what I would like to see is really a distribution of scenarios.\nWe're working on it. I I agree with the problem you're pointing out. Um, we currently have a project to make a good ending, so to speak, at a similar level of detail to what we already have, and then also a mini project to make a like more scrappy spread of possible scenarios illustrating different stuff at lower levels of detail, like just like a few pages.\nI I think that that would be helpful. My own personal scenario is, like, in three or four years, neurosymbolic AI starts to take off. Like, it's... I already see signs of this. AlphaFold just won a Nobel Prize. That's a nice thing for neurosymbolic AI. The conferences for it are getting bigger and and and so forth. Um, and I think eventually there will be a state change. I find it very hard to know when there will be a state change, but I think in 2035, we will look at LLMs and be like, \"Nice try.\" We still use them for some things, but that wasn't really the answer. Um, I think Yan LeCun would say the same thing. Again, despite our differences, I think we both think LLMs are not really the route to AGI and that when we get there, it's going to look pretty different. It might use LLMs. They're great at kind of distributional learning. It might replace them because they're very inefficient in terms of energy and data. So, somebody might find a better way to do the same kind of thing of learning the models of distributions of things, which is a super helpful cognitive skill. It's not the only one, but it's super helpful. But we'll have much better ways of doing reasoning and planning. We'll have much more stable world models. I think it will take five or 10 years to develop that. I think the semantics that the current models have is very superficial. It's really about distributions of words. Um, and we need a deeper one. Like, if you talk about \"three in a row,\" you should understand what a \"three in a row\" is. And I think we're missing something to get that. We will get it. Like, I don't think it's... and you don't think we'll get it after automating R&amp;D. We'll get it in, like, humans will come up with the ideas that get us there. I mean, I guess it relates. So, for me, automating R&amp;D, like, there's a plausible version and a much more distal version.\nSo, the near-term version is, like, a lot of people do a lot of experiments on LLMs, and I think you can automate a bunch of that. Um, but having genuinely new ideas has not been the forte of this. Somebody just did a paper, I'll try to dig up the reference, in which they looked at whether these systems develop new or are able to infer causal laws, and they're not that good at it. Like, I don't expect that an LLM is going to do Einstein-level look at a problem, come up with a completely different solution anytime soon. All the solutions they have seem to me to be kind of inside the box, and I think that getting to AGI is going to require outside-the-box solutions. They might automate the stuff inside the box, and inside the box, they may even do better than people. Like, the famous, was it move 37 or web from one of the Go championships? It's kind of inside the box. It was still within the realm of things that, you know, it's still within Go, you know, it's not outside the box the way that thinking about relativity, like, there's a whole different way to think about physics than we thought before. I think that we will need some Einstein-level innovations in order to get to AGI. Um, and certainly to get to ASI, and I don't expect that at least automating the current machines will do that.\nOkay. When we do get to AGI, how fast do you think the takeoff will be to ASI?\nYeah. Like, for example, you just mentioned now that, like, the current systems are very inefficient in terms of how much energy they need. Uh, that's a bit scary if you if you really think that in the next, you know, 10 years, human scientists doing neurosymbolic research will come up with much more efficient systems that are also better at generalizing. Uh, holy cow, that's that's like going to be orders of magnitude better in various dimensions than what we currently... I think, you know, whether I'm right about neurosymbolic AI or not, I think that there are orders of magnitudes more data efficiency to be carved out than than we have. I mean, you just look at human children, they don't need that much data.\nOkay. So you're saying it'd be orders of magnitude more data efficient, but still only about as data efficient as humans, or...?\nPossible, you could find better. I mean, humans are pretty good data efficiency-wise, but I doubt that they're at the, like, theoretical limits. There are some things in psychometrics where people really are at the theoretical limits. So, like, we can notice the presence or absence, I think, of a photon. Like, you can't do better than that. So, um, there are things where we're at the theoretical limits. There are things where we're not. Like, we are constrained very much, I argued in my book, *Cluge*, by a lack of location-addressable memory. And so, like, you know, my daughter just memorized 105 digits of pi, which I could never do, but it's still nothing compared to what a computer could do, memorized billions of digits of pi. And so, you know, there are some advantages to machines and places where they should really be doing better than people if we had the right way of writing the software. Um, so at least we should be able to get to human levels. Like, humans are existence proof of data efficiency, and humans are fabulously data efficient on many problems, not all. And then we have problems like we have cognitive impairments such that we have confirmation bias and motivated reasoning. Motivated reasoning is, like, um, I, you know, want this argument to be true, and so I I kind of, you know, play little games, and we all do this. I mean, scientists are better because we recognize the behavior in ourselves and try to self-correct. But scientists do it, too. Um, machines shouldn't need that. Like, some of our things are to use Freudian technology, that terminology, even though I'm not a Freudian. Um, we have ego-protective ways of reasoning. Machines should not need that, right? Like, I believe in my political party, and so when my political party does dumb things, then I go and try to, you know, come up with a rationale for it. Machines shouldn't need to do that kind of stuff. And so, in that way, you know, certainly the upper bound is going to be way beyond we are. I I did a panel once with Daniel Kahneman, and he's... he, well, he's not with us anymore, but he was very fond of these studies that showed that in certain domains, machines were already better than people, and these are problems basically of multiple regression, weighing multiple factors, and he was right. And I think I came back with some, you know, where the machines were not very good and people were better, and in the end, he said something like, \"You know, humans are a really low bar,\" and you know, his whole research... well, not his, he had many research, but one of his whole research lines was showing that humans, in fact, were pretty bad at all kinds of reasoning. So he says, \"Humans are a low bar. We're doing this panel.\" I said, \"Yeah, and machines still haven't met them. They will someday. They will exceed them. There's no question about that in my mind. It's a question of when and how and and so forth, but we really are a low bar because of the all of the kind of cognitive biases and illusions, the problems with memory. My book, *Cluge*, was all about this kind of stuff. There's absolutely room to do better. And yes, it could happen in 10 years. I don't, you know, probably it will happen on some of those dimensions and not others. It already did on, like, math, you know, 60 years ago or 80 years ago. Um, it will happen, you know, dimension by dimension, maybe several all at once when there's a breakthrough or something like that. But it will happen, and it could happen in 10 years. Again, I don't think it can happen in two. I think we're missing some ideas right now. We're missing some critical ideas, but when we get those critical ideas, they could go fast. Just like molecular biology, once Watson and Crick figured out, you know, DNA, things moved pretty fast. In 40 years, you know, now we can do CRISPR and stuff like that, or not 40 years, but se... 70 years, you know, in remarkable progress. There will be, I think, periods of AI progress that exceed the last few years. I know that the last few years feel like a lot to a lot of\n\n\nPeople, but I think in hindsight, you know, 30 years from now, AI will be enormously ahead of where we are now.\nI mean, almost on any of these kinds of projections, right?\nAnd we will be like, \"Yeah, a bunch of stuff happened, and they were really proud of themselves, but you know, the way that we look back at like flip phones are like, \"Yeah, those were kind of cool, but they didn't know about smartphones.\"\nI agree with everything you said about what we agree on.\nUh, we continue to disagree about what the next couple years are going to look like.\nI think that, um, well, I think it's going to look more like AI 2027, where the rather than sort of like tapering off and running into sort of rather than the the the limitations that you're talking about becoming bottlenecks that the companies can't work around, I think that they're going to be more like a series of road bumps that the companies sort of like bash through.\nI mean, that is that is the question.\nSo we should have a second edition of this two years from the day and see how this goes on.\nYeah.\nOkay.\nSo, so one thing worth clarifying since some people may be confused.\nI'm sort of in the middle of thinking through AI timelines and largely because I'm sort of trying to reflect on uh what intelligence is in a sort of multi-dimensional way.\nWe maybe got a little bit spoiled with reading writing ability and crystallized intelligence.\nI mean, progress over the past two years or so has been in mathematical ability and uh its short-term memory is also better.\nUh, but um, so I'm still wrapping my head around that, since hence I'm being more non-committal about some of these different forecasts, where I think maybe still seems very plausible, um, uh, more than plausible than by 2030, something that is has the cognitive abilities of a typical human, um, some system like that.\nUh, there's some differences, um, in how things might play out at a technical level, um, in AI 2026 of it.\nI don't think much really goes through technical mechanistic interpretability or technical solutions really solving much of it.\nI think you need uh to really ease the geopolitical competitive pressures.\nI think the main dynamics that make way for that are uh transparency and the espion and how easy it is to do espionage as well as the sabotageability of that, which I think are very important dynamics that aren't uh that are in some ways reflected in there, but not totally captured on on sabotageability, for instance, if China were interested in stopping the US, they could do some sort of cyber attack on some power utilities, but say that that doesn't work, they can also there's there's just there's basically a lot of vulnerabilities uh that they can exploit.\nFor instance, they could they could from a few miles away snipe the power plants transformers and that would take down the the data center.\nSo I think that that and there's lower attributability.\nWas it Russia?\nWas it Iran?\nWas it China?\nWas it some uh US citizen as an example?\nSo I think that affects the strategic dynamics where I speak about that in in super intelligence strategy as well as I think the transparency that China has to the US would be relatively high.\nRight now it's a matter of just hacking Slack and then you can see anthropic Slack, you can see OpenAI Slack, you can see XAI slack, uh Google Deep Mind Slack.\nSo you can have very high transparency there and hack the phones of of top leadership as well.\nUm, so uh, so this this paints a you know, in some ways a different picture, but I think we would agree that we want to um uh work toward a verification regime so as to have red lines around things like intelligence explosions and and things like that.\nI won't really say anything more substantive, but I will say this.\nI thought this was a fantastic conversation.\nUm, I hope that it won't be cut too much.\nUm, because it was really interesting, and I I salute anybody who made it through watching the entire thing.\nUm, we we got pretty technical at times and really laid out I think where the state of play is today, which was my fondest hope.\nI I think we did a great job with that.\nThank you, gentlemen.\nShake hands for the camera.\nAll right.\n",
  "dumpedAt": "2025-07-21T18:43:25.051Z"
}