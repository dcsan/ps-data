{
  "episodeId": "vC9nAosXrJw",
  "channelSlug": "@machinelearningstreettalk",
  "title": "Wild breakthrough on Math after 56 years... [Exclusive]",
  "publishedAt": "2025-05-14T18:33:20.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "We've had exclusive early access to the",
      "offset": 0.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "brand new Google Alpha Evolve paper",
      "offset": 2.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "which just got released one minute ago.",
      "offset": 4.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "We did a technical interview with the",
      "offset": 7.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "authors before anyone else. The paper",
      "offset": 8.72,
      "duration": 5.999
    },
    {
      "lang": "en",
      "text": "itself drops a bombshell setting world",
      "offset": 12.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "records for many algorithmic and",
      "offset": 14.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "mathematical challenges. In the world of",
      "offset": 16.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "computer science, few problems are as",
      "offset": 18.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "fundamental as matrix multiplication.",
      "offset": 21.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "For over half a century, a specific",
      "offset": 24.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "efficiency benchmark in this domain,",
      "offset": 26.4,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "particularly for 4x4 matrices seemed",
      "offset": 28.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "insurmountable. The search space for",
      "offset": 32.12,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "optimal algorithms is immense, making",
      "offset": 34,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "exhaustive exploration practically",
      "offset": 36.399,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "impossible even for relatively small",
      "offset": 38.52,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "matrices. In 1969, Vulkist Drason",
      "offset": 40.879,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "revolutionized the field by discovering",
      "offset": 44.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "an algorithm to multiply two 2x2",
      "offset": 46.239,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "matrices using only seven scalar",
      "offset": 49.2,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "multiplications down from the standard",
      "offset": 52.199,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "8. The established best practice for",
      "offset": 54.52,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "larger matrices like 4x4s was to apply",
      "offset": 57.12,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "Straen's 2x2 method",
      "offset": 60.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "recursively. For 4x4 matrices, this",
      "offset": 62.92,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "meant 7 * 7 resulting in 49 scalar",
      "offset": 65.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "multiplications.",
      "offset": 69.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Today, Alpha Evolve beat this record.",
      "offset": 71.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "Google DeepMind has a long history of",
      "offset": 74.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "building AI systems which actually",
      "offset": 76.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "invent new knowledge through",
      "offset": 78.32,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "experimentation and iteration rather",
      "offset": 80.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "than just building a glorified database.",
      "offset": 82.479,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "We saw Alph Go, which beat Lisa Dole,",
      "offset": 85.2,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "learning from human games and even",
      "offset": 87.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "surpassing champions through self-play.",
      "offset": 89.759,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "Alpha Zero was purely selfplay and Alpha",
      "offset": 92.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Fold predicted millions of 3D protein",
      "offset": 96.32,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "structures that had never been measured.",
      "offset": 98.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "experimentally and alpha dev discovered",
      "offset": 100.759,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "faster sorting algorithms. Google",
      "offset": 103.28,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "DeepMind recently has been focused on",
      "offset": 105.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "scientific discovery with Alpha Tensor,",
      "offset": 107.439,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "which framed the problem of finding",
      "offset": 109.92,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "faster matrix multiplication algorithms",
      "offset": 111.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "as a game achieving breakthroughs. And",
      "offset": 113.759,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Fun Search took us even further using",
      "offset": 116.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "large language models to find new",
      "offset": 118.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "mathematical solutions by evolving code.",
      "offset": 120.88,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "And now, Alpha Evolve represents the",
      "offset": 124,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "next stage in this lineage. for the",
      "offset": 126.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "usual case of matrices that have",
      "offset": 129.119,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "arbitrary numbers in them. Still,",
      "offset": 131.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "nothing better was known than doing",
      "offset": 132.879,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "stress and twice using 49",
      "offset": 134.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "multiplications. So, we're like really",
      "offset": 136.68,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "excited when we used alpha evolve in",
      "offset": 139.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this setting. We actually didn't even",
      "offset": 142,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "hope that it would find something better",
      "offset": 144.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "than 49 because we're trying for so long",
      "offset": 145.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "with alpha tensor. We just ran it for",
      "offset": 147.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "completeness because we wanted to have",
      "offset": 150.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this, you know, this table in the paper",
      "offset": 151.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that we actually have that we tried on",
      "offset": 153.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "all the sizes up to up to five or six",
      "offset": 155.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and and remarkably it found a faster",
      "offset": 157.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "algorithm which uses 48 instead of 49",
      "offset": 160.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "multiplications.",
      "offset": 162.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "It's a bit like cursor on steroids. It",
      "offset": 164.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "iteratively refineses algorithms drawing",
      "offset": 167.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "on the creative power of LLMs using",
      "offset": 169.519,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "metalarning, library learning, automated",
      "offset": 172.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "evaluation and evolutionary search. The",
      "offset": 174.959,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Alpha Evolve paper describes it as an",
      "offset": 177.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "evolutionary coding agent which",
      "offset": 180.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "substantially enhances capabilities of",
      "offset": 182.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "pre-trained large language models on",
      "offset": 184.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "difficult tasks. Of course, their their",
      "offset": 186.8,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "first paper was fun search which was",
      "offset": 189.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "very very similar. The main difference I",
      "offset": 190.879,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "think was that it was just searching for",
      "offset": 193.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "a single function rather than alpha",
      "offset": 195.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "evolve which can essentially work over",
      "offset": 198.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "an entire codebase. to just demarcate",
      "offset": 200.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "um adaptable regions in a codebase and",
      "offset": 203.12,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "it can search for those and of course",
      "offset": 205.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it's even optimizing for interactions",
      "offset": 207.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "between those those functions in",
      "offset": 209.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "different parts of your codebase. One",
      "offset": 211.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "potential issue as Keith Dugar pointed",
      "offset": 213.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "out is the classic halting problem in",
      "offset": 215.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "computer science. But there's also a",
      "offset": 217.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "really subtle limitation here I wanted",
      "offset": 219.519,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to explore with you guys a bit which is",
      "offset": 222,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "running the programs themselves. You",
      "offset": 224.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "know we we face issues right? you face",
      "offset": 227.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "issues of the halting problem for one",
      "offset": 229.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thing like you start running some code",
      "offset": 231.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like the code compiles fine or doesn't",
      "offset": 233.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "crash immediately and it it starts",
      "offset": 235.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "running but you know maybe some time",
      "offset": 237.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "goes by like an hour and you're like",
      "offset": 240,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "well gez is this thing ever going to",
      "offset": 242.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "terminate in in theory you're of course",
      "offset": 244.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "right you can never tell that if you run",
      "offset": 246.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "an algorithm for longer what it would",
      "offset": 248.239,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "have done but in practice it actually",
      "offset": 249.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "hasn't been any sort of issue for us in",
      "offset": 251.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the applications that we looked at",
      "offset": 254.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Alexander Novikov added that this",
      "offset": 256,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "challenge is very much like a",
      "offset": 257.84,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "fundamental aspect of human research",
      "offset": 259.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "too. How do you know that you should",
      "offset": 261.759,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "stop working on your problem as a human?",
      "offset": 263.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Uh like or maybe like maybe you spend a",
      "offset": 265.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "month more and then you solve it, right?",
      "offset": 267.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "It's it's it's hard. I mean, of course,",
      "offset": 268.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's still there's still limitations.",
      "offset": 270.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "They they didn't bite much on my uh my",
      "offset": 272.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "halting problem, you know, limitations.",
      "offset": 274.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So, like in practice, you know, it was",
      "offset": 276.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "it was it was okay for them, but they",
      "offset": 278.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "had these very well-chosen well-chosen",
      "offset": 280.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "problems, right? were like, &quot;Okay,",
      "offset": 283.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "matrix multiplication, you know, if it's",
      "offset": 285.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "any slower than Strassen's algorithm,",
      "offset": 286.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "you know, maybe you can you can turn it",
      "offset": 289.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "off, but the problem is that that",
      "offset": 291.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "restricts your open-ended search",
      "offset": 293.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "capabilities, right? You can't go down",
      "offset": 294.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the path of a potentially slower",
      "offset": 296.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "algorithm for now that may get you to a",
      "offset": 298.8,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "stepping stone that hops you over to",
      "offset": 301.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "something more efficient, right?&quot; It is",
      "offset": 303.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "interesting, isn't it, that so many",
      "offset": 306,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "breakthroughs in science are unknown",
      "offset": 307.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "unknowns. We might have a nose for what",
      "offset": 309.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is interesting, but we never really know",
      "offset": 311.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "where to find the answers. And so often",
      "offset": 313.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's the case that we find them through",
      "offset": 316,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "sheer luck. But there must be at least",
      "offset": 317.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "one step we can take in the direction of",
      "offset": 319.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "mechanizing and accelerating this",
      "offset": 322.199,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "process. In the case of matrix",
      "offset": 324.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "multiplication and several other",
      "offset": 326.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "scientific problems which had a clear",
      "offset": 328.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "evaluation function, this new",
      "offset": 330.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "evolutionary approach achieved much",
      "offset": 332.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "which decades of human research could",
      "offset": 335.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "not. We just ran it for completeness",
      "offset": 337.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "because we wanted to have this, you",
      "offset": 340,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "know, this table in the paper that we",
      "offset": 341.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "actually have that we tried on all the",
      "offset": 342.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "sizes up to up to five or six and and",
      "offset": 344.8,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "remarkably it found a faster algorithm",
      "offset": 347.84,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "which uses 48 instead of 49",
      "offset": 349.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "multiplications. Yeah. When when when",
      "offset": 352.039,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "one of my teammates messaged on the",
      "offset": 354.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "channel that oh it seems like we have",
      "offset": 355.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "this result. I just couldn't believe it.",
      "offset": 357.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "Like let's triple check it. AI's growing",
      "offset": 358.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "ability to generate entirely new",
      "offset": 360.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "provably correct algorithms can advance",
      "offset": 362.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the frontier of science. And the really",
      "offset": 365.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "cool thing is that Alpha Evolve has",
      "offset": 368,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "already been applied to optimize",
      "offset": 369.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "missionritical real world systems within",
      "offset": 371.24,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "Google. They're able to speed up really",
      "offset": 374.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "already heavily optimized pieces of",
      "offset": 377.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "important computational infrastructure",
      "offset": 379.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "within Google. For instance, take",
      "offset": 381.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Google's massive data centers",
      "offset": 383.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "efficiently. Scheduling computing jobs",
      "offset": 385.36,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "is a really complex operation. If done",
      "offset": 387.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "suboptimally, expensive servers will sit",
      "offset": 390.039,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "around idling. The Google engineers",
      "offset": 392.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "placed a candidate solution into Alpha",
      "offset": 394.639,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Revolve and then evolved a smarter",
      "offset": 396.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "huristic which assigned jobs to machines",
      "offset": 399.039,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "much more efficiently. They said post-",
      "offset": 401.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "deployment measurements across Google's",
      "offset": 404.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "fleet confirmed the simulator's results,",
      "offset": 406.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "revealing that this remarkably simple",
      "offset": 409.36,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "yet effective function continuously",
      "offset": 411.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "recovers on average 0.7% of Google's",
      "offset": 414.479,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "fleetwide compute resources which would",
      "offset": 418.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "have otherwise been stranded. Now that's",
      "offset": 420.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a huge saving on Google's scale, right?",
      "offset": 422.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And in another instance of",
      "offset": 425.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "self-improvement, it even found ways to",
      "offset": 426.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "accelerate the training of the very",
      "offset": 429.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Gemini models which powers alpha evolve",
      "offset": 431.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "itself. We have found a way to speed up",
      "offset": 434.479,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the training of the next version of",
      "offset": 437.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Gemini by by 1% and we've been able to",
      "offset": 438.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "unstrand resources in in the in the B",
      "offset": 442.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "data center. This instance was also",
      "offset": 444.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "interesting because it didn't generate",
      "offset": 446.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the solutions but also the programs",
      "offset": 448,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "which generated them. Whenever you look",
      "offset": 450.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "at the index in the for loop, you just",
      "offset": 453.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "look at the index module of four. And",
      "offset": 454.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "when you index into the the array,",
      "offset": 456.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "you're indexing at position i and i + 4",
      "offset": 458.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and i plus 8. Like okay, what's going",
      "offset": 461.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on? And and just by inspecting the code,",
      "offset": 463.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we actually were able to develop can",
      "offset": 465.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "think of this like a mathematical",
      "offset": 467.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "insight or a mathematical hypothesis.",
      "offset": 468.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And that hypothesis turned out to be",
      "offset": 470.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "really crucial for then improving the",
      "offset": 472.72,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "result. A key aspect of Alpha Evolve",
      "offset": 474.639,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "sophistication lies in its flexible",
      "offset": 476.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "approach to representing the search",
      "offset": 478.879,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "problem. The alarms will propose you",
      "offset": 480.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like a broad range of things and some of",
      "offset": 482.639,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "them will be stupid, some of them will",
      "offset": 484.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "be amazing, some of them will be like",
      "offset": 485.599,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "really weird and then by having the",
      "offset": 487.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "ability you can filter through those and",
      "offset": 489.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "identify the ones that are actually kind",
      "offset": 491.12,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "of important and improving things. So,",
      "offset": 492.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "isn't it really cool that rather than",
      "offset": 494.879,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "trying to generate the solution itself,",
      "offset": 496,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Alpha Revolve can, just like Inception,",
      "offset": 497.759,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "generate the thing which generates the",
      "offset": 500.319,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "solution. We should play some Inception",
      "offset": 503.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "music in the background. The thing we",
      "offset": 505.919,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "found really interesting about Alpha",
      "offset": 508,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "Revolve is that it's still very much a",
      "offset": 509.199,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "humans in the loop thing. Humans",
      "offset": 510.879,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "identify what's interesting. They find",
      "offset": 513.039,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "problems that have clear evaluators.",
      "offset": 514.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "They place candidate solutions in the",
      "offset": 517.599,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "loop and then alpha evolve will traverse",
      "offset": 519.68,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "this cone of possibilities and make",
      "offset": 522.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "jumps along the way and then the cycle",
      "offset": 523.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "continues. So this is very much",
      "offset": 526.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sketching a future of AI where there is",
      "offset": 528.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a strong collaborative loop between",
      "offset": 530.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "humans and AIs and we should just bring",
      "offset": 532.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "in how you modeled this representation.",
      "offset": 535.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So you had these different approaches.",
      "offset": 538.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "You could directly model the solution.",
      "offset": 540,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "You could model a constructor function.",
      "offset": 542.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "So, you know, you're actually learning a",
      "offset": 544.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "function which itself constructs the",
      "offset": 545.839,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "solution. You could be learning a search",
      "offset": 547.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "algorithm which is what you did with the",
      "offset": 549.36,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "matrix multiplication. And you also",
      "offset": 550.64,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "spoke about um co-evolution as as a",
      "offset": 552.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "possibility. So many people are talking",
      "offset": 554.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "about this vision of AIs which can",
      "offset": 556.8,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "autonomously just drive cars or just do",
      "offset": 559.08,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "anything generate content without any",
      "offset": 562.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "supervision from humans. And that hasn't",
      "offset": 565.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "really panned out to be honest. And",
      "offset": 567.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "certainly a lot of the content on the",
      "offset": 569.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "internet is slop. Do you remember that",
      "offset": 571.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "dead internet theory by Illuminati",
      "offset": 573.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "pirate? This guy on an internet forum a",
      "offset": 576.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "couple of years ago. Uh he said that by",
      "offset": 578.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "now most content on the internet will be",
      "offset": 581.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "generated by AI and it will have a kind",
      "offset": 583.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of superficiality to it. And he was",
      "offset": 585.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "right. But it doesn't necessarily mean",
      "offset": 588.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that AI is bad. Right? What's missing is",
      "offset": 590.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that we need to have this exchange. We",
      "offset": 592.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "need to use AIS as tools and we need to",
      "offset": 594.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "guide them and refine the results and do",
      "offset": 597.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "the process iteratively. That's kind of",
      "offset": 600.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "what Alpha Revolve does. It it",
      "offset": 603.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "mechanizes the the correct way of using",
      "offset": 605.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "AI. The thing that makes uh Alpha Evolve",
      "offset": 608.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "so cool and powerful is is kind of this",
      "offset": 612,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "back and forth between humans and",
      "offset": 614,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "machines, right? And like the humans ask",
      "offset": 615.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "questions. So the the system gives you",
      "offset": 617.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "some form of the answer and then you",
      "offset": 619.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like improve your intuition. you you",
      "offset": 621.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "improve your question answering a",
      "offset": 622.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "question asking ability, right? And you",
      "offset": 624.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "ask more questions. I gave this this",
      "offset": 626.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "talk a few times about like generation",
      "offset": 628.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "AI and the big warning I gave is just",
      "offset": 630.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the rise of mediocrity. We're just going",
      "offset": 633.36,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "to be inundated and flooded with",
      "offset": 635.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "mediocrity and the best content will",
      "offset": 638.839,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "still be produced by the most skilled",
      "offset": 641.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "people. And all that's going to happen",
      "offset": 643.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is as this tide of mediocrity grows",
      "offset": 645.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "larger and larger, people will get",
      "offset": 648.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "hungrier and hungrier for for the cream,",
      "offset": 650.32,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "for the things that are rising to the",
      "offset": 654.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "top, right? So, um I think all that's",
      "offset": 656.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "going to happen in the end is that",
      "offset": 659.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "skilled people, their productivity is",
      "offset": 661.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "going to just rise um and they'll",
      "offset": 663.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "they'll continue to be differentiated",
      "offset": 665.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "from from the mediocre sort of hordes",
      "offset": 667.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that are whose productivity is also",
      "offset": 670.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "enabled. So, you know, the you know, the",
      "offset": 672,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "hordes become more productive and and",
      "offset": 674.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "experts become more productive and and",
      "offset": 676.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just as a whole, we all become more",
      "offset": 678.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "productive. And while we're on the",
      "offset": 680.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "subject of amazing innovative",
      "offset": 682,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "architectures for discrete program",
      "offset": 684.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "synthesis and reasoning and whatnot, why",
      "offset": 686.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "don't you consider applying to work at",
      "offset": 689.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "two for AI labs? That's if you're an ML",
      "offset": 691.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "research scientist or ML engineer. Um,",
      "offset": 694,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Benjamin Cruette is running the lab.",
      "offset": 697.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "It's in Zurich at the moment and they're",
      "offset": 698.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thinking about opening an office in San",
      "offset": 700.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Francisco as well. They would love for",
      "offset": 702.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you to get in contact with them and",
      "offset": 705.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "apply if you're interested in working",
      "offset": 706.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "for them. So give Benjamin a shout guys.",
      "offset": 708.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Welcome to MLST. It's an honor to have",
      "offset": 711.519,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "you both here. Thank you for having us.",
      "offset": 713.12,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "So we have had privileged access to read",
      "offset": 717.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "your Alpha Evolve paper. It's really",
      "offset": 719.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "really exciting because this is very",
      "offset": 722.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "much up our street. We love program",
      "offset": 724.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "synthesis and we love evolutionary",
      "offset": 726.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "methods. I mean we've had Kenneth",
      "offset": 728.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Stanley on and um uh you know Jeff Cloon",
      "offset": 729.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I was speaking to him at Nurips and of",
      "offset": 733.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "course he wrote the map elites paper",
      "offset": 734.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "which influenced some of the stuff that",
      "offset": 736.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that you guys have done but you know",
      "offset": 738.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just just from a thousand miles away",
      "offset": 740,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "could you describe the the work that",
      "offset": 742.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "you've done? Yeah. So we are presenting",
      "offset": 744.639,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "a coding agent which we called alpha",
      "offset": 747.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "evolve. And what this agent is able to",
      "offset": 749.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "do it's uh it's able to design quite",
      "offset": 752.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "advanced algorithms. When I say advanced",
      "offset": 754.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "I mean it's algorithms that are able to",
      "offset": 757.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "make new discoveries in the sciences and",
      "offset": 759.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we have quite a few examples in",
      "offset": 761.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "mathematics and computer science in this",
      "offset": 763.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "paper. or on the practical side of",
      "offset": 765.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "things, they're they're able to speed up",
      "offset": 767.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "really already heavily optimized pieces",
      "offset": 769.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of important computational",
      "offset": 772.16,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "infrastructure within Google.",
      "offset": 773.6,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Yeah, I'm curious. I mean, what uh what",
      "offset": 777.92,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "made you go down the path of including",
      "offset": 781.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "EA algorithms, you know, evolutionary",
      "offset": 784.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "algorithms? what um what kind of said to",
      "offset": 786.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you guys this is a component that we",
      "offset": 789.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "need in a hybrid system along with you",
      "offset": 791.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "know verifiers and LLM and whatnot to to",
      "offset": 795.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "take us in a step forward.",
      "offset": 798.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yeah, I think if you consider the",
      "offset": 800.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "process of scientific discovery then",
      "offset": 802.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "it's a very natural choice. uh and you",
      "offset": 804.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "mentioned you you spoke to Kenneth and",
      "offset": 807.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "uh like evolutionary algorithms on a",
      "offset": 809.519,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "high level they give you this this",
      "offset": 811.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "diversity of the in the explor in the",
      "offset": 812.959,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "exploration process making sure that you",
      "offset": 815.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "don't early on in the process kind of",
      "offset": 818.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "just zoom in on into a particular",
      "offset": 820.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "approach which might be sub-optimal in",
      "offset": 822.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the end but you keep exploring the vast",
      "offset": 824,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "array of possibilities that you have and",
      "offset": 826.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "especially when you think about trying",
      "offset": 828.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to solve like really difficult problems",
      "offset": 830.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and making new scientific discoveries",
      "offset": 833.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then a pri priority there is no way of",
      "offset": 835.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "knowing what is going to be the right",
      "offset": 837.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "approach. So you do need to make sure",
      "offset": 839.12,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "that you keep exploring different",
      "offset": 841.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "possibilities and evolutionary",
      "offset": 842.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "algorithms are just a good technical",
      "offset": 844.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tool that fit the bill uh really well",
      "offset": 847.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for this purpose and also I think it's",
      "offset": 848.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "they're like very fun to play with right",
      "offset": 851.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like if you want to set up an RL",
      "offset": 853.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "algorithm it's it's going to take you",
      "offset": 855.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like I mean depending on on the",
      "offset": 856.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "algorithm I guess but it's going to take",
      "offset": 858.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "some some time right and with uh EA",
      "offset": 859.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "algorithms you can just do it right like",
      "offset": 862.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it's you have LM APIs you just call",
      "offset": 864,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "things you try things it's it's",
      "offset": 866.639,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Alex, can you just um just sketch out",
      "offset": 870,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 872.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "architecture? Many folks at home will",
      "offset": 872.839,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "will be familiar with fun search for",
      "offset": 874.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "example and I guess this is an evolution",
      "offset": 876.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of that, you know, pun intended, but um",
      "offset": 878.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "can can you just kind of sketch out how",
      "offset": 881.36,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "the whole thing works? Yeah, of course.",
      "offset": 882.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Um I mean should I assume that people",
      "offset": 884.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "are aware or should I start with from",
      "offset": 887.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "scratch with fans stuff bits? Yeah,",
      "offset": 889.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "let's keep this bit really really really",
      "offset": 892.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "simple. We'll kind of um the way we'll",
      "offset": 894.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "do the show is we'll have a progressive",
      "offset": 895.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "disclosure of complexity. So we'll have",
      "offset": 897.279,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "a have a hook and it'll be super super",
      "offset": 898.72,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "broad and then we'll kind of",
      "offset": 900.56,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "progressively get more detail. But you",
      "offset": 901.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "know this bit just do like super high",
      "offset": 903.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "level. How does the whole thing work?",
      "offset": 905.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Yeah, makes sense. And we'll also we'll",
      "offset": 906.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "also show you know diagrams from your",
      "offset": 908.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "paper and any others you give us and you",
      "offset": 910.399,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "know animations and some visual aids.",
      "offset": 912.639,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Awesome. Uh yes. So the high level",
      "offset": 916.44,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "architecture of alpha evolve is it's an",
      "offset": 919.519,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "evolutionary method where you um",
      "offset": 921.68,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "basically pair the so we only focus on",
      "offset": 924.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "problems where you have a a way of",
      "offset": 927.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "evaluating the progress right like for",
      "offset": 930,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "any given proposal for for any given",
      "offset": 931.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "like piece of code that the system gives",
      "offset": 933.199,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "you you can automatically test if it's",
      "offset": 934.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "good or not and like how good it is and",
      "offset": 936.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "this is I think the key aspect of like",
      "offset": 939.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "maybe the results we got that like this",
      "offset": 941.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "having this simulator is on the one hand",
      "offset": 943.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it restricts you to the set of problems",
      "offset": 946.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "where you you have you can have it but",
      "offset": 947.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "on the other hand it's quite a broad",
      "offset": 950.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "range of problems and then it gives you",
      "offset": 952.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "a lot right like you can quickly iterate",
      "offset": 954.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "and like get feedback and what in",
      "offset": 956.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "particular it gives you is that you can",
      "offset": 959.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "pair the creativity of LMS with the",
      "offset": 960.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "simulator right so like the LMS will",
      "offset": 963.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "propose you some kind of like a broad",
      "offset": 965.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "range of things and some of them will be",
      "offset": 969.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "stupid some of them will be amazing some",
      "offset": 970.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of them will be like really weird and",
      "offset": 972.24,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "then by having the ability you can",
      "offset": 974.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "filter through for those and identify",
      "offset": 975.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "the ones that are actually kind of uh",
      "offset": 977.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you know important and improving things",
      "offset": 979.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and then this kind of pairing LMS with",
      "offset": 981.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh evaluators is kind of wrapped around",
      "offset": 984,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "in an uh evolutionary pipeline which try",
      "offset": 986.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "to iteratively identify the most",
      "offset": 989.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "promising pieces of code and then like",
      "offset": 991.279,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "focus on improving those and like",
      "offset": 992.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "exposing them to the L like here's what",
      "offset": 994.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we tried before this thing worked this",
      "offset": 996.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "thing doesn't work like please try to",
      "offset": 997.839,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "propose a new thing and then maybe the",
      "offset": 999.6,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "final ingredient is scale right like uh",
      "offset": 1001.759,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "making those things in parallel",
      "offset": 1003.759,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So you you mentioned that needing the",
      "offset": 1007.8,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "evaluator it limit it limits the class",
      "offset": 1010.48,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "of problems. But there's also a really",
      "offset": 1012.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "subtle limitation here I wanted to",
      "offset": 1014.279,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "explore with you guys a bit which is",
      "offset": 1016.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "running the programs themselves. You",
      "offset": 1019.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "know we we face issues right? You face",
      "offset": 1021.519,
      "duration": 4.001
    },
    {
      "lang": "en",
      "text": "issues of the halting problem for one",
      "offset": 1023.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "thing like you start running some code",
      "offset": 1025.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like the code compiles fine or doesn't",
      "offset": 1027.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "crash immediately and it it starts",
      "offset": 1030.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "running but you know maybe some time",
      "offset": 1032.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "goes by like an hour and you're like",
      "offset": 1034.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "well geez is this thing ever going to",
      "offset": 1036.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "terminate? You know is this going to",
      "offset": 1038.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "contribute to my gradient or not? I",
      "offset": 1040,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "don't know. So maybe I have to just",
      "offset": 1042.319,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "terminate it after some resources are",
      "offset": 1043.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "consumed and but if I waited just you",
      "offset": 1045.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "know five more minutes I would have",
      "offset": 1048.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "gotten an answer that was the godly",
      "offset": 1050.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "algorithm right so like this fundamental",
      "offset": 1052.16,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "problem how do you how do you deal with",
      "offset": 1055.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it now how do you think we're ever going",
      "offset": 1057.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "to get around issues like that you know",
      "offset": 1059.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "what do you envision for overcoming that",
      "offset": 1062.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "limitation so in in theory you're of",
      "offset": 1064.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "course right you can never tell that if",
      "offset": 1067.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you run an algorithm for longer what it",
      "offset": 1069.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "would have done but in practice is it",
      "offset": 1070.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "actually hasn't been uh any sort of",
      "offset": 1072.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "issue for us in the applications that we",
      "offset": 1074.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "looked at and one concrete thing I can",
      "offset": 1076.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "say is that often uh you might frame the",
      "offset": 1078.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "problem in the way where the kind of",
      "offset": 1081.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "time constraint is built into the pro",
      "offset": 1083.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "problem definition. So let's say you",
      "offset": 1085.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "could say uh I'm trying to solve this",
      "offset": 1087.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "open problem in mathematics and I'm",
      "offset": 1090.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "looking for a search algorithm that is",
      "offset": 1092.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "able to make progress on this open",
      "offset": 1094.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "problem but I want a search algorithm",
      "offset": 1095.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that is able to make progress in 10",
      "offset": 1098.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "minutes. So that is part of my problem",
      "offset": 1100.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "definition. And then when I'm evaluating",
      "offset": 1102.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the proposals that the language models",
      "offset": 1104.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "make, I only run them for the 10",
      "offset": 1106.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "minutes. And so I I only explore the",
      "offset": 1108.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "space of algorithms that is able to make",
      "offset": 1110.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "something happen within those 10",
      "offset": 1112.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "minutes. Sure, I might miss out on",
      "offset": 1113.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "algorithms that would have done even",
      "offset": 1116,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "better when run for longer. Um so that",
      "offset": 1117.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is indeed like a principle thing that",
      "offset": 1120.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you can never eliminate, but in practice",
      "offset": 1122.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it actually hasn't been a an issue for",
      "offset": 1125.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "us.",
      "offset": 1127.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "And I guess uh it's kind of this",
      "offset": 1128.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "question is also fundamental to internal",
      "offset": 1130.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "research, right? Like how do you know",
      "offset": 1132.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that you should stop working on your",
      "offset": 1134.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "problem as a human? Uh like or maybe",
      "offset": 1135.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "like maybe you spend a month more and",
      "offset": 1137.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "then you solve it, right? It's it's it's",
      "offset": 1139.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "hard. I don't know.",
      "offset": 1141.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yeah, it's like the secretary problem.",
      "offset": 1144.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Um, one thing I'm I'm fascinated in, you",
      "offset": 1146.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know, interviewing so many people in the",
      "offset": 1149.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "space is, you know, we we speak about",
      "offset": 1150.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "diversity, preservation, novelty,",
      "offset": 1152,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "serendipity, open-endedness, creativity,",
      "offset": 1154.44,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "and we really want to design algorithms",
      "offset": 1157.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that can break free, that can make",
      "offset": 1159.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "creative jumps. I mean, Demis spoke",
      "offset": 1162.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "about this ladder of creativity where",
      "offset": 1164.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you have inventive creativity. That's",
      "offset": 1166.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the thing that we really need. And as I",
      "offset": 1168.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "understand it now in in your system, uh",
      "offset": 1170,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's a little bit like an automated",
      "offset": 1172.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "version of cursor where you kind of have",
      "offset": 1173.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "these code gates and you put an initial",
      "offset": 1175.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "solution in there. So there's a little",
      "offset": 1177.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "bit of domain knowledge. There was one",
      "offset": 1178.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "example where I think you decided to use",
      "offset": 1180.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a bin packing um algorithm for doing",
      "offset": 1181.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "theuling on you know on on the um uh the",
      "offset": 1184.32,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "hardware um you know at Google. And I",
      "offset": 1187.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "guess the question is is that depending",
      "offset": 1190.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "on the starting solution there's a kind",
      "offset": 1192.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of cone of things of leaps that we can",
      "offset": 1194.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "make and can you speak to any examples",
      "offset": 1197.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "where the system made really imaginative",
      "offset": 1200.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "leaps.",
      "offset": 1203.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yes. So I can talk to that and indeed",
      "offset": 1205.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you identified one interesting feature",
      "offset": 1208.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "of the system which is that depending on",
      "offset": 1210,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "what you tell it at the beginning you",
      "offset": 1212.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "can guide the process. So if you give it",
      "offset": 1214.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "fairly specific instructions or you ask",
      "offset": 1217.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the system to start from a specific type",
      "offset": 1219.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "of solution then what it will usually do",
      "offset": 1221.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it will like squeeze out the juice of",
      "offset": 1223.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that idea that you gave it or squeeze",
      "offset": 1225.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "out the juice of that initial solution",
      "offset": 1227.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "see how it can tweak it and bring it to",
      "offset": 1228.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "its maximal potential but and sometimes",
      "offset": 1230.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this is the right appro approach to take",
      "offset": 1233.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "maybe when the problem is particularly",
      "offset": 1235.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "difficult or has some specific features",
      "offset": 1237.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but by default you would start with a",
      "offset": 1239.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "solution that's like really really empty",
      "offset": 1241.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so so you you give alpha evolve a code",
      "offset": 1243.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "skele skeleton where um where all the",
      "offset": 1245.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "like all the functions are like almost",
      "offset": 1248.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "an empty implementation. and you just",
      "offset": 1250.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like return zero or return false and so",
      "offset": 1252,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "on and and you just let it let it to be",
      "offset": 1254.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "completely creative and so it just has",
      "offset": 1256.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to rely on like its background knowledge",
      "offset": 1258.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "of the base LLMs and it can explore in",
      "offset": 1260.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "all possible directions and the um the",
      "offset": 1262.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "evolutionary algorithm makes sense that",
      "offset": 1265.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you preserve the diversity as you as you",
      "offset": 1266.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "keep doing the exploration and um one",
      "offset": 1268.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "particular example I can talk about is u",
      "offset": 1272.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we applied alpha evolve to discovering",
      "offset": 1274.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "algorithms for matrix multiplication and",
      "offset": 1276.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "we did it by asking it to",
      "offset": 1279.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "design kind of a search algorithm, a",
      "offset": 1281.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "gradient based search algorithm that",
      "offset": 1283.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "looks for for the matrix multiplication",
      "offset": 1285.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "algorithms in turn. So, it's a bit of a",
      "offset": 1286.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "meta thing like you you look for an",
      "offset": 1289.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "algorithm that finds an algorithm, but",
      "offset": 1290.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "within that first algorithm, the search",
      "offset": 1293.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "algorithm, uh we started from a really",
      "offset": 1295.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "simple code skeleton giving it basically",
      "offset": 1297.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "nothing like just we just told it use",
      "offset": 1299.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "gradients basically. And then it was",
      "offset": 1300.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "able to to write these complex um loss",
      "offset": 1303.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "functions and update functions which had",
      "offset": 1305.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "all all sorts of tricks about uh",
      "offset": 1308.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "penalizing various behaviors and",
      "offset": 1310.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "introducing randomness in completely",
      "offset": 1312.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "unexpected ways which were like okay wow",
      "offset": 1314.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like like these this is the type of code",
      "offset": 1316.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that maybe a human could plausibly write",
      "offset": 1319.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "but but would they have actually thought",
      "offset": 1322.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "of writing this particular piece of",
      "offset": 1323.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "code? uh that yeah that that was really",
      "offset": 1325.679,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "a an aha moment at least for me that wow",
      "offset": 1328.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this is doing something kind of",
      "offset": 1331.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "humanlike but not something that",
      "offset": 1332.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "obviously a human would",
      "offset": 1334.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "try and uh maybe another cool story",
      "offset": 1337,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "about this is that I don't think we we",
      "offset": 1340.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "added it to the paper because maybe",
      "offset": 1342.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "paper is not the right format for this",
      "offset": 1343.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "but uh uh Adam and our team did a cool",
      "offset": 1345.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "experiment on trying to give um advice",
      "offset": 1347.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "from humans to to the system and then he",
      "offset": 1351.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "asked a few people for like you know you",
      "offset": 1353.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "please think about this problem for 2",
      "offset": 1355.84,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "minutes, you please think about the",
      "offset": 1357.52,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "problem for 30 minutes and then like",
      "offset": 1358.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "compare it uh and write down the notes",
      "offset": 1359.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and then give it to the system to kind",
      "offset": 1362.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of guide it through through the process",
      "offset": 1363.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and then he compared like what what",
      "offset": 1365.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "would be the outcome of that and you can",
      "offset": 1367.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "see that as Mate was saying it's kind of",
      "offset": 1369.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "squeezing all the juice out of the idea.",
      "offset": 1371.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "So it will like preserve the essence of",
      "offset": 1372.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the idea because it kind of guides the",
      "offset": 1374.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "LM towards things like that but it will",
      "offset": 1376,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "optimize a lot of small things and yeah",
      "offset": 1378.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and like in a lot of cases will be in",
      "offset": 1380.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "intelligence ways intelligent ways in a",
      "offset": 1382.559,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "lot of case it will be in kind of you",
      "offset": 1384.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "know I'll try a bunch of things and one",
      "offset": 1385.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "of them will stick but yeah it's kind of",
      "offset": 1387.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "cool to watch yeah and so at the moment",
      "offset": 1389.84,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "the architecture has let's say maybe two",
      "offset": 1393.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "sources of um base knowledge it has the",
      "offset": 1396.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "base model itself right which is",
      "offset": 1399.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "obviously",
      "offset": 1401.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "compressed, you know, all the corpus",
      "offset": 1402.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it's been trained on, including tons of",
      "offset": 1404.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "code and algorithms and numerical",
      "offset": 1406.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "recipes and whatever. And then it has",
      "offset": 1408.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the the starting program that you put in",
      "offset": 1410.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that the sort of gated bits of logic and",
      "offset": 1412.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and whatnot. Is there is there any room",
      "offset": 1414.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "for an augmentation that's almost a",
      "offset": 1417.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "middle ground where there's say a",
      "offset": 1419.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "secondary database that contains modules",
      "offset": 1420.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "or pieces of code that have known to",
      "offset": 1423.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "been very be very effective in other",
      "offset": 1425.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "problems that it can somehow",
      "offset": 1427.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "simultaneously draw on?",
      "offset": 1429.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Yes. Uh so a few points on this one is",
      "offset": 1431.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that you mentioned there are two sources",
      "offset": 1434.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "of kind of knowledge in the system and",
      "offset": 1436.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "debatably I would say that there is a",
      "offset": 1438.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "third source which is that the system",
      "offset": 1440,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "can decide to augment its own knowledge.",
      "offset": 1441.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "What I mean specifically is that the",
      "offset": 1444.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "system proposes an algorithm and then",
      "offset": 1446.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "it's going that algorithm is going to be",
      "offset": 1448.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "executed on a machine and you will see",
      "offset": 1450.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the results of of having run that",
      "offset": 1453.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "algorithm. So on a sufficiently high",
      "offset": 1454.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "level you can think of it as the system",
      "offset": 1456.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "can decide okay I want to gain the piece",
      "offset": 1458.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "of knowledge what does this algorithm do",
      "offset": 1460.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "when you actually run it. So so that",
      "offset": 1462.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that is an important component but but",
      "offset": 1464.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "maybe going closer to the essence of",
      "offset": 1466.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "your question indeed. So there could be",
      "offset": 1468.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "a a separate either cur like a human",
      "offset": 1471.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "curated database of useful modules or",
      "offset": 1473.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "any of that sort but even more",
      "offset": 1476.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "excitingly this database can be curated",
      "offset": 1478.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "by the system itself. And so this is an",
      "offset": 1480.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "idea that um we are thinking about for",
      "offset": 1483.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "alpha evolve but there is a a related",
      "offset": 1486.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "idea that is already implemented and",
      "offset": 1488.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "mentioned in the paper which is not",
      "offset": 1489.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "building a curated set of modules which",
      "offset": 1492.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "are generally useful but a curated set",
      "offset": 1494.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of prompts that tend to work well. So",
      "offset": 1497.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "there is this idea called the meta",
      "offset": 1499.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "prompting described in the paper where",
      "offset": 1501.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "um we actually ask our language models",
      "offset": 1503.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "to propose their own prompts. So just",
      "offset": 1505.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tell them what we are trying to do like",
      "offset": 1507.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "we're trying to do this evolutionary",
      "offset": 1510.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "algorithm for um improving on this",
      "offset": 1511.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "particular problem and we are going to",
      "offset": 1514.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to to prompt you with this particular",
      "offset": 1516,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "prompt but before we do that please",
      "offset": 1518,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "propose a modification to this prompt",
      "offset": 1520.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "itself and then we curate set of prompts",
      "offset": 1522.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that actually um work well for this",
      "offset": 1524.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "purpose and so in spirit that's a",
      "offset": 1526.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "similar idea although it's curating",
      "offset": 1529.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "prompts rather than programs but both",
      "offset": 1531.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "make sense to me. Yeah, I I think if I",
      "offset": 1533.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "understand um the direction Keith was",
      "offset": 1536.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "going in because everything you've just",
      "offset": 1538.159,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "described is is fascinating. It's",
      "offset": 1539.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "various forms of metalarning to",
      "offset": 1541.039,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "basically create diversity and",
      "offset": 1543.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "divergence. But um I feel that the next",
      "offset": 1545.559,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "trillion dollar business could be where",
      "offset": 1549.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "we you know like in program learning",
      "offset": 1552.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "right we want to construct a library and",
      "offset": 1554.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "right now we're we're kind of expanding",
      "offset": 1557.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this library of functions for a",
      "offset": 1559.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "particular purpose. But what if the",
      "offset": 1561.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "library itself was the new oil? You know",
      "offset": 1563.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what what what if there was strong",
      "offset": 1566.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "robustness between these programs that",
      "offset": 1568.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we've learned for this thing and they",
      "offset": 1570.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "generalize um you know through some",
      "offset": 1572.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "analogical relation to other programs",
      "offset": 1574.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and other domains. I mean what what if",
      "offset": 1577.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we just had like the new language model",
      "offset": 1578.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "paradigm was actually a kind of program",
      "offset": 1580.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "database. I mean do you think that could",
      "offset": 1582.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "work?",
      "offset": 1584,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Yeah, I think that's a fascinating idea",
      "offset": 1586.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and uh like we see maybe some first",
      "offset": 1589.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "glimpses of that. So it is true that for",
      "offset": 1592,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "now we are going in the death direction,",
      "offset": 1594.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "we just focus deeply on a problem and",
      "offset": 1596.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "just try to solve that problem. But even",
      "offset": 1597.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "within that path, let's say when we",
      "offset": 1600.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "worked on uh matrix multiplication, we",
      "offset": 1601.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "saw that um like when we run alpha above",
      "offset": 1604.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "different times, we discover slightly",
      "offset": 1607.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "different algorithms and then it's",
      "offset": 1608.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "actually a useful technique to uh take",
      "offset": 1610.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "those algorithms and use them to",
      "offset": 1613.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "initialize future experiments. So that's",
      "offset": 1615.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "kind of a first step towards like",
      "offset": 1618,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "building this database of you know",
      "offset": 1619.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "things that were useful in the past to",
      "offset": 1621.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "act as inspiration for solving future",
      "offset": 1623.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "problems. Um so yeah that maybe Alex has",
      "offset": 1625.039,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "more thoughts. Maybe another similar",
      "offset": 1627.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "thing we kind of I feel that is",
      "offset": 1630.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "happening is not that we store not that",
      "offset": 1632.4,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "we use the necessarily the database of",
      "offset": 1635.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "programs we produced over all the",
      "offset": 1637.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "experiments we did but kind of our own",
      "offset": 1638.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "human intuition as like users of the",
      "offset": 1641.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "system is definitely evolving and I kind",
      "offset": 1642.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "of feel like a consultancy right like",
      "offset": 1645.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "person like we we collaborate with a lot",
      "offset": 1647.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of teams at Google trying to help them",
      "offset": 1649.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "run things with alpha evolve and like",
      "offset": 1650.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "through that process we kind of gain a",
      "offset": 1652.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "lot of knowledge of like what works what",
      "offset": 1654.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "doesn't like what what should what",
      "offset": 1656.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "should we try next and and this kind of",
      "offset": 1657.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sort of things like somewhat similar to",
      "offset": 1659.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "what you're describing, right?",
      "offset": 1662.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Yeah. I mean, I can almost imagine Alpha",
      "offset": 1665.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Evolve having its own repo, you know,",
      "offset": 1667.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "whether it's um internal or or maybe be",
      "offset": 1669.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "nice enough to uh put it up on GitHub",
      "offset": 1672.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for us, but it would just be constantly",
      "offset": 1674.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "evolving and contributing to its own",
      "offset": 1676.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "repo and maintaining it and categorizing",
      "offset": 1678.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it and people can go take a look. I",
      "offset": 1681.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "wonder if uh Alpha Evolve's come up with",
      "offset": 1683.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "a better search algorithm. go check the",
      "offset": 1685.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the search area and see if it's got",
      "offset": 1687.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "anything new there. Yeah, like",
      "offset": 1689.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "technologically I I don't see hurdles",
      "offset": 1691.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "for this. It's maybe the organizational",
      "offset": 1693.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "question of how exactly to make it",
      "offset": 1696.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "happen. And one particular technological",
      "offset": 1697.679,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "piece that's already there is that we",
      "offset": 1700.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "already now look for programs that work",
      "offset": 1702.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "well across a range of tasks. And right",
      "offset": 1704.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "now maybe this range is let's say fairly",
      "offset": 1707.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "constrained because we care about a",
      "offset": 1709.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "constrained range but there's nothing",
      "offset": 1711.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "blocking us from expanding it. So for",
      "offset": 1712.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "like concretely we would be looking for",
      "offset": 1715.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "search algorithms that are able to find",
      "offset": 1717.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "let's say matrix multiplication",
      "offset": 1720.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "algorithms for different sizes",
      "offset": 1722.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "simultaneously. So there is this sort of",
      "offset": 1723.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "generality but there is nothing",
      "offset": 1726.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "preventing us from saying let's look for",
      "offset": 1728.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "search algorithms that actually work",
      "offset": 1730.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "well across a much broader range of",
      "offset": 1732,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "tasks. That's not just matrix",
      "offset": 1733.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "multiplication but also other search",
      "offset": 1734.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "problems.",
      "offset": 1737.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Can we meditate on the matrix",
      "offset": 1738.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "multiplication thing only because that's",
      "offset": 1740.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "that's the headline hook of this video.",
      "offset": 1742.159,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "You know, I think we're going to start",
      "offset": 1744.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the video by saying there's this amazing",
      "offset": 1745.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "result. There was this, you know,",
      "offset": 1747.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Strassing guy 56 years ago. He had this",
      "offset": 1748.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "big result. Now, Alpha Alpha Revolve is",
      "offset": 1751.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "has just defeated it. There's quite a",
      "offset": 1753.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "few things to explore here. I mean,",
      "offset": 1754.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "obviously like please just explain the",
      "offset": 1756,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "whole thing, but also there's some",
      "offset": 1758,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "interesting stuff around we went up to I",
      "offset": 1760.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "think a maximum of this is like um",
      "offset": 1763.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "complex 2D matrix multiplication. and",
      "offset": 1765.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "you went up to rank six and even that",
      "offset": 1767.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "was a bit of an overshoot and there was",
      "offset": 1770.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "some interesting properties where as the",
      "offset": 1772.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "rank went up it started to get a little",
      "offset": 1774.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "bit sketchy but it it did improve more.",
      "offset": 1776.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Can you can you just like you know talk",
      "offset": 1779.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "through that whole story?",
      "offset": 1781.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "So let me start maybe with the high",
      "offset": 1783.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "level picture first. So multiplying",
      "offset": 1785.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "matrices that's like a super basic",
      "offset": 1787.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "operation and and some of us like get",
      "offset": 1789.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "taught this operation in high school and",
      "offset": 1791.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "there is a very specific way how you",
      "offset": 1793.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "multiply matrices when you're taught to",
      "offset": 1795.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do this in high school like you at every",
      "offset": 1797.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "step you need to take one row of one",
      "offset": 1799.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "matrix and one column from the other",
      "offset": 1801.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "matrix. You compute like the scalar",
      "offset": 1802.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "dotproduct of these two things and that",
      "offset": 1805.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "gives you one entry in the output. And",
      "offset": 1806.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "so this is one specific algorithm which",
      "offset": 1809.279,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is kind of the basic algorithm for",
      "offset": 1811.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "multiplying matrices. For for every",
      "offset": 1812.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "element in the output matrix, you'll",
      "offset": 1814.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "need to do this one dotproduct. And for",
      "offset": 1816.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a long long time, people thought that",
      "offset": 1819.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this was kind of the obviously the only",
      "offset": 1820.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "way to multiply matrices. How could",
      "offset": 1822.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "there even be something something",
      "offset": 1824.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "better? And then you mentioned Fer",
      "offset": 1826.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Strasen in 1969 was really a shock to",
      "offset": 1829.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the mathematical community. he he wrote",
      "offset": 1832.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this paper saying that actually uh there",
      "offset": 1834.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "is a faster way of doing it and so",
      "offset": 1837.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "already for multiplying 2x2 matrices so",
      "offset": 1840,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "the smallest non-trivial case uh if you",
      "offset": 1843.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "do it the high school way you need to do",
      "offset": 1846.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "eight multiplications because there are",
      "offset": 1848.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "four entries in the output matrix is a",
      "offset": 1850.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "2x2 matrix and each scale like each",
      "offset": 1851.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "inner product requires two",
      "offset": 1854.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "multiplications so four * 2 is eight but",
      "offset": 1855.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Strasen he came up with this ingenious",
      "offset": 1858.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "way of making a procedure that only",
      "offset": 1860.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "requires seven multiplic applications",
      "offset": 1862.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and it's kind of a magical procedure",
      "offset": 1864.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "where you you build some combinations of",
      "offset": 1865.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "entries from the first matrix and the",
      "offset": 1868.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "second matrix and you multiply them and",
      "offset": 1870.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "then you are able to combine the seven",
      "offset": 1872.399,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "products in such a way that you get",
      "offset": 1874.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "these magical cancellations and the the",
      "offset": 1875.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "result is just correct. So so this was a",
      "offset": 1878.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "like a big surprise in 1969 and it",
      "offset": 1881.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "opened this entire new area of research",
      "offset": 1884.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "like okay so for 2x two you can actually",
      "offset": 1887.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "do seven instead of eight. Um so then",
      "offset": 1889.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "people quickly proved that seven is",
      "offset": 1892.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "actually optimal for that small case.",
      "offset": 1893.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "But already for 3x3 matrices which you",
      "offset": 1895.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "can think okay I mean that that's like",
      "offset": 1898.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "laughably small surely people must have",
      "offset": 1900.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "figured out what is the best way to do",
      "offset": 1902.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that but we still don't know. So even",
      "offset": 1904.399,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "today we know that you need at least 19",
      "offset": 1907.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "multiplications to to do that multiply",
      "offset": 1909.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "two 3x3 matrices but the best algorithm",
      "offset": 1912.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we have is using 23. So there's this gap",
      "offset": 1914.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "between 19 and 23 that people just",
      "offset": 1917.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "haven't been able to close for for years",
      "offset": 1919.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and and and the reason is that even",
      "offset": 1921.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "though the matrices are very small the",
      "offset": 1923.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "space of possible algorithms how you",
      "offset": 1926.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "could multiply them is just just",
      "offset": 1928.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "completely immense. So uh",
      "offset": 1929.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "computationally there is just no hope at",
      "offset": 1931.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "all to do this exhaustively. Um so",
      "offset": 1933.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "already for for 3x3 it's it's it's like",
      "offset": 1936.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "open which is crazy. And then okay so",
      "offset": 1938.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "for 3x3 the best algorithm is using 23",
      "offset": 1941.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that's at least better than the",
      "offset": 1944.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "algorithm you get taught in high school",
      "offset": 1946.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "which would be 27. So at least there is",
      "offset": 1947.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "some progress on making it better. But",
      "offset": 1950,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "then for 4x4 which is like the next",
      "offset": 1952.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "size the best algorithm that that had",
      "offset": 1955.72,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "been known was just apply strassen",
      "offset": 1958.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "algorithm recursively twice. So because",
      "offset": 1961.279,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "trassen is for 2x2 matrices what you can",
      "offset": 1963.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "do if you have a 4x4 matrix we you",
      "offset": 1966.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "consider it as a block matrix of 2x2",
      "offset": 1969.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "blocks and then each block itself is a",
      "offset": 1971.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "2x2 matrix. So you can do strassen twice",
      "offset": 1972.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and because strassen needs seven",
      "offset": 1976.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "multiplications if you do it twice you",
      "offset": 1977.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "will get 7 * 7 49 multiplications. So",
      "offset": 1979.6,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "for the uh like yeah so so this was the",
      "offset": 1982.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "o only way known how to multiply 4x4",
      "offset": 1985.919,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "matrices um in in in a fast way like",
      "offset": 1988.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "using 49 multiplications just to stress",
      "offset": 1992.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "and twice and this has been the kind of",
      "offset": 1994.24,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the situation since",
      "offset": 1997.12,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "1969 and so this is where maybe our work",
      "offset": 1998.84,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "comes in. So two years ago um we we",
      "offset": 2002.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "built Alpha Tensor which was a",
      "offset": 2006,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "specialized reinforcement learning agent",
      "offset": 2008.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "for discovering matrix multiplication",
      "offset": 2010.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "algorithms and that agent actually did",
      "offset": 2012.399,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "find something faster but only for uh",
      "offset": 2015.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "boolean matrices. So it's this very",
      "offset": 2017.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "special case where you want to multiply",
      "offset": 2020.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "matrices where every entry is just zero",
      "offset": 2022.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "or one and when you do the",
      "offset": 2024.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "multiplication you do everything modulo",
      "offset": 2025.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "2. So for that case alpha tensor found",
      "offset": 2027.679,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "something faster. But apart from that",
      "offset": 2030.399,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "uh for the usual case of matrices that",
      "offset": 2033.64,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "have arbitrary numbers in them still",
      "offset": 2036.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "nothing better was known than doing",
      "offset": 2038.88,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "stress and twice using 49",
      "offset": 2040.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "multiplications. So we're like really",
      "offset": 2042.679,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "excited when we used alpha evolve in",
      "offset": 2045.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "this setting. We actually didn't even",
      "offset": 2048,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "hope that it would find something better",
      "offset": 2050.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "than 49 because we're trying for so long",
      "offset": 2051.599,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with alpha tensor. We just ran it for",
      "offset": 2053.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "completeness because we wanted to have",
      "offset": 2056.159,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "this, you know, this table in the paper",
      "offset": 2057.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that we actually have that we tried on",
      "offset": 2059.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "all the sizes up to up to five or six",
      "offset": 2061.119,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and and remarkably it found a faster",
      "offset": 2063.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "algorithm which uses 48 instead of 49",
      "offset": 2066.159,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "multiplications and I mean so that it",
      "offset": 2069,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "itself like yeah when when one of my",
      "offset": 2072.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "teammates messaged on the channel that",
      "offset": 2074.879,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "oh it seems like we have this result I",
      "offset": 2076.399,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "just couldn't believe it like let's",
      "offset": 2078.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "triple check it. Uh but then it was",
      "offset": 2079.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "indeed correct and it actually has this",
      "offset": 2081.359,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "one um really appealing feature that um",
      "offset": 2083.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "like usually you think about multiplying",
      "offset": 2086.879,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "matrices, you want to multiply matrices",
      "offset": 2089.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "where the entries are real numbers or",
      "offset": 2090.879,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "maybe integers. Um the the case of",
      "offset": 2093.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "multiplying matrices where the numbers",
      "offset": 2095.919,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "are complex is maybe a bit less less",
      "offset": 2097.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "common. Um but real matrices are just a",
      "offset": 2099.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "special case of complex matrices. So if",
      "offset": 2102.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you find an algorithm that can multiply",
      "offset": 2104.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "complex matrices, you can also apply to",
      "offset": 2107.2,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "real matrices. It's just a just a",
      "offset": 2110,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "generalization. So what's kind of cool",
      "offset": 2113,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "here is that you let's say you care",
      "offset": 2115.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "about multiplying real matrices, which",
      "offset": 2117.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "is what you care about when you train",
      "offset": 2119.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "neural networks. Let's say a very common",
      "offset": 2120.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "use case. Um a priority, you might just",
      "offset": 2122.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "look for an algorithm that uses real",
      "offset": 2125.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "numbers. But you can say that oh",
      "offset": 2126.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually what if we look for a complex",
      "offset": 2129.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "algorithm which a priority would think",
      "offset": 2130.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "well that's a more difficult task",
      "offset": 2133.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "because that algorithm would actually",
      "offset": 2135.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "apply not only to real matrices but also",
      "offset": 2136.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "to complex matrices but by making the",
      "offset": 2138.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "task more difficult actually alpha",
      "offset": 2141.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "evolve was able to find an algorithm",
      "offset": 2143.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that uses complex numbers and therefore",
      "offset": 2145.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "applies to both complex matrices and",
      "offset": 2147.599,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "real matrices. So that's that's kind of",
      "offset": 2149.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the the result that we were most excited",
      "offset": 2152.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to to actually get. But then as you were",
      "offset": 2154.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "asking in your question indeed we we",
      "offset": 2156.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "were applying alpha evolve to other",
      "offset": 2159.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "matrix sizes as well and and indeed as",
      "offset": 2160.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you go to uh bigger and bigger cases",
      "offset": 2163.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "like 5x 5 6x6 the problem becomes much",
      "offset": 2165.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "much harder very very quickly. It's kind",
      "offset": 2169.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "of a an exponential with a quadratic in",
      "offset": 2171.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "the exponent because um the the tensor",
      "offset": 2173.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this cube that we sometimes show in the",
      "offset": 2176.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "visualizations that you have to",
      "offset": 2178.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "decompose it grows quadratically with",
      "offset": 2179.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the size of the matrices that you",
      "offset": 2181.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "multiply. So for 4x4 matrices you have",
      "offset": 2183.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to deal with a tensor of size 16 16. For",
      "offset": 2185.76,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "5x5 matrices it's 25 25. So it's it's",
      "offset": 2188.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "like exploding very quickly and of",
      "offset": 2192.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "course as as you go higher at some point",
      "offset": 2194.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "your method will will not scale there.",
      "offset": 2196.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "But what we show is that alpha well it",
      "offset": 2198.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "scales further than alpha tensor. Uh so",
      "offset": 2201.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so there is some progress on the scaling",
      "offset": 2203.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "direction. And then just one important",
      "offset": 2205.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "point to clarify is that in all these",
      "offset": 2208,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "cases sure we look at small cases of",
      "offset": 2210.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "matrix multiplication 2x2 3x3 4x4 but",
      "offset": 2212.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that doesn't mean that you can only",
      "offset": 2216,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "apply these algorithms to matrices that",
      "offset": 2217.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "are this small like as I already alluded",
      "offset": 2219.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to with stress algorithm you can apply",
      "offset": 2221.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "them recursively. So if you have a big",
      "offset": 2223.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "matrix you treat it as a block matrix",
      "offset": 2225.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and you apply these algorithms that are",
      "offset": 2227.44,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "for smaller matrices uh recursively.",
      "offset": 2229.04,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Yeah. I mean and it's super fascinating",
      "offset": 2233.599,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and and it is such a fun a fun domain to",
      "offset": 2236.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "work in and it almost magical that even",
      "offset": 2239.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "when you get to 3x3",
      "offset": 2241.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "um it becomes you know intractable and",
      "offset": 2243.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "I'm always fascinated by cases like that",
      "offset": 2246.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "in mathematics it's like oh we can do it",
      "offset": 2248,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for dimension one and two and three but",
      "offset": 2249.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "then when we get to four it's just",
      "offset": 2252.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "totally different you know it completely",
      "offset": 2254.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "falls apart but as you went higher you",
      "offset": 2255.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "also had cases where alpha evolve it it",
      "offset": 2258.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "couldn't match the the current",
      "offset": 2261.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "performance and actually found worse",
      "offset": 2263.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like it sort of found worse algorithms.",
      "offset": 2265.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "So what do you attribute attribute that",
      "offset": 2266.96,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "to why was it so so in in the case",
      "offset": 2269.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that's the biggest one that we show in",
      "offset": 2273.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the paper 6x6 uh matrices there is a",
      "offset": 2275.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "very uh clear reason for that which is",
      "offset": 2278.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that we try to apply alpha evolve",
      "offset": 2280.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "without giving it domain knowledge about",
      "offset": 2282.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "the the problem we just wanted to see",
      "offset": 2285.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "how good is alpha evolve as a general",
      "offset": 2287.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "purpose tool and we start it kind of",
      "offset": 2289.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "from scratch and we don't tell it about",
      "offset": 2291.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "any you know like tricks for developing",
      "offset": 2293.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "specifically matrix multip",
      "offset": 2295.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "multiplication algorithms. Um the the",
      "offset": 2296.56,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "best known algorithm for 6x6 matrix",
      "offset": 2299.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "multiplication, it uh uses a kind of",
      "offset": 2301.72,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "very specific inductive bias which is",
      "offset": 2304.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "which means that it's looking for",
      "offset": 2306.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "algorithms that have a specific symmetry",
      "offset": 2308.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in them. Meaning that it only looks for",
      "offset": 2310.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "algorithms that have a regularity in the",
      "offset": 2313.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "algorithm. And if you only look for",
      "offset": 2315.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "algorithms that have this regularity,",
      "offset": 2317.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that's a much smaller search space. So",
      "offset": 2319.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "in that search space uh you are able to",
      "offset": 2322.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "scale to much larger sizes. Um but we",
      "offset": 2324.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "just didn't try to do this to",
      "offset": 2328,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "incorporate this symmetry into our",
      "offset": 2329.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "search. We just looked for algorithms of",
      "offset": 2331.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "unrestricted form. And that I think is",
      "offset": 2333.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "at least to me the clearest reason why",
      "offset": 2336.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we didn't match the best known solution",
      "offset": 2338.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "in that case.",
      "offset": 2340.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Yeah. So um one of the things that",
      "offset": 2343.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "fascinated me most about the paper I",
      "offset": 2345.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "mean obviously we're very interested in",
      "offset": 2347.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "abstraction and and representation and",
      "offset": 2348.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "even in the example that you just gave",
      "offset": 2351.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that Straussen was learned to be used as",
      "offset": 2352.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a sort of dynamic programming",
      "offset": 2355.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "formulation so being used recursively",
      "offset": 2356.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and I'm thinking to myself is that a",
      "offset": 2359.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "demonstration of deep abstraction or is",
      "offset": 2361.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it a kind of superficial one-step",
      "offset": 2363.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "abstraction where the um the knowledge",
      "offset": 2365.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "of Strason was in its kind of local",
      "offset": 2368.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "neighborhood and it was composing that",
      "offset": 2371.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and in an ideal world what we want",
      "offset": 2374.32,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "algorithms to do is compose abstract",
      "offset": 2376.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "basis knowledge. So knowledge which is",
      "offset": 2379.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "as far down the stack as you can",
      "offset": 2381.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "possibly do to sort of increase our our",
      "offset": 2383.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "flexibility. And we should just bring in",
      "offset": 2385.56,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "how you modeled this representation. So",
      "offset": 2388.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you had these different approaches. You",
      "offset": 2390.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "could directly model the solution. You",
      "offset": 2392.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "could model a constructor function. So",
      "offset": 2394.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you know you're actually learning a",
      "offset": 2396.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "function which itself constructs the",
      "offset": 2397.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "solution. you could be learning a search",
      "offset": 2399.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "algorithm which is what you did with the",
      "offset": 2401.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "matrix multiplication and you also spoke",
      "offset": 2402.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about um co-evolution as as a",
      "offset": 2404.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "possibility. So this is mind-blowing in",
      "offset": 2406.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the sense that there's some human design",
      "offset": 2408.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and intuition in how you designed the",
      "offset": 2410.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "optimization target but there's this",
      "offset": 2412.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "ambiguity in in the ways that you can",
      "offset": 2415.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "represent so many of these problems and",
      "offset": 2417.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "how they relate to each other. So can",
      "offset": 2418.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you talk me through that? Yeah. So first",
      "offset": 2420.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "of all I want to be upfront about the",
      "offset": 2423.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "fact that we ourselves don't have all",
      "offset": 2425.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the answers here. We have this tool",
      "offset": 2427.2,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "alpha evolve and we see that okay",
      "offset": 2428.8,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "it's like it's like yeah not even about",
      "offset": 2431.48,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "alpha evolve uh so so we have this",
      "offset": 2435.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "general tool that we see we can apply it",
      "offset": 2437.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "here and here and here and for every",
      "offset": 2440.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "problem there are different ways we can",
      "offset": 2441.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "apply this tool but a priority we only",
      "offset": 2443.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "have some intuition what is the right",
      "offset": 2446,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "level of abstraction to apply this tool",
      "offset": 2447.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "on and um as you were mentioning Tim",
      "offset": 2449.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "yeah sometimes the best thing you can do",
      "offset": 2452.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is you directly search for the solution",
      "offset": 2454,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sometimes you d You search for a like a",
      "offset": 2456.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "simple constructor which constructs the",
      "offset": 2458.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "solution. And here as a like illustrated",
      "offset": 2460.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "example, let's say you're looking for a",
      "offset": 2463.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "solution and you think it's going to be",
      "offset": 2466,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "very regular. Maybe it's going to be",
      "offset": 2467.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like look like a fractal like that's the",
      "offset": 2468.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "typical example. Then you know that to",
      "offset": 2470.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "describe a fractal you can do it with a",
      "offset": 2472.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "very short piece of code. So in that",
      "offset": 2474.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "case it makes sense to look for a",
      "offset": 2476.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "description of the fractal not as a like",
      "offset": 2478.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "a grid of pixels but instead as a short",
      "offset": 2480.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "piece of function that just generates",
      "offset": 2483.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that fractal. But then in other",
      "offset": 2485.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "applications maybe the solution is very",
      "offset": 2487.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "different, not so regular and maybe you",
      "offset": 2490.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "want to either look for the solution",
      "offset": 2492.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "directly or you want to look for a",
      "offset": 2493.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "complex search algorithm that finds the",
      "offset": 2495.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "solution. And sometimes you want to have",
      "offset": 2497.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "a sequence of algorithms that gradually",
      "offset": 2498.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "refine the solution which is the um",
      "offset": 2500.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "co-evolution approach and a priority.",
      "offset": 2502.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "It's not clear at all which one is going",
      "offset": 2506.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to work best when. So that is something",
      "offset": 2508.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that is definitely in kind of the future",
      "offset": 2510.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "work category to to build up that",
      "offset": 2512.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "understanding. But one positive side of",
      "offset": 2514.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "things is that um alpha evolve is easy",
      "offset": 2517.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to set up in in all the different",
      "offset": 2520.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "formulations. So often in practice, you",
      "offset": 2522.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "just try different things and and see",
      "offset": 2524.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "what works best.",
      "offset": 2526.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Well, yeah, I I can't believe you guys",
      "offset": 2529.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "don't have all the answers. Like I was I",
      "offset": 2531.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "don't know why I'm here now, but uh no,",
      "offset": 2534.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "in all seriousness, and Tim mentioned",
      "offset": 2536.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "it, these sort of three modes that that",
      "offset": 2538.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "um Alpha Evolve, you know, currently",
      "offset": 2541.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "runs in, right? And it really reminded",
      "offset": 2543.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "me of um mathematicians because because",
      "offset": 2545.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "you know",
      "offset": 2548.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "mathematicians or there's at least three",
      "offset": 2549.8,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "kind of main ways in which proofs are",
      "offset": 2552.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "done, right? They're either done by",
      "offset": 2554.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "deduction, construction or enumeration.",
      "offset": 2555.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "And those almost seem to be like very",
      "offset": 2558.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "parallel to to these three methods. And",
      "offset": 2560.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I feel like there's probably some deep",
      "offset": 2563.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "deep connection there that I'm missing.",
      "offset": 2565.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I'm curious if if either of you have",
      "offset": 2567.839,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "have any thoughts on that.",
      "offset": 2569.44,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "Yeah. any any thoughts on that uh deep",
      "offset": 2573.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "deep connection there or or not?",
      "offset": 2575.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Not sure. Yeah, I it's an interesting",
      "offset": 2579.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "observation.",
      "offset": 2582.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Yeah. Mate, did you want to say",
      "offset": 2584.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "something? Yeah. So, so one thing is",
      "offset": 2585.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that when you think about constructions,",
      "offset": 2587.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that is the space where alpha is like",
      "offset": 2589.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "most obviously applicable. So the the",
      "offset": 2591.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "examples that we show in the paper is",
      "offset": 2594.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "where you have open problems and you",
      "offset": 2596.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "make progress on those open problems by",
      "offset": 2598,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "finding better constructions. So that's",
      "offset": 2600.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "where you're applicable out of the box.",
      "offset": 2602.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "But then if you think about other",
      "offset": 2604.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "approaches of making progress on other",
      "offset": 2607.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "types of mathematical problems, let's",
      "offset": 2609.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "say the problem is not obviously about",
      "offset": 2610.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the construction. Let's say you want to",
      "offset": 2613.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "prove impossibility results like lower",
      "offset": 2615.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "bounds in a sense. Then then you have",
      "offset": 2618.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "maybe two approaches on a high level",
      "offset": 2619.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that you can take. One is that often",
      "offset": 2621.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "problems that don't look like a",
      "offset": 2624,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "construction, they become a construction",
      "offset": 2625.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "if you frame them in the right way. So",
      "offset": 2627.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "for example, you you have the like",
      "offset": 2629.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "duality theorems for linear programming",
      "offset": 2631.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "between the primal and the dual. So you",
      "offset": 2634.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "can often switch the side that you're",
      "offset": 2635.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually trying to prove and turn",
      "offset": 2637.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "something that isn't a constructive",
      "offset": 2638.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "problem into a constructive one. So",
      "offset": 2640.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that's kind of a alibistic answer, but",
      "offset": 2642.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that's something you can often do. Um",
      "offset": 2644.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the more difficult and more general",
      "offset": 2646.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "answer is that sometimes you're actually",
      "offset": 2648.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "looking for a proof rather than a",
      "offset": 2650.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "construction. Now, now proof is you can",
      "offset": 2652.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "think of a proof also as an algorithm",
      "offset": 2656.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "like it's a step of like a sequence of",
      "offset": 2658.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "steps that you need to execute to prove",
      "offset": 2660.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "a statement. So this is still within the",
      "offset": 2662.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "space of algorithm discovery and",
      "offset": 2664.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "something that we we can be thinking of",
      "offset": 2666.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "doing. But there is one technical hurdle",
      "offset": 2668.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that we have maybe some initial signs of",
      "offset": 2671.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "overcoming but it's not something that",
      "offset": 2673.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "uh we have done in this paper which is",
      "offset": 2675.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that if you're looking for an algorithm",
      "offset": 2677.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "that is a proof then the proof is at the",
      "offset": 2679.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "end of the day either correct or not. So",
      "offset": 2682.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the reward is binary it's like zero or",
      "offset": 2684.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "one. Um for now we have focused on",
      "offset": 2686.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "problems where um you can make gradual",
      "offset": 2688.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "progress like can gradually improve the",
      "offset": 2691.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "score not just switch from zero to one",
      "offset": 2693.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "in a single step but gradually uh become",
      "offset": 2695.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "better and better until you improve on",
      "offset": 2698.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the best on construction. Now but of",
      "offset": 2700.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "course even as humans when we write",
      "offset": 2702.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "proofs we we face the same issue like uh",
      "offset": 2705.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the the proof is only correct once it's",
      "offset": 2707.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "actually done but as humans as we are",
      "offset": 2709.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "writing working on the proof we we'll",
      "offset": 2711.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "have some intuitions of okay have we",
      "offset": 2714.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "actually made some progress like have we",
      "offset": 2715.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "have we actually built some",
      "offset": 2718.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "understanding about the problem if so",
      "offset": 2719.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then that seems likely that this will be",
      "offset": 2721.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "a part of the eventual proof. So we have",
      "offset": 2723.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "been exploring the possibility of using",
      "offset": 2727.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "um scores which are not hard scores but",
      "offset": 2730.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "soft scores. Maybe a a language model",
      "offset": 2732.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "can itself uh provide feedback like does",
      "offset": 2734.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it look like we have made progress",
      "offset": 2737.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "towards solving the problem. So we we do",
      "offset": 2739.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "see the path of uh attacking these",
      "offset": 2741.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "binary problems. Let's say uh searching",
      "offset": 2743.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "for proofs but it's not something that",
      "offset": 2746,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "uh that we have already done. It's just",
      "offset": 2748.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "something we see as a possibility of uh",
      "offset": 2750.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "doing in the future with this type of",
      "offset": 2753.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "technique.",
      "offset": 2755.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Yeah. I mean, and there's there's so",
      "offset": 2757.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "many surprising things in this paper and",
      "offset": 2759.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "I know Tim and I uh want to ask about",
      "offset": 2760.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "more of them, but I'm kind of curious,",
      "offset": 2763.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "you know, what each of you saw is I",
      "offset": 2765.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "mean, after you did this work and you",
      "offset": 2767.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "had the paper or the research done at",
      "offset": 2769.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "least, you know, which was most",
      "offset": 2771.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "surprising to you? And I'll start with",
      "offset": 2773.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Alex. like what uh what did you walk",
      "offset": 2774.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "away from this work thinking wow I",
      "offset": 2776.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "wasn't actually expecting that I think",
      "offset": 2778.96,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "it keeps amazing amazing me uh how like",
      "offset": 2781.92,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "how much progress you can make with this",
      "offset": 2787.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sort of systems right like when when we",
      "offset": 2789.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "started with fun search a few years ago",
      "offset": 2791.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you would go to a chatbot interface",
      "offset": 2792.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "somewhere right and like you would ask",
      "offset": 2795.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "it to solve you an open problem it will",
      "offset": 2796.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like not give you anything basically",
      "offset": 2798.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right and then still today like you can",
      "offset": 2800.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "try the same thing and like okay it will",
      "offset": 2802.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "probably makes much more sense to like",
      "offset": 2803.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "think for a few minutes, it will give",
      "offset": 2805.599,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "you a lot of reasonable approaches. Uh",
      "offset": 2806.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "but generally it's not the experience of",
      "offset": 2809.079,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "people that you like ask a chatbot to",
      "offset": 2811.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "solve you an open problem and it will",
      "offset": 2813.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just do that right and then it's kind of",
      "offset": 2814.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "amazing that like by using this the same",
      "offset": 2817.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "tools right the same LLMs in in kind of",
      "offset": 2819.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this iterative evolution loops you can",
      "offset": 2822.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "get so much more out of them. It's yeah",
      "offset": 2824.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I think like every time we solve",
      "offset": 2826.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "something new it's it's really kind of",
      "offset": 2828.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "exciting and surprising in a sense",
      "offset": 2830.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and and mate what what uh most surprised",
      "offset": 2833.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you I think what is really new to me is",
      "offset": 2836.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that the generality of the approach and",
      "offset": 2839.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "I don't just mean a generality across",
      "offset": 2841.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "scientific open problems but it's not my",
      "offset": 2844,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "experience from my like admittedly short",
      "offset": 2846.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "research career that you you build some",
      "offset": 2848.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tool for like scientific purposes and",
      "offset": 2850.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "then out of the box you can do it apply",
      "offset": 2852.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to real world challenges and have so",
      "offset": 2856,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "much impact. Like usually there is an",
      "offset": 2857.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "entire body of work, research work that",
      "offset": 2860.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "needs to happen to to translate a",
      "offset": 2862.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "scientific technology into something",
      "offset": 2864.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "that's actually useful in the real",
      "offset": 2866.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "world. Like usually there is so many",
      "offset": 2868.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "challenges you have to have to tackle",
      "offset": 2869.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and and here there is a tool which out",
      "offset": 2871.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of the box at the same time is able to",
      "offset": 2873.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "make new discoveries on on mathematical",
      "offset": 2875.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and scientific problems and at the same",
      "offset": 2877.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "time is able to discover algorithms that",
      "offset": 2880,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you can directly deploy into into",
      "offset": 2882.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Google's like critical compute stack. So",
      "offset": 2884.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "so that's something I certainly hadn't",
      "offset": 2887.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "experienced before and maybe wasn't",
      "offset": 2889.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "expecting to be honest. Yeah. I don't",
      "offset": 2892.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "know if you're familiar with the arc",
      "offset": 2895.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "challenge and a guy called Ryan",
      "offset": 2897.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Greenblat. He did this famous approach",
      "offset": 2898.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "where he just sampled a language model",
      "offset": 2901.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like 30,000 times to generate programs.",
      "offset": 2903.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "We're in this really interesting space",
      "offset": 2906.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "as as you spoke about in the paper where",
      "offset": 2908.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we have an evaluator which means we can",
      "offset": 2909.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "sidestep hallucinations, right? So isn't",
      "offset": 2911.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "it fascinating that these nuggets of",
      "offset": 2914.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "gold are in there in the search space",
      "offset": 2916.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and of course most of us just use",
      "offset": 2918.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "language models in a single shot, right?",
      "offset": 2920.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "just do greedy sampling and and now we",
      "offset": 2921.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "can just um do these very sophisticated",
      "offset": 2925.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "search routines. But I want to get to um",
      "offset": 2927.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "well actually maybe like a quick sub",
      "offset": 2930.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "question is what what what you folks",
      "offset": 2932.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "have done is you've mixed so many",
      "offset": 2935.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "interesting paradigms together. You know",
      "offset": 2936.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "we're talking about metalarning and um",
      "offset": 2938.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "evolution and diversity preservation and",
      "offset": 2940.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and program learning and library",
      "offset": 2942.48,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "learning and whatnot.",
      "offset": 2944.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "But like if you could explain simply how",
      "offset": 2946.04,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "is that different from just sampling a",
      "offset": 2948.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "language model let's say you could",
      "offset": 2951.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sample a language model a billion times",
      "offset": 2952.8,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "has what you is alpha evolve like in a",
      "offset": 2956,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "different category to that. Yeah. Yeah",
      "offset": 2959.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "for sure. So so maybe just one uh like",
      "offset": 2962,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "personal observation um on on what you",
      "offset": 2964.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "just said that is indeed the right way",
      "offset": 2967.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of thinking about it. If you just keep",
      "offset": 2969.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "asking a language model in a in a like a",
      "offset": 2970.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "chat window repeatedly, you'll get a",
      "offset": 2973.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "completely the wrong idea about what is",
      "offset": 2976.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the capability if you actually scale",
      "offset": 2978.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "things up. Like when we started working",
      "offset": 2980.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "on the this type of evolutionary",
      "offset": 2982.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "approaches um initially we were like",
      "offset": 2984.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "pretty skeptical what this is going to",
      "offset": 2987.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "be able to do because indeed like we",
      "offset": 2989.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "were just trying in in a in a chat",
      "offset": 2991.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "window and asking it to to to write some",
      "offset": 2993.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "simple algorithms and I it wasn't doing",
      "offset": 2996,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that well initially and and the magic",
      "offset": 2998.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "really happens when you when you scale",
      "offset": 3000.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "things up but there are different ways",
      "offset": 3002.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "in which you can scale things up. One is",
      "offset": 3004.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that indeed you just keep asking",
      "offset": 3005.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "repeatedly the same question and okay",
      "offset": 3007.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "sure like there will be some nuggets but",
      "offset": 3009.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "there but they there'll be nuggets there",
      "offset": 3011.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "will not be the full solution. So it is",
      "offset": 3013.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "really important that you just find",
      "offset": 3015.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "those good nuggets and then you",
      "offset": 3016.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "iteratively build on top of them in",
      "offset": 3017.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "subsequent iterations and and that's",
      "offset": 3019.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "what you get through through through",
      "offset": 3021.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "these evolutionary evolutionary",
      "offset": 3023.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "algorithms. So and just to speak very",
      "offset": 3025.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "quantitatively we do have a comparison",
      "offset": 3028.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in the ablations in the paper where we",
      "offset": 3030.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "try to get rid of evolution and I mean",
      "offset": 3032.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "as you might expect that that works much",
      "offset": 3034.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "much worse. Yeah I think maybe a good",
      "offset": 3036,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "test case for kind of building additions",
      "offset": 3038.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "about that is is the capset example from",
      "offset": 3040.559,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the original fun search paper. Uh so",
      "offset": 3042.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "there we used a very simple and like",
      "offset": 3044.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "small language model and like we didn't",
      "offset": 3047.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "even provide context about the problem",
      "offset": 3049.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "because we wouldn't ever expect the",
      "offset": 3051.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "language model to actually you know",
      "offset": 3053.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "think through the problem and like solve",
      "offset": 3054.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it by by thinking hard and like being",
      "offset": 3056.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "very smart about it. What we're hoping",
      "offset": 3057.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to get from the language model is just",
      "offset": 3059.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "like trying a few things like a lot of",
      "offset": 3061.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "things, right? But the problem is that",
      "offset": 3062.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the final solution is like this. It's",
      "offset": 3064.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "not that long. It's like maybe like 50",
      "offset": 3067.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "lines of Python. But it's very very",
      "offset": 3068.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "unlikely that you'll stumble upon them",
      "offset": 3070.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "by accident if you don't even think if",
      "offset": 3073.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you don't even know the problem, right?",
      "offset": 3075.119,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "Like you're just trying Python things",
      "offset": 3076.16,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "and like how would you generate the",
      "offset": 3077.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "particular kind of 50 line Python",
      "offset": 3079.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "program that works. So there it was like",
      "offset": 3081.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "completely essential to hill climb like",
      "offset": 3082.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you have to like improve gradually and",
      "offset": 3084.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like see what works and trying to modify",
      "offset": 3086.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it a little bit in the air like in in in",
      "offset": 3088.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the neighborhood.",
      "offset": 3091.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Yeah. Can you can you actually talk a",
      "offset": 3093.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "little bit more about that the hill",
      "offset": 3095.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "climbing because you've incor you've you",
      "offset": 3097.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "found clever ways in which to build in",
      "offset": 3099.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "hill climbing into problems that at",
      "offset": 3102.16,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "least on their face",
      "offset": 3104.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "seem so discreet that hill climbing",
      "offset": 3106.2,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "would would not be possible. So I'm just",
      "offset": 3108.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "curious if you could just expand a bit",
      "offset": 3110.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "on the general idea behind how one takes",
      "offset": 3112.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "a discrete problem like looking at",
      "offset": 3115.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "different programs you know which are",
      "offset": 3117.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "fragile and digital um and somehow",
      "offset": 3119.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "incorporates hill climbing into that",
      "offset": 3122.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "process.",
      "offset": 3124.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Yeah, I guess for many problems like you",
      "offset": 3126.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you do have a natural kind of hill",
      "offset": 3128,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "climbing reward. For example, um when we",
      "offset": 3129.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "do this gradient based method for",
      "offset": 3133.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "finding uh matrix multiplication",
      "offset": 3134.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "algorithms like the natural reward will",
      "offset": 3136.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be the the loss, right? Like how close",
      "offset": 3138,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you are to the solution. Uh but uh",
      "offset": 3140.319,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "unfortunately that doesn't necessarily",
      "offset": 3143.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "work very well and what we've like what",
      "offset": 3145.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we found again and again is that like",
      "offset": 3148.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you have to be somewhat creative about",
      "offset": 3149.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "what kind of auxiliary rewards or like",
      "offset": 3152.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "auxiliary signals you can come up with",
      "offset": 3154.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and uh in particular for the matrix",
      "offset": 3156.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "modification case what was very useful",
      "offset": 3158.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "for us is to realize that you know if",
      "offset": 3160,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you have a curriculum of of matrix sizes",
      "offset": 3162,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "then it's probably going to be somewhat",
      "offset": 3164.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "easy to solve the small ones even with",
      "offset": 3166.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like you know simple gradient based",
      "offset": 3168.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "methods and And you can heal clip on",
      "offset": 3170,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "kind of this probability of soft, right?",
      "offset": 3172.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Like you let's say run your sh 10 times",
      "offset": 3174.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and then like how many times you",
      "offset": 3176.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually get the thing you're looking",
      "offset": 3177.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "for. Uh and then that can be like your",
      "offset": 3179.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "seal and then like because you have a",
      "offset": 3182.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "curriculum you can like say okay first I",
      "offset": 3183.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "want to solve 2x two then once I'm like",
      "offset": 3185.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "solid on that I can I want to like uh",
      "offset": 3187.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "continue evolving by trying to solve",
      "offset": 3189.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "3x3. So that was really helpful in that",
      "offset": 3191.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "particular case. But yeah, in general",
      "offset": 3193.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you just you just try things and you",
      "offset": 3195.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "build intuitions about like how do you",
      "offset": 3197.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "how do you approach coming up with this",
      "offset": 3199.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "auxiliary words it's very problem",
      "offset": 3201.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "specific and and maybe just to add one",
      "offset": 3203.8,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "like orthogonal point is that often",
      "offset": 3206.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "you're trying to solve one particular",
      "offset": 3208.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "problem and then maybe it comes with an",
      "offset": 3210.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "associated reward or maybe it doesn't",
      "offset": 3212.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "but it's nevertheless useful to also",
      "offset": 3215.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "optimize other things simultaneously. So",
      "offset": 3217.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "let's say you want to find a know a",
      "offset": 3219.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "matrix multiplication of a particular",
      "offset": 3221.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "size. You also want to find algorithms",
      "offset": 3223.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for other sizes because if you do that",
      "offset": 3225.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that allows you to explore the space of",
      "offset": 3228,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "algorithms in a more broad way and often",
      "offset": 3229.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "you can translate ideas that you develop",
      "offset": 3232.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for one matrix size later on to the",
      "offset": 3234.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "matrix size that you actually care",
      "offset": 3237.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "about. But if you only optimized for the",
      "offset": 3239.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "size you care about from the start with",
      "offset": 3241.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "maybe that idea wouldn't have been",
      "offset": 3242.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "useful initially. you first had to",
      "offset": 3244.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "develop it and refine it a bit further",
      "offset": 3246.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for a for a related problem. So the",
      "offset": 3248.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "overall point I'm trying to make is that",
      "offset": 3250.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it often intro makes sense to introduce",
      "offset": 3252.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "kind of similar other tasks and try to",
      "offset": 3254.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "improve on them even if you don't",
      "offset": 3257.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "intrinsically care about them.",
      "offset": 3258.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Could we touch on the benefits of",
      "offset": 3262.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "program synthesis in in general? So uh",
      "offset": 3264.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we interviewed Kevin Ellis recently.",
      "offset": 3267.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "He's the guy who invented Dream Coder.",
      "offset": 3269.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "fascinating guy. I used to work under",
      "offset": 3271.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Josh Tannon and um and the way he",
      "offset": 3272.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "described it coming from a cognitive",
      "offset": 3275.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "science point of view is um a lot of uh",
      "offset": 3277.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "program induction is about explanation.",
      "offset": 3280.48,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "It's about intelligibility. It's about",
      "offset": 3282.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "legibility. And there was this",
      "offset": 3284.599,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "incredible example when you were talking",
      "offset": 3286.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "about doing theuling of of jobs on, you",
      "offset": 3289.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "know, in Google's data center and you've",
      "offset": 3291.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "got this kind of matching problem and",
      "offset": 3293.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "whatnot. And I I think you said there",
      "offset": 3295.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "was some previous experiments where",
      "offset": 3296.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you'd used like an inscrutable, you",
      "offset": 3298.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "know, reinforcement learning model or",
      "offset": 3300,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "something like that and it wasn't",
      "offset": 3301.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "debugable. It wasn't intelligible. And",
      "offset": 3302.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "now you've got this beautiful three",
      "offset": 3304.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "lines. But even then there's the",
      "offset": 3306.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "question of is it legible? Have you",
      "offset": 3308.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "found things that are not legible?",
      "offset": 3311.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Remember there was that move um 37 in",
      "offset": 3312.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Alph Go and that was a famous example of",
      "offset": 3315.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of a discovery which was a little bit",
      "offset": 3316.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "weird and perhaps we wouldn't have",
      "offset": 3318.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "discovered it but maybe we understood",
      "offset": 3319.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it, maybe we could build theory around",
      "offset": 3321.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "it. But when when you look at some of",
      "offset": 3322.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "these discoveries as well, are you",
      "offset": 3324.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "finding deep abstract principles that",
      "offset": 3326.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you can derive from those discoveries? I",
      "offset": 3328.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "mean, talk me through all of that. Yeah.",
      "offset": 3330.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So, you can span the broad spectrum",
      "offset": 3333.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "actually. So with Alpha Evolve, you",
      "offset": 3335.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sometimes discover algorithms that are",
      "offset": 3337.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "really simple and they're so simple that",
      "offset": 3339.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you can like a human can verify that",
      "offset": 3341.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "they're actually going to be correct on",
      "offset": 3343.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "all inputs. And indeed, as you alluded",
      "offset": 3344.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "to, they're so simple that you're just",
      "offset": 3346.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "happy to submit them to production",
      "offset": 3348.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "almost immediately like no further",
      "offset": 3350.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "checks checks needed. and it's in a",
      "offset": 3352.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "completely different league compared to",
      "offset": 3354.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "trying to deploy a neural network which",
      "offset": 3356.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you have to think about things like",
      "offset": 3358.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "retraining it and hosting it and and and",
      "offset": 3360.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the resources of running inference and",
      "offset": 3363.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "all those kinds of things. So in indeed",
      "offset": 3365.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you can you can be in this very simpler",
      "offset": 3367.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "regime um for scientific discovery. can",
      "offset": 3368.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "also be in the regime where you",
      "offset": 3371.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "explicitly look for programs that are",
      "offset": 3373.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "interpretable. And you can do this by",
      "offset": 3375.64,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "setting up the skeleton that you ask",
      "offset": 3378.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Alpha Evolve to fill in in such a way",
      "offset": 3379.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that by design you expect it to be",
      "offset": 3381.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "fairly simple. And we actually maybe the",
      "offset": 3384.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "clearest example is already in the fun",
      "offset": 3386.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "search paper where um we looked for",
      "offset": 3388.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "these big capsets like a specific",
      "offset": 3390.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "mathematical object and it found a",
      "offset": 3392.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "function that we could look at inspect",
      "offset": 3394.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and we just notice that oh this function",
      "offset": 3396.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is is using the number four in this",
      "offset": 3398.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "interesting way that whenever you look",
      "offset": 3401.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "at the index in the for loop you just",
      "offset": 3403.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "look at the index modulo four and when",
      "offset": 3405.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you index into the the array you're",
      "offset": 3407.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "indexing at position i and i +4 and i",
      "offset": 3409.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "plus 8 like okay what's going on and and",
      "offset": 3412.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "just by inspecting the code we actually",
      "offset": 3414.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "were able to develop can think of it as",
      "offset": 3416.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like a mathematical insight or a",
      "offset": 3418.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "mathematical hypothesis and that",
      "offset": 3420,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "hypothesis turned out to be really",
      "offset": 3421.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "crucial for then improving the results",
      "offset": 3423.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "like we we took that insight from the",
      "offset": 3425.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "code incorporated that insight into the",
      "offset": 3427.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "next run and we got much better results.",
      "offset": 3429.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So indeed this can happen. Um but in",
      "offset": 3431.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some applications maybe you don't care",
      "offset": 3434.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "as much about interpretability and then",
      "offset": 3436.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "um alpha evolve can can develop even",
      "offset": 3438.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like very complex algorithms or a",
      "offset": 3440.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "sequence of algorithms where you will",
      "offset": 3442.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "perhaps not have the holistic",
      "offset": 3444.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "understanding of how exactly this",
      "offset": 3446.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "complex search heristic works but what",
      "offset": 3448.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you care about is the final result that",
      "offset": 3450.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the final result is as good as possible.",
      "offset": 3452.4,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "So you can you can span this this whole",
      "offset": 3454.559,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "spectrum and and maybe uh Alex would",
      "offset": 3456.68,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "also mention the the work that is not",
      "offset": 3460.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "from our group but another team has",
      "offset": 3462.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "applied fund search to uh cognitive",
      "offset": 3464.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "science to exactly discover",
      "offset": 3466.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "interpretable programs of behavior. So",
      "offset": 3467.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that is actually pretty cool application",
      "offset": 3469.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that you can do. Yeah. And also if you",
      "offset": 3471.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "maybe go back to the matrix",
      "offset": 3474.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "multiplication example there we kind of",
      "offset": 3476.079,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "build",
      "offset": 3478.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this grieving based machine learning",
      "offset": 3479.72,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "pipeline right like for for looking for",
      "offset": 3482.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "those algorithms and there I think if",
      "offset": 3484.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "you look at the proposed code changes by",
      "offset": 3486.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "alpha evolve you would see maybe two",
      "offset": 3488.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "type of changes two types of maybe",
      "offset": 3491.28,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "reactions to the changes I I I",
      "offset": 3494.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "experienced one is like yeah that makes",
      "offset": 3496.359,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "sense right like it's and it's it's",
      "offset": 3499.599,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "usually this it's usually the case of",
      "offset": 3500.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "machine learning right when you write",
      "offset": 3502.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "machine when you read machine learning",
      "offset": 3504.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "papers you're like oh yeah this makes",
      "offset": 3505.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "sense right like that's a good idea I",
      "offset": 3507.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "would maybe have done it myself and then",
      "offset": 3508.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "usually the problem is not coming up",
      "offset": 3511.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "with the idea it's like ideas are sort",
      "offset": 3513.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of cheap it's just like the problem is",
      "offset": 3515.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "coming up with the idea that actually",
      "offset": 3517.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "works like there's so many things that",
      "offset": 3518.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "make sense and then like only like five",
      "offset": 3519.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "of them actually work so like it gives",
      "offset": 3521.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you ideas that you kind of understand",
      "offset": 3523.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and can relate to but it's it's hard to",
      "offset": 3525.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "know up pure like which of those will",
      "offset": 3527.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "actually work right and then maybe other",
      "offset": 3529.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "type of ideas uh uh or like code changes",
      "offset": 3531.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "you would see is uh things you would",
      "offset": 3534.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "probably not even have tried just",
      "offset": 3537.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "because they're over complicated and",
      "offset": 3539.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "sometimes it's for a good reason,",
      "offset": 3540.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "sometimes it's for a bad reason. And for",
      "offset": 3542.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "example for the matrix multiplication",
      "offset": 3544.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "case you would see uh the kind of um so",
      "offset": 3545.92,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "we have uh this uh quantization loss",
      "offset": 3549.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "there because uh we want the solutions",
      "offset": 3553.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "to be integers or like some kind of",
      "offset": 3555.359,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "maybe fractions but specified range such",
      "offset": 3557.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that we can exactly verify in exact",
      "offset": 3560.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "arithmetic that they're correct and then",
      "offset": 3562.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we'll have this quantization loss which",
      "offset": 3564.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "will drive the solution towards that set",
      "offset": 3566,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of integers for example and then you",
      "offset": 3568.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "know normally we as humans would only",
      "offset": 3570.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "try like I",
      "offset": 3572.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "tuning the the uh the weight of that",
      "offset": 3573.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "quantization term or maybe in the worst",
      "offset": 3577.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "case like the schedule of the weight",
      "offset": 3579.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "like like maybe you will have less of it",
      "offset": 3580.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "in the beginning and then like more of",
      "offset": 3582,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "it in the end but uh what Alfrey Evolve",
      "offset": 3583.119,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "did was producing like a whole kind of",
      "offset": 3586.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "time evolving shape of the quantization",
      "offset": 3589.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "loss which which kind of again makes",
      "offset": 3592.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "sense right like you you wouldn't say",
      "offset": 3594.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that it's not going to work or you",
      "offset": 3595.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "wouldn't or otherwise you wouldn't",
      "offset": 3597.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "looking at it you wouldn't say that oh",
      "offset": 3599.359,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "this is amazing this is like definitely",
      "offset": 3600.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "going to work But the thing is like you",
      "offset": 3602,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "wouldn't even try it as a human because",
      "offset": 3604,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "it's like so complicated that you would",
      "offset": 3605.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "never even think about tuning such a",
      "offset": 3606.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "such a complicated like kind of function",
      "offset": 3608.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "which changes shape over time with",
      "offset": 3610.559,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "iterations.",
      "offset": 3612.319,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "That's pretty fascinating. Definitely.",
      "offset": 3615.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Um it's kind of like how human players",
      "offset": 3617.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "are able to glean some new insights from",
      "offset": 3619.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "uh chess, you know, from from Alpha Zero",
      "offset": 3622.319,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "and and whatnot. Um you did uh Alex you",
      "offset": 3625.359,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "also mentioned you know about how",
      "offset": 3628.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "surprised you were at the general or the",
      "offset": 3631.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "broad application the technique and I",
      "offset": 3633.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "know um you have some prior prior work",
      "offset": 3635.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "in robotics like I'm just curious for",
      "offset": 3637.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "for cases where it's more difficult to",
      "offset": 3640.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "assess the code you know so for robots",
      "offset": 3642.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you might be able to do some training",
      "offset": 3645.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "kind of in a virtual reality or",
      "offset": 3647.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "something but then at some point you got",
      "offset": 3649.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to just I don't know throw it out in the",
      "offset": 3650.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "woods and see if it's able to navigate",
      "offset": 3652.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to the other side or or something like",
      "offset": 3654.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that. How do you see bridging the gap",
      "offset": 3656.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "between uh easily automated uh",
      "offset": 3658.24,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "validation versus versus more complex",
      "offset": 3662.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "real world scenarios yet still being",
      "offset": 3666,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "able to apply alpha evolve?",
      "offset": 3668.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Yeah. Um I was thinking that maybe one",
      "offset": 3671.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "direction that makes sense and I saw",
      "offset": 3673.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "people doing that is uh looking into",
      "offset": 3675.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "trying to convert kind of fuzzy reward",
      "offset": 3678.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "functions into code. So for example,",
      "offset": 3681.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "maybe you have uh a reward function",
      "offset": 3682.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "which is defined by the image of the",
      "offset": 3685.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "final state you want to achieve, right?",
      "offset": 3687.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Or maybe you want to have a reward",
      "offset": 3689.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "function which is defined by like a",
      "offset": 3690.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "natural language description of what you",
      "offset": 3692.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "want to do. And then uh your task is to",
      "offset": 3693.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "convert that into a Python function",
      "offset": 3696.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "which actually scores it. And maybe",
      "offset": 3698,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "maybe it's not that hard to have a",
      "offset": 3699.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "binary word here for for the RL to",
      "offset": 3700.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually train the robot because you",
      "offset": 3703.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "know you can ask the visual language",
      "offset": 3704.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "model to to verify it. But uh it's very",
      "offset": 3706.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "hard to train against binary words. Like",
      "offset": 3710.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "I saw people um trying to use systems",
      "offset": 3711.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "like Alpha Evolve to find a piece of",
      "offset": 3714.799,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Python code which would uh provide an",
      "offset": 3717.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "auxiliary reward for kind of shape like",
      "offset": 3720.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "shaped reward basically right like to",
      "offset": 3722.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "drive you towards that binary reward to",
      "offset": 3724,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "to make the learning actually faster. I",
      "offset": 3725.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "think that that makes a lot of sense as",
      "offset": 3727.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "a direction and uh yeah I wonder I don't",
      "offset": 3729.359,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "know if that answers your question.",
      "offset": 3732.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Yeah I mean it does. Um but I'm also",
      "offset": 3735.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "thinking just in terms of the efficiency",
      "offset": 3738.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "of the procedure you know so for example",
      "offset": 3739.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you mentioned in the paper that many of",
      "offset": 3742.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the programs it generates they just fail",
      "offset": 3744.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "immediately they uh you know they crash",
      "offset": 3746.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "they're they're not valid programs those",
      "offset": 3749.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "are kind of easy to filter out um if you",
      "offset": 3751.119,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "have an evaluator um and then at the at",
      "offset": 3753.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the opposite extreme even once you've",
      "offset": 3756.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "got those programs say running the real",
      "offset": 3758.72,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "world test case um involves performing",
      "offset": 3760.72,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "experiments and clinical trials or a",
      "offset": 3764.799,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "robot trying to navigate a complex field",
      "offset": 3767.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "that may end up in a getting damaged or",
      "offset": 3770.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "something like that. So there's this",
      "offset": 3772.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "huge gap between kind of automatic",
      "offset": 3773.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "verification and expensive and I'm just",
      "offset": 3775.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "wondering how can we you know how can we",
      "offset": 3778.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "bridge that in some sensible way for",
      "offset": 3780.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "systems like alpha evolve. Um just",
      "offset": 3783.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "curious like if you have any thoughts on",
      "offset": 3786.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that. Yeah, we are thinking about those",
      "offset": 3788.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "things and and I think one one thing",
      "offset": 3790.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that Mate alluded to uh before with like",
      "offset": 3791.68,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "this proving like finding proofs which",
      "offset": 3794.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is binary rewards and then you uh can",
      "offset": 3797.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "try to ask LMS for a feedback to get",
      "offset": 3799.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like you again like some sort of shape",
      "offset": 3801.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "rewards to to drive you towards the",
      "offset": 3803.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "solution. I think that that that is a",
      "offset": 3804.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "direction we're also thinking about",
      "offset": 3806.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "because indeed like in many cases you do",
      "offset": 3808.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "have this kind of ladder of uh you know",
      "offset": 3810.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "more and more expensive evaluation like",
      "offset": 3812.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "for example if uh if you want to do some",
      "offset": 3814.4,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "sort of simulation based as uh maybe",
      "offset": 3817.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "maybe for robotics maybe for some like",
      "offset": 3821.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "biology or physics or whatever right you",
      "offset": 3823.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "can do some simulation based reward and",
      "offset": 3825.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "then like you ultimately want to try try",
      "offset": 3826.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "it in real world and maybe it's like",
      "offset": 3828.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "going to be again trying on the robot",
      "offset": 3830.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "maybe you go to the lab whatever like uh",
      "offset": 3832.319,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you do want like for every stage of this",
      "offset": 3834.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "evolation ladder which is more and more",
      "offset": 3837.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "expensive you don't want to have not",
      "offset": 3838.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just trying all the things that you",
      "offset": 3840.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "found that work on the previous stage",
      "offset": 3842.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but maybe have some sort of parization",
      "offset": 3844.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "mechanism which right can can be you",
      "offset": 3846.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "know can be a length feedback can be",
      "offset": 3848.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "something else can be like scores yeah",
      "offset": 3850.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "those are the things we're thinking",
      "offset": 3852.64,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "about and uh",
      "offset": 3854,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "maybe one project which also recently",
      "offset": 3856.599,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "came out of Google which kind of is very",
      "offset": 3859.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "good at this is co-scientists and they",
      "offset": 3861.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uh basically attack the same problem,",
      "offset": 3864.64,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "right? Like how do you how do you uh",
      "offset": 3865.92,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "score ideas which are very hard to",
      "offset": 3869.88,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "evaluate maybe with hard hard feedback",
      "offset": 3872.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and and I think they're doing really",
      "offset": 3875.039,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "good job at that.",
      "offset": 3876.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "Makes sense. And maybe just one thing to",
      "offset": 3879.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "add is that technologically I think",
      "offset": 3882.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Alpha Evolve is already well set up for",
      "offset": 3883.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this. We had this idea of evaluation",
      "offset": 3885.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "cascades where you evaluate a large",
      "offset": 3887.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "number of programs very quickly and then",
      "offset": 3889.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "a smaller number of programs you can",
      "offset": 3891.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "spend longer to evaluate. So you can",
      "offset": 3893.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just expand this cascade all the way to",
      "offset": 3895.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the regime where maybe you can only",
      "offset": 3897.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "afford to evaluate 10 things and you",
      "offset": 3898.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "actually have to go into the real world",
      "offset": 3900.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and run some real world experiments. But",
      "offset": 3902.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "in principle the the mechanism is there",
      "offset": 3904.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and I mean this is what you have to do",
      "offset": 3906.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "even if you try to solve the problems",
      "offset": 3908.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "manually or what actual researchers do",
      "offset": 3909.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like you have a finite budget. So first",
      "offset": 3912.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "you try to filter the ideas using",
      "offset": 3914.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "cheaper methods and then you only only",
      "offset": 3916.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "have the budget to to try the most",
      "offset": 3919.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "promising ideas on the most expensive",
      "offset": 3921.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "evaluation and just if we can mimic",
      "offset": 3923.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "that. Yeah. Exactly. Yeah. Same way",
      "offset": 3926.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "those are done. Yeah. So I'm curious",
      "offset": 3927.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "about LLMs. I mean first of all let me",
      "offset": 3930.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "give you my heartfelt congratulations on",
      "offset": 3933.28,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "Gemini Pro. It is ridiculous. It's so",
      "offset": 3936.64,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "good. Honestly, it's it's it's just I",
      "offset": 3941.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have no words to describe how good it",
      "offset": 3944.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is. But um we're in an interesting time,",
      "offset": 3945.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "right? Because in in Alpha Evolve, you",
      "offset": 3948.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "had an ensemble of um Flash 2.0, I",
      "offset": 3949.92,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "think, and and Pro 2. And it just raises",
      "offset": 3953.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "so many questions to me. What would",
      "offset": 3957.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "happen if you did it with 2.5?",
      "offset": 3958.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Presumably internally, you have even",
      "offset": 3960.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "better models that that we don't know",
      "offset": 3962.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "about yet. And how much how much uplift",
      "offset": 3963.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is there? And and if you think about it,",
      "offset": 3967.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "there's this parto curve of models and",
      "offset": 3968.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Google's models are on the parto curve.",
      "offset": 3970.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "So, you know, we're trading off cost and",
      "offset": 3972.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "latency and and performance and and",
      "offset": 3974.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "whatnot. And if you were just using one",
      "offset": 3976.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "model, there might be some optimal place",
      "offset": 3978.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "on that parto curve in alpha evolve. I",
      "offset": 3980.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "mean, maybe maybe we should have a small",
      "offset": 3982.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "model, but we should, you know, sample",
      "offset": 3984.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it loads of times or maybe we should",
      "offset": 3986.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "have a big fat model, but not sample it",
      "offset": 3987.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "as many times or with an ensemble, maybe",
      "offset": 3989.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "there's some perfect distribution of of",
      "offset": 3991.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of those models. Um but we really seem",
      "offset": 3993.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to have especially with the reasoning",
      "offset": 3996.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "versions of the models we seem to have",
      "offset": 3998.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "unlocked something which maybe wasn't",
      "offset": 3999.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "there until relatively recently. Could",
      "offset": 4002.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you talk to that? Yeah. So one thing I",
      "offset": 4004.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "can say specifically that is",
      "offset": 4007.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "particularly exciting to me about alpha",
      "offset": 4009.039,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "evolve is that it does get this uplift",
      "offset": 4010.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "from improving the the base language",
      "offset": 4013.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "model. So this was not necessarily the",
      "offset": 4014.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "the case in fund search but in alpha",
      "offset": 4016.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "evolve we do see this and we actually",
      "offset": 4019.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have a like a quantitative confirmation",
      "offset": 4021.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "again in the ablations that if you also",
      "offset": 4023.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "include the the the 2.0 pro in in the",
      "offset": 4025.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "ensemble then you get better results",
      "offset": 4028.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "compared to just using the flash model.",
      "offset": 4030.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "So it's it's very clear that we are like",
      "offset": 4032.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "leveraging the kind of the frontier",
      "offset": 4035.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "capabilities of these models and and of",
      "offset": 4037.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "course we cannot guarantee into the",
      "offset": 4039.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "future what will happen but at least for",
      "offset": 4040.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "now we are definitely like riding the",
      "offset": 4042.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "wave. So, so indeed we are we are very",
      "offset": 4044.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "keen in seeing how the base models",
      "offset": 4047.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "improve and what kind of uplift the",
      "offset": 4049.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "method is uh is going to get with this",
      "offset": 4051.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "together. Um the other kind of related",
      "offset": 4053.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "point I would just quickly mention is",
      "offset": 4056.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that um Alpha Evolve also offers this",
      "offset": 4058.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "opportunity which we haven't taken it",
      "offset": 4060.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "but just forward looking that it's a",
      "offset": 4062.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "it's a system that is able to enhance",
      "offset": 4065.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the capability of the base model like",
      "offset": 4067.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "the capability of the base model is",
      "offset": 4069.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "somewhere but then this system makes it",
      "offset": 4071.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "even better through orchestrating this",
      "offset": 4073.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like test time compute pipeline and",
      "offset": 4074.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "actually so much better that you can",
      "offset": 4076.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "make like a new scientific discovery. So",
      "offset": 4078.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "it raises the natural question like can",
      "offset": 4080.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we somehow distill this improved",
      "offset": 4082.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "capability back into the base model. So",
      "offset": 4084.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that's something that you would get if",
      "offset": 4087.52,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "you were to close the reinforcement",
      "offset": 4088.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "learning loop. It's not something we",
      "offset": 4090.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "have done with alpha evolve but that",
      "offset": 4091.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "possibility is is clearly on the table.",
      "offset": 4093.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "So maybe um you just mentioned improving",
      "offset": 4096.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the base model and you also mentioned in",
      "offset": 4098.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the paper you know using alpha evolve to",
      "offset": 4101.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "improve the infrastructure of uh of",
      "offset": 4103.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "alpha evolve and and the base models",
      "offset": 4106.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "itself. So, so you finally now managed",
      "offset": 4108.88,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "to close the recursive",
      "offset": 4111.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "self-improvement, you know, loop. Uh, I",
      "offset": 4113.48,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "don't know. That's going to trigger some",
      "offset": 4116,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "folks. Any any thoughts on that? What",
      "offset": 4117.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "would Schmidt Huber say?",
      "offset": 4119.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "At this point, we want to be like very,",
      "offset": 4121.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "you know, specific about what we have",
      "offset": 4123.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "done like we have we have found a way to",
      "offset": 4125.199,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "speed up the training of the next",
      "offset": 4126.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "version of Gemini by 1% and we have been",
      "offset": 4128.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "able to unstrand resources in in the in",
      "offset": 4131.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "the B data center. So currently if you",
      "offset": 4134.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "think about the the feedback loop it's",
      "offset": 4137.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "maybe on the order of months right like",
      "offset": 4139.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "um when we you speed up the training of",
      "offset": 4141.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the next version of Gemini then that",
      "offset": 4143.359,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "will take some time to to actually",
      "offset": 4144.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "arrive but indeed like we we do see the",
      "offset": 4146.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "steps in the in the direction that you",
      "offset": 4148.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that you described",
      "offset": 4150.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Alex any thoughts on the recursive",
      "offset": 4153.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "self-improvement of alpha evolve",
      "offset": 4155.96,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "yeah I mean I don't think I have to have",
      "offset": 4159.199,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "anything to add like as mate said it's",
      "offset": 4161.359,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "uh it's interesting to see how how pun",
      "offset": 4163.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "out and uh we are seeing some signs of",
      "offset": 4165.759,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "uh you know helping the Gemini training",
      "offset": 4168.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "ground. So yeah that's exciting on in",
      "offset": 4170.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that in that thread though we want to",
      "offset": 4173.359,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "have more autonomy. So, so right now",
      "offset": 4175.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "this is very much a kind of didactic",
      "offset": 4177.199,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "exchange between the human supervisor,",
      "offset": 4179.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "we select the problems, we design the",
      "offset": 4181.359,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "evaluation functions, we seed the",
      "offset": 4183.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "solutions and so on. And I just wonder",
      "offset": 4185.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "what would the next step of autonomy",
      "offset": 4187.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "look like, you know, in terms of maybe",
      "offset": 4188.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "we could actually have the thing imagine",
      "offset": 4190.319,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "what its own evaluation function is and",
      "offset": 4192.319,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "maybe it could kind of go several steps",
      "offset": 4194.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "further. What would that look like? to",
      "offset": 4196.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "be honest from my perspective like I",
      "offset": 4198.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "guess automating some things is is cool",
      "offset": 4200.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and exciting but also at the same time I",
      "offset": 4202.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "would even lean towards less automation",
      "offset": 4204.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "in sense like right like uh I think the",
      "offset": 4206.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "thing that makes uh alpha evolves so",
      "offset": 4209.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "cool and powerful is is kind of this",
      "offset": 4211.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "back and forth between humans and",
      "offset": 4213.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "machines right and like the humans ask",
      "offset": 4214.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "questions so the the system gives you",
      "offset": 4217.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "some form of the answer and then you",
      "offset": 4219.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like improve your intuition you improve",
      "offset": 4220.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "your question answering question asking",
      "offset": 4222.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "ability right and you ask more questions",
      "offset": 4224.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "So we are thinking a lot about uh",
      "offset": 4227.4,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "providing access to to Alpha Evolve to",
      "offset": 4230.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "academics as trusted testers. like",
      "offset": 4233.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "seeing what they can do with it and",
      "offset": 4235.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "while kind of trying to build that and",
      "offset": 4237.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "trying to build the UI we're thinking a",
      "offset": 4239.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "lot about you know not just implementing",
      "offset": 4241.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "the thing we have as a website but just",
      "offset": 4244.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like what would be the next kind of",
      "offset": 4246.159,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "level of human and I human AI",
      "offset": 4249.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "interaction here like what can the",
      "offset": 4252.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "humans do in the like kind of intervene",
      "offset": 4254.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "in the process like maybe they want to",
      "offset": 4256.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "if the humans would want to supervise",
      "offset": 4258.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the pro like I don't know comment on",
      "offset": 4260.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "ideas and like inject more ideas and",
      "offset": 4262.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "things like like we're exploring that a",
      "offset": 4264.159,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "lot and I think it's very exciting to",
      "offset": 4265.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "see like what can be done in this kind",
      "offset": 4267.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "of symbiosis uh space.",
      "offset": 4268.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "So f final question you said in the",
      "offset": 4273.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "paper that um I think I think you",
      "offset": 4275.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "mentioned on the order of a 100 compute",
      "offset": 4277.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "hours to evaluate a new solution that",
      "offset": 4279.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "was in section 2.3 but just before",
      "offset": 4282.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "everyone at home rushes to reimplement",
      "offset": 4284.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "alpha evolve could you just give us some",
      "offset": 4287.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "examples maybe on the the matrix",
      "offset": 4289.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "multiplication thing I mean how many",
      "offset": 4291.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "compute hours you know did it run for",
      "offset": 4292.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "how how much is it realistically costing",
      "offset": 4295.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to do this yeah so so one nice feature",
      "offset": 4297.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "of alpha above is that it is really",
      "offset": 4300.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "elastic so it can match the difficulty",
      "offset": 4302.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "of your problem. So if you ask it to",
      "offset": 4304.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "solve a problem that's actually not that",
      "offset": 4306.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "difficult and maybe it's still an open",
      "offset": 4308.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "problem but no one has really worked on",
      "offset": 4309.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it then maybe even if you ask a chatbot",
      "offset": 4311.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it would almost solve it or solve it in",
      "offset": 4313.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that case alpha will also give you the",
      "offset": 4315.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "answer basically immediately and it will",
      "offset": 4317.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "not cost uh a lot at all but if you ask",
      "offset": 4320,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "for you know like a very difficult",
      "offset": 4323.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "problem maybe like decades open decades",
      "offset": 4325.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "long open scientific problem then you do",
      "offset": 4327.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "expect that it's a difficult problem",
      "offset": 4330.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you'll need to spend more time playing",
      "offset": 4331.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "with different ideas and iterative ly",
      "offset": 4333.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "building on top of that and the nice",
      "offset": 4335.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "feature of Alpha Evolve is that it is",
      "offset": 4337.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "able to sustain the scaling and as you",
      "offset": 4339.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "keep running it for longer find better",
      "offset": 4341.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and better ideas. I like I know it kind",
      "offset": 4343.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of maybe sounds trivial but I don't",
      "offset": 4345.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "think it's actually easy to build these",
      "offset": 4347.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "systems that are able to sustain this",
      "offset": 4349.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "continual improvement without plateauing",
      "offset": 4351.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "at at some point and and in this case",
      "offset": 4353.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "with Alpha Evolve you you see this",
      "offset": 4355.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "elasticity kind of stretching all the",
      "offset": 4357.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "way to to making new scientific",
      "offset": 4359.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "discoveries. So I think that that's a",
      "offset": 4360.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "nice feature of the system and so to",
      "offset": 4362.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "answer your question like concretely it",
      "offset": 4364.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "depends on the difficulty of the",
      "offset": 4367.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "problem. Uh so if the problem is",
      "offset": 4369.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "difficult then you do expect that you",
      "offset": 4370.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "will need to investigate more ideas and",
      "offset": 4372.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "spend more compute. If it's easier then",
      "offset": 4374.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you can have the answer very quickly. So",
      "offset": 4376.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "if you think about the problems that we",
      "offset": 4379.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "actually presented in the paper then um",
      "offset": 4381.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "even within matrix multiplication some",
      "offset": 4384.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "matrix sizes are much easier than",
      "offset": 4385.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "others. So the the compute would be",
      "offset": 4387.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "vastly different and the same goes for",
      "offset": 4389.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the uh open problems in maths like some",
      "offset": 4392.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "of them are fairly easy some of them are",
      "offset": 4394.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "very difficult. So there is there is no",
      "offset": 4396.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "no single answer that would yeah",
      "offset": 4398.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "position you on this on this spectrum",
      "offset": 4400.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "without knowing the the problem and and",
      "offset": 4402.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "unfortunately as is the case with open",
      "offset": 4404.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "problems often you don't actually know a",
      "offset": 4406.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "priority how difficult it is. So you",
      "offset": 4409.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "can't even predict ahead of time and you",
      "offset": 4411.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "know sometimes you try and you don't",
      "offset": 4413.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "find anything better. that can also",
      "offset": 4415.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "happen.",
      "offset": 4417.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Awesome. Well, um, guys, it's been such",
      "offset": 4419.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "an honor having you both on MLSD. Thank",
      "offset": 4422,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you so much for joining us today. Thank",
      "offset": 4424.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you. Thank you so much for inviting us.",
      "offset": 4425.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Yeah. Awesome.",
      "offset": 4427.36,
      "duration": 3.2
    }
  ],
  "cleanText": "We've had exclusive early access to the brand new Google AlphaEvolve paper, which just got released one minute ago. We did a technical interview with the authors before anyone else. The paper itself drops a bombshell, setting world records for many algorithmic and mathematical challenges. In the world of computer science, few problems are as fundamental as matrix multiplication. For over half a century, a specific efficiency benchmark in this domain, particularly for 4x4 matrices, seemed insurmountable. The search space for optimal algorithms is immense, making exhaustive exploration practically impossible, even for relatively small matrices. In 1969, Vulkist Drason revolutionized the field by discovering an algorithm to multiply two 2x2 matrices using only seven scalar multiplications, down from the standard 8. The established best practice for larger matrices like 4x4s was to apply Strassen's 2x2 method recursively. For 4x4 matrices, this meant 7 * 7, resulting in 49 scalar multiplications.\n\nToday, AlphaEvolve beat this record. Google DeepMind has a long history of building AI systems which actually invent new knowledge through experimentation and iteration rather than just building a glorified database. We saw AlphaGo, which beat Lisa Dole, learning from human games and even surpassing champions through self-play. AlphaZero was purely self-play, and AlphaFold predicted millions of 3D protein structures that had never been measured experimentally, and AlphaDev discovered faster sorting algorithms. Google DeepMind recently has been focused on scientific discovery with AlphaTensor, which framed the problem of finding faster matrix multiplication algorithms as a game, achieving breakthroughs. And FunSearch took us even further, using large language models to find new mathematical solutions by evolving code. And now, AlphaEvolve represents the next stage in this lineage. For the usual case of matrices that have arbitrary numbers in them, still, nothing better was known than doing stress and twice using 49 multiplications. So, we're like really excited when we used AlphaEvolve in this setting. We actually didn't even hope that it would find something better than 49 because we're trying for so long with AlphaTensor. We just ran it for completeness because we wanted to have this, you know, this table in the paper that we actually have that we tried on all the sizes up to up to five or six, and remarkably, it found a faster algorithm which uses 48 instead of 49 multiplications.\n\nIt's a bit like cursor on steroids. It iteratively refines algorithms, drawing on the creative power of LLMs using metalearning, library learning, automated evaluation, and evolutionary search. The AlphaEvolve paper describes it as an evolutionary coding agent which substantially enhances capabilities of pre-trained large language models on difficult tasks. Of course, their first paper was FunSearch, which was very, very similar. The main difference I think was that it was just searching for a single function rather than AlphaEvolve, which can essentially work over an entire codebase, to just demarcate adaptable regions in a codebase, and it can search for those, and of course, it's even optimizing for interactions between those functions in different parts of your codebase. One potential issue, as Keith Dugar pointed out, is the classic halting problem in computer science. But there's also a really subtle limitation here I wanted to explore with you guys a bit, which is running the programs themselves. You know, we face issues, right? You face issues of the halting problem for one thing, like you start running some code, like the code compiles fine or doesn't crash immediately, and it starts running, but you know, maybe some time goes by, like an hour, and you're like, \"Well, gez, is this thing ever going to terminate?\" In theory, you're of course right, you can never tell that if you run an algorithm for longer what it would have done, but in practice, it actually hasn't been any sort of issue for us in the applications that we looked at. Alexander Novikov added that this challenge is very much like a fundamental aspect of human research too. How do you know that you should stop working on your problem as a human? Uh, like, or maybe like, maybe you spend a month more and then you solve it, right? It's it's it's hard. I mean, of course, there's still there's still limitations. They didn't bite much on my halting problem, you know, limitations. So, like in practice, you know, it was it was it was okay for them, but they had these very well-chosen, well-chosen problems, right? Were like, \"Okay, matrix multiplication, you know, if it's any slower than Strassen's algorithm, you know, maybe you can you can turn it off, but the problem is that that restricts your open-ended search capabilities, right? You can't go down the path of a potentially slower algorithm for now that may get you to a stepping stone that hops you over to something more efficient, right?\" It is interesting, isn't it, that so many breakthroughs in science are unknown unknowns. We might have a nose for what is interesting, but we never really know where to find the answers. And so often it's the case that we find them through sheer luck. But there must be at least one step we can take in the direction of mechanizing and accelerating this process. In the case of matrix multiplication and several other scientific problems which had a clear evaluation function, this new evolutionary approach achieved much which decades of human research could not. We just ran it for completeness because we wanted to have this, you know, this table in the paper that we actually have that we tried on all the sizes up to up to five or six, and remarkably, it found a faster algorithm which uses 48 instead of 49 multiplications. Yeah. When when one of my teammates messaged on the channel that, \"Oh, it seems like we have this result,\" I just couldn't believe it. Like, \"Let's triple check it.\" AI's growing ability to generate entirely new, provably correct algorithms can advance the frontier of science. And the really cool thing is that AlphaEvolve has already been applied to optimize mission-critical real-world systems within Google. They're able to speed up really already heavily optimized pieces of important computational infrastructure within Google. For instance, take Google's massive data centers efficiently. Scheduling computing jobs is a really complex operation. If done suboptimally, expensive servers will sit around idling. The Google engineers placed a candidate solution into AlphaRevolve and then evolved a smarter heuristic which assigned jobs to machines much more efficiently. They said post-deployment measurements across Google's fleet confirmed the simulator's results, revealing that this remarkably simple yet effective function continuously recovers on average 0.7% of Google's fleetwide compute resources which would have otherwise been stranded. Now that's a huge saving on Google's scale, right? And in another instance of self-improvement, it even found ways to accelerate the training of the very Gemini models which powers AlphaEvolve itself. We have found a way to speed up the training of the next version of Gemini by 1%, and we've been able to unstrand resources in the B data center. This instance was also interesting because it didn't generate the solutions but also the programs which generated them. Whenever you look at the index in the for loop, you just look at the index module of four. And when you index into the array, you're indexing at position i and i + 4 and i plus 8. Like, \"Okay, what's going on?\" And and just by inspecting the code, we actually were able to develop, can think of this like a mathematical insight or a mathematical hypothesis. And that hypothesis turned out to be really crucial for then improving the result. A key aspect of AlphaEvolve's sophistication lies in its flexible approach to representing the search problem. The alarms will propose you like a broad range of things, and some of them will be stupid, some of them will be amazing, some of them will be like really weird, and then by having the ability, you can filter through those and identify the ones that are actually kind of important and improving things. So, isn't it really cool that rather than trying to generate the solution itself, AlphaRevolve can, just like Inception, generate the thing which generates the solution. We should play some Inception music in the background. The thing we found really interesting about AlphaRevolve is that it's still very much a humans in the loop thing. Humans identify what's interesting. They find problems that have clear evaluators. They place candidate solutions in the loop, and then AlphaEvolve will traverse this cone of possibilities and make jumps along the way, and then the cycle continues. So this is very much sketching a future of AI where there is a strong collaborative loop between humans and AIs, and we should just bring in how you modeled this representation. So you had these different approaches. You could directly model the solution. You could model a constructor function. So, you know, you're actually learning a function which itself constructs the solution. You could be learning a search algorithm, which is what you did with the matrix multiplication. And you also spoke about co-evolution as a possibility. So many people are talking about this vision of AIs which can autonomously just drive cars or just do anything, generate content without any supervision from humans. And that hasn't really panned out, to be honest. And certainly a lot of the content on the internet is slop. Do you remember that dead internet theory by Illuminati pirate? This guy on an internet forum a couple of years ago. Uh, he said that by now most content on the internet will be generated by AI, and it will have a kind of superficiality to it. And he was right. But it doesn't necessarily mean that AI is bad, right? What's missing is that we need to have this exchange. We need to use AIs as tools, and we need to guide them and refine the results and do the process iteratively. That's kind of what AlphaRevolve does. It mechanizes the correct way of using AI. The thing that makes AlphaEvolve so cool and powerful is this back and forth between humans and machines, right? And like the humans ask questions. So the system gives you some form of the answer, and then you like improve your intuition, you improve your question-answering, a question-asking ability, right? And you ask more questions. I gave this talk a few times about like generation AI, and the big warning I gave is just the rise of mediocrity. We're just going to be inundated and flooded with mediocrity, and the best content will still be produced by the most skilled people. And all that's going to happen is as this tide of mediocrity grows larger and larger, people will get hungrier and hungrier for the cream, for the things that are rising to the top, right? So, um, I think all that's going to happen in the end is that skilled people, their productivity is going to just rise, um, and they'll they'll continue to be differentiated from the mediocre sort of hordes that are whose productivity is also enabled. So, you know, the you know, the hordes become more productive, and experts become more productive, and and just as a whole, we all become more productive. And while we're on the subject of amazing innovative architectures for discrete program synthesis and reasoning and whatnot, why don't you consider applying to work at Tufa AI Labs? That's if you're an ML research scientist or ML engineer. Um, Benjamin Crouzier is running the lab. It's in Zurich at the moment, and they're thinking about opening an office in San Francisco as well. They would love for you to get in contact with them and apply if you're interested in working for them. So give Benjamin a shout, guys. Welcome to MLST. It's an honor to have you both here. Thank you for having us. So we have had privileged access to read your AlphaEvolve paper. It's really, really exciting because this is very much up our street. We love program synthesis, and we love evolutionary methods. I mean, we've had Kenneth Stanley on, and um, uh, you know, Jeff Clune, I was speaking to him at NeurIPS, and of course, he wrote the MAP Elites paper, which influenced some of the stuff that that you guys have done, but you know, just just from a thousand miles away, could you describe the work that you've done? Yeah. So we are presenting a coding agent which we called AlphaEvolve. And what this agent is able to do, it's uh, it's able to design quite advanced algorithms. When I say advanced, I mean it's algorithms that are able to make new discoveries in the sciences, and we have quite a few examples in mathematics and computer science in this paper. Or on the practical side of things, they're they're able to speed up really already heavily optimized pieces of important computational infrastructure within Google.\nYeah, I'm curious. I mean, what uh, what made you go down the path of including EA algorithms, you know, evolutionary algorithms? What um, what kind of said to you guys, \"This is a component that we need in a hybrid system along with, you know, verifiers and LLM and whatnot to to take us in a step forward?\"\nYeah, I think if you consider the process of scientific discovery, then it's a very natural choice. Uh, and you mentioned you you spoke to Kenneth, and uh, like evolutionary algorithms on a high level, they give you this this diversity of the in the explor in the exploration process, making sure that you don't early on in the process kind of just zoom in on into a particular approach which might be sub-optimal in the end, but you keep exploring the vast array of possibilities that you have, and especially when you think about trying to solve like really difficult problems and making new scientific discoveries, then a priority there is no way of knowing what is going to be the right approach. So you do need to make sure that you keep exploring different possibilities, and evolutionary algorithms are just a good technical tool that fit the bill uh, really well for this purpose, and also I think it's they're like very fun to play with, right? Like if you want to set up an RL algorithm, it's it's going to take you like, I mean, depending on on the algorithm, I guess, but it's going to take some some time, right? And with uh, EA algorithms, you can just do it, right? Like it's you have LM APIs, you just call things, you try things, it's\nAlex, can you just um, just sketch out the architecture? Many folks at home will will be familiar with FunSearch, for example, and I guess this is an evolution of that, you know, pun intended, but um, can can you just kind of sketch out how the whole thing works? Yeah, of course. Um, I mean, should I assume that people are aware or should I start with from scratch with fans stuff bits? Yeah, let's keep this bit really, really, really simple. We'll kind of um, the way we'll do the show is we'll have a progressive disclosure of complexity. So we'll have a have a hook, and it'll be super, super broad, and then we'll kind of progressively get more detail. But you know, this bit just do like super high level. How does the whole thing work?\n\n\nYeah, makes sense. And we'll also show you know diagrams from your paper and any others you give us and you know animations and some visual aids.\nAwesome. Uh, yes. So the high-level architecture of AlphaEvolve is it's an evolutionary method where you um, basically pair the, so we only focus on problems where you have a way of evaluating the progress, right? Like for any given proposal, for any given like piece of code that the system gives you, you can automatically test if it's good or not and like how good it is. And this is I think the key aspect of like maybe the results we got, that like this having this simulator is, on the one hand, it restricts you to the set of problems where you, you have, you can have it, but on the other hand, it's quite a broad range of problems, and then it gives you a lot, right? Like you can quickly iterate and like get feedback. And what in particular it gives you is that you can pair the creativity of LLMs with the simulator, right? So like the LLM will propose you some kind of like a broad range of things, and some of them will be stupid, some of them will be amazing, some of them will be like really weird. And then by having the ability, you can filter through for those and identify the ones that are actually kind of uh, you know, important and improving things. And then this kind of pairing LLMs with uh, evaluators is kind of wrapped around in an uh, evolutionary pipeline which tries to iteratively identify the most promising pieces of code and then like focus on improving those and like exposing them to the LLM, like here's what we tried before, this thing worked, this thing doesn't work, like please try to propose a new thing. And then maybe the final ingredient is scale, right? Like uh, making those things in parallel.\n\nSo you, you mentioned that needing the evaluator, it limit, it limits the class of problems. But there's also a really subtle limitation here I wanted to explore with you guys a bit, which is running the programs themselves. You know, we, we face issues, right? You face issues of the halting problem for one thing, like you start running some code, like the code compiles fine or doesn't crash immediately, and it, it starts running, but you know, maybe some time goes by, like an hour, and you're like, well, geez, is this thing ever going to terminate? You know, is this going to contribute to my gradient or not? I don't know. So maybe I have to just terminate it after some resources are consumed, and but if I waited just, you know, five more minutes, I would have gotten an answer that was the godly algorithm, right? So like this fundamental problem, how do you, how do you deal with it now? How do you think we're ever going to get around issues like that? You know, what do you envision for overcoming that limitation?\n\nSo in, in theory, you're of course right, you can never tell that if you run an algorithm for longer what it would have done, but in practice, is it actually hasn't been uh, any sort of issue for us in the applications that we looked at. And one concrete thing I can say is that often uh, you might frame the problem in the way where the kind of time constraint is built into the problem definition. So let's say you could say uh, I'm trying to solve this open problem in mathematics, and I'm looking for a search algorithm that is able to make progress on this open problem, but I want a search algorithm that is able to make progress in 10 minutes. So that is part of my problem definition. And then when I'm evaluating the proposals that the language models make, I only run them for the 10 minutes. And so I, I only explore the space of algorithms that is able to make something happen within those 10 minutes. Sure, I might miss out on algorithms that would have done even better when run for longer. Um, so that is indeed like a principle thing that you can never eliminate, but in practice, it actually hasn't been an issue for us.\n\nAnd I guess uh, it's kind of this question is also fundamental to internal research, right? Like how do you know that you should stop working on your problem as a human? Uh, like or maybe like maybe you spend a month more and then you solve it, right? It's, it's, it's hard. I don't know.\nYeah, it's like the secretary problem.\nUm, one thing I'm I'm fascinated in, you know, interviewing so many people in the space is, you know, we, we speak about diversity, preservation, novelty, serendipity, open-endedness, creativity, and we really want to design algorithms that can break free, that can make creative jumps. I mean, Demis spoke about this ladder of creativity where you have inventive creativity. That's the thing that we really need. And as I understand it now in in your system, uh, it's a little bit like an automated version of cursor where you kind of have these code gates and you put an initial solution in there. So there's a little bit of domain knowledge. There was one example where I think you decided to use a bin packing um algorithm for doing theuling on, you know, on the um uh, the hardware um, you know, at Google. And I guess the question is, is that depending on the starting solution, there's a kind of cone of things of leaps that we can make, and can you speak to any examples where the system made really imaginative leaps?\n\nYes. So I can talk to that, and indeed you identified one interesting feature of the system, which is that depending on what you tell it at the beginning, you can guide the process. So if you give it fairly specific instructions or you ask the system to start from a specific type of solution, then what it will usually do, it will like squeeze out the juice of that idea that you gave it or squeeze out the juice of that initial solution, see how it can tweak it and bring it to its maximal potential. But and sometimes this is the right approach to take, maybe when the problem is particularly difficult or has some specific features. But by default, you would start with a solution that's like really, really empty. So, so you, you give AlphaEvolve a code skeleton where um, where all the, like all the functions are like almost an empty implementation, and you just like return zero or return false and so on. And and you just let it, let it to be completely creative, and so it just has to rely on like its background knowledge of the base LLMs, and it can explore in all possible directions. And the um, the evolutionary algorithm makes sense that you preserve the diversity as you as you keep doing the exploration. And um, one particular example I can talk about is, uh, we applied AlphaEvolve to discovering algorithms for matrix multiplication, and we did it by asking it to design kind of a search algorithm, a gradient-based search algorithm that looks for for the matrix multiplication algorithms in turn. So, it's a bit of a meta thing, like you, you look for an algorithm that finds an algorithm, but within that first algorithm, the search algorithm, uh, we started from a really simple code skeleton, giving it basically nothing, like just we just told it use gradients basically. And then it was able to to write these complex um, loss functions and update functions which had all all sorts of tricks about uh, penalizing various behaviors and introducing randomness in completely unexpected ways, which were like, okay, wow, like, like these, this is the type of code that maybe a human could plausibly write, but but would they have actually thought of writing this particular piece of code? Uh, that, yeah, that that was really a an aha moment, at least for me, that wow, this is doing something kind of humanlike, but not something that obviously a human would try. And uh, maybe another cool story about this is that I don't think we, we added it to the paper because maybe paper is not the right format for this, but uh, uh, Adam and our team did a cool experiment on trying to give um, advice from humans to to the system, and then he asked a few people for like, you know, you please think about this problem for 2 minutes, you please think about the problem for 30 minutes, and then like compare it uh, and write down the notes and then give it to the system to kind of guide it through through the process. And then he compared like what what would be the outcome of that, and you can see that as Mate was saying, it's kind of squeezing all the juice out of the idea. So it will like preserve the essence of the idea because it kind of guides the LM towards things like that, but it will optimize a lot of small things, and yeah, and like in a lot of cases will be in intelligence ways, intelligent ways, in a lot of case it will be in kind of, you know, I'll try a bunch of things and one of them will stick, but yeah, it's kind of cool to watch, yeah.\n\nAnd so at the moment, the architecture has, let's say, maybe two sources of um, base knowledge. It has the base model itself, right? Which is obviously compressed, you know, all the corpus it's been trained on, including tons of code and algorithms and numerical recipes and whatever. And then it has the the starting program that you put in, that the sort of gated bits of logic and and whatnot. Is there, is there any room for an augmentation that's almost a middle ground where there's say a secondary database that contains modules or pieces of code that have known to been very be very effective in other problems that it can somehow simultaneously draw on?\n\nYes. Uh, so a few points on this one is that you mentioned there are two sources of kind of knowledge in the system, and debatably I would say that there is a third source, which is that the system can decide to augment its own knowledge. What I mean specifically is that the system proposes an algorithm, and then it's going that algorithm is going to be executed on a machine, and you will see the results of of having run that algorithm. So on a sufficiently high level, you can think of it as the system can decide, okay, I want to gain the piece of knowledge, what does this algorithm do when you actually run it. So, so that that is an important component, but but maybe going closer to the essence of your question, indeed. So there could be a a separate, either cur, like a human curated database of useful modules or any of that sort, but even more excitingly, this database can be curated by the system itself. And so this is an idea that um, we are thinking about for AlphaEvolve, but there is a a related idea that is already implemented and mentioned in the paper, which is not building a curated set of modules which are generally useful, but a curated set of prompts that tend to work well. So there is this idea called the meta prompting described in the paper where um, we actually ask our language models to propose their own prompts. So just tell them what we are trying to do, like we're trying to do this evolutionary algorithm for um, improving on this particular problem, and we are going to to to prompt you with this particular prompt, but before we do that, please propose a modification to this prompt itself, and then we curate set of prompts that actually um, work well for this purpose. And so in spirit, that's a similar idea, although it's curating prompts rather than programs, but both make sense to me.\n\nYeah, I, I think if I understand um, the direction Keith was going in, because everything you've just described is is fascinating. It's various forms of metalearning to basically create diversity and divergence. But um, I feel that the next trillion dollar business could be where we, you know, like in program learning, right? We want to construct a library, and right now we're, we're kind of expanding this library of functions for a particular purpose. But what if the library itself was the new oil? You know, what, what, what if there was strong robustness between these programs that we've learned for this thing, and they generalize um, you know, through some analogical relation to other programs and other domains. I mean, what, what if we just had like the new language model paradigm was actually a kind of program database? I mean, do you think that could work?\n\nYeah, I think that's a fascinating idea, and uh, like we see maybe some first glimpses of that. So it is true that for now we are going in the death direction, we just focus deeply on a problem and just try to solve that problem. But even within that path, let's say when we worked on uh, matrix multiplication, we saw that um, like when we run AlphaEvolve different times, we discover slightly different algorithms, and then it's actually a useful technique to uh, take those algorithms and use them to initialize future experiments. So that's kind of a first step towards like building this database of, you know, things that were useful in the past to act as inspiration for solving future problems. Um, so yeah, that maybe Alex has more thoughts. Maybe another similar thing we kind of I feel that is happening is not that we store, not that we use the necessarily the database of programs we produced over all the experiments we did, but kind of our own human intuition as like users of the system is definitely evolving, and I kind of feel like a consultancy, right? Like person, like we, we collaborate with a lot of teams at Google trying to help them run things with AlphaEvolve, and like through that process, we kind of gain a lot of knowledge of like what works, what doesn't, like what what should, what should we try next, and and this kind of sort of things, like somewhat similar to what you're describing, right?\n\nYeah. I mean, I can almost imagine AlphaEvolve having its own repo, you know, whether it's um, internal or or maybe be nice enough to uh, put it up on GitHub for us, but it would just be constantly evolving and contributing to its own repo and maintaining it and categorizing it and people can go take a look. I wonder if uh, AlphaEvolve's come up with a better search algorithm, go check the the search area and see if it's got anything new there. Yeah, like technologically, I, I don't see hurdles for this. It's maybe the organizational question of how exactly to make it happen. And one particular technological piece that's already there is that we already now look for programs that work well across a range of tasks. And right now maybe this range is let's say fairly constrained because we care about a constrained range, but there's nothing blocking us from expanding it. So for like concretely, we would be looking for search algorithms that are able to find, let's say, matrix multiplication algorithms for different sizes simultaneously. So there is this sort of generality, but there is nothing preventing us from saying, let's look for search algorithms that actually work well across a much broader range of tasks. That's not just matrix multiplication, but also other search problems.\n\nCan we meditate on the matrix multiplication thing only because that's that's the headline hook of this video. You know, I think we're going to start the video by saying there's this amazing result. There was this, you know, Strassen guy 56 years ago. He had this big result. Now, AlphaEvolve is has just defeated it. There's quite a few things to explore here. I mean, obviously like please just explain the whole thing, but also there's some interesting stuff around we went up to I think a maximum of this is like um, complex 2D matrix multiplication, and you went up to rank six, and even that was a bit of an overshoot, and there was some interesting properties where as the rank went up, it started to get a little bit sketchy, but it, it did improve more. Can you, can you just like, you know, talk through\n\n\nSo let me start maybe with the high-level picture first.\nSo multiplying matrices, that's like a super basic operation, and some of us get taught this operation in high school, and there is a very specific way how you multiply matrices.\nWhen you're taught to do this in high school, like you at every step, you need to take one row of one matrix and one column from the other matrix.\nYou compute like the scalar dot product of these two things, and that gives you one entry in the output.\nAnd so this is one specific algorithm, which is kind of the basic algorithm for multiplying matrices.\nFor every element in the output matrix, you'll need to do this one dot product.\nAnd for a long, long time, people thought that this was kind of the obviously the only way to multiply matrices.\nHow could there even be something better?\nAnd then you mentioned Fer Strassen in 1969 was really a shock to the mathematical community.\nHe wrote this paper saying that actually, there is a faster way of doing it, and so already for multiplying 2x2 matrices, so the smallest non-trivial case, if you do it the high school way, you need to do eight multiplications because there are four entries in the output matrix, it's a 2x2 matrix, and each scale, like each inner product requires two multiplications, so four * 2 is eight.\nBut Strassen, he came up with this ingenious way of making a procedure that only requires seven multiplications, and it's kind of a magical procedure where you build some combinations of entries from the first matrix and the second matrix, and you multiply them, and then you are able to combine the seven products in such a way that you get these magical cancellations, and the result is just correct.\nSo this was a big surprise in 1969, and it opened this entire new area of research, like, okay, so for 2x2, you can actually do seven instead of eight.\nSo then people quickly proved that seven is actually optimal for that small case.\nBut already for 3x3 matrices, which you can think, okay, I mean, that that's like laughably small, surely people must have figured out what is the best way to do that, but we still don't know.\nSo even today, we know that you need at least 19 multiplications to do that, multiply two 3x3 matrices, but the best algorithm we have is using 23.\nSo there's this gap between 19 and 23 that people just haven't been able to close for years, and the reason is that even though the matrices are very small, the space of possible algorithms, how you could multiply them, is just completely immense.\nSo computationally, there is just no hope at all to do this exhaustively.\nSo already for 3x3, it's it's like open, which is crazy.\nAnd then, okay, so for 3x3, the best algorithm is using 23, that's at least better than the algorithm you get taught in high school, which would be 27.\nSo at least there is some progress on making it better.\nBut then for 4x4, which is like the next size, the best algorithm that had been known was just apply Strassen's algorithm recursively twice.\nSo because Strassen is for 2x2 matrices, what you can do if you have a 4x4 matrix, we you consider it as a block matrix of 2x2 blocks, and then each block itself is a 2x2 matrix.\nSo you can do Strassen twice, and because Strassen needs seven multiplications, if you do it twice, you will get 7 * 7, 49 multiplications.\nSo for the, like, yeah, so this was the only way known how to multiply 4x4 matrices in a fast way, like using 49 multiplications, just to stress and twice, and this has been the kind of the situation since 1969, and so this is where maybe our work comes in.\nSo two years ago, we built Alpha Tensor, which was a specialized reinforcement learning agent for discovering matrix multiplication algorithms, and that agent actually did find something faster, but only for boolean matrices.\nSo it's this very special case where you want to multiply matrices where every entry is just zero or one, and when you do the multiplication, you do everything modulo 2.\nSo for that case, Alpha Tensor found something faster.\nBut apart from that, for the usual case of matrices that have arbitrary numbers in them, still nothing better was known than doing Strassen twice using 49 multiplications.\nSo we're like really excited when we used AlphaEvolve in this setting.\nWe actually didn't even hope that it would find something better than 49 because we're trying for so long with Alpha Tensor.\nWe just ran it for completeness because we wanted to have this, you know, this table in the paper that we actually have that we tried on all the sizes up to up to five or six, and remarkably, it found a faster algorithm which uses 48 instead of 49 multiplications.\nAnd I mean, so that itself, like, yeah, when one of my teammates messaged on the channel that, oh, it seems like we have this result, I just couldn't believe it, like, let's triple check it.\nBut then it was indeed correct, and it actually has this one really appealing feature that, um, like, usually you think about multiplying matrices, you want to multiply matrices where the entries are real numbers or maybe integers.\nThe case of multiplying matrices where the numbers are complex is maybe a bit less common.\nBut real matrices are just a special case of complex matrices.\nSo if you find an algorithm that can multiply complex matrices, you can also apply to real matrices.\nIt's just a generalization.\nSo what's kind of cool here is that you, let's say you care about multiplying real matrices, which is what you care about when you train neural networks, let's say a very common use case.\nA priority, you might just look for an algorithm that uses real numbers.\nBut you can say that, oh, actually, what if we look for a complex algorithm, which a priority would think, well, that's a more difficult task because that algorithm would actually apply not only to real matrices but also to complex matrices, but by making the task more difficult, actually, AlphaEvolve was able to find an algorithm that uses complex numbers and therefore applies to both complex matrices and real matrices.\nSo that's that's kind of the result that we were most excited to actually get.\nBut then, as you were asking in your question, indeed, we were applying AlphaEvolve to other matrix sizes as well, and indeed, as you go to bigger and bigger cases, like 5x5, 6x6, the problem becomes much, much harder very, very quickly.\nIt's kind of an exponential with a quadratic in the exponent because the tensor, this cube that we sometimes show in the visualizations that you have to decompose, it grows quadratically with the size of the matrices that you multiply.\nSo for 4x4 matrices, you have to deal with a tensor of size 16 16.\nFor 5x5 matrices, it's 25 25.\nSo it's like exploding very quickly, and of course, as you go higher, at some point, your method will not scale there.\nBut what we show is that Alpha, well, it scales further than Alpha Tensor.\nSo there is some progress on the scaling direction.\nAnd then just one important point to clarify is that in all these cases, sure, we look at small cases of matrix multiplication, 2x2, 3x3, 4x4, but that doesn't mean that you can only apply these algorithms to matrices that are this small, like as I already alluded to with Strassen's algorithm, you can apply them recursively.\nSo if you have a big matrix, you treat it as a block matrix, and you apply these algorithms that are for smaller matrices recursively.\nYeah.\nI mean, and it's super fascinating, and it is such a fun domain to work in, and it's almost magical that even when you get to 3x3, it becomes, you know, intractable.\nAnd I'm always fascinated by cases like that in mathematics.\nIt's like, oh, we can do it for dimension one and two and three, but then when we get to four, it's just totally different, you know, it completely falls apart.\nBut as you went higher, you also had cases where AlphaEvolve, it couldn't match the current performance and actually found worse, like it sort of found worse algorithms.\nSo what do you attribute that to?\nWhy was it so?\nSo in the case that's the biggest one that we show in the paper, 6x6 matrices, there is a very clear reason for that, which is that we try to apply AlphaEvolve without giving it domain knowledge about the problem.\nWe just wanted to see how good is AlphaEvolve as a general-purpose tool, and we start it kind of from scratch, and we don't tell it about any, you know, like tricks for developing specifically matrix multiplication algorithms.\nThe best-known algorithm for 6x6 matrix multiplication, it uses a kind of very specific inductive bias, which is which means that it's looking for algorithms that have a specific symmetry in them, meaning that it only looks for algorithms that have a regularity in the algorithm.\nAnd if you only look for algorithms that have this regularity, that's a much smaller search space.\nSo in that search space, you are able to scale to much larger sizes.\nBut we just didn't try to do this to incorporate this symmetry into our search.\nWe just looked for algorithms of unrestricted form.\nAnd that I think is at least to me the clearest reason why we didn't match the best-known solution in that case.\nYeah.\nSo one of the things that fascinated me most about the paper, I mean, obviously, we're very interested in abstraction and representation, and even in the example that you just gave that Strassen was learned to be used as a sort of dynamic programming formulation, so being used recursively.\nAnd I'm thinking to myself, is that a demonstration of deep abstraction, or is it a kind of superficial one-step abstraction where the knowledge of Strassen was in its kind of local neighborhood, and it was composing that?\nAnd in an ideal world, what we want algorithms to do is compose abstract basis knowledge.\nSo knowledge which is as far down the stack as you can possibly do to sort of increase our flexibility.\nAnd we should just bring in how you modeled this representation.\nSo you had these different approaches.\nYou could directly model the solution.\nYou could model a constructor function.\nSo, you know, you're actually learning a function which itself constructs the solution.\nYou could be learning a search algorithm, which is what you did with the matrix multiplication, and you also spoke about co-evolution as a possibility.\nSo this is mind-blowing in the sense that there's some human design and intuition in how you designed the optimization target, but there's this ambiguity in the ways that you can represent so many of these problems and how they relate to each other.\nSo can you talk me through that?\nYeah.\nSo first of all, I want to be upfront about the fact that we ourselves don't have all the answers here.\nWe have this tool, AlphaEvolve, and we see that, okay, it's like, it's like, yeah, not even about AlphaEvolve, so we have this general tool that we see we can apply it here and here and here, and for every problem, there are different ways we can apply this tool, but a priority, we only have some intuition what is the right level of abstraction to apply this tool on.\nAnd as you were mentioning, Tim, yeah, sometimes the best thing you can do is you directly search for the solution, sometimes you search for a like a simple constructor which constructs the solution.\nAnd here, as a like illustrated example, let's say you're looking for a solution and you think it's going to be very regular.\nMaybe it's going to be like look like a fractal, like that's the typical example.\nThen you know that to describe a fractal, you can do it with a very short piece of code.\nSo in that case, it makes sense to look for a description of the fractal, not as a like a grid of pixels, but instead as a short piece of function that just generates that fractal.\nBut then in other applications, maybe the solution is very different, not so regular, and maybe you want to either look for the solution directly or you want to look for a complex search algorithm that finds the solution.\nAnd sometimes you want to have a sequence of algorithms that gradually refine the solution, which is the co-evolution approach, and a priority, it's not clear at all which one is going to work best when.\nSo that is something that is definitely in kind of the future work category to build up that understanding.\nBut one positive side of things is that AlphaEvolve is easy to set up in all the different formulations.\nSo often in practice, you just try different things and see what works best.\nWell, yeah, I I can't believe you guys don't have all the answers.\nLike I was, I don't know why I'm here now, but no, in all seriousness, and Tim mentioned it, these sort of three modes that that AlphaEvolve, you know, currently runs in, right?\nAnd it really reminded me of mathematicians because because, you know, mathematicians or there's at least three kind of main ways in which proofs are done, right?\nThey're either done by deduction, construction, or enumeration.\nAnd those almost seem to be like very parallel to these three methods.\nAnd I feel like there's probably some deep, deep connection there that I'm missing.\nI'm curious if either of you have any thoughts on that.\nYeah.\nAny thoughts on that deep, deep connection there or not?\nNot sure.\nYeah, it's an interesting observation.\nYeah.\nMate, did you want to say something?\nYeah.\nSo, so one thing is that when you think about constructions, that is the space where Alpha is like most obviously applicable.\nSo the examples that we show in the paper is where you have open problems and you make progress on those open problems by finding better constructions.\nSo that's where you're applicable out of the box.\nBut then if you think about other approaches of making progress on other types of mathematical problems, let's say the problem is not obviously about the construction.\nLet's say you want to prove impossibility results, like lower bounds in a sense.\nThen you have maybe two approaches on a high level that you can take.\nOne is that often problems that don't look like a construction, they become a construction if you frame them in the right way.\nSo for example, you have the like duality theorems for linear programming between the primal and the dual.\nSo you can often switch the side that you're actually trying to prove and turn something that isn't a constructive problem into a constructive one.\nSo that's kind of a alibistic answer, but that's something you can often do.\nThe more difficult and more general answer is that sometimes you're actually looking for a proof rather than a construction.\nNow, now proof is you can think of a proof also as an algorithm, like it's a step of like a sequence of steps that you need to execute to prove a statement.\nSo this is still within the space of algorithm discovery and something that we we can be thinking of doing.\nBut there is one technical hurdle that we have maybe some initial signs of overcoming, but it's not something that we have done in\n\n\nThis paper which is that if you're looking for an algorithm that is a proof, then the proof is at the end of the day either correct or not.\nSo the reward is binary, it's like zero or one.\nUm, for now, we have focused on problems where, um, you can make gradual progress, like can gradually improve the score, not just switch from zero to one in a single step, but gradually uh become better and better until you improve on the best on construction.\nNow, but of course, even as humans when we write proofs, we we face the same issue, like uh the the proof is only correct once it's actually done, but as humans as we are writing, working on the proof, we we'll have some intuitions of, okay, have we actually made some progress, like have we have we actually built some understanding about the problem, if so, then that seems likely that this will be a part of the eventual proof.\nSo we have been exploring the possibility of using, um, scores which are not hard scores, but soft scores.\nMaybe a a language model can itself uh provide feedback, like does it look like we have made progress towards solving the problem.\nSo we we do see the path of uh attacking these binary problems.\nLet's say uh searching for proofs, but it's not something that uh that we have already done.\nIt's just something we see as a possibility of uh doing in the future with this type of technique.\nYeah.\nI mean, and there's there's so many surprising things in this paper and I know Tim and I uh want to ask about more of them, but I'm kind of curious, you know, what each of you saw is, I mean, after you did this work and you had the paper or the research done at least, you know, which was most surprising to you?\nAnd I'll start with Alex.\nLike what uh what did you walk away from this work thinking, wow, I wasn't actually expecting that?\nI think it keeps amazing, amazing me uh how like how much progress you can make with this sort of systems, right?\nLike when when we started with FunSearch a few years ago, you would go to a chatbot interface somewhere, right?\nAnd like you would ask it to solve you an open problem, it will like not give you anything basically, right?\nAnd then still today, like you can try the same thing and like, okay, it will probably makes much more sense to like think for a few minutes, it will give you a lot of reasonable approaches.\nUh, but generally, it's not the experience of people that you like ask a chatbot to solve you an open problem and it will just do that, right?\nAnd then it's kind of amazing that like by using this the same tools, right?\nThe same LLMs in in kind of this iterative evolution loops, you can get so much more out of them.\nIt's yeah, I think like every time we solve something new, it's it's really kind of exciting and surprising in a sense.\nAnd and Matej, what what uh most surprised you?\nI think what is really new to me is that the generality of the approach and I don't just mean a generality across scientific open problems, but it's not my experience from my like admittedly short research career that you you build some tool for like scientific purposes and then out of the box you can do it apply to real world challenges and have so much impact.\nLike usually there is an entire body of work, research work that needs to happen to to translate a scientific technology into something that's actually useful in the real world.\nLike usually there is so many challenges you have to have to tackle and and here there is a tool which out of the box at the same time is able to make new discoveries on on mathematical and scientific problems and at the same time is able to discover algorithms that you can directly deploy into into Google's like critical compute stack.\nSo so that's something I certainly hadn't experienced before and maybe wasn't expecting to be honest.\nYeah.\nI don't know if you're familiar with the ARC challenge and a guy called Ryan Greenblat.\nHe did this famous approach where he just sampled a language model like 30,000 times to generate programs.\nWe're in this really interesting space as as you spoke about in the paper where we have an evaluator, which means we can sidestep hallucinations, right?\nSo isn't it fascinating that these nuggets of gold are in there in the search space and of course most of us just use language models in a single shot, right?\nJust do greedy sampling and and now we can just um do these very sophisticated search routines.\nBut I want to get to um, well actually maybe like a quick sub question is what what what you folks have done is you've mixed so many interesting paradigms together.\nYou know, we're talking about metalearning and um evolution and diversity preservation and and program learning and library learning and whatnot.\nBut like if you could explain simply how is that different from just sampling a language model, let's say you could sample a language model a billion times, has what you is AlphaEvolve like in a different category to that?\nYeah.\nYeah, for sure.\nSo so maybe just one uh like personal observation um on on what you just said that is indeed the right way of thinking about it.\nIf you just keep asking a language model in a in a like a chat window repeatedly, you'll get a completely the wrong idea about what is the capability if you actually scale things up.\nLike when we started working on the this type of evolutionary approaches, um initially we were like pretty skeptical what this is going to be able to do because indeed like we were just trying in in a in a chat window and asking it to to to write some simple algorithms and I it wasn't doing that well initially and and the magic really happens when you when you scale things up, but there are different ways in which you can scale things up.\nOne is that indeed you just keep asking repeatedly the same question and okay, sure, like there will be some nuggets, but there but they there'll be nuggets, there will not be the full solution.\nSo it is really important that you just find those good nuggets and then you iteratively build on top of them in subsequent iterations and and that's what you get through through through these evolutionary evolutionary algorithms.\nSo and just to speak very quantitatively, we do have a comparison in the ablations in the paper where we try to get rid of evolution and I mean, as you might expect, that that works much, much worse.\nYeah, I think maybe a good test case for kind of building additions about that is is the capset example from the original FunSearch paper.\nUh, so there we used a very simple and like small language model and like we didn't even provide context about the problem because we wouldn't ever expect the language model to actually, you know, think through the problem and like solve it by by thinking hard and like being very smart about it.\nWhat we're hoping to get from the language model is just like trying a few things, like a lot of things, right?\nBut the problem is that the final solution is like this.\nIt's not that long.\nIt's like maybe like 50 lines of Python.\nBut it's very, very unlikely that you'll stumble upon them by accident if you don't even think if you don't even know the problem, right?\nLike you're just trying Python things and like how would you generate the particular kind of 50 line Python program that works.\nSo there it was like completely essential to hill climb, like you have to like improve gradually and like see what works and trying to modify it a little bit in the air, like in in in the neighborhood.\nYeah.\nCan you can you actually talk a little bit more about that, the hill climbing, because you've incor you've you found clever ways in which to build in hill climbing into problems that at least on their face seem so discreet that hill climbing would would not be possible.\nSo I'm just curious if you could just expand a bit on the general idea behind how one takes a discrete problem like looking at different programs, you know, which are fragile and digital, um, and somehow incorporates hill climbing into that process.\nYeah, I guess for many problems, like you you do have a natural kind of hill climbing reward.\nFor example, um, when we do this gradient based method for finding uh matrix multiplication algorithms, like the natural reward will be the the loss, right?\nLike how close you are to the solution.\nUh, but uh, unfortunately that doesn't necessarily work very well and what we've like what we found again and again is that like you have to be somewhat creative about what kind of auxiliary rewards or like auxiliary signals you can come up with and uh in particular for the matrix modification case what was very useful for us is to realize that you know if you have a curriculum of of matrix sizes, then it's probably going to be somewhat easy to solve the small ones even with like you know simple gradient based methods and And you can heal clip on kind of this probability of soft, right?\nLike you let's say run your sh 10 times and then like how many times you actually get the thing you're looking for.\nUh, and then that can be like your seal and then like because you have a curriculum, you can like say, okay, first I want to solve 2x two, then once I'm like solid on that, I can I want to like uh continue evolving by trying to solve 3x3.\nSo that was really helpful in that particular case.\nBut yeah, in general, you just you just try things and you build intuitions about like how do you how do you approach coming up with this auxiliary words, it's very problem specific and and maybe just to add one like orthogonal point is that often you're trying to solve one particular problem and then maybe it comes with an associated reward or maybe it doesn't, but it's nevertheless useful to also optimize other things simultaneously.\nSo let's say you want to find a know a matrix multiplication of a particular size.\nYou also want to find algorithms for other sizes because if you do that, that allows you to explore the space of algorithms in a more broad way and often you can translate ideas that you develop for one matrix size later on to the matrix size that you actually care about.\nBut if you only optimized for the size you care about from the start with maybe that idea wouldn't have been useful initially.\nYou first had to develop it and refine it a bit further for a for a related problem.\nSo the overall point I'm trying to make is that it often intro makes sense to introduce kind of similar other tasks and try to improve on them even if you don't intrinsically care about them.\nCould we touch on the benefits of program synthesis in in general?\nSo uh we interviewed Kevin Ellis recently.\nHe's the guy who invented DreamCoder.\nFascinating guy.\nI used to work under Josh Tannon and um and the way he described it coming from a cognitive science point of view is um a lot of uh program induction is about explanation.\nIt's about intelligibility.\nIt's about legibility.\nAnd there was this incredible example when you were talking about doing theuling of of jobs on, you know, in Google's data center and you've got this kind of matching problem and whatnot.\nAnd I I think you said there was some previous experiments where you'd used like an inscrutable, you know, reinforcement learning model or something like that and it wasn't debugable.\nIt wasn't intelligible.\nAnd now you've got this beautiful three lines.\nBut even then there's the question of is it legible?\nHave you found things that are not legible?\nRemember there was that move um 37 in AlphaGo and that was a famous example of of a discovery which was a little bit weird and perhaps we wouldn't have discovered it, but maybe we understood it, maybe we could build theory around it.\nBut when when you look at some of these discoveries as well, are you finding deep abstract principles that you can derive from those discoveries?\nI mean, talk me through all of that.\nYeah.\nSo, you can span the broad spectrum actually.\nSo with AlphaEvolve, you sometimes discover algorithms that are really simple and they're so simple that you can like a human can verify that they're actually going to be correct on all inputs.\nAnd indeed, as you alluded to, they're so simple that you're just happy to submit them to production almost immediately, like no further checks checks needed.\nAnd it's in a completely different league compared to trying to deploy a neural network which you have to think about things like retraining it and hosting it and and and the resources of running inference and all those kinds of things.\nSo in indeed, you can you can be in this very simpler regime um for scientific discovery.\nCan also be in the regime where you explicitly look for programs that are interpretable.\nAnd you can do this by setting up the skeleton that you ask AlphaEvolve to fill in in such a way that by design you expect it to be fairly simple.\nAnd we actually maybe the clearest example is already in the FunSearch paper where um we looked for these big capsets, like a specific mathematical object and it found a function that we could look at, inspect, and we just notice that, oh, this function is is using the number four in this interesting way that whenever you look at the index in the for loop, you just look at the index modulo four and when you index into the the array, you're indexing at position i and i +4 and i plus 8, like, okay, what's going on?\nAnd and just by inspecting the code, we actually were able to develop, can think of it as like a mathematical insight or a mathematical hypothesis and that hypothesis turned out to be really crucial for then improving the results, like we we took that insight from the code, incorporated that insight into the next run and we got much better results.\nSo indeed, this can happen.\nUm, but in some applications, maybe you don't care as much about interpretability and then um AlphaEvolve can can develop even like very complex algorithms or a sequence of algorithms where you will perhaps not have the holistic understanding of how exactly this complex search heristic works, but what you care about is the final result that the final result is as good as possible.\nSo you can you can span this this whole spectrum and and maybe uh Alex would also mention the the work that is not from our group, but another team has applied FunSearch to uh cognitive science to exactly discover interpretable programs of behavior.\nSo that is actually pretty cool application that you can do.\nYeah.\nAnd also if you maybe go back to the matrix multiplication example, there we kind of build this grieving based machine learning pipeline, right?\nLike for for looking for those algorithms and there I think if you look at the proposed code changes by AlphaEvolve, you would see maybe two type of changes, two types of maybe reactions to the changes I I I experienced.\nOne is like, yeah, that makes sense, right?\nLike it's and it's it's usually this it's usually the case of machine learning, right?\nWhen you write machine when you read machine learning papers, you're like, oh yeah, this makes sense, right?\nLike that's a good idea, I would maybe have done it myself and then usually the problem is not coming up with the idea, it's like ideas are sort of cheap, it's just like the problem is coming up with the idea that actually works, like there's so many things that make sense and then like only like five of them actually work, so like it gives you ideas that you kind of understand and can relate to, but it's it's hard to know up pure like which of those will actually work, right?\nAnd then maybe other type of ideas uh uh or like code changes you would see is uh things you would probably not even have tried just because they're over complicated and sometimes it's for a good reason, sometimes it's for a bad reason.\nAnd for example, for the matrix multiplication case, you would see\n\n\nUh, the kind of, um, so, we have, uh, this, uh, quantization loss there because, uh, we want the solutions to be integers or like some kind of, maybe fractions, but specified range, such that we can exactly verify in exact arithmetic that they're correct. And then we'll have this quantization loss, which will drive the solution towards that set of integers, for example. And then, you know, normally, we as humans would only try, like, I, tuning the, the, uh, the weight of that quantization term, or maybe in the worst case, like the schedule of the weight, like, like maybe you will have less of it in the beginning and then, like, more of it in the end. But, uh, what AlphaEvolve did was producing like a whole kind of time evolving shape of the quantization loss, which, which kind of again makes sense, right? Like, you, you wouldn't say that it's not going to work, or you wouldn't, or otherwise, you wouldn't looking at it, you wouldn't say that, oh, this is amazing, this is like definitely going to work. But the thing is, like, you wouldn't even try it as a human because it's like so complicated that you would never even think about tuning such a, such a complicated, like, kind of function which changes shape over time with iterations.\nThat's pretty fascinating. Definitely.\nUm, it's kind of like how human players are able to glean some new insights from, uh, chess, you know, from from AlphaZero and and whatnot. Um, you did, uh, Alex, you also mentioned, you know, about how surprised you were at the general or the broad application of the technique. And I know, um, you have some prior work in robotics. Like, I'm just curious for, for cases where it's more difficult to assess the code, you know, so for robots, you might be able to do some training kind of in a virtual reality or something, but then at some point, you got to just, I don't know, throw it out in the woods and see if it's able to navigate to the other side or or something like that. How do you see bridging the gap between, uh, easily automated, uh, validation versus versus more complex real-world scenarios, yet still being able to apply AlphaEvolve?\nYeah. Um, I was thinking that maybe one direction that makes sense, and I saw people doing that, is, uh, looking into trying to convert kind of fuzzy reward functions into code. So, for example, maybe you have, uh, a reward function which is defined by the image of the final state you want to achieve, right? Or maybe you want to have a reward function which is defined by, like, a natural language description of what you want to do. And then, uh, your task is to convert that into a Python function which actually scores it. And maybe, maybe it's not that hard to have a binary word here for the RL to actually train the robot because, you know, you can ask the visual language model to verify it. But, uh, it's very hard to train against binary words. Like, I saw people, um, trying to use systems like AlphaEvolve to find a piece of Python code which would, uh, provide an auxiliary reward for kind of shape, like, shaped reward, basically, right? Like, to drive you towards that binary reward to to make the learning actually faster. I think that that makes a lot of sense as a direction. And, uh, yeah, I wonder, I don't know if that answers your question.\nYeah, I mean, it does. Um, but I'm also thinking just in terms of the efficiency of the procedure, you know, so, for example, you mentioned in the paper that many of the programs it generates, they just fail immediately, they, uh, you know, they crash, they're, they're not valid programs. Those are kind of easy to filter out, um, if you have an evaluator. Um, and then at the at the opposite extreme, even once you've got those programs, say, running the real-world test case, um, involves performing experiments and clinical trials or a robot trying to navigate a complex field that may end up in a getting damaged or something like that. So there's this huge gap between kind of automatic verification and expensive. And I'm just wondering how can we, you know, how can we bridge that in some sensible way for systems like AlphaEvolve? Um, just curious, like, if you have any thoughts on that. Yeah, we are thinking about those things. And and I think one thing that Mate alluded to, uh, before with, like, this proving, like, finding proofs, which is binary rewards, and then you, uh, can try to ask LLMs for a feedback to get, like, you again, like, some sort of shape rewards to to drive you towards the solution. I think that that that is a direction we're also thinking about because indeed, like, in many cases, you do have this kind of ladder of, uh, you know, more and more expensive evaluation. Like, for example, if, uh, if you want to do some sort of simulation based as, uh, maybe, maybe for robotics, maybe for some, like, biology or physics or whatever, right? You can do some simulation based reward, and then, like, you ultimately want to try, try it in real world. And maybe it's like going to be again trying on the robot, maybe you go to the lab, whatever. Like, uh, you do want, like, for every stage of this evolution ladder, which is more and more expensive, you don't want to have not just trying all the things that you found that work on the previous stage, but maybe have some sort of parization mechanism, which, right, can, can be, you know, can be a length feedback, can be something else, can be, like, scores. Yeah, those are the things we're thinking about. And, uh, maybe one project which also recently came out of Google, which kind of is very good at this, is co-scientists, and they, uh, basically attack the same problem, right? Like, how do you, how do you, uh, score ideas which are very hard to evaluate, maybe with hard, hard feedback? And and I think they're doing really good job at that.\nMakes sense. And maybe just one thing to add is that technologically, I think AlphaEvolve is already well set up for this. We had this idea of evaluation cascades where you evaluate a large number of programs very quickly, and then a smaller number of programs you can spend longer to evaluate. So you can just expand this cascade all the way to the regime where maybe you can only afford to evaluate 10 things, and you actually have to go into the real world and run some real-world experiments. But in principle, the the mechanism is there, and I mean, this is what you have to do even if you try to solve the problems manually or what actual researchers do, like, you have a finite budget. So first, you try to filter the ideas using cheaper methods, and then you only, only have the budget to to try the most promising ideas on the most expensive evaluation. And just if we can mimic that. Yeah. Exactly. Yeah. Same way those are done. Yeah. So I'm curious about LLMs. I mean, first of all, let me give you my heartfelt congratulations on Gemini Pro. It is ridiculous. It's so good. Honestly, it's, it's, it's just, I have no words to describe how good it is. But, um, we're in an interesting time, right? Because in in AlphaEvolve, you had an ensemble of, um, Flash 2.0, I think, and and Pro 2. And it just raises so many questions to me. What would happen if you did it with 2.5? Presumably, internally, you have even better models that that we don't know about yet. And how much, how much uplift is there? And and if you think about it, there's this Pareto curve of models, and Google's models are on the Pareto curve. So, you know, we're trading off cost and latency and and performance and and whatnot. And if you were just using one model, there might be some optimal place on that Pareto curve in AlphaEvolve. I mean, maybe, maybe we should have a small model, but we should, you know, sample it loads of times, or maybe we should have a big fat model, but not sample it as many times, or with an ensemble, maybe there's some perfect distribution of of of those models. Um, but we really seem to have, especially with the reasoning versions of the models, we seem to have unlocked something which maybe wasn't there until relatively recently. Could you talk to that? Yeah. So one thing I can say specifically that is particularly exciting to me about AlphaEvolve is that it does get this uplift from improving the the base language model. So this was not necessarily the the case in FunSearch, but in AlphaEvolve, we do see this. And we actually have a, like, a quantitative confirmation again in the ablations that if you also include the the the 2.0 Pro in in the ensemble, then you get better results compared to just using the Flash model. So it's, it's very clear that we are, like, leveraging the kind of the frontier capabilities of these models. And and of course, we cannot guarantee into the future what will happen, but at least for now, we are definitely, like, riding the wave. So, so indeed, we are, we are very keen in seeing how the base models improve and what kind of uplift the method is, uh, is going to get with this together. Um, the other kind of related point I would just quickly mention is that, um, AlphaEvolve also offers this opportunity, which we haven't taken it, but just forward looking, that it's a, it's a system that is able to enhance the capability of the base model. Like, the capability of the base model is somewhere, but then this system makes it even better through orchestrating this, like, test time compute pipeline and actually so much better that you can make, like, a new scientific discovery. So it raises the natural question, like, can we somehow distill this improved capability back into the base model? So that's something that you would get if you were to close the reinforcement learning loop. It's not something we have done with AlphaEvolve, but that possibility is is clearly on the table.\nSo maybe, um, you just mentioned improving the base model, and you also mentioned in the paper, you know, using AlphaEvolve to improve the infrastructure of, uh, of AlphaEvolve and and the base models itself. So, so you finally now managed to close the recursive self-improvement, you know, loop. Uh, I don't know. That's going to trigger some folks. Any any thoughts on that? What would Schmidt Huber say?\nAt this point, we want to be, like, very, you know, specific about what we have done. Like, we have, we have found a way to speed up the training of the next version of Gemini by 1%, and we have been able to unstrand resources in in the B data center. So currently, if you think about the the feedback loop, it's maybe on the order of months, right? Like, um, when we you speed up the training of the next version of Gemini, then that will take some time to to actually arrive. But indeed, like, we, we do see the steps in the in the direction that you that you described.\nAlex, any thoughts on the recursive self-improvement of AlphaEvolve?\nYeah, I mean, I don't think I have to have anything to add. Like, as Mate said, it's, uh, it's interesting to see how how it pans out. And, uh, we are seeing some signs of, uh, you know, helping the Gemini training ground. So, yeah, that's exciting. On in that in that thread, though, we want to have more autonomy. So, so right now, this is very much a kind of didactic exchange between the human supervisor, we select the problems, we design the evaluation functions, we seed the solutions, and so on. And I just wonder what would the next step of autonomy look like, you know, in terms of maybe we could actually have the thing imagine what its own evaluation function is, and maybe it could kind of go several steps further. What would that look like? To be honest, from my perspective, like, I guess automating some things is is cool and exciting, but also at the same time, I would even lean towards less automation in sense, like, right? Like, uh, I think the thing that makes, uh, AlphaEvolve so cool and powerful is is kind of this back and forth between humans and machines, right? And like, the humans ask questions, so the the system gives you some form of the answer, and then you, like, improve your intuition, you improve your question answering, question asking ability, right? And you ask more questions.\nSo we are thinking a lot about, uh, providing access to to AlphaEvolve to academics as trusted testers. Like, seeing what they can do with it. And while kind of trying to build that and trying to build the UI, we're thinking a lot about, you know, not just implementing the thing we have as a website, but just, like, what would be the next kind of level of human and I human AI interaction here? Like, what can the humans do in the, like, kind of intervene in the process? Like, maybe they want to, if the humans would want to supervise the pro, like, I don't know, comment on ideas and, like, inject more ideas and things like, like, we're exploring that a lot. And I think it's very exciting to see, like, what can be done in this kind of symbiosis, uh, space.\nSo final question, you said in the paper that, um, I think, I think you mentioned on the order of a 100 compute hours to evaluate a new solution, that was in section 2.3. But just before everyone at home rushes to reimplement AlphaEvolve, could you just give us some examples, maybe on the the matrix multiplication thing? I mean, how many compute hours, you know, did it run for? How, how much is it realistically costing to do this? Yeah, so, so one nice feature of AlphaEvolve is that it is really elastic, so it can match the difficulty of your problem. So if you ask it to solve a problem that's actually not that difficult, and maybe it's still an open problem, but no one has really worked on it, then maybe even if you ask a chatbot, it would almost solve it or solve it. In that case, Alpha will also give you the answer basically immediately, and it will not cost, uh, a lot at all. But if you ask for, you know, like, a very difficult problem, maybe like decades open, decades long open scientific problem, then you do expect that it's a difficult problem, you'll need to spend more time playing with different ideas and iterative ly building on top of that. And the nice feature of AlphaEvolve is that it is able to sustain the scaling, and as you keep running it for longer, find better and better ideas. I like, I know it kind of maybe sounds trivial, but I don't think it's actually easy to build these systems that are able to sustain this continual improvement without plateauing at at some point. And and in this case, with AlphaEvolve, you, you see this elasticity kind of stretching all the way to to making new scientific discoveries. So I think that that's a nice feature of the system. And so to answer your question, like, concretely, it depends on the difficulty of the problem. Uh, so if the problem is difficult, then you do expect that you will need to investigate more ideas and spend more compute. If it's easier, then you can have the answer very quickly. So if you think about the problems that we actually presented in the paper, then, um, even within matrix multiplication, some matrix sizes are much easier than others. So the the compute would be vastly different. And the same goes for the, uh, open problems in maths, like some of them are fairly easy, some of them are very difficult. So there is, there is no, no single answer that would, yeah, position you on this on this spectrum without knowing the the problem. And and unfortunately, as is the case with open problems, often you don't actually know a priority how difficult it is. So you can't even predict ahead of time. And you know, sometimes you try and you don't find anything better. That can also happen.\nAwesome. Well, um, guys, it's been such an honor having you both on MLSD. Thank you so much for joining us today. Thank you. Thank you so much for inviting us.\nYeah. Awesome.\n",
  "dumpedAt": "2025-07-21T18:43:25.012Z"
}