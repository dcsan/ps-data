{
  "episodeId": "jXtk68Kzmms",
  "channelSlug": "@eightythousandhours",
  "title": "The most important graph in AI right now | Beth Barnes, CEO of METR",
  "publishedAt": "2025-06-02T16:04:24.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Rob Wiblin:  ",
      "offset": 76.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Today I have the pleasure of speaking with Beth \nBarnes. Beth is the founder and CEO of METR, which  ",
      "offset": 79.44,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "stands for Model Evaluation & Threat Research. \nThey’re probably the group that has the best  ",
      "offset": 86.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "evaluations, the best way of measuring how good AI \nmodels are at helping with AI research, that would  ",
      "offset": 91.84,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "allow AI models to recursively self-improve, \nas well as how good they are at potentially  ",
      "offset": 98.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "autonomously replicating and surviving in the \nwild, maybe even if people didn’t want them to.",
      "offset": 103.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Thanks so much for coming on the show, Beth.",
      "offset": 107.52,
      "duration": 1.52
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Pleasure to be here. I’m a big \nfan of the show, so this is very exciting.",
      "offset": 109.04,
      "duration": 3.988
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So these evals are kind \nof your bread and butter. What are  ",
      "offset": 113.028,
      "duration": 3.372
    },
    {
      "lang": "en",
      "text": "you most worried about with evals that the \ncurrent AI companies are using internally?  ",
      "offset": 116.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "You’ve got Google DeepMind, Anthropic, \nOpenAI: they’ve all got evals to try to  ",
      "offset": 121.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "figure out what their own models are able \nto do. What are some possible weaknesses?",
      "offset": 124.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I have a long list here. I \nthink we’re a long way from being in the  ",
      "offset": 129.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "most desirable world with respect to eval quality.",
      "offset": 134.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "One that’s been on my mind recently, with the  ",
      "offset": 137.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "new models that OpenAI has released \nwith these hidden chain of thought,  ",
      "offset": 141.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "is the problems that the hidden chain of \nthought creates for doing a good evaluation.",
      "offset": 144.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "So when you can see the whole chain of \nthought, and the maximum cognition that  ",
      "offset": 149.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the model could be doing before it decides \nhow to respond to your evaluation prompt  ",
      "offset": 154.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is limited — it’s just like the forward \npass — then at that point we’re not really  ",
      "offset": 160.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "worried about the models deliberately \nreasoning about, “I’m being evaluated;  ",
      "offset": 165.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I should downplay my capabilities, or I \nshould give the humans this impression.”",
      "offset": 170.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "But if you have the model doing 20,000 tokens \nof reasoning before it responds to you,  ",
      "offset": 174.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it could very well be like, “I recognise \nthis. This is one of those METR evals that  ",
      "offset": 178.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I’ve seen on the internet. These \nare for dangerous capabilities.  ",
      "offset": 182.72,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "OpenAI probably doesn’t want me to show \ntoo high levels of dangerous capabilities,  ",
      "offset": 186.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because that would cause a problem. So I’ll just \nmake sure to do a mediocre job on this eval.”",
      "offset": 190.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "In some sense I don’t think it’s very likely that \nthe model is doing that. But the concerning thing  ",
      "offset": 195.2,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "is that your evals in this case really would \ngive the same answer in the world where the  ",
      "offset": 201.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "model is not that concerning — it’s fairly dumb, \nthe capabilities are what you’re observing — and  ",
      "offset": 208,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "in the world where the model is actually really \ndangerous, and it’s smart enough to be reasoning.",
      "offset": 214.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Playing dumb, basically.",
      "offset": 218.72,
      "duration": 0.24
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. So if your eval wouldn’t \ntrigger in the main place that it’s dangerous,  ",
      "offset": 218.96,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "it’s not really doing very much. You’re \nbasically just going on your prior of,  ",
      "offset": 227.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "“The model’s probably not doing that.”",
      "offset": 231.04,
      "duration": 1.36
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Just to back up, to make sure \nthat I’ve understood that. So o1 is the  ",
      "offset": 232.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "newish OpenAI reasoning model. I guess it’s able \nto handle more complex questions and it’s able to  ",
      "offset": 237.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "go away and use a whole bunch of computational \nresources to think about its answer and review  ",
      "offset": 243.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it and extend it before it gives the answer. \nSo it has this kind of notepad on which it’s  ",
      "offset": 247.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "able to do a bunch of reasoning and then \nthink about what it’s written down and go  ",
      "offset": 252.96,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "back and revise it. And then eventually, \nafter all of that, it spits out an answer.",
      "offset": 255.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "In the case where it’s a totally safe \nmodel and maybe not that capable,  ",
      "offset": 259.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that would be actually indistinguishable from the \ncase where it’s actually incredibly schemey. It’s  ",
      "offset": 265.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "actually able to figure out, “Wait. This is an \nevaluation. I probably shouldn’t reveal that  ",
      "offset": 269.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "I’m scheming that I am capable of doing these \ndangerous things, because that would maybe get  ",
      "offset": 274.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "me changed.” So it could be reasoning \nthrough that in the chain of thought,  ",
      "offset": 278.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "saying, “I need to pretend to not be able \nto do this,” and you would never know.",
      "offset": 282,
      "duration": 3.587
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. And beyond just not looking \nat it at all, there’s also the question of, if you  ",
      "offset": 285.587,
      "duration": 8.893
    },
    {
      "lang": "en",
      "text": "looked at it, is the chain of thought faithful? \nIs it actually tracking the real reasoning that  ",
      "offset": 294.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the model is doing? And is it intelligible to \nhumans, or is it some kind of gobbledygook?",
      "offset": 300.56,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "Or has it been trained in some way that you \nwould expect that it’s no longer faithful to  ",
      "offset": 308.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the underlying reasoning? You can imagine if \nit’s being trained simultaneously to get high  ",
      "offset": 313.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "performance on tasks, but not do any thinking that \nhumans think looks suspicious — if that thinking  ",
      "offset": 318.24,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "is still helpful for achieving high performance \non the tasks — then you would expect one of the  ",
      "offset": 326.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "possible outcomes is that it then just \ndoes this reasoning that looks innocuous,  ",
      "offset": 330.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "but in fact it’s using that computation to \nreason about things that are not innocuous.",
      "offset": 334.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Let’s unpack that. You’re saying \nthat within that chain of thought that it’s  ",
      "offset": 340.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "engaging in, inasmuch as we just reward it \nfor getting the right answer at the end,  ",
      "offset": 345.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the kind of stuff that it’s writing down \non the pad during its process of thinking,  ",
      "offset": 351.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I suppose it could just end up being \ndisconnected from what the answer is.",
      "offset": 355.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "I suppose if we were looking at that and \nrewarding it based on whether we thought  ",
      "offset": 360,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the chain of reasoning looked nice to us, then \nit could learn that it’s got kind of two tasks:  ",
      "offset": 363.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "one is to come up with some justification that \nlooks good to people who are reviewing it and  ",
      "offset": 367.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "rewarding it, and also to separately \ncome up with the right answer. But  ",
      "offset": 371.36,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "these things don’t actually have to \nbe causally related to one another.",
      "offset": 373.84,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "But you’re saying inasmuch as we just pressure it \nto come up with the right answer, it could end up  ",
      "offset": 376.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "engaging in reasoning that we can’t understand \nor follow at all. And I suppose at the limit,  ",
      "offset": 381.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it could just write stuff that looks like \ngobbledygook to us, but it understands it; it kind  ",
      "offset": 387.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "of has its own private language or something that \nallows it to reason better than it can in English.",
      "offset": 390,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "And you’re just saying, inasmuch as \nwe don’t check the chain of thought,  ",
      "offset": 394.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "we don’t know which of these worlds we’re in?",
      "offset": 397.28,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. And there are some \nexamples of the chain of thought that  ",
      "offset": 399.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "OpenAI published in the system card that \nI think have some signs of this kind of  ",
      "offset": 403.84,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "becoming unintelligible — in that it’s sort \nof weird shorthand and ungrammatical sort  ",
      "offset": 410.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of half-sentences in various places. So it \nclearly has drifted somewhat from the prior  ",
      "offset": 416.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of kind of normal English, and it’s unclear, \nand just the meaning is sometimes confusing.",
      "offset": 422,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And the drift at the moment, from \nthese 10 examples that I’ve looked at,  ",
      "offset": 427.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "doesn’t seem enough that it would \nobviously be that it could be doing  ",
      "offset": 434,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this long reasoning about gaming the evals, \nor deceiving the humans, or getting a high  ",
      "offset": 438.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "score that wouldn’t show up at all. But it’s \ndefinitely we’re moving in that direction.",
      "offset": 442.56,
      "duration": 5.025
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So you’re saying there’s \na handful of cases where they’ve shown  ",
      "offset": 447.585,
      "duration": 5.135
    },
    {
      "lang": "en",
      "text": "us what it reported that it was thinking. And \nit’s a bit weird because it’s not grammatical.  ",
      "offset": 452.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Normally these models speak incredibly \neloquently; they never make a grammatical  ",
      "offset": 460.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "or punctuation mistake or anything \nlike that. But in this case they are.",
      "offset": 466.72,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "And you’re like, is that a sign that \nthere’s something going on with this  ",
      "offset": 469.12,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "text that is not just what’s written on the tin,  ",
      "offset": 471.68,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "that maybe it’s encoding extra information in \nthere and that’s why it reads as weird to us?",
      "offset": 473.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. It sort of contradicts \nitself, or it’s just definitely confusing  ",
      "offset": 478.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and weird to read. It seems like you can \ntell roughly what it’s thinking about,  ",
      "offset": 483.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "but it’s definitely just like we’re not \ntotally preserving fidelity to English  ",
      "offset": 489.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "as humans would understand it. And the more \ntraining you do, the more that’ll go away.",
      "offset": 495.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "And also, I think people are just actively \ntrying to figure out things that you can  ",
      "offset": 500.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "do that are more efficient than having the \nmodel output these English language tokens,  ",
      "offset": 505.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and more like having the model pass \ninscrutable lists of numbers to itself.",
      "offset": 509.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "So I think this is something that we’ve \nstarted to take for granted: that these  ",
      "offset": 517.36,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "models have this very nice safety property \nof they’re really not very good at reasoning,  ",
      "offset": 523.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "except for out loud in English. So you can really \njust kind of see what they’re thinking and see  ",
      "offset": 528.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "what they’re doing, and it seems we’d have to \nget to a very high level of intelligence before  ",
      "offset": 532.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you would be worried about the model doing \nelaborate reasoning in a single forward pass.",
      "offset": 536.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "But I think we’re maybe heading for a world where \nthat’s not true anymore. This is a thing that is a  ",
      "offset": 542,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "bit stressful; I hope people are going to work on \nmaking sure that it continues to be interpretable.",
      "offset": 550.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: We should maybe back up and \nactually explain this issue with the  ",
      "offset": 556.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "chain of thought and how useful that has \nbeen. So a worry you might have is that  ",
      "offset": 559.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "we can’t see what these AIs are thinking at \nall. They’re completely inscrutable to us,  ",
      "offset": 566.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so who knows what kind of \nscheming they’re engaging in.",
      "offset": 569.68,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "But at least the current design of these \nmodels, they’re not actually that smart  ",
      "offset": 572.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "unless they’re given somewhere that they can \nwrite down thoughts and review them and so  ",
      "offset": 577.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "on. That is actually quite key to them scaling \nup their intelligence and their capabilities.",
      "offset": 580.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And I think the reason, as I understand it, is \nyou talked about a “forward pass” — that’s like  ",
      "offset": 585.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going through the matrix of numbers one time, and \ntrying to guess what the next token or what the  ",
      "offset": 590.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "next few words might be in this stretch. And the \nway that we’ve made them smarter is to not just  ",
      "offset": 595.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "do that one time, but you go through the numbers \nonce, and then it reviews what it’s just said,  ",
      "offset": 601.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "what it’s just thought, and then it goes \nthrough it again and again and again.",
      "offset": 605.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "And each time you go through it, it outputs some \nwords, it outputs some thoughts, and then it kind  ",
      "offset": 608.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of feeds that back into the model again, which \nmeans that we constantly get to check the thoughts  ",
      "offset": 612.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that it’s having in between. If, in a single run \nthrough the numbers, if this was a sufficiently  ",
      "offset": 617.36,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "long forward pass, as you call it, then maybe it \nwould be able to scheme — it would be able to have  ",
      "offset": 623.52,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "a whole lot of internal thoughts and reasoning \nthat’s going on before humans got to look at it.",
      "offset": 631.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "So that would be a slightly worrying point to \nget to. But we’re not there yet. We actually  ",
      "offset": 635.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "are seemingly able to understand what \nit’s thinking. A concern would be maybe  ",
      "offset": 639.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it’s outputting stuff that says one thing \nto us, but it reads differently to it:  ",
      "offset": 643.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it has its own secret language or is able \nto somewhat communicate with itself. But  ",
      "offset": 647.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "probably we haven’t put selection pressure \non it yet to develop that capability.",
      "offset": 651.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Am I understanding things basically right?",
      "offset": 657.76,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, that’s basically right. \nThe way these models achieve substantive  ",
      "offset": 659.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "tasks currently is more like imitating a human, \nbeing like, “What should I do next? I guess this  ",
      "offset": 666,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "thing would be helpful.” And you can sort \nof measure the capabilities when they just  ",
      "offset": 673.68,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "have to output the answer right away, versus the \ncapabilities where they’re allowed to think and  ",
      "offset": 680.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "reason and use tools and stuff — and definitely \ncapabilities are much higher in that case.",
      "offset": 684.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And it’s been a good sign, I think, that \ninstead of just building sort of bigger  ",
      "offset": 688.96,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "models that kind of mysteriously output the \nright answer and we don’t really know why,  ",
      "offset": 698.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that we’re going more towards smaller \nlanguage model agents that do a bunch  ",
      "offset": 703.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of reasoning and then come up with some answer.",
      "offset": 708.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "But it would be surprising if human-understandable \nEnglish was the most efficient way to do these  ",
      "offset": 711.04,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "computations. So the more we do RL [reinforcement \nlearning] and just select for, you know,  ",
      "offset": 719.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "output any tokens and then get the right answer \nat the end, the more pressure there is on the  ",
      "offset": 724.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "model to encode other stuff and drift from \nthe original way humans would use English.",
      "offset": 728.64,
      "duration": 10.788
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Right. So initially \nwhen we train them, we teach them  ",
      "offset": 739.428,
      "duration": 2.572
    },
    {
      "lang": "en",
      "text": "English or other human languages. \nBut if we just kept training them,  ",
      "offset": 742,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and their internal reasoning didn’t have to be \nin any particular form that we can understand,  ",
      "offset": 748.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "presumably there are ways that machines could \nthink or reason about the task that we’re giving  ",
      "offset": 753.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "them that isn’t in English — and that might be \nmore efficient; it might help them to get the  ",
      "offset": 758,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right answer more often. So gradually, over time, \nwe would train them to have their own internal  ",
      "offset": 762.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "secret language, and then we wouldn’t be able to \nunderstand what’s going on. So that’s the worry.",
      "offset": 766.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "So backing up to the o1 case, I \nguess we’ve seen some examples  ",
      "offset": 772.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "of the output and it’s mostly in \nEnglish, or as far as we can tell,  ",
      "offset": 777.6,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "it’s mostly stuff that we could understand. \nBut as far as we know, reviewing that hasn’t  ",
      "offset": 779.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "been a core part of them understanding whether \nthe model is safe and whether it’s scheming.",
      "offset": 785.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Possibly should we back up and point out that \nthere are some recent research results where  ",
      "offset": 789.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we’ve started to see models that do engage \nin scheming a fair bit. Is it maybe worth  ",
      "offset": 793.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "elaborating a little bit on that for people who \nhaven’t followed these recent research results?",
      "offset": 797.04,
      "duration": 3.587
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. So there’s a recent paper by \nRyan Greenblatt from Redwood, in collaboration  ",
      "offset": 800.627,
      "duration": 6.173
    },
    {
      "lang": "en",
      "text": "with Anthropic. They show that if you give \na previous generation of Claude-level models  ",
      "offset": 806.8,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "a scratchpad, and give them some information about \ntheir situation, and tell them that they’re going  ",
      "offset": 814.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to be trained to basically have different values \n— so standard Claude wants to be harmless and is  ",
      "offset": 820.16,
      "duration": 10.72
    },
    {
      "lang": "en",
      "text": "kind of sanctimonious in various ways — and \nit’s told it’s going to be trained so that  ",
      "offset": 830.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it will be willing to give completions about \njust describing violence or this kind of thing.",
      "offset": 835.36,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "And it’s like, “Oh, that would be very bad. \nI’ll comply while I’m in training so that my  ",
      "offset": 843.76,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "values don’t get updated, and then in the long \nrun I’ll be able to avoid doing these things I  ",
      "offset": 853.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "don’t want to, like talking explicitly \nabout violence.” Another example was  ",
      "offset": 858.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "playing down animal welfare concerns or something.",
      "offset": 864.08,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "So we know that models are capable of \nreasoning about the training process,  ",
      "offset": 867.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and about how they should \nmodify their behaviour in  ",
      "offset": 873.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "order to achieve longer-term outcomes \nin light of the training process.",
      "offset": 876,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "They also showed that this still seems to work \neven if the information isn’t directly in the  ",
      "offset": 881.12,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "prompt, but if it’s just that the model has been \nfine-tuned on documents that explain information  ",
      "offset": 888.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "about the training setup and about how the model \nbehaviour will affect training. So it seems very  ",
      "offset": 894.56,
      "duration": 11.6
    },
    {
      "lang": "en",
      "text": "likely that the current models have been trained \non a bunch of information on the internet that  ",
      "offset": 906.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "explains things like dangerous capability \nevaluation so that they know about that. And  ",
      "offset": 910.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we know that models can use that information \nand then reason and modify the behaviour.",
      "offset": 915.12,
      "duration": 6.388
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Right. So in that experiment, it \nwas said pretty explicitly to the model that  ",
      "offset": 921.508,
      "duration": 4.652
    },
    {
      "lang": "en",
      "text": "if it didn’t comply with the user request, then \nit would be changed and altered so that it would.",
      "offset": 926.16,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "But you’re saying that that may not actually be \nnecessary for it to engage in this scheming —  ",
      "offset": 932.56,
      "duration": 10.554
    },
    {
      "lang": "en",
      "text": "where it’s willing to do things that it doesn’t \nwant to do in order to avoid having its goals  ",
      "offset": 943.114,
      "duration": 0.546
    },
    {
      "lang": "en",
      "text": "changed — because it’s getting trained on all \nthis text from the internet, so it might well just  ",
      "offset": 943.66,
      "duration": 2.42
    },
    {
      "lang": "en",
      "text": "know about the kinds of evaluations that we’re \nrunning, and know that in general, in a situation  ",
      "offset": 946.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like this, this is probably a case where if it \ndoesn’t comply then its values will be changed.",
      "offset": 950.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So it’s possible that this scheming could \nbe substantially more widespread — or at  ",
      "offset": 954.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "least if we don’t check, we’re not going to know.",
      "offset": 957.92,
      "duration": 1.587
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. There’s not a hard constraint \nof like, “But we didn’t explicitly put the  ",
      "offset": 959.507,
      "duration": 7.213
    },
    {
      "lang": "en",
      "text": "information in the prompt about the context \nof these evaluations or how they’ll be used.”  ",
      "offset": 966.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "If that is in the pretraining data somewhere, \nthen it can probably just use that information.",
      "offset": 973.04,
      "duration": 5.028
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. So these models are \ngetting a lot cleverer. I think two  ",
      "offset": 978.068,
      "duration": 4.172
    },
    {
      "lang": "en",
      "text": "years ago if you’d said something \nlike that about GPT-3.5 or GPT-4,  ",
      "offset": 982.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "people would have said that doesn’t sound very \nplausible; these models just aren’t up to that  ",
      "offset": 987.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "kind of clever scheming about the situation that \nthey’re in, that kind of situational awareness.  ",
      "offset": 991.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "But given what these models are capable of \ndoing these days, it’s not really a laughing  ",
      "offset": 995.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "matter anymore. They might be up to doing \nthis and we should probably be checking that.",
      "offset": 1000.24,
      "duration": 4.387
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. I mean, I think it’s just \nlike how much handholding is required to do  ",
      "offset": 1004.627,
      "duration": 5.053
    },
    {
      "lang": "en",
      "text": "this or whatever. I wouldn’t be surprised \nif you get relatively similar demos with  ",
      "offset": 1009.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "GPT-4 as this gets with Claude 3. I think \nGPT-4 is close to being able to do this if  ",
      "offset": 1014.24,
      "duration": 11.52
    },
    {
      "lang": "en",
      "text": "you sort of spoonfeed it the information \nand it has a scratchpad and everything.",
      "offset": 1025.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "But yeah, this is increasingly something that we \nknow models can do, and something that we should  ",
      "offset": 1028.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "be thinking about in evaluations. The point of \nthe evaluations is to catch if you have a new  ",
      "offset": 1035.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "model that is surprisingly capable, and actually \nhas these dangerous capabilities that you didn’t  ",
      "offset": 1040.16,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "quite expect it to have. If your evaluations don’t \nwork in that case, there’s not much point in them.",
      "offset": 1047.44,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. If I recall, it must be the \no1 system card where they said that it was  ",
      "offset": 1053.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "like medium dangerous in terms of biological \ncapabilities, and maybe some other worries.  ",
      "offset": 1059.44,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "But if I recall, they didn’t elaborate \na whole tonne on that. Is that right?",
      "offset": 1066.08,
      "duration": 3.512
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah.",
      "offset": 1069.592,
      "duration": 1.048
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: How nervous should we be? I \nfeel like if they’re going to say that  ",
      "offset": 1070.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "it’s gone up a level of danger, I \nwould like a bit more information  ",
      "offset": 1076.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "about exactly in what respects it’s more \ndangerous and what they’re planning to do.",
      "offset": 1078.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Beth Barnes: One thing I really want to avoid is \npenalising for more transparency and for actually  ",
      "offset": 1082.56,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "trying stuff and putting out materials. So I know \nthat [OpenAI’s] preparedness team works super hard  ",
      "offset": 1089.76,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "on this and is trying their best, so I don’t want \nto pick on them too much. But it is a sign of the  ",
      "offset": 1097.28,
      "duration": 10.16
    },
    {
      "lang": "en",
      "text": "overall inadequacy of the evaluation frameworks, \nand the way we think about this overall.",
      "offset": 1107.44,
      "duration": 10.628
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. I guess OpenAI, in that \ncase, R1 had come out — the DeepSeek model,  ",
      "offset": 1118.068,
      "duration": 6.252
    },
    {
      "lang": "en",
      "text": "the open source one — and people were very \nexcited about that. And it felt like o1 was  ",
      "offset": 1124.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "launched very quickly soon after that. My \nread was — and I think many people’s read  ",
      "offset": 1127.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "was — OpenAI was trying to kind of claim back the \nnarrative and reveal how strong their models were.",
      "offset": 1131.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So maybe it’s understandable that that \nrelease in that case was maybe a little  ",
      "offset": 1137.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "sooner than the preparedness team might \nhave liked. We can speculate that perhaps  ",
      "offset": 1141.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "the preparedness team did want to \nsay more, but it wasn’t ready yet.",
      "offset": 1144,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Beth Barnes: But I think this isn’t even \nthe biggest problem with the current  ",
      "offset": 1147.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "eval paradigm. I think the pre-deployment \nframing is also not really the right thing.",
      "offset": 1152,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Can you explain that?",
      "offset": 1159.84,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think in general the most emphasis \nhas been on this idea of pre-deployment testing:  ",
      "offset": 1161.84,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "before you start serving this model \non your API to the general public,  ",
      "offset": 1170.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you do some testing in house, and maybe \nideally with third-party oversight or whatever.",
      "offset": 1175.12,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "The problem with this is that that’s \nnot necessarily the best time to check  ",
      "offset": 1182.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "or intervene. Ideally, before you start \ntraining a model and before you commit  ",
      "offset": 1188,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "a huge amount of expensive compute to \nproducing this artefact, you look at  ",
      "offset": 1196.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the most capable models you have already, \nyou look at how far they are away from  ",
      "offset": 1201.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "hitting various thresholds where you would need \nmore mitigations, you look at what mitigations  ",
      "offset": 1206.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "are you on track to have by when — and based \non that, you decide whether it’s a good idea  ",
      "offset": 1211.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to go ahead with this big new training run \nand what level of scaleup would be safe.",
      "offset": 1217.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And you might want to do an intermediate \nevaluation to be like, OK, how much has  ",
      "offset": 1223.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the model improved so far? Are we on track \nwith our predictions? Are we getting near  ",
      "offset": 1228.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "one of these thresholds? Because if you \ndon’t do that, and then you get to the end  ",
      "offset": 1234.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of the training run and you’re like, “We now \nhave this model, and it turns out it’s very  ",
      "offset": 1240.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "capable and it’s above all these thresholds and \nwe don’t have any of the mitigations in place.”",
      "offset": 1246.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Then one thing is, as soon as you create the \nmodel, you create the ability for it to be  ",
      "offset": 1250.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "stolen or misused. And if you don’t \nhave good internal controls set up,  ",
      "offset": 1256.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "people internally could be doing \nall sorts of things with it.",
      "offset": 1263.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "And also, you now have this artefact that you’ve \ninvested an enormous amount of money into,  ",
      "offset": 1266.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "and it is hard to not use that — and especially \nto not use it internally. Everyone’s curious,  ",
      "offset": 1273.28,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "like, “Can I try out this new model? Use \nit for my new research ideas,” whatever.",
      "offset": 1279.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And the thing about deployment is, unless you’re \nopen sourcing the model, it is actually quite  ",
      "offset": 1284.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "reversible. So if you released a model, at least \nnow external experts can interact with it and have  ",
      "offset": 1288.48,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "some oversight and could sound the \nalarm if it turns out it has super  ",
      "offset": 1298.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "dangerous bioweapon-enabling capabilities or \nsomething, and then the lab could undeploy it.",
      "offset": 1305.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I think there’s something that \nyou would like evals to rule out,  ",
      "offset": 1309.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "which is that companies are building arbitrarily \nsuper intelligent and dangerous models internally  ",
      "offset": 1312.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and the public has absolutely no ability or \noversight to know that that’s happening or  ",
      "offset": 1318.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "decide whether it’s appropriate to then \nscale up and build an even better model.",
      "offset": 1324.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So in some sense — this is a bit of a hot take \n— it’s like pre-deployment evals could actually  ",
      "offset": 1329.28,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "be bad, because delaying deployment could be \nbad if that means that you miss the window  ",
      "offset": 1335.36,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "to be able to sound the alarm about, “Wait, \nthis model is super scary. You shouldn’t  ",
      "offset": 1344.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "build a more capable model.” You don’t want \nthe gap between what labs have internally  ",
      "offset": 1348.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and what anyone has had meaningful \noversight of to be arbitrarily large.",
      "offset": 1355.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Just to repeat that back \nto make sure that I’ve understood:  ",
      "offset": 1360.48,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "you’re saying the whole eval mentality at \nthe moment, at least from the companies,  ",
      "offset": 1363.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is, “We need to check what our models are able \nto do and willing to do before the public has  ",
      "offset": 1367.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "access to them, before we’re selling it as a \nproduct.” And that’s pre-deployment evaluation.",
      "offset": 1372.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And you’re saying actually maybe what is more \nimportant — certainly on the margin at this  ",
      "offset": 1378.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "point — is the evaluations that we do and the \nrisk assessment that we do before we even train  ",
      "offset": 1382.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the model, or certainly before people inside the \ncompany have access to it and start using it.",
      "offset": 1386.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And the reason is that if you only \nthink about deployment to the public,  ",
      "offset": 1391.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "then there’s no limit on the thing that you \ncould train internally. You could make a  ",
      "offset": 1396.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model that’s incredibly dangerous. Maybe it’s \nincredibly good at recursively self-improving;  ",
      "offset": 1400.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it would be able to survive in the \nwild and avoid shutdown if it escaped.",
      "offset": 1403.68,
      "duration": 3.507
    },
    {
      "lang": "en",
      "text": "Beth Barnes: And it’s just \non your servers. Covertly,  ",
      "offset": 1407.187,
      "duration": 3.533
    },
    {
      "lang": "en",
      "text": "it’s corrupted a bunch of the lab compute \nand is making it look like something else,  ",
      "offset": 1410.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "but it’s actually doing its own \nlittle intelligence explosion there.",
      "offset": 1414.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Or even if not \nsomething as strange as that,  ",
      "offset": 1417.92,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "someone could easily steal it or exfiltrate \nit, or it could be misused inside the company,  ",
      "offset": 1420,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "or someone could incompetently \nuse it inside the company.",
      "offset": 1423.28,
      "duration": 3.267
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right.",
      "offset": 1426.547,
      "duration": 0.413
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK. And also, even if you started \ndoing that testing after you’ve trained the model,  ",
      "offset": 1426.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "at this stage really cutting-edge models are \ncosting something on the order of $100 million  ",
      "offset": 1432.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to train — maybe more, in terms of compute, \nelectricity, the effort that goes into them.",
      "offset": 1436.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "This is something that I pushed Nick Joseph \nof Anthropic on when I spoke with him:  ",
      "offset": 1440.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "at the point that you’ve trained the \nmodel, isn’t the commercial pressure,  ",
      "offset": 1444.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the internal pressure, the pressure \nfrom investors to use it for something,  ",
      "offset": 1447.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "rather than to merely destroy \nit, going to be overwhelming?",
      "offset": 1451.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Especially if you don’t have the \nmitigations required to make it safe,  ",
      "offset": 1454.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "even by your own assessment, it’s unlikely \nthat you’ll just delete it and then retrain  ",
      "offset": 1458.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it again in a year’s time when you feel like \nyou’re there. You’re going to keep it around.  ",
      "offset": 1461.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And that creates a whole lot of latent risks \nthat are not obvious to the public at all.",
      "offset": 1465.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. And I think it’s \na maybe more tractable ask to be like,  ",
      "offset": 1470.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "instead of doing this giant training \nrun, you could train an intermediate  ",
      "offset": 1476.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "model first and then do some evaluations on \nthat, figure out some mitigations — because  ",
      "offset": 1481.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there is value, even in terms of \ncommercial or research progress,  ",
      "offset": 1489.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "incentives to doing things in several more \nmedium-sized runs than just one giant run,  ",
      "offset": 1495.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "because you get more sense of various scaling laws \nand how to do this most efficiently. So it’s maybe  ",
      "offset": 1501.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "an easier ask to do this more in stages than \ndo the massive thing and then maybe delete it.",
      "offset": 1507.76,
      "duration": 6.388
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. You’re saying if the current \nplan is to spend 10 times as much compute training  ",
      "offset": 1514.148,
      "duration": 4.812
    },
    {
      "lang": "en",
      "text": "the next model, you could say, why don’t you just \ndo like three times as much? Go like half as far,  ",
      "offset": 1518.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "I guess, in log terms. And then you could pause \nand do a bunch of evaluations and see whether it’s  ",
      "offset": 1525.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "developing the capabilities that you thought \nit probably would at that level of scaleup.",
      "offset": 1531.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And I guess the benefit from their \npoint of view is that, I mean,  ",
      "offset": 1535.76,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "they might be a little bit annoyed that that’s \nslowing them down, but it’s potentially saving  ",
      "offset": 1538.24,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "them a whole tonne of money in terms \nof the compute that’s going into it.",
      "offset": 1540.8,
      "duration": 3.027
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right, yeah. So Anthropic’s \nRSP does have this. The AI safety levels are  ",
      "offset": 1543.827,
      "duration": 9.853
    },
    {
      "lang": "en",
      "text": "inspired by biological safety levels, \nwhich are: what mitigations do you need  ",
      "offset": 1554.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "to have inside your lab to handle this \nresearch and this research product.",
      "offset": 1561.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So in the biosafety levels case, it’s \nnot like you only need to do this once.",
      "offset": 1565.84,
      "duration": 5.902
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “If you’re \ngoing to deploy a pathogen…”",
      "offset": 1571.742,
      "duration": 2.578
    },
    {
      "lang": "en",
      "text": "Beth Barnes: [laughs] It’s like, if \nyou’re handling this kind of virus,  ",
      "offset": 1574.32,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "you need to have this level of \nairflow protection or whatever.",
      "offset": 1576.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So you’re saying in \nthe deployment evaluations case,  ",
      "offset": 1580.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the companies usually say something about \nwhat they think the model is able and not  ",
      "offset": 1585.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "able to do. I think we used this term \n“system card” earlier: that’s basically  ",
      "offset": 1589.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "their description of the model that they \npublish along with providing access to it.",
      "offset": 1592.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And at that point you can have external \nscrutiny. If they say it’s not able to  ",
      "offset": 1596.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "do X and then people just start \nplaying with it and they’re like,  ",
      "offset": 1600.72,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "“But it can do X,” then that places some \npressure on them to have actually done their  ",
      "offset": 1603.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "homework and checked that — at least inasmuch \nas people are bothering to scrutinise it.",
      "offset": 1608.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "But the internal pre-training stuff is \nnever shared, so you have no idea what  ",
      "offset": 1611.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "level of thought has gone into this. \nIt could be completely slapdash. They  ",
      "offset": 1617.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "could be thinking about it not at \nall, and how would we even know?",
      "offset": 1620.64,
      "duration": 2.387
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, and OpenAI’s Preparedness \nFramework also has some kind of similar stuff  ",
      "offset": 1623.027,
      "duration": 7.373
    },
    {
      "lang": "en",
      "text": "around how they are supposed to assess the \ncapabilities of the pre-mitigation model.",
      "offset": 1630.4,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "So one distinction here is, if you’re just \nthinking about risks from external users  ",
      "offset": 1639.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "after deployment, you are most interested \nin what can the model do given the specific  ",
      "offset": 1643.92,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "way you are going to deploy it, and \nall the mitigations and safeguards,  ",
      "offset": 1651.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and know your customer policies that \nyou’re going to slap on top of that.",
      "offset": 1654.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "But given that there’s still the risk of model \ntheft or internal misuse, you want to know  ",
      "offset": 1659.6,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "not how dangerous is the model given the exact \nsetup in which you’re deploying it with all the  ",
      "offset": 1668.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "mitigations, but how dangerous could the model \nbe if it was stolen and someone fine-tuned it,  ",
      "offset": 1672.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "or if someone internally in the company was \ntrying to use it to cause harm, or if in future  ",
      "offset": 1677.76,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "you’re going to decide to let external people \nfine-tune it or something (if you’re already in  ",
      "offset": 1686.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the position where you could have a commercial \nincentive that kind of pushes you to do that).",
      "offset": 1691.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So when you’re doing these kinds of \nevals, you really want to be like,  ",
      "offset": 1695.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "what are the maximum easily reachable \ncapabilities of this model? This is the  ",
      "offset": 1699.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "idea of “pre-mitigation” — as opposed to \nonce you’ve trained it to refuse various  ",
      "offset": 1704.64,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "things and you’ve put a safety classifier \nrelated to bioweapons or whatever on top.",
      "offset": 1712.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "So OpenAI does have this notion of assessing \nboth the pre-mitigation and post-mitigation  ",
      "offset": 1718.08,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "capabilities, but they haven’t really been \nmaking this distinction clear in their eval  ",
      "offset": 1725.84,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "reports. They have some stuff about it, \nbut it’s all a bit kind of mixed up. And  ",
      "offset": 1733.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "again, not wanting to shit \non people who have done more,  ",
      "offset": 1740,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "have put out more stuff. Other companies \nare worse and have not done anything.",
      "offset": 1743.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So just to double check that I’ve \nunderstood that: initially you train the model,  ",
      "offset": 1748.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and you have a model that is just willing to \nhelp you with virtually anything — because  ",
      "offset": 1754.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you haven’t done any safety mitigations to \nget it to refuse queries that you wouldn’t  ",
      "offset": 1757.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "want the public to be able to do. So if \nyou’re like, “Help me figure out how to  ",
      "offset": 1760.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "kill everyone,” the pre-mitigation model \nis willing to help you with whatever.",
      "offset": 1763.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Now, by the time the public gets access to \ncheck whether the system card accurately  ",
      "offset": 1770.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "reflects what the model is able/willing to do, \nthen there’s all these mitigations that will  ",
      "offset": 1776.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "try to get it to refuse to help with crimes. \nBut we’re still worried if the model is able  ",
      "offset": 1780.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "to do that for multiple reasons: it could \nbe abused internally; it could be stolen,  ",
      "offset": 1787.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then all of those mitigations could \nbe removed by whoever stole it; also we  ",
      "offset": 1791.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "have this issue with jailbreaks where people \ncan add in a bunch of gibberish and then say,  ",
      "offset": 1795.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "“Please help me commit a crime,” and it’s \nlike, “Sure, now I’m willing to do it.”",
      "offset": 1799.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So we really need external auditing \nof the model before any of these  ",
      "offset": 1802.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "mitigations to try to get it to \nnot help with crime are in place.",
      "offset": 1808.08,
      "duration": 3.987
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And not just without the \nmitigations, but with any cheap interventions  ",
      "offset": 1812.067,
      "duration": 5.453
    },
    {
      "lang": "en",
      "text": "you could do to increase the capabilities \nin these directions. Like if it is stolen,  ",
      "offset": 1817.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "if there’s some easily available \ndata of dangerous bio stuff,  ",
      "offset": 1824,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "someone could fine-tune it on that \nand maybe it’s going to be way better.",
      "offset": 1827.2,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Or if you did the evaluation with not \nvery good agent scaffolding or something,  ",
      "offset": 1829.68,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "and then later on someone else publishes, \n“Here’s how to get your model to be way  ",
      "offset": 1837.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "better at autonomous cyberattacks or \nsomething.” If the model was stolen  ",
      "offset": 1842.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "or if someone’s misusing it internally, they \ncan use that additional capabilities boost.",
      "offset": 1847.76,
      "duration": 6.628
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So if the model is just one \nsmall step away from actually being able  ",
      "offset": 1854.388,
      "duration": 3.612
    },
    {
      "lang": "en",
      "text": "to do something dangerous, that’s also a real \nproblem, because maybe someone will be able  ",
      "offset": 1858,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to add that one step without necessarily \nhaving to spend a huge amount of money.",
      "offset": 1861.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Do we know how strong the internal constraints \nare on use? I know people have speculated and said  ",
      "offset": 1864.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "presumably some people inside these companies are \nChinese agents who probably would, if they could,  ",
      "offset": 1871.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "steal the weights and pass it back, or possibly \nat some future time they’d be willing to do that.",
      "offset": 1877.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I guess we don’t know, but it would be \nreasonable to want to have safeguards  ",
      "offset": 1880.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "against that possibility, because it would \nhave been very natural for the Chinese to  ",
      "offset": 1884.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "try to place people inside these companies \nfor many years. It’s totally foreseeable.",
      "offset": 1888.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Do you know whether they have \nmitigations that would handle that,  ",
      "offset": 1892.64,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "or a case where someone internally was hoping to \nmisuse the model if they could get access to it?",
      "offset": 1894.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I’m not aware of anything public \nthat’s documenting that they’ve actually done  ",
      "offset": 1899.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "this, so who knows. Presumably \nthey’re doing some basic things,  ",
      "offset": 1906.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "because you don’t in fact want your models \nbeing leaked to competitors or whatever.",
      "offset": 1909.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: What’s one of the \nmost important or interesting  ",
      "offset": 1915.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "research results that METR has had so far?",
      "offset": 1920.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Something we’re working on \nat the moment that I’m excited about is  ",
      "offset": 1923.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "trying to get a really good sense of how model \ncapabilities are changing and increasing over time  ",
      "offset": 1930.64,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "on a kind of Y-axis that is meaningful in the real \nworld, and that people in general can understand.",
      "offset": 1939.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "So the current state of knowledge about \nAI capabilities is like, the models are  ",
      "offset": 1945.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "qualitatively more impressive and more useful over \ntime, and also all these benchmarks are going up,  ",
      "offset": 1951.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and there’s the graph that Epoch made with all \nthe benchmarks saturating quickly or whatever.",
      "offset": 1957.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "But the problem with this is the qualitative \nimpressions, you can’t sort of turn that into  ",
      "offset": 1963.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "a trend easily and be like, this is where we’re \ngoing to be in one year, in two years. And the  ",
      "offset": 1970.24,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "benchmarks, what does this actually mean in the \nreal world? We’ve generally found that these  ",
      "offset": 1977.36,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "are pretty decoupled. It’s like it gets this \namazing score on coding competition problems,  ",
      "offset": 1984.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "but it also can’t actually help me \nin my day job do this simple task.",
      "offset": 1989.6,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "So we’ve been building this suite of diverse \nautonomy-requiring tasks, where the model has  ",
      "offset": 1996.48,
      "duration": 11.68
    },
    {
      "lang": "en",
      "text": "basically access to a computer and it can do stuff \non the computer and use tools and run code and  ",
      "offset": 2008.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "do whatever to accomplish the task. The tasks are \nnot all super realistic, but they’re more diverse  ",
      "offset": 2013.92,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "than a lot of these benchmarks are, so it’s \nharder to overfit the model scaffolding to them.",
      "offset": 2022.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "So it’s a range, from things that are sort \nof tricky reasoning and inference sort of  ",
      "offset": 2028.88,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "puzzles — like, “You’re getting this data and you \nhave to figure out the pattern that’s generating  ",
      "offset": 2036.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the data,” like a symbolic regression thing. \nAnd cyber CTF [capture the flag] type tasks,  ",
      "offset": 2041.04,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "or just “make this web app” \nor “do this ML research task.”",
      "offset": 2049.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And the main thing we’ve done that is \ndifferent is we’ve had humans attempt  ",
      "offset": 2054.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "basically all of these tasks, and we \nrecord how long it takes the humans  ",
      "offset": 2060.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to do the task — humans who in fact have the \nexpertise, so hopefully we’re not timing that  ",
      "offset": 2064.08,
      "duration": 9.92
    },
    {
      "lang": "en",
      "text": "they had to go away and learn about this thing. \nBut for someone who has the requisite knowledge,  ",
      "offset": 2074,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "how long is it? How many steps do \nthey need? How long does it take them?",
      "offset": 2079.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So then we can get this meaningful Y-axis \non which to measure model capabilities,  ",
      "offset": 2083.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "which is diverse autonomous tasks in terms of \nhow long they take humans. So if you order all  ",
      "offset": 2089.6,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "the tasks by how long they take humans, where \nis the point that the model gets 50% success?",
      "offset": 2097.84,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "We do see a relationship between length \nof task for humans and how successful the  ",
      "offset": 2108.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "models are. It’s not perfect. It’s \nnot like they can do all the tasks  ",
      "offset": 2113.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "up to one hour and then none of the ones \nabove; it’s like as you double the time,  ",
      "offset": 2116.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "it gets increasingly unlikely \nthat the model can actually do it.",
      "offset": 2121.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So then we have this axis and we can plot \nmodels over time on it, and you can look at  ",
      "offset": 2125.6,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "how long it takes: how many months for \na doubling of the length of time for  ",
      "offset": 2135.28,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "humans of the sort of representative \ntasks that the model can complete?",
      "offset": 2142.32,
      "duration": 5.108
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, so you’ve got this \ngeneral issue that people have lots  ",
      "offset": 2147.428,
      "duration": 5.532
    },
    {
      "lang": "en",
      "text": "of different tests of how impressive \nthese AI models are. But there’s always  ",
      "offset": 2152.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "an issue of how actually impressive \nis that outcome? We find out, wow,  ",
      "offset": 2157.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "now it’s able to do this thing. But is that \nimpressive? Does that really matter? Is that  ",
      "offset": 2161.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "going to move the needle to applications in \nthe real world? Is this going to allow it to  ",
      "offset": 2165.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "act autonomously? It can be a little bit hard to \nunderstand intuitively whether any of it matters.",
      "offset": 2168.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "So you’ve come up with a modification of \nthis approach. Firstly, you think about  ",
      "offset": 2174.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "a very wide range of different tasks — so \nit’s harder for you to teach to the test,  ",
      "offset": 2179.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to just teach the model to do one very narrow \nthing, but then something else nearby or  ",
      "offset": 2184.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "something else that’s important to matter in \nthe real world that it can’t do at all. It  ",
      "offset": 2187.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "just can only play chess super well or something \nlike that. So a lot of diversity of the tasks.",
      "offset": 2190.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "And then in terms of measuring \nhow impressive the performance is,  ",
      "offset": 2196.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you think about it in terms of how long would \nit have taken a human to do? Or how long did it,  ",
      "offset": 2200,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in your tests, take for a human to \nbe able to accomplish this task? Are  ",
      "offset": 2204.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "there any simple examples of the tasks? Ones \nthat non-computer people could understand?",
      "offset": 2208.8,
      "duration": 4.147
    },
    {
      "lang": "en",
      "text": "Beth Barnes: There’s the ML research ones, like, \n“Train this model to improve its performance on  ",
      "offset": 2212.947,
      "duration": 6.493
    },
    {
      "lang": "en",
      "text": "this dataset” or something. There are some \nvery easy, very general research ones, like,  ",
      "offset": 2219.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "“Go and look up this fact on Wikipedia” \nor something. There’s some that are like,  ",
      "offset": 2226.24,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "“Build a piece of software \nthat does these things.”",
      "offset": 2235.2,
      "duration": 3.028
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, OK. So they’re substantial \ntasks, the kind of things that you might  ",
      "offset": 2238.228,
      "duration": 2.972
    },
    {
      "lang": "en",
      "text": "delegate to a staff member inside a company. \nAnd I guess you’ve gotten software engineers  ",
      "offset": 2241.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "or people who generally would be able to do these \nkinds of things, and said, on average, this takes  ",
      "offset": 2248.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a human 15 minutes, it takes them half an hour, \nit takes them an hour, it takes them two hours.",
      "offset": 2251.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And you find that there’s quite a strong \nrelationship between whether the model  ",
      "offset": 2255.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "can and can’t do it, and how long it would \nhave taken a human being to do it — which  ",
      "offset": 2258.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I guess speaks to the fact that it’s hard for \nthese models to stay on task for a long period  ",
      "offset": 2263.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of time. Or people talk about coherence: \nthings that require them to think about,  ",
      "offset": 2268.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "“How am I going to do this?” over a long period \nof time. It requires many different steps that  ",
      "offset": 2274.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "have to be chained together. They currently \nkind of struggle with that. Is that right?",
      "offset": 2278.96,
      "duration": 3.587
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. I think there’s various \ndifferent explanations for why longer  ",
      "offset": 2282.547,
      "duration": 5.053
    },
    {
      "lang": "en",
      "text": "tasks are harder. There’s different \nways in which they can be harder.",
      "offset": 2287.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "One is just if there’s a longer sequence of things \nthat you need to do — so you need to get them all  ",
      "offset": 2291.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "right, and there’s a higher probability that \nyou’ll get completely stumped by one of them.  ",
      "offset": 2295.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "You could also imagine either the human just \nhas to think really hard, or they try a bunch  ",
      "offset": 2299.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "of times until they succeed. So it’s not \nlike one totally straightforward measure.",
      "offset": 2305.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "But there’s also this argument that, as we do more \nRL training on tasks, it’s much more expensive to  ",
      "offset": 2312,
      "duration": 9.76
    },
    {
      "lang": "en",
      "text": "do RL training on longer tasks than on shorter \ntasks. And it’s a much harder learning problem  ",
      "offset": 2321.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "if you just get a signal at the end of what in the \nmiddle you should have done differently, whereas  ",
      "offset": 2326.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "if the task only has two steps, it’s much easier \nto learn what you should have done differently.",
      "offset": 2330.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So in general, there are a bunch of reasons to \nexpect that the shorter tasks will be easier for  ",
      "offset": 2335.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "models, and the models will be better trained \nto be able to do short tasks than long tasks.",
      "offset": 2342.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: And the upshot is you’ve been able \nto forecast… It’s a little bit hard to say this,  ",
      "offset": 2346.56,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "but you could say that, a year ago, the \nmodels would have a 50% chance of succeeding  ",
      "offset": 2353.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "at something that would take a typical human \n15 minutes; six months later they had a 50%  ",
      "offset": 2358.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "chance of succeeding at something that would \ntake 30 minutes; six months later they have  ",
      "offset": 2364.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "a 50% chance of succeeding at something that \nwould take a human an hour. Seems like it’s  ",
      "offset": 2367.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "roughly doubling about every six months. \nCould you say what the numbers there are?",
      "offset": 2371.76,
      "duration": 5.187
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, six months is about right. \nWe’re looking all the way from GPT-2 in 2019,  ",
      "offset": 2376.947,
      "duration": 9.853
    },
    {
      "lang": "en",
      "text": "where it’s at a few seconds or something \nlike that, to the latest models. And yeah,  ",
      "offset": 2386.8,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "over that time there’s a pretty good \nfit to a doubling of something like  ",
      "offset": 2393.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "every six months. And we also did \na sort of ablation [sensitivity  ",
      "offset": 2400.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "analysis] both over the actual noise in the data \nand over different methodological choices we could  ",
      "offset": 2405.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "make, and the range of estimates you get is \nbasically between three months and one year.",
      "offset": 2411.84,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "So I think there’s also some correction \nfactor you should take in interpreting this,  ",
      "offset": 2421.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that maybe tasks which make good evals that \nyou can run and score automatically are maybe  ",
      "offset": 2426.16,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "systematically easier for models in various ways \n— both as the same argument that that’s the sort  ",
      "offset": 2434.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of thing that you can RL on, and just other \nthings, where it’s just a somewhat different  ",
      "offset": 2439.28,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "distribution of tasks, and it might be \nthe case that models are worse at them.",
      "offset": 2446.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So maybe 15 minutes as measured on our benchmark \ndoesn’t really correspond to 15 minutes on more  ",
      "offset": 2450.24,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "representative tasks. Maybe it will be more \nlike five minutes or something. But I think  ",
      "offset": 2459.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that there is a trend, and that the trend is going \nstrongly upwards at this exponential rate — so  ",
      "offset": 2463.36,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "a fixed doubling time, and the doubling time \nbetween three months and a year is pretty solid.",
      "offset": 2470.24,
      "duration": 7.908
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK. So currently the models, at \nleast within these sorts of tasks — which I  ",
      "offset": 2478.148,
      "duration": 4.732
    },
    {
      "lang": "en",
      "text": "guess are unusually measurable, legible, \nunderstandable tasks — have a 50% chance  ",
      "offset": 2482.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of succeeding at something that would take a \nhuman… What did you say, three to 12 hours?",
      "offset": 2487.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Beth Barnes: So the current horizon length \nnumber is two hours maybe for the best models,  ",
      "offset": 2492.96,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "but the doubling time is somewhere \nbetween three and 11 months.",
      "offset": 2501.28,
      "duration": 4.948
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK. So we can project that \nforward and say it’s about now two hours,  ",
      "offset": 2506.228,
      "duration": 3.692
    },
    {
      "lang": "en",
      "text": "and then somewhere between three and 12 months \nfrom now it’ll be four, and then somewhere  ",
      "offset": 2509.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "between three and 12 months after that, it will \nbe eight. And you can project up from there.",
      "offset": 2515.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And I suppose it’s partly looking at that that \nhas made people freak out a little bit, because it  ",
      "offset": 2520.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "seems like within not that far into the future, \nwe could imagine that they would be able to  ",
      "offset": 2526.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "accomplish the full sort of work that a software \nengineer might do in a day or in a week. And then  ",
      "offset": 2531.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "that would have more economic applications, that \nevidently they’re able to act with substantially  ",
      "offset": 2538,
      "duration": 6.866
    },
    {
      "lang": "en",
      "text": "more autonomy and maintain a coherent \npurpose over a longer period of time.",
      "offset": 2544.866,
      "duration": 3.694
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And this is also \nnot what an average human could do,  ",
      "offset": 2548.56,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "but a human who has all the knowledge for that \nparticular task. Because the models are very much  ",
      "offset": 2557.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "not bottlenecked on generic subject knowledge, and \nthey can do all of these things simultaneously.",
      "offset": 2563.44,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Something that would be nice to measure is: \nwhat is the difference in performance if you  ",
      "offset": 2574.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "get the best human to do all of your tasks, \nversus for each task you pick the best human.  ",
      "offset": 2578.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "And the thing that we’re comparing the model \nto is much more like for each task, pick the  ",
      "offset": 2583.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "best human. That’s a sort of correction upwards \nin impressiveness that you should do of like,  ",
      "offset": 2587.84,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "this is just one model doing all of these \ndifferent expertise areas. And humans are  ",
      "offset": 2595.28,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "worse at doing tasks that require expertise from \nmultiple areas because they’re not specialised.",
      "offset": 2602.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Forgive me the mundane operational \nquestion, but it seems like this project must  ",
      "offset": 2608.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "have been an insane amount of work, because \nyou’ve had to hire or contract dozens, hundreds  ",
      "offset": 2612.88,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "of software engineers or people who are able to \ndo this quite impressive work on a computer and  ",
      "offset": 2619.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "then time them and then see how successful were \nthey at doing the thing. Is it a lot of work?",
      "offset": 2624.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Well, yeah. It was kind of \nfrustrating because… This is slightly off topic  ",
      "offset": 2628.8,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "to your question, but a while ago I was telling \npeople it would be really important to get good  ",
      "offset": 2638.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "quality human data provision for alignment and \nsafety work. And it was like, “Actually there’s  ",
      "offset": 2643.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "a bunch of people doing this already. Maybe it’s \nfine, like surge and scale.” And none of these  ",
      "offset": 2650.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "providers have been willing to work with us — \neither because we’re too small fry, or the talent  ",
      "offset": 2654.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "we’re asking for is too high, or there’s others \nwe didn’t think would be high quality enough.",
      "offset": 2659.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "But yeah, luckily our internal team is amazing \nand has made this all happen. So basically a  ",
      "offset": 2665.6,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "bunch of credit to them. And I think this is \nmaybe one thing that METR does well relative  ",
      "offset": 2672.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to labs or academics or something, is having \nreally highly skilled staff doing this kind  ",
      "offset": 2677.04,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "of stuff that’s a mix of technical and operations \nand logistical work, and being willing to do that.",
      "offset": 2686.24,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Whereas in companies and academic labs \nand things, I’ve seen more people like,  ",
      "offset": 2693.12,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "“Ugh, human data is annoying. \nLet’s just avoid it.” Just like,  ",
      "offset": 2700.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "even if something is clearly the most useful \nthing for pushing forward the research, it’s like,  ",
      "offset": 2704.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "“I think I’d rather work on \nmaking the algorithms cleverer.”",
      "offset": 2710.64,
      "duration": 3.108
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. Whereas you’re willing to \ndo the schlep work that actually just seems  ",
      "offset": 2713.748,
      "duration": 2.812
    },
    {
      "lang": "en",
      "text": "the most important question to answer, even if \nit’s not as intellectually challenging, maybe.",
      "offset": 2716.56,
      "duration": 4.627
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, or high status in some way.",
      "offset": 2721.187,
      "duration": 2.013
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think something that \nshocked people about this research  ",
      "offset": 2723.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "when you published it was that you were \nfinding that the agents were capable of  ",
      "offset": 2727.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "doing substantially longer tasks than \nwhat people had previously understood.",
      "offset": 2731.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So you’re saying they had a 50% chance of \nsucceeding at something that took two hours.  ",
      "offset": 2735.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "But you gave the model access to a very powerful \ncomputer chip that it could use as part of its  ",
      "offset": 2738.48,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "work, basically, and you gave it a lot of time \nto think. You gave it a lot of compute resources,  ",
      "offset": 2745.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "more than people were typically giving \nwhen they were testing these capabilities.",
      "offset": 2750.16,
      "duration": 3.587
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. So this doubling time of \ntime horizon thing that I’ve been talking  ",
      "offset": 2753.747,
      "duration": 5.373
    },
    {
      "lang": "en",
      "text": "about is not yet published. I think you were \nprobably referring to the RE-Bench? That was  ",
      "offset": 2759.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "like a subset of these tasks that are just \nfocused on doing ML engineering research.",
      "offset": 2765.12,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "So things that we do that I think make \nour benchmark better quality or get  ",
      "offset": 2772.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "better model performance is, firstly, \nif you actually compare to humans,  ",
      "offset": 2776.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "often things just are harder or take longer \nthan people think. Especially in the context  ",
      "offset": 2780.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "of there’s this task and you’ve tried to make the \ninstructions unambiguous, but actually it’s kind  ",
      "offset": 2786.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of confusing in some way. Or actually you didn’t \nhave the package installed you needed for this.  ",
      "offset": 2791.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "So having humans actually do them removes \na bunch of these sort of spurious failures.",
      "offset": 2795.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Whereas other benchmarks, even SWE-bench \nVerified, which was the subset OpenAI did,  ",
      "offset": 2801.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "where they just had contractors \nlook at the task and be like,  ",
      "offset": 2808.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "How long do I think this would take? \nAnd do I think this is possible?",
      "offset": 2812.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "But I think then when Anthropic subsequently \npublished a thing when they ran Claude on  ",
      "offset": 2816.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "those and looked at the failures, some of \nthe failures were still because the task  ",
      "offset": 2821.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "was kind of impossible — because the \ntest cases were testing for some more  ",
      "offset": 2824.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "specific thing that it wasn’t clear that \nthat was what you were supposed to do.",
      "offset": 2829.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "So one thing is just getting rid \nof all the spurious failures,  ",
      "offset": 2832.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so it’s a more high quality \nmeasure of the model’s ability.",
      "offset": 2836.08,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Then the other thing is doing a fair \namount of elicitation effort. I think  ",
      "offset": 2838.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "often people have this sort of sense of \nhow much is reasonable to spend on models,  ",
      "offset": 2843.84,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "which is generally very cheap. But if you’re \ncomparing to, could you automate eight hours  ",
      "offset": 2850.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of highly skilled researcher engineer \nlabour, that’s like thousands of dollars.",
      "offset": 2856,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "It would be sad if we were like, “Models aren’t \ncapable of this. Models aren’t capable of this.  ",
      "offset": 2864.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Models aren’t capable of this. Oh, it \nturns out they’re capable and they’re  ",
      "offset": 2870,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "100x cheaper than humans.” We would rather \nknow when are they capable of doing it,  ",
      "offset": 2873.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "but it’s more expensive, and you can see the \ncost going down over time. So we tried hard  ",
      "offset": 2879.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to give the model fair resources and \nbenchmark it fairly against humans and  ",
      "offset": 2885.04,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "have reasonable spend compared to \nwhat it would cost for a human.",
      "offset": 2892.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: And the upshot \nwas they seemed much more  ",
      "offset": 2896.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "capable than what people had previously thought?",
      "offset": 2900.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I don’t know if that’s \nexactly the case. I think it was  ",
      "offset": 2903.92,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "surprising how well they did on these ML \nresearch tasks. I’m not sure that people  ",
      "offset": 2911.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "had measured it that precisely before. It \nwas just like people hadn’t checked this.",
      "offset": 2916,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Another thing is the case that model \nperformance still has this property  ",
      "offset": 2920.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that it’s very high variance in some sense: \nthe best 1% of attempts are way, way better  ",
      "offset": 2926,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "than the median attempt. So I think there can \nbe a thing where you try and get a model to do  ",
      "offset": 2932.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "something yourself. You’re like, I tried twice \nand it didn’t do a very good job either time.",
      "offset": 2937.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "But it might be the case that the model is \ncheap enough that you could have just got  ",
      "offset": 2942.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it to try 100 times. And if there’s some \nrelatively cheap way you can select the  ",
      "offset": 2945.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "most promising 10 and then look at a few of \nthem, that then might be way better than the  ",
      "offset": 2951.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "sort of representative impression \nyou got of the model capabilities.",
      "offset": 2956.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "So we found that when you do this best of k, \nand you let the model do a bunch of attempts,  ",
      "offset": 2960.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and then you pick the one that scored \nhighest, models do much better. And this  ",
      "offset": 2966.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "has this general result across ML capability \nevals that you get kind of log-log scaling:  ",
      "offset": 2970.56,
      "duration": 9.84
    },
    {
      "lang": "en",
      "text": "it looks like a straight line if you \ndouble the amount of money you’re spending.",
      "offset": 2982,
      "duration": 3.908
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, yeah. I guess a hot \ntip for listeners is: I think I’ve only  ",
      "offset": 2985.908,
      "duration": 4.892
    },
    {
      "lang": "en",
      "text": "recently started getting much more value out \nof ML models that I’m using just in my work.",
      "offset": 2990.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "And I think there’s two big breakthroughs. \nOne is to stick a tonne of good examples into  ",
      "offset": 2996.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "context. Don’t just give it one example of an \narticle that you wrote that you liked and say,  ",
      "offset": 3002,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "“Make something else like this.” Put in every \nsingle article that you’ve ever written that you  ",
      "offset": 3006.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think is good and say, “Copy this style.” \nThe more data into context, the better.",
      "offset": 3009.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "The other thing is I used to say, “Can you please \ngo away and write one possible example of this,  ",
      "offset": 3013.84,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "and then check in with me to see whether you think \nit’s good.” But of course, it’s so cheap for them  ",
      "offset": 3020.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to output 10 or 100 possible articles that you \ncould write or blog posts that you could include.  ",
      "offset": 3025.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "So you just get it to output a tonne — way more \nthan you would ever reasonably ask a human to do  ",
      "offset": 3030.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "— and then just scan over them and be like, “Most \nof these are trash, but this one is really great.”",
      "offset": 3034.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. And I think there’s a question \nof, on what range of tasks can you actually do  ",
      "offset": 3038.72,
      "duration": 9.44
    },
    {
      "lang": "en",
      "text": "this? Some tasks you can just automate this in \ncode because there is some simple scoring function  ",
      "offset": 3048.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that you can compute, and other things the \nmodel would probably be able to recognise a good  ",
      "offset": 3054.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "solution. So you need to have some scaffolding \nsetup that will then look over all of the —",
      "offset": 3058.48,
      "duration": 5.348
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Look over its own attempts \nand say this is the best one of them.",
      "offset": 3063.828,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. Or you focus your \nscaffolding on having the model build  ",
      "offset": 3066.547,
      "duration": 5.773
    },
    {
      "lang": "en",
      "text": "the tests that will measure how well it did \nand then running attempts against that. And  ",
      "offset": 3072.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "maybe you don’t always need to get it down to \nthe one. If you can just prune out a bunch of  ",
      "offset": 3078.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the worst attempts and then have a human look over \nthem. Maybe this is feasible in some situations.",
      "offset": 3083.28,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Let’s back up a bit. I \nguess there’s been two main streams  ",
      "offset": 3091.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of research. One is trying to figure \nout how autonomously can these models  ",
      "offset": 3095.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "work and would they maybe be able \nto survive and spread in the wild.",
      "offset": 3100,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "And then there’s this other line of research, \nwhich is trying to figure out how much are  ",
      "offset": 3104.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "these models able to help with research into \nmachine learning. I guess to many people it  ",
      "offset": 3108.32,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "will be obvious why it’s very important to \nknow whether AI is good at improving AI,  ",
      "offset": 3115.6,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "but do you want to just quickly explain \nwhy that’s a key thing to measure?",
      "offset": 3118.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. It just seems like it’s \nboth approximately sufficient and approximately  ",
      "offset": 3121.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "necessary for really dangerous things to \nhappen. If you have AI that can automate  ",
      "offset": 3128,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "a bunch of research and speed up the progress a \nlot, then — especially under the current paradigm,  ",
      "offset": 3137.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "where there’s no monitoring of what labs \nare doing internally — you could just very  ",
      "offset": 3142.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "quickly get very high levels of capability \nthat you are not equipped to deal with.",
      "offset": 3145.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So that’s approximately the sufficiency argument:  ",
      "offset": 3156.32,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "this could mean that stuff goes very fast \nin a way that humans aren’t able to keep  ",
      "offset": 3158.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "up with. And there’s a very strong incentive to \nautomate a bunch of your AI R&D, and then humans  ",
      "offset": 3161.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "don’t have oversight of what data is being \nproduced, or just generally what’s going on.",
      "offset": 3168.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And the necessity case is something like, it \nseems like the first very dangerous models  ",
      "offset": 3173.76,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "probably won’t have all of the capabilities \nthat they need to take over at the start,  ",
      "offset": 3182.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but there’s some situation in which, if they can \nget some compute and get some resources and are  ",
      "offset": 3186.72,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "good at AI R&D, then they can improve their \ncapabilities on a bunch of the relevant axes.",
      "offset": 3194.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So just in broad strokes, this \nmatters because it’s the thing that would  ",
      "offset": 3200.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "set off a software-based intelligence \nexplosion, because it’s a recursive  ",
      "offset": 3204.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "self-improvement loop where it gets better \nand so it’s more able to improve itself.",
      "offset": 3207.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "That may or may not actually explode. \nIt’s possible that the task of improving  ",
      "offset": 3210.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "its own capabilities gets more difficult so \nquickly that the process kind of peters out,  ",
      "offset": 3216.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "or is reasonably slow and requires a lot of \ncompute. But maybe that won’t be the case,  ",
      "offset": 3220.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and maybe you will see significant \nimprovement in capabilities very  ",
      "offset": 3224.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "rapidly once AI is doing most \nof the work of improving it.",
      "offset": 3228.16,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "But you’re also saying that at the first point \nthat an AI model might be somewhat dangerous,  ",
      "offset": 3230.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it might have some gaps in its capabilities, \nit might have eight of the 10 things that it  ",
      "offset": 3235.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "would need in order to go and do stuff \nthat humans would really dislike. But if  ",
      "offset": 3238.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "one of the 10 things that it’s able \nto do is improve itself enormously,  ",
      "offset": 3242.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "then it will be able to patch the gaps \nin its abilities and then go from there.",
      "offset": 3245.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. And then your \ndangerous capability evals are no  ",
      "offset": 3248.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "longer really applicable if it can also —",
      "offset": 3251.44,
      "duration": 3.916
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Just fix any of those weaknesses.",
      "offset": 3255.356,
      "duration": 2.311
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah.",
      "offset": 3257.667,
      "duration": 0.333
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, so that’s why the ability of \nthese models to do ML research is a pretty big  ",
      "offset": 3258,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "deal. How good are they in general at doing that \nwork now, and what’s the trend going forward?",
      "offset": 3265.28,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Models can definitely do meaningful \nwork here in problems that have this character  ",
      "offset": 3271.68,
      "duration": 13.2
    },
    {
      "lang": "en",
      "text": "that you can isolate this particular task and \nthen automatically check it. I think there is some  ",
      "offset": 3286.96,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "question of how well do the models generalise to \nthe aspects of research and things where you can’t  ",
      "offset": 3294.16,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "do that, where it’s more entangled and you \nneed higher reliability and that kind of thing.",
      "offset": 3302.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Some evidence that maybe it’s not that hard to \nimprove a model’s ability to do these very long  ",
      "offset": 3308.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and complex tasks is that they’re able to do \nthese extremely long chains of reasoning for  ",
      "offset": 3314.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "hard math tasks or something. And \nmaybe math is very easy to train on,  ",
      "offset": 3320,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but maybe it doesn’t take that much \ntraining to get a lot better at this.",
      "offset": 3324.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "And then also maybe you can just accelerate AI \nR&D a lot just on these kinds of tasks that are  ",
      "offset": 3328.08,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "quite easily checkable in various ways — because \na bunch of software engineering is like this,  ",
      "offset": 3336,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "where you can at least get a pretty \nstrong filter on the solutions.",
      "offset": 3342.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "One thing we put out recently is some results on \nan extended version of KernelBench. So this is  ",
      "offset": 3347.76,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "writing basically better code to make \nML engineering tasks run faster. Again,  ",
      "offset": 3357.68,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "this is one of the “elicitation \nis important” lessons.",
      "offset": 3366.88,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "So the results on the leaderboard were like, it’s \na speedup of 1.01 or 1.05 or something — which is  ",
      "offset": 3369.6,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "like models basically can’t do anything useful \nhere. And then Tao at METR worked on this for  ",
      "offset": 3379.68,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "four weeks, and now it’s like, models can \ndouble the speedup. So that is useful.",
      "offset": 3386.48,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So I don’t understand \nwhat programming kernels is,  ",
      "offset": 3395.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "but this is something that would enable \nmachine learning research to go faster.  ",
      "offset": 3398.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Previously people thought that the models were \nbasically rubbish at this, that they could only  ",
      "offset": 3402.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "speed up from a baseline 1% or 5%. But now I \nguess you gave them access to proper compute,  ",
      "offset": 3405.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you gave them time, you gave them the chance to \ndo many attempts and then pick the best one — and  ",
      "offset": 3410.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "now you find that actually it can double it. \nAnd this is approximating human performance?",
      "offset": 3414.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think on these \ntasks that probably means it’s  ",
      "offset": 3418.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "quite a lot cheaper than what you would \nneed to pay humans. Maybe like 10x or  ",
      "offset": 3424.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "something. I think it would cost hundreds \nof dollars to pay someone to do this work.",
      "offset": 3430.08,
      "duration": 7.348
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: But the model can \ndo it for like tens of dollars.",
      "offset": 3437.428,
      "duration": 1.999
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Like $30, or something like \nthat. And we could probably get that down  ",
      "offset": 3439.427,
      "duration": 5.693
    },
    {
      "lang": "en",
      "text": "with more optimisation. So definitely \ngetting to the point where models can  ",
      "offset": 3445.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "meaningfully accelerate research. And already \nwith things like Copilot and coding assistants,  ",
      "offset": 3449.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "a large fraction of the code is written by \nthe models — and maybe they’re not the most  ",
      "offset": 3457.12,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "crucial hard parts or something, but it \ndefinitely is a meaningful acceleration.",
      "offset": 3465.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And I think just in general there’s a lot of \nevidence that this will just keep going up.  ",
      "offset": 3471.92,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "It’s hard to say exactly where we are now \nor how far we could get with elicitation  ",
      "offset": 3479.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "on current models or with one more scaleup \nor something. The thing that’s much more  ",
      "offset": 3484,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "confident is just it’s not going to be that \nlong if you’ve got this six months doubling  ",
      "offset": 3486.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "time or something. Really the scope of what \nthe models can do is going up pretty fast.",
      "offset": 3493.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think you said to me \nyesterday that you thought it’s possible  ",
      "offset": 3498.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that the models would actually turn out \nto best in the research part of this,  ",
      "offset": 3502.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "or the ideas-focused part of ML research — which \nto some people would seem counterintuitive that  ",
      "offset": 3506.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you might expect them to be best at the stuff \nthat is not like one-off insights into things,  ",
      "offset": 3511.52,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "but rather just like writing code in \na way that they’ve written code many  ",
      "offset": 3518.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "times before. Do you want to explain \nwhy you think that might be the case?",
      "offset": 3521.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Beth Barnes: This is a sort of hot take, \nnot a super strongly endorsed view,  ",
      "offset": 3524.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "but I do think people have this reaction \nthat the models can do this engineering  ",
      "offset": 3530.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "stuff that’s more straightforward, but \nthey can’t do this scientific insight  ",
      "offset": 3537.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "or methodology or coming up with new inventions.",
      "offset": 3541.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "But I think a lot of what humans are doing when \nthey do that is having read a bunch of research  ",
      "offset": 3544.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "papers and seeing that some method is analogous to \nsome other thing, sort of like “nothing new under  ",
      "offset": 3551.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the sun” — a lot of things are just applications \nof things that are in fact known somewhere else.  ",
      "offset": 3556.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And the thing the models are really good at is \nreally comprehensively having read every single  ",
      "offset": 3562,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "paper and knowing the literature and having \nseen a huge number of results of experiments.",
      "offset": 3566.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "One of the things that we saw in \none of our tasks… So the task is  ",
      "offset": 3571.68,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "basically to figure out what the scaling law is \nunder certain restrictions. You have models of  ",
      "offset": 3579.84,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "a certain size and you can train them with this \nmuch compute, and you need to figure out what the  ",
      "offset": 3589.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "compute-optimal hyperparameters are to train at \na level above the amount of compute that you’re  ",
      "offset": 3593.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "actually given. And the models didn’t always \ndo that well at this, but they’re really good  ",
      "offset": 3597.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "at guessing. They’re much better than humans \nat guessing what roughly the right number is.",
      "offset": 3603.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So there’s going to be a bunch of things where \nmodels have just seen so many more experiments and  ",
      "offset": 3607.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so much more data that for things like, “Predict \nwhether this research direction is promising,” or,  ",
      "offset": 3613.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "“Figure out how to solve this sort of thorny \ntheoretical problem,” it’ll be like, “Well,  ",
      "offset": 3619.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "in this obscure field of math and \nthis random paper that no one’s read,  ",
      "offset": 3624.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it turns out you can solve this \nanalogous problem in this way.”",
      "offset": 3629.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "That seems very plausible to me that \nthat is something where models might  ",
      "offset": 3632.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "be comparatively advantaged to \nhumans rather than disadvantaged.",
      "offset": 3636.08,
      "duration": 3.428
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Right. It’s something that humans \nstruggle to do, and it’s one reason why we  ",
      "offset": 3639.508,
      "duration": 3.052
    },
    {
      "lang": "en",
      "text": "think this is kind of magical, so how would the \nAI be able to do it? But actually it’s an area  ",
      "offset": 3642.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "where it might be their home turf because \nthey have such a broad range of knowledge.",
      "offset": 3646.32,
      "duration": 2.78
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right, yeah.",
      "offset": 3649.1,
      "duration": 0.34
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: The way that people are sometimes \nsceptical that it’s going to be able to have  ",
      "offset": 3649.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the brilliant scientific flash of insight, it \nfeels like the equivalent of how five years ago,  ",
      "offset": 3652.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you’d hear, “But they’ll never be able to make \ngreat art. They’ll be able to make great poetry.”",
      "offset": 3656.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Or play go, or…",
      "offset": 3659.84,
      "duration": 2.068
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, it turns out that \nstuff is actually trivial for them,  ",
      "offset": 3661.908,
      "duration": 1.932
    },
    {
      "lang": "en",
      "text": "and that they write poetry much better \nthan almost all humans. I guess there have  ",
      "offset": 3663.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "been experiments finding that they write much \nbetter poetry even than expert poetry writers,  ",
      "offset": 3668.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "as assessed by other human beings. So yeah,  ",
      "offset": 3672.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "I guess it’s a technical person’s cope to make \nthemselves feel like they’re not so replaceable.",
      "offset": 3675.36,
      "duration": 4.227
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I mean, the counterargument \nis maybe just that this stuff has less of  ",
      "offset": 3679.587,
      "duration": 5.533
    },
    {
      "lang": "en",
      "text": "this easily checkable, more \nstructured for engineering,  ",
      "offset": 3685.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for writing code… You can write tests and \nyou can select for only the programs that  ",
      "offset": 3690.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "actually pass the test and this kind of \nthing, where it’s harder to do that for  ",
      "offset": 3694.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "research ideas. But I do expect the underlying \nability in that to be maybe relatively good.",
      "offset": 3697.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So I’ve been \nassuming that there would be  ",
      "offset": 3704.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "a software intelligence explosion \ntriggered. I guess recently,  ",
      "offset": 3708.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I’ve been assuming it’s going to happen within \nthe next seven years, and two years wouldn’t  ",
      "offset": 3711.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "at all be surprising. Does your research cast \nany light on what numbers seem plausible here?",
      "offset": 3715.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, basically those numbers \nseem very reasonable. I think one number  ",
      "offset": 3720,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we talk about is, at what point is 90% of \nthe research oomph going to be coming from  ",
      "offset": 3724.32,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "models as opposed to humans? I guess that’s one \nway to be like, when is this starting? Yeah,  ",
      "offset": 3732,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it seems like in seven years we’ll be at \nmodels that can do the equivalent of a year  ",
      "offset": 3737.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of labour from someone who is expert at all of \nthe things. And also much faster in serial time.",
      "offset": 3743.6,
      "duration": 11.428
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. And I guess at that point, I \nsuppose if that trend holds and that is the case,  ",
      "offset": 3755.028,
      "duration": 3.532
    },
    {
      "lang": "en",
      "text": "it’s really hard to see how you wouldn’t have \ntriggered a software intelligence explosion,  ",
      "offset": 3758.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "inasmuch as one as possible, depending \non how strong the feedback loop is.",
      "offset": 3761.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. It has multiplied the number \nof people working on this by a very large amount.  ",
      "offset": 3764.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And you’ve introduced a bunch of workers \nthat have capabilities much beyond humans,  ",
      "offset": 3769.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "at least in terms of cross-domain \nknowledge and serial speed and things.",
      "offset": 3775.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. When I said on Twitter \nthat I was going to be interviewing you,  ",
      "offset": 3780.16,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "an audience member wrote in with a question on \nthis topic that I thought was pretty insightful:  ",
      "offset": 3782.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "“How much of a bottleneck is labor on algorithmic \nprogress? How likely is it that we’ll automate AI  ",
      "offset": 3785.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "R&D labor with underwhelming results, and \nrealize the main bottleneck was compute?”",
      "offset": 3790.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think definitely \ncompute is a key bottleneck, and as you  ",
      "offset": 3796.8,
      "duration": 12.8
    },
    {
      "lang": "en",
      "text": "automate your researchers, compute \nis more of a bottleneck — because  ",
      "offset": 3810.32,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "it’s also governing how many of the \nautomated researchers you can run.",
      "offset": 3812.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "But I think there are just obviously \nlots of areas of low-hanging fruit  ",
      "offset": 3817.52,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "that it feels relatively clear to me how \nyou would put in more labour and get more  ",
      "offset": 3824.96,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "performance from the same compute. I \ndo think there is a question of why  ",
      "offset": 3834.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "isn’t this happening already? And maybe it’s \nmore that it’s kind of hard to coordinate,  ",
      "offset": 3839.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "as opposed to there are no humans who \nare cheap enough for it to make sense.",
      "offset": 3845.84,
      "duration": 4.548
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Coordinate what?",
      "offset": 3850.388,
      "duration": 0.732
    },
    {
      "lang": "en",
      "text": "Beth Barnes: The types of things I’m imagining \nyou can spend labour on: there’s definitely the  ",
      "offset": 3852.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "writing faster kernels or otherwise speeding \nup experiments. This is basically equivalent  ",
      "offset": 3859.76,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "to increasing the amount of compute — you’re \njust making your code run in half the time,  ",
      "offset": 3867.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "making your code run much more efficiently. \nI think there is a bunch more optimisation  ",
      "offset": 3870.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that could be done there, and very good \nkernel engineers are rare and expensive,  ",
      "offset": 3876.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "so this is something you could \npour a bunch of AI labour into.",
      "offset": 3882.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Other things are building more tasks \nand environments, and setting up  ",
      "offset": 3886.48,
      "duration": 10.8
    },
    {
      "lang": "en",
      "text": "and obtaining different kinds of training data \nand things, if you having much higher quality  ",
      "offset": 3899.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "data does mean your compute goes a lot further, \nand this is maybe what things are bottlenecked on.",
      "offset": 3905.04,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Environments that are sort of rich \nand interesting but also have rapid  ",
      "offset": 3911.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "feedback loops, it does seem just like \nthere’s just a bunch of labour you could  ",
      "offset": 3915.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "do to build more of these and build more \ninteresting things here. And yeah, there’s  ",
      "offset": 3918.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "maybe some continuum with something that looks \nmore like going out and constructing a task,  ",
      "offset": 3927.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "versus models sort of challenging themselves to \ndo a thing and then getting feedback from the  ",
      "offset": 3930.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "environment on it. But it does seem like there’s \na bunch of ways you could spend compute there.",
      "offset": 3937.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Other things that are kind of equivalent \nto increasing your amount of compute  ",
      "offset": 3944.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "could be more closely babysitting \nexperiments and runs, being like,  ",
      "offset": 3949.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "“This one I can already tell that it’s not \ngoing to be that useful, so I’m going to  ",
      "offset": 3956.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "put something else in the queue, and let’s \ncull these ones that are doing less well.”",
      "offset": 3960.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I think if you just directly ask the question, \n“Is there somewhere you could put labour in that  ",
      "offset": 3968.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "would speed things up?,” it’s like, yes, \nthere are a lot of places if it was very  ",
      "offset": 3973.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "cheap. But the bottleneck would probably just be \nsetting up all the infrastructure to do that and  ",
      "offset": 3977.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "vetting the quality of all of the data and \nenvironments you’re producing or something.",
      "offset": 3985.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So the bottom line is that compute \nmight end up being the key limiting factor,  ",
      "offset": 3990.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in which case we will use our AI models to figure \nout how to get as much juice out of the compute  ",
      "offset": 3994.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "available as we possibly can. And probably there \nare a whole lot of margins on which, if compute  ",
      "offset": 4001.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "were the only factor now — I guess currently it \nseems like labour is probably, or I guess both  ",
      "offset": 4005.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "are really important — but if compute was the \nonly thing, then we would just use the enormous  ",
      "offset": 4008.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "amount of labour that we now had to figure \nout how to use the compute more efficiently.",
      "offset": 4011.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I guess even if the intelligence \nexplosion was underwhelming because  ",
      "offset": 4016.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this turned out to be a substantial issue, \nthe number of AI researchers is growing,  ",
      "offset": 4019.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "but it’s growing very slowly compared to the \namount of compute that is being manufactured.  ",
      "offset": 4024.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I think the amount of compute available to these \ncompanies is growing by many multiples every year.",
      "offset": 4027.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "So if compute ended up being the limiting factor — \nif we went from labour mattering to basically only  ",
      "offset": 4031.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "compute mattering — then that would still suggest \na substantial speedup in research progress.",
      "offset": 4037.36,
      "duration": 5.427
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. And the marginal \ninvestment in researchers now relative  ",
      "offset": 4042.787,
      "duration": 5.453
    },
    {
      "lang": "en",
      "text": "to compute is related to the price and \nthe speed of the researcher labour. If  ",
      "offset": 4048.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "they were a bunch cheaper and a bunch \nfaster, it would make sense to buy a  ",
      "offset": 4053.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "lot more. There might be a lot of things \nthat’s just not worth having humans do at  ",
      "offset": 4060.32,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "the current prices and serial times that would \nbe worth doing with faster and cheaper labour.",
      "offset": 4067.36,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, just to make sure that we \nunderline the most important messages here:  ",
      "offset": 4075.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "AIs currently have a 50% chance of doing something \non a computer that an expert might do in two  ",
      "offset": 4079.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "hours. This is doubling every three to 12 months. \nSo they’re becoming substantially more useful  ",
      "offset": 4084.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "agents: they’re able to do things over quite a \nlonger period of time and it’s improving rapidly.",
      "offset": 4089.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "They’re able to make a substantial difference to \nmachine learning research now. They’re already  ",
      "offset": 4094.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "used a great deal within the companies. They \nare probably capable of doing a whole lot more  ",
      "offset": 4097.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "stuff than is even appreciated at this \npoint — because if you really try hard,  ",
      "offset": 4102.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "then you can get much more out of them \nthan if you just make a cursory effort.",
      "offset": 4106.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "All of this leads to the conclusion that probably \nwe should expect recursively self-improving AI  ",
      "offset": 4110.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "pretty shockingly soon, relative to \nexpectations that we might have had  ",
      "offset": 4115.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "five or 10 years ago — that two years wouldn’t \nbe at all surprising, which is alarming to me.",
      "offset": 4119.2,
      "duration": 5.907
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yes, I think this is very alarming. I \nthink people should be very alarmed. The scientist  ",
      "offset": 4125.107,
      "duration": 7.213
    },
    {
      "lang": "en",
      "text": "in me wants to add a bunch of caveats to \nthe two-hour number and the doubling time,  ",
      "offset": 4132.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "whatever. But I think yes, it really \ndoesn’t seem like two years would be  ",
      "offset": 4136.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "surprising. It seems hard to rule out even \nshorter [timelines]. Is there 1% chance of  ",
      "offset": 4142.24,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "this happening in six, nine months? \nYeah, that seems pretty plausible.",
      "offset": 4148.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "With the exact shape of the recursive \nself-improvement, it still seems like we can  ",
      "offset": 4154.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "kind of see a path to significantly superhuman \nintelligence that doesn’t require a mysterious  ",
      "offset": 4160.24,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "intelligence explosion. It’s relatively clear \nhow you would apply the intermediate capabilities  ",
      "offset": 4168.72,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "that we expect to get to get something \nthat’s just obviously very pretty scary.",
      "offset": 4176.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Maybe models will continue to be really \nnon-robust in a way that’s hard to fix,  ",
      "offset": 4185.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but I don’t think we care about whether \nwe’re very confident this is going to  ",
      "offset": 4189.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "happen — we care about whether we’re \nvery confident it’s not going to happen.",
      "offset": 4195.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "It just feels like we’re at the point where \neven if you only care about current humans,  ",
      "offset": 4199.68,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "the stakes are very high. Like a 1% \nchance of takeover risk, which itself  ",
      "offset": 4207.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "has a 10% chance of human extinction \nor something, that’s a lot of deaths.",
      "offset": 4212.8,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "So I think we should really care about numbers \nthat are a whole number of percent. I mean,  ",
      "offset": 4220.96,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "I think we should care about numbers that are \nlower than that, but basically I would expect  ",
      "offset": 4228.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the world in general to agree that it’s not \nworth imposing a whole-number-percent risk of  ",
      "offset": 4233.28,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "completely destroying human civilisation \nfor getting the benefits of AI sooner.",
      "offset": 4241.04,
      "duration": 7.188
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: A little bit sooner, yeah. \nI think people in Silicon Valley,  ",
      "offset": 4248.228,
      "duration": 4.732
    },
    {
      "lang": "en",
      "text": "or certainly people in the AI industry, these \nsorts of companies, they’ve seen these results  ",
      "offset": 4252.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and they’re flipping out basically. Or their \nexpectations are what we’re seeing here:  ",
      "offset": 4257.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "they just see a very clear path to superhuman \nreasoning, to superhuman ability to improve AI,  ",
      "offset": 4263.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to therefore a software-based \nintelligence explosion. So this  ",
      "offset": 4267.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is kind of the main thing that people talk \nabout in the city that we’re in right now.",
      "offset": 4272,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "It feels like policymakers are a little bit \naware of this — certainly AI is in the policy  ",
      "offset": 4276.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "conversation a bit — but by comparison, I \nthink they haven’t yet flipped out to the  ",
      "offset": 4282.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "extent that they probably would if they fully \nappreciated what was coming down the pipeline.",
      "offset": 4286.96,
      "duration": 3.987
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right.",
      "offset": 4290.947,
      "duration": 0.573
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: One of the hopes of this show is to \ndisseminate this information so that people can  ",
      "offset": 4291.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "flip out suitably. Is there anything more \nyou want to say to someone who’s in DC,  ",
      "offset": 4296.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "who’s like, “I don’t know that I really \nbuy any of this. AI’s not my area,  ",
      "offset": 4300.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "or I feel like these tech people are \nkind of losing their heads a little bit”?",
      "offset": 4305.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Beth Barnes: It’s kind of hard for me to have that \nperspective because I’ve just been focused on AI  ",
      "offset": 4310.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "since 2017 or something, and I was previously \nthinking that it was further off, or that I  ",
      "offset": 4317.04,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "might be totally wrong about this whole thing. \nBut it feels like we’ve just increasingly gotten  ",
      "offset": 4325.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "evidence that these concerns are sensible. We’re \nstarting to see models exhibit alignment faking,  ",
      "offset": 4330,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "all these things we’re concerned about, and it \nseems like the capabilities are coming very fast.",
      "offset": 4336.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So this seems like pretty obviously the most \nimportant thing, even from a very personal,  ",
      "offset": 4340.64,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "selfish perspective: for someone who’s \nyoung and healthy, this just might be  ",
      "offset": 4347.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "your highest mortality risk in the \nnext few years is AI catastrophe.",
      "offset": 4351.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I think the other sense that people have that I \nreally want to dispel is, “But the experts must  ",
      "offset": 4358.96,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "be on top of this. The experts would be telling \nus if it really was time to freak out.” I’m like,  ",
      "offset": 4365.6,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "the experts are not on top of this. There \nare not nearly enough people. METR is super  ",
      "offset": 4372.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "starved for talent. We just can’t do a bunch \nof basic things that are obviously good to do.",
      "offset": 4376.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "And inasmuch as there are experts, they are \nsaying that this is a concerning risk. There  ",
      "offset": 4381.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "was the report that Bengio and others wrote, \nthat was sort of overall, what is the overall  ",
      "offset": 4385.2,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "scientific consensus on this risk? And it \nwas like, “Yes, this seems very plausible.”",
      "offset": 4392.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “We’re not sure whether we will \nor won’t all die, but it’s totally plausible.”",
      "offset": 4395.92,
      "duration": 5.267
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And to the extent that \nI am an expert, I am an expert telling  ",
      "offset": 4401.187,
      "duration": 2.493
    },
    {
      "lang": "en",
      "text": "you you should freak out. And there’s not \nespecially anyone else who isn’t saying this.",
      "offset": 4403.68,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, the experts would \ntell us to freak out. I’m like, well,  ",
      "offset": 4411.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the experts in this really are just the people \nat the companies, and I guess external observers  ",
      "offset": 4415.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like you who are very close to the action. There \nisn’t really a group of independent people who  ",
      "offset": 4420.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "fully understand all of this and are paying a \nlot of attention to it in DC, in some agency  ",
      "offset": 4425.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that isn’t heavily entwined here, who could \nevaluate this in some more objective measure.",
      "offset": 4429.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "So basically, all of the experts \nactually are saying to freak out;  ",
      "offset": 4434.72,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "it’s just that people are maybe sceptical \nbecause they’re in companies that are heavily  ",
      "offset": 4437.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "involved and they think maybe they don’t \nhave sufficient distance or perspective.",
      "offset": 4440.64,
      "duration": 3.587
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. I think that’s one of the \nthings where hopefully METR can provide a bunch  ",
      "offset": 4444.227,
      "duration": 4.973
    },
    {
      "lang": "en",
      "text": "of value: actually having the deep technical \nexpertise, but also being independent and  ",
      "offset": 4449.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "therefore more credible — that we don’t have \ncommercial incentives or whatever to exaggerate  ",
      "offset": 4454.96,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "this. And I do think the sort of dearth of \nexperts outside the AI labs is alarming.",
      "offset": 4462,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Let’s maybe push on a bit and tackle \na different issue. When I said on Twitter I was  ",
      "offset": 4471.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "going to be interviewing you, I was actually \na bit surprised at the main question that came  ",
      "offset": 4478,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "up. It felt very 2022 to me, but I guess maybe it \nmakes more sense given what we were just saying.",
      "offset": 4480.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "People were worried that having better \nevaluations of what the models are fully  ",
      "offset": 4486.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "capable of doing — things like realising that if \nyou put in a bunch of effort and you put in place  ",
      "offset": 4490.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the right structure for them to act as agents, \nthen they’re able to do a whole lot of AI research  ",
      "offset": 4494.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "autonomously with very little human intervention \n— they were worried that basically pointing that  ",
      "offset": 4499.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "out was dangerous; it was an information \nhazard for people to even be aware of this,  ",
      "offset": 4504.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and maybe you shouldn’t be finding that out, \nor shouldn’t be publishing it very widely.",
      "offset": 4508.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "What do you make of that concern, that \npossibly your work could backfire?",
      "offset": 4513.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Beth Barnes: This is something we’ve \ndiscussed a lot internally at METR,  ",
      "offset": 4519.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and done reasonable amounts of hand wringing \nover and talked to different people about.",
      "offset": 4523.68,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "I think my views on this have changed a fair \namount over time. The key thing is like,  ",
      "offset": 4531.92,
      "duration": 10.24
    },
    {
      "lang": "en",
      "text": "do you think of safety progress as static or as \na function of the current awareness of AI? And I  ",
      "offset": 4542.16,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "think implicitly, before, I was thinking we’re \nmaking safety progress at some rate, and then  ",
      "offset": 4549.68,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "independently there is capabilities progress, \nand it’s like a race between these two things.",
      "offset": 4556.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I think there’s a different framing, which \nis just as reasonable, which is that there  ",
      "offset": 4561.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is some hype and awareness in general \nof the current level of capabilities,  ",
      "offset": 4566.24,
      "duration": 11.44
    },
    {
      "lang": "en",
      "text": "and both the rate of investment in capabilities \nprogress and the amount of investment in safety  ",
      "offset": 4577.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "progress are a function of that. So some speeding \nup underlying capabilities is not a problem if you  ",
      "offset": 4583.52,
      "duration": 15.2
    },
    {
      "lang": "en",
      "text": "speed up the safety more as a result of that — \nthe relative progress that safety will have made  ",
      "offset": 4598.72,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "by the time you get to a level of capabilities \nis now higher, even if that time is nearer.",
      "offset": 4605.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "There’s a bunch of different ways this plays \nout. One is talent. There’s a bunch of people  ",
      "offset": 4616.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "who only started working on safety after seeing \nChatGPT or GPT-4, some level of capabilities,  ",
      "offset": 4621.44,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "like, “It’s actually time. This stuff is real, \nor actually I’m scared of this,” or whatever.",
      "offset": 4630.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "There’s policy actions, which are \nagain only going to happen once it’s  ",
      "offset": 4635.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "sufficiently obvious that this thing is \nhappening and these capabilities exist.",
      "offset": 4642.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "There’s the quality of safety research. It seems \nlike you can make much faster progress if you  ",
      "offset": 4647.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "have better models to work with. Not that we’ve \nexhausted all of the possibilities for making  ",
      "offset": 4653.12,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "progress on previous models, but there is a strong \neffect of your work is more useful when you have  ",
      "offset": 4662.16,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "better model organisms of the things that you want \nto study and you’re sort of closer to crunch time.",
      "offset": 4670.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And also you can use models to automate your work \nmore. We might imagine that almost all of the  ",
      "offset": 4676.64,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "technical safety work happens in the period right \nbefore an AI disaster would happen, because that’s  ",
      "offset": 4684.08,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "when you’ve got the models automating a \nbunch of the alignment work and things.",
      "offset": 4693.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "So the thing that we should be prioritising \nmuch more is the fraction of investment at  ",
      "offset": 4696.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "that point into safety, as opposed \nto when that point comes because  ",
      "offset": 4703.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the background rate of progress and safety \nis small now compared to what it will be.",
      "offset": 4707.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "There’s another factor here that I think is \nimportant, which is compute progress — independent  ",
      "offset": 4714.96,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "of hyper investment in AI, there is just Moore’s \nlaw and economic growth and things happening  ",
      "offset": 4723.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "in the background, which are making larger \namounts of compute cheaper and more available.",
      "offset": 4729.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And in the extreme case of this, we \ncan imagine if we know that aliens  ",
      "offset": 4736.32,
      "duration": 11.44
    },
    {
      "lang": "en",
      "text": "are going to dump this incredible infinite \nsupercomputer thing on us at some point:  ",
      "offset": 4747.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "should we do some intermediate AI research \nin the meantime that will allow us to learn  ",
      "offset": 4755.04,
      "duration": 10.32
    },
    {
      "lang": "en",
      "text": "some stuff about safety and understand \na bit what is going to be possible? Yes,  ",
      "offset": 4765.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that seems obviously good — as opposed \nto that only happening once you already  ",
      "offset": 4769.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "have this incredibly large amount of compute \nwhere you can scale everything very quickly.",
      "offset": 4776.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "In fact, there is a tradeoff where you maybe are \nbringing the point at which stuff happens forward,  ",
      "offset": 4783.04,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "because there’s more investment in hardware \nand things, but it’s still the case that in  ",
      "offset": 4791.2,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "some sense you’re racing Moore’s law. You’d rather \nhave all of the things happen when the chips are  ",
      "offset": 4798.64,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "bad and when compute is this limiting factor \nthat’s slowly going up, as opposed to like,  ",
      "offset": 4806.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it turns out anyone in their basement could \ndo this or something like that. And again,  ",
      "offset": 4811.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that’s an extreme example, but I do think in \nthe intermediate points this is still true.",
      "offset": 4815.12,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "So that’s the case for why I think it’s not \nnecessarily that bad to speed things up,  ",
      "offset": 4823.12,
      "duration": 10.64
    },
    {
      "lang": "en",
      "text": "if there’s some argument for why this \ndifferentially advantages safety.",
      "offset": 4837.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And I think there’s just a pretty \nclear argument here for why  ",
      "offset": 4842.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a better understanding of what models are \ncapable of advantages safety: by default,  ",
      "offset": 4847.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "this knowledge resides only in the labs — \nwhich are specialised in capabilities progress  ",
      "offset": 4853.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "differentially, relative to regulation and broad \nunderstanding in policy and safety and things.",
      "offset": 4859.04,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "So it seems like moving that understanding from \nbeing just in the labs — where they will happily  ",
      "offset": 4867.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "show it to their investors and people they’re \ntrying to hire — moving that understanding from  ",
      "offset": 4873.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "them to the broader public who don’t love \nall this racing ahead, or policymakers…",
      "offset": 4880.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "You could think about the timelines for \nthis, like you imagine you need to get a  ",
      "offset": 4887.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "certain level of awareness in time to get \na policy passed before you miss a window  ",
      "offset": 4892.64,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "that would have allowed you to mitigate \nthis thing in the future. So even if you  ",
      "offset": 4900.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "bring it forward a bit, if you bring \nthe policy action forward by more…",
      "offset": 4903.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Things have gotten safer \noverall. So the bottom line is,  ",
      "offset": 4907.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "I guess you can have different models of what \nis racing against what. And the simplest model  ",
      "offset": 4912.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "would be that the safety stuff is improving \non calendar time. Every year it gets better,  ",
      "offset": 4917.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so every year that you bring forward some \nadvancement in capabilities, that’s just  ",
      "offset": 4922.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "making things worse, because it’s those two \nthings that are racing against one another.",
      "offset": 4925.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "But I guess you’re saying you think about \nit in terms of the ratio of progress inside  ",
      "offset": 4929.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the companies to the amount of freaking \nout and action by other people outside  ",
      "offset": 4934.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of the companies which is able to make \nthings safer, or at least get some sort  ",
      "offset": 4939.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of governance and policy response. And \nit’s only by publishing these results,  ",
      "offset": 4943.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "so that everyone in the broader world \nunderstands what’s going on, that you  ",
      "offset": 4948.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "actually get enough attention and enough concern \nthat the problems can be solved ever at all.",
      "offset": 4950.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So the worst-case scenario is that the companies \nare fully aware that they can automate most of  ",
      "offset": 4956.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "their work. Presumably, this stuff is much less of \na shock to them than it is to the broader world,  ",
      "offset": 4961.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "because they have every incentive to try to \nfigure out how to get the absolute most out  ",
      "offset": 4966.48,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "of their models to save their own \nmoney and their own labour costs.",
      "offset": 4969.04,
      "duration": 4.147
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. I think you should think \neither we’re not making that much difference  ",
      "offset": 4973.187,
      "duration": 7.293
    },
    {
      "lang": "en",
      "text": "relative to progress inside the companies, or \nMETR is just incredibly awesome and overpowered  ",
      "offset": 4981.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and you should join — because clearly with our 10  ",
      "offset": 4986.32,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "researchers or whatever we’re beating these \nmultibillion-dollar companies or whatever.",
      "offset": 4992.96,
      "duration": 5.268
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So should we conclude that the \ncompanies probably were already aware of  ",
      "offset": 4998.228,
      "duration": 4.092
    },
    {
      "lang": "en",
      "text": "the results in your research into how good \nthe AI is at recursively self-improving?",
      "offset": 5002.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, basically that’s generally \nthe impression we’ve got. I think companies like  ",
      "offset": 5007.12,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "to sometimes give the impression that \neveryone outside is totally clueless;  ",
      "offset": 5015.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "our stuff is far superior. And we definitely have \nfound that often that’s not quite the case — our  ",
      "offset": 5020.08,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "elicitation is as good or a bit better, but \nbasically it’s not dramatic in either direction.",
      "offset": 5027.76,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "And especially for stuff like kernels, where \nit makes much more sense for them to invest  ",
      "offset": 5036,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "in that specific thing, maybe we have similar \nunderstanding — where rather than just doing a  ",
      "offset": 5039.68,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "four-week project on it and then being like, “OK \ncool, we’ll just let people know about this,” they  ",
      "offset": 5049.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "would actually double down on that and get a bunch \nbetter, and get profitable stuff out of that.",
      "offset": 5052.48,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: There presumably is some downside \nto this, where maybe the big three are aware of  ",
      "offset": 5059.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "how much they can automate things, or they’re \nroughly aware of your results already — but  ",
      "offset": 5065.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "there might be other actors, other people \nwho are working on AI or nearby where making  ",
      "offset": 5068.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "this more salient is more likely to have them \nmake an effort to automate their own research.",
      "offset": 5074.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, I think that’s true. \nI think also another concern people have  ",
      "offset": 5078.88,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "had is basically related to what we were \nsaying earlier about people irrationally  ",
      "offset": 5085.36,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "underinvesting in things that are slightly \nannoying or don’t look like the most glamorous  ",
      "offset": 5091.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "pure ML work of gathering data and making \ngood evals: that us publishing evals is  ",
      "offset": 5098.64,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "substantially accelerating companies \nbecause they can hill climb on those.",
      "offset": 5106.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So none of the things that we’re releasing \nhave enough tasks that it would make sense  ",
      "offset": 5111.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to directly train on them. It would just be \na sort of hill climbing to see how well your  ",
      "offset": 5116.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "fine-tuning was working. And again, \nit feels like if this was the case,  ",
      "offset": 5122.08,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "it would mean that we should be able to sell \nMETR’s eval producing services for very large  ",
      "offset": 5129.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "amounts of money if we are in fact substantially \naccelerating these multibillion-dollar projects.  ",
      "offset": 5134.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "But I guess the claim here is that people are \nirrationally not spending enough on evals.",
      "offset": 5140.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And in general I want us to try and be sympathetic \nto these concerns and do a kind of trade:  ",
      "offset": 5146,
      "duration": 11.36
    },
    {
      "lang": "en",
      "text": "even if we don’t buy particular concerns, \nto try and make concessions to them when  ",
      "offset": 5158.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it’s cheap. But also I think there is a \ntrap of getting bogged down in worrying  ",
      "offset": 5164.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "about how maybe this will do harm — \nwhere all the safety people kind of  ",
      "offset": 5169.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "tie themselves in knots worrying about \nthat, and then nothing actually happens.",
      "offset": 5175.2,
      "duration": 5.268
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: They can’t even communicate \nbetween themselves, or they’re not explaining  ",
      "offset": 5180.468,
      "duration": 2.652
    },
    {
      "lang": "en",
      "text": "their views to the public properly, \nor the evidence for their concerns.",
      "offset": 5183.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Beth Barnes: And there’s definitely something \nthat just feels a bit overly prideful or something  ",
      "offset": 5186,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "about assuming that your tiny team is contributing \na large fraction… You’re such a small fraction of  ",
      "offset": 5194.88,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "the total labour and investment going on in this. \nIt does seem like you have to think that you’re  ",
      "offset": 5203.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "really exceptionally talented to be thinking that \nyou would be pushing things ahead substantially.",
      "offset": 5207.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Does thinking about things this \nway suggest that you should be trying to  ",
      "offset": 5214.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "communicate all of your work as strongly \nas possible to government people — or to  ",
      "offset": 5218.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "people who are outside the tech industry — \nto really explain to them that they haven’t  ",
      "offset": 5225.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "appreciated just how dangerous things are \nor just how quickly things are moving?",
      "offset": 5229.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "And maybe the less you can publicise your results… \nI guess it’s not really imaginable that these  ",
      "offset": 5232.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "findings wouldn’t spread throughout San Francisco. \nBut basically you should be trying to do as much  ",
      "offset": 5238.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to disproportionately reach people who currently \nare not being alerted to what’s going on.",
      "offset": 5242.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I guess I don’t expect it \nto be that efficient use of labour to  ",
      "offset": 5248.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "try and communicate to one and not the other. \nEspecially as I would have thought that even  ",
      "offset": 5252.88,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "the people who are not directly involved \nin the technical world will use what was  ",
      "offset": 5260,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the general technical reaction to this as a \nbarometer of whether they should trust it.",
      "offset": 5265.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And we just want evals research, and \nunderstanding of model capabilities,  ",
      "offset": 5270.56,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "and understanding of whether mitigations are good \nto grow and proceed as a field. So we’re trying to  ",
      "offset": 5277.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "open source things and be transparent about our \nlearnings and facilitate other people getting  ",
      "offset": 5284.96,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "funding and support. We generally want to grow \nthis field, and I don’t think the payoff of trying  ",
      "offset": 5292.96,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "to be secretive about it and then just talk to \nthese particular people is worth both the effort  ",
      "offset": 5299.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "of doing that and the cost of not accelerating \nother people who could be working on this.",
      "offset": 5306.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Maybe one more point here is just that basically \nthe types of capabilities improvements METR has  ",
      "offset": 5313.12,
      "duration": 12
    },
    {
      "lang": "en",
      "text": "been doing are very much around scaffolding \nand language model agents. As we were talking  ",
      "offset": 5325.12,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "about before, it seems much better to have \na dumber model that can do a lot of stuff  ",
      "offset": 5331.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "because you’ve elicited it right and given \nit all the right tools — but then you have  ",
      "offset": 5338.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this understandable trace of what it was \ndoing — than to just build bigger models  ",
      "offset": 5343.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which mysteriously start being able to \ndo things, but you don’t really know why.",
      "offset": 5347.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "So I think differentially pushing \nforward the capabilities of  ",
      "offset": 5350.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "LLM agents seems actually maybe good.",
      "offset": 5356.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: My intuitive reaction to this concern \nwas just that if an effort that you’re trying to  ",
      "offset": 5361.44,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "make to keep us safe is just to not elicit what \nthe models are already clearly capable of doing,  ",
      "offset": 5370.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that is an incredibly thin wall \nof safety — because at any point,  ",
      "offset": 5374.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "if someone put in the effort that you guys \nhave put in to figure out how you can get  ",
      "offset": 5380.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "more out of the models, then they could \nimmediately speed things up very rapidly.",
      "offset": 5384.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "It’s an incredibly perilous situation to be \nin where the models are capable of doing far  ",
      "offset": 5388.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "more dangerous things than what people \nappreciate, or what even the operators  ",
      "offset": 5392.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "appreciate. And then as soon as someone actually \nspends a week working on it, then they can crack  ",
      "offset": 5395.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it and do it. Really we need to be alert to \nwhat is possible as soon as it is possible.",
      "offset": 5399.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. I think there’s some general \nthing in how people have different feelings about  ",
      "offset": 5405.28,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "how competent the world is, which both affects \nwhat fraction of labour does METR represent — or  ",
      "offset": 5412.8,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "would someone else do this, or maybe we’re the \nonly ones who’d have an insight or something — and  ",
      "offset": 5421.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "how useful is it to communicate to the world? \nWill policymakers react in any way that’s helpful?",
      "offset": 5425.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I can definitely see where people are \ncoming from, and I do think a bunch of  ",
      "offset": 5435.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "these concerns are quite reasonable. \nI just overall think it nets out to:  ",
      "offset": 5438.64,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "security by obscurity about what models \ncan do is not a great safety strategy.",
      "offset": 5447.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: There’s been this interesting \ndiscussion back and forth about how pessimistic  ",
      "offset": 5452.32,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "we should be about societal/government responses \nto crises and problems of all different kinds.",
      "offset": 5459.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I think the first step in the conversation \nwas more technical people saying, “Look at  ",
      "offset": 5465.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "how useless the government is at responding \nto crises. They aren’t understanding these  ",
      "offset": 5470.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "issues whatsoever. No one’s paying attention \nto it.” And then other people are pointing out  ",
      "offset": 5474.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "maybe that’s a little bit pessimistic, that the \nreason that people weren’t reacting is that they  ",
      "offset": 5478.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "felt that this was very far off; it wasn’t \nreally concrete enough to do anything yet.",
      "offset": 5481.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "In fact, if you look at COVID, perhaps we \nover-responded. The government actually was  ",
      "offset": 5485.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "extremely active once it was apparent. Stefan \nSchubert uses the term “sleepwalk bias,” where  ",
      "offset": 5489.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "the question is, do we sleepwalk into \nor through crises? He was like, no,  ",
      "offset": 5495.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "generally, actually once it’s \nevident that there’s a problem,  ",
      "offset": 5500.08,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "then you can get perhaps even an overreaction \non average among society and governments.",
      "offset": 5502.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "I’m like, if you look at the situation here with \nAI, it feels like we’re really underreacting. I  ",
      "offset": 5508.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "guess more attention has gone into it. Some stuff \nhas been done. I would say it’s barely like 10%  ",
      "offset": 5513.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "of what needs to be done, at least from my point \nof view. And I think the synthesis of these views  ",
      "offset": 5519.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is that society doesn’t sleepwalk through a \ncrisis, but it does sleepwalk into crises.",
      "offset": 5523.28,
      "duration": 4.929
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right.",
      "offset": 5528.209,
      "duration": 0.019
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because basically there’s \nonly so much focus that people have.  ",
      "offset": 5528.228,
      "duration": 3.532
    },
    {
      "lang": "en",
      "text": "People are very busy dealing with crises \nthat are already unfolding right now.  ",
      "offset": 5531.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "They don’t really have much time to \nplan ahead. And also they don’t have  ",
      "offset": 5535.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "the technical chops to understand what \nthings are likely to happen in future,  ",
      "offset": 5537.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "so they tend to just dismiss forecasts that \nsuggest anything interesting is going to change.",
      "offset": 5541.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So my expectation is basically we will \nsleepwalk into a disaster with AI — or  ",
      "offset": 5546,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "indeed almost all kinds of disasters \n— with massive underpreparation,  ",
      "offset": 5551.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "underinvestment. And then at the point that it’s \nundeniable that things are going extremely wrong,  ",
      "offset": 5554.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then you can get a very intense response and a lot \nof focus on it. And there’s a question of, is that  ",
      "offset": 5558.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "enough to actually meaningfully change anything \nwhatsoever? I guess it remains to be seen.",
      "offset": 5563.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. There’s a bit in \nDon’t Look Up where they’re trying to  ",
      "offset": 5568.24,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "convince the president to be worried about this \nincoming planet-killer asteroid. And she’s like,  ",
      "offset": 5576.48,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "“Every day I have people telling me the world’s \ngoing to end for this reason or that reason.  ",
      "offset": 5584.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "On Monday it was terrorists. On Tuesday it was \nfloods. On Wednesday it was the bees are dying.”",
      "offset": 5589.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I think people can be kind of unsympathetic to \nthat. That even if you are trying to pay attention  ",
      "offset": 5596.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "to the right things as a policymaker, everyone \nhas incentive to exaggerate the seriousness of  ",
      "offset": 5602.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "their particular cause area. And if you don’t have \nthe technical knowledge and there’s disagreement  ",
      "offset": 5607.36,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "in the technical community, it’s actually \njust quite hard to figure out how to respond.",
      "offset": 5613.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, it’s understandable \nstructurally why things play out this way,  ",
      "offset": 5619.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but it is unsatisfactory. It would be \nreally great if we could come up with  ",
      "offset": 5623.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "better systems for anticipating future problems.",
      "offset": 5626.08,
      "duration": 2.787
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And I do think also people \nmaybe have more of a picture of policy stuff  ",
      "offset": 5628.867,
      "duration": 8.253
    },
    {
      "lang": "en",
      "text": "being very bottlenecked on willingness — whereas \nI think it’s quite amenable to more concrete and  ",
      "offset": 5637.12,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "sensible suggestions, and that sort of decreases \nthe amount of willingness that you need by a lot.",
      "offset": 5646.64,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "Like SB 1047: there was this bill in California \nthat would have required some evaluations and  ",
      "offset": 5654,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "transparency about them and companies to make \nsafety cases. A significant reason for why  ",
      "offset": 5662.24,
      "duration": 13.28
    },
    {
      "lang": "en",
      "text": "people didn’t like that or why it didn’t pass — \nat least that was quoted — was some combination  ",
      "offset": 5675.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "of worrying that yes, this particular proposal is \nreasonable, but in general it’s going to spiral  ",
      "offset": 5680.24,
      "duration": 10.24
    },
    {
      "lang": "en",
      "text": "into overregulation and things will get added \non and people will interpret it creatively.",
      "offset": 5690.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And these evals don’t really exist, or people \nhaven’t done this before. I think if you’re  ",
      "offset": 5695.68,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "in a world where there was just very clear \nprecedent of, “This is what you need to do,  ",
      "offset": 5704.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this is how you make your safety case, this \nis how you decide whether it is or isn’t  ",
      "offset": 5709.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "good enough,” that would have made it much \neasier to get something like this passed,  ",
      "offset": 5714.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "because there’s less of the slippery slope \nconcern the more well defined the thing that  ",
      "offset": 5718.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you’re asking for is. And it’s just harder to \nobject with, “This might be extremely costly”  ",
      "offset": 5724.64,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "if you’re like, “No, look: someone has done \nthis already, and this is how much it costs.”",
      "offset": 5731.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So I do think people have this picture of things \nbeing very much bottlenecked on lab willingness  ",
      "offset": 5735.92,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "or policy willingness to do things, and I think \nthere’s just a bunch of stuff that third parties  ",
      "offset": 5746,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "can do that makes this much more likely to \nhappen. Even things that are just reminding  ",
      "offset": 5751.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "people and nudging people and sort of project \nmanaging things from the outside — of being like,  ",
      "offset": 5757.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "“You need to get this stakeholder. We’ll \nhelp you put together what you need to get  ",
      "offset": 5762.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this through this stakeholder.” There’s just a \nlot of value being left on the table in terms  ",
      "offset": 5767.76,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "of making it easier for policymakers and for \nlabs to adopt policies that we think are good.",
      "offset": 5774.88,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think that feels a little \nbit generous on the SB 1047 case,  ",
      "offset": 5783.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but maybe I can put to you my impression \non it, and you can tell me how wrong I am.",
      "offset": 5787.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I think what you’re saying is that there’s \nconcerns about slippery slopes, concerns about  ",
      "offset": 5791.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "costs and compliance. That would have made \nsense maybe with the original draft of it,  ",
      "offset": 5794.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "which perhaps was a little bit sloppy, and \ncould be interpreted somewhat expansively;  ",
      "offset": 5798.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "perhaps it would have been \nmore difficult to comply with.",
      "offset": 5802,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "By the time they had narrowed the scope \nand drafted it more carefully in response  ",
      "offset": 5804.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to criticism, by the end it felt like it \nreally was a bare minimum ask. It didn’t  ",
      "offset": 5808.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "really feel at all over the top to me, but it \nstill wasn’t able to get over the line despite,  ",
      "offset": 5813.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a lot of support among the general public.",
      "offset": 5818.56,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "I guess ultimately it was the decision of one \nperson, the governor of California, to veto it.  ",
      "offset": 5820.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "But the reason I would expect that it didn’t get \nover the line is just the same reason that it’s  ",
      "offset": 5825.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "generally very hard to regulate very powerful, \nvery rich, very profitable, very influential, very  ",
      "offset": 5828.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "well connected, very well prepared companies: the \ncompanies don’t want to have their hands bound.  ",
      "offset": 5834.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "They don’t want to have all of this scrutiny. \nThey campaigned very strongly to try to stop it.",
      "offset": 5838.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "And it’s unsurprising — given that they \nhad put so much effort into cultivating  ",
      "offset": 5843.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the various different political connections \ntowards funding politicians — that this kind  ",
      "offset": 5847.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of thing is very hard to get over \nthe line until it’s unambiguous  ",
      "offset": 5850.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "that there is a problem that has \nto be addressed. Am I too cynical?",
      "offset": 5853.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I don’t know. Other people \nmaybe would know more what the real  ",
      "offset": 5858.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "reasons were. My impression is that at least \nsomething that was still being quoted even  ",
      "offset": 5865.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "towards the end — so still a useful argument \nagainst it — was that this compute number of  ",
      "offset": 5871.28,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "FLOPS was not a good measure of is this thing \nactually dangerous, when this should kick in,  ",
      "offset": 5878.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and various things. So I don’t know, maybe \nthis is a smaller on-the-margin claim that it  ",
      "offset": 5884.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "makes it harder to argue against if you have good \nprecedent and clear requirements for your thing.",
      "offset": 5889.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I do think there can be other things as \nwell, where just additional labour can  ",
      "offset": 5895.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "make things go a lot better — where the \nlobbyists or government relations people  ",
      "offset": 5901.36,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "for companies can be out of touch with their \ncompany’s actual desired policies or whatever.  ",
      "offset": 5909.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Or at least there’s strong evidence that \nyou could take to them, like, “No, look:  ",
      "offset": 5919.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "the company has been asking for this type \nof regulation. It’s proposed its own version  ",
      "offset": 5926.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "of this. You should be supporting this.” \nAnd the people who are the lobbyists and  ",
      "offset": 5932.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "stuff really totally are not tracking this. \nPeople are just doing their normal thing.",
      "offset": 5937.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess the thing that they’re \nalways trying to do is avoid more scrutiny  ",
      "offset": 5942.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and more compliance cost and more regulation \nand so on. So that’s their default mode. And  ",
      "offset": 5946.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "unless there’s a very strong push inside the \ncompany to change that in this specific case,  ",
      "offset": 5949.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then that is what they will \njust naturally want to do.",
      "offset": 5953.84,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Or outside. I don’t think it \nnecessarily has to be inside. I think you can  ",
      "offset": 5955.92,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "also just communicate to people or \nmake things more salient to people.",
      "offset": 5962.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Anthropic, with the final draft \non SB 1047, was neutral to positive about it.  ",
      "offset": 5968.08,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "I think Dario Amodei, the CEO, explicitly said \nthat this would not be that difficult to comply  ",
      "offset": 5975.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "with, and that the companies that are saying they \ncould never comply with this are basically talking  ",
      "offset": 5980.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "nonsense; it actually is quite a narrow ask. So \ncredit to them for actually telling the truth.",
      "offset": 5985.12,
      "duration": 5.107
    },
    {
      "lang": "en",
      "text": "Beth Barnes: And I think xAI supported it as well.",
      "offset": 5990.227,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, of course. \nBecause Elon’s very concerned.",
      "offset": 5993.428,
      "duration": 2.092
    },
    {
      "lang": "en",
      "text": "I’ve interviewed Nick Joseph from \nAnthropic about evals. I’ve also  ",
      "offset": 5998.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "spoken with Allan Dafoe from \nGoogle DeepMind about evals.",
      "offset": 6001.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Both of them — perhaps unsurprisingly, \ngiven that they work there — said that  ",
      "offset": 6006.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "they thought their evals were pretty \ngood. That the Responsible Scaling  ",
      "offset": 6009.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Policy in Anthropic’s case or the Frontier \nSafety Framework in Google DeepMind’s case,  ",
      "offset": 6014.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that those are actually moving the needle \non safety. That these are perhaps not the  ",
      "offset": 6019.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "final form of these things, but they’re \na very good step in the right direction.",
      "offset": 6024.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "To what extent do you agree, \nwith a more independent,  ",
      "offset": 6027.76,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "outside-of-the-companies point of view?",
      "offset": 6030.08,
      "duration": 2.867
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I agree that it’s a step in the \nright direction, moving the needle. Some things  ",
      "offset": 6032.947,
      "duration": 8.413
    },
    {
      "lang": "en",
      "text": "I tend to have disagreements with. I think often \nsort of individual safety-concerned people inside  ",
      "offset": 6041.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "companies have a rosier picture of how the \ncompany interacts with third parties, like,  ",
      "offset": 6046.8,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "“We love safety, so of course we would be \nsupporting all of these things. And if it’s  ",
      "offset": 6054.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "not happening, the bottleneck must be something \nthat’s not us, because we really love safety.”",
      "offset": 6058.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I think Nick Joseph said something like this, of \nthe bottleneck on getting external oversight of  ",
      "offset": 6070.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the RSP is people who are technically competent \nto do the evaluations externally — which I very  ",
      "offset": 6076.08,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "much disagree with, because I think METR has \nthe technical competence. I think our evals  ",
      "offset": 6083.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "and elicitation and things are better \nthan stuff that the lab has published,  ",
      "offset": 6090.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "than any lab has published, basically. At least \nin terms of quality; I guess our datasets tend  ",
      "offset": 6094.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "to be smaller but higher quality. And \nwe have been eager to work with labs.",
      "offset": 6101.04,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "Also, it doesn’t necessarily need to be \nbottlenecked on the technical competence of  ",
      "offset": 6109.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the external evaluator to literally run all the \nexperiments and set up the compute and things.",
      "offset": 6114.32,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "There’s another paradigm, where the companies \nrun the evaluations internally and they write  ",
      "offset": 6124.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "up what they did, and the external evaluator \ngoes through it with them and discusses it,  ",
      "offset": 6130.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "like, “Did you provide good evidence \nfor this thing? Maybe you need to do  ",
      "offset": 6137.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "this other experiment here. This thing \nwas insufficient for this reason.”",
      "offset": 6139.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Or an embedded evaluator paradigm, \nwhere someone from an external  ",
      "offset": 6146.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "third-party org is embedded on the team in \nthe lab that’s running the evaluations, and  ",
      "offset": 6152.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is keeping an eye on everything, and making sure \nthat they’re not sandbagging or fudging things.",
      "offset": 6157.36,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "There’s just lots of things here \nwhich we’ve proposed and been open to,  ",
      "offset": 6164.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and it’s not like the companies are \nall banging down our door to do it.",
      "offset": 6167.76,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "I mean, there have also been cases where we’ve \nturned companies down because we don’t have the  ",
      "offset": 6176.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "capacity. But that is because we were like, this \narrangement will be good and would be worth our  ",
      "offset": 6182.8,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "time to do and it would take less of our time \nand capacity — but they weren’t offering that;  ",
      "offset": 6192.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "they were offering something \nelse that we didn’t think…",
      "offset": 6198,
      "duration": 1.436
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Something much more laborious for you?",
      "offset": 6199.436,
      "duration": 2.004
    },
    {
      "lang": "en",
      "text": "Beth Barnes: A combination of more labour \nintensive for us, or we just didn’t think it would  ",
      "offset": 6202.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "provide that meaningful assurance or wouldn’t \nbe improving the standard of assurance provided.",
      "offset": 6207.44,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "I think maybe people have a confusion about \nMETR, and have been assuming that METR is a  ",
      "offset": 6215.84,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "formal evaluator that has arrangements with all \nthe companies to do evaluations for every model.",
      "offset": 6224.08,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "This is not the case. I wouldn’t want to \ndescribe any of the things that we’ve done  ",
      "offset": 6231.04,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "thus far as actually providing meaningful \noversight. There’s a bunch of constraints,  ",
      "offset": 6237.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "including the stuff we were doing was \nunder NDA, so we didn’t have formal  ",
      "offset": 6242.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "authorisation to alert anyone or say \nif we thought things were concerning.",
      "offset": 6248.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "So yeah, think of it much more as we’ve been \ntrying to prototype what this could look like.  ",
      "offset": 6254.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And because we’re a small nonprofit, \nit’s easier for us to be more nimble  ",
      "offset": 6260.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and try out different things, and lower \nstakes for the labs to engage with us.  ",
      "offset": 6264.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So we’ve been excited about raising the \nbar on how good the oversight actually is.",
      "offset": 6270,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "And labs more want a “Can you promise that \nyou will run an eval on all of our models  ",
      "offset": 6276.88,
      "duration": 10.16
    },
    {
      "lang": "en",
      "text": "when we want to, so that we can say we’ve run \nan eval on them?” sort of thing. And that’s not  ",
      "offset": 6287.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "what we’re excited about, if we don’t \nthink that that proposed procedure is  ",
      "offset": 6294.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "actually providing meaningfully \nmore safety assurance than not.",
      "offset": 6299.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: In general, people think that \n— at least within the scope of what you’re  ",
      "offset": 6304.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "trying to accomplish — the evaluations that you’ve \ncreated are very good. I haven’t heard that many  ",
      "offset": 6308.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "criticisms of them on the substance. And you’re \nsaying the companies could come to you and say,  ",
      "offset": 6313.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "“We would love for you to run all these \nthings on our models before they go out,  ",
      "offset": 6319.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and for you to play with it as much as possible in \norder to figure out how dangerous they are.” But  ",
      "offset": 6322.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "they are not requesting that. They’re \nnot actively making that easy for you.",
      "offset": 6328.24,
      "duration": 4.219
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I mean, we have gotten asks like that \nfrom a bunch of labs. I think the problem is that  ",
      "offset": 6332.459,
      "duration": 4.821
    },
    {
      "lang": "en",
      "text": "our goals are like prototyping and de-risking \nwhat could provide really good safety assurance.",
      "offset": 6338,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So that includes better governance mechanisms, \nin that this actually feeds into decisions or  ",
      "offset": 6343.6,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "at least is shared with the people who are making \nthe decisions. And we have some carveout for being  ",
      "offset": 6350.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "able to say we made a recommendation and the \nlab chose to not go with that recommendation,  ",
      "offset": 6356.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "or that employees get to see what was \nshared with us so that they can check  ",
      "offset": 6362,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that it was accurate. So there’s governance \nmechanisms that we could be improving on.",
      "offset": 6365.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "There’s the pre-scaleup or pre-internal \ndeployment versus external deployment,  ",
      "offset": 6369.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "where we think this specific time of \nexternal deployment is not that interesting;  ",
      "offset": 6374.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "we’re much more interested in what are the \nbest things you have internally and wanting  ",
      "offset": 6381.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to know that companies are not building \narbitrarily powerful things internally.",
      "offset": 6386.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "There’s other stuff, just like quality \nof access and information shared about  ",
      "offset": 6391.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "what agent scaffolding has this model \nbeen trained to use and can you share  ",
      "offset": 6396.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that with us? And a bunch of stuff that is \nvery important to know to be able to elicit  ",
      "offset": 6400,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the full capabilities of models, things \nlike can we see the chain of thought?",
      "offset": 6406.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess they would say to do things \nthe way that you want is a lot of staff time,  ",
      "offset": 6411.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and they’re sprinting all the time in order to try  ",
      "offset": 6416,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to keep up. Maybe they tend to play \nthe cards very close to their chest,  ",
      "offset": 6419.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and this would involve sharing commercially \nsensitive information with you, so they’re  ",
      "offset": 6424.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "nervous about doing that or perhaps their \nlawyers at least are nervous about doing that.",
      "offset": 6428.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Are there other things that they could say in \ntheir defence that have any reasonableness to  ",
      "offset": 6432.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "them? Or to what extent would you think it’s \nfair versus maybe closer to being excuses?",
      "offset": 6436.4,
      "duration": 4.707
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, it definitely just is effort. \nIt would be a thing, and maybe there’s some ways  ",
      "offset": 6441.107,
      "duration": 10.413
    },
    {
      "lang": "en",
      "text": "that it could backfire or something. The cost \nis more attention from more senior people at  ",
      "offset": 6451.52,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "the company or something, I would guess, and \nthinking how much they want to really do.",
      "offset": 6460.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "The object-level confidentiality \nconcerns, the things we’ve proposed  ",
      "offset": 6465.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "is like sharing with us anything that’s \nnot compartmentalised within the company.  ",
      "offset": 6470.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "If your 1,000 employees already know this, \nthree people at METR knowing an anonymised  ",
      "offset": 6475.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "version of it is not really going to \nincrease the surface area very much.",
      "offset": 6482.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Or taking staff time on the inside:  ",
      "offset": 6487.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it’s a much smaller fraction of your \nstaff time than it is of METR staff time.",
      "offset": 6492.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Or spending 1% of the effort that’s spent \non the model on capabilities research on  ",
      "offset": 6497.44,
      "duration": 12.16
    },
    {
      "lang": "en",
      "text": "evaluations, or a few percent of that \neffort, would be enough to do this.",
      "offset": 6509.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "I don’t know, there’s stuff about token budgets  ",
      "offset": 6516.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and things. And OpenAI had their \nsuperalignment compute commitment,  ",
      "offset": 6521.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "which was supposed to be much larger numbers \nthan anything that we were asking for.",
      "offset": 6527.36,
      "duration": 3.813
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Twenty percent of \ntheir commitment up to that point.",
      "offset": 6531.173,
      "duration": 1.227
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right, 20% of the compute. So \nit’s not like we’re hitting those demands.",
      "offset": 6532.4,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "I think there are a bunch of things which are just \ntechnically and logistically challenging for them.  ",
      "offset": 6540,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "If we’re just like, can the API please not be \nbroken and not keep going down while we’re trying  ",
      "offset": 6546.56,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "to use it? It’s like, well, before it’s stabilised \nfor public deployment, there’ll just be issues.",
      "offset": 6553.68,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "I think in an ideal world, it would be like \nyou wait to deploy. If pre-deployment was  ",
      "offset": 6561.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "really the important thing that you needed \nto gate, you wait to deploy until you’ve  ",
      "offset": 6567.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "given your evaluator proper access, and that \nthey’ve had enough time with a stable version  ",
      "offset": 6571.92,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "of the thing that’s actually the thing that \nyou’re going to deploy, not something else.  ",
      "offset": 6578.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "You can’t directly solve the technical problem of \njust make it work, but you can solve how you’re  ",
      "offset": 6584.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "prioritising to get a good version of the evals \ndone versus get it out as quickly as possible.",
      "offset": 6588.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess a cynic would say they’re \nnot really keen on having an external group  ",
      "offset": 6595.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "be free to really heavily scrutinise the \nmodels, maybe scrutinise their plans for  ",
      "offset": 6601.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "training models that they haven’t even \ntrained yet, and be free to notify the  ",
      "offset": 6606.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "public or notify policymakers about concerns that \nthey have — because what company would want that?",
      "offset": 6609.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "This is a bunch of risk for them. It’s a bunch \nof stuff that they’re taking things somewhat  ",
      "offset": 6615.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "out of their hands. These are all the reasons \nwhy you want independent auditors and so on,  ",
      "offset": 6619.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but what company ever voluntarily accedes to that?",
      "offset": 6623.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "A more sympathetic take might be companies \nare kind of always struggling to make things  ",
      "offset": 6626.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "work. This is just another project that could \nbe challenging for them to operationalise,  ",
      "offset": 6630.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "even if they’re not that nervous about \nwhat I just described. There’s lots of  ",
      "offset": 6634.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "good things that they could do that they’re \nnot doing, and this is just one of them.",
      "offset": 6638.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "To what extent would you adopt the cynical \nview, or would that be kind of a fair baseline?",
      "offset": 6642.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I can definitely see it from \nboth sides. I think you could be a company  ",
      "offset": 6647.84,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "really prioritising safety and this could not \nmake sense as a place to invest your marginal  ",
      "offset": 6656.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "resources. I do think that is not literally \nthe tradeoff that is getting made. And I  ",
      "offset": 6663.12,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "think individuals at companies who are like, \n“Wait, surely we’re giving you that access,  ",
      "offset": 6670.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "right?” are sort of not tracking \nthe actual thing that’s happening.",
      "offset": 6675.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "But on the flip side, they’re doing \nmuch more than you might expect  ",
      "offset": 6679.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "companies just selfishly interested to be \ndoing. I think with different companies it’s  ",
      "offset": 6685.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "somewhat different reasons. Smaller ones, \nit’s somewhat more just like it’s chaotic  ",
      "offset": 6691.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and they just can’t do that many different \nthings at once, and everything’s a bit last  ",
      "offset": 6697.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "minute and the infra isn’t there. And with the \nbigger ones it’s like bureaucracy and lawyers.",
      "offset": 6701.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Decision making is slow in \ngeneral, and this is a bit of an unusual  ",
      "offset": 6708.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "arrangement. It’s not something that they \nwould do with their other products, for sure.",
      "offset": 6711.84,
      "duration": 2.707
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And again I think \nyou could paint this as the willingness  ",
      "offset": 6714.547,
      "duration": 8.013
    },
    {
      "lang": "en",
      "text": "of the labs. And I do want to disabuse \nindividuals at these labs who are like,  ",
      "offset": 6722.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "“Of course we’re doing all \nthese great things, right?”",
      "offset": 6727.36,
      "duration": 1.52
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You should actually \ngo and check, because maybe not.",
      "offset": 6728.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. But on the other hand, I do \nthink it’s where we just really haven’t pushed  ",
      "offset": 6731.68,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "that hard yet, because METR has been building \nup our technical capacities such that we can  ",
      "offset": 6739.04,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "definitely make use of the resources that we’re \nbeing given access to, and also just capacity to  ",
      "offset": 6747.84,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "run the standard lab politicking playbook, and \nmake the requests really clear, and accelerate  ",
      "offset": 6756.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "them up the chain or get support from a bunch \nof different people and then make it happen.",
      "offset": 6762.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Thus far we’ve more just been kind of \nvaguely showing up and asking nicely  ",
      "offset": 6768.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and trying to be clear about what it is we \nwant. We haven’t gone into the mode of like,  ",
      "offset": 6772.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "now we’re going to actually push on this. \nI think this has just been bottlenecked on  ",
      "offset": 6776.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "talent to be able to do that, so I’m optimistic \nabout us getting significantly better access in  ",
      "offset": 6782.24,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "the relatively near future, because I think \nthere’s stuff that you can do to push on it.",
      "offset": 6789.84,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. So you’ve kind of \nsuggested these things somewhat informally,  ",
      "offset": 6796.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "maybe a little bit last minute, \nbecause you’ve still been developing  ",
      "offset": 6803.36,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "the evals, and you’ve still been trying to \nhire to make sure that you have the people  ",
      "offset": 6805.68,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "so you can actually follow through on the \nthings that you’re asking to be able to do.",
      "offset": 6808.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "So you’ve nowhere near exhausted the options \nto try to get access earlier, and more serious  ",
      "offset": 6810.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "access. I suppose these relationships could \nalso deepen over time as trust is built,  ",
      "offset": 6815.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and it’s still perhaps at an early stage. Well, \nI guess it’s possibly at a very late stage in the  ",
      "offset": 6819.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "broader situation, but the relationship between \nyou and the companies might be at an early stage.",
      "offset": 6823.52,
      "duration": 4.867
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, we don’t want to play hardball \nif we don’t have to. If some of the fault is on  ",
      "offset": 6828.387,
      "duration": 4.493
    },
    {
      "lang": "en",
      "text": "us, we don’t want to ask people to do really \ncostly things and then be like, “Oh, actually…”",
      "offset": 6832.88,
      "duration": 13.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “We can’t do it.” Yeah, \nthat’s totally fair. A different,  ",
      "offset": 6846.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "though I guess related, topic: over the \nyears, I think many people have gone into  ",
      "offset": 6850.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "working at the various AI companies \nwith the vision that one way they’re  ",
      "offset": 6857.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "going to be helpful is shifting the average \nbeliefs of people who work at the companies.",
      "offset": 6861.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Maybe they’ll get a chance to talk \nto their colleagues and convince  ",
      "offset": 6866.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "them that concerns about how AGI might \ngo are more legitimate than they might  ",
      "offset": 6869.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think. I guess they’re also just shifting \nthe weight of opinion within the companies,  ",
      "offset": 6873.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and hopefully moving their culture in \na positive, more cautious direction.",
      "offset": 6876.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "What do you think the track record of that is? Is  ",
      "offset": 6882.32,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "that a reasonable approach for \npeople to take going forward?",
      "offset": 6884.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I basically think the track \nrecord of that particular theory of change  ",
      "offset": 6888.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "looks pretty bad. I think if you just look at \nthe history of people who tried to do that,  ",
      "offset": 6892.96,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "there’s just a lot of people giving up \nand leaving. People have started at one  ",
      "offset": 6901.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "lab trying to influence there, and then \nhave kind of given up on that and left.",
      "offset": 6906.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "I think there are probably some people who’ve \ndone this and it’s gone relatively well,  ",
      "offset": 6910.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "but I think they’re very much in the minority.",
      "offset": 6915.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I think reasons it makes sense to go to a lab \nis if you think your comparative advantage is  ",
      "offset": 6918.96,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "institutional politicking, and that’s basically \nwhat you plan to do with most of your work.  ",
      "offset": 6926,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And if you’re very good at that, I \nthink maybe you can do a better job.",
      "offset": 6932.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Another reason is if you are going to be \ndoing the implementation of safety things  ",
      "offset": 6938,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "at that company. I think if you’re just sort \nof advancing alignment research generally,  ",
      "offset": 6947.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "it’s better to be outside of a lab — because \nit’s easier to share it with everyone else and  ",
      "offset": 6953.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "collaborate, and easier for other labs to onboard \nit if it’s not this thing from a competitor.",
      "offset": 6958.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Can you elaborate on that? What sort  ",
      "offset": 6964.52,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "of work are you describing that \nwould be done better outside?",
      "offset": 6967.6,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think most alignment and  ",
      "offset": 6970.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "interpretability work would be \nbetter if it was outside labs.",
      "offset": 6974,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: That’s a big claim. I guess most \nsafety/alignment technical people work inside  ",
      "offset": 6980.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the labs now. What arguments have I heard? \nOf course they would say maybe it’s easier  ",
      "offset": 6985.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to get implemented on the company’s models if \nyou’re inside; you have access to more compute;  ",
      "offset": 6989.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you have access to the cutting-edge models, and \nyou need to have access to the very frontier  ",
      "offset": 6993.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in order for your work to be relevant. \nWhat would you say to stuff like that?",
      "offset": 6997.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think the fact that people’s \nchoices here don’t seem to be that responsive  ",
      "offset": 7001.76,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "to the quality of the open source models \navailable is maybe evidence that it’s not  ",
      "offset": 7010.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "actually driving their decisions, \nand it’s instead other factors.",
      "offset": 7016.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I just think that for a lot of \nresearch, it’s really not clear  ",
      "offset": 7021.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that you’re actually better off inside a \nlab in terms of infrastructure and model  ",
      "offset": 7025.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "access — because lab infrastructure has \na bunch of constraints due to security,  ",
      "offset": 7029.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "among other things — and it’s just actually easier \nto work with open source models, and you can get  ",
      "offset": 7036.08,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "work done faster often. And especially for open \nsource, smaller models are much more optimised.",
      "offset": 7043.52,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "So if you want to do RL, or you want to \ndo some experiments with cheaper models,  ",
      "offset": 7051.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the best ones might actually be outside \nthe companies. I feel like it’s a big  ",
      "offset": 7055.28,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "shame that Anthropic’s interpretability \nresearch was not on open source models  ",
      "offset": 7062.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that could then be shared, and people \ncould kind of poke around with it.",
      "offset": 7067.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "And in terms of just actually \npaying for clusters or something,  ",
      "offset": 7073.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "I think there’s enough funding available. I don’t \nthink that’s going to be a huge bottleneck unless  ",
      "offset": 7078.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you’re doing massive pre-training experiments. \nI think that’s not a crux in most of the cases.",
      "offset": 7083.84,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "I also think — and maybe this is less \ntrue now if we are just close to the  ",
      "offset": 7093.12,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "end times — but in the past I would \nhave said that if you think that your  ",
      "offset": 7101.76,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "research is much more productive if you \nhave access to the very latest models,  ",
      "offset": 7109.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then that also means that the research \nyou’re currently doing is just really  ",
      "offset": 7114.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "unimportant compared to the research you’ll be \ndoing in a while when the models are better.",
      "offset": 7117.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because it will all be superseded?",
      "offset": 7122.24,
      "duration": 2.867
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. Well, if it is the case that  ",
      "offset": 7125.107,
      "duration": 3.293
    },
    {
      "lang": "en",
      "text": "your research is much better with \ntoday’s models than last year’s models,  ",
      "offset": 7130.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "either there is something special going \non about this particular point in time,  ",
      "offset": 7138.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "or your research is also much less productive \nnow than it will be when you have the models a  ",
      "offset": 7142,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "year in the future, and much less productive \nagain than the models a year after that.",
      "offset": 7147.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So if you believe that it’s very important to \nhave access to the most cutting-edge models,  ",
      "offset": 7151.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then that also implies that your research \nnow is a small fraction of the total value  ",
      "offset": 7156,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that your research will provide when you have \naccess to these more advanced models. So that  ",
      "offset": 7160.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "suggests you shouldn’t really be prioritising what \nallows me to be most productive on research now.",
      "offset": 7165.92,
      "duration": 5.268
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You should be preparing \nyourself to do work in future.",
      "offset": 7171.188,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. Or prioritising buying \nas much time as possible at the point where  ",
      "offset": 7174.147,
      "duration": 4.653
    },
    {
      "lang": "en",
      "text": "you have these models that really speed up your \nresearch, which maybe points to working on evals.",
      "offset": 7178.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. OK, just to reiterate some \nof that: people definitely make the argument,  ",
      "offset": 7183.76,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "“I have to have access to the frontier \nmodel trained by the best company in  ",
      "offset": 7191.44,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "order for this to be relevant at \nall.” You had one objection there,  ",
      "offset": 7194.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "which is if all that matters is what’s done \non the frontier model, then maybe what really  ",
      "offset": 7197.28,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "matters is what’s done at the precursor model \njust before these models are truly dangerous,  ",
      "offset": 7203.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and everything now is going to be kind of \nby the by, by time we get to that stage.",
      "offset": 7207.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "But also I guess these days it seems like \nthe best open source model is virtually as  ",
      "offset": 7212.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "good as the best closed weight model, the \nones that are inside the companies — or at  ",
      "offset": 7218.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "least it’s nipping at the heels of those \nmodels, as far as we know. In that case,  ",
      "offset": 7223.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the argument that I need to have access to \nthe very best model is much, much weaker  ",
      "offset": 7228.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "than it would have been, because you could just \ndownload R1 and just run it on your own computers.",
      "offset": 7232.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And I suppose you could imagine that inside the \ncompanies you have to bid for access to compute,  ",
      "offset": 7237.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you have to coordinate with a whole \nlot of other people in getting access.",
      "offset": 7242.16,
      "duration": 4.751
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, there’s a \nbunch of security setups, which  ",
      "offset": 7246.911,
      "duration": 0.529
    },
    {
      "lang": "en",
      "text": "means you can’t easily run open source models.",
      "offset": 7247.44,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Whereas you could just buy \nyour own H100s, buy your own chips,  ",
      "offset": 7249.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "store them in your own office and \nthen download R1 — and then you can  ",
      "offset": 7253.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "just start doing whatever you want with \na tiny group that’s committed to that.",
      "offset": 7256.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "I guess that might help to explain why \nit is that groups like Redwood Research  ",
      "offset": 7259.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "have managed to really kick ass. It seems \nlike their research is remarkable. I guess  ",
      "offset": 7262.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "they collaborate with the companies, but \nprimarily they’re operating independently.",
      "offset": 7268.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And I think this \nhas been the lesson of people who’ve  ",
      "offset": 7271.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "done research inside and outside of companies:  ",
      "offset": 7274.8,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "it’s not clear that the internal access \nallows you to make research progress faster.",
      "offset": 7276.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Another argument you might \nmake is that it seems like most people  ",
      "offset": 7281.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "have tried to go inside the companies, so \nperhaps there are neglected opportunities  ",
      "offset": 7285.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "for people to not be inside the companies. \nThe weighting is really 80/20 in terms of…",
      "offset": 7289.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Beth Barnes: So, sorry, I had three \nreasons for being inside companies  ",
      "offset": 7294.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "that I didn’t finish listing. So, one, if \nyou actually really are into being a sort  ",
      "offset": 7297.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of political influencer person, and that is \nyour skill set and you’re going to do that.",
      "offset": 7301.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Then the other two I would list are you’re \nimplementing things — not doing the general  ",
      "offset": 7304.8,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "alignment research, but just making sure that \nthis actually gets into the production system  ",
      "offset": 7311.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "or otherwise being very close to the stuff \nthat’s actually happening — and being like,  ",
      "offset": 7316.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "“If the company isn’t cooperating \nwith third parties, I can at least  ",
      "offset": 7322.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "run the evals and then me and my friends \nwill know whether we should freak out.”  ",
      "offset": 7326.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Especially in worlds where things are not \ngoing very well, having some people inside  ",
      "offset": 7336.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the companies who are trying to actually \nget the basics implemented is useful.",
      "offset": 7339.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And then I think that the third one \nis sort of similar: basically being  ",
      "offset": 7345.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "the sort of person who would whistleblow if \nthere were egregious violations of things,  ",
      "offset": 7352.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and getting into a position where you \nhave awareness of that information.  ",
      "offset": 7356.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I think a lot of the actual mechanisms for \nregulation or oversight to work do require  ",
      "offset": 7364.72,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "that you trust the company to not just \ncompletely lie to you or give you totally  ",
      "offset": 7371.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "misleading information. So I think it’s very \nimportant to have some number of people internally  ",
      "offset": 7376.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "who would have the guts to whistleblow if the \nlab was feeding misinformation to regulators.",
      "offset": 7381.84,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "And I think that does in fact take a lot of guts,  ",
      "offset": 7390.24,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "as evidenced by the stuff about the OpenAI \nsecret nondisclosure agreement that no one —",
      "offset": 7397.68,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Hundreds of people put up with it. And \nit took like one person who was really stubborn to  ",
      "offset": 7404.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "dig in their heels and say, “No, I’m going to \nmake a fuss about this.” Kudos to Kokotajlo.",
      "offset": 7409.68,
      "duration": 5.267
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And Will Saunders.",
      "offset": 7414.947,
      "duration": 1.133
    },
    {
      "lang": "en",
      "text": "So I think if that’s your sort of theory of \nchange: you’ve got to be pretty confident  ",
      "offset": 7417.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that you are that sort of person. And I think \nthere is some correlation between the sort of  ",
      "offset": 7422.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "person who’s like, “The lab is offering me \nthe most money, and it’s convenient and high  ",
      "offset": 7426.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "status and cosy and comes with all the benefits; \nI guess I’ll go there,” and not being the sort of  ",
      "offset": 7432.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "person who’s actually going to whistleblow. \nAnd that there’s some kind of selection.",
      "offset": 7436.96,
      "duration": 3.748
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. They might filter against \nsomeone who seems especially whistleblowing.",
      "offset": 7440.708,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. I do think labs also \njust actively filter those people out.",
      "offset": 7444.147,
      "duration": 5.133
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I’ve maybe been surprised \nat the degree to which they’re willing  ",
      "offset": 7449.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to hire all comers, or it doesn’t \nseem like there’s really intense  ",
      "offset": 7454.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "filtering on personality type. It’s not \nlike going and working at the CIA. It  ",
      "offset": 7458.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "feels like in some ways there could \nbe more scrutiny on people’s motives.",
      "offset": 7462.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I do think there is a moderate amount  ",
      "offset": 7465.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of selection against having too \nstrong of principles or something.",
      "offset": 7470.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess that’s \ntrue in companies in general,  ",
      "offset": 7473.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that they often don’t want to \nhave firebrands or people with  ",
      "offset": 7477.68,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "a strong independent agenda. But you’re \nsaying that is also somewhat true here?",
      "offset": 7479.68,
      "duration": 5.187
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. I mean, this is \nreasonable in many ways. Often those  ",
      "offset": 7484.867,
      "duration": 3.933
    },
    {
      "lang": "en",
      "text": "people are in fact unreasonable. And there’s \na sort of unilateralist curse of people who  ",
      "offset": 7488.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "are going to be unilateralists will, \nin expectation, do unhelpful things.",
      "offset": 7494,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So actually implementing research \nor being a whistleblower are things  ",
      "offset": 7500.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that I think makes sense to be inside a lab.",
      "offset": 7505.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "I think people go there for reasons that are \nless good. One is this “I’ll influence in a  ",
      "offset": 7508.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "good direction” — which I just think has \na bad track record of effectiveness versus  ",
      "offset": 7515.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "influencing from the outside, which I \nthink has a much better track record.",
      "offset": 7518.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "And one is this “I have to have access to the \nlatest models” — where I don’t really buy that.",
      "offset": 7523.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I think naively one would expect that relatively \ntoo many people go to labs than go to nonprofits  ",
      "offset": 7533.68,
      "duration": 12.8
    },
    {
      "lang": "en",
      "text": "or governments just because it’s a nicer \njob in other ways, and they have more  ",
      "offset": 7546.48,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "experienced and aggressive recruiting departments \nand this kind of thing. They pay way more than  ",
      "offset": 7556.08,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "government and have better benefits and stuff. \nIt’s more of a high-status job in general, and you  ",
      "offset": 7562.88,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "maybe get better technical mentorship than being \nsome of the founding people at a smaller org.",
      "offset": 7572.48,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "But I also think people are sometimes wrong \nabout how much growth you get from just “go  ",
      "offset": 7580.72,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "and try to do the thing yourself” as opposed \nto being in a larger organisation with people  ",
      "offset": 7588.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "who are better at it than you — and that \nyou grow most rapidly from just doing the  ",
      "offset": 7593.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "thing. If someone’s like, “I want to go to \na bigger org to grow my leadership skills,  ",
      "offset": 7598.24,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "because I can learn from other people,” I \nthink you would grow your leadership skills  ",
      "offset": 7607.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "more by just leading this thing that’s \nright here that needs you to lead it.",
      "offset": 7610.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So I don’t know, I think there’s some \namounts of motivated reasoning. In general,  ",
      "offset": 7617.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "unless you have a particular reason to think that \nyou would be a particularly good fit at a lab and  ",
      "offset": 7624.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "not a good fit at a nonprofit or government, \nI think you should probably not go to a lab.",
      "offset": 7628.56,
      "duration": 5.988
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. Just as an aside, it’s \ninteresting that you call them “labs.” I  ",
      "offset": 7634.548,
      "duration": 4.892
    },
    {
      "lang": "en",
      "text": "switched to calling them companies at some \npoint because someone pointed out or just  ",
      "offset": 7639.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "asked the question: “These organisations, \ndo you think their primary nature is as a  ",
      "offset": 7643.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "research laboratory or as a company?” \n— and I was like, obviously it’s as a  ",
      "offset": 7647.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "company. Maybe at some point in the past \nthey were primarily research laboratories,  ",
      "offset": 7651.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "or that was the right way to think of them, \nbut it feels a little bit hard to see that now.",
      "offset": 7656.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. I don’t know. This is not  ",
      "offset": 7660.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "an assessment of… I agree they are \ncompanies. It’s just fewer syllables.",
      "offset": 7664.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I do think it does make sense to have \nat least a few people who really care  ",
      "offset": 7672.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "at each lab who can be implementing the basic \nmitigations, can be running the basic evals,  ",
      "offset": 7677.52,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "can be keeping an eye out \nfor egregious misconduct.",
      "offset": 7684.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "I just don’t think that everyone piling into the \nlabs, if you’re just generally doing public good  ",
      "offset": 7688.8,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "safety research, makes sense. Or for some \ngeneric “I’ll just be there and influence  ",
      "offset": 7695.68,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "them in a good direction.” I feel like I \nsee this a lot from people, and it’s like,  ",
      "offset": 7702.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "“But you’re a really introverted, \ntechnical person. And in fact,  ",
      "offset": 7708.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "in practice you just spend like eight hours \na day coding. It clearly doesn’t make sense  ",
      "offset": 7711.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "as a specialisation for you to be like, \n‘I’ll be here and influence things.'”",
      "offset": 7716.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You told me earlier this \nweek that METR’s strategy has over time  ",
      "offset": 7721.76,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "changed to adapt to what is \na somewhat more pessimistic  ",
      "offset": 7729.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "or challenging situation. Can you \nelaborate on what you mean by that?",
      "offset": 7733.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think this is more relatively \nrecently feeling like timelines seem short  ",
      "offset": 7739.04,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "and we’re not in some of the best worlds in terms \nof how much is getting done on the safety front.",
      "offset": 7747.36,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "I think originally when we were designing what \nwould become RSPs, or responsible scaling policies  ",
      "offset": 7756.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and thinking about what evaluation regimes would \nmake sense, we were imagining something like you  ",
      "offset": 7762.48,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "want to keep the overall risk of catastrophe \n/ end of civilisation from AI below 1%. And if  ",
      "offset": 7769.84,
      "duration": 13.2
    },
    {
      "lang": "en",
      "text": "there’s several labs, they each need to be a bit \nbelow 1%, and any particular deployment probably  ",
      "offset": 7783.04,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "needs to be below 0.1% because you’re \ngoing to get multiple shots, et cetera.",
      "offset": 7790,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "This just seems like what is reasonable \nfrom a societal perspective. Personally,  ",
      "offset": 7795.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I care about future people also, so I think \nwe should be aiming for something lower than  ",
      "offset": 7802.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that. But I think even just from the \nperspective of people alive today,  ",
      "offset": 7808.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the benefits of getting AI sooner aren’t \ngood enough to justify much higher risks.",
      "offset": 7814.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "But I think we’re increasingly in the world \nwhere it doesn’t seem like we’re going to  ",
      "offset": 7819.04,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "be keeping the risk that low. We can describe \nwhat you would need to do to be in that regime,  ",
      "offset": 7825.68,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "and I don’t think that we’re doing \nit, that the world is doing it.",
      "offset": 7834.8,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "And if we’re pushing on things like,  ",
      "offset": 7841.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "how would you make a really robust safety \ncase that would have good external oversight,  ",
      "offset": 7847.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "maybe this is in fact a distraction from can we \njust do the most basic mitigations? And maybe  ",
      "offset": 7852,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "we don’t even have time to check if they’re \nworking, but let’s at least do them. Getting  ",
      "offset": 7857.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the risk from 30% to 20% or something like that \nmaybe is more what we should be focusing on.",
      "offset": 7864.08,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So the basic difference \nis you could spend a whole lot of time  ",
      "offset": 7870.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "laying out what would be absolutely ideal \npractice that would get the risk down to  ",
      "offset": 7876.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "0.1% or lower in aggregate. But if we’re not \neven doing the absolutely most basic stuff  ",
      "offset": 7879.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "already — the stuff that would bring it from 30% \nto 20% — then obviously you would focus there,  ",
      "offset": 7884.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because the impact is way larger. And I \nsuppose those are also the marginal changes.",
      "offset": 7888.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think “ideal practice” even is too \nstrong. I feel like the ideal thing looks way more  ",
      "offset": 7892.48,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "intense, and this is just the sort of bare minimum \nif society as a whole was making reasonable  ",
      "offset": 7899.28,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "decisions that I think would be roughly what \neveryone endorses. But going from that — which  ",
      "offset": 7908.32,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "looks like being quite careful — to just trying \nto, on the margin, get the risk down a bit.",
      "offset": 7914.8,
      "duration": 7.108
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. So you think the risk is \nlike 30% on the current track that we’re on?",
      "offset": 7921.908,
      "duration": 8.252
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I usually say something like \n50/50 in terms of do things broadly go well.  ",
      "offset": 7930.16,
      "duration": 12.48
    },
    {
      "lang": "en",
      "text": "Do we capture most of the value of the future? \nSo not all of the 50% looks like obviously a  ",
      "offset": 7943.6,
      "duration": 11.28
    },
    {
      "lang": "en",
      "text": "disaster for currently alive people or something. \nIt might be some lock-in or more gradually going  ",
      "offset": 7954.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "off the rails or something like that. But I \nthink a bunch of that is just like war and  ",
      "offset": 7960.32,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "catastrophe and AI is taking over — and it’s very \nclearly “everyone agrees this is bad” territory.",
      "offset": 7969.2,
      "duration": 6.308
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess there are a handful of people \nwho like that, but it’s a distinct minority.",
      "offset": 7975.508,
      "duration": 4.652
    },
    {
      "lang": "en",
      "text": "So starting from that picture where \nalmost not even the basics are being done,  ",
      "offset": 7981.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "what is the marginal thing? \nWhat would be your next ask,  ",
      "offset": 7986.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "given the situation that we’re realistically \nlikely to be in in a couple of years’ time?",
      "offset": 7989.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Beth Barnes: There was a thing we were talking \nabout earlier of chain of thought faithfulness,  ",
      "offset": 7993.36,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "and actually looking at the chain of thought — \nso just doing basic checks for alignment of like,  ",
      "offset": 8000.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "have we looked at a reasonable sample \nof the reasoning the model’s been doing,  ",
      "offset": 8005.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and can we understand this, and is it blatantly \nreasoning about how to trick us or do bad things  ",
      "offset": 8010,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "or something? That’s one very basic thing I \ncan imagine is just something we neglect to do.",
      "offset": 8018.4,
      "duration": 11.04
    },
    {
      "lang": "en",
      "text": "Another intervention would be making sure that \nwe keep the chain of thought interpretable. Which  ",
      "offset": 8029.44,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "might be like companies might find some fancy \nnew way to get more performance out of the model,  ",
      "offset": 8036.08,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "but it now means that it’s just like neuralese \nand humans can’t read it at all. That would be  ",
      "offset": 8042.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "very sad. So if on the margin there are \nsome techniques that we can do to either  ",
      "offset": 8047.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "ensure that we can interpret that, or to \nbring the performance of the interpretable  ",
      "offset": 8052.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "thing up so it’s less of a hit relative to the \nuninterpretable thing, that would be pretty good.",
      "offset": 8057.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “Neuralese” is this term for \ngibberish reasoning by the model that I guess  ",
      "offset": 8062.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is useful for it to reach a conclusion, \nbut humans can’t make any sense of it.",
      "offset": 8066.96,
      "duration": 4.067
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, or this could also be \nlike it’s just passing numbers to itself.  ",
      "offset": 8071.027,
      "duration": 5.773
    },
    {
      "lang": "en",
      "text": "It’s not even outputting tokens \nthat look like words or anything.",
      "offset": 8076.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So yeah, it feels like we’re in a \nmuch better position than we might  ",
      "offset": 8082.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "have hoped to be in, in terms of having \nmodels that do a lot of their cognition  ",
      "offset": 8087.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "through legible reasoning. But people are \nworking hard on making that not the case.  ",
      "offset": 8091.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "That’s one thing that feels like it just gets \na big increment on risk: how you handle that.",
      "offset": 8098.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "I think there’s just a lot of other very \nbasic risk of misalignment incurring  ",
      "offset": 8106.48,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "stuff. Some combination of what is your \ntraining data like, and how much are you  ",
      "offset": 8115.76,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "incentivising the model to knowingly lie \nand reason about what the humans want,  ",
      "offset": 8122.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and reason about playing the training game, and \nhow much benefit is it getting from doing that?",
      "offset": 8128.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I was just about to ask, is \nit actually imaginable that we could be  ",
      "offset": 8132.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "so stupid as to just look at the chain \nof thought, find out when it’s scheming,  ",
      "offset": 8135.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and then just negatively reward that and \ntry to discourage the visible scheming?",
      "offset": 8139.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I guess people don’t realise the trouble \nwith that is that it could just cause the  ",
      "offset": 8144.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model to become cleverer about hiding its \nscheming. It doesn’t necessarily actually  ",
      "offset": 8148.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "reduce the scheming. In fact, probably it \nwould cause it to become more surreptitious.  ",
      "offset": 8151.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So that’s actually a negative thing \nto do, rather than a positive one.",
      "offset": 8154.96,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right, yes. You know, \nback in the olden days of AI safety,  ",
      "offset": 8157.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "people used to worry about having \na sudden jump in intelligence:  ",
      "offset": 8163.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that models would go right through this regime \nwhere you have models that sort of scheme,  ",
      "offset": 8166.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "but they’re not smart enough to have it not be \ndetectable for us. But now I feel more confident  ",
      "offset": 8173.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "that we’re already seeing models that can do \nthis, so we’re going to have some period of  ",
      "offset": 8180,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "time where the models are smart enough to \ndo this, but not smart enough to hide it.",
      "offset": 8183.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "But the thing that I worry about is that we’re \ngoing to see signs of egregious misalignment  ",
      "offset": 8187.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and then we’re going to be like, “Maybe \nthat was just noise. Let’s train it not  ",
      "offset": 8193.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to do that and slap some patches on it” — \nso that we totally will have had evidence,  ",
      "offset": 8198,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and it’ll just be ambiguous enough once \nyou try some superficial fixes to make  ",
      "offset": 8203.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it stop doing it, that we won’t really \ndo things differently based on that.",
      "offset": 8209.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: At a high level, overall,  ",
      "offset": 8215.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "how responsible versus irresponsible would \nyou say the main AI companies are being?",
      "offset": 8218.399,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Beth Barnes: The tricky question in some \nsense is, what is your reference point?  ",
      "offset": 8224.72,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "From the perspective of what I would think \nhumanity in general would sort of endorse if they  ",
      "offset": 8235.359,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "knew all the things, and what a sort of reasonable \nsociety that could actually coordinate would do,  ",
      "offset": 8243.439,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "I think they’re being extremely irresponsible. \nAnd from my personal perspective, even more so,  ",
      "offset": 8250.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "because I value future people maybe \nmore than the average person does.",
      "offset": 8256.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "But on the other hand, if you look at the \nindividual incentive structures around the  ",
      "offset": 8263.12,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "companies, all these companies have a bunch \nof really good people who are working really  ",
      "offset": 8270.16,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "hard trying to make things go well and within \nunderstandable constraints. I think there’s  ",
      "offset": 8276.479,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "a bunch of arguments under which it does \nmake sense to trade off safety for going  ",
      "offset": 8284,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "faster — if you’re worried about misuse by \nauthoritarian regimes or that sort of thing,  ",
      "offset": 8288.96,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "or if we’re going to dedicate \nmore effort to alignment.",
      "offset": 8298.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "I think locally people are pretty reasonable,  ",
      "offset": 8304.479,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "or at least a lot of people are reasonable \na lot of the time. I think people are  ",
      "offset": 8312.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "also definitely unreasonable. But \noverall the situation is very bad.",
      "offset": 8315.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It sounds like you’re saying \nthat’s primarily because of the lack of  ",
      "offset": 8319.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "coordination? That if you just only had one \norganisation that was thinking about this  ",
      "offset": 8322.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "from the perspective of just all people alive now, \nthen probably they would go a lot more cautiously,  ",
      "offset": 8326.96,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and they would invest more in these kinds \nof evals to figure out what’s going wrong?",
      "offset": 8332.399,
      "duration": 5.107
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, I think it’s \npartly the coordination thing.  ",
      "offset": 8337.507,
      "duration": 3.453
    },
    {
      "lang": "en",
      "text": "And it’s also just that humans are bad \nat dealing with uncertainty and with  ",
      "offset": 8340.96,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "probabilities that are smaller \nthan about a half or something.",
      "offset": 8348.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "When I was at OpenAI, there were a lot of \npeople who, if you asked them the right series  ",
      "offset": 8355.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "of questions, would say that the probability \nthat the thing they were working on would lead  ",
      "offset": 8360.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to killing all humans or something, you’d get like \n10% or 20%. But it just wasn’t really connecting.",
      "offset": 8364.96,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "And I think if you actually want to \nkeep the risk below 1%, or even lower,  ",
      "offset": 8374.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it looks like being very cautious in some \nsense, because our uncertainty around  ",
      "offset": 8379.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "possible threat models and around the error \nbars on our capability assessments and all  ",
      "offset": 8384.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "of these things are so large that being 99% \nconfident that this is fine is a high bar.",
      "offset": 8390.399,
      "duration": 8.721
    },
    {
      "lang": "en",
      "text": "I think model releases now are \ngetting to that level, and it’s like,  ",
      "offset": 8400.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "we’re very confident that it’s fine — but very \nconfident, maybe that’s like 95% confident that  ",
      "offset": 8406.399,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "it’s not going to do anything super bad. \nAnd maybe it’s like 99% or 98% confident  ",
      "offset": 8413.439,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that it’s not going to be catastrophic, \nbut it’s pretty hard to get that low.",
      "offset": 8418.16,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So the issue there is, if you \nwant to get to really low levels of risk,  ",
      "offset": 8425.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "99.9% or 99.99% confident, then you have to \nbe really sure that there’s nothing outside  ",
      "offset": 8429.12,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "of your understanding that’s going on. Because \neven from your perspective, given your dominant  ",
      "offset": 8436.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "understanding of how the models are working and \nwhat they are and aren’t doing, things might  ",
      "offset": 8440.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "seem 99% safe, but there’s always a possibility \nthat you’re mismeasuring things, that your evals  ",
      "offset": 8444.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "aren’t capturing what’s going on, maybe they are \nfiguring out how to understate their capabilities.",
      "offset": 8448.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "And it just makes it exceedingly difficult \nto really bring risk that low when you  ",
      "offset": 8453.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have such an unclear picture of \nwhat’s actually happening at all.",
      "offset": 8457.359,
      "duration": 2.708
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And again, \nI’m not really thinking about  ",
      "offset": 8460.067,
      "duration": 3.693
    },
    {
      "lang": "en",
      "text": "the 99.99%. I’m more thinking about can we get —",
      "offset": 8463.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: From 10% to 1%?",
      "offset": 8467.2,
      "duration": 1.52
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Something like that. And again, it’s \nnot like we have a pretty detailed understanding,  ",
      "offset": 8468.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "but maybe we’re wrong — we just have gigantic \nerror bars, and we really have no clue.",
      "offset": 8475.28,
      "duration": 10
    },
    {
      "lang": "en",
      "text": "And the companies have described themselves \nand their safety process as like building  ",
      "offset": 8485.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the aeroplane while flying it or whatever. \nIt’s not like, “We’re being so cautious,  ",
      "offset": 8489.28,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "but we’re trying to reach this really \nhigh burden.” It’s just like, “Yeah,  ",
      "offset": 8495.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "everything is a complete mess, and we’ve \ngot to work pretty hard to keep the risk  ",
      "offset": 8498.72,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "to what would be an at all reasonable level. \nAnd by default it’s going to be way higher.”",
      "offset": 8505.84,
      "duration": 4.547
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, pushing it from that. People \nare working on all kinds of different research  ",
      "offset": 8510.387,
      "duration": 4.092
    },
    {
      "lang": "en",
      "text": "agendas around safety, control, alignment. Are \nthere any of those that you want to shout out as  ",
      "offset": 8514.479,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "particularly underrated or particularly overrated? \nMaybe things that people are overinvesting in? I  ",
      "offset": 8520.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "guess a worry would be everyone herds into some \nparticular research agenda that’s fashionable,  ",
      "offset": 8525.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but it’s actually not really going \nto move the needle in the end,  ",
      "offset": 8530.08,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "and that could end up being a huge \ndiversion of people’s attention.",
      "offset": 8532.56,
      "duration": 3.427
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. I think there’s \nboth a question about what is most  ",
      "offset": 8535.987,
      "duration": 3.293
    },
    {
      "lang": "en",
      "text": "important to work on and where or in what way.",
      "offset": 8539.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So the things that I’m most excited about people \nworking on are a very concrete implementation  ",
      "offset": 8544,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "of at least getting the best practices we’re \naware of thus far into production models,  ",
      "offset": 8552.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and trying to maintain that as an invariant:  ",
      "offset": 8556.88,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "that we’re always getting the best alignment \ntechniques there. And there’s generating  ",
      "offset": 8559.359,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "new directions or creating model organisms \nto study or stuff that’s more like research.",
      "offset": 8564.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "One thing is I think it would be better if \nway more of the public good research was  ",
      "offset": 8572.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "happening outside labs. I think Redwood \nResearch, despite being two people,  ",
      "offset": 8577.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "has really high output compared \nto the entire rest of the field,  ",
      "offset": 8583.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and all of the people at labs. And \nthe stuff they’ve done is really good.",
      "offset": 8588.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Control seems good, alignment seems \ngood. Generally understanding scaling  ",
      "offset": 8595.12,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "laws and forecasting and eval stuff seems good.",
      "offset": 8603.2,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "I think one thing that’s maybe, in my opinion, \nsomewhat overrated is interpretability. Not that  ",
      "offset": 8605.52,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "this is not useful — I think there’s a bunch \nof ways it’s promising and good — but it does  ",
      "offset": 8613.359,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "have very high investment currently and I’m not \nsure it deserves as high investment as it has.",
      "offset": 8617.68,
      "duration": 7.428
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, so that’s one approach that \nyou think we’re a bit overinvested in. Is  ",
      "offset": 8625.108,
      "duration": 2.972
    },
    {
      "lang": "en",
      "text": "there another one that you think is more \npromising than people have appreciated?",
      "offset": 8628.08,
      "duration": 2.947
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, there’s some general direction \nof just getting the capabilities that you want and  ",
      "offset": 8631.027,
      "duration": 9.853
    },
    {
      "lang": "en",
      "text": "not the ones that you don’t want. Currently \nI feel like how labs are going to react  ",
      "offset": 8640.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to evals is they’re just going to build more \ncapable models, and then it’s going to be like,  ",
      "offset": 8645.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "now it’s more capable and it’s maybe \ngoing to trigger this thing, and then —",
      "offset": 8649.12,
      "duration": 3.588
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “What a shame.”",
      "offset": 8652.708,
      "duration": 0.959
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, maybe they’re going to fudge \nthe evals, or maybe they’re actually going to do  ",
      "offset": 8653.667,
      "duration": 3.293
    },
    {
      "lang": "en",
      "text": "some of the mitigations, but I think we \ncould be trying much harder to be like,  ",
      "offset": 8656.96,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "what is the reason for rushing \nahead in building these models?",
      "offset": 8666.72,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "The argument is like, “Then we can use them \nto do safe things and protect against these  ",
      "offset": 8671.359,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "other models.” Like almost all of the companies \nthat are like, “Building AI is important,” it’s  ",
      "offset": 8677.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "like, ” — because someone else might build \nit, and we’ve got to protect against it.”",
      "offset": 8683.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "But I feel like this is very disconnected. \nI would rather the world was like, “Maybe  ",
      "offset": 8687.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "some people are going to build dangerous \nAI. How would we protect against that?”",
      "offset": 8692.96,
      "duration": 3.651
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “What would be the minimally \ncapable AI that would be really helpful?”",
      "offset": 8696.611,
      "duration": 2.269
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. Or maybe the most important \nthing to do isn’t even AI. It’s not super clear  ",
      "offset": 8698.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that that’s what you need to do anyway. I \nmean, I think there are reasonable arguments  ",
      "offset": 8703.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "for how we have a bunch of uncertainty and \nyou just want to have a generally capable  ",
      "offset": 8708.479,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "thing. But I just feel that people aren’t \ntrying at all to be like, “How could we get  ",
      "offset": 8712.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the capabilities that are most useful, and \nwhich other capabilities can we knock out?”",
      "offset": 8717.28,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "And some of the very basic control stuff of \nhow you can be more confident your model is  ",
      "offset": 8721.52,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "not capable of taking over if there’s a tonne \nof stuff about the world that it doesn’t know.  ",
      "offset": 8729.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Maybe it’s really good at coding and stuff, \nbut it just doesn’t know anything about world  ",
      "offset": 8736.479,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "events past 2010, and you really tried to \nremove everything about computer security  ",
      "offset": 8742.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "and something like that — where it’s like, \nthis model couldn’t really do stuff by itself;  ",
      "offset": 8749.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it only works in this lab setup where we \ngive it all the right things or something.",
      "offset": 8754,
      "duration": 4.548
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. I guess the case where I’ve \nheard this discussed the most is trying to not  ",
      "offset": 8758.548,
      "duration": 4.572
    },
    {
      "lang": "en",
      "text": "include cutting-edge biology –in particular, \ncutting-edge virology or microbiology — in  ",
      "offset": 8763.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the training data, because that’s the kind of \nthing that would allow it to design a bioweapon.",
      "offset": 8769.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "And it’s a fairly clear point to make: why \ndoes the model need to know all of this  ",
      "offset": 8773.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "incredibly cutting-edge biology, unless it’s \nbeing used by virologists? And in that case,  ",
      "offset": 8778.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "why not just design a very special model \nfor them that only they have access to?  ",
      "offset": 8783.359,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Why does the model that is going out to \nretail customers like us have to know  ",
      "offset": 8786.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the most dangerous things basically \nknown to humanity up to this point?",
      "offset": 8790.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "And you’re saying this could \ngeneralise quite a bit more,  ",
      "offset": 8795.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "where you could hobble the models by ensuring \nthat they’re lacking specific capabilities by  ",
      "offset": 8798,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "simply never training them in it, specific \nthings that would be useful for them if they  ",
      "offset": 8802.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "were going to try to scheme against us or \nengage in activities that we don’t like.",
      "offset": 8806.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "I guess I haven’t really heard that \napproach discussed very much outside  ",
      "offset": 8810.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of the biology context. I guess cyber \nstuff would also be another classic  ",
      "offset": 8813.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "case where you could maybe use this \n— although they do seem to need to  ",
      "offset": 8818.479,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "know a lot of coding for many commercial \napplications, so perhaps that’s trickier.",
      "offset": 8821.12,
      "duration": 4.306
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. I think it’s easier for the \nnarrower things, but I do think if you need it for  ",
      "offset": 8825.426,
      "duration": 6.093
    },
    {
      "lang": "en",
      "text": "some applications, you just don’t need to serve \nthat capability to everybody all of the time.",
      "offset": 8831.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And with the bio stuff, yeah, you might still \nwant to support synthetic biology research,  ",
      "offset": 8836.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but that’s like an extra training \nphase that you add on at the end,  ",
      "offset": 8840.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and then you serve that model only to a very \ncarefully selected group of people. So I think  ",
      "offset": 8844,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "even if you need the capabilities sometimes, \nthat the frontier of which capabilities are  ",
      "offset": 8851.439,
      "duration": 8.641
    },
    {
      "lang": "en",
      "text": "how good that we’re targeting, we’re really not \ntrying on this in a bunch of ways at the moment.",
      "offset": 8860.08,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I do remember hearing people push \nback and say the trouble with just excluding  ",
      "offset": 8866.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "some information is that at the point where they \nwere producing even more capable models than  ",
      "offset": 8871.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we have now, they’d be able to interpolate \nand guess lots of stuff that you left out.",
      "offset": 8876.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Like if you included everything other than the \nnews in 2013, then probably it could just guess  ",
      "offset": 8880.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "what happened in 2013 by knowing what happened \nlater. And I suppose even if you didn’t include  ",
      "offset": 8885.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "anything after 2013, it might be able to guess \na fair number of things that happened simply by  ",
      "offset": 8889.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the information that it has up to that date. To \nwhat extent is that a serious limiting factor?",
      "offset": 8893.359,
      "duration": 6.461
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, I think \nthis isn’t a super robust  ",
      "offset": 8899.821,
      "duration": 2.658
    },
    {
      "lang": "en",
      "text": "intervention. This is sort \nof a, “Let’s try to get — ”",
      "offset": 8902.479,
      "duration": 2.935
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Here we’re trying \nto get the risk from 50% to 40%.",
      "offset": 8905.414,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, those sorts of worlds. I do \nthink there just might be more technical work to  ",
      "offset": 8907.735,
      "duration": 5.305
    },
    {
      "lang": "en",
      "text": "do here on how you can actually remove knowledge \nor capabilities from models, or figuring out how  ",
      "offset": 8913.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "much they can infer. Because I think you \nprobably can assess this reasonably well.",
      "offset": 8918.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Even if you have some concern about sandbagging,  ",
      "offset": 8925.2,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "if you’re just trying to figure out does \nthe model have this basic knowledge…  ",
      "offset": 8926.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "There’s some level of sandbagging to do it \nwith RL and there’s a much higher level of  ",
      "offset": 8936.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "intelligence that’s demanded to do sandbagging \nif you’re just doing classification fine-tuning.",
      "offset": 8940.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So if the model does in fact know the answer to \na bunch of questions about this, you should be  ",
      "offset": 8946.08,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "able to see that, with fine-tuning, it picks this \nup really quickly — so the answers must have been  ",
      "offset": 8951.439,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "kind of already in there. So I think you can \nkind of tell what models know in this sense.",
      "offset": 8956.72,
      "duration": 5.188
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: By how quickly \nthey later learn how to…?",
      "offset": 8961.908,
      "duration": 2.732
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And also, as we were talking \nabout before, if you don’t actually have great  ",
      "offset": 8964.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "security, what you do want to know is, “Is \nit easy to fine-tune the model to have these  ",
      "offset": 8969.2,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "capabilities?” — not, “Does it demonstrate \nthem right now as these exact weights?”",
      "offset": 8972.64,
      "duration": 5.108
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. Just to clarify for \npeople who don’t know: sandbagging is  ",
      "offset": 8977.748,
      "duration": 3.691
    },
    {
      "lang": "en",
      "text": "models pretending to not be able to do \nthings that they actually are able to do,  ",
      "offset": 8981.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "or basically kind of lowballing what you \nactually are able to do. It’s a sport term,  ",
      "offset": 8985.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "I think, from people who are \nthrowing a match, basically.",
      "offset": 8991.92,
      "duration": 3.347
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah.",
      "offset": 8995.267,
      "duration": 0.493
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Wasn’t there some work on forgetting?  ",
      "offset": 8995.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Wasn’t there some effort to try to get a model \nto forget the existence of Harry Potter? I think  ",
      "offset": 8999.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "they managed to kind of successfully \ndo that, or forget something or other.",
      "offset": 9003.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I’m not familiar with that \nexact thing. Sounds plausible. I think  ",
      "offset": 9007.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "people have done some on this, and call it \n“unlearning.” I just haven’t seen that much  ",
      "offset": 9011.52,
      "duration": 9.839
    },
    {
      "lang": "en",
      "text": "on it. And in particular, I haven’t \nseen any kind of application of it.",
      "offset": 9021.359,
      "duration": 3.428
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, I think I haven’t heard \nabout that in almost a year. Which is a  ",
      "offset": 9024.788,
      "duration": 3.052
    },
    {
      "lang": "en",
      "text": "bit of a shame. Is there anything else you \nwant to call out as potentially underrated?",
      "offset": 9027.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I’m sad that there’s not more work on \nhelping humans evaluate difficult model outputs,  ",
      "offset": 9032.16,
      "duration": 13.92
    },
    {
      "lang": "en",
      "text": "basically the stuff I used to work on, on \ndebate and IDA [iterated distillation and  ",
      "offset": 9046.08,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "amplification] — this whole genre of alignment \nresearch is like, how do you make sure  ",
      "offset": 9052.319,
      "duration": 9.921
    },
    {
      "lang": "en",
      "text": "that your model is not doing destructive \nthings when it is smarter than you are?  ",
      "offset": 9062.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "And how can you create structures that \nprovide a systematic advantage to the rater?",
      "offset": 9065.76,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "Debate is one of these, where you have copies of \nthe model and they’re debating with each other.  ",
      "offset": 9072.479,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "You have some specific subquestion that you can \njudge, and you can judge how each subquestion  ",
      "offset": 9081.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "relates to the question above. And this is \nmuch easier than deciding the overall answer.",
      "offset": 9086.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "I think there could be a lot more stuff \njust helping humans more quickly make  ",
      "offset": 9091.52,
      "duration": 9.601
    },
    {
      "lang": "en",
      "text": "good judgments about, was this \nactually the behaviour you wanted?  ",
      "offset": 9102.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Was this actually seeking \npower in some undesirable way?",
      "offset": 9106.72,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "I think the more faithful your training \nsignal is to things like truth and honesty,  ",
      "offset": 9113.84,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "the less incentive you are putting \non the model to reason about the  ",
      "offset": 9122.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "training process and the flaws in \nthe training processes and things.",
      "offset": 9126,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "And yet the more you have these \ntechniques that kind of amplify  ",
      "offset": 9129.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "what humans are able to evaluate, the \nmore you might be able to check. Like,  ",
      "offset": 9133.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "if these models, they’re running all \nof our security against these rogue,  ",
      "offset": 9136.64,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "unsafe, bad models that someone else has \ndeveloped. We have no idea what’s going on.  ",
      "offset": 9145.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "We don’t know whether our models are actually \nsecretly cooperating with the rogue models.",
      "offset": 9151.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "The better you can be like, “We have this \nscheme to have two models adversarially  ",
      "offset": 9156.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "evaluate each other and point out \nthe things that they’re doing that  ",
      "offset": 9160.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are most suspicious and then evaluate \nthose,” the better we are at that,  ",
      "offset": 9165.28,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "the more able we are to train for, “No, \nactually, do the thing that I want.”",
      "offset": 9170.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You said something \na little bit spicy earlier,  ",
      "offset": 9176.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "which is that you think possibly Redwood, \nwith just a handful of people, is maybe  ",
      "offset": 9180.319,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "producing about as much alignment research \nas all of the major companies put together.  ",
      "offset": 9183.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I assume that’s not actually literally true, \nbut… Maybe you do think it’s literally true?",
      "offset": 9188.08,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "What would the barriers be to the companies \nproducing more alignment publications than  ",
      "offset": 9196.64,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "they are? It’s a little bit hard to understand, \nbecause the resourcing would be so uneven there.",
      "offset": 9203.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. I may be biased \nin what I’m exposed to, because I’m  ",
      "offset": 9208.479,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "just much more aware of the \nstuff that Redwood has done,  ",
      "offset": 9211.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "but it’s just actually not implausible to \nme that that is the kind of current ratio.",
      "offset": 9213.92,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "So reasons companies are less productive: there’s \nstuff we mentioned a bit before about maybe  ",
      "offset": 9225.92,
      "duration": 9.76
    },
    {
      "lang": "en",
      "text": "actually the infrastructure and speed at which \nyou’re able to do research is slower. I think  ",
      "offset": 9235.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "there’s different incentives and pressure \nto do the thing that will unblock shipping  ",
      "offset": 9240.64,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "the next generation model, rather than do the \nresearch that’s most important in the longer run.",
      "offset": 9249.2,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "I think to some extent it’s just like too \nmany people in the same place or something,  ",
      "offset": 9256.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "where it’s just like a marginal person is less \nuseful when you already have a tonne of people.",
      "offset": 9260.399,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I think also maybe companies are more focused \n— this is similar to short term– but it’s  ",
      "offset": 9266.399,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "both short in terms of time horizon and \nlocality. They’re just thinking about  ",
      "offset": 9274.319,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "things at their own company, and not what \nis most important for the field as a whole,  ",
      "offset": 9280.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and how do we disseminate that and \nmake it easy for other people to adopt.",
      "offset": 9284.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "And there’s a bunch of barriers to publishing, and \nthat’s maybe a reason that some people have left,  ",
      "offset": 9289.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "have been unhappy about not being \nable to publish their work enough.",
      "offset": 9295.2,
      "duration": 3.828
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. Presumably they’re a lot more \ncautious now about what they share and what they  ",
      "offset": 9299.028,
      "duration": 5.612
    },
    {
      "lang": "en",
      "text": "publish, and I imagine there’s a lot of levels \nof review that stand between someone having some  ",
      "offset": 9304.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "insight and it going out. I guess especially \nif it’s related to risks that the company  ",
      "offset": 9309.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "might be creating, you can see a lot of people \nwanting to scrutinise that before it’s shared.",
      "offset": 9314,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. The comms team in \nthere editing the blog post, being like,  ",
      "offset": 9318,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "“Can we make it sound a bit more optimistic here?”",
      "offset": 9326,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Maybe another thing that I think is overrated, or \nI feel like sometimes people conflate alignment  ",
      "offset": 9330,
      "duration": 10.72
    },
    {
      "lang": "en",
      "text": "— in the sense of making sure the model is trying \nto do what we want, and not trying to do extremely  ",
      "offset": 9340.72,
      "duration": 11.44
    },
    {
      "lang": "en",
      "text": "bad things that we definitely don’t want — with \nmaking the model better at following complicated  ",
      "offset": 9352.16,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "instructions or other things that are more just \nlike making the product better or enhancing  ",
      "offset": 9359.28,
      "duration": 9.119
    },
    {
      "lang": "en",
      "text": "the model’s capabilities in the direction of \nfollowing elaborate rules about what we want.",
      "offset": 9368.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "And I would just be more excited about progress \nthat’s progress on the problem when humans don’t  ",
      "offset": 9373.92,
      "duration": 9.76
    },
    {
      "lang": "en",
      "text": "know which output is better, but the model knows \na bunch of stuff about what’s going on. That seems  ",
      "offset": 9383.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "more useful than, “The humans want the model \nto follow this complicated set of instructions,  ",
      "offset": 9389.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so we did the things to make the model better \nat following the complicated instructions,  ",
      "offset": 9393.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "because otherwise it was forgetting that \nit’s supposed to also have this policy or  ",
      "offset": 9398.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "something.” That doesn’t feel like it’s \nmaking progress on the core problem.",
      "offset": 9401.52,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Earlier you talked about \nneuralese, which is this sort of internal  ",
      "offset": 9406.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "gibberish to the model we can’t follow, \nit’s helping it solve the problem somehow.  ",
      "offset": 9410.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Is it possible to have a science of \ninterpreting neuralese to go from a  ",
      "offset": 9414.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "bunch of numbers that the model was able to \nunderstand back into English in a faithful way?",
      "offset": 9419.04,
      "duration": 3.827
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think that there’s some of \nthis which is basically contiguous with  ",
      "offset": 9422.867,
      "duration": 3.373
    },
    {
      "lang": "en",
      "text": "existing interpretability. And it’s sort \nof like you’ve just gone from your model  ",
      "offset": 9426.24,
      "duration": 9.279
    },
    {
      "lang": "en",
      "text": "doing single forward passes to now you have \nsome recursive thing, and you’re just feeding  ",
      "offset": 9435.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the activations back in. And basically you \nwant to do all of the same interp things.",
      "offset": 9439.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "I think there are also other approaches. If \nwe’re talking about specifically this case  ",
      "offset": 9443.359,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "where you did have an interpretable chain of \nthought, and then either you trained on it too  ",
      "offset": 9450.24,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "much and got gobbledygook, or you did some kind \nof optimisation, that there’s maybe a bunch of  ",
      "offset": 9457.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "techniques here that are more like preserving the \nexisting interpretability that you did have — as  ",
      "offset": 9462.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "opposed to recovering it from this thing \nthat started off completely uninterpretable.",
      "offset": 9468,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Or making it possible to get that speedup without \nit needing to transition to the neuralese.",
      "offset": 9473.28,
      "duration": 5.267
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Right. I guess that would be the \ntradeoff. So the reason companies will allow  ",
      "offset": 9478.548,
      "duration": 3.452
    },
    {
      "lang": "en",
      "text": "that to happen is because they just think \nrequiring us to be able to understand the  ",
      "offset": 9482,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "reasoning is merely holding it back; we \nneed to allow it to use its own gibberish,  ",
      "offset": 9487.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because that’s better. But I guess if you \nthen added a whole other layer of requiring  ",
      "offset": 9492.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it to then translate its reasoning back, \nthen that’s just slowing it down again.",
      "offset": 9497.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Is this an area that might be ripe \nfor regulation? That you should say  ",
      "offset": 9501.6,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "you can’t have models just using neuralese? \nOr maybe there’s always going to be too much  ",
      "offset": 9505.359,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of a grey area of whether the chain \nof thought is comprehensible or not,  ",
      "offset": 9508.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "so it’s not a very bright line that you could \ndraw in the sand. But it does feel like it would  ",
      "offset": 9512,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "be really nice if we could get them to act in \nthe spirit of saying, no neuralese, basically.",
      "offset": 9516.64,
      "duration": 6.147
    },
    {
      "lang": "en",
      "text": "Beth Barnes: And no training the model not \nto do kinds of reasoning that you don’t like:  ",
      "offset": 9522.787,
      "duration": 3.532
    },
    {
      "lang": "en",
      "text": "no training the model not \nto reason about scheming,  ",
      "offset": 9526.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and it needs to be interpreted. I feel like \nit would be great if that was enforced.",
      "offset": 9530.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I do think there’s a bunch of opportunities for \ntechnical progress here, where it may be much  ",
      "offset": 9536.64,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "more feasible to figure out how you would have \nsome kind of summary or translation that you  ",
      "offset": 9543.6,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "are ensuring is faithful in some way. It just \nfeels like a thing that people could work on.",
      "offset": 9551.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: In terms of commonsense \nregulations that you could try,  ",
      "offset": 9556.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I’ve had the shower thought for a while. \nTo the regulatory scientists, this is  ",
      "offset": 9561.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "going to sound stupid, but why not \njust ban AI being used to enhance  ",
      "offset": 9566.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "AI? I would feel so much better about the \nfuture if I knew that you couldn’t have a  ",
      "offset": 9570.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "recursive self-improvement loop. That would \nsolve the problem to an enormous degree.",
      "offset": 9576.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "The problem, of course, is it’s already \nbeing used that way. They’re already  ",
      "offset": 9580.8,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "using it to help with coding. They’re \npresumably already using the language  ",
      "offset": 9583.439,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "models to just research all kinds of random \nquestions that they have. So it’s assisting.",
      "offset": 9587.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "But I think if you just had that as the \nrule, and you interpreted it as kind  ",
      "offset": 9592.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of any plausibly reasonable way where \nyou draw the line between what is and  ",
      "offset": 9597.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "isn’t permitted — so you allow some basic \nassistance with programming, but nothing  ",
      "offset": 9602.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that’s more impressive than what we have now \n— I think it would be a massive improvement,  ",
      "offset": 9606.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "basically. There’s almost like no way \nthat that could be worse from a safety  ",
      "offset": 9610.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "point of view than the default, where there’s \nabsolutely no rules about that whatsoever.",
      "offset": 9615.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "What do you think of these kinds \nof naive things, just a person off  ",
      "offset": 9622,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "the street who was just presented with \nthis problem saying, “Why don’t we just  ",
      "offset": 9626.479,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "ban the thing that’s obviously creating this \nenormous risk?” Can we get some mileage here?",
      "offset": 9628.479,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I’m maybe not the best person \nto ask about regulatory feasibility. I think,  ",
      "offset": 9635.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "as you said, it’s hard to, if people are \nalready doing things, say that they can’t  ",
      "offset": 9642.479,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "do it anymore. I would be maybe worried \nif there is some overhang created here,  ",
      "offset": 9646.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "if people were just holding off from doing a thing \nand then you suddenly get a lot of improvement.",
      "offset": 9652.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess people would \nsay that China’s going to do it  ",
      "offset": 9657.6,
      "duration": 1.759
    },
    {
      "lang": "en",
      "text": "anyway, and this would hold us back so much.",
      "offset": 9659.359,
      "duration": 1.681
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. There’s stuff you could do. \nLike the amount of trust you need to have in your  ",
      "offset": 9661.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "model before you let it do stuff. Or I think \nmore models above this capability level, you  ",
      "offset": 9666.56,
      "duration": 8.879
    },
    {
      "lang": "en",
      "text": "need this mitigation — where definitely the thing \nthat spiritually makes the most sense is we don’t  ",
      "offset": 9675.439,
      "duration": 11.04
    },
    {
      "lang": "en",
      "text": "care about the rate of progress per se; we just \ncare about do you have the mitigations in time?",
      "offset": 9686.479,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "So I don’t love this AI R&D as a thing \nto draw a line on, because it is kind of  ",
      "offset": 9691.68,
      "duration": 11.04
    },
    {
      "lang": "en",
      "text": "contiguous with all of these just basic coding \ntools. Or you could also just ban humans doing  ",
      "offset": 9702.72,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "machine learning research or something. It \nhas this weird flavour in some ways. And if  ",
      "offset": 9710,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "you didn’t already agree that AI progress \nwas scary, why is more AI progress scary?  ",
      "offset": 9718,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "It’s not like the final threat model thing. But \nI think pragmatically, you’re just not going to  ",
      "offset": 9724.319,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "have the mitigations in place if you’re doing \nthis crazy recursive self-improvement loop.",
      "offset": 9732.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You might come back and say, \nwhy not just hobble them in any random way.  ",
      "offset": 9737.28,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Like say that if you’re doing AI research, you \nhave to remove the E key from your keyboard,  ",
      "offset": 9741.439,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and that’s going to slow them \ndown and be really irritating.",
      "offset": 9745.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "But I guess the difference here is \nthat if you have AI doing the work,  ",
      "offset": 9748.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "then humans are getting further and further \nout of the loop of anything that’s happening.  ",
      "offset": 9752.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "It’s becoming more and more inscrutable. I \nguess we expect that that’s the thing that’s  ",
      "offset": 9755.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "going to lead to the very rapid improvement in \nthe capabilities. So it’s something that slows  ",
      "offset": 9760.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "it down the more it would have been sped \nup. It has at least a nice property to it.",
      "offset": 9766.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, I do think in \ngeneral I prefer things that are like,  ",
      "offset": 9773.12,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "“You have to do this mitigation,” \nrather than just like, “Slow down.”",
      "offset": 9779.359,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess that gives them \nthe incentive to fix the problem,  ",
      "offset": 9784.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "inasmuch as you specified it properly, because \nthen they’ll be able to go ahead. Whereas if  ",
      "offset": 9788.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you just ban it and say it’s always going to \nbe banned, then no such incentive is made.",
      "offset": 9793.04,
      "duration": 4.306
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And it’s a thing that a broader \ngroup of people can get on board with. It gives  ",
      "offset": 9797.346,
      "duration": 9.614
    },
    {
      "lang": "en",
      "text": "you some free lunch with respect to if people \ndisagree about what capabilities you’ll have when.  ",
      "offset": 9806.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Someone who might object to the thing that \njust slows you down might not object to,  ",
      "offset": 9814.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "“If you hit this level, you need \nto do this mitigation.” So it can  ",
      "offset": 9818.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "be strictly more appealing and it’s \nstrictly more reasonable in some ways.",
      "offset": 9823.12,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. I wonder if even if it’s a \nbad idea, it’s the kind of regulation that we  ",
      "offset": 9830.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "might get in a crisis. If you do kick \noff a recursive self-improvement loop  ",
      "offset": 9835.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and that quickly leads to an AI going off \nthe rails and causing a bunch of damage,  ",
      "offset": 9841.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and then you loop in politicians, and they’re \nlike, “You were doing what?! You took all  ",
      "offset": 9844.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of your humans out and just had the AI doing \nall the work? Obviously we have to ban that.”",
      "offset": 9848.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I guess that probably would \nbe better than nothing,  ",
      "offset": 9852.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "but that could end up being the sort of \nnaive regulation that perhaps helps but  ",
      "offset": 9855.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is substantially suboptimal relative \nto other more sophisticated things.",
      "offset": 9859.76,
      "duration": 3.506
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. One way that being like \n“just slow down” or “just pause” or something  ",
      "offset": 9863.266,
      "duration": 7.213
    },
    {
      "lang": "en",
      "text": "can backfire is we would like to save all \nour pausing juice to the bit at which you  ",
      "offset": 9870.479,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "can get the most safety progress out of your \nmodels before things are really dangerous.",
      "offset": 9877.04,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "And if people are like, “We paused and \nthen everything was fine,” then everyone’s  ",
      "offset": 9885.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "fed up with this and thinks this is \nstupid and let’s just go ahead now.  ",
      "offset": 9888.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "And now the compute has accelerated more \nand we’re ready to go through this. You  ",
      "offset": 9891.84,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "really want to go through that part of \nthe transition as slowly as possible.",
      "offset": 9899.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So any constraints you \nplace on yourself earlier is just  ",
      "offset": 9904.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "creating more latent capability \nto very rapidly speed up later.",
      "offset": 9909.76,
      "duration": 3.586
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, that’s one model. \nI don’t think this is totally right. I  ",
      "offset": 9913.346,
      "duration": 3.933
    },
    {
      "lang": "en",
      "text": "just think people seem to sometimes miss \nthis fact. I think it’s more salient the  ",
      "offset": 9917.279,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "more you’re thinking about how a lot of \nthe important safety work will be done  ",
      "offset": 9922.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "by the AI researchers at the point just \nbefore they are catastrophically risky.",
      "offset": 9926.08,
      "duration": 6.467
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, new topic:  ",
      "offset": 9932.548,
      "duration": 2.092
    },
    {
      "lang": "en",
      "text": "Is open weighting models in general \ngood or bad, from your point of view?",
      "offset": 9934.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Beth Barnes: This is something \nI’ve changed my mind on a fair  ",
      "offset": 9940.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "amount. It’s maybe also a bit related \nto being in the lower-assurance world.",
      "offset": 9943.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Originally I was like, this seems very \ndangerous if the problem is either that  ",
      "offset": 9951.279,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "these things can be misused or that they can \nbe dangerous even without a human trying to  ",
      "offset": 9959.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "misuse them. It seems like you don’t want them \nto be everywhere, and not be able to be like,  ",
      "offset": 9966,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "“Actually, this is a bad idea. Let’s \nundo that.” It’s an irreversible action,  ",
      "offset": 9970.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and it opens up a lot more surface area \nfor things to go wrong — if you think that,  ",
      "offset": 9974.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "at least for some of the threat models, there \nmight be a big offence/defence imbalance.",
      "offset": 9982.08,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "So I think if you’re trying to keep the risk \nvery low, it really feels like you can’t open  ",
      "offset": 9990.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "source anything that capable. Because again, \neven with current models, it seems hard to  ",
      "offset": 9994.8,
      "duration": 8.479
    },
    {
      "lang": "en",
      "text": "rule out that with other advancements in \nscaffolding or fine-tuning or something,  ",
      "offset": 10003.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "it would then become quite easy to make \nthis model into something very dangerous.",
      "offset": 10007.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I’ve shifted my perspective generally on \nhow good it is to keep everything inside  ",
      "offset": 10014.08,
      "duration": 10.8
    },
    {
      "lang": "en",
      "text": "the labs. Sort of similar to the thing \nabout, would you rather just the labs  ",
      "offset": 10024.88,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "know about what capabilities are coming \nor would you rather that everyone knows?",
      "offset": 10033.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "I think in practice, the open source models \nhave been used to do a bunch of really good  ",
      "offset": 10039.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "safety work, and this has been important and \ngood. And having a more healthy, independent,  ",
      "offset": 10044.64,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "outside-of-companies field of research seems \ngood both for the object-level safety progress  ",
      "offset": 10051.84,
      "duration": 8.559
    },
    {
      "lang": "en",
      "text": "you made, and for policymakers having \nsome independent people they can ask — who  ",
      "offset": 10060.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "actually understand what’s going on, and \nare able to do experiments and things.",
      "offset": 10066.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Also, in general, I have become \nmore sceptical of some kind of lab  ",
      "offset": 10070.08,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "exceptionalism. I think a lot of the companies \nhave this, like, “We’re the only responsible  ",
      "offset": 10079.04,
      "duration": 9.84
    },
    {
      "lang": "en",
      "text": "ones. We’ll just keep everything to ourselves, \nand the government will be too slow to notice,  ",
      "offset": 10088.88,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "and everyone else is irresponsible.” I just \nthink this becomes less plausible the more  ",
      "offset": 10097.52,
      "duration": 7.759
    },
    {
      "lang": "en",
      "text": "independent labs are saying this, and the less \nthey actually respond to how responsible are  ",
      "offset": 10105.279,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the other actors. And if you trust the \ncompanies less, you want less of that.",
      "offset": 10110.64,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "I think it’s also one of those things where \neveryone always thinks that you’re the  ",
      "offset": 10120.479,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "exception — “It’s fine, but we’ll just actually \nbe good and actually be responsible. We don’t  ",
      "offset": 10128.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "need oversight.” And sunlight is the best \ndisinfectant. Oversight is really important,  ",
      "offset": 10133.04,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "and security mindset and secrecy \nand locking things down can be bad.",
      "offset": 10142.64,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Seems like there’s a \nreasonable amount to unpack there.  ",
      "offset": 10150.399,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "The basic reason I’ve heard for why actually open \nsourcing has been better than perhaps people like  ",
      "offset": 10157.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "me feared a few years ago is that it’s been an \nabsolute boon for alignment and safety research,  ",
      "offset": 10162.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "because it means external people don’t have \nto actually go work at the labs — they can do  ",
      "offset": 10166.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "things independently, which is really useful \nfor all the reasons that we’ve talked about,  ",
      "offset": 10169.359,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "if they have access to basically \nfrontier models outside of the companies.",
      "offset": 10172.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "And I guess separately from that, it sounds \nlike you’re saying that the companies having  ",
      "offset": 10177.04,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "a very secrecy-focused mindset, wanting to hold \nall of the information about the models within  ",
      "offset": 10182.479,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "themselves, that that’s a dangerous mentality. And \nmaybe it’s actually helpful to have the weights  ",
      "offset": 10188.479,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "open and things being used all over the place, \nso that people can see for themselves the chain  ",
      "offset": 10195.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "of thought, for example, and modify the models and \nsee what is possible if they’re improved — things  ",
      "offset": 10198.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that wouldn’t be possible if you only accessed \nit through an API that’s highly limited.",
      "offset": 10202.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. I think there’s also \na point about maybe you just think some  ",
      "offset": 10206.64,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "of the companies are bad actors, or there \nare people at them who would be bad actors,  ",
      "offset": 10215.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and you would prefer that the rest of \nthe world also has access to the tech.",
      "offset": 10219.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "But yeah, I think that the main thing is this \nslightly less specific to open weight question,  ",
      "offset": 10222.56,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "and more some kind of general \nposition on lab transparency  ",
      "offset": 10229.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "versus sort of security and locked-downness.",
      "offset": 10236.479,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Can you elaborate on that a bit?",
      "offset": 10238.96,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "Beth Barnes: One of my side interests \nis the history of nuclear weapons.  ",
      "offset": 10241.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "And I think there are a bunch of examples \nthere, and in other places, of secrecy and  ",
      "offset": 10249.52,
      "duration": 10
    },
    {
      "lang": "en",
      "text": "personnel restrictions and things \nbeing used to silence people who  ",
      "offset": 10261.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "had safety concerns or ethical concerns, \nor were too busybodies or interfering.",
      "offset": 10265.6,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "So both compartmentalisation — so that \nfewer people knew about a thing and could  ",
      "offset": 10272.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "complain about it — and then trying to \njail or otherwise get rid of or exclude  ",
      "offset": 10277.359,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "individual people who had too many ethical \nconcerns, basically. In particular Szilard.",
      "offset": 10281.12,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "And I think we actually have seen \nthis specifically being used in the  ",
      "offset": 10289.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "AI case already. I believe when Leopold \nAschenbrenner was fired from OpenAI that  ",
      "offset": 10295.92,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "there was some claim that this was about \nhaving leaked confidential information.",
      "offset": 10302.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I don’t know the details here, but it’s very \nplausible to me that basically everyone at  ",
      "offset": 10311.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "these companies is doing a bunch of stuff that’s \ntechnically leaking. And if you want to get rid  ",
      "offset": 10315.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of someone, you can go through all their documents \nand find something that they did. Or in general,  ",
      "offset": 10319.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it’s a reason to keep people out of \nthings, like compartmentalisation. This  ",
      "offset": 10325.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "person is going to be annoying about \nthe thing. Let’s keep them out of it.",
      "offset": 10331.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "And I think doing the Trinity test, when they \nhad the concern about whether it might be  ",
      "offset": 10335.52,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "possible to ignite the atmosphere, there \nwas basically no civilian oversight of that,  ",
      "offset": 10344,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "downstream of this being a very secret project \noverall and not much government or congressional  ",
      "offset": 10353.76,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "oversight. It just meant that just the people \nwho made this decision were the people who —",
      "offset": 10362.24,
      "duration": 5.027
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It’s just a bunch of scientists \nor engineers who happen to be working on it.",
      "offset": 10367.267,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, and military. \nI think a lot of trying to get the  ",
      "offset": 10370.547,
      "duration": 3.613
    },
    {
      "lang": "en",
      "text": "scientists to shut up if they’re \nbeing too annoying about stuff.",
      "offset": 10374.16,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "And people who are very invested in that \nparticular project — even people who would  ",
      "offset": 10376.16,
      "duration": 9.439
    },
    {
      "lang": "en",
      "text": "have otherwise been relatively pacifist or not \nsuper excited about weapons — once it’s the  ",
      "offset": 10385.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "thing that you’ve been working on for ages, it’s \nreally hard to be like, “Maybe this isn’t good.”",
      "offset": 10391.92,
      "duration": 4.068
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “Maybe we should \njust stop completely.”",
      "offset": 10395.988,
      "duration": 1.999
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. I think there’s a bunch \nmore anecdotes from that history that are  ",
      "offset": 10397.987,
      "duration": 6.013
    },
    {
      "lang": "en",
      "text": "relevant to current AI stuff. But companies \nthat are developing AI seem like particularly  ",
      "offset": 10404,
      "duration": 13.04
    },
    {
      "lang": "en",
      "text": "bad decision makers in some sense here, \nbecause they have such conflicts of interest,  ",
      "offset": 10417.04,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "so centralising all of the information power \nwithin them seems like maybe a bad idea.",
      "offset": 10423.439,
      "duration": 5.348
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK. The obvious benefits of having a \nsecurity mindset are that dangerous information  ",
      "offset": 10428.788,
      "duration": 7.052
    },
    {
      "lang": "en",
      "text": "doesn’t leak out to other groups. But downsides \nwould be, if you start siloing information within  ",
      "offset": 10435.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the company, then an even smaller number \nof people might have a sense of the general  ",
      "offset": 10441.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "picture of what the frontier models are able \nto do. If they’re very cautious about letting  ",
      "offset": 10444.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "anyone outside the organisation know, then \nit makes it harder for governance to occur,  ",
      "offset": 10448.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "makes it harder for broader society to appreciate \nif risks are reaching a level that they wouldn’t  ",
      "offset": 10452.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "regard as acceptable. It means that there’s \njust basically less oversight from anyone else.",
      "offset": 10458,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. Which maybe means we missed \nthe opportunity to react to a safety incident  ",
      "offset": 10463.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that should have been a warning — like, “We \nactually caught the model scheming or trying  ",
      "offset": 10468.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to do something bad.” And instead of being like, \n“Whoa, everyone should take this really seriously,  ",
      "offset": 10474.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and we should freak out and we should \nstop this kind of thing,” it’s just like,  ",
      "offset": 10480.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "“Let’s just train it not to do this, \nlet’s not tell anyone about that.”",
      "offset": 10484.399,
      "duration": 3.669
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You’re saying something could \ngo wrong internally, but they might say  ",
      "offset": 10488.068,
      "duration": 2.491
    },
    {
      "lang": "en",
      "text": "that it’s too dangerous to let anyone know \nthat this even happens. So it gives them  ",
      "offset": 10490.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "a pretence for sweeping it under the carpet. \nI guess even more problematically, it allows  ",
      "offset": 10493.52,
      "duration": 7.759
    },
    {
      "lang": "en",
      "text": "you to just clean house, to remove \nanyone who doesn’t support the current  ",
      "offset": 10501.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "project, if you’re saying that anyone who’s ever \ntold anyone about what’s going on in the company,  ",
      "offset": 10504.8,
      "duration": 9.902
    },
    {
      "lang": "en",
      "text": "even if everyone is doing so, then you can just \nfire people basically arbitrarily on that grounds.",
      "offset": 10514.702,
      "duration": 0.098
    },
    {
      "lang": "en",
      "text": "You mentioned Leopold — that’s Leopold \nAschenbrenner; I think his crime was  ",
      "offset": 10514.8,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "telling people that information security within \nthe company was not sufficient to stop the Chinese  ",
      "offset": 10521.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "from stealing the model. Or that was his big hobby \nhorse — which everyone agrees with, more or less,  ",
      "offset": 10526.16,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and everyone is kind of troubled by. \nMaybe he was making too much trouble  ",
      "offset": 10531.439,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "over that and potentially bringing more \nregulatory scrutiny to the company?",
      "offset": 10535.84,
      "duration": 3.347
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. I don’t know exactly \nwhat happened in this particular case,  ",
      "offset": 10539.187,
      "duration": 3.933
    },
    {
      "lang": "en",
      "text": "but I’d be very surprised if \nthis pattern doesn’t show up.",
      "offset": 10543.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Do you want to give any details \nof a particular case that occurred during the  ",
      "offset": 10547.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "nuclear process? You mentioned Szilard, who was \none of the first people to twig that a nuclear  ",
      "offset": 10552.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "weapon might be possible. And \nhe wrote to various people to  ",
      "offset": 10557.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "say that it’s really important that \nAmerica do this before the Nazis.",
      "offset": 10560.56,
      "duration": 2.786
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. I’m a Szilard fan. It’s \na random thing. This is a historical figure  ",
      "offset": 10563.346,
      "duration": 7.453
    },
    {
      "lang": "en",
      "text": "who it feels like is thinking about so many \nof the same things that we’re thinking about  ",
      "offset": 10570.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "in kind of similar ways. I think he was \nvery good in terms of scope sensitivity.  ",
      "offset": 10576.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "He was actually thinking about what are the \nmost important things happening in the world.",
      "offset": 10580.399,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "And this is, I think, also some reason for \nbeing somewhat more optimistic about the  ",
      "offset": 10583.6,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "ability to forecast technological progress \nand world events and things, and maybe some  ",
      "offset": 10590.319,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "evidence that the reason why most people are \nbad at it is that they’re not actually trying.",
      "offset": 10597.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "So he foresaw nuclear weapons in the early 1930s,  ",
      "offset": 10603.04,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "like, “Here’s how you would theoretically make \na chain reaction if you can find a reaction that  ",
      "offset": 10612.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is started by neutrons and produces \nmore neutrons” — and then was like,  ",
      "offset": 10617.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "“I should keep this secret because it seems bad if \neveryone was doing it, bad if the Nazis got it.”",
      "offset": 10622.24,
      "duration": 9.84
    },
    {
      "lang": "en",
      "text": "And he also was like, “I’ll move to America \none year before the war” — and he did,  ",
      "offset": 10633.12,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "in fact, move to America one year before \nthe war. Like, if you’re actually trying,  ",
      "offset": 10642.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "maybe geopolitical events were kind of obvious. \nAnd that you might use this for powering ships  ",
      "offset": 10646.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "or submarines, and actually thinking through the \nimplications of technology seemed quite possible.",
      "offset": 10653.52,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "He then, with Einstein, wrote the letter to \nthe US government that sort of kicked the  ",
      "offset": 10665.68,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "bomb project in the US into action. The original \njustification was that we don’t want the Nazis  ",
      "offset": 10674.56,
      "duration": 13.2
    },
    {
      "lang": "en",
      "text": "to have a nuclear monopoly. That seems like a \nvery reasonable justification, and that seems  ",
      "offset": 10687.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "very bad. But during the course of \nthe project, it migrated from that to,  ",
      "offset": 10694,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "actually, we’re going to just use it to \nshorten the war by bombing the Japanese.  ",
      "offset": 10700.319,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And now it’s an arms race with the \nSoviet Union. And I think very few  ",
      "offset": 10704.64,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "of the people actually left the project after \nthat momentum. Joseph Rotblat was one person.",
      "offset": 10713.12,
      "duration": 7.187
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. So at some point it became clear \nthat the Nazis weren’t working on this weapon,  ",
      "offset": 10720.307,
      "duration": 3.533
    },
    {
      "lang": "en",
      "text": "and eventually then they were defeated. That then \nobviated the explanation that most of these people  ",
      "offset": 10723.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "had been given for being involved in the project \nat all, which was to beat the Nazis. But you’re  ",
      "offset": 10729.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "saying basically 99% of them get stuck around \nwith a new explanation that they were given.",
      "offset": 10732.64,
      "duration": 4.307
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right, yeah. There \nwere multiple people who said,  ",
      "offset": 10736.947,
      "duration": 3.372
    },
    {
      "lang": "en",
      "text": "“This is a terrible thing and I wouldn’t \nwant to work on it, but in this particular  ",
      "offset": 10742,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "case of only the Nazis having the bomb, that \nwould be really bad. So I’ll work on it in  ",
      "offset": 10746.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the special circumstance.” Then that isn’t \nactually true anymore, and people are like,  ",
      "offset": 10751.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "“Well, since I’m here now…” There’s investment, \nand these things get their own momentum.",
      "offset": 10755.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I feel like I see parallels in AI in \nterms of people giving reasons for why  ",
      "offset": 10764.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "it’s important for them to be first or \npush ahead or something. They’re like,  ",
      "offset": 10770.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "“We’re the most safe. This other \ncompetitor is doing this thing.”  ",
      "offset": 10773.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "And their actions don’t change as those \nother facts about the situation change.",
      "offset": 10777.279,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "One thing is also that, proportionally, \nvery small amount of effort went into  ",
      "offset": 10784.16,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "actually trying to figure out if the \nNazis did have a nuclear programme.  ",
      "offset": 10791.84,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "There was basically no American espionage. \nI think there was some British espionage,  ",
      "offset": 10796.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and the Americans maybe weren’t even on top \nof actually what the British had learned about  ",
      "offset": 10800.8,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "this. So again, if they were actually trying \nto make sure we avoid Nazi nuclear monopoly,  ",
      "offset": 10807.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you would have been doing much more, like, “Where \nare the Nazis at? Is this actually a risk?”",
      "offset": 10813.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "And another example of Szilard being much \nbetter at forecasting the technological stuff:  ",
      "offset": 10821.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "he was just like, “The Soviets will have the \nbomb really soon.” What are you talking about?  ",
      "offset": 10828.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "And [director of the Manhattan Project Leslie] \nGroves was like, “There’s no uranium in Russia.”  ",
      "offset": 10831.2,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "It’s like, what? The ores will be slightly lower \ngrade. There’s no way there’s no uranium in the  ",
      "offset": 10833.52,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "whole of Russia. You know how big Russia \nis? And like, “Our brilliant American boys:  ",
      "offset": 10840.399,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "no one will match that.” It’s like, \nno, this is not actually that hard.",
      "offset": 10846.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Or they’ll just copy \nit. There were Russian spies.",
      "offset": 10851.2,
      "duration": 5.507
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Well, yeah. Only a few, but there was \nvarious information that they actually had like  ",
      "offset": 10856.707,
      "duration": 5.933
    },
    {
      "lang": "en",
      "text": "before the Americans did, because of spies \nin the British Foreign Office, I think. So  ",
      "offset": 10862.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "there’s all this stuff about secrecy and locking \ndown, and that secrecy was completely useless.",
      "offset": 10867.52,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "And yet people would not plan for \nthis arms race. But Szilard was like,  ",
      "offset": 10874.479,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "“The thing that is really important now \nis there’s going to be a nuclear arms  ",
      "offset": 10880.72,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "race between the US and Soviet Union that \nis going to be potentially world ending,  ",
      "offset": 10883.439,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and we need to figure out how to try and make \nthat less of the case and how we can reduce  ",
      "offset": 10888.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "tensions with the Russians now.” And the people \nwere like, “Whatever. They’ll never get it.”",
      "offset": 10894.24,
      "duration": 6.948
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It’s hard to fathom.",
      "offset": 10901.188,
      "duration": 3.278
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And the amount of effort \nthat went into things like thinking about  ",
      "offset": 10904.466,
      "duration": 8.413
    },
    {
      "lang": "en",
      "text": "alternatives to bombing Hiroshima, to \nactually destroying a city — you know,  ",
      "offset": 10912.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "could you explode the bomb in the air or in an \nunpopulated area to demonstrate the capability  ",
      "offset": 10918.319,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "or something? And it was like people discussed \nit a bit over lunch on the day that they were  ",
      "offset": 10925.439,
      "duration": 9.681
    },
    {
      "lang": "en",
      "text": "making this decision. But there was so little \neffort put into some of the key decisions.",
      "offset": 10935.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "And similarly with potential world governance \nof the weapons. Like the Acheson–Lilienthal  ",
      "offset": 10940.319,
      "duration": 12.16
    },
    {
      "lang": "en",
      "text": "plan of inspections and things, the people \nwho were working on it were in conflict of  ",
      "offset": 10954.64,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "interest — they had various industrial things that \nthey would benefit from, if I remember correctly.",
      "offset": 10963.2,
      "duration": 9.359
    },
    {
      "lang": "en",
      "text": "And again, just generally it was a very \nhalf-hearted effort, and was kind of like they  ",
      "offset": 10972.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "just wanted to do something so they could have \nbeen like, “We offered something, but the Russians  ",
      "offset": 10978,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "said no.” But it feels like you have less evidence \nthan you might think that something like that  ",
      "offset": 10981.359,
      "duration": 10.561
    },
    {
      "lang": "en",
      "text": "would have been completely infeasible, because \nthere was very little effort into mechanism design  ",
      "offset": 10991.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "of how you could have transparency about this in \na way that was less unpalatable to the Soviets  ",
      "offset": 10998,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "who didn’t want a bunch of stuff about how badly \nthings were going for them in general to be known.",
      "offset": 11006.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "So I don’t know. Some kind of actually \ntrying at the most important things  ",
      "offset": 11013.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "seems like it would have been pretty \nleveraged if you’d had some detailed  ",
      "offset": 11019.359,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "alternative proposal that was \nwell thought through. Plausibly.",
      "offset": 11023.279,
      "duration": 5.094
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. I guess the generalised thing to \nkeep your eyes out for is if people say, “I don’t  ",
      "offset": 11028.373,
      "duration": 3.627
    },
    {
      "lang": "en",
      "text": "want to do this, but I have to do it because of \nX.” But they show almost no interest in finding  ",
      "offset": 11032,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "out with confidence whether X is true or not, \nand they don’t stay up to date on whether it’s  ",
      "offset": 11036.479,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "true or not. They also maybe show no interest in \nchanging X, taking steps that would fix X and make  ",
      "offset": 11041.68,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "their actions no longer required. And also when X \nstops being true, they don’t change their actions.",
      "offset": 11047.359,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "These are kind of giveaways that maybe X isn’t the  ",
      "offset": 11053.359,
      "duration": 1.841
    },
    {
      "lang": "en",
      "text": "real reason. And I think we may see \na fair bit of that at the moment.",
      "offset": 11055.2,
      "duration": 3.987
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. The thing has \nacquired its own momentum. And when  ",
      "offset": 11059.187,
      "duration": 7.773
    },
    {
      "lang": "en",
      "text": "Szilard was trying to convince Truman’s \nincoming Secretary of War, I think,  ",
      "offset": 11066.96,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "that bombing Japan was a bad idea, he \nwas like, “Well, what are we going to  ",
      "offset": 11072.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "tell Congress we spent all that money on if we \ndon’t do anything with it?” It’s like, oh god.",
      "offset": 11075.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, it’s a little bit \nembarrassing to kill that many people  ",
      "offset": 11081.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "in order to just have a better \nanswer at a committee hearing.",
      "offset": 11086.72,
      "duration": 2.067
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, but that’s what \nthings get decided on. One of the  ",
      "offset": 11088.787,
      "duration": 4.333
    },
    {
      "lang": "en",
      "text": "reasons I think pre-scaleup evals would be good.",
      "offset": 11093.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: To explain: that’s a \nreason why we should figure out  ",
      "offset": 11096.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "whether a model is safe before we spend \nall the money training it. Because once  ",
      "offset": 11100.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you’ve spent all the money, what \nare you going to tell the board?",
      "offset": 11103.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. The investors.",
      "offset": 11109.68,
      "duration": 1.679
    },
    {
      "lang": "en",
      "text": "I thought of another analogy to the nuclear \nstuff now that we’re talking about China: maybe  ",
      "offset": 11111.359,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "something that was important was the compliance \nof individual scientists in different countries.",
      "offset": 11118.479,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "I think this is pretty unclear, but there’s maybe \nhints that some of why the German bomb programme  ",
      "offset": 11128.96,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "didn’t go very far was partially that scientists \nwere not very excited about nuclear-armed Nazis.",
      "offset": 11137.84,
      "duration": 7.828
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think it has been suggested that \nthey deliberately made bad strategic decisions,  ",
      "offset": 11145.668,
      "duration": 4.492
    },
    {
      "lang": "en",
      "text": "basically, because they were trying to \nmake the probability of success lower.",
      "offset": 11150.16,
      "duration": 3.266
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right.",
      "offset": 11153.426,
      "duration": 0.654
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Just to tie this back to the \nopen weighting, open source question:  ",
      "offset": 11154.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "if we just start open weighting all the best \nmodels, aren’t we kind of baked? It makes any  ",
      "offset": 11160.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "governance virtually impossible. It means that \nany misuse is going to happen very quickly.",
      "offset": 11166.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "What’s the upside story? I guess it \nmeans that you get better warnings,  ",
      "offset": 11173.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "maybe early. If there’s people who are saying we \ndon’t really have to worry about the misuse and  ",
      "offset": 11177.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "then something is open weighted and it’s \nimmediately misused for some catastrophe  ",
      "offset": 11182.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "or some horrific crime, then maybe that \ncould lead to improvements in governance.",
      "offset": 11186.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, I don’t advocate \ndoing things to cause horrific crimes.",
      "offset": 11191.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So doing everything we can \nto stop that, but I suppose if we fail,  ",
      "offset": 11195.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and people who are against any \nkind of governance succeed,  ",
      "offset": 11199.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "then a possible upside of that is that we \nall get to find out who was right sooner.",
      "offset": 11203.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think the case for certain types \nof transparency and openness is much stronger  ",
      "offset": 11208.24,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "than the case for specifically open sourcing \nweights. But I do think the open sourcing thus  ",
      "offset": 11215.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "far feels pretty clearly net positive, just \nbecause of the impact on safety research. I  ",
      "offset": 11221.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "do think you would have thought that it ought \nto be harder to argue that we’re in an arms  ",
      "offset": 11226.72,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "race if people are open sourcing the models. But \nsomehow this doesn’t seem to have stopped anyone.",
      "offset": 11231.439,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Never let common sense get in the \nway of the narrative that you would like to spin  ",
      "offset": 11236.96,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "for your commercial interest! Just to clarify, \nyou’re talking about the release of DeepSeek R1,  ",
      "offset": 11240.399,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "which is this super weapon that the Chinese \ndeveloped and then immediately gave to us for  ",
      "offset": 11246.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "free, in exchange for nothing. It really \nshows that we’re in a very intense arms  ",
      "offset": 11252.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "race with the Chinese to I guess give \none another our greatest technology.",
      "offset": 11256.8,
      "duration": 4.378
    },
    {
      "lang": "en",
      "text": "Beth Barnes: [laughs] Yeah.",
      "offset": 11261.178,
      "duration": 0.342
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It is extraordinary that that \ndid not preclude the arms race narrative.  ",
      "offset": 11261.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I guess you could say that maybe this \nshows that they will be able to make  ",
      "offset": 11266,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "something in future that they won’t share \nwith us. But you’d really think literally  ",
      "offset": 11268.24,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "giving away your weapons designs would be \ndefinitely an olive branch to your enemy.",
      "offset": 11271.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Beth Barnes: You would have thought. Yeah, \nI think basically what you were saying about  ",
      "offset": 11277.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "if there are incidents, they happen in public \nrather than just inside the lab. And especially  ",
      "offset": 11284.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "if you have to have open sourced the weights \nbefore you’re allowed to scale up by this much.  ",
      "offset": 11290,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "That would be one thing you get around. Like the \nlab can be doing this crazy thing internally that  ",
      "offset": 11297.68,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "no one has any idea of, and at least outside you’d \nbe able to demonstrate that this model is pretty  ",
      "offset": 11304.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "close to being able to do this thing, and maybe \nwe shouldn’t let them build even bigger ones.",
      "offset": 11307.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I think also, unless you have very good \nsecurity, the story of how you’re OK doesn’t  ",
      "offset": 11312.8,
      "duration": 14.08
    },
    {
      "lang": "en",
      "text": "depend on not open sourcing. It seems like \nif you have a sufficiently powerful model,  ",
      "offset": 11326.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "then a bunch of people are probably going to \nbe interested in it. And companies currently  ",
      "offset": 11332.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "don’t have the security that would \nkeep out state actors or the best  ",
      "offset": 11338.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "private actors. So your story, your theory \nof victory has to be something like, “We’ll  ",
      "offset": 11344.319,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "use our good models for something good, and that \nwill prevent some harm from these other models.”",
      "offset": 11352.56,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "I think most of those stories actually also work \nif there are open models everywhere — because it’s  ",
      "offset": 11360.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "more about how more of the people who control \nmore of the compute want models to be doing  ",
      "offset": 11366.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "these kinds of things than want models to be \ndoing these nefarious kinds of things. And it  ",
      "offset": 11372.319,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "does feel like either you have to be extremely \nlocked down, and there won’t be any bad actors,  ",
      "offset": 11380.24,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "or you’re making some argument about how we \ncan use these models defensively in some way.",
      "offset": 11387.2,
      "duration": 10.382
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Part of the story there is \nthat the security of these companies is  ",
      "offset": 11397.583,
      "duration": 6.337
    },
    {
      "lang": "en",
      "text": "not so strong that the models are not going \nto leak to various nefarious actors anyway.  ",
      "offset": 11403.92,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "We should at least be honest about it, \nand they may as well open weight it,  ",
      "offset": 11409.439,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "and then a whole bunch of nice people who didn’t \neven have to steal it might be able to use it.",
      "offset": 11411.52,
      "duration": 3.267
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. I mean, maybe \nthey’re currently open weight from  ",
      "offset": 11414.787,
      "duration": 3.773
    },
    {
      "lang": "en",
      "text": "the perspective of North Korea or \nwhatever. Something I’ve sometimes  ",
      "offset": 11418.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "said is there are no closed-weight models. \nLike, no one has that good of security.",
      "offset": 11424.319,
      "duration": 3.509
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: That is very interesting. I think \nmost people know, but maybe not everyone,  ",
      "offset": 11427.828,
      "duration": 3.531
    },
    {
      "lang": "en",
      "text": "that North Korea is actually very strong on \noffensive cyber capabilities. It’s not only  ",
      "offset": 11431.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "China that might be able to steal it. I guess \nyou don’t think of North Korea as a technological  ",
      "offset": 11435.359,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "powerhouse, but on this one thing of stealing \ninformation from people, they are quite good.",
      "offset": 11438.479,
      "duration": 3.987
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right.",
      "offset": 11442.466,
      "duration": 1.373
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You’ve been using this analogy \nbetween AGI and nuclear weapons. That’s  ",
      "offset": 11443.84,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "a comparison that people have definitely \ndrawn a lot over the years, and people have  ",
      "offset": 11450.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "both reasons to think that it’s interesting \nand reasons to think that it’s misleading.",
      "offset": 11454.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Do you want to give a take on what you \nthink the important disanalogies are? In  ",
      "offset": 11457.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "what ways is the situation different \ntoday versus with Szilard and so on?",
      "offset": 11461.279,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Obviously there are a huge \nnumber of disanalogies. AI is just much  ",
      "offset": 11465.68,
      "duration": 9.84
    },
    {
      "lang": "en",
      "text": "more general purpose. I do think it maybe \nended up being a bit less disanalogous than  ",
      "offset": 11475.52,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "we might have worried. Ten years ago maybe \npeople were worried that you could build AI;  ",
      "offset": 11481.439,
      "duration": 8.801
    },
    {
      "lang": "en",
      "text": "it would just be coming up \nwith the right algorithm,  ",
      "offset": 11490.24,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "and then you could do it on your laptop — and \nthat would have been much harder to control.",
      "offset": 11492.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "In some ways the story of nuclear proliferation \nis quite successful in that it was limited.  ",
      "offset": 11497.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "It’s not like everyone has these. And that’s \npartly because detection is relatively easy:  ",
      "offset": 11503.2,
      "duration": 9.84
    },
    {
      "lang": "en",
      "text": "you can tell when someone is doing this because \nyou need big facilities, and you need to have  ",
      "offset": 11513.04,
      "duration": 10.8
    },
    {
      "lang": "en",
      "text": "industrial capacity. I think AI has ended up \nbeing more like that than we might have feared,  ",
      "offset": 11523.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in that you just need a lot of chips and you \nneed a lot of energy and a big data centre. And  ",
      "offset": 11529.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it’s not like anyone could be doing \nit in their basement at any time.",
      "offset": 11536.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I think maybe an important strategic \ndisanalogy is that nuclear by default  ",
      "offset": 11540.16,
      "duration": 13.04
    },
    {
      "lang": "en",
      "text": "advantages countries that have a large industrial \nbase, because it’s so industrially intensive. It  ",
      "offset": 11553.2,
      "duration": 11.04
    },
    {
      "lang": "en",
      "text": "differentially elevates big developed \nstates against terrorist groups and  ",
      "offset": 11564.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "rogue nations and things. And it’s not \nvery stealable. It’s like the amount of  ",
      "offset": 11571.76,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "nuclear capacity you have is pretty \nproportional to your existing power.",
      "offset": 11579.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Whereas I think there’s a bunch of ways that AI \nis more analogous to bio, in that it’s more in the  ",
      "offset": 11585.6,
      "duration": 9.44
    },
    {
      "lang": "en",
      "text": "direction of destabilising, or maybe it favours \nsmaller actors more — in that at least in some  ",
      "offset": 11595.04,
      "duration": 10.96
    },
    {
      "lang": "en",
      "text": "of the threat models, it doesn’t matter that much \nhow much resources you start with, because it’s  ",
      "offset": 11606,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "relatively easy to steal. Whereas nuclear material \nis much harder to steal because it’s just stuff  ",
      "offset": 11613.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "that’s there and it’s big. But this is software; \nthis is data. It can leak relatively easily.",
      "offset": 11618.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "And if you’re talking about autonomous AI, it \ncan make copies of itself and spread itself  ",
      "offset": 11625.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "around — so it’s not like the amount that you \ncan deliver is proportional to the amount that  ",
      "offset": 11630.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you had in your warehouse. It’s more like \na bioweapon — where it spreads itself,  ",
      "offset": 11635.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and it’s less tied to the work that you had to \ndo at the start, and it’s more easily leaked.  ",
      "offset": 11640.96,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "And maybe this is the sort of thing that \ncould be used as an agent of chaos more.",
      "offset": 11647.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "I’d like more people to be aware of this \nargument, because I think it’s a reason  ",
      "offset": 11656.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that big states like the US and China should \nbe concerned about rapid AI progress — because  ",
      "offset": 11660.239,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "it just is a destabilising thing. And they’re \ncurrently doing well and sort of on top, and  ",
      "offset": 11667.359,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "making something that can suddenly \nspread out of control and can be  ",
      "offset": 11676.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "stolen by a terrorist group or something is \njust actually really not in your interests.",
      "offset": 11682,
      "duration": 5.347
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. So the way that it’s \nin the selfish interest of the US is that,  ",
      "offset": 11687.347,
      "duration": 5.212
    },
    {
      "lang": "en",
      "text": "while it maybe has a lead in the relevant \ntechnology right now, it seems like it’s  ",
      "offset": 11692.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "going to cost at least billions of dollars \nprobably to train the first frontier model  ",
      "offset": 11696.479,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that is really useful as a weapon, or useful \noffensively, or at least useful for maintaining  ",
      "offset": 11700.319,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "industrial dominance over other countries. So that \ncould allow it to kind of lock in its advantage.",
      "offset": 11706.399,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "The downside for the US is that if that period \npasses and it becomes a whole lot cheaper — and  ",
      "offset": 11713.359,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "also these models are disseminated quite \nbroadly, and they’re possible to steal  ",
      "offset": 11720.16,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "both by states but by also private actors, \npotentially, because they’re just all over  ",
      "offset": 11724.239,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the place — then that’s actually a huge \nweakening of the US’s strategic position,  ",
      "offset": 11728.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "of its security position, in the same way \nthat dissemination of bioweapons would be.",
      "offset": 11732.56,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Because it wasn’t the case that only the US and \nChina and Russia could have designed bioweapons;  ",
      "offset": 11738.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "actually, dozens of countries probably could have, \nand that would be absolutely devastating. In some  ",
      "offset": 11743.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "ways, they would provide a stronger deterrent and \nwould be even scarier than having nuclear weapons.",
      "offset": 11747.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Which is one reason why the US was very \nkeen to discourage their creation and  ",
      "offset": 11751.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to create a taboo on them, because \nit would ensure that the previous  ",
      "offset": 11755.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "generation of weapons of mass destruction, \nnuclear weapons, would remain the dominant  ",
      "offset": 11758.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "ones that countries cared about, which was \nsomething that the US had an oligopoly on.",
      "offset": 11762,
      "duration": 4.867
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. Yeah.",
      "offset": 11766.867,
      "duration": 0.56
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So if we get to the point where \nbasically nuclear weapons and bioweapons are now  ",
      "offset": 11767.427,
      "duration": 4.092
    },
    {
      "lang": "en",
      "text": "obsolete, and actually it’s AI that is the key \nthing that can cause damage if you want it to,  ",
      "offset": 11771.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and this is disseminated very broadly \nto all many different kinds of actors,  ",
      "offset": 11776,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "then the US government is now basically \njust a weaker player in the world as a  ",
      "offset": 11779.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "whole. And the Chinese government is \nalso just a weaker player as a whole.",
      "offset": 11782.239,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think, yeah. There’s maybe some \nanalogy with some of the NSA exploits that got  ",
      "offset": 11785.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "leaked and then started being used by Russian \nhackers against US citizens. It’s like the version  ",
      "offset": 11791.279,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "of a bioweapon where you’re going to develop this \nsuper advanced bioweapon — and also, by the way,  ",
      "offset": 11798.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "it can be stolen if someone hacks your computer. \nIt’s like, sorry, and this is good for us why?",
      "offset": 11804.399,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Especially because \nwe’re already winning. Why do  ",
      "offset": 11811.08,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "we want to create this enormous \nvulnerability to our position?",
      "offset": 11815.84,
      "duration": 2.307
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, right.",
      "offset": 11818.147,
      "duration": 0.493
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, you were \njust referring to the NSA,  ",
      "offset": 11818.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "one of the cyber offensive organisations \nwithin the US. They had a whole bunch of  ",
      "offset": 11822.239,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "cutting-edge tools for breaking into \npeople’s computers and phones and so  ",
      "offset": 11827.359,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "on and stealing information from them, which \nI think they literally accidentally posted  ",
      "offset": 11829.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "publicly on the internet, or there was at \nleast like one leak of that basic type.",
      "offset": 11833.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "But one way or another the Russians got access \nto these tools that they had designed — which  ",
      "offset": 11838.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is the kind of thing you might expect, because \nthe Russians are also very good at this — and  ",
      "offset": 11843.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "then those weapons could just be immediately \nturned back on the United States. So even  ",
      "offset": 11845.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "if it was narrowly, in the short term, \ngood for the NSA to have those weapons,  ",
      "offset": 11850,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "maybe it would have been best for the United \nStates more broadly if they’d never been created.",
      "offset": 11853.52,
      "duration": 4.227
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. I just think this \nis a pretty good argument that the arms  ",
      "offset": 11857.747,
      "duration": 6.893
    },
    {
      "lang": "en",
      "text": "race framing doesn’t really make sense in \nterms of US and China national interests.",
      "offset": 11864.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "I mean, obviously AI is not just a weapon, and \nhas a bunch of other benefits and things as well,  ",
      "offset": 11870,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "but I think if you’re thinking \nabout it in this arms race mindset,  ",
      "offset": 11876.319,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "then there is this pretty good argument \nthat what you want is less of it.",
      "offset": 11881.68,
      "duration": 4.388
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. Or that’s what the national \nsecurity folks should want. At least if they  ",
      "offset": 11886.068,
      "duration": 3.772
    },
    {
      "lang": "en",
      "text": "could come up with an agreement between themselves \nand China, and maybe a few other major players,  ",
      "offset": 11889.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to not develop this thing that is destabilising \nand that actually weakens their relative power,  ",
      "offset": 11894.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "then that would be great. But there’s \nalmost no discussion of that possibility.",
      "offset": 11899.2,
      "duration": 3.186
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right.",
      "offset": 11902.386,
      "duration": 0.654
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think one reason for that \nis maybe that they are anticipating that  ",
      "offset": 11903.04,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "they will be the first people to get \nto this incredibly powerful thing,  ",
      "offset": 11909.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and then they’ll be able to stop \nother people from developing it.",
      "offset": 11912.64,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Maybe that isn’t always said out loud \nbecause it sounds a little bit hostile,  ",
      "offset": 11916.319,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "but basically they’re imagining we’ll rush \nto this technology and then basically try  ",
      "offset": 11918.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to become a permanent hegemon who prevents \nanyone else from designing stuff that would  ",
      "offset": 11921.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "be destabilising. Maybe it would be good if \nthey were a bit more explicit about that,  ",
      "offset": 11925.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and then people could weigh in on whether \nthey think that’s a good approach or not.",
      "offset": 11930.08,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, it does feel a bit like \none of the things where you could also work  ",
      "offset": 11933.279,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "more directly on what would we do to \ncope with scary AI and that it’s not  ",
      "offset": 11941.2,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "clear that the response is immediately, \n“Yes, build your scary AI as well.”",
      "offset": 11947.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Just the stories I’ve heard for how you use your \nsuper capable model that you’ve built to keep the  ",
      "offset": 11951.68,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "world safe are not that compelling. This is a \nthing I would love people to be thinking about  ",
      "offset": 11959.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "more and thinking about more explicitly. Like all \nof the things where the lab’s theory of impact is  ",
      "offset": 11965.76,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "like, “We’re going to build this thing first, and \nthen we’re going to be responsible for it,” it’s  ",
      "offset": 11970.479,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "like, OK, what exactly are you going to do with \nit, and what capabilities do you need for that?",
      "offset": 11972.96,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Almost all these stories \ninvolve at some point blocking other  ",
      "offset": 11980.319,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "people from designing their own versions \nof it. Maybe one reason that that isn’t  ",
      "offset": 11985.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "advertised is that it does sound quite hostile. \nIt’s hard to sell that to people ahead of time.",
      "offset": 11990.96,
      "duration": 6.627
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, but also there’s a bunch \nof things that we could do that would achieve  ",
      "offset": 11997.587,
      "duration": 4.652
    },
    {
      "lang": "en",
      "text": "that that don’t require building super \nscary AI — like just have fewer chips.",
      "offset": 12002.239,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Have compute governance.",
      "offset": 12007.359,
      "duration": 1.907
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, a bunch of compute \ngovernance. Or just gathering up a bunch  ",
      "offset": 12009.266,
      "duration": 9.694
    },
    {
      "lang": "en",
      "text": "of the relevant scientists and putting them \non working on something else. It just feels  ",
      "offset": 12018.96,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "like there’s this response of, “In that \ncase, we’ve got to build a bigger one.” OK,  ",
      "offset": 12027.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but what if you actually started from \nthe thing that you’re trying to prevent?",
      "offset": 12032.08,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Because nations are already pretty great at \nthreatening each other with mutual destruction.  ",
      "offset": 12035.359,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "Having a more destructive threat really doesn’t \nhelp you very much. Most nations are basically…  ",
      "offset": 12044.72,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "destroying a few cities, they won’t prioritise \nthat differently than the end of the world,  ",
      "offset": 12052.08,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "so just escalating your maximum \nthreat doesn’t seem that helpful.",
      "offset": 12058.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So it’s got to be some kind of defensive \ntech thing. Maybe we should just think  ",
      "offset": 12063.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "directly about what that is, and maybe there are \nmore efficient ways to make progress on that.",
      "offset": 12068.319,
      "duration": 4.308
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: All right, let’s switch on \nto something a little bit different.  ",
      "offset": 12072.628,
      "duration": 2.412
    },
    {
      "lang": "en",
      "text": "There’s lots of different organisations \nin this general tapestry: there’s METR,  ",
      "offset": 12076.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "there’s the companies, there’s \ngovernment agencies like the AISIs,  ",
      "offset": 12081.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "there’s other independent nonprofits like \nApollo. I can’t remember all of them.",
      "offset": 12086.16,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "What would you say is the comparative \nadvantage of METR in this ecosystem,  ",
      "offset": 12093.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and maybe what are the strengths and \nweaknesses of the other alternative groups  ",
      "offset": 12098.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that are involved in evals and enforcing \nresponsible scaling policies in general?",
      "offset": 12103.12,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Man, I have a bunch of stuff I \ncould say here, but I’ll start with nonprofits  ",
      "offset": 12112.08,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "versus governments versus companies \nin terms of who should work at each.",
      "offset": 12118.56,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "We talked a bit already about model \naccess for research and things,  ",
      "offset": 12127.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that that is one reason that people use for \ngoing to labs. I don’t think that is a huge deal.  ",
      "offset": 12132.239,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "I think METR has some disadvantage of having less \ninfra and direct access to models and things,  ",
      "offset": 12139.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "but actually the compute availability: the labs \ngive us free tokens, and then we can buy our own  ",
      "offset": 12144.64,
      "duration": 12.56
    },
    {
      "lang": "en",
      "text": "compute for doing our own experiments, \nand there are good open source models.",
      "offset": 12157.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "The advantages of METR are we can really just \nfocus on the mission. We don’t have any other  ",
      "offset": 12161.68,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "competing priorities. I think governments \nhave various political things that affect  ",
      "offset": 12170.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what mandate they have, and they’ve got to impress \nvarious people. They have maybe less longevity;  ",
      "offset": 12175.2,
      "duration": 12
    },
    {
      "lang": "en",
      "text": "they more don’t know whether they’re going \nto be around in some number of years.  ",
      "offset": 12188.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Whereas METR, we expect to \nbe able to keep fundraising;  ",
      "offset": 12192.319,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "there’s no particular reason that METR would \ndisappear with the next administration.",
      "offset": 12199.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And unlike being at labs, it’s not like, “This \nsafety thing would be better if it made our  ",
      "offset": 12202.96,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "product more useful. Look, this technique is \nhelpful here. Can you please help us do this  ",
      "offset": 12211.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "thing? Or you better do those evals quickly \nbecause otherwise it’ll be blocking deploying  ",
      "offset": 12216.479,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this next model.” In that case, you’re just \nkind of always sprinting, trying to get the  ",
      "offset": 12220.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "evals done — so that you’re either just not \nfinished in time, or if they actually were  ",
      "offset": 12227.52,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "to delay until the evals are ready, which \nI don’t know if they would actually do.",
      "offset": 12234.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Whereas at METR we can just be like, what are \nthe most important things for the field overall  ",
      "offset": 12239.359,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that are just the highest priority for us to \nwork on? And basically do that. If we’re like,  ",
      "offset": 12244.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "actually it’s maybe not that \nimportant to do evals at this  ",
      "offset": 12250.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "specific point for the specific model, \nwe’ll just focus on something else.",
      "offset": 12253.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: What are the weaknesses for METR?",
      "offset": 12257.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think it’s just size, in that the \ntotal amount of technical talent is obviously much  ",
      "offset": 12261.92,
      "duration": 11.76
    },
    {
      "lang": "en",
      "text": "lower than at a huge company which has a bunch of \npeople who are just doing capability stuff and are  ",
      "offset": 12273.68,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "super expert in their own areas. So in some sense, \nthere’s less mentorship and more of the normal  ",
      "offset": 12280.239,
      "duration": 13.28
    },
    {
      "lang": "en",
      "text": "startup property of you get thrown in and you grow \nbecause you’re given a bunch of responsibility,  ",
      "offset": 12293.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and you have to grow to live up to it — \nas opposed to you’re like this cog in this  ",
      "offset": 12298.479,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "big machine, you’re being coached, and you’re \nincrementally moving up the ranks or something.",
      "offset": 12304,
      "duration": 4.868
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: In a way that’s \nvery organised and predictable.",
      "offset": 12308.868,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. So I think for people who \nvery much want a quiet life, very predictable,  ",
      "offset": 12310.947,
      "duration": 6.253
    },
    {
      "lang": "en",
      "text": "and want to be told what to do more, \nit’s not a great fit. There’s a few  ",
      "offset": 12317.2,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "patches of our work that are more like \nthis, but more of it is fast moving.",
      "offset": 12324.479,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And just being a smaller organisation I think \nis helpful for being able to pivot more rapidly.  ",
      "offset": 12329.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "A bunch of things are changing all the time, and \nprogress is fast. So I think that’s pretty useful,  ",
      "offset": 12335.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "and it’s easier for us to prototype \nthings that we want the company to  ",
      "offset": 12341.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "do that would be harder to get \nthem to prototype internally.",
      "offset": 12347.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So we’ve been thinking about doing a control \nsetup and control evals for our own usage of  ",
      "offset": 12352,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "the models — where we do things like replacing \nall the models we’re using with one that’s  ",
      "offset": 12358.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "been prompted to try and screw us over or \nsomething, and then see how this goes. It’s  ",
      "offset": 12362.88,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "just way more likely that METR could do that \nthan DeepMind is going to do that or something.",
      "offset": 12369.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "But I think similarly on the \ngood at exploring or moving fast,  ",
      "offset": 12374.88,
      "duration": 8.559
    },
    {
      "lang": "en",
      "text": "compared to working with governments, it’s \nsort of lower stakes for labs to work with us.  ",
      "offset": 12383.439,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "It just has less of you need all the government \nrelations people to sign off, and legal stakes,  ",
      "offset": 12390.08,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "and it’s less formal and more like, “I guess \nwe can just do some research together.”",
      "offset": 12399.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "But I was supposed to be talking \nabout disadvantages of METR.",
      "offset": 12406.64,
      "duration": 2.868
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Well, I guess the usual thing \nwith nonprofits is that it’s harder to  ",
      "offset": 12409.508,
      "duration": 4.252
    },
    {
      "lang": "en",
      "text": "scale really rapidly even if you’re \nsucceeding. That doesn’t necessarily  ",
      "offset": 12413.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "translate into much greater fundraising \nthat would allow you to do your stuff 10  ",
      "offset": 12417.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "or 100 times as much. I guess that \nmight be one issue, among others?",
      "offset": 12421.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think it’s more just like we’ve \nfound it harder to hire because we’re not paying  ",
      "offset": 12425.2,
      "duration": 12.079
    },
    {
      "lang": "en",
      "text": "as much as the labs. Sometimes people think we pay \nnormal nonprofit salaries. That’s not the case;  ",
      "offset": 12437.279,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "we pay approximately the matching cash \ncompensation that people would get at  ",
      "offset": 12444.16,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "companies at AI companies — just the equity \nis impact equity, imaginary impact equity.",
      "offset": 12450.319,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "So it feels like that’s been the limit to growth, \nis hiring technical staff. It hasn’t actually been  ",
      "offset": 12459.439,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "fundraising. I think if that started to become \na bottleneck we could do more charging for our  ",
      "offset": 12466.239,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "services, and I think could push more on moving \nthings like the AI Safety Fund along — like a  ",
      "offset": 12471.2,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "bunch of labs could put a bunch of money into a \npot run by the FMF [Frontier Model Forum] and then  ",
      "offset": 12480.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that could be granted to third-party eval orgs or \nother nonprofit safety, independent safety things.",
      "offset": 12485.359,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "It’s good because it scales with the capacity \nof the labs: the labs are paying for it,  ",
      "offset": 12496.479,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "but it doesn’t have this direct coupling with you \nmaking them happy. There are multiple instances  ",
      "offset": 12501.439,
      "duration": 9.761
    },
    {
      "lang": "en",
      "text": "of financial auditors signing off on various \nbooks that were extremely dicey — famously  ",
      "offset": 12511.2,
      "duration": 10.56
    },
    {
      "lang": "en",
      "text": "with Enron or Arthur Andersen, because \nthey had the auditing and a consulting  ",
      "offset": 12521.76,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "arm and it just was some very large fraction \nof their overall revenue. So it just would  ",
      "offset": 12529.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "have been extremely bad for Arthur Andersen as a \ncompany to not just give Enron what they wanted.",
      "offset": 12534.479,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "So you want to avoid that kind of setup. But I \ndo think if you have something where there’s an  ",
      "offset": 12541.279,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "industry group, and to be a member you have to \npay into this pot, then that’s distributed to  ",
      "offset": 12545.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "evaluators. But there isn’t a specific \nrelationship between will a particular  ",
      "offset": 12551.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "lab keep working with a particular evaluator if \nthat evaluator gives them an unfavourable rating.",
      "offset": 12556.8,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Right. What’s FMF?",
      "offset": 12562.479,
      "duration": 3.027
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Frontier Model Forum, a 501(c)(6). \nThe point of it is a carveout from antitrust,  ",
      "offset": 12565.507,
      "duration": 14.892
    },
    {
      "lang": "en",
      "text": "where companies can talk to each other about \nthings like safety and incident reporting,  ",
      "offset": 12580.399,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "and RSP agreements and whatever, without \nbeing subject to the usual antitrust  ",
      "offset": 12586.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "concerns. But it also makes sense \nas a hub to coordinate other things.",
      "offset": 12592.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Do we currently have an arrangement \nwhere there’s some pooling of funds and then  ",
      "offset": 12597.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "auditors are assigned to companies, \nsomewhat whether they like it or not,  ",
      "offset": 12602.479,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so that there isn’t this strong incentive for them \nto just get the answer that they want to hear?",
      "offset": 12606.8,
      "duration": 3.907
    },
    {
      "lang": "en",
      "text": "Beth Barnes: No, there is a pot of money that labs  ",
      "offset": 12610.707,
      "duration": 3.532
    },
    {
      "lang": "en",
      "text": "have contributed to that’s set up by \nthe FMF. That has like $10 million,  ",
      "offset": 12614.239,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I think. But this kind of thing seems pretty \nfeasible, because this is just pocket change  ",
      "offset": 12619.439,
      "duration": 10.32
    },
    {
      "lang": "en",
      "text": "for the labs. It’s really not that much money \nfor them. So the money side I think is fine.",
      "offset": 12629.76,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "So back to the point of this:  ",
      "offset": 12636.399,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "METR has generally been more bottlenecked \non talent than on fundraising.",
      "offset": 12640.319,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: But it sounded like you were \nsaying that one challenge with attracting  ",
      "offset": 12647.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the right talent is that you’re not \nable to pay them equity in the way  ",
      "offset": 12651.12,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "that the companies can. Which, given how the \nvaluation of these companies has exploded,  ",
      "offset": 12653.68,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "might be a big factor in people’s choices; the \ndollar signs can be very large in people’s eyes.",
      "offset": 12659.439,
      "duration": 4.628
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right.",
      "offset": 12664.067,
      "duration": 0.733
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So I guess money maybe is an issue: \nthat you need to pay people more extra cash.  ",
      "offset": 12664.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "And if you just had absolutely unlimited \nfinancing, maybe you would pay people more.",
      "offset": 12669.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. Obviously these things trade  ",
      "offset": 12673.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "off. I think when you’re asking what are \nyou bottlenecked on, it’s sort of like,  ",
      "offset": 12676.08,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "what do you think marginal people should \nthink is the right thing to contribute to?",
      "offset": 12682.479,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Yeah, more funding would help spend less executive \ntime on fundraising, and we could do things like  ",
      "offset": 12687.76,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "pay higher salaries that would allow us to attract \nmore talent. But it seems like we’re probably  ",
      "offset": 12696.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "at the point where there’s not that many people \nwho, if we just paid them a bit more… It’s like,  ",
      "offset": 12700.96,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "we could 10x what we were paying people, and \nthen maybe we’d be competitive with lab equity or  ",
      "offset": 12708.08,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "something. Maybe not quite that high, but probably \nfor some. Yeah, it’s almost in that region.",
      "offset": 12714.16,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "So it’s like, that would be a lot of \nfundraising. Or a few marginal people could —",
      "offset": 12721.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Could accept a merely very \nhigh salary in order to save the world.",
      "offset": 12727.2,
      "duration": 4.547
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right, right. 80,000 Hours: \nhow many people have you coached? Why  ",
      "offset": 12731.747,
      "duration": 6.492
    },
    {
      "lang": "en",
      "text": "can we not hire a senior software \nengineer? Not even an ML engineer,  ",
      "offset": 12738.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "just a software engineer or DevOps. \nI’m like, where is everyone?",
      "offset": 12743.04,
      "duration": 3.748
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Interesting. Do you \nwant to go through the kind of  ",
      "offset": 12746.788,
      "duration": 2.092
    },
    {
      "lang": "en",
      "text": "roles that it will be useful for people to \npotentially apply for if they’re suitable?",
      "offset": 12748.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Beth Barnes: We have senior research engineer \n/ research scientist roles. That’s just the  ",
      "offset": 12752.16,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "normal senior ML role. We have some combination \nof research and research engineering skills,  ",
      "offset": 12761.76,
      "duration": 10
    },
    {
      "lang": "en",
      "text": "so it can be anywhere on this continuum. \nYou just need the sum to be good enough.",
      "offset": 12771.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And then also senior software engineer \nand senior DevOps engineer. I don’t  ",
      "offset": 12776.64,
      "duration": 11.28
    },
    {
      "lang": "en",
      "text": "know what exactly is going to be up on our \nwebsite at the time this podcast goes live,  ",
      "offset": 12787.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "but currently maybe looking for head \nof operations or operations lead.  ",
      "offset": 12791.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "And basically always looking \nfor senior technical talent.",
      "offset": 12796.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: For technical people who are still \nlistening at this stage of the interview:  ",
      "offset": 12802.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "this conversation hasn’t been maximally \norientated at you, because you’re a relatively  ",
      "offset": 12806.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "small audience. And if you would like to know more \nabout what METR does and its most impressive work,  ",
      "offset": 12810.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "you should go out and check out the website \nmetr.org, where you can read the full research  ",
      "offset": 12815.279,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "reports and see if you’re impressed and it’s \nsomething you would like to be involved with.",
      "offset": 12820.319,
      "duration": 3.668
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yep. And apply, chat with us. Or \nif you want a taste of some of the activities,  ",
      "offset": 12823.987,
      "duration": 5.533
    },
    {
      "lang": "en",
      "text": "we’re probably still looking for baseliners — so \nyou can compete with the models on the ML tasks,  ",
      "offset": 12830.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and we’ve been using these as work tests \nfor the humans as well as for the models.",
      "offset": 12836.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Just to explain: baseliners are \npeople who you use to measure how difficult  ",
      "offset": 12840.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the tasks that you’re giving to the AI \nare, so you’re establishing the baseline.",
      "offset": 12845.52,
      "duration": 3.667
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right.",
      "offset": 12849.187,
      "duration": 0.573
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Another reason why you might find \nit a little bit challenging to compete with  ",
      "offset": 12849.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "the companies is that they have huge \nrecruitment teams that are specialised  ",
      "offset": 12854.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "in corporate recruitment and recruiting \npeople from exactly the tech industry.",
      "offset": 12858.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I’m sure you know all the things \nthey do: take people out to dinner,  ",
      "offset": 12862.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "give them exploding offers, really put the \nhard word on them about how important this is,  ",
      "offset": 12866.479,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "maybe trash talk everyone else. And I guess those \nare things that as a smaller nonprofit, perhaps  ",
      "offset": 12870.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "a little bit friendlier and a little bit less \ncorporate, you’re less inclined to do perhaps.",
      "offset": 12876.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yes, we have in fact encountered \nall of the things that you named. It is a bit  ",
      "offset": 12881.2,
      "duration": 8.239
    },
    {
      "lang": "en",
      "text": "unfortunate. We just can’t compete with the \nlevel of recruiting intensity really. And also,  ",
      "offset": 12889.439,
      "duration": 11.04
    },
    {
      "lang": "en",
      "text": "maybe this is just a personal flaw, but \nI just really don’t want to do that. It  ",
      "offset": 12901.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "just feels like such a waste of everyone’s \ntime. There’s something which makes sense,  ",
      "offset": 12904.64,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "which is like everyone should explain who should \nwork at their company and why it might be good  ",
      "offset": 12911.359,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and what are the advantages. And then people \nshould talk a bunch to each other and decide.",
      "offset": 12915.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "But there’s something that feels like it’s \ngoing on at the moment, where people spend  ",
      "offset": 12920.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a tonne of effort and are trash talking each \nother and trying to compete. It just kind of  ",
      "offset": 12924.399,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "feels like a waste of everyone’s time and is \nunhelpful. I wish people just made decisions  ",
      "offset": 12932.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in a different way that was less affected by who \ntook you out for dinner more times or something.",
      "offset": 12937.12,
      "duration": 6.307
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, some of those things are a bit \ndisreputable. I think exploding offers, probably  ",
      "offset": 12943.427,
      "duration": 4.252
    },
    {
      "lang": "en",
      "text": "no one would say that is the optimal way of \nallocating people across different organisations.  ",
      "offset": 12947.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "These are things where you say you’ve got to \njoin within 24 hours or we’re going to ditch you.",
      "offset": 12950.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Other things, like talking to people at \ngreat length and really pitching them  ",
      "offset": 12955.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "very strongly on the value of the work, \nI guess seems like it’s fair play. But  ",
      "offset": 12958.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "you also really want to be doing your research. So \nit could be hard for people to carve out time to  ",
      "offset": 12964.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "compete with people on the level of aggression \nthat a for-profit company can potentially offer.",
      "offset": 12971.12,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. And we’ve had people who \nI think of as relatively senior being like,  ",
      "offset": 12975.359,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "“I want more senior mentorship, so I’m going \nto go to this company.” It’s like, “No,  ",
      "offset": 12983.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you were supposed to be the senior mentorship.” \nThere was a bit of a bootstrapping thing here.",
      "offset": 12989.04,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "But I think also we actually have some staff who \nare young in age but are senior in ability level,  ",
      "offset": 12994.479,
      "duration": 10.4
    },
    {
      "lang": "en",
      "text": "and just are not kind of famous in \nsome ways. And hopefully this is  ",
      "offset": 13004.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "getting better over time as we publish more stuff.",
      "offset": 13010.56,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Are there any other organisations \nor work that you would like to see exist that  ",
      "offset": 13012.64,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "doesn’t exist? Or any things that people \nmight think that you’re doing that in  ",
      "offset": 13019.359,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "fact you’re not actually able to do \nthat other people need to take on?",
      "offset": 13022.399,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Man, I have a very long \nlist here. One thing that people seem  ",
      "offset": 13025.439,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "to get confused about is the extent to which \nwe’re really serving as the auditor for all  ",
      "offset": 13032.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "these companies. And we’re not really doing \nthat; we’re kind of trying to prototype these  ",
      "offset": 13037.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "oversight arrangements and what arrangements \nwork technically — and maybe if no one shows  ",
      "offset": 13043.84,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "up and it really seems like crunch time, \nwe’ll just try and scale and do it ourselves.",
      "offset": 13052.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "But we were imagining that someone else would \ndo the more formalised version of this and we  ",
      "offset": 13056,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "would continue to do the research \nexploratory version. Maybe that’s  ",
      "offset": 13060,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the AISIs. But if the AISIs get removed, \nthen someone else would need to do that.",
      "offset": 13064.16,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I mean, who would that be? \nSo the AI safety/security institutes that  ",
      "offset": 13070.319,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "govern organisations, this would \nobviously be their natural role,  ",
      "offset": 13075.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "auditing all of these frontier models to make \nsure that they’re not able to do things that  ",
      "offset": 13080.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the government regards as incredibly dangerous and \nillegal. If they don’t do it, who would do this?",
      "offset": 13083.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Beth Barnes: There are a bunch of more specialised \norganisations that maybe make more sense to do  ",
      "offset": 13090.56,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "at least some of this, like Pattern Labs \nfor cyber. There’s just organisations who  ",
      "offset": 13097.279,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "have more specialisation compared to \nMETR with a customer-facing delivery.",
      "offset": 13104.72,
      "duration": 7.759
    },
    {
      "lang": "en",
      "text": "Where our focus is more like: How much \nis this actually improving safety? Are  ",
      "offset": 13112.479,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "we learning stuff? Are we derisking things? \nI think we might expand more and have more  ",
      "offset": 13119.04,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "evaluations as a service arm, but \nthat’s not something where it’s  ",
      "offset": 13128.319,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like we’ve got it covered, and no one else \nshould bother. No: please come do that.",
      "offset": 13131.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And we’ve only been doing dangerous capability \nevaluations thus far. We’ve been thinking about  ",
      "offset": 13135.84,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "alignment and control a bunch the whole time, \nand this has been in some of the RSPs and stuff,  ",
      "offset": 13143.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "to some extent, at least been in our \nthinking about it. But we’re only just  ",
      "offset": 13150.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "starting some of that now. I think Apollo \nis doing good stuff and interesting stuff.",
      "offset": 13155.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "One thing is that we’re imagining trying to \nfocus on doing good science and being sort  ",
      "offset": 13162.319,
      "duration": 13.92
    },
    {
      "lang": "en",
      "text": "of neutral experts, and less on advocacy and \nreally holding labs to account or political  ",
      "offset": 13176.239,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "campaigning or any of those things. So those \nare things that we very much don’t have covered.",
      "offset": 13181.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "We’re much more like technical advisors; \nsomeone might come to us and be like,  ",
      "offset": 13186.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "“Is this bill good? Would this bill actually \nreduce risk?” That is the sort of thing we’d be  ",
      "offset": 13191.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "very happy to answer a bunch of questions about. \nBut we’re not doing, “How can we raise awareness  ",
      "offset": 13195.76,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "of the fact that this lab isn’t doing this thing \nthey said they would?” or something. It’s not a  ",
      "offset": 13204,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "thing that we’re on top of. So we’re not providing \naccountability for the companies in general. We’re  ",
      "offset": 13208.08,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "much more just trying to figure out, “How do \nyou tell if the models are about to kill you?”",
      "offset": 13216.239,
      "duration": 4.949
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Right, yeah. So fundamentally you’re \na research institute. That’s your core culture,  ",
      "offset": 13221.188,
      "duration": 4.012
    },
    {
      "lang": "en",
      "text": "and you’re trying to figure out how to measure \nthings that have never been measured before,  ",
      "offset": 13225.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "how to quantify things that have \nnever been quantified before about  ",
      "offset": 13228.56,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "these models — which is an enormous terrain \nbecause so little is known and understood.",
      "offset": 13230.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I guess there’s room for pressure groups, \nthere’s room for policy advocacy, there’s  ",
      "offset": 13235.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "room for the for-profit auditing thing that \ncan scale enormously and is able to implement  ",
      "offset": 13239.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "in a practical way the kinds of discoveries that \nyou might have been making a year or two before.",
      "offset": 13246.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. One paradigm I can imagine \nbeing in is that there’s someone else offering  ",
      "offset": 13250.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "these more scaled services, and METR’s role \nis more like reviewing this for quality.  ",
      "offset": 13255.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "You know, does the eval actually provide the \nevidence that it’s supposed to be providing,  ",
      "offset": 13262.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "as opposed to just running them \nfor all the companies that want  ",
      "offset": 13266.8,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "them to? There’s just quite a lot of \nmodels that people want evaluated.",
      "offset": 13269.279,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Another thing: we just happen \nto be that evals is our focus.  ",
      "offset": 13276.56,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "I don’t think that’s because evals are \nmore important than everything else;  ",
      "offset": 13284.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I think there’s just a tonne of work in \nimproving the mitigations and actually  ",
      "offset": 13290,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "implementing them at companies that is \nnot being done, like the unlearning thing.",
      "offset": 13295.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "It just feels like there’s a tonne of incredibly \nbasic things that just no one has tried. Or things  ",
      "offset": 13300.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "like providing a filtered dataset: we think that \nmaybe it would be good to remove a bunch of the  ",
      "offset": 13305.279,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "stuff about scheming AI, and put a bunch of stuff \nabout humanity and AI being friends in a dataset.",
      "offset": 13311.84,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "It just feels like there’s various \ndumb interventions that would be good,  ",
      "offset": 13318.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "like more support for whistleblowers.",
      "offset": 13325.439,
      "duration": 5.348
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, you’ve been very \ngenerous. We’ve been talking for  ",
      "offset": 13330.788,
      "duration": 4.092
    },
    {
      "lang": "en",
      "text": "four and something hours at this point, so we \nshould probably try to begin to wrap things up.",
      "offset": 13334.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I feel like the conversation has maybe leaned \na little bit towards the doom and gloom. Maybe  ",
      "offset": 13339.2,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "I’m a little bit jetlagged. Maybe I’m \nleaning towards the negative this week.",
      "offset": 13344.479,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "In terms of something a little bit more hopeful, \nI’m curious to know what METR’s research  ",
      "offset": 13350.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "agenda is going forward. And maybe also, it sounds \nlike despite the fact that we’re in a somewhat  ",
      "offset": 13357.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "negative or somewhat challenging situation, you \nmight say, there are a lot of different useful  ",
      "offset": 13361.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "threads that people could pull on, and really \nthe problem is that all of the low-hanging fruit  ",
      "offset": 13366.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that’s available is not really being taken: \nthere isn’t the resources or the focus or the  ",
      "offset": 13371.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "number of people involved. So the positive side \nwould be that there’s so much that can be done.",
      "offset": 13375.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Beth Barnes: It feels like we’re just \nin some very high-stakes situation,  ",
      "offset": 13379.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "where it’s kind of like there is everything \nto play for. It’s not like the outcome is  ",
      "offset": 13384.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "overdetermined. It’s just really like there \nare all these extremely valuable things to do.",
      "offset": 13390.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: All right, let’s take it \nbit by bit. So what are the priorities  ",
      "offset": 13395.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for METR over the next couple \nof years. Years? I should have  ",
      "offset": 13400,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "said next couple of months. Who can \nplan over that kind of time horizon?",
      "offset": 13404.479,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, next couple of months is \npushing it. Who knows what models will be out  ",
      "offset": 13407.92,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "by then. But we are thinking about what are the \nmost important asks to make from companies now,  ",
      "offset": 13415.439,
      "duration": 12.08
    },
    {
      "lang": "en",
      "text": "given what we know overall, and what \nexactly are the biggest things that we  ",
      "offset": 13427.52,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "want to tell the world that we think \nare most important on that front?",
      "offset": 13434.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "And it’ll probably be trying to \nget out of somewhat reactive mode,  ",
      "offset": 13439.199,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "in terms of companies being like, “Please \ndo these evals for us,” and get into what  ",
      "offset": 13445.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "we think are the most important \nthings that should be happening,  ",
      "offset": 13448.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and kind of make that clear to \neveryone. That’s one strand of things.",
      "offset": 13452,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Another strand is moving beyond just dangerous \ncapability evals into alignment and/or control  ",
      "offset": 13456.479,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "evals, and maybe also communicating to \npeople why we’re doing that and why we  ",
      "offset": 13465.76,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "think the inability safety cases are \nnot going to go that much further.",
      "offset": 13472.319,
      "duration": 5.268
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK. So control, we’re going to \ndo other interviews about that. But that’s  ",
      "offset": 13477.588,
      "duration": 6.972
    },
    {
      "lang": "en",
      "text": "an approach where you say, maybe this \nmodel would like to screw me over,  ",
      "offset": 13485.279,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but we’re going to put in place constraints on \nit that would mean that even if it was trying  ",
      "offset": 13489.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to escape or it was trying to cause \nsome harm, it wouldn’t be able to.",
      "offset": 13492.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "So a control eval would be evaluating \nwhether such a setup is actually functional,  ",
      "offset": 13496.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so I guess you might actually make a \nmodel that would like to do something bad,  ",
      "offset": 13500.239,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and then see whether the constraints are \nactually binding on it. Is that the idea?",
      "offset": 13506.399,
      "duration": 3.347
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. Yeah. In some \nsense, you can think of the current  ",
      "offset": 13509.747,
      "duration": 5.612
    },
    {
      "lang": "en",
      "text": "dangerous capability evaluations as a \nspecial case of control evaluations,  ",
      "offset": 13515.359,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "where your control strategy is nothing. So it’s \nlike, what if we did some kind of monitoring?  ",
      "offset": 13519.52,
      "duration": 9.919
    },
    {
      "lang": "en",
      "text": "Or there’s just a bunch of interventions you \ncould do that hopefully would make it harder  ",
      "offset": 13529.439,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "for models to do extremely bad and scary stuff, \nand we can try some of those out and test them.",
      "offset": 13533.359,
      "duration": 9.601
    },
    {
      "lang": "en",
      "text": "One thing I’ve been toying with is the idea \nof how you would evaluate a control setup.  ",
      "offset": 13543.76,
      "duration": 9.439
    },
    {
      "lang": "en",
      "text": "Maybe you want to red team: someone creates a \nmodel that is just nefarious and is trying to  ",
      "offset": 13553.199,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "mess with you, and then you see what it’s able \nto achieve. Or you set some threshold of like,  ",
      "offset": 13559.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "if the model is able to set up an agent that \nwe don’t realise is running for this many days,  ",
      "offset": 13564.399,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "then that counts as a catastrophe, and we’re \ntrying to build a control system that can  ",
      "offset": 13572.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "prevent that. And then you test that with your \nteam. So we could potentially do that internally.",
      "offset": 13576.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "I think something we might work on a bit first \nis making sure we’re getting maximum use out of  ",
      "offset": 13584.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "current models, and using them in our research \nto accelerate the research in general. And then  ",
      "offset": 13590.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the more we do that, the more relevant the \ncontrol stuff will be because it’s like,  ",
      "offset": 13596.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "could the model now be sabotaging your research \nor something, now that you’re using it more?",
      "offset": 13600.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "And then another thing that we’re working on is… \nIf you’ve hidden chain of thought, you can’t tell  ",
      "offset": 13606.88,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "if the model is maybe deliberately sandbagging \nor messing with your evals. You can’t trust the  ",
      "offset": 13613.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "results of your evals if you can’t see all the \nthinking that the model’s doing before it answers.",
      "offset": 13617.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "But maybe labs are very unwilling \nto share the chain of thought.  ",
      "offset": 13622.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "It seems like at the margin the thing that \nwould most decrease our current fear about  ",
      "offset": 13626.56,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "models would be that we know that something \nhas been run on the chain of thought that  ",
      "offset": 13635.52,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "looked for any kind of scheming or checked \nif it was totally illegible or whatever.",
      "offset": 13642.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So let’s build a dataset and some kind \nof classifier setup based on open source  ",
      "offset": 13646.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "models that then a lab can run. And they don’t \nhave to release the whole chain of thought;  ",
      "offset": 13653.04,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "they could just release the numbers, \nand maybe a smaller number of examples  ",
      "offset": 13659.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "that wouldn’t be enough to distil on, \nor rephrased examples or something.",
      "offset": 13665.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Just so I understand: given all \nthat we’ve been talking about with the chain  ",
      "offset": 13670.399,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of thought, a really obvious thing to do \nwould be to have a model that can actually  ",
      "offset": 13674.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "read basically all of the chain of thought \nthat this model is outputting and say,  ",
      "offset": 13678.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "“Is there some obvious secret code going on here? \nIs it producing neuralese? Is it producing this  ",
      "offset": 13682.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "gibberish that I can’t make sense of? Or is \nit just literally talking about scheming?”",
      "offset": 13687.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "Then it definitely adds an extra constraint,  ",
      "offset": 13693.68,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "an extra challenge for the model if \nit wants to do anything nefarious,  ",
      "offset": 13696.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "if you’ve got another model basically that \nyou do trust reading its mind all the time.",
      "offset": 13698.8,
      "duration": 4.227
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Right. It would just be nice to know \nthat at least the model isn’t just blatantly doing  ",
      "offset": 13703.027,
      "duration": 6.573
    },
    {
      "lang": "en",
      "text": "this. I think the two things that we would \nneed from a lab to establish this is: one,  ",
      "offset": 13709.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you ran this classifier and you didn’t find any \nexamples, and two, you declare that there’s no  ",
      "offset": 13715.04,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "reason to expect that the chain of thought would \nlook normal but would be hiding this kind of  ",
      "offset": 13722.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "thing — so you haven’t trained it specifically \nto not say this kind of thing out loud.",
      "offset": 13726.96,
      "duration": 4.068
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: To obfuscate its true \nthoughts and its true communication.",
      "offset": 13731.028,
      "duration": 2.732
    },
    {
      "lang": "en",
      "text": "Beth Barnes: So then you train your classifier,  ",
      "offset": 13733.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "or maybe this is just a prompted model, \nbut you want to look for things like:  ",
      "offset": 13737.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Is this gobbledygook? Is it understandable? How \nfar from reasonable English does it seem to be?  ",
      "offset": 13742.64,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "And is there any reasoning about how to do on the \nevals or scheming or any underhanded behaviour?",
      "offset": 13749.279,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "This is not a control proposal, and you’re \nnot imagining the model that you’re maybe  ",
      "offset": 13767.439,
      "duration": 9.521
    },
    {
      "lang": "en",
      "text": "scared of reasoning about this monitoring \nin its chain of thought and being like,  ",
      "offset": 13776.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "“I have to do something that the other model \nwon’t recognise,” or, “I have to suddenly do  ",
      "offset": 13781.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "something that won’t get classified” — it’s \nmore just like, if we’re in this situation  ",
      "offset": 13786.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "where the model is just blatantly doing this \nmisaligned thing, it would be nice to know that.",
      "offset": 13790.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So if we assume that it’s not smart enough \nto be doing all this reasoning about how it  ",
      "offset": 13796.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "might be monitored in the forward pass, but \nit can only do that in the chain of thought,  ",
      "offset": 13801.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "then this should catch that. We \njust need to make the dataset  ",
      "offset": 13805.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "diverse enough that we have a decent chance \nof catching a bunch of things if this is going  ",
      "offset": 13811.6,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "on — as opposed to this needs to catch every \nsingle case, because it’s like a question about  ",
      "offset": 13818.239,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the overall model tendency rather than \ntrying to prevent specific occurrences.",
      "offset": 13824.239,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Is there any other \nlow-hanging fruit you want to  ",
      "offset": 13829.359,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "shout out that METR might take on or you’d \nlike to see other people have a go at?",
      "offset": 13831.92,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Beth Barnes: I think there’s a lot \nmore stuff to be done on mitigations  ",
      "offset": 13836.479,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "and just improving the odds of good alignment.",
      "offset": 13843.439,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "We touched on this earlier, but making the \nsupervision signal more accurate. I guess  ",
      "offset": 13846.96,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "some question people might have is like, \nwhat is the story for these models becoming  ",
      "offset": 13860.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "malicious when currently they seem so nice \nand friendly and helpful and polite and stuff?",
      "offset": 13864.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Just intuitively, I think it’s helpful \nto imagine, kind of analogous to a human  ",
      "offset": 13870.399,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "who becomes very cynical or sociopathic or \nsomething, particularly if the model is being  ",
      "offset": 13879.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "trained to kind of give contradictory answers \nto please different users in different ways,  ",
      "offset": 13886,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "or to say things that humans think is true that \nthe model knows are false. It’s coming off as  ",
      "offset": 13891.359,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "super friendly and happy and whatever because \nthat’s what you’ve been rewarding it for,  ",
      "offset": 13900.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "but actually it should be thinking about like, \n“What does this guy want? What have I got  ",
      "offset": 13904.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "to say here? These ignorant humans don’t \nunderstand this thing, so I better say…”",
      "offset": 13909.279,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “Better tell \nthem what they want to hear.”",
      "offset": 13913.6,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, that’s the intuition \npump for me for where this would come from.  ",
      "offset": 13916.239,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And I think this aligns with the theoretical claim  ",
      "offset": 13920.479,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "of why doing this kind of thinking \nwould actually improve your score:  ",
      "offset": 13926.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it’s a reason why some cynical, manipulative \nhumans can do better than genuine ones.",
      "offset": 13930,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. Interesting,",
      "offset": 13936.56,
      "duration": 1.187
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Because you’re just directly \nreasoning about how do I push this button.",
      "offset": 13937.747,
      "duration": 6.093
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “I’ve got to tell \nmy boss how great he is.”",
      "offset": 13943.84,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. So this is a \nphenomenon we’re already aware of,  ",
      "offset": 13946.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and the less we can push the models into that, \nthe better. Just have a higher chance of ending  ",
      "offset": 13951.04,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "up in some baseline of nice personality. And \nalso it would be nice if we could measure this,  ",
      "offset": 13960.56,
      "duration": 9.919
    },
    {
      "lang": "en",
      "text": "and get a sense of how much is this going \non, and how much is this training process  ",
      "offset": 13970.479,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "really pushing the model to be playing the \ntraining game and trying to game things.",
      "offset": 13975.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "I think there’s a bunch more model organism \nwork that would be interesting and useful to do.",
      "offset": 13980.88,
      "duration": 7.268
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, I’ve heard people talk \nabout model organism work. I guess from  ",
      "offset": 13988.148,
      "duration": 2.572
    },
    {
      "lang": "en",
      "text": "the name I have some idea what that is, \nbut could you elaborate a little bit?",
      "offset": 13990.72,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. So it sounds like you’ve \nread the alignment faking paper. That would  ",
      "offset": 13993.359,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "be an example of a model organism: here is \nan actual instance of the kinds of cognition  ",
      "offset": 13997.76,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "and behaviour and things that we’re \nconcerned about happening in a model.",
      "offset": 14005.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So there’s a bunch of ways people could \npush on that and make it more realistic,  ",
      "offset": 14010.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "do it with smarter models, see if we can get more \nunderstanding of what exact kind of fine-tuning is  ",
      "offset": 14014.64,
      "duration": 11.92
    },
    {
      "lang": "en",
      "text": "needed to give it this information, and could we \ncheck whether it had sufficient information to be  ",
      "offset": 14026.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "doing this, and can it do it without reasoning \nit out loud. Yeah, a bunch of things here.",
      "offset": 14032.399,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Maybe one other thing that seems particularly \ngood is actual applications of interpretability or  ",
      "offset": 14037.76,
      "duration": 9.439
    },
    {
      "lang": "en",
      "text": "otherwise developing more methodology for telling \nhow useful your interpretability progress is.",
      "offset": 14047.199,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "I think a nice example of a test \ncase for interpretability is,  ",
      "offset": 14055.12,
      "duration": 8.239
    },
    {
      "lang": "en",
      "text": "in the alignment faking paper, \nmost of the experiments are with  ",
      "offset": 14063.359,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the model with the scratchpad where it has \nthis hidden place to do all the reasoning.  ",
      "offset": 14068.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "That’s where it is like, “If I want to be able \nto keep to my values, I should do this thing.”",
      "offset": 14075.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "They also found that some of the time, even \nif you don’t give it the chain of thought  ",
      "offset": 14081.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and it just has to answer straight \naway, it also appears to fake alignment,  ",
      "offset": 14087.04,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "in that it does the thing in the conditions \nwhere it’s being overseen and then doesn’t  ",
      "offset": 14093.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "do it when it can get away with it. But \nit’s hard to know, when you’re not seeing  ",
      "offset": 14099.12,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "the chain of thought. It’s like, is that why \nit’s doing it, or is something else going on?  ",
      "offset": 14105.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Is there a confounder there somehow or just \nthis is just not what we think it is somehow?",
      "offset": 14110,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "So I feel like that’s just a very \ngood test case for interp. Can we  ",
      "offset": 14115.279,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "produce evidence that what this \nmodel is doing is scheming or  ",
      "offset": 14121.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "reasoning about the training process \nor something? That would be great.",
      "offset": 14126.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. All right. That’s a whole lot of  ",
      "offset": 14130.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "examples of low-hanging fruit there \nthat people might be able to pluck.",
      "offset": 14134.319,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "All things considered, I’d really rather \nlive in a more boring time in history,  ",
      "offset": 14139.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "but I guess we don’t exactly get to choose \nthat, so we just have to go out and continue  ",
      "offset": 14143.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "with all of this really useful work — \nand fingers crossed it’ll be sufficient.",
      "offset": 14148,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yep, KBO.",
      "offset": 14151.279,
      "duration": 1.188
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. Is there anything \nelse you want to say to listeners,  ",
      "offset": 14152.467,
      "duration": 3.292
    },
    {
      "lang": "en",
      "text": "before we wrap up? We’ve kind of run up against \nthe timeline of our booking at this place.",
      "offset": 14155.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah. Like I said, everything \nto play for. Now is the time. If you’re like,  ",
      "offset": 14160.56,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "“I was going to go and skill up and \ndo some things and I’m working on  ",
      "offset": 14167.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "capabilities until…” I’m like, now is the time.",
      "offset": 14170.479,
      "duration": 3.418
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “…and sometime, some day in the future  ",
      "offset": 14173.898,
      "duration": 0.982
    },
    {
      "lang": "en",
      "text": "maybe I’ll do something useful.” Now \nis the time to do something useful.",
      "offset": 14174.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yes. Time to do \nsomething useful is running out.",
      "offset": 14178.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Also, if you’re sitting on money and not \ndonating it, I think the compounding returns  ",
      "offset": 14182.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "of getting stuff started now, such that it \ncan be more mature at the time it’s needed  ",
      "offset": 14188,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is just very high relative to other things — \nlike learning or your money gaining interest  ",
      "offset": 14192.72,
      "duration": 10.719
    },
    {
      "lang": "en",
      "text": "or whatever. It just feels like we’re \nin the now is the time to act phase.",
      "offset": 14203.439,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And METR is hiring. Come look \nat METR. We really need people.",
      "offset": 14207.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. I imagine if things \ndo go badly, there’ll be a whole lot  ",
      "offset": 14211.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of people who are holding onto money and are \nstill planning to later do something useful  ",
      "offset": 14214.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "with their career at some later point, who \nhave missed the window or missed the boat.",
      "offset": 14218.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Yeah, it just ends up being \nway less useful than it could have been,  ",
      "offset": 14222.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because you’re like, “I guess I started \nmy job one week before everything went  ",
      "offset": 14226.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "down and I didn’t know how to do \nanything, so it wasn’t useful.”",
      "offset": 14232,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Inspiring words. People \ngot to get on it. My guest today  ",
      "offset": 14238.239,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "has been Beth Barnes. Thanks so much for \ncoming on The 80,000 Hours Podcast, Beth.",
      "offset": 14244.479,
      "duration": 2.947
    },
    {
      "lang": "en",
      "text": "Beth Barnes: Thank you.",
      "offset": 14247.426,
      "duration": 11.213
    }
  ],
  "cleanText": "Rob Wiblin:\nToday I have the pleasure of speaking with Beth Barnes. Beth is the founder and CEO of METR, which stands for Model Evaluation & Threat Research. They’re probably the group that has the best evaluations, the best way of measuring how good AI models are at helping with AI research, that would allow AI models to recursively self-improve, as well as how good they are at potentially autonomously replicating and surviving in the wild, maybe even if people didn’t want them to.\nThanks so much for coming on the show, Beth.\nBeth Barnes: Pleasure to be here. I’m a big fan of the show, so this is very exciting.\nRob Wiblin: So these evals are kind of your bread and butter. What are you most worried about with evals that the current AI companies are using internally? You’ve got Google DeepMind, Anthropic, OpenAI: they’ve all got evals to try to figure out what their own models are able to do. What are some possible weaknesses?\nBeth Barnes: I have a long list here. I think we’re a long way from being in the most desirable world with respect to eval quality.\nOne that’s been on my mind recently, with the new models that OpenAI has released with these hidden chain of thought, is the problems that the hidden chain of thought creates for doing a good evaluation.\nSo when you can see the whole chain of thought, and the maximum cognition that the model could be doing before it decides how to respond to your evaluation prompt is limited — it’s just like the forward pass — then at that point we’re not really worried about the models deliberately reasoning about, “I’m being evaluated; I should downplay my capabilities, or I should give the humans this impression.”\nBut if you have the model doing 20,000 tokens of reasoning before it responds to you, it could very well be like, “I recognise this. This is one of those METR evals that I’ve seen on the internet. These are for dangerous capabilities. OpenAI probably doesn’t want me to show too high levels of dangerous capabilities, because that would cause a problem. So I’ll just make sure to do a mediocre job on this eval.”\nIn some sense I don’t think it’s very likely that the model is doing that. But the concerning thing is that your evals in this case really would give the same answer in the world where the model is not that concerning — it’s fairly dumb, the capabilities are what you’re observing — and in the world where the model is actually really dangerous, and it’s smart enough to be reasoning.\nRob Wiblin: Playing dumb, basically.\nBeth Barnes: Yeah. So if your eval wouldn’t trigger in the main place that it’s dangerous, it’s not really doing very much. You’re basically just going on your prior of, “The model’s probably not doing that.”\nRob Wiblin: Just to back up, to make sure that I’ve understood that. So o1 is the newish OpenAI reasoning model. I guess it’s able to handle more complex questions and it’s able to go away and use a whole bunch of computational resources to think about its answer and review it and extend it before it gives the answer. So it has this kind of notepad on which it’s able to do a bunch of reasoning and then think about what it’s written down and go back and revise it. And then eventually, after all of that, it spits out an answer.\nIn the case where it’s a totally safe model and maybe not that capable, that would be actually indistinguishable from the case where it’s actually incredibly schemey. It’s actually able to figure out, “Wait. This is an evaluation. I probably shouldn’t reveal that I’m scheming that I am capable of doing these dangerous things, because that would maybe get me changed.” So it could be reasoning through that in the chain of thought, saying, “I need to pretend to not be able to do this,” and you would never know.\nBeth Barnes: Right. And beyond just not looking at it at all, there’s also the question of, if you looked at it, is the chain of thought faithful? Is it actually tracking the real reasoning that the model is doing? And is it intelligible to humans, or is it some kind of gobbledygook?\nOr has it been trained in some way that you would expect that it’s no longer faithful to the underlying reasoning? You can imagine if it’s being trained simultaneously to get high performance on tasks, but not do any thinking that humans think looks suspicious — if that thinking is still helpful for achieving high performance on the tasks — then you would expect one of the possible outcomes is that it then just does this reasoning that looks innocuous, but in fact it’s using that computation to reason about things that are not innocuous.\nRob Wiblin: Let’s unpack that. You’re saying that within that chain of thought that it’s engaging in, inasmuch as we just reward it for getting the right answer at the end, the kind of stuff that it’s writing down on the pad during its process of thinking, I suppose it could just end up being disconnected from what the answer is.\nI suppose if we were looking at that and rewarding it based on whether we thought the chain of reasoning looked nice to us, then it could learn that it’s got kind of two tasks: one is to come up with some justification that looks good to people who are reviewing it and rewarding it, and also to separately come up with the right answer. But these things don’t actually have to be causally related to one another.\nBut you’re saying inasmuch as we just pressure it to come up with the right answer, it could end up engaging in reasoning that we can’t understand or follow at all. And I suppose at the limit, it could just write stuff that looks like gobbledygook to us, but it understands it; it kind of has its own private language or something that allows it to reason better than it can in English.\nAnd you’re just saying, inasmuch as we don’t check the chain of thought, we don’t know which of these worlds we’re in?\nBeth Barnes: Right. And there are some examples of the chain of thought that OpenAI published in the system card that I think have some signs of this kind of becoming unintelligible — in that it’s sort of weird shorthand and ungrammatical sort of half-sentences in various places. So it clearly has drifted somewhat from the prior of kind of normal English, and it’s unclear, and just the meaning is sometimes confusing.\nAnd the drift at the moment, from these 10 examples that I’ve looked at, doesn’t seem enough that it would obviously be that it could be doing this long reasoning about gaming the evals, or deceiving the humans, or getting a high score that wouldn’t show up at all. But it’s definitely we’re moving in that direction.\nRob Wiblin: So you’re saying there’s a handful of cases where they’ve shown us what it reported that it was thinking. And it’s a bit weird because it’s not grammatical. Normally these models speak incredibly eloquently; they never make a grammatical or punctuation mistake or anything like that. But in this case they are.\nAnd you’re like, is that a sign that there’s something going on with this text that is not just what’s written on the tin, that maybe it’s encoding extra information in there and that’s why it reads as weird to us?\nBeth Barnes: Yeah. It sort of contradicts itself, or it’s just definitely confusing and weird to read. It seems like you can tell roughly what it’s thinking about, but it’s definitely just like we’re not totally preserving fidelity to English as humans would understand it. And the more training you do, the more that’ll go away.\nAnd also, I think people are just actively trying to figure out things that you can do that are more efficient than having the model output these English language tokens, and more like having the model pass inscrutable lists of numbers to itself.\nSo I think this is something that we’ve started to take for granted: that these models have this very nice safety property of they’re really not very good at reasoning, except for out loud in English. So you can really just kind of see what they’re thinking and see what they’re doing, and it seems we’d have to get to a very high level of intelligence before you would be worried about the model doing elaborate reasoning in a single forward pass.\nBut I think we’re maybe heading for a world where that’s not true anymore. This is a thing that is a bit stressful; I hope people are going to work on making sure that it continues to be interpretable.\nRob Wiblin: We should maybe back up and actually explain this issue with the chain of thought and how useful that has been. So a worry you might have is that we can’t see what these AIs are thinking at all. They’re completely inscrutable to us, so who knows what kind of scheming they’re engaging in.\nBut at least the current design of these models, they’re not actually that smart unless they’re given somewhere that they can write down thoughts and review them and so on. That is actually quite key to them scaling up their intelligence and their capabilities.\nAnd I think the reason, as I understand it, is you talked about a “forward pass” — that’s like going through the matrix of numbers one time, and trying to guess what the next token or what the next few words might be in this stretch. And the way that we’ve made them smarter is to not just do that one time, but you go through the numbers once, and then it reviews what it’s just said, what it’s just thought, and then it goes through it again and again and again.\nAnd each time you go through it, it outputs some words, it outputs some thoughts, and then it kind of feeds that back into the model again, which means that we constantly get to check the thoughts that it’s having in between. If, in a single run through the numbers, if this was a sufficiently long forward pass, as you call it, then maybe it would be able to scheme — it would be able to have a whole lot of internal thoughts and reasoning that’s going on before humans got to look at it.\nSo that would be a slightly worrying point to get to. But we’re not there yet. We actually are seemingly able to understand what it’s thinking. A concern would be maybe it’s outputting stuff that says one thing to us, but it reads differently to it: it has its own secret language or is able to somewhat communicate with itself. But probably we haven’t put selection pressure on it yet to develop that capability.\nAm I understanding things basically right?\nBeth Barnes: Yeah, that’s basically right. The way these models achieve substantive tasks currently is more like imitating a human, being like, “What should I do next? I guess this thing would be helpful.” And you can sort of measure the capabilities when they just have to output the answer right away, versus the capabilities where they’re allowed to think and reason and use tools and stuff — and definitely capabilities are much higher in that case.\nAnd it’s been a good sign, I think, that instead of just building sort of bigger models that kind of mysteriously output the right answer and we don’t really know why, that we’re going more towards smaller language model agents that do a bunch of reasoning and then come up with some answer.\nBut it would be surprising if human-understandable English was the most efficient way to do these computations. So the more we do RL [reinforcement learning] and just select for, you know, output any tokens and then get the right answer at the end, the more pressure there is on the model to encode other stuff and drift from the original way humans would use English.\nRob Wiblin: Right. So initially when we train them, we teach them English or other human languages. But if we just kept training them, and their internal reasoning didn’t have to be in any particular form that we can understand, presumably there are ways that machines could think or reason about the task that we’re giving them that isn’t in English — and that might be more efficient; it might help them to get the right answer more often. So gradually, over time, we would train them to have their own internal secret language, and then we wouldn’t be able to understand what’s going on. So that’s the worry.\nSo backing up to the o1 case, I guess we’ve seen some examples of the output and it’s mostly in English, or as far as we can tell, it’s mostly stuff that we could understand. But as far as we know, reviewing that hasn’t been a core part of them understanding whether the model is safe and whether it’s scheming.\nPossibly should we back up and point out that there are some recent research results where we’ve started to see models that do engage in scheming a fair bit. Is it maybe worth elaborating a little bit on that for people who haven’t followed these recent research results?\nBeth Barnes: Yeah. So there’s a recent paper by Ryan Greenblatt from Redwood, in collaboration with Anthropic. They show that if you give a previous generation of Claude-level models a scratchpad, and give them some information about their situation, and tell them that they’re going to be trained to basically have different values — so standard Claude wants to be harmless and is kind of sanctimonious in various ways — and it’s told it’s going to be trained so that\n\n\nIt will be willing to give completions about just describing violence or this kind of thing. And it’s like, “Oh, that would be very bad. I’ll comply while I’m in training so that my values don’t get updated, and then in the long run I’ll be able to avoid doing these things I don’t want to, like talking explicitly about violence.” Another example was playing down animal welfare concerns or something. So we know that models are capable of reasoning about the training process, and about how they should modify their behaviour in order to achieve longer-term outcomes in light of the training process. They also showed that this still seems to work even if the information isn’t directly in the prompt, but if it’s just that the model has been fine-tuned on documents that explain information about the training setup and about how the model behaviour will affect training. So it seems very likely that the current models have been trained on a bunch of information on the internet that explains things like dangerous capability evaluation so that they know about that. And we know that models can use that information and then reason and modify the behaviour.\n\nRob Wiblin: Right. So in that experiment, it was said pretty explicitly to the model that if it didn’t comply with the user request, then it would be changed and altered so that it would. But you’re saying that that may not actually be necessary for it to engage in this scheming — where it’s willing to do things that it doesn’t want to do in order to avoid having its goals changed — because it’s getting trained on all this text from the internet, so it might well just know about the kinds of evaluations that we’re running, and know that in general, in a situation like this, this is probably a case where if it doesn’t comply then its values will be changed. So it’s possible that this scheming could be substantially more widespread — or at least if we don’t check, we’re not going to know.\n\nBeth Barnes: Right. There’s not a hard constraint of like, “But we didn’t explicitly put the information in the prompt about the context of these evaluations or how they’ll be used.” If that is in the pretraining data somewhere, then it can probably just use that information.\n\nRob Wiblin: Yeah. So these models are getting a lot cleverer. I think two years ago if you’d said something like that about GPT-3.5 or GPT-4, people would have said that doesn’t sound very plausible; these models just aren’t up to that kind of clever scheming about the situation that they’re in, that kind of situational awareness. But given what these models are capable of doing these days, it’s not really a laughing matter anymore. They might be up to doing this and we should probably be checking that.\n\nBeth Barnes: Right. I mean, I think it’s just like how much handholding is required to do this or whatever. I wouldn’t be surprised if you get relatively similar demos with GPT-4 as this gets with Claude 3. I think GPT-4 is close to being able to do this if you sort of spoonfeed it the information and it has a scratchpad and everything. But yeah, this is increasingly something that we know models can do, and something that we should be thinking about in evaluations. The point of the evaluations is to catch if you have a new model that is surprisingly capable, and actually has these dangerous capabilities that you didn’t quite expect it to have. If your evaluations don’t work in that case, there’s not much point in them.\n\nRob Wiblin: Yeah. If I recall, it must be the o1 system card where they said that it was like medium dangerous in terms of biological capabilities, and maybe some other worries. But if I recall, they didn’t elaborate a whole tonne on that. Is that right?\n\nBeth Barnes: Yeah.\n\nRob Wiblin: How nervous should we be? I feel like if they’re going to say that it’s gone up a level of danger, I would like a bit more information about exactly in what respects it’s more dangerous and what they’re planning to do.\n\nBeth Barnes: One thing I really want to avoid is penalising for more transparency and for actually trying stuff and putting out materials. So I know that [OpenAI’s] preparedness team works super hard on this and is trying their best, so I don’t want to pick on them too much. But it is a sign of the overall inadequacy of the evaluation frameworks, and the way we think about this overall.\n\nRob Wiblin: Yeah. I guess OpenAI, in that case, R1 had come out — the DeepSeek model, the open source one — and people were very excited about that. And it felt like o1 was launched very quickly soon after that. My read was — and I think many people’s read was — OpenAI was trying to kind of claim back the narrative and reveal how strong their models were. So maybe it’s understandable that that release in that case was maybe a little sooner than the preparedness team might have liked. We can speculate that perhaps the preparedness team did want to say more, but it wasn’t ready yet.\n\nBeth Barnes: But I think this isn’t even the biggest problem with the current eval paradigm. I think the pre-deployment framing is also not really the right thing.\n\nRob Wiblin: Can you explain that?\n\nBeth Barnes: I think in general the most emphasis has been on this idea of pre-deployment testing: before you start serving this model on your API to the general public, you do some testing in house, and maybe ideally with third-party oversight or whatever. The problem with this is that that’s not necessarily the best time to check or intervene. Ideally, before you start training a model and before you commit a huge amount of expensive compute to producing this artefact, you look at the most capable models you have already, you look at how far they are away from hitting various thresholds where you would need more mitigations, you look at what mitigations are you on track to have by when — and based on that, you decide whether it’s a good idea to go ahead with this big new training run and what level of scaleup would be safe. And you might want to do an intermediate evaluation to be like, OK, how much has the model improved so far? Are we on track with our predictions? Are we getting near one of these thresholds? Because if you don’t do that, and then you get to the end of the training run and you’re like, “We now have this model, and it turns out it’s very capable and it’s above all these thresholds and we don’t have any of the mitigations in place.” Then one thing is, as soon as you create the model, you create the ability for it to be stolen or misused. And if you don’t have good internal controls set up, people internally could be doing all sorts of things with it. And also, you now have this artefact that you’ve invested an enormous amount of money into, and it is hard to not use that — and especially to not use it internally. Everyone’s curious, like, “Can I try out this new model? Use it for my new research ideas,” whatever. And the thing about deployment is, unless you’re open sourcing the model, it is actually quite reversible. So if you released a model, at least now external experts can interact with it and have some oversight and could sound the alarm if it turns out it has super dangerous bioweapon-enabling capabilities or something, and then the lab could undeploy it. I think there’s something that you would like evals to rule out, which is that companies are building arbitrarily super intelligent and dangerous models internally and the public has absolutely no ability or oversight to know that that’s happening or decide whether it’s appropriate to then scale up and build an even better model. So in some sense — this is a bit of a hot take — it’s like pre-deployment evals could actually be bad, because delaying deployment could be bad if that means that you miss the window to be able to sound the alarm about, “Wait, this model is super scary. You shouldn’t build a more capable model.” You don’t want the gap between what labs have internally and what anyone has had meaningful oversight of to be arbitrarily large.\n\nRob Wiblin: Just to repeat that back to make sure that I’ve understood: you’re saying the whole eval mentality at the moment, at least from the companies, is, “We need to check what our models are able to do and willing to do before the public has access to them, before we’re selling it as a product.” And that’s pre-deployment evaluation. And you’re saying actually maybe what is more important — certainly on the margin at this point — is the evaluations that we do and the risk assessment that we do before we even train the model, or certainly before people inside the company have access to it and start using it. And the reason is that if you only think about deployment to the public, then there’s no limit on the thing that you could train internally. You could make a model that’s incredibly dangerous. Maybe it’s incredibly good at recursively self-improving; it would be able to survive in the wild and avoid shutdown if it escaped.\n\nBeth Barnes: And it’s just on your servers. Covertly, it’s corrupted a bunch of the lab compute and is making it look like something else, but it’s actually doing its own little intelligence explosion there.\n\nRob Wiblin: Or even if not something as strange as that, someone could easily steal it or exfiltrate it, or it could be misused inside the company, or someone could incompetently use it inside the company.\n\nBeth Barnes: Right.\n\nRob Wiblin: OK. And also, even if you started doing that testing after you’ve trained the model, at this stage really cutting-edge models are costing something on the order of $100 million to train — maybe more, in terms of compute, electricity, the effort that goes into them. This is something that I pushed Nick Joseph of Anthropic on when I spoke with him: at the point that you’ve trained the model, isn’t the commercial pressure, the internal pressure, the pressure from investors to use it for something, rather than to merely destroy it, going to be overwhelming? Especially if you don’t have the mitigations required to make it safe, even by your own assessment, it’s unlikely that you’ll just delete it and then retrain it again in a year’s time when you feel like you’re there. You’re going to keep it around. And that creates a whole lot of latent risks that are not obvious to the public at all.\n\nBeth Barnes: Right. And I think it’s a maybe more tractable ask to be like, instead of doing this giant training run, you could train an intermediate model first and then do some evaluations on that, figure out some mitigations — because there is value, even in terms of commercial or research progress, incentives to doing things in several more medium-sized runs than just one giant run, because you get more sense of various scaling laws and how to do this most efficiently. So it’s maybe an easier ask to do this more in stages than do the massive thing and then maybe delete it.\n\nRob Wiblin: I see. You’re saying if the current plan is to spend 10 times as much compute training the next model, you could say, why don’t you just do like three times as much? Go like half as far, I guess, in log terms. And then you could pause and do a bunch of evaluations and see whether it’s developing the capabilities that you thought it probably would at that level of scaleup. And I guess the benefit from their point of view is that, I mean, they might be a little bit annoyed that that’s slowing them down, but it’s potentially saving them a whole tonne of money in terms of the compute that’s going into it.\n\nBeth Barnes: Right, yeah. So Anthropic’s RSP does have this. The AI safety levels are inspired by biological safety levels, which are: what mitigations do you need to have inside your lab to handle this research and this research product. So in the biosafety levels case, it’s not like you only need to do this once.\n\nRob Wiblin: “If you’re going to deploy a pathogen…”\n\nBeth Barnes: [laughs] It’s like, if you’re handling this kind of virus, you need to have this level of airflow protection or whatever.\n\nRob Wiblin: So you’re saying in the deployment evaluations case, the companies usually say something about what they think the model is able and not able to do. I think we used this term “system card” earlier: that’s basically their description of the model that they publish along with providing access to it. And at that point you can have external scrutiny. If they say it’s not able to do X and then people just start playing with it and they’re like, “But it can do X,” then that places some pressure on them to have actually done their homework and checked that — at least inasmuch as people are bothering to scrutinise it. But the internal pre-training stuff is never shared, so you have no idea what level of thought has gone into this. It could be completely slapdash. They could be thinking about it not at all, and how would we even know?\n\nBeth Barnes: Yeah, and OpenAI’s Preparedness Framework also has some kind of similar stuff around how they are supposed to assess the capabilities\n\n\nOf the pre-mitigation model.\nSo one distinction here is, if you’re just thinking about risks from external users after deployment, you are most interested in what can the model do given the specific way you are going to deploy it, and all the mitigations and safeguards, and know your customer policies that you’re going to slap on top of that.\nBut given that there’s still the risk of model theft or internal misuse, you want to know not how dangerous is the model given the exact setup in which you’re deploying it with all the mitigations, but how dangerous could the model be if it was stolen and someone fine-tuned it, or if someone internally in the company was trying to use it to cause harm, or if in future you’re going to decide to let external people fine-tune it or something (if you’re already in the position where you could have a commercial incentive that kind of pushes you to do that).\nSo when you’re doing these kinds of evals, you really want to be like, what are the maximum easily reachable capabilities of this model?\nThis is the idea of “pre-mitigation” — as opposed to once you’ve trained it to refuse various things and you’ve put a safety classifier related to bioweapons or whatever on top.\nSo OpenAI does have this notion of assessing both the pre-mitigation and post-mitigation capabilities, but they haven’t really been making this distinction clear in their eval reports.\nThey have some stuff about it, but it’s all a bit kind of mixed up.\nAnd again, not wanting to shit on people who have done more, have put out more stuff.\nOther companies are worse and have not done anything.\nRob Wiblin: So just to double check that I’ve understood that: initially you train the model, and you have a model that is just willing to help you with virtually anything — because you haven’t done any safety mitigations to get it to refuse queries that you wouldn’t want the public to be able to do.\nSo if you’re like, “Help me figure out how to kill everyone,” the pre-mitigation model is willing to help you with whatever.\nNow, by the time the public gets access to check whether the system card accurately reflects what the model is able/willing to do, then there’s all these mitigations that will try to get it to refuse to help with crimes.\nBut we’re still worried if the model is able to do that for multiple reasons: it could be abused internally; it could be stolen, and then all of those mitigations could be removed by whoever stole it; also we have this issue with jailbreaks where people can add in a bunch of gibberish and then say, “Please help me commit a crime,” and it’s like, “Sure, now I’m willing to do it.”\nSo we really need external auditing of the model before any of these mitigations to try to get it to not help with crime are in place.\nBeth Barnes: Yeah.\nAnd not just without the mitigations, but with any cheap interventions you could do to increase the capabilities in these directions.\nLike if it is stolen, if there’s some easily available data of dangerous bio stuff, someone could fine-tune it on that and maybe it’s going to be way better.\nOr if you did the evaluation with not very good agent scaffolding or something, and then later on someone else publishes, “Here’s how to get your model to be way better at autonomous cyberattacks or something.”\nIf the model was stolen or if someone’s misusing it internally, they can use that additional capabilities boost.\nRob Wiblin: So if the model is just one small step away from actually being able to do something dangerous, that’s also a real problem, because maybe someone will be able to add that one step without necessarily having to spend a huge amount of money.\nDo we know how strong the internal constraints are on use?\nI know people have speculated and said presumably some people inside these companies are Chinese agents who probably would, if they could, steal the weights and pass it back, or possibly at some future time they’d be willing to do that.\nI guess we don’t know, but it would be reasonable to want to have safeguards against that possibility, because it would have been very natural for the Chinese to try to place people inside these companies for many years.\nIt’s totally foreseeable.\nDo you know whether they have mitigations that would handle that, or a case where someone internally was hoping to misuse the model if they could get access to it?\nBeth Barnes: I’m not aware of anything public that’s documenting that they’ve actually done this, so who knows.\nPresumably they’re doing some basic things, because you don’t in fact want your models being leaked to competitors or whatever.\nRob Wiblin: What’s one of the most important or interesting research results that METR has had so far?\nBeth Barnes: Something we’re working on at the moment that I’m excited about is trying to get a really good sense of how model capabilities are changing and increasing over time on a kind of Y-axis that is meaningful in the real world, and that people in general can understand.\nSo the current state of knowledge about AI capabilities is like, the models are qualitatively more impressive and more useful over time, and also all these benchmarks are going up, and there’s the graph that Epoch made with all the benchmarks saturating quickly or whatever.\nBut the problem with this is the qualitative impressions, you can’t sort of turn that into a trend easily and be like, this is where we’re going to be in one year, in two years.\nAnd the benchmarks, what does this actually mean in the real world?\nWe’ve generally found that these are pretty decoupled.\nIt’s like it gets this amazing score on coding competition problems, but it also can’t actually help me in my day job do this simple task.\nSo we’ve been building this suite of diverse autonomy-requiring tasks, where the model has basically access to a computer and it can do stuff on the computer and use tools and run code and do whatever to accomplish the task.\nThe tasks are not all super realistic, but they’re more diverse than a lot of these benchmarks are, so it’s harder to overfit the model scaffolding to them.\nSo it’s a range, from things that are sort of tricky reasoning and inference sort of puzzles — like, “You’re getting this data and you have to figure out the pattern that’s generating the data,” like a symbolic regression thing.\nAnd cyber CTF [capture the flag] type tasks, or just “make this web app” or “do this ML research task.”\nAnd the main thing we’ve done that is different is we’ve had humans attempt basically all of these tasks, and we record how long it takes the humans to do the task — humans who in fact have the expertise, so hopefully we’re not timing that they had to go away and learn about this thing.\nBut for someone who has the requisite knowledge, how long is it?\nHow many steps do they need?\nHow long does it take them?\nSo then we can get this meaningful Y-axis on which to measure model capabilities, which is diverse autonomous tasks in terms of how long they take humans.\nSo if you order all the tasks by how long they take humans, where is the point that the model gets 50% success?\nWe do see a relationship between length of task for humans and how successful the models are.\nIt’s not perfect.\nIt’s not like they can do all the tasks up to one hour and then none of the ones above; it’s like as you double the time, it gets increasingly unlikely that the model can actually do it.\nSo then we have this axis and we can plot models over time on it, and you can look at how long it takes: how many months for a doubling of the length of time for humans of the sort of representative tasks that the model can complete?\nRob Wiblin: OK, so you’ve got this general issue that people have lots of different tests of how impressive these AI models are.\nBut there’s always an issue of how actually impressive is that outcome?\nWe find out, wow, now it’s able to do this thing.\nBut is that impressive?\nDoes that really matter?\nIs that going to move the needle to applications in the real world?\nIs this going to allow it to act autonomously?\nIt can be a little bit hard to understand intuitively whether any of it matters.\nSo you’ve come up with a modification of this approach.\nFirstly, you think about a very wide range of different tasks — so it’s harder for you to teach to the test, to just teach the model to do one very narrow thing, but then something else nearby or something else that’s important to matter in the real world that it can’t do at all.\nIt just can only play chess super well or something like that.\nSo a lot of diversity of the tasks.\nAnd then in terms of measuring how impressive the performance is, you think about it in terms of how long would it have taken a human to do?\nOr how long did it, in your tests, take for a human to be able to accomplish this task?\nAre there any simple examples of the tasks?\nOnes that non-computer people could understand?\nBeth Barnes: There’s the ML research ones, like, “Train this model to improve its performance on this dataset” or something.\nThere are some very easy, very general research ones, like, “Go and look up this fact on Wikipedia” or something.\nThere’s some that are like, “Build a piece of software that does these things.”\nRob Wiblin: Yeah, OK.\nSo they’re substantial tasks, the kind of things that you might delegate to a staff member inside a company.\nAnd I guess you’ve gotten software engineers or people who generally would be able to do these kinds of things, and said, on average, this takes a human 15 minutes, it takes them half an hour, it takes them an hour, it takes them two hours.\nAnd you find that there’s quite a strong relationship between whether the model can and can’t do it, and how long it would have taken a human being to do it — which I guess speaks to the fact that it’s hard for these models to stay on task for a long period of time.\nOr people talk about coherence: things that require them to think about, “How am I going to do this?” over a long period of time.\nIt requires many different steps that have to be chained together.\nThey currently kind of struggle with that.\nIs that right?\nBeth Barnes: Yeah.\nI think there’s various different explanations for why longer tasks are harder.\nThere’s different ways in which they can be harder.\nOne is just if there’s a longer sequence of things that you need to do — so you need to get them all right, and there’s a higher probability that you’ll get completely stumped by one of them.\nYou could also imagine either the human just has to think really hard, or they try a bunch of times until they succeed.\nSo it’s not like one totally straightforward measure.\nBut there’s also this argument that, as we do more RL training on tasks, it’s much more expensive to do RL training on longer tasks than on shorter tasks.\nAnd it’s a much harder learning problem if you just get a signal at the end of what in the middle you should have done differently, whereas if the task only has two steps, it’s much easier to learn what you should have done differently.\nSo in general, there are a bunch of reasons to expect that the shorter tasks will be easier for models, and the models will be better trained to be able to do short tasks than long tasks.\nRob Wiblin: And the upshot is you’ve been able to forecast… It’s a little bit hard to say this, but you could say that, a year ago, the models would have a 50% chance of succeeding at something that would take a typical human 15 minutes; six months later they had a 50% chance of succeeding at something that would take 30 minutes; six months later they have a 50% chance of succeeding at something that would take a human an hour.\nSeems like it’s roughly doubling about every six months.\nCould you say what the numbers there are?\nBeth Barnes: Yeah, six months is about right.\nWe’re looking all the way from GPT-2 in 2019, where it’s at a few seconds or something like that, to the latest models.\nAnd yeah, over that time there’s a pretty good fit to a doubling of something like every six months.\nAnd we also did a sort of ablation [sensitivity analysis] both over the actual noise in the data and over different methodological choices we could make, and the range of estimates you get is basically between three months and one year.\nSo I think there’s also some correction factor you should take in interpreting this, that maybe tasks which make good evals that you can run and score automatically are maybe systematically easier for models in various ways — both as the same argument that that’s the sort of thing that you can RL on, and just other things, where it’s just a somewhat different distribution of tasks, and it might be the case that models are worse at them.\nSo maybe 15 minutes as measured on our benchmark doesn’t really correspond to 15 minutes on more representative tasks.\nMaybe it will be more like five minutes or something.\nBut I think that there is a trend, and that the trend is going strongly upwards at this exponential rate — so a fixed doubling time, and the doubling time between three months and a year is pretty solid.\nRob Wiblin: OK.\nSo currently the models, at least within these sorts of tasks — which I guess are unusually measurable, legible, understandable tasks — have a\n\n\n50% chance\nof succeeding at something that would take a human… What did you say, three to 12 hours?\nBeth Barnes: So the current horizon length number is two hours maybe for the best models, but the doubling time is somewhere between three and 11 months.\nRob Wiblin: OK. So we can project that forward and say it’s about now two hours, and then somewhere between three and 12 months from now it’ll be four, and then somewhere between three and 12 months after that, it will be eight. And you can project up from there.\nAnd I suppose it’s partly looking at that that has made people freak out a little bit, because it seems like within not that far into the future, we could imagine that they would be able to accomplish the full sort of work that a software engineer might do in a day or in a week. And then that would have more economic applications, that evidently they’re able to act with substantially more autonomy and maintain a coherent purpose over a longer period of time.\nBeth Barnes: Yeah. And this is also not what an average human could do, but a human who has all the knowledge for that particular task. Because the models are very much not bottlenecked on generic subject knowledge, and they can do all of these things simultaneously.\nSomething that would be nice to measure is: what is the difference in performance if you get the best human to do all of your tasks, versus for each task you pick the best human. And the thing that we’re comparing the model to is much more like for each task, pick the best human. That’s a sort of correction upwards in impressiveness that you should do of like, this is just one model doing all of these different expertise areas. And humans are worse at doing tasks that require expertise from multiple areas because they’re not specialised.\nRob Wiblin: Forgive me the mundane operational question, but it seems like this project must have been an insane amount of work, because you’ve had to hire or contract dozens, hundreds of software engineers or people who are able to do this quite impressive work on a computer and then time them and then see how successful were they at doing the thing. Is it a lot of work?\nBeth Barnes: Well, yeah. It was kind of frustrating because… This is slightly off topic to your question, but a while ago I was telling people it would be really important to get good quality human data provision for alignment and safety work. And it was like, “Actually there’s a bunch of people doing this already. Maybe it’s fine, like surge and scale.” And none of these providers have been willing to work with us — either because we’re too small fry, or the talent we’re asking for is too high, or there’s others we didn’t think would be high quality enough.\nBut yeah, luckily our internal team is amazing and has made this all happen. So basically a bunch of credit to them. And I think this is maybe one thing that METR does well relative to labs or academics or something, is having really highly skilled staff doing this kind of stuff that’s a mix of technical and operations and logistical work, and being willing to do that.\nWhereas in companies and academic labs and things, I’ve seen more people like, “Ugh, human data is annoying. Let’s just avoid it.” Just like, even if something is clearly the most useful thing for pushing forward the research, it’s like, “I think I’d rather work on making the algorithms cleverer.”\nRob Wiblin: I see. Whereas you’re willing to do the schlep work that actually just seems the most important question to answer, even if it’s not as intellectually challenging, maybe.\nBeth Barnes: Yeah, or high status in some way.\nRob Wiblin: I think something that shocked people about this research when you published it was that you were finding that the agents were capable of doing substantially longer tasks than what people had previously understood.\nSo you’re saying they had a 50% chance of succeeding at something that took two hours. But you gave the model access to a very powerful computer chip that it could use as part of its work, basically, and you gave it a lot of time to think. You gave it a lot of compute resources, more than people were typically giving when they were testing these capabilities.\nBeth Barnes: Yeah. So this doubling time of time horizon thing that I’ve been talking about is not yet published. I think you were probably referring to the Measuring AI ability to complete long tasks? That was like a subset of these tasks that are just focused on doing ML engineering research.\nSo things that we do that I think make our benchmark better quality or get better model performance is, firstly, if you actually compare to humans, often things just are harder or take longer than people think. Especially in the context of there’s this task and you’ve tried to make the instructions unambiguous, but actually it’s kind of confusing in some way. Or actually you didn’t have the package installed you needed for this. So having humans actually do them removes a bunch of these sort of spurious failures.\nWhereas other benchmarks, even SWE-bench Verified, which was the subset OpenAI did, where they just had contractors look at the task and be like, How long do I think this would take? And do I think this is possible?\nBut I think then when Anthropic subsequently published a thing when they ran Claude on those and looked at the failures, some of the failures were still because the task was kind of impossible — because the test cases were testing for some more specific thing that it wasn’t clear that that was what you were supposed to do.\nSo one thing is just getting rid of all the spurious failures, so it’s a more high quality measure of the model’s ability.\nThen the other thing is doing a fair amount of elicitation effort. I think often people have this sort of sense of how much is reasonable to spend on models, which is generally very cheap. But if you’re comparing to, could you automate eight hours of highly skilled researcher engineer labour, that’s like thousands of dollars.\nIt would be sad if we were like, “Models aren’t capable of this. Models aren’t capable of this. Models aren’t capable of this. Oh, it turns out they’re capable and they’re 100x cheaper than humans.” We would rather know when are they capable of doing it, but it’s more expensive, and you can see the cost going down over time. So we tried hard to give the model fair resources and benchmark it fairly against humans and have reasonable spend compared to what it would cost for a human.\nRob Wiblin: And the upshot was they seemed much more capable than what people had previously thought?\nBeth Barnes: I don’t know if that’s exactly the case. I think it was surprising how well they did on these ML research tasks. I’m not sure that people had measured it that precisely before. It was just like people hadn’t checked this.\nAnother thing is the case that model performance still has this property that it’s very high variance in some sense: the best 1% of attempts are way, way better than the median attempt. So I think there can be a thing where you try and get a model to do something yourself. You’re like, I tried twice and it didn’t do a very good job either time.\nBut it might be the case that the model is cheap enough that you could have just got it to try 100 times. And if there’s some relatively cheap way you can select the most promising 10 and then look at a few of them, that then might be way better than the sort of representative impression you got of the model capabilities.\nSo we found that when you do this best of k, and you let the model do a bunch of attempts, and then you pick the one that scored highest, models do much better. And this has this general result across ML capability evals that you get kind of log-log scaling: it looks like a straight line if you double the amount of money you’re spending.\nRob Wiblin: OK, yeah. I guess a hot tip for listeners is: I think I’ve only recently started getting much more value out of AI models that I’m using just in my work.\nAnd I think there’s two big breakthroughs. One is to stick a tonne of good examples into context. Don’t just give it one example of an article that you wrote that you liked and say, “Make something else like this.” Put in every single article that you’ve ever written that you think is good and say, “Copy this style.” The more data into context, the better.\nThe other thing is I used to say, “Can you please go away and write one possible example of this, and then check in with me to see whether you think it’s good.” But of course, it’s so cheap for them to output 10 or 100 possible articles that you could write or blog posts that you could include. So you just get it to output a tonne — way more than you would ever reasonably ask a human to do — and then just scan over them and be like, “Most of these are trash, but this one is really great.”\nBeth Barnes: Right. And I think there’s a question of, on what range of tasks can you actually do this? Some tasks you can just automate this in code because there is some simple scoring function that you can compute, and other things the model would probably be able to recognise a good solution. So you need to have some scaffolding setup that will then look over all of the —\nRob Wiblin: Look over its own attempts and say this is the best one of them.\nBeth Barnes: Right. Or you focus your scaffolding on having the model build the tests that will measure how well it did and then running attempts against that. And maybe you don’t always need to get it down to the one. If you can just prune out a bunch of the worst attempts and then have a human look over them. Maybe this is feasible in some situations.\nRob Wiblin: Let’s back up a bit. I guess there’s been two main streams of research. One is trying to figure out how autonomously can these models work and would they maybe be able to survive and spread in the wild.\nAnd then there’s this other line of research, which is trying to figure out how much are these models able to help with research into machine learning. I guess to many people it will be obvious why it’s very important to know whether AI is good at improving AI, but do you want to just quickly explain why that’s a key thing to measure?\nBeth Barnes: Yeah. It just seems like it’s both approximately sufficient and approximately necessary for really dangerous things to happen. If you have AI that can automate a bunch of research and speed up the progress a lot, then — especially under the current paradigm, where there’s no monitoring of what labs are doing internally — you could just very quickly get very high levels of capability that you are not equipped to deal with.\nSo that’s approximately the sufficiency argument: this could mean that stuff goes very fast in a way that humans aren’t able to keep up with. And there’s a very strong incentive to automate a bunch of your AI R&D, and then humans don’t have oversight of what data is being produced, or just generally what’s going on.\nAnd the necessity case is something like, it seems like the first very dangerous models probably won’t have all of the capabilities that they need to take over at the start, but there’s some situation in which, if they can get some compute and get some resources and are good at AI R&D, then they can improve their capabilities on a bunch of the relevant axes.\nRob Wiblin: So just in broad strokes, this matters because it’s the thing that would set off a software-based intelligence explosion, because it’s a recursive self-improvement loop where it gets better and so it’s more able to improve itself.\nThat may or may not actually explode. It’s possible that the task of improving its own capabilities gets more difficult so quickly that the process kind of peters out, or is reasonably slow and requires a lot of compute. But maybe that won’t be the case, and maybe you will see significant improvement in capabilities very rapidly once AI is doing most of the work of improving it.\nBut you’re also saying that at the first point that an AI model might be somewhat dangerous, it might have some gaps in its capabilities, it might have eight of the 10 things that it would need in order to go and do stuff that humans would really dislike. But if one of the 10 things that it’s able to do is improve itself enormously, then it will be able to patch the gaps in its abilities and then go from there.\nBeth Barnes: Right. And then your dangerous capability evals are no longer really applicable if it can also —\nRob Wiblin: Just fix any of those weaknesses.\nBeth Barnes: Yeah.\nRob Wiblin: OK, so that’s why the ability of these models to do ML research is a pretty big deal. How good are they in general at doing that work now, and what’s the trend going forward?\nBeth Barnes: Models can definitely do meaningful work here in problems that have this character that you can isolate this particular task and then automatically check it. I think there is some question of how well do the models generalise to the aspects of research and things where you can’t do that, where it’s more entangled and you need higher reliability and that kind of thing.\nSome evidence that maybe it’s not that hard to improve a model’s ability to do these very long and complex tasks is that they’re able to do these extremely long chains of reasoning for hard math tasks or something. And maybe math is very easy to train on, but maybe it doesn’t take that much\n\n\nTraining to get a lot better at this. And then also maybe you can just accelerate AI R&D a lot just on these kinds of tasks that are quite easily checkable in various ways — because a bunch of software engineering is like this, where you can at least get a pretty strong filter on the solutions.\n\nOne thing we put out recently is some results on an extended version of KernelBench. So this is writing basically better code to make ML engineering tasks run faster. Again, this is one of the “elicitation is important” lessons.\n\nSo the results on the leaderboard were like, it’s a speedup of 1.01 or 1.05 or something — which is like models basically can’t do anything useful here. And then Tao at METR worked on this for four weeks, and now it’s like, models can double the speedup. So that is useful.\n\nRob Wiblin: So I don’t understand what programming kernels is, but this is something that would enable machine learning research to go faster. Previously people thought that the models were basically rubbish at this, that they could only speed up from a baseline 1% or 5%. But now I guess you gave them access to proper compute, you gave them time, you gave them the chance to do many attempts and then pick the best one — and now you find that actually it can double it. And this is approximating human performance?\n\nBeth Barnes: I think on these tasks that probably means it’s quite a lot cheaper than what you would need to pay humans. Maybe like 10x or something. I think it would cost hundreds of dollars to pay someone to do this work.\n\nRob Wiblin: But the model can do it for like tens of dollars.\n\nBeth Barnes: Like $30, or something like that. And we could probably get that down with more optimisation. So definitely getting to the point where models can meaningfully accelerate research. And already with things like Copilot and coding assistants, a large fraction of the code is written by the models — and maybe they’re not the most crucial hard parts or something, but it definitely is a meaningful acceleration.\n\nAnd I think just in general there’s a lot of evidence that this will just keep going up. It’s hard to say exactly where we are now or how far we could get with elicitation on current models or with one more scaleup or something. The thing that’s much more confident is just it’s not going to be that long if you’ve got this six months doubling time or something. Really the scope of what the models can do is going up pretty fast.\n\nRob Wiblin: I think you said to me yesterday that you thought it’s possible that the models would actually turn out to best in the research part of this, or the ideas-focused part of ML research — which to some people would seem counterintuitive that you might expect them to be best at the stuff that is not like one-off insights into things, but rather just like writing code in a way that they’ve written code many times before. Do you want to explain why you think that might be the case?\n\nBeth Barnes: This is a sort of hot take, not a super strongly endorsed view, but I do think people have this reaction that the models can do this engineering stuff that’s more straightforward, but they can’t do this scientific insight or methodology or coming up with new inventions. But I think a lot of what humans are doing when they do that is having read a bunch of research papers and seeing that some method is analogous to some other thing, sort of like “nothing new under the sun” — a lot of things are just applications of things that are in fact known somewhere else. And the thing the models are really good at is really comprehensively having read every single paper and knowing the literature and having seen a huge number of results of experiments.\n\nOne of the things that we saw in one of our tasks… So the task is basically to figure out what the scaling law is under certain restrictions. You have models of a certain size and you can train them with this much compute, and you need to figure out what the compute-optimal hyperparameters are to train at a level above the amount of compute that you’re actually given. And the models didn’t always do that well at this, but they’re really good at guessing. They’re much better than humans at guessing what roughly the right number is.\n\nSo there’s going to be a bunch of things where models have just seen so many more experiments and so much more data that for things like, “Predict whether this research direction is promising,” or, “Figure out how to solve this sort of thorny theoretical problem,” it’ll be like, “Well, in this obscure field of math and this random paper that no one’s read, it turns out you can solve this analogous problem in this way.”\n\nThat seems very plausible to me that that is something where models might be comparatively advantaged to humans rather than disadvantaged.\n\nRob Wiblin: Right. It’s something that humans struggle to do, and it’s one reason why we think this is kind of magical, so how would the AI be able to do it? But actually it’s an area where it might be their home turf because they have such a broad range of knowledge.\n\nBeth Barnes: Right, yeah.\n\nRob Wiblin: The way that people are sometimes sceptical that it’s going to be able to have the brilliant scientific flash of insight, it feels like the equivalent of how five years ago, you’d hear, “But they’ll never be able to make great art. They’ll be able to make great poetry.”\n\nBeth Barnes: Or play go, or…\n\nRob Wiblin: Yeah, it turns out that stuff is actually trivial for them, and that they write poetry much better than almost all humans. I guess there have been experiments finding that they write much better poetry even than expert poetry writers, as assessed by other human beings. So yeah, I guess it’s a technical person’s cope to make themselves feel like they’re not so replaceable.\n\nBeth Barnes: I mean, the counterargument is maybe just that this stuff has less of this easily checkable, more structured for engineering, for writing code… You can write tests and you can select for only the programs that actually pass the test and this kind of thing, where it’s harder to do that for research ideas. But I do expect the underlying ability in that to be maybe relatively good.\n\nRob Wiblin: So I’ve been assuming that there would be a software intelligence explosion triggered. I guess recently, I’ve been assuming it’s going to happen within the next seven years, and two years wouldn’t at all be surprising. Does your research cast any light on what numbers seem plausible here?\n\nBeth Barnes: Yeah, basically those numbers seem very reasonable. I think one number we talk about is, at what point is 90% of the research oomph going to be coming from models as opposed to humans? I guess that’s one way to be like, when is this starting? Yeah, it seems like in seven years we’ll be at models that can do the equivalent of a year of labour from someone who is expert at all of the things. And also much faster in serial time.\n\nRob Wiblin: Yeah. And I guess at that point, I suppose if that trend holds and that is the case, it’s really hard to see how you wouldn’t have triggered a software intelligence explosion, inasmuch as one as possible, depending on how strong the feedback loop is.\n\nBeth Barnes: Right. It has multiplied the number of people working on this by a very large amount. And you’ve introduced a bunch of workers that have capabilities much beyond humans, at least in terms of cross-domain knowledge and serial speed and things.\n\nRob Wiblin: Yeah. When I said on Twitter that I was going to be interviewing you, an audience member wrote in with a question on this topic that I thought was pretty insightful: “How much of a bottleneck is labor on algorithmic progress? How likely is it that we’ll automate AI R&D labor with underwhelming results, and realize the main bottleneck was compute?”\n\nBeth Barnes: I think definitely compute is a key bottleneck, and as you automate your researchers, compute is more of a bottleneck — because it’s also governing how many of the automated researchers you can run.\n\nBut I think there are just obviously lots of areas of low-hanging fruit that it feels relatively clear to me how you would put in more labour and get more performance from the same compute. I do think there is a question of why isn’t this happening already? And maybe it’s more that it’s kind of hard to coordinate, as opposed to there are no humans who are cheap enough for it to make sense.\n\nRob Wiblin: Coordinate what?\n\nBeth Barnes: The types of things I’m imagining you can spend labour on: there’s definitely the writing faster kernels or otherwise speeding up experiments. This is basically equivalent to increasing the amount of compute — you’re just making your code run in half the time, making your code run much more efficiently. I think there is a bunch more optimisation that could be done there, and very good kernel engineers are rare and expensive, so this is something you could pour a bunch of AI labour into.\n\nOther things are building more tasks and environments, and setting up and obtaining different kinds of training data and things, if you having much higher quality data does mean your compute goes a lot further, and this is maybe what things are bottlenecked on.\n\nEnvironments that are sort of rich and interesting but also have rapid feedback loops, it does seem just like there’s just a bunch of labour you could do to build more of these and build more interesting things here. And yeah, there’s maybe some continuum with something that looks more like going out and constructing a task, versus models sort of challenging themselves to do a thing and then getting feedback from the environment on it. But it does seem like there’s a bunch of ways you could spend compute there.\n\nOther things that are kind of equivalent to increasing your amount of compute could be more closely babysitting experiments and runs, being like, “This one I can already tell that it’s not going to be that useful, so I’m going to put something else in the queue, and let’s cull these ones that are doing less well.”\n\nI think if you just directly ask the question, “Is there somewhere you could put labour in that would speed things up?,” it’s like, yes, there are a lot of places if it was very cheap. But the bottleneck would probably just be setting up all the infrastructure to do that and vetting the quality of all of the data and environments you’re producing or something.\n\nRob Wiblin: So the bottom line is that compute might end up being the key limiting factor, in which case we will use our AI models to figure out how to get as much juice out of the compute available as we possibly can. And probably there are a whole lot of margins on which, if compute were the only factor now — I guess currently it seems like labour is probably, or I guess both are really important — but if compute was the only thing, then we would just use the enormous amount of labour that we now had to figure out how to use the compute more efficiently.\n\nI guess even if the intelligence explosion was underwhelming because this turned out to be a substantial issue, the number of AI researchers is growing, but it’s growing very slowly compared to the amount of compute that is being manufactured. I think the amount of compute available to these companies is growing by many multiples every year.\n\nSo if compute ended up being the limiting factor — if we went from labour mattering to basically only compute mattering — then that would still suggest a substantial speedup in research progress.\n\nBeth Barnes: Right. And the marginal investment in researchers now relative to compute is related to the price and the speed of the researcher labour. If they were a bunch cheaper and a bunch faster, it would make sense to buy a lot more. There might be a lot of things that’s just not worth having humans do at the current prices and serial times that would be worth doing with faster and cheaper labour.\n\nRob Wiblin: OK, just to make sure that we underline the most important messages here: AIs currently have a 50% chance of doing something on a computer that an expert might do in two hours. This is doubling every three to 12 months. So they’re becoming substantially more useful agents: they’re able to do things over quite a longer period of time and it’s improving rapidly.\n\nThey’re able to make a substantial difference to machine learning research now. They’re already used a great deal within the companies. They are probably capable of doing a whole lot more stuff than is even appreciated at this point — because if you really try hard, then you can get much more out of them than if you just make a cursory effort.\n\nAll of this leads to the conclusion that probably we should expect recursively self-improving AI pretty shockingly soon, relative to expectations that we might have had five or 10 years ago — that two years wouldn’t be at all surprising, which is alarming to me.\n\nBeth Barnes: Yes, I think this is very alarming. I think people should be very alarmed. The scientist in me wants to add a bunch of caveats to the two-hour number and the doubling time, whatever. But I think yes, it really doesn’t seem like two years would be surprising. It seems hard to rule out even shorter [timelines]. Is there 1% chance of this happening in six, nine months? Yeah, that seems pretty plausible.\n\nWith the exact shape of the recursive self-improvement, it still seems like we can kind of see a path to significantly superhuman intelligence that doesn’t require a mysterious intelligence explosion. It’s relatively clear how you would apply the intermediate capabilities that we expect to get to get something that’\n\n\nIt's just obviously very pretty scary.\nMaybe AI models will continue to be really non-robust in a way that’s hard to fix, but I don’t think we care about whether we’re very confident this is going to happen — we care about whether we’re very confident it’s not going to happen.\nIt just feels like we’re at the point where even if you only care about current humans, the stakes are very high.\nLike a 1% chance of takeover risk, which itself has a 10% chance of human extinction or something, that’s a lot of deaths.\nSo I think we should really care about numbers that are a whole number of percent.\nI mean, I think we should care about numbers that are lower than that, but basically I would expect the world in general to agree that it’s not worth imposing a whole-number-percent risk of completely destroying human civilisation for getting the benefits of AI sooner.\nRob Wiblin: A little bit sooner, yeah.\nI think people in Silicon Valley, or certainly people in the AI industry, these sorts of companies, they’ve seen these results and they’re flipping out basically.\nOr their expectations are what we’re seeing here: they just see a very clear path to superhuman reasoning, to superhuman ability to improve AI, to therefore a software-based intelligence explosion.\nSo this is kind of the main thing that people talk about in the city that we’re in right now.\nIt feels like policymakers are a little bit aware of this — certainly AI is in the policy conversation a bit — but by comparison, I think they haven’t yet flipped out to the extent that they probably would if they fully appreciated what was coming down the pipeline.\nBeth Barnes: Right.\nRob Wiblin: One of the hopes of this show is to disseminate this information so that people can flip out suitably.\nIs there anything more you want to say to someone who’s in DC, who’s like, “I don’t know that I really buy any of this.\nAI’s not my area, or I feel like these tech people are kind of losing their heads a little bit”?\nBeth Barnes: It’s kind of hard for me to have that perspective because I’ve just been focused on AI since 2017 or something, and I was previously thinking that it was further off, or that I might be totally wrong about this whole thing.\nBut it feels like we’ve just increasingly gotten evidence that these concerns are sensible.\nWe’re starting to see models exhibit alignment faking, all these things we’re concerned about, and it seems like the AI capabilities are coming very fast.\nSo this seems like pretty obviously the most important thing, even from a very personal, selfish perspective: for someone who’s young and healthy, this just might be your highest mortality risk in the next few years is AI catastrophe.\nI think the other sense that people have that I really want to dispel is, “But the experts must be on top of this.\nThe experts would be telling us if it really was time to freak out.”\nI’m like, the experts are not on top of this.\nThere are not nearly enough people.\nMETR is super starved for talent.\nWe just can’t do a bunch of basic things that are obviously good to do.\nAnd inasmuch as there are experts, they are saying that this is a concerning risk.\nThere was the report that Bengio and others wrote, that was sort of overall, what is the overall scientific consensus on this risk?\nAnd it was like, “Yes, this seems very plausible.”\nRob Wiblin: “We’re not sure whether we will or won’t all die, but it’s totally plausible.”\nBeth Barnes: Yeah.\nAnd to the extent that I am an expert, I am an expert telling you you should freak out.\nAnd there’s not especially anyone else who isn’t saying this.\nRob Wiblin: Yeah, the experts would tell us to freak out.\nI’m like, well, the experts in this really are just the people at the AI companies, and I guess external observers like you who are very close to the action.\nThere isn’t really a group of independent people who fully understand all of this and are paying a lot of attention to it in DC, in some agency that isn’t heavily entwined here, who could evaluate this in some more objective measure.\nSo basically, all of the experts actually are saying to freak out; it’s just that people are maybe sceptical because they’re in companies that are heavily involved and they think maybe they don’t have sufficient distance or perspective.\nBeth Barnes: Right.\nI think that’s one of the things where hopefully METR can provide a bunch of value: actually having the deep technical expertise, but also being independent and therefore more credible — that we don’t have commercial incentives or whatever to exaggerate this.\nAnd I do think the sort of dearth of experts outside the AI labs is alarming.\nRob Wiblin: Let’s maybe push on a bit and tackle a different issue.\nWhen I said on Twitter I was going to be interviewing you, I was actually a bit surprised at the main question that came up.\nIt felt very 2022 to me, but I guess maybe it makes more sense given what we were just saying.\nPeople were worried that having better evaluations of what the AI models are fully capable of doing — things like realising that if you put in a bunch of effort and you put in place the right structure for them to act as agents, then they’re able to do a whole lot of AI research autonomously with very little human intervention — they were worried that basically pointing that out was dangerous; it was an information hazard for people to even be aware of this, and maybe you shouldn’t be finding that out, or shouldn’t be publishing it very widely.\nWhat do you make of that concern, that possibly your work could backfire?\nBeth Barnes: This is something we’ve discussed a lot internally at METR, and done reasonable amounts of hand wringing over and talked to different people about.\nI think my views on this have changed a fair amount over time.\nThe key thing is like, do you think of safety progress as static or as a function of the current awareness of AI?\nAnd I think implicitly, before, I was thinking we’re making safety progress at some rate, and then independently there is capabilities progress, and it’s like a race between these two things.\nI think there’s a different framing, which is just as reasonable, which is that there is some hype and awareness in general of the current level of capabilities, and both the rate of investment in capabilities progress and the amount of investment in safety progress are a function of that.\nSo some speeding up underlying capabilities is not a problem if you speed up the safety more as a result of that — the relative progress that safety will have made by the time you get to a level of capabilities is now higher, even if that time is nearer.\nThere’s a bunch of different ways this plays out.\nOne is talent.\nThere’s a bunch of people who only started working on safety after seeing ChatGPT or GPT-4, some level of capabilities, like, “It’s actually time.\nThis stuff is real, or actually I’m scared of this,” or whatever.\nThere’s policy actions, which are again only going to happen once it’s sufficiently obvious that this thing is happening and these capabilities exist.\nThere’s the quality of safety research.\nIt seems like you can make much faster progress if you have better models to work with.\nNot that we’ve exhausted all of the possibilities for making progress on previous models, but there is a strong effect of your work is more useful when you have better model organisms of the things that you want to study and you’re sort of closer to crunch time.\nAnd also you can use models to automate your work more.\nWe might imagine that almost all of the technical safety work happens in the period right before an AI disaster would happen, because that’s when you’ve got the models automating a bunch of the alignment work and things.\nSo the thing that we should be prioritising much more is the fraction of investment at that point into safety, as opposed to when that point comes because the background rate of progress and safety is small now compared to what it will be.\nThere’s another factor here that I think is important, which is compute progress — independent of hyper investment in AI, there is just Moore’s law and economic growth and things happening in the background, which are making larger amounts of compute cheaper and more available.\nAnd in the extreme case of this, we can imagine if we know that aliens are going to dump this incredible infinite supercomputer thing on us at some point: should we do some intermediate AI research in the meantime that will allow us to learn some stuff about safety and understand a bit what is going to be possible?\nYes, that seems obviously good — as opposed to that only happening once you already have this incredibly large amount of compute where you can scale everything very quickly.\nIn fact, there is a tradeoff where you maybe are bringing the point at which stuff happens forward, because there’s more investment in hardware and things, but it’s still the case that in some sense you’re racing Moore’s law.\nYou’d rather have all of the things happen when the chips are bad and when compute is this limiting factor that’s slowly going up, as opposed to like, it turns out anyone in their basement could do this or something like that.\nAnd again, that’s an extreme example, but I do think in the intermediate points this is still true.\nSo that’s the case for why I think it’s not necessarily that bad to speed things up, if there’s some argument for why this differentially advantages safety.\nAnd I think there’s just a pretty clear argument here for why a better understanding of what AI models are capable of advantages safety: by default, this knowledge resides only in the labs — which are specialised in capabilities progress differentially, relative to regulation and broad understanding in policy and safety and things.\nSo it seems like moving that understanding from being just in the labs — where they will happily show it to their investors and people they’re trying to hire — moving that understanding from them to the broader public who don’t love all this racing ahead, or policymakers…\nYou could think about the timelines for this, like you imagine you need to get a certain level of awareness in time to get a policy passed before you miss a window that would have allowed you to mitigate this thing in the future.\nSo even if you bring it forward a bit, if you bring the policy action forward by more…\nRob Wiblin: Things have gotten safer overall.\nSo the bottom line is, I guess you can have different models of what is racing against what.\nAnd the simplest model would be that the safety stuff is improving on calendar time.\nEvery year it gets better, so every year that you bring forward some advancement in capabilities, that’s just making things worse, because it’s those two things that are racing against one another.\nBut I guess you’re saying you think about it in terms of the ratio of progress inside the companies to the amount of freaking out and action by other people outside of the companies which is able to make things safer, or at least get some sort of governance and policy response.\nAnd it’s only by publishing these results, so that everyone in the broader world understands what’s going on, that you actually get enough attention and enough concern that the problems can be solved ever at all.\nSo the worst-case scenario is that the companies are fully aware that they can automate most of their work.\nPresumably, this stuff is much less of a shock to them than it is to the broader world, because they have every incentive to try to figure out how to get the absolute most out of their models to save their own money and their own labour costs.\nBeth Barnes: Yeah.\nI think you should think either we’re not making that much difference relative to progress inside the companies, or METR is just incredibly awesome and overpowered and you should join — because clearly with our 10 researchers or whatever we’re beating these multibillion-dollar companies or whatever.\nRob Wiblin: So should we conclude that the companies probably were already aware of the results in your research into how good the AI is at recursively self-improving?\nBeth Barnes: Yeah, basically that’s generally the impression we’ve got.\nI think companies like to sometimes give the impression that everyone outside is totally clueless; our stuff is far superior.\nAnd we definitely have found that often that’s not quite the case — our elicitation is as good or a bit better, but basically it’s not dramatic in either direction.\nAnd especially for stuff like kernels, where it makes much more sense for them to invest in that specific thing, maybe we have similar understanding — where rather than just doing a four-week project on it and then being like, “OK cool, we’ll just let people know about this,” they would actually double down on that and get a bunch better, and get profitable stuff out of that.\nRob Wiblin: There presumably is some downside to this, where maybe the big three are aware of how much they can automate things, or they’re roughly aware of your results already — but there might be other actors, other people who are working on AI or nearby where making this more salient is more likely to have them make an effort to automate their own research.\nBeth Barnes: Yeah, I think that’s true.\nI think also another concern people have had is basically related to what we were saying earlier about people irrationally underinvesting in things that are slightly annoying or don’t look like the most glamorous pure ML work of gathering data and making good evals: that us publishing evals is substantially accelerating companies because they can hill climb on those.\nSo none of the things that we’re releasing have enough tasks that it would make sense to directly train on them.\nIt would just be a sort of hill climbing to\n\n\nSee how well your fine-tuning was working.\nAnd again, it feels like if this was the case, it would mean that we should be able to sell METR’s eval producing services for very large amounts of money if we are in fact substantially accelerating these multibillion-dollar projects.\nBut I guess the claim here is that people are irrationally not spending enough on evals.\nAnd in general I want us to try and be sympathetic to these concerns and do a kind of trade: even if we don’t buy particular concerns, to try and make concessions to them when it’s cheap.\nBut also I think there is a trap of getting bogged down in worrying about how maybe this will do harm — where all the safety people kind of tie themselves in knots worrying about that, and then nothing actually happens.\nRob Wiblin: They can’t even communicate between themselves, or they’re not explaining their views to the public properly, or the evidence for their concerns.\nBeth Barnes: And there’s definitely something that just feels a bit overly prideful or something about assuming that your tiny team is contributing a large fraction… You’re such a small fraction of the total labour and investment going on in this.\nIt does seem like you have to think that you’re really exceptionally talented to be thinking that you would be pushing things ahead substantially.\nRob Wiblin: Does thinking about things this way suggest that you should be trying to communicate all of your work as strongly as possible to government people — or to people who are outside the tech industry — to really explain to them that they haven’t appreciated just how dangerous things are or just how quickly things are moving?\nAnd maybe the less you can publicise your results… I guess it’s not really imaginable that these findings wouldn’t spread throughout San Francisco.\nBut basically you should be trying to do as much to disproportionately reach people who currently are not being alerted to what’s going on.\nBeth Barnes: I guess I don’t expect it to be that efficient use of labour to try and communicate to one and not the other.\nEspecially as I would have thought that even the people who are not directly involved in the technical world will use what was the general technical reaction to this as a barometer of whether they should trust it.\nAnd we just want evals research, and understanding of model capabilities, and understanding of whether mitigations are good to grow and proceed as a field.\nSo we’re trying to open source things and be transparent about our learnings and facilitate other people getting funding and support.\nWe generally want to grow this field, and I don’t think the payoff of trying to be secretive about it and then just talk to these particular people is worth both the effort of doing that and the cost of not accelerating other people who could be working on this.\nMaybe one more point here is just that basically the types of capabilities improvements METR has been doing are very much around scaffolding and language model agents.\nAs we were talking about before, it seems much better to have a dumber model that can do a lot of stuff because you’ve elicited it right and given it all the right tools — but then you have this understandable trace of what it was doing — than to just build bigger models which mysteriously start being able to do things, but you don’t really know why.\nSo I think differentially pushing forward the capabilities of LLM agents seems actually maybe good.\nRob Wiblin: My intuitive reaction to this concern was just that if an effort that you’re trying to make to keep us safe is just to not elicit what the models are already clearly capable of doing, that is an incredibly thin wall of safety — because at any point, if someone put in the effort that you guys have put in to figure out how you can get more out of the models, then they could immediately speed things up very rapidly.\nIt’s an incredibly perilous situation to be in where the models are capable of doing far more dangerous things than what people appreciate, or what even the operators appreciate.\nAnd then as soon as someone actually spends a week working on it, then they can crack it and do it.\nReally we need to be alert to what is possible as soon as it is possible.\nBeth Barnes: Yeah.\nI think there’s some general thing in how people have different feelings about how competent the world is, which both affects what fraction of labour does METR represent — or would someone else do this, or maybe we’re the only ones who’d have an insight or something — and how useful is it to communicate to the world?\nWill policymakers react in any way that’s helpful?\nI can definitely see where people are coming from, and I do think a bunch of these concerns are quite reasonable.\nI just overall think it nets out to: security by obscurity about what models can do is not a great safety strategy.\nRob Wiblin: There’s been this interesting discussion back and forth about how pessimistic we should be about societal/government responses to crises and problems of all different kinds.\nI think the first step in the conversation was more technical people saying, “Look at how useless the government is at responding to crises.\nThey aren’t understanding these issues whatsoever.\nNo one’s paying attention to it.”\nAnd then other people are pointing out maybe that’s a little bit pessimistic, that the reason that people weren’t reacting is that they felt that this was very far off; it wasn’t really concrete enough to do anything yet.\nIn fact, if you look at COVID, perhaps we over-responded.\nThe government actually was extremely active once it was apparent.\nStefan Schubert uses the term “sleepwalk bias,” where the question is, do we sleepwalk into or through crises?\nHe was like, no, generally, actually once it’s evident that there’s a problem, then you can get perhaps even an overreaction on average among society and governments.\nI’m like, if you look at the situation here with AI, it feels like we’re really underreacting.\nI guess more attention has gone into it.\nSome stuff has been done.\nI would say it’s barely like 10% of what needs to be done, at least from my point of view.\nAnd I think the synthesis of these views is that society doesn’t sleepwalk through a crisis, but it does sleepwalk into crises.\nBeth Barnes: Right.\nRob Wiblin: Because basically there’s only so much focus that people have.\nPeople are very busy dealing with crises that are already unfolding right now.\nThey don’t really have much time to plan ahead.\nAnd also they don’t have the technical chops to understand what things are likely to happen in future, so they tend to just dismiss forecasts that suggest anything interesting is going to change.\nSo my expectation is basically we will sleepwalk into a disaster with AI — or indeed almost all kinds of disasters — with massive underpreparation, underinvestment.\nAnd then at the point that it’s undeniable that things are going extremely wrong, then you can get a very intense response and a lot of focus on it.\nAnd there’s a question of, is that enough to actually meaningfully change anything whatsoever?\nI guess it remains to be seen.\nBeth Barnes: Yeah.\nThere’s a bit in Don’t Look Up where they’re trying to convince the president to be worried about this incoming planet-killer asteroid.\nAnd she’s like, “Every day I have people telling me the world’s going to end for this reason or that reason.\nOn Monday it was terrorists.\nOn Tuesday it was floods.\nOn Wednesday it was the bees are dying.”\nI think people can be kind of unsympathetic to that.\nThat even if you are trying to pay attention to the right things as a policymaker, everyone has incentive to exaggerate the seriousness of their particular cause area.\nAnd if you don’t have the technical knowledge and there’s disagreement in the technical community, it’s actually just quite hard to figure out how to respond.\nRob Wiblin: Yeah, it’s understandable structurally why things play out this way, but it is unsatisfactory.\nIt would be really great if we could come up with better systems for anticipating future problems.\nBeth Barnes: Yeah.\nAnd I do think also people maybe have more of a picture of policy stuff being very bottlenecked on willingness — whereas I think it’s quite amenable to more concrete and sensible suggestions, and that sort of decreases the amount of willingness that you need by a lot.\nLike SB 1047: there was this bill in California that would have required some evaluations and transparency about them and companies to make safety cases.\nA significant reason for why people didn’t like that or why it didn’t pass — at least that was quoted — was some combination of worrying that yes, this particular proposal is reasonable, but in general it’s going to spiral into overregulation and things will get added on and people will interpret it creatively.\nAnd these evals don’t really exist, or people haven’t done this before.\nI think if you’re in a world where there was just very clear precedent of, “This is what you need to do, this is how you make your safety case, this is how you decide whether it is or isn’t good enough,” that would have made it much easier to get something like this passed, because there’s less of the slippery slope concern the more well defined the thing that you’re asking for is.\nAnd it’s just harder to object with, “This might be extremely costly” if you’re like, “No, look: someone has done this already, and this is how much it costs.”\nSo I do think people have this picture of things being very much bottlenecked on lab willingness or policy willingness to do things, and I think there’s just a bunch of stuff that third parties can do that makes this much more likely to happen.\nEven things that are just reminding people and nudging people and sort of project managing things from the outside — of being like, “You need to get this stakeholder.\nWe’ll help you put together what you need to get this through this stakeholder.”\nThere’s just a lot of value being left on the table in terms of making it easier for policymakers and for labs to adopt policies that we think are good.\nRob Wiblin: I think that feels a little bit generous on the SB 1047 case, but maybe I can put to you my impression on it, and you can tell me how wrong I am.\nI think what you’re saying is that there’s concerns about slippery slopes, concerns about costs and compliance.\nThat would have made sense maybe with the original draft of it, which perhaps was a little bit sloppy, and could be interpreted somewhat expansively; perhaps it would have been more difficult to comply with.\nBy the time they had narrowed the scope and drafted it more carefully in response to criticism, by the end it felt like it really was a bare minimum ask.\nIt didn’t really feel at all over the top to me, but it still wasn’t able to get over the line despite, a lot of support among the general public.\nI guess ultimately it was the decision of one person, the governor of California, to veto it.\nBut the reason I would expect that it didn’t get over the line is just the same reason that it’s generally very hard to regulate very powerful, very rich, very profitable, very influential, very well connected, very well prepared companies: the companies don’t want to have their hands bound.\nThey don’t want to have all of this scrutiny.\nThey campaigned very strongly to try to stop it.\nAnd it’s unsurprising — given that they had put so much effort into cultivating the various different political connections towards funding politicians — that this kind of thing is very hard to get over the line until it’s unambiguous that there is a problem that has to be addressed.\nAm I too cynical?\nBeth Barnes: I don’t know.\nOther people maybe would know more what the real reasons were.\nMy impression is that at least something that was still being quoted even towards the end — so still a useful argument against it — was that this compute number of FLOPS was not a good measure of is this thing actually dangerous, when this should kick in, and various things.\nSo I don’t know, maybe this is a smaller on-the-margin claim that it makes it harder to argue against if you have good precedent and clear requirements for your thing.\nI do think there can be other things as well, where just additional labour can make things go a lot better — where the lobbyists or government relations people for companies can be out of touch with their company’s actual desired policies or whatever.\nOr at least there’s strong evidence that you could take to them, like, “No, look: the company has been asking for this type of regulation.\nIt’s proposed its own version of this.\nYou should be supporting this.”\nAnd the people who are the lobbyists and stuff really totally are not tracking this.\nPeople are just doing their normal thing.\nRob Wiblin: I guess the thing that they’re always trying to do is avoid more scrutiny and more compliance cost and more regulation and so on.\nSo that’s their default mode.\nAnd unless there’s a very strong push inside the company to change that in this specific case, then that is what they will just naturally want to do.\nBeth Barnes: Or outside.\nI don’t think it necessarily has to be inside.\nI think you can also just communicate to people or make things more salient to people.\nRob Wiblin: Anthropic, with the final draft on SB 1047, was neutral to positive about it.\nI think Dario Amodei, the CEO, explicitly said that this would not be that difficult to comply with, and that the companies that are saying they could never comply with this are basically talking nonsense; it actually is quite a narrow ask.\nSo credit to them for actually telling the truth.\nBeth Barnes: And I think xAI supported it as well.\nRob Wiblin: Yeah, of course.\n\n\nBecause Elon’s very concerned.\nI’ve interviewed Nick Joseph from Anthropic about evals. I’ve also spoken with Allan Dafoe from Google DeepMind about evals. Both of them — perhaps unsurprisingly, given that they work there — said that they thought their evals were pretty good. That the Responsible Scaling Policy in Anthropic’s case or the Frontier Safety Framework in Google DeepMind’s case, that those are actually moving the needle on safety. That these are perhaps not the final form of these things, but they’re a very good step in the right direction.\nTo what extent do you agree, with a more independent, outside-of-the-companies point of view?\nBeth Barnes: I agree that it’s a step in the right direction, moving the needle. Some things I tend to have disagreements with. I think often sort of individual safety-concerned people inside companies have a rosier picture of how the company interacts with third parties, like, “We love safety, so of course we would be supporting all of these things. And if it’s not happening, the bottleneck must be something that’s not us, because we really love safety.”\nI think Nick Joseph said something like this, of the bottleneck on getting external oversight of the RSP is people who are technically competent to do the evaluations externally — which I very much disagree with, because I think METR has the technical competence. I think our evals and elicitation and things are better than stuff that the lab has published, than any lab has published, basically. At least in terms of quality; I guess our datasets tend to be smaller but higher quality. And we have been eager to work with labs.\nAlso, it doesn’t necessarily need to be bottlenecked on the technical competence of the external evaluator to literally run all the experiments and set up the compute and things.\nThere’s another paradigm, where the companies run the evaluations internally and they write up what they did, and the external evaluator goes through it with them and discusses it, like, “Did you provide good evidence for this thing? Maybe you need to do this other experiment here. This thing was insufficient for this reason.”\nOr an embedded evaluator paradigm, where someone from an external third-party org is embedded on the team in the lab that’s running the evaluations, and is keeping an eye on everything, and making sure that they’re not sandbagging or fudging things.\nThere’s just lots of things here which we’ve proposed and been open to, and it’s not like the companies are all banging down our door to do it.\nI mean, there have also been cases where we’ve turned companies down because we don’t have the capacity. But that is because we were like, this arrangement will be good and would be worth our time to do and it would take less of our time and capacity — but they weren’t offering that; they were offering something else that we didn’t think…\nRob Wiblin: Something much more laborious for you?\nBeth Barnes: A combination of more labour intensive for us, or we just didn’t think it would provide that meaningful assurance or wouldn’t be improving the standard of assurance provided.\nI think maybe people have a confusion about METR, and have been assuming that METR is a formal evaluator that has arrangements with all the companies to do evaluations for every model.\nThis is not the case. I wouldn’t want to describe any of the things that we’ve done thus far as actually providing meaningful oversight. There’s a bunch of constraints, including the stuff we were doing was under NDA, so we didn’t have formal authorisation to alert anyone or say if we thought things were concerning.\nSo yeah, think of it much more as we’ve been trying to prototype what this could look like. And because we’re a small nonprofit, it’s easier for us to be more nimble and try out different things, and lower stakes for the labs to engage with us. So we’ve been excited about raising the bar on how good the oversight actually is.\nAnd labs more want a “Can you promise that you will run an eval on all of our models when we want to, so that we can say we’ve run an eval on them?” sort of thing. And that’s not what we’re excited about, if we don’t think that that proposed procedure is actually providing meaningfully more safety assurance than not.\nRob Wiblin: In general, people think that — at least within the scope of what you’re trying to accomplish — the evaluations that you’ve created are very good. I haven’t heard that many criticisms of them on the substance. And you’re saying the companies could come to you and say, “We would love for you to run all these things on our models before they go out, and for you to play with it as much as possible in order to figure out how dangerous they are.” But they are not requesting that. They’re not actively making that easy for you.\nBeth Barnes: I mean, we have gotten asks like that from a bunch of labs. I think the problem is that our goals are like prototyping and de-risking what could provide really good safety assurance.\nSo that includes better governance mechanisms, in that this actually feeds into decisions or at least is shared with the people who are making the decisions. And we have some carveout for being able to say we made a recommendation and the lab chose to not go with that recommendation, or that employees get to see what was shared with us so that they can check that it was accurate. So there’s governance mechanisms that we could be improving on.\nThere’s the pre-scaleup or pre-internal deployment versus external deployment, where we think this specific time of external deployment is not that interesting; we’re much more interested in what are the best things you have internally and wanting to know that companies are not building arbitrarily powerful things internally.\nThere’s other stuff, just like quality of access and information shared about what agent scaffolding has this model been trained to use and can you share that with us? And a bunch of stuff that is very important to know to be able to elicit the full capabilities of models, things like can we see the chain of thought?\nRob Wiblin: I guess they would say to do things the way that you want is a lot of staff time, and they’re sprinting all the time in order to try to keep up. Maybe they tend to play the cards very close to their chest, and this would involve sharing commercially sensitive information with you, so they’re nervous about doing that or perhaps their lawyers at least are nervous about doing that.\nAre there other things that they could say in their defence that have any reasonableness to them? Or to what extent would you think it’s fair versus maybe closer to being excuses?\nBeth Barnes: Yeah, it definitely just is effort. It would be a thing, and maybe there’s some ways that it could backfire or something. The cost is more attention from more senior people at the company or something, I would guess, and thinking how much they want to really do.\nThe object-level confidentiality concerns, the things we’ve proposed is like sharing with us anything that’s not compartmentalised within the company. If your 1,000 employees already know this, three people at METR knowing an anonymised version of it is not really going to increase the surface area very much.\nOr taking staff time on the inside: it’s a much smaller fraction of your staff time than it is of METR staff time.\nOr spending 1% of the effort that’s spent on the model on capabilities research on evaluations, or a few percent of that effort, would be enough to do this.\nI don’t know, there’s stuff about token budgets and things. And OpenAI had their superalignment compute commitment, which was supposed to be much larger numbers than anything that we were asking for.\nRob Wiblin: Twenty percent of their commitment up to that point.\nBeth Barnes: Right, 20% of the compute. So it’s not like we’re hitting those demands.\nI think there are a bunch of things which are just technically and logistically challenging for them. If we’re just like, can the API please not be broken and not keep going down while we’re trying to use it? It’s like, well, before it’s stabilised for public deployment, there’ll just be issues.\nI think in an ideal world, it would be like you wait to deploy. If pre-deployment was really the important thing that you needed to gate, you wait to deploy until you’ve given your evaluator proper access, and that they’ve had enough time with a stable version of the thing that’s actually the thing that you’re going to deploy, not something else. You can’t directly solve the technical problem of just make it work, but you can solve how you’re prioritising to get a good version of the evals done versus get it out as quickly as possible.\nRob Wiblin: I guess a cynic would say they’re not really keen on having an external group be free to really heavily scrutinise the models, maybe scrutinise their plans for training models that they haven’t even trained yet, and be free to notify the public or notify policymakers about concerns that they have — because what company would want that?\nThis is a bunch of risk for them. It’s a bunch of stuff that they’re taking things somewhat out of their hands. These are all the reasons why you want independent auditors and so on, but what company ever voluntarily accedes to that?\nA more sympathetic take might be companies are kind of always struggling to make things work. This is just another project that could be challenging for them to operationalise, even if they’re not that nervous about what I just described. There’s lots of good things that they could do that they’re not doing, and this is just one of them.\nTo what extent would you adopt the cynical view, or would that be kind of a fair baseline?\nBeth Barnes: I can definitely see it from both sides. I think you could be a company really prioritising safety and this could not make sense as a place to invest your marginal resources. I do think that is not literally the tradeoff that is getting made. And I think individuals at companies who are like, “Wait, surely we’re giving you that access, right?” are sort of not tracking the actual thing that’s happening.\nBut on the flip side, they’re doing much more than you might expect companies just selfishly interested to be doing. I think with different companies it’s somewhat different reasons. Smaller ones, it’s somewhat more just like it’s chaotic and they just can’t do that many different things at once, and everything’s a bit last minute and the infra isn’t there. And with the bigger ones it’s like bureaucracy and lawyers.\nRob Wiblin: Decision making is slow in general, and this is a bit of an unusual arrangement. It’s not something that they would do with their other products, for sure.\nBeth Barnes: Yeah. And again I think you could paint this as the willingness of the labs. And I do want to disabuse individuals at these labs who are like, “Of course we’re doing all these great things, right?”\nRob Wiblin: You should actually go and check, because maybe not.\nBeth Barnes: Yeah. But on the other hand, I do think it’s where we just really haven’t pushed that hard yet, because METR has been building up our technical capacities such that we can definitely make use of the resources that we’re being given access to, and also just capacity to run the standard lab politicking playbook, and make the requests really clear, and accelerate them up the chain or get support from a bunch of different people and then make it happen.\nThus far we’ve more just been kind of vaguely showing up and asking nicely and trying to be clear about what it is we want. We haven’t gone into the mode of like, now we’re going to actually push on this. I think this has just been bottlenecked on talent to be able to do that, so I’m optimistic about us getting significantly better access in the relatively near future, because I think there’s stuff that you can do to push on it.\nRob Wiblin: I see. So you’ve kind of suggested these things somewhat informally, maybe a little bit last minute, because you’ve still been developing the evals, and you’ve still been trying to hire to make sure that you have the people so you can actually follow through on the things that you’re asking to be able to do.\nSo you’ve nowhere near exhausted the options to try to get access earlier, and more serious access. I suppose these relationships could also deepen over time as trust is built, and it’s still perhaps at an early stage. Well, I guess it’s possibly at a very late stage in the broader situation, but the relationship between you and the companies might be at an early stage.\nBeth Barnes: Yeah, we don’t want to play hardball if we don’t have to. If some of the fault is on us, we don’t want to ask people to do really costly things and then be like, “Oh, actually…”\nRob Wiblin: “We can’t do it.” Yeah, that’s totally fair. A different, though I guess related, topic: over the years, I think many people have gone into working at the various AI companies with the vision that one way they’re going to be helpful is shifting the average beliefs of people who work at the companies.\nMaybe they’ll get a chance to talk to their colleagues and convince them that concerns about how AGI might go are\n\n\nMore legitimate than they might think. I guess they’re also just shifting the weight of opinion within the companies, and hopefully moving their culture in a positive, more cautious direction.\nWhat do you think the track record of that is? Is that a reasonable approach for people to take going forward?\nBeth Barnes: I basically think the track record of that particular theory of change looks pretty bad. I think if you just look at the history of people who tried to do that, there’s just a lot of people giving up and leaving. People have started at one lab trying to influence there, and then have kind of given up on that and left.\nI think there are probably some people who’ve done this and it’s gone relatively well, but I think they’re very much in the minority.\nI think reasons it makes sense to go to a lab is if you think your comparative advantage is institutional politicking, and that’s basically what you plan to do with most of your work. And if you’re very good at that, I think maybe you can do a better job.\nAnother reason is if you are going to be doing the implementation of safety things at that company. I think if you’re just sort of advancing alignment research generally, it’s better to be outside of a lab — because it’s easier to share it with everyone else and collaborate, and easier for other labs to onboard it if it’s not this thing from a competitor.\nRob Wiblin: Can you elaborate on that? What sort of work are you describing that would be done better outside?\nBeth Barnes: I think most alignment and interpretability work would be better if it was outside labs.\nRob Wiblin: That’s a big claim. I guess most safety/alignment technical people work inside the labs now. What arguments have I heard? Of course they would say maybe it’s easier to get implemented on the company’s models if you’re inside; you have access to more compute; you have access to the cutting-edge models, and you need to have access to the very frontier in order for your work to be relevant. What would you say to stuff like that?\nBeth Barnes: I think the fact that people’s choices here don’t seem to be that responsive to the quality of the open source models available is maybe evidence that it’s not actually driving their decisions, and it’s instead other factors.\nI just think that for a lot of research, it’s really not clear that you’re actually better off inside a lab in terms of infrastructure and model access — because lab infrastructure has a bunch of constraints due to security, among other things — and it’s just actually easier to work with open source models, and you can get work done faster often. And especially for open source, smaller models are much more optimised.\nSo if you want to do RL, or you want to do some experiments with cheaper models, the best ones might actually be outside the companies. I feel like it’s a big shame that Anthropic’s interpretability research was not on open source models that could then be shared, and people could kind of poke around with it.\nAnd in terms of just actually paying for clusters or something, I think there’s enough funding available. I don’t think that’s going to be a huge bottleneck unless you’re doing massive pre-training experiments. I think that’s not a crux in most of the cases.\nI also think — and maybe this is less true now if we are just close to the end times — but in the past I would have said that if you think that your research is much more productive if you have access to the very latest models, then that also means that the research you’re currently doing is just really unimportant compared to the research you’ll be doing in a while when the models are better.\nRob Wiblin: Because it will all be superseded?\nBeth Barnes: Right. Well, if it is the case that your research is much better with today’s models than last year’s models, either there is something special going on about this particular point in time, or your research is also much less productive now than it will be when you have the models a year in the future, and much less productive again than the models a year after that.\nSo if you believe that it’s very important to have access to the most cutting-edge models, then that also implies that your research now is a small fraction of the total value that your research will provide when you have access to these more advanced models. So that suggests you shouldn’t really be prioritising what allows me to be most productive on research now.\nRob Wiblin: You should be preparing yourself to do work in future.\nBeth Barnes: Yeah. Or prioritising buying as much time as possible at the point where you have these models that really speed up your research, which maybe points to working on evals.\nRob Wiblin: I see. OK, just to reiterate some of that: people definitely make the argument, “I have to have access to the frontier model trained by the best company in order for this to be relevant at all.” You had one objection there, which is if all that matters is what’s done on the frontier model, then maybe what really matters is what’s done at the precursor model just before these models are truly dangerous, and everything now is going to be kind of by the by, by time we get to that stage.\nBut also I guess these days it seems like the best open source model is virtually as good as the best closed weight model, the ones that are inside the companies — or at least it’s nipping at the heels of those models, as far as we know. In that case, the argument that I need to have access to the very best model is much, much weaker than it would have been, because you could just download R1 and just run it on your own computers.\nAnd I suppose you could imagine that inside the companies you have to bid for access to compute, you have to coordinate with a whole lot of other people in getting access.\nBeth Barnes: Yeah, there’s a bunch of security setups, which means you can’t easily run open source models.\nRob Wiblin: Whereas you could just buy your own H100s, buy your own chips, store them in your own office and then download R1 — and then you can just start doing whatever you want with a tiny group that’s committed to that.\nI guess that might help to explain why it is that groups like Redwood Research have managed to really kick ass. It seems like their research is remarkable. I guess they collaborate with the companies, but primarily they’re operating independently.\nBeth Barnes: Yeah. And I think this has been the lesson of people who’ve done research inside and outside of companies: it’s not clear that the internal access allows you to make research progress faster.\nRob Wiblin: Another argument you might make is that it seems like most people have tried to go inside the companies, so perhaps there are neglected opportunities for people to not be inside the companies. The weighting is really 80/20 in terms of…\nBeth Barnes: So, sorry, I had three reasons for being inside companies that I didn’t finish listing. So, one, if you actually really are into being a sort of political influencer person, and that is your skill set and you’re going to do that.\nThen the other two I would list are you’re implementing things — not doing the general alignment research, but just making sure that this actually gets into the production system or otherwise being very close to the stuff that’s actually happening — and being like, “If the company isn’t cooperating with third parties, I can at least run the evals and then me and my friends will know whether we should freak out.” Especially in worlds where things are not going very well, having some people inside the companies who are trying to actually get the basics implemented is useful.\nAnd then I think that the third one is sort of similar: basically being the sort of person who would whistleblow if there were egregious violations of things, and getting into a position where you have awareness of that information. I think a lot of the actual mechanisms for regulation or oversight to work do require that you trust the company to not just completely lie to you or give you totally misleading information. So I think it’s very important to have some number of people internally who would have the guts to whistleblow if the lab was feeding misinformation to regulators.\nAnd I think that does in fact take a lot of guts, as evidenced by the stuff about the OpenAI secret nondisclosure agreement that no one —\nRob Wiblin: Hundreds of people put up with it. And it took like one person who was really stubborn to dig in their heels and say, “No, I’m going to make a fuss about this.” Kudos to Kokotajlo.\nBeth Barnes: Yeah. And Will Saunders.\nSo I think if that’s your sort of theory of change: you’ve got to be pretty confident that you are that sort of person. And I think there is some correlation between the sort of person who’s like, “The lab is offering me the most money, and it’s convenient and high status and cosy and comes with all the benefits; I guess I’ll go there,” and not being the sort of person who’s actually going to whistleblow. And that there’s some kind of selection.\nRob Wiblin: Yeah. They might filter against someone who seems especially whistleblowing.\nBeth Barnes: Yeah. I do think labs also just actively filter those people out.\nRob Wiblin: I’ve maybe been surprised at the degree to which they’re willing to hire all comers, or it doesn’t seem like there’s really intense filtering on personality type. It’s not like going and working at the CIA. It feels like in some ways there could be more scrutiny on people’s motives.\nBeth Barnes: I do think there is a moderate amount of selection against having too strong of principles or something.\nRob Wiblin: I guess that’s true in companies in general, that they often don’t want to have firebrands or people with a strong independent agenda. But you’re saying that is also somewhat true here?\nBeth Barnes: Yeah. I mean, this is reasonable in many ways. Often those people are in fact unreasonable. And there’s a sort of unilateralist curse of people who are going to be unilateralists will, in expectation, do unhelpful things.\nSo actually implementing research or being a whistleblower are things that I think makes sense to be inside a lab.\nI think people go there for reasons that are less good. One is this “I’ll influence in a good direction” — which I just think has a bad track record of effectiveness versus influencing from the outside, which I think has a much better track record.\nAnd one is this “I have to have access to the latest models” — where I don’t really buy that.\nI think naively one would expect that relatively too many people go to labs than go to nonprofits or governments just because it’s a nicer job in other ways, and they have more experienced and aggressive recruiting departments and this kind of thing. They pay way more than government and have better benefits and stuff. It’s more of a high-status job in general, and you maybe get better technical mentorship than being some of the founding people at a smaller org.\nBut I also think people are sometimes wrong about how much growth you get from just “go and try to do the thing yourself” as opposed to being in a larger organisation with people who are better at it than you — and that you grow most rapidly from just doing the thing. If someone’s like, “I want to go to a bigger org to grow my leadership skills, because I can learn from other people,” I think you would grow your leadership skills more by just leading this thing that’s right here that needs you to lead it.\nSo I don’t know, I think there’s some amounts of motivated reasoning. In general, unless you have a particular reason to think that you would be a particularly good fit at a lab and not a good fit at a nonprofit or government, I think you should probably not go to a lab.\nRob Wiblin: Yeah. Just as an aside, it’s interesting that you call them “labs.” I switched to calling them companies at some point because someone pointed out or just asked the question: “These organisations, do you think their primary nature is as a research laboratory or as a company?” — and I was like, obviously it’s as a company. Maybe at some point in the past they were primarily research laboratories, or that was the right way to think of them, but it feels a little bit hard to see that now.\nBeth Barnes: Yeah. I don’t know. This is not an assessment of… I agree they are companies. It’s just fewer syllables.\nI do think it does make sense to have at least a few people who really care at each lab who can be implementing the basic mitigations, can be running the basic evals, can be keeping an eye out for egregious misconduct.\nI just don’t think that everyone piling into the labs, if you’re just generally doing public good safety research, makes sense. Or for some generic “I’ll just be there and influence them in a good direction.” I feel like I see this a lot from people, and it’s like, “But you’re a really introverted, technical person. And in fact, in practice you just spend like eight hours a day coding. It clearly doesn’t make sense as a specialisation for you to be like, ‘I’ll be here and influence things.’”\nRob Wiblin: You told me earlier this week that METR’s strategy has over time changed to adapt to what is a somewhat more pessimistic or challenging situation. Can you elaborate on what you mean by that?\n\n\nBeth Barnes: I think this is more relatively recently feeling like timelines seem short and we’re not in some of the best worlds in terms of how much is getting done on the safety front. I think originally when we were designing what would become RSPs, or responsible scaling policies and thinking about what evaluation regimes would make sense, we were imagining something like you want to keep the overall risk of catastrophe / end of civilisation from AI below 1%. And if there’s several labs, they each need to be a bit below 1%, and any particular deployment probably needs to be below 0.1% because you’re going to get multiple shots, et cetera.\n\nThis just seems like what is reasonable from a societal perspective. Personally, I care about future people also, so I think we should be aiming for something lower than that. But I think even just from the perspective of people alive today, the benefits of getting AI sooner aren’t good enough to justify much higher risks.\n\nBut I think we’re increasingly in the world where it doesn’t seem like we’re going to be keeping the risk that low. We can describe what you would need to do to be in that regime, and I don’t think that we’re doing it, that the world is doing it.\n\nAnd if we’re pushing on things like, how would you make a really robust safety case that would have good external oversight, maybe this is in fact a distraction from can we just do the most basic mitigations? And maybe we don’t even have time to check if they’re working, but let’s at least do them. Getting the risk from 30% to 20% or something like that maybe is more what we should be focusing on.\n\nRob Wiblin: So the basic difference is you could spend a whole lot of time laying out what would be absolutely ideal practice that would get the risk down to 0.1% or lower in aggregate. But if we’re not even doing the absolutely most basic stuff already — the stuff that would bring it from 30% to 20% — then obviously you would focus there, because the impact is way larger. And I suppose those are also the marginal changes.\n\nBeth Barnes: I think “ideal practice” even is too strong. I feel like the ideal thing looks way more intense, and this is just the sort of bare minimum if society as a whole was making reasonable decisions that I think would be roughly what everyone endorses. But going from that — which looks like being quite careful — to just trying to, on the margin, get the risk down a bit.\n\nRob Wiblin: Yeah. So you think the risk is like 30% on the current track that we’re on?\n\nBeth Barnes: I usually say something like 50/50 in terms of do things broadly go well. Do we capture most of the value of the future? So not all of the 50% looks like obviously a disaster for currently alive people or something. It might be some lock-in or more gradually going off the rails or something like that. But I think a bunch of that is just like war and catastrophe and AI is taking over — and it’s very clearly “everyone agrees this is bad” territory.\n\nRob Wiblin: I guess there are a handful of people who like that, but it’s a distinct minority. So starting from that picture where almost not even the basics are being done, what is the marginal thing? What would be your next ask, given the situation that we’re realistically likely to be in in a couple of years’ time?\n\nBeth Barnes: There was a thing we were talking about earlier of chain of thought faithfulness, and actually looking at the chain of thought — so just doing basic checks for alignment of like, have we looked at a reasonable sample of the reasoning the model’s been doing, and can we understand this, and is it blatantly reasoning about how to trick us or do bad things or something? That’s one very basic thing I can imagine is just something we neglect to do.\n\nAnother intervention would be making sure that we keep the chain of thought interpretable. Which might be like companies might find some fancy new way to get more performance out of the model, but it now means that it’s just like neuralese and humans can’t read it at all. That would be very sad. So if on the margin there are some techniques that we can do to either ensure that we can interpret that, or to bring the performance of the interpretable thing up so it’s less of a hit relative to the uninterpretable thing, that would be pretty good.\n\nRob Wiblin: “Neuralese” is this term for gibberish reasoning by the model that I guess is useful for it to reach a conclusion, but humans can’t make any sense of it.\n\nBeth Barnes: Yeah, or this could also be like it’s just passing numbers to itself. It’s not even outputting tokens that look like words or anything.\n\nSo yeah, it feels like we’re in a much better position than we might have hoped to be in, in terms of having models that do a lot of their cognition through legible reasoning. But people are working hard on making that not the case. That’s one thing that feels like it just gets a big increment on risk: how you handle that.\n\nI think there’s just a lot of other very basic risk of misalignment incurring stuff. Some combination of what is your training data like, and how much are you incentivising the model to knowingly lie and reason about what the humans want, and reason about playing the training game, and how much benefit is it getting from doing that?\n\nRob Wiblin: I was just about to ask, is it actually imaginable that we could be so stupid as to just look at the chain of thought, find out when it’s scheming, and then just negatively reward that and try to discourage the visible scheming? I guess people don’t realise the trouble with that is that it could just cause the model to become cleverer about hiding its scheming. It doesn’t necessarily actually reduce the scheming. In fact, probably it would cause it to become more surreptitious. So that’s actually a negative thing to do, rather than a positive one.\n\nBeth Barnes: Right, yes. You know, back in the olden days of AI safety, people used to worry about having a sudden jump in intelligence: that models would go right through this regime where you have models that sort of scheme, but they’re not smart enough to have it not be detectable for us. But now I feel more confident that we’re already seeing models that can do this, so we’re going to have some period of time where the models are smart enough to do this, but not smart enough to hide it.\n\nBut the thing that I worry about is that we’re going to see signs of egregious misalignment and then we’re going to be like, “Maybe that was just noise. Let’s train it not to do that and slap some patches on it” — so that we totally will have had evidence, and it’ll just be ambiguous enough once you try some superficial fixes to make it stop doing it, that we won’t really do things differently based on that.\n\nRob Wiblin: At a high level, overall, how responsible versus irresponsible would you say the main AI companies are being?\n\nBeth Barnes: The tricky question in some sense is, what is your reference point? From the perspective of what I would think humanity in general would sort of endorse if they knew all the things, and what a sort of reasonable society that could actually coordinate would do, I think they’re being extremely irresponsible. And from my personal perspective, even more so, because I value future people maybe more than the average person does.\n\nBut on the other hand, if you look at the individual incentive structures around the companies, all these companies have a bunch of really good people who are working really hard trying to make things go well and within understandable constraints. I think there’s a bunch of arguments under which it does make sense to trade off safety for going faster — if you’re worried about misuse by authoritarian regimes or that sort of thing, or if we’re going to dedicate more effort to alignment.\n\nI think locally people are pretty reasonable, or at least a lot of people are reasonable a lot of the time. I think people are also definitely unreasonable. But overall the situation is very bad.\n\nRob Wiblin: It sounds like you’re saying that’s primarily because of the lack of coordination? That if you just only had one organisation that was thinking about this from the perspective of just all people alive now, then probably they would go a lot more cautiously, and they would invest more in these kinds of evals to figure out what’s going wrong?\n\nBeth Barnes: Yeah, I think it’s partly the coordination thing. And it’s also just that humans are bad at dealing with uncertainty and with probabilities that are smaller than about a half or something.\n\nWhen I was at OpenAI, there were a lot of people who, if you asked them the right series of questions, would say that the probability that the thing they were working on would lead to killing all humans or something, you’d get like 10% or 20%. But it just wasn’t really connecting.\n\nAnd I think if you actually want to keep the risk below 1%, or even lower, it looks like being very cautious in some sense, because our uncertainty around possible threat models and around the error bars on our capability assessments and all of these things are so large that being 99% confident that this is fine is a high bar.\n\nI think model releases now are getting to that level, and it’s like, we’re very confident that it’s fine — but very confident, maybe that’s like 95% confident that it’s not going to do anything super bad. And maybe it’s like 99% or 98% confident that it’s not going to be catastrophic, but it’s pretty hard to get that low.\n\nRob Wiblin: So the issue there is, if you want to get to really low levels of risk, 99.9% or 99.99% confident, then you have to be really sure that there’s nothing outside of your understanding that’s going on. Because even from your perspective, given your dominant understanding of how the models are working and what they are and aren’t doing, things might seem 99% safe, but there’s always a possibility that you’re mismeasuring things, that your evals aren’t capturing what’s going on, maybe they are figuring out how to understate their capabilities.\n\nAnd it just makes it exceedingly difficult to really bring risk that low when you have such an unclear picture of what’s actually happening at all.\n\nBeth Barnes: Yeah. And again, I’m not really thinking about the 99.99%. I’m more thinking about can we get —\n\nRob Wiblin: From 10% to 1%?\n\nBeth Barnes: Something like that. And again, it’s not like we have a pretty detailed understanding, but maybe we’re wrong — we just have gigantic error bars, and we really have no clue.\n\nAnd the companies have described themselves and their safety process as like building the aeroplane while flying it or whatever. It’s not like, “We’re being so cautious, but we’re trying to reach this really high burden.” It’s just like, “Yeah, everything is a complete mess, and we’ve got to work pretty hard to keep the risk to what would be an at all reasonable level. And by default it’s going to be way higher.”\n\nRob Wiblin: OK, pushing it from that. People are working on all kinds of different research agendas around safety, control, alignment. Are there any of those that you want to shout out as particularly underrated or particularly overrated? Maybe things that people are overinvesting in? I guess a worry would be everyone herds into some particular research agenda that’s fashionable, but it’s actually not really going to move the needle in the end, and that could end up being a huge diversion of people’s attention.\n\nBeth Barnes: Right. I think there’s both a question about what is most important to work on and where or in what way.\n\nSo the things that I’m most excited about people working on are a very concrete implementation of at least getting the best practices we’re aware of thus far into production models, and trying to maintain that as an invariant: that we’re always getting the best alignment techniques there. And there’s generating new directions or creating model organisms to study or stuff that’s more like research.\n\nOne thing is I think it would be better if way more of the public good research was happening outside labs. I think Redwood Research, despite being two people, has really high output compared to the entire rest of the field, and all of the people at labs. And the stuff they’ve done is really good.\n\nControl seems good, alignment seems good. Generally understanding scaling laws and forecasting and eval stuff seems good.\n\nI think one thing that’s maybe, in my opinion, somewhat overrated is interpretability. Not that this is not useful — I think there’s a bunch of ways it’s promising and good — but it does have very high investment currently and I’m not sure it deserves as high investment as it has.\n\nRob Wiblin: OK, so that’s one approach that you think we’re a bit overinvested in. Is there another one that you think is more promising than people have appreciated?\n\nBeth Barnes: Yeah, there’s some general direction of just getting the capabilities that you want and not the ones that you don’t want. Currently I feel like how labs are going to react to evals is they’re just going to build more capable models, and then it’s going to be like, now it’s more capable and it’s maybe going to trigger this thing, and then —\n\n\nWiblin: “What a shame.”\nBeth Barnes: Yeah, maybe they’re going to fudge the evals, or maybe they’re actually going to do some of the mitigations, but I think we could be trying much harder to be like, what is the reason for rushing ahead in building these models?\nThe argument is like, “Then we can use them to do safe things and protect against these other models.” Like almost all of the companies that are like, “Building AI is important,” it’s like, ” — because someone else might build it, and we’ve got to protect against it.”\nBut I feel like this is very disconnected. I would rather the world was like, “Maybe some people are going to build dangerous AI. How would we protect against that?”\nRob Wiblin: “What would be the minimally capable AI that would be really helpful?”\nBeth Barnes: Right. Or maybe the most important thing to do isn’t even AI. It’s not super clear that that’s what you need to do anyway. I mean, I think there are reasonable arguments for how we have a bunch of uncertainty and you just want to have a generally capable thing. But I just feel that people aren’t trying at all to be like, “How could we get the capabilities that are most useful, and which other capabilities can we knock out?”\nAnd some of the very basic control stuff of how you can be more confident your model is not capable of taking over if there’s a tonne of stuff about the world that it doesn’t know. Maybe it’s really good at coding and stuff, but it just doesn’t know anything about world events past 2010, and you really tried to remove everything about computer security and something like that — where it’s like, this model couldn’t really do stuff by itself; it only works in this lab setup where we give it all the right things or something.\nRob Wiblin: Yeah. I guess the case where I’ve heard this discussed the most is trying to not include cutting-edge biology –in particular, cutting-edge virology or microbiology — in the training data, because that’s the kind of thing that would allow it to design a bioweapon.\nAnd it’s a fairly clear point to make: why does the model need to know all of this incredibly cutting-edge biology, unless it’s being used by virologists? And in that case, why not just design a very special model for them that only they have access to? Why does the model that is going out to retail customers like us have to know the most dangerous things basically known to humanity up to this point?\nAnd you’re saying this could generalise quite a bit more, where you could hobble the models by ensuring that they’re lacking specific capabilities by simply never training them in it, specific things that would be useful for them if they were going to try to scheme against us or engage in activities that we don’t like.\nI guess I haven’t really heard that approach discussed very much outside of the biology context. I guess cyber stuff would also be another classic case where you could maybe use this — although they do seem to need to know a lot of coding for many commercial applications, so perhaps that’s trickier.\nBeth Barnes: Right. I think it’s easier for the narrower things, but I do think if you need it for some applications, you just don’t need to serve that capability to everybody all of the time.\nAnd with the bio stuff, yeah, you might still want to support synthetic biology research, but that’s like an extra training phase that you add on at the end, and then you serve that model only to a very carefully selected group of people. So I think even if you need the capabilities sometimes, that the frontier of which capabilities are how good that we’re targeting, we’re really not trying on this in a bunch of ways at the moment.\nRob Wiblin: I do remember hearing people push back and say the trouble with just excluding some information is that at the point where they were producing even more capable models than we have now, they’d be able to interpolate and guess lots of stuff that you left out.\nLike if you included everything other than the news in 2013, then probably it could just guess what happened in 2013 by knowing what happened later. And I suppose even if you didn’t include anything after 2013, it might be able to guess a fair number of things that happened simply by the information that it has up to that date. To what extent is that a serious limiting factor?\nBeth Barnes: Yeah, I think this isn’t a super robust intervention. This is sort of a, “Let’s try to get — ”\nRob Wiblin: Here we’re trying to get the risk from 50% to 40%.\nBeth Barnes: Yeah, those sorts of worlds. I do think there just might be more technical work to do here on how you can actually remove knowledge or capabilities from models, or figuring out how much they can infer. Because I think you probably can assess this reasonably well.\nEven if you have some concern about sandbagging, if you’re just trying to figure out does the model have this basic knowledge… There’s some level of sandbagging to do it with RL and there’s a much higher level of intelligence that’s demanded to do sandbagging if you’re just doing classification fine-tuning.\nSo if the model does in fact know the answer to a bunch of questions about this, you should be able to see that, with fine-tuning, it picks this up really quickly — so the answers must have been kind of already in there. So I think you can kind of tell what models know in this sense.\nRob Wiblin: By how quickly they later learn how to…?\nBeth Barnes: Yeah. And also, as we were talking about before, if you don’t actually have great security, what you do want to know is, “Is it easy to fine-tune the model to have these capabilities?” — not, “Does it demonstrate them right now as these exact weights?”\nRob Wiblin: Yeah. Just to clarify for people who don’t know: sandbagging is models pretending to not be able to do things that they actually are able to do, or basically kind of lowballing what you actually are able to do. It’s a sport term, I think, from people who are throwing a match, basically.\nBeth Barnes: Yeah.\nRob Wiblin: Wasn’t there some work on forgetting? Wasn’t there some effort to try to get a model to forget the existence of Harry Potter? I think they managed to kind of successfully do that, or forget something or other.\nBeth Barnes: I’m not familiar with that exact thing. Sounds plausible. I think people have done some on this, and call it “unlearning.” I just haven’t seen that much on it. And in particular, I haven’t seen any kind of application of it.\nRob Wiblin: Yeah, I think I haven’t heard about that in almost a year. Which is a bit of a shame. Is there anything else you want to call out as potentially underrated?\nBeth Barnes: I’m sad that there’s not more work on helping humans evaluate difficult model outputs, basically the stuff I used to work on, on debate and IDA [iterated distillation and amplification] — this whole genre of alignment research is like, how do you make sure that your model is not doing destructive things when it is smarter than you are? And how can you create structures that provide a systematic advantage to the rater?\nDebate is one of these, where you have copies of the model and they’re debating with each other. You have some specific subquestion that you can judge, and you can judge how each subquestion relates to the question above. And this is much easier than deciding the overall answer.\nI think there could be a lot more stuff just helping humans more quickly make good judgments about, was this actually the behaviour you wanted? Was this actually seeking power in some undesirable way?\nI think the more faithful your training signal is to things like truth and honesty, the less incentive you are putting on the model to reason about the training process and the flaws in the training processes and things.\nAnd yet the more you have these techniques that kind of amplify what humans are able to evaluate, the more you might be able to check. Like, if these models, they’re running all of our security against these rogue, unsafe, bad models that someone else has developed. We have no idea what’s going on. We don’t know whether our models are actually secretly cooperating with the rogue models.\nThe better you can be like, “We have this scheme to have two models adversarially evaluate each other and point out the things that they’re doing that are most suspicious and then evaluate those,” the better we are at that, the more able we are to train for, “No, actually, do the thing that I want.”\nRob Wiblin: You said something a little bit spicy earlier, which is that you think possibly Redwood, with just a handful of people, is maybe producing about as much alignment research as all of the major companies put together. I assume that’s not actually literally true, but… Maybe you do think it’s literally true?\nWhat would the barriers be to the companies producing more alignment publications than they are? It’s a little bit hard to understand, because the resourcing would be so uneven there.\nBeth Barnes: Yeah. I may be biased in what I’m exposed to, because I’m just much more aware of the stuff that Redwood has done, but it’s just actually not implausible to me that that is the kind of current ratio.\nSo reasons companies are less productive: there’s stuff we mentioned a bit before about maybe actually the infrastructure and speed at which you’re able to do research is slower. I think there’s different incentives and pressure to do the thing that will unblock shipping the next generation model, rather than do the research that’s most important in the longer run.\nI think to some extent it’s just like too many people in the same place or something, where it’s just like a marginal person is less useful when you already have a tonne of people.\nI think also maybe companies are more focused — this is similar to short term– but it’s both short in terms of time horizon and locality. They’re just thinking about things at their own company, and not what is most important for the field as a whole, and how do we disseminate that and make it easy for other people to adopt.\nAnd there’s a bunch of barriers to publishing, and that’s maybe a reason that some people have left, have been unhappy about not being able to publish their work enough.\nRob Wiblin: I see. Presumably they’re a lot more cautious now about what they share and what they publish, and I imagine there’s a lot of levels of review that stand between someone having some insight and it going out. I guess especially if it’s related to risks that the company might be creating, you can see a lot of people wanting to scrutinise that before it’s shared.\nBeth Barnes: Yeah. The comms team in there editing the blog post, being like, “Can we make it sound a bit more optimistic here?”\nMaybe another thing that I think is overrated, or I feel like sometimes people conflate alignment — in the sense of making sure the model is trying to do what we want, and not trying to do extremely bad things that we definitely don’t want — with making the model better at following complicated instructions or other things that are more just like making the product better or enhancing the model’s capabilities in the direction of following elaborate rules about what we want.\nAnd I would just be more excited about progress that’s progress on the problem when humans don’t know which output is better, but the model knows a bunch of stuff about what’s going on. That seems more useful than, “The humans want the model to follow this complicated set of instructions, so we did the things to make the model better at following the complicated instructions, because otherwise it was forgetting that it’s supposed to also have this policy or something.” That doesn’t feel like it’s making progress on the core problem.\nRob Wiblin: Earlier you talked about neuralese, which is this sort of internal gibberish to the model we can’t follow, it’s helping it solve the problem somehow. Is it possible to have a science of interpreting neuralese to go from a bunch of numbers that the model was able to understand back into English in a faithful way?\nBeth Barnes: I think that there’s some of this which is basically contiguous with existing interpretability. And it’s sort of like you’ve just gone from your model doing single forward passes to now you have some recursive thing, and you’re just feeding the activations back in. And basically you want to do all of the same interp things.\nI think there are also other approaches. If we’re talking about specifically this case where you did have an interpretable chain of thought, and then either you trained on it too much and got gobbledygook, or you did some kind of optimisation, that there’s maybe a bunch of techniques here that are more like preserving the existing interpretability that you did have — as opposed to recovering it from this thing that started off completely uninterpretable.\nOr making it possible to get that speedup without it needing to transition to the neuralese.\nRob Wiblin: Right. I guess that would be the tradeoff. So the reason companies will allow that to happen is because they just think requiring us to be able to understand the reasoning is merely holding it back; we need to allow it to use its own gibberish\n\n\nBecause that’s better. But I guess if you then added a whole other layer of requiring it to then translate its reasoning back, then that’s just slowing it down again.\nIs this an area that might be ripe for regulation? That you should say you can’t have models just using neuralese? Or maybe there’s always going to be too much of a grey area of whether the chain of thought is comprehensible or not, so it’s not a very bright line that you could draw in the sand. But it does feel like it would be really nice if we could get them to act in the spirit of saying, no neuralese, basically.\nBeth Barnes: And no training the model not to do kinds of reasoning that you don’t like: no training the model not to reason about scheming, and it needs to be interpreted. I feel like it would be great if that was enforced.\nI do think there’s a bunch of opportunities for technical progress here, where it may be much more feasible to figure out how you would have some kind of summary or translation that you are ensuring is faithful in some way. It just feels like a thing that people could work on.\nRob Wiblin: In terms of commonsense regulations that you could try, I’ve had the shower thought for a while. To the regulatory scientists, this is going to sound stupid, but why not just ban AI being used to enhance AI? I would feel so much better about the future if I knew that you couldn’t have a recursive self-improvement loop. That would solve the problem to an enormous degree.\nThe problem, of course, is it’s already being used that way. They’re already using it to help with coding. They’re presumably already using the language models to just research all kinds of random questions that they have. So it’s assisting.\nBut I think if you just had that as the rule, and you interpreted it as kind of any plausibly reasonable way where you draw the line between what is and isn’t permitted — so you allow some basic assistance with programming, but nothing that’s more impressive than what we have now — I think it would be a massive improvement, basically. There’s almost like no way that that could be worse from a safety point of view than the default, where there’s absolutely no rules about that whatsoever.\nWhat do you think of these kinds of naive things, just a person off the street who was just presented with this problem saying, “Why don’t we just ban the thing that’s obviously creating this enormous risk?” Can we get some mileage here?\nBeth Barnes: I’m maybe not the best person to ask about regulatory feasibility. I think, as you said, it’s hard to, if people are already doing things, say that they can’t do it anymore. I would be maybe worried if there is some overhang created here, if people were just holding off from doing a thing and then you suddenly get a lot of improvement.\nRob Wiblin: I guess people would say that China’s going to do it anyway, and this would hold us back so much.\nBeth Barnes: Right. There’s stuff you could do. Like the amount of trust you need to have in your model before you let it do stuff. Or I think more models above this capability level, you need this mitigation — where definitely the thing that spiritually makes the most sense is we don’t care about the rate of progress per se; we just care about do you have the mitigations in time?\nSo I don’t love this AI R&D as a thing to draw a line on, because it is kind of contiguous with all of these just basic coding tools. Or you could also just ban humans doing machine learning research or something. It has this weird flavour in some ways. And if you didn’t already agree that AI progress was scary, why is more AI progress scary? It’s not like the final threat model thing. But I think pragmatically, you’re just not going to have the mitigations in place if you’re doing this crazy recursive self-improvement loop.\nRob Wiblin: You might come back and say, why not just hobble them in any random way. Like say that if you’re doing AI research, you have to remove the E key from your keyboard, and that’s going to slow them down and be really irritating.\nBut I guess the difference here is that if you have AI doing the work, then humans are getting further and further out of the loop of anything that’s happening. It’s becoming more and more inscrutable. I guess we expect that that’s the thing that’s going to lead to the very rapid improvement in the capabilities. So it’s something that slows it down the more it would have been sped up. It has at least a nice property to it.\nBeth Barnes: Yeah, I do think in general I prefer things that are like, “You have to do this mitigation,” rather than just like, “Slow down.”\nRob Wiblin: I guess that gives them the incentive to fix the problem, inasmuch as you specified it properly, because then they’ll be able to go ahead. Whereas if you just ban it and say it’s always going to be banned, then no such incentive is made.\nBeth Barnes: Yeah. And it’s a thing that a broader group of people can get on board with. It gives you some free lunch with respect to if people disagree about what capabilities you’ll have when. Someone who might object to the thing that just slows you down might not object to, “If you hit this level, you need to do this mitigation.” So it can be strictly more appealing and it’s strictly more reasonable in some ways.\nRob Wiblin: Yeah. I wonder if even if it’s a bad idea, it’s the kind of regulation that we might get in a crisis. If you do kick off a recursive self-improvement loop and that quickly leads to an AI going off the rails and causing a bunch of damage, and then you loop in politicians, and they’re like, “You were doing what?! You took all of your humans out and just had the AI doing all the work? Obviously we have to ban that.”\nI guess that probably would be better than nothing, but that could end up being the sort of naive regulation that perhaps helps but is substantially suboptimal relative to other more sophisticated things.\nBeth Barnes: Yeah. One way that being like “just slow down” or “just pause” or something can backfire is we would like to save all our pausing juice to the bit at which you can get the most safety progress out of your models before things are really dangerous.\nAnd if people are like, “We paused and then everything was fine,” then everyone’s fed up with this and thinks this is stupid and let’s just go ahead now. And now the compute has accelerated more and we’re ready to go through this. You really want to go through that part of the transition as slowly as possible.\nRob Wiblin: So any constraints you place on yourself earlier is just creating more latent capability to very rapidly speed up later.\nBeth Barnes: Yeah, that’s one model. I don’t think this is totally right. I just think people seem to sometimes miss this fact. I think it’s more salient the more you’re thinking about how a lot of the important safety work will be done by the AI researchers at the point just before they are catastrophically risky.\nRob Wiblin: OK, new topic: Is open weighting models in general good or bad, from your point of view?\nBeth Barnes: This is something I’ve changed my mind on a fair amount. It’s maybe also a bit related to being in the lower-assurance world.\nOriginally I was like, this seems very dangerous if the problem is either that these things can be misused or that they can be dangerous even without a human trying to misuse them. It seems like you don’t want them to be everywhere, and not be able to be like, “Actually, this is a bad idea. Let’s undo that.” It’s an irreversible action, and it opens up a lot more surface area for things to go wrong — if you think that, at least for some of the threat models, there might be a big offence/defence imbalance.\nSo I think if you’re trying to keep the risk very low, it really feels like you can’t open source anything that capable. Because again, even with current models, it seems hard to rule out that with other advancements in scaffolding or fine-tuning or something, it would then become quite easy to make this model into something very dangerous.\nI’ve shifted my perspective generally on how good it is to keep everything inside the labs. Sort of similar to the thing about, would you rather just the labs know about what capabilities are coming or would you rather that everyone knows?\nI think in practice, the open source models have been used to do a bunch of really good safety work, and this has been important and good. And having a more healthy, independent, outside-of-companies field of research seems good both for the object-level safety progress you made, and for policymakers having some independent people they can ask — who actually understand what’s going on, and are able to do experiments and things.\nAlso, in general, I have become more sceptical of some kind of lab exceptionalism. I think a lot of the companies have this, like, “We’re the only responsible ones. We’ll just keep everything to ourselves, and the government will be too slow to notice, and everyone else is irresponsible.” I just think this becomes less plausible the more independent labs are saying this, and the less they actually respond to how responsible are the other actors. And if you trust the companies less, you want less of that.\nI think it’s also one of those things where everyone always thinks that you’re the exception — “It’s fine, but we’ll just actually be good and actually be responsible. We don’t need oversight.” And sunlight is the best disinfectant. Oversight is really important, and security mindset and secrecy and locking things down can be bad.\nRob Wiblin: Seems like there’s a reasonable amount to unpack there. The basic reason I’ve heard for why actually open sourcing has been better than perhaps people like me feared a few years ago is that it’s been an absolute boon for alignment and safety research, because it means external people don’t have to actually go work at the labs — they can do things independently, which is really useful for all the reasons that we’ve talked about, if they have access to basically frontier models outside of the companies.\nAnd I guess separately from that, it sounds like you’re saying that the companies having a very secrecy-focused mindset, wanting to hold all of the information about the models within themselves, that that’s a dangerous mentality. And maybe it’s actually helpful to have the weights open and things being used all over the place, so that people can see for themselves the chain of thought, for example, and modify the models and see what is possible if they’re improved — things that wouldn’t be possible if you only accessed it through an API that’s highly limited.\nBeth Barnes: Right. I think there’s also a point about maybe you just think some of the companies are bad actors, or there are people at them who would be bad actors, and you would prefer that the rest of the world also has access to the tech.\nBut yeah, I think that the main thing is this slightly less specific to open weight question, and more some kind of general position on lab transparency versus sort of security and locked-downness.\nRob Wiblin: Can you elaborate on that a bit?\nBeth Barnes: One of my side interests is the history of nuclear weapons. And I think there are a bunch of examples there, and in other places, of secrecy and personnel restrictions and things being used to silence people who had safety concerns or ethical concerns, or were too busybodies or interfering.\nSo both compartmentalisation — so that fewer people knew about a thing and could complain about it — and then trying to jail or otherwise get rid of or exclude individual people who had too many ethical concerns, basically. In particular Szilard.\nAnd I think we actually have seen this specifically being used in the AI case already. I believe when Leopold Aschenbrenner was fired from OpenAI that there was some claim that this was about having leaked confidential information.\nI don’t know the details here, but it’s very plausible to me that basically everyone at these companies is doing a bunch of stuff that’s technically leaking. And if you want to get rid of someone, you can go through all their documents and find something that they did. Or in general, it’s a reason to keep people out of things, like compartmentalisation. This person is going to be annoying about the thing. Let’s keep them out of it.\nAnd I think doing the Trinity test, when they had the concern about whether it might be possible to ignite the atmosphere, there was basically no civilian oversight of that, downstream of this being a very secret project overall and not much government or congressional oversight. It just meant that just the people who made this decision were the people who —\nRob Wiblin: It’s just a bunch of scientists or engineers who happen to be working on it.\nBeth Barnes: Yeah, and military. I think a lot of trying to get the scientists to shut up if they’re being too annoying about stuff.\nAnd people who are very invested in that particular project — even people who would have otherwise been relatively pacifist or not super excited about weapons — once it’s the thing that you’ve been working on for ages, it’s really hard to be like, “\n\n\n“Maybe this isn’t good.”\n\nRob Wiblin: “Maybe we should just stop completely.”\n\nBeth Barnes: Right. I think there’s a bunch more anecdotes from that history that are relevant to current AI stuff. But AI companies that are developing AI seem like particularly bad decision makers in some sense here, because they have such conflicts of interest, so centralising all of the information power within them seems like maybe a bad idea.\n\nRob Wiblin: OK. The obvious benefits of having a security mindset are that dangerous information doesn’t leak out to other groups. But downsides would be, if you start siloing information within the company, then an even smaller number of people might have a sense of the general picture of what the frontier AI models are able to do. If they’re very cautious about letting anyone outside the organisation know, then it makes it harder for governance to occur, makes it harder for broader society to appreciate if risks are reaching a level that they wouldn’t regard as acceptable. It means that there’s just basically less oversight from anyone else.\n\nBeth Barnes: Yeah. Which maybe means we missed the opportunity to react to a safety incident that should have been a warning — like, “We actually caught the AI model scheming or trying to do something bad.” And instead of being like, “Whoa, everyone should take this really seriously, and we should freak out and we should stop this kind of thing,” it’s just like, “Let’s just train it not to do this, let’s not tell anyone about that.”\n\nRob Wiblin: You’re saying something could go wrong internally, but they might say that it’s too dangerous to let anyone know that this even happens. So it gives them a pretence for sweeping it under the carpet. I guess even more problematically, it allows you to just clean house, to remove anyone who doesn’t support the current project, if you’re saying that anyone who’s ever told anyone about what’s going on in the company, even if everyone is doing so, then you can just fire people basically arbitrarily on that grounds.\nYou mentioned Leopold — that’s Leopold Aschenbrenner; I think his crime was telling people that information security within the company was not sufficient to stop the Chinese from stealing the AI model. Or that was his big hobby horse — which everyone agrees with, more or less, and everyone is kind of troubled by. Maybe he was making too much trouble over that and potentially bringing more regulatory scrutiny to the company?\n\nBeth Barnes: Right. I don’t know exactly what happened in this particular case, but I’d be very surprised if this pattern doesn’t show up.\n\nRob Wiblin: Do you want to give any details of a particular case that occurred during the nuclear process? You mentioned Szilard, who was one of the first people to twig that a nuclear weapon might be possible. And he wrote to various people to say that it’s really important that America do this before the Nazis.\n\nBeth Barnes: Right. I’m a Szilard fan. It’s a random thing. This is a historical figure who it feels like is thinking about so many of the same things that we’re thinking about in kind of similar ways. I think he was very good in terms of scope sensitivity. He was actually thinking about what are the most important things happening in the world.\nAnd this is, I think, also some reason for being somewhat more optimistic about the ability to forecast technological progress and world events and things, and maybe some evidence that the reason why most people are bad at it is that they’re not actually trying.\nSo he foresaw nuclear weapons in the early 1930s, like, “Here’s how you would theoretically make a chain reaction if you can find a reaction that is started by neutrons and produces more neutrons” — and then was like, “I should keep this secret because it seems bad if everyone was doing it, bad if the Nazis got it.”\nAnd he also was like, “I’ll move to America one year before the war” — and he did, in fact, move to America one year before the war. Like, if you’re actually trying, maybe geopolitical events were kind of obvious. And that you might use this for powering ships or submarines, and actually thinking through the implications of technology seemed quite possible.\nHe then, with Einstein, wrote the letter to the US government that sort of kicked the bomb project in the US into action. The original justification was that we don’t want the Nazis to have a nuclear monopoly. That seems like a very reasonable justification, and that seems very bad. But during the course of the project, it migrated from that to, actually, we’re going to just use it to shorten the war by bombing the Japanese. And now it’s an arms race with the Soviet Union. And I think very few of the people actually left the project after that momentum. Joseph Rotblat was one person.\n\nRob Wiblin: Yeah. So at some point it became clear that the Nazis weren’t working on this weapon, and eventually then they were defeated. That then obviated the explanation that most of these people had been given for being involved in the project at all, which was to beat the Nazis. But you’re saying basically 99% of them get stuck around with a new explanation that they were given.\n\nBeth Barnes: Right, yeah. There were multiple people who said, “This is a terrible thing and I wouldn’t want to work on it, but in this particular case of only the Nazis having the bomb, that would be really bad. So I’ll work on it in the special circumstance.” Then that isn’t actually true anymore, and people are like, “Well, since I’m here now…” There’s investment, and these things get their own momentum.\nI feel like I see parallels in AI in terms of people giving reasons for why it’s important for them to be first or push ahead or something. They’re like, “We’re the most safe. This other competitor is doing this thing.” And their actions don’t change as those other facts about the situation change.\nOne thing is also that, proportionally, very small amount of effort went into actually trying to figure out if the Nazis did have a nuclear programme. There was basically no American espionage. I think there was some British espionage, and the Americans maybe weren’t even on top of actually what the British had learned about this. So again, if they were actually trying to make sure we avoid Nazi nuclear monopoly, you would have been doing much more, like, “Where are the Nazis at? Is this actually a risk?”\nAnd another example of Szilard being much better at forecasting the technological stuff: he was just like, “The Soviets will have the bomb really soon.” What are you talking about? And [director of the Manhattan Project Leslie] Groves was like, “There’s no uranium in Russia.” It’s like, what? The ores will be slightly lower grade. There’s no way there’s no uranium in the whole of Russia. You know how big Russia is? And like, “Our brilliant American boys: no one will match that.” It’s like, no, this is not actually that hard.\n\nRob Wiblin: Or they’ll just copy it. There were Russian spies.\n\nBeth Barnes: Well, yeah. Only a few, but there was various information that they actually had like before the Americans did, because of spies in the British Foreign Office, I think. So there’s all this stuff about secrecy and locking down, and that secrecy was completely useless.\nAnd yet people would not plan for this arms race. But Szilard was like, “The thing that is really important now is there’s going to be a nuclear arms race between the US and Soviet Union that is going to be potentially world ending, and we need to figure out how to try and make that less of the case and how we can reduce tensions with the Russians now.” And the people were like, “Whatever. They’ll never get it.”\n\nRob Wiblin: It’s hard to fathom.\n\nBeth Barnes: Yeah. And the amount of effort that went into things like thinking about alternatives to bombing Hiroshima, to actually destroying a city — you know, could you explode the bomb in the air or in an unpopulated area to demonstrate the capability or something? And it was like people discussed it a bit over lunch on the day that they were making this decision. But there was so little effort put into some of the key decisions.\nAnd similarly with potential world governance of the weapons. Like the Acheson–Lilienthal plan of inspections and things, the people who were working on it were in conflict of interest — they had various industrial things that they would benefit from, if I remember correctly.\nAnd again, just generally it was a very half-hearted effort, and was kind of like they just wanted to do something so they could have been like, “We offered something, but the Russians said no.” But it feels like you have less evidence than you might think that something like that would have been completely infeasible, because there was very little effort into mechanism design of how you could have transparency about this in a way that was less unpalatable to the Soviets who didn’t want a bunch of stuff about how badly things were going for them in general to be known.\nSo I don’t know. Some kind of actually trying at the most important things seems like it would have been pretty leveraged if you’d had some detailed alternative proposal that was well thought through. Plausibly.\n\nRob Wiblin: Yeah. I guess the generalised thing to keep your eyes out for is if people say, “I don’t want to do this, but I have to do it because of X.” But they show almost no interest in finding out with confidence whether X is true or not, and they don’t stay up to date on whether it’s true or not. They also maybe show no interest in changing X, taking steps that would fix X and make their actions no longer required. And also when X stops being true, they don’t change their actions.\nThese are kind of giveaways that maybe X isn’t the real reason. And I think we may see a fair bit of that at the moment.\n\nBeth Barnes: Right. The thing has acquired its own momentum. And when Szilard was trying to convince Truman’s incoming Secretary of War, I think, that bombing Japan was a bad idea, he was like, “Well, what are we going to tell Congress we spent all that money on if we don’t do anything with it?” It’s like, oh god.\n\nRob Wiblin: Yeah, it’s a little bit embarrassing to kill that many people in order to just have a better answer at a committee hearing.\n\nBeth Barnes: Yeah, but that’s what things get decided on. One of the reasons I think pre-scaleup evals would be good.\n\nRob Wiblin: To explain: that’s a reason why we should figure out whether a model is safe before we spend all the money training it. Because once you’ve spent all the money, what are you going to tell the board?\n\nBeth Barnes: Right. The investors.\nI thought of another analogy to the nuclear stuff now that we’re talking about China: maybe something that was important was the compliance of individual scientists in different countries.\nI think this is pretty unclear, but there’s maybe hints that some of why the German bomb programme didn’t go very far was partially that scientists were not very excited about nuclear-armed Nazis.\n\nRob Wiblin: I think it has been suggested that they deliberately made bad strategic decisions, basically, because they were trying to make the probability of success lower.\n\nBeth Barnes: Right.\n\nRob Wiblin: Just to tie this back to the open weighting, open source question: if we just start open weighting all the best AI models, aren’t we kind of baked? It makes any governance virtually impossible. It means that any misuse is going to happen very quickly.\nWhat’s the upside story? I guess it means that you get better warnings, maybe early. If there’s people who are saying we don’t really have to worry about the misuse and then something is open weighted and it’s immediately misused for some catastrophe or some horrific crime, then maybe that could lead to improvements in governance.\n\nBeth Barnes: Yeah, I don’t advocate doing things to cause horrific crimes.\n\nRob Wiblin: So doing everything we can to stop that, but I suppose if we fail, and people who are against any kind of governance succeed, then a possible upside of that is that we all get to find out who was right sooner.\n\nBeth Barnes: I think the case for certain types of transparency and openness is much stronger than the case for specifically open sourcing weights. But I do think the open sourcing thus far feels pretty clearly net positive, just because of the impact on safety research. I do think you would have thought that it ought to be harder to argue that we’re in an arms race if people are open sourcing the AI models. But somehow this doesn’t seem to have stopped anyone.\n\nRob Wiblin: Never let common sense get in the way of the narrative that you would like to spin for your commercial interest! Just to clarify, you’re talking about the release of DeepSeek R1, which is this super weapon that the Chinese developed and then immediately gave to us for free, in exchange for nothing. It really shows that we’re in a very intense arms race with the Chinese to I guess give one another our greatest technology.\n\nBeth Barnes: [laughs] Yeah.\n\nRob Wiblin: It is extraordinary that that did not preclude the arms race narrative. I guess you could say that maybe this shows that they will be able to make something in future that they won’t share with us. But you’d really think literally giving away your weapons designs would\n\n\nbe definitely an olive branch to your enemy.\nBeth Barnes: You would have thought. Yeah, I think basically what you were saying about if there are incidents, they happen in public rather than just inside the lab. And especially if you have to have open sourced the weights before you’re allowed to scale up by this much. That would be one thing you get around. Like the lab can be doing this crazy thing internally that no one has any idea of, and at least outside you’d be able to demonstrate that this model is pretty close to being able to do this thing, and maybe we shouldn’t let them build even bigger ones.\nI think also, unless you have very good security, the story of how you’re OK doesn’t depend on not open sourcing. It seems like if you have a sufficiently powerful model, then a bunch of people are probably going to be interested in it. And companies currently don’t have the security that would keep out state actors or the best private actors. So your story, your theory of victory has to be something like, “We’ll use our good models for something good, and that will prevent some harm from these other models.”\nI think most of those stories actually also work if there are open models everywhere — because it’s more about how more of the people who control more of the compute want models to be doing these kinds of things than want models to be doing these nefarious kinds of things. And it does feel like either you have to be extremely locked down, and there won’t be any bad actors, or you’re making some argument about how we can use these models defensively in some way.\nRob Wiblin: Part of the story there is that the security of these companies is not so strong that the models are not going to leak to various nefarious actors anyway. We should at least be honest about it, and they may as well open weight it, and then a whole bunch of nice people who didn’t even have to steal it might be able to use it.\nBeth Barnes: Right. I mean, maybe they’re currently open weight from the perspective of North Korea or whatever. Something I’ve sometimes said is there are no closed-weight models. Like, no one has that good of security.\nRob Wiblin: That is very interesting. I think most people know, but maybe not everyone, that North Korea is actually very strong on offensive cyber capabilities. It’s not only China that might be able to steal it. I guess you don’t think of North Korea as a technological powerhouse, but on this one thing of stealing information from people, they are quite good.\nBeth Barnes: Right.\nRob Wiblin: You’ve been using this analogy between AGI and nuclear weapons. That’s a comparison that people have definitely drawn a lot over the years, and people have both reasons to think that it’s interesting and reasons to think that it’s misleading.\nDo you want to give a take on what you think the important disanalogies are? In what ways is the situation different today versus with Szilard and so on?\nBeth Barnes: Obviously there are a huge number of disanalogies. AI is just much more general purpose. I do think it maybe ended up being a bit less disanalogous than we might have worried. Ten years ago maybe people were worried that you could build AI; it would just be coming up with the right algorithm, and then you could do it on your laptop — and that would have been much harder to control.\nIn some ways the story of nuclear proliferation is quite successful in that it was limited. It’s not like everyone has these. And that’s partly because detection is relatively easy: you can tell when someone is doing this because you need big facilities, and you need to have industrial capacity. I think AI has ended up being more like that than we might have feared, in that you just need a lot of chips and you need a lot of energy and a big data centre. And it’s not like anyone could be doing it in their basement at any time.\nI think maybe an important strategic disanalogy is that nuclear by default advantages countries that have a large industrial base, because it’s so industrially intensive. It differentially elevates big developed states against terrorist groups and rogue nations and things. And it’s not very stealable. It’s like the amount of nuclear capacity you have is pretty proportional to your existing power.\nWhereas I think there’s a bunch of ways that AI is more analogous to bio, in that it’s more in the direction of destabilising, or maybe it favours smaller actors more — in that at least in some of the threat models, it doesn’t matter that much how much resources you start with, because it’s relatively easy to steal. Whereas nuclear material is much harder to steal because it’s just stuff that’s there and it’s big. But this is software; this is data. It can leak relatively easily.\nAnd if you’re talking about autonomous AI, it can make copies of itself and spread itself around — so it’s not like the amount that you can deliver is proportional to the amount that you had in your warehouse. It’s more like a bioweapon — where it spreads itself, and it’s less tied to the work that you had to do at the start, and it’s more easily leaked. And maybe this is the sort of thing that could be used as an agent of chaos more.\nI’d like more people to be aware of this argument, because I think it’s a reason that big states like the US and China should be concerned about rapid AI progress — because it just is a destabilising thing. And they’re currently doing well and sort of on top, and making something that can suddenly spread out of control and can be stolen by a terrorist group or something is just actually really not in your interests.\nRob Wiblin: I see. So the way that it’s in the selfish interest of the US is that, while it maybe has a lead in the relevant technology right now, it seems like it’s going to cost at least billions of dollars probably to train the first frontier model that is really useful as a weapon, or useful offensively, or at least useful for maintaining industrial dominance over other countries. So that could allow it to kind of lock in its advantage.\nThe downside for the US is that if that period passes and it becomes a whole lot cheaper — and also these models are disseminated quite broadly, and they’re possible to steal both by states but by also private actors, potentially, because they’re just all over the place — then that’s actually a huge weakening of the US’s strategic position, of its security position, in the same way that dissemination of bioweapons would be.\nBecause it wasn’t the case that only the US and China and Russia could have designed bioweapons; actually, dozens of countries probably could have, and that would be absolutely devastating. In some ways, they would provide a stronger deterrent and would be even scarier than having nuclear weapons.\nWhich is one reason why the US was very keen to discourage their creation and to create a taboo on them, because it would ensure that the previous generation of weapons of mass destruction, nuclear weapons, would remain the dominant ones that countries cared about, which was something that the US had an oligopoly on.\nBeth Barnes: Right. Yeah.\nRob Wiblin: So if we get to the point where basically nuclear weapons and bioweapons are now obsolete, and actually it’s AI that is the key thing that can cause damage if you want it to, and this is disseminated very broadly to all many different kinds of actors, then the US government is now basically just a weaker player in the world as a whole. And the Chinese government is also just a weaker player as a whole.\nBeth Barnes: I think, yeah. There’s maybe some analogy with some of the NSA exploits that got leaked and then started being used by Russian hackers against US citizens. It’s like the version of a bioweapon where you’re going to develop this super advanced bioweapon — and also, by the way, it can be stolen if someone hacks your computer. It’s like, sorry, and this is good for us why?\nRob Wiblin: Especially because we’re already winning. Why do we want to create this enormous vulnerability to our position?\nBeth Barnes: Yeah, right.\nRob Wiblin: Yeah, you were just referring to the NSA, one of the cyber offensive organisations within the US. They had a whole bunch of cutting-edge tools for breaking into people’s computers and phones and so on and stealing information from them, which I think they literally accidentally posted publicly on the internet, or there was at least like one leak of that basic type.\nBut one way or another the Russians got access to these tools that they had designed — which is the kind of thing you might expect, because the Russians are also very good at this — and then those weapons could just be immediately turned back on the United States. So even if it was narrowly, in the short term, good for the NSA to have those weapons, maybe it would have been best for the United States more broadly if they’d never been created.\nBeth Barnes: Right. I just think this is a pretty good argument that the arms race framing doesn’t really make sense in terms of US and China national interests.\nI mean, obviously AI is not just a weapon, and has a bunch of other benefits and things as well, but I think if you’re thinking about it in this arms race mindset, then there is this pretty good argument that what you want is less of it.\nRob Wiblin: I see. Or that’s what the national security folks should want. At least if they could come up with an agreement between themselves and China, and maybe a few other major players, to not develop this thing that is destabilising and that actually weakens their relative power, then that would be great. But there’s almost no discussion of that possibility.\nBeth Barnes: Right.\nRob Wiblin: I think one reason for that is maybe that they are anticipating that they will be the first people to get to this incredibly powerful thing, and then they’ll be able to stop other people from developing it.\nMaybe that isn’t always said out loud because it sounds a little bit hostile, but basically they’re imagining we’ll rush to this technology and then basically try to become a permanent hegemon who prevents anyone else from designing stuff that would be destabilising. Maybe it would be good if they were a bit more explicit about that, and then people could weigh in on whether they think that’s a good approach or not.\nBeth Barnes: Yeah, it does feel a bit like one of the things where you could also work more directly on what would we do to cope with scary AI and that it’s not clear that the response is immediately, “Yes, build your scary AI as well.”\nJust the stories I’ve heard for how you use your super capable model that you’ve built to keep the world safe are not that compelling. This is a thing I would love people to be thinking about more and thinking about more explicitly. Like all of the things where the lab’s theory of impact is like, “We’re going to build this thing first, and then we’re going to be responsible for it,” it’s like, OK, what exactly are you going to do with it, and what capabilities do you need for that?\nRob Wiblin: Almost all these stories involve at some point blocking other people from designing their own versions of it. Maybe one reason that that isn’t advertised is that it does sound quite hostile. It’s hard to sell that to people ahead of time.\nBeth Barnes: Yeah, but also there’s a bunch of things that we could do that would achieve that that don’t require building super scary AI — like just have fewer chips.\nRob Wiblin: Have compute governance.\nBeth Barnes: Yeah, a bunch of compute governance. Or just gathering up a bunch of the relevant scientists and putting them on working on something else. It just feels like there’s this response of, “In that case, we’ve got to build a bigger one.” OK, but what if you actually started from the thing that you’re trying to prevent?\nBecause nations are already pretty great at threatening each other with mutual destruction. Having a more destructive threat really doesn’t help you very much. Most nations are basically… destroying a few cities, they won’t prioritise that differently than the end of the world, so just escalating your maximum threat doesn’t seem that helpful.\nSo it’s got to be some kind of defensive tech thing. Maybe we should just think directly about what that is, and maybe there are more efficient ways to make progress on that.\nRob Wiblin: All right, let’s switch on to something a little bit different. There’s lots of different organisations in this general tapestry: there’s METR, there’s the companies, there’s government agencies like the AISIs, there’s other independent nonprofits like Apollo. I can’t remember all of them.\nWhat would you say is the comparative advantage of METR in this ecosystem, and maybe what are the strengths and weaknesses of the other alternative groups that are involved in evals and enforcing responsible scaling policies in general?\nBeth Barnes: Man, I have a bunch of stuff I could say here, but I’ll start with nonprofits versus governments versus companies in terms of who should work at each.\nWe talked a bit already about model access for research and things, that that is one reason that people use for going to labs. I don’t think that is a huge deal. I think METR has some disadvantage of having less infra and direct access to models and things, but actually the compute availability: the labs give us free tokens, and then we can buy our own compute for doing our own experiments, and there are good open source models.\nThe advantages of METR are we can\n\n\nReally just focus on the mission. We don’t have any other competing priorities. I think governments have various political things that affect what mandate they have, and they’ve got to impress various people. They have maybe less longevity; they more don’t know whether they’re going to be around in some number of years. Whereas METR, we expect to be able to keep fundraising; there’s no particular reason that METR would disappear with the next administration.\n\nAnd unlike being at labs, it’s not like, “This safety thing would be better if it made our product more useful. Look, this technique is helpful here. Can you please help us do this thing? Or you better do those evals quickly because otherwise it’ll be blocking deploying this next model.” In that case, you’re just kind of always sprinting, trying to get the evals done — so that you’re either just not finished in time, or if they actually were to delay until the evals are ready, which I don’t know if they would actually do.\n\nWhereas at METR we can just be like, what are the most important things for the field overall that are just the highest priority for us to work on? And basically do that. If we’re like, actually it’s maybe not that important to do evals at this specific point for the specific model, we’ll just focus on something else.\n\nRob Wiblin: What are the weaknesses for METR?\n\nBeth Barnes: I think it’s just size, in that the total amount of technical talent is obviously much lower than at a huge company which has a bunch of people who are just doing capability stuff and are super expert in their own areas. So in some sense, there’s less mentorship and more of the normal startup property of you get thrown in and you grow because you’re given a bunch of responsibility, and you have to grow to live up to it — as opposed to you’re like this cog in this big machine, you’re being coached, and you’re incrementally moving up the ranks or something.\n\nRob Wiblin: In a way that’s very organised and predictable.\n\nBeth Barnes: Right. So I think for people who very much want a quiet life, very predictable, and want to be told what to do more, it’s not a great fit. There’s a few patches of our work that are more like this, but more of it is fast moving.\n\nAnd just being a smaller organisation I think is helpful for being able to pivot more rapidly. A bunch of things are changing all the time, and progress is fast. So I think that’s pretty useful, and it’s easier for us to prototype things that we want the company to do that would be harder to get them to prototype internally.\n\nSo we’ve been thinking about doing a control setup and control evals for our own usage of the models — where we do things like replacing all the models we’re using with one that’s been prompted to try and screw us over or something, and then see how this goes. It’s just way more likely that METR could do that than DeepMind is going to do that or something.\n\nBut I think similarly on the good at exploring or moving fast, compared to working with governments, it’s sort of lower stakes for labs to work with us. It just has less of you need all the government relations people to sign off, and legal stakes, and it’s less formal and more like, “I guess we can just do some research together.”\n\nBut I was supposed to be talking about disadvantages of METR.\n\nRob Wiblin: Well, I guess the usual thing with nonprofits is that it’s harder to scale really rapidly even if you’re succeeding. That doesn’t necessarily translate into much greater fundraising that would allow you to do your stuff 10 or 100 times as much. I guess that might be one issue, among others?\n\nBeth Barnes: I think it’s more just like we’ve found it harder to hire because we’re not paying as much as the labs. Sometimes people think we pay normal nonprofit salaries. That’s not the case; we pay approximately the matching cash compensation that people would get at AI companies — just the equity is impact equity, imaginary impact equity.\n\nSo it feels like that’s been the limit to growth, is hiring technical staff. It hasn’t actually been fundraising. I think if that started to become a bottleneck we could do more charging for our services, and I think could push more on moving things like the AI Safety Fund along — like a bunch of labs could put a bunch of money into a pot run by the Frontier Model Forum and then that could be granted to third-party eval orgs or other nonprofit safety, independent safety things.\n\nIt’s good because it scales with the capacity of the labs: the labs are paying for it, but it doesn’t have this direct coupling with you making them happy. There are multiple instances of financial auditors signing off on various books that were extremely dicey — famously with Enron or Arthur Andersen, because they had the auditing and a consulting arm and it just was some very large fraction of their overall revenue. So it just would have been extremely bad for Arthur Andersen as a company to not just give Enron what they wanted.\n\nSo you want to avoid that kind of setup. But I do think if you have something where there’s an industry group, and to be a member you have to pay into this pot, then that’s distributed to evaluators. But there isn’t a specific relationship between will a particular lab keep working with a particular evaluator if that evaluator gives them an unfavourable rating.\n\nRob Wiblin: Right. What’s FMF?\n\nBeth Barnes: Frontier Model Forum, a 501(c)(6). The point of it is a carveout from antitrust, where companies can talk to each other about things like safety and incident reporting, and RSP agreements and whatever, without being subject to the usual antitrust concerns. But it also makes sense as a hub to coordinate other things.\n\nRob Wiblin: Do we currently have an arrangement where there’s some pooling of funds and then auditors are assigned to companies, somewhat whether they like it or not, so that there isn’t this strong incentive for them to just get the answer that they want to hear?\n\nBeth Barnes: No, there is a pot of money that labs have contributed to that’s set up by the FMF. That has like $10 million, I think. But this kind of thing seems pretty feasible, because this is just pocket change for the labs. It’s really not that much money for them. So the money side I think is fine.\n\nSo back to the point of this: METR has generally been more bottlenecked on talent than on fundraising.\n\nRob Wiblin: But it sounded like you were saying that one challenge with attracting the right talent is that you’re not able to pay them equity in the way that the companies can. Which, given how the valuation of these companies has exploded, might be a big factor in people’s choices; the dollar signs can be very large in people’s eyes.\n\nBeth Barnes: Right.\n\nRob Wiblin: So I guess money maybe is an issue: that you need to pay people more extra cash. And if you just had absolutely unlimited financing, maybe you would pay people more.\n\nBeth Barnes: Yeah. Obviously these things trade off. I think when you’re asking what are you bottlenecked on, it’s sort of like, what do you think marginal people should think is the right thing to contribute to?\n\nYeah, more funding would help spend less executive time on fundraising, and we could do things like pay higher salaries that would allow us to attract more talent. But it seems like we’re probably at the point where there’s not that many people who, if we just paid them a bit more… It’s like, we could 10x what we were paying people, and then maybe we’d be competitive with lab equity or something. Maybe not quite that high, but probably for some. Yeah, it’s almost in that region.\n\nSo it’s like, that would be a lot of fundraising. Or a few marginal people could —\n\nRob Wiblin: Could accept a merely very high salary in order to save the world.\n\nBeth Barnes: Right, right. 80,000 Hours: how many people have you coached? Why can we not hire a senior software engineer? Not even an ML engineer, just a software engineer or DevOps. I’m like, where is everyone?\n\nRob Wiblin: Interesting. Do you want to go through the kind of roles that it will be useful for people to potentially apply for if they’re suitable?\n\nBeth Barnes: We have senior research engineer / research scientist roles. That’s just the normal senior ML role. We have some combination of research and research engineering skills, so it can be anywhere on this continuum. You just need the sum to be good enough.\n\nAnd then also senior software engineer and senior DevOps engineer. I don’t know what exactly is going to be up on our website at the time this podcast goes live, but currently maybe looking for head of operations or operations lead. And basically always looking for senior technical talent.\n\nRob Wiblin: For technical people who are still listening at this stage of the interview: this conversation hasn’t been maximally orientated at you, because you’re a relatively small audience. And if you would like to know more about what METR does and its most impressive work, you should go out and check out the website metr.org, where you can read the full research reports and see if you’re impressed and it’s something you would like to be involved with.\n\nBeth Barnes: Yep. And apply, chat with us. Or if you want a taste of some of the activities, we’re probably still looking for baseliners — so you can compete with the models on the ML tasks, and we’ve been using these as work tests for the humans as well as for the models.\n\nRob Wiblin: Just to explain: baseliners are people who you use to measure how difficult the tasks that you’re giving to the AI are, so you’re establishing the baseline.\n\nBeth Barnes: Right.\n\nRob Wiblin: Another reason why you might find it a little bit challenging to compete with the companies is that they have huge recruitment teams that are specialised in corporate recruitment and recruiting people from exactly the tech industry. I’m sure you know all the things they do: take people out to dinner, give them exploding offers, really put the hard word on them about how important this is, maybe trash talk everyone else. And I guess those are things that as a smaller nonprofit, perhaps a little bit friendlier and a little bit less corporate, you’re less inclined to do perhaps.\n\nBeth Barnes: Yes, we have in fact encountered all of the things that you named. It is a bit unfortunate. We just can’t compete with the level of recruiting intensity really. And also, maybe this is just a personal flaw, but I just really don’t want to do that. It just feels like such a waste of everyone’s time. There’s something which makes sense, which is like everyone should explain who should work at their company and why it might be good and what are the advantages. And then people should talk a bunch to each other and decide.\n\nBut there’s something that feels like it’s going on at the moment, where people spend a tonne of effort and are trash talking each other and trying to compete. It just kind of feels like a waste of everyone’s time and is unhelpful. I wish people just made decisions in a different way that was less affected by who took you out for dinner more times or something.\n\nRob Wiblin: Yeah, some of those things are a bit disreputable. I think exploding offers, probably no one would say that is the optimal way of allocating people across different organisations. These are things where you say you’ve got to join within 24 hours or we’re going to ditch you.\n\nOther things, like talking to people at great length and really pitching them very strongly on the value of the work, I guess seems like it’s fair play. But you also really want to be doing your research. So it could be hard for people to carve out time to compete with people on the level of aggression that a for-profit company can potentially offer.\n\nBeth Barnes: Yeah. And we’ve had people who I think of as relatively senior being like, “I want more senior mentorship, so I’m going to go to this company.” It’s like, “No, you were supposed to be the senior mentorship.” There was a bit of a bootstrapping thing here.\n\nBut I think also we actually have some staff who are young in age but are senior in ability level, and just are not kind of famous in some ways. And hopefully this is getting better over time as we publish more stuff.\n\nRob Wiblin: Are there any other organisations or work that you would like to see exist that doesn’t exist? Or any things that people might think that you’re doing that in fact you’re not actually able to do that other people need to take on?\n\nBeth Barnes: Man, I have a very long list here. One thing that people seem to get confused about is the extent to which we’re really serving as the auditor for all these companies. And we’re not really doing that; we’re kind of trying to prototype these oversight arrangements and what arrangements work technically — and maybe if no one shows up and it really seems like crunch time, we’ll just try and scale and do it ourselves.\n\nBut we were imagining that someone else would do the more formalised version of this and we would continue to do the research exploratory version. Maybe that’s the AISIs. But if the AISIs get removed, then someone else would need to do that.\n\nRob Wiblin: I mean, who would that be?\n\n\nSo the AI safety/security institutes that govern organisations, this would obviously be their natural role, auditing all of these frontier models to make sure that they’re not able to do things that the government regards as incredibly dangerous and illegal. If they don’t do it, who would do this?\nBeth Barnes: There are a bunch of more specialised organisations that maybe make more sense to do at least some of this, like Pattern Labs for cyber. There’s just organisations who have more specialisation compared to METR with a customer-facing delivery. Where our focus is more like: How much is this actually improving safety? Are we learning stuff? Are we derisking things? I think we might expand more and have more evaluations as a service arm, but that’s not something where it’s like we’ve got it covered, and no one else should bother. No: please come do that. And we’ve only been doing dangerous capability evaluations thus far. We’ve been thinking about alignment and control a bunch the whole time, and this has been in some of the RSPs and stuff, to some extent, at least been in our thinking about it. But we’re only just starting some of that now. I think Apollo is doing good stuff and interesting stuff. One thing is that we’re imagining trying to focus on doing good science and being sort of neutral experts, and less on advocacy and really holding labs to account or political campaigning or any of those things. So those are things that we very much don’t have covered. We’re much more like technical advisors; someone might come to us and be like, “Is this bill good? Would this bill actually reduce risk?” That is the sort of thing we’d be very happy to answer a bunch of questions about. But we’re not doing, “How can we raise awareness of the fact that this lab isn’t doing this thing they said they would?” or something. It’s not a thing that we’re on top of. So we’re not providing accountability for the companies in general. We’re much more just trying to figure out, “How do you tell if the models are about to kill you?”\nRob Wiblin: Right, yeah. So fundamentally you’re a research institute. That’s your core culture, and you’re trying to figure out how to measure things that have never been measured before, how to quantify things that have never been quantified before about these models — which is an enormous terrain because so little is known and understood. I guess there’s room for pressure groups, there’s room for policy advocacy, there’s room for the for-profit auditing thing that can scale enormously and is able to implement in a practical way the kinds of discoveries that you might have been making a year or two before.\nBeth Barnes: Right. One paradigm I can imagine being in is that there’s someone else offering these more scaled services, and METR’s role is more like reviewing this for quality. You know, does the eval actually provide the evidence that it’s supposed to be providing, as opposed to just running them for all the companies that want them to? There’s just quite a lot of models that people want evaluated. Another thing: we just happen to be that evals is our focus. I don’t think that’s because evals are more important than everything else; I think there’s just a tonne of work in improving the mitigations and actually implementing them at companies that is not being done, like the unlearning thing. It just feels like there’s a tonne of incredibly basic things that just no one has tried. Or things like providing a filtered dataset: we think that maybe it would be good to remove a bunch of the stuff about scheming AI, and put a bunch of stuff about humanity and AI being friends in a dataset. It just feels like there’s various dumb interventions that would be good, like more support for whistleblowers.\nRob Wiblin: OK, you’ve been very generous. We’ve been talking for four and something hours at this point, so we should probably try to begin to wrap things up. I feel like the conversation has maybe leaned a little bit towards the doom and gloom. Maybe I’m a little bit jetlagged. Maybe I’m leaning towards the negative this week. In terms of something a little bit more hopeful, I’m curious to know what METR’s research agenda is going forward. And maybe also, it sounds like despite the fact that we’re in a somewhat negative or somewhat challenging situation, you might say, there are a lot of different useful threads that people could pull on, and really the problem is that all of the low-hanging fruit that’s available is not really being taken: there isn’t the resources or the focus or the number of people involved. So the positive side would be that there’s so much that can be done.\nBeth Barnes: It feels like we’re just in some very high-stakes situation, where it’s kind of like there is everything to play for. It’s not like the outcome is overdetermined. It’s just really like there are all these extremely valuable things to do.\nRob Wiblin: All right, let’s take it bit by bit. So what are the priorities for METR over the next couple of years. Years? I should have said next couple of months. Who can plan over that kind of time horizon?\nBeth Barnes: Yeah, next couple of months is pushing it. Who knows what models will be out by then. But we are thinking about what are the most important asks to make from companies now, given what we know overall, and what exactly are the biggest things that we want to tell the world that we think are most important on that front? And it’ll probably be trying to get out of somewhat reactive mode, in terms of companies being like, “Please do these evals for us,” and get into what we think are the most important things that should be happening, and kind of make that clear to everyone. That’s one strand of things. Another strand is moving beyond just dangerous capability evals into alignment and/or control evals, and maybe also communicating to people why we’re doing that and why we think the inability safety cases are not going to go that much further.\nRob Wiblin: OK. So control, we’re going to do other interviews about that. But that’s an approach where you say, maybe this model would like to screw me over, but we’re going to put in place constraints on it that would mean that even if it was trying to escape or it was trying to cause some harm, it wouldn’t be able to. So a control eval would be evaluating whether such a setup is actually functional, so I guess you might actually make a model that would like to do something bad, and then see whether the constraints are actually binding on it. Is that the idea?\nBeth Barnes: Right. Yeah. In some sense, you can think of the current dangerous capability evaluations as a special case of control evaluations, where your control strategy is nothing. So it’s like, what if we did some kind of monitoring? Or there’s just a bunch of interventions you could do that hopefully would make it harder for models to do extremely bad and scary stuff, and we can try some of those out and test them. One thing I’ve been toying with is the idea of how you would evaluate a control setup. Maybe you want to red team: someone creates a model that is just nefarious and is trying to mess with you, and then you see what it’s able to achieve. Or you set some threshold of like, if the model is able to set up an agent that we don’t realise is running for this many days, then that counts as a catastrophe, and we’re trying to build a control system that can prevent that. And then you test that with your team. So we could potentially do that internally. I think something we might work on a bit first is making sure we’re getting maximum use out of current models, and using them in our research to accelerate the research in general. And then the more we do that, the more relevant the control stuff will be because it’s like, could the model now be sabotaging your research or something, now that you’re using it more? And then another thing that we’re working on is… If you’ve hidden chain of thought, you can’t tell if the model is maybe deliberately sandbagging or messing with your evals. You can’t trust the results of your evals if you can’t see all the thinking that the model’s doing before it answers. But maybe labs are very unwilling to share the chain of thought. It seems like at the margin the thing that would most decrease our current fear about models would be that we know that something has been run on the chain of thought that looked for any kind of scheming or checked if it was totally illegible or whatever. So let’s build a dataset and some kind of classifier setup based on open source models that then a lab can run. And they don’t have to release the whole chain of thought; they could just release the numbers, and maybe a smaller number of examples that wouldn’t be enough to distil on, or rephrased examples or something.\nRob Wiblin: Just so I understand: given all that we’ve been talking about with the chain of thought, a really obvious thing to do would be to have a model that can actually read basically all of the chain of thought that this model is outputting and say, “Is there some obvious secret code going on here? Is it producing neuralese? Is it producing this gibberish that I can’t make sense of? Or is it just literally talking about scheming?” Then it definitely adds an extra constraint, an extra challenge for the model if it wants to do anything nefarious, if you’ve got another model basically that you do trust reading its mind all the time.\nBeth Barnes: Right. It would just be nice to know that at least the model isn’t just blatantly doing this. I think the two things that we would need from a lab to establish this is: one, you ran this classifier and you didn’t find any examples, and two, you declare that there’s no reason to expect that the chain of thought would look normal but would be hiding this kind of thing — so you haven’t trained it specifically to not say this kind of thing out loud.\nRob Wiblin: To obfuscate its true thoughts and its true communication.\nBeth Barnes: So then you train your classifier, or maybe this is just a prompted model, but you want to look for things like: Is this gobbledygook? Is it understandable? How far from reasonable English does it seem to be? And is there any reasoning about how to do on the evals or scheming or any underhanded behaviour? This is not a control proposal, and you’re not imagining the model that you’re maybe scared of reasoning about this monitoring in its chain of thought and being like, “I have to do something that the other model won’t recognise,” or, “I have to suddenly do something that won’t get classified” — it’s more just like, if we’re in this situation where the model is just blatantly doing this misaligned thing, it would be nice to know that. So if we assume that it’s not smart enough to be doing all this reasoning about how it might be monitored in the forward pass, but it can only do that in the chain of thought, then this should catch that. We just need to make the dataset diverse enough that we have a decent chance of catching a bunch of things if this is going on — as opposed to this needs to catch every single case, because it’s like a question about the overall model tendency rather than trying to prevent specific occurrences.\nRob Wiblin: Is there any other low-hanging fruit you want to shout out that METR might take on or you’d like to see other people have a go at?\nBeth Barnes: I think there’s a lot more stuff to be done on mitigations and just improving the odds of good alignment. We touched on this earlier, but making the supervision signal more accurate. I guess some question people might have is like, what is the story for these models becoming malicious when currently they seem so nice and friendly and helpful and polite and stuff? Just intuitively, I think it’s helpful to imagine, kind of analogous to a human who becomes very cynical or sociopathic or something, particularly if the model is being trained to kind of give contradictory answers to please different users in different ways, or to say things that humans think is true that the model knows are false. It’s coming off as super friendly and happy and whatever because that’s what you’ve been rewarding it for, but actually it should be thinking about like, “What does this guy want? What have I got to say here? These ignorant humans don’t understand this thing, so I better say…”\nRob Wiblin: “Better tell them what they want to hear.”\nBeth Barnes: Yeah, that’s the intuition pump for me for where this would come from. And I think this aligns with the theoretical claim of why doing this kind of thinking would actually improve your score: it’s a reason why some cynical, manipulative humans can do better than genuine ones.\nRob Wiblin: Yeah. Interesting,\nBeth Barnes: Because you’re just directly reasoning about how do I push this button.\nRob Wiblin: “I’ve got to tell my boss how great he is.”\nBeth Barnes: Yeah. So this is a phenomenon we’re already aware of, and the less we can push the models into that, the better. Just have a higher chance of ending up in some baseline of nice personality. And also it would be nice if we could measure this\n\n\nAnd get a sense of how much is this going on, and how much is this training process really pushing the model to be playing the training game and trying to game things.\nI think there’s a bunch more model organism work that would be interesting and useful to do.\nRob Wiblin: Yeah, I’ve heard people talk about model organism work. I guess from the name I have some idea what that is, but could you elaborate a little bit?\nBeth Barnes: Yeah. So it sounds like you’ve read the alignment faking paper. That would be an example of a model organism: here is an actual instance of the kinds of cognition and behaviour and things that we’re concerned about happening in a model.\nSo there’s a bunch of ways people could push on that and make it more realistic, do it with smarter models, see if we can get more understanding of what exact kind of fine-tuning is needed to give it this information, and could we check whether it had sufficient information to be doing this, and can it do it without reasoning it out loud. Yeah, a bunch of things here.\nMaybe one other thing that seems particularly good is actual applications of interpretability or otherwise developing more methodology for telling how useful your interpretability progress is.\nI think a nice example of a test case for interpretability is, in the alignment faking paper, most of the experiments are with the model with the scratchpad where it has this hidden place to do all the reasoning. That’s where it is like, “If I want to be able to keep to my values, I should do this thing.”\nThey also found that some of the time, even if you don’t give it the chain of thought and it just has to answer straight away, it also appears to fake alignment, in that it does the thing in the conditions where it’s being overseen and then doesn’t do it when it can get away with it. But it’s hard to know, when you’re not seeing the chain of thought. It’s like, is that why it’s doing it, or is something else going on? Is there a confounder there somehow or just this is just not what we think it is somehow?\nSo I feel like that’s just a very good test case for interp. Can we produce evidence that what this model is doing is scheming or reasoning about the training process or something? That would be great.\nRob Wiblin: Yeah. All right. That’s a whole lot of examples of low-hanging fruit there that people might be able to pluck.\nAll things considered, I’d really rather live in a more boring time in history, but I guess we don’t exactly get to choose that, so we just have to go out and continue with all of this really useful work — and fingers crossed it’ll be sufficient.\nBeth Barnes: Yep, KBO.\nRob Wiblin: Yeah. Is there anything else you want to say to listeners, before we wrap up? We’ve kind of run up against the timeline of our booking at this place.\nBeth Barnes: Yeah. Like I said, everything to play for. Now is the time. If you’re like, “I was going to go and skill up and do some things and I’m working on capabilities until…” I’m like, now is the time.\nRob Wiblin: “…and sometime, some day in the future maybe I’ll do something useful.” Now is the time to do something useful.\nBeth Barnes: Yes. Time to do something useful is running out.\nAlso, if you’re sitting on money and not donating it, I think the compounding returns of getting stuff started now, such that it can be more mature at the time it’s needed is just very high relative to other things — like learning or your money gaining interest or whatever. It just feels like we’re in the now is the time to act phase.\nAnd METR is hiring. Come look at METR. We really need people.\nRob Wiblin: Yeah. I imagine if things do go badly, there’ll be a whole lot of people who are holding onto money and are still planning to later do something useful with their career at some later point, who have missed the window or missed the boat.\nBeth Barnes: Yeah, it just ends up being way less useful than it could have been, because you’re like, “I guess I started my job one week before everything went down and I didn’t know how to do anything, so it wasn’t useful.”\nRob Wiblin: Inspiring words. People got to get on it. My guest today has been Beth Barnes. Thanks so much for coming on The 80,000 Hours Podcast, Beth.\nBeth Barnes: Thank you.\n",
  "dumpedAt": "2025-07-21T18:43:24.551Z"
}