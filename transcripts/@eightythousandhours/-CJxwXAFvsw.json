{
  "episodeId": "-CJxwXAFvsw",
  "channelSlug": "@eightythousandhours",
  "title": "The four most likely ways for AI to take over | Ryan Greenblatt, Chief Scientist at Redwood Research",
  "publishedAt": "2025-07-08T15:40:16.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Rob Wiblin: Today I have the pleasure of speaking \nwith Ryan Greenblatt. Ryan is the chief scientist  ",
      "offset": 71.348,
      "duration": 4.012
    },
    {
      "lang": "en",
      "text": "at Redwood Research and the lead author on \nthe paper “Alignment faking in large language  ",
      "offset": 75.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "models” — which has been described as probably the \nmost important empirical result ever on the topic  ",
      "offset": 79.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of loss of control of artificial intelligence. \nThanks so much for coming on the show, Ryan.",
      "offset": 84.08,
      "duration": 3.663
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, it’s great to be here.",
      "offset": 87.743,
      "duration": 1.045
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Let’s start by talking \nabout the best arguments for and against  ",
      "offset": 88.788,
      "duration": 3.692
    },
    {
      "lang": "en",
      "text": "a software-based intelligence explosion in \nthe relatively near future — by which I’m  ",
      "offset": 92.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "thinking maybe under four years, approximately.",
      "offset": 97.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "What do you think is the chance that \nwe’ll be able to largely automate AI R&D,  ",
      "offset": 99.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "or maybe roughly automate a full AI company as it \nexists today, within the next four years or so?",
      "offset": 104.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I think the probability of that \ncapability existing — not necessarily being used,  ",
      "offset": 109.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "not necessarily being fully \ndeployed — is about 25%,  ",
      "offset": 114.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and then maybe about 50% if we \nextend it to like eight years.",
      "offset": 118.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think many people hear \nthat and they would wonder, is this just  ",
      "offset": 122.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "pure speculation? How can we form realistic or \ngrounded expectations about this kind of thing?",
      "offset": 127.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "I’d be interested to know what are the \nkey pieces of evidence that bear on you  ",
      "offset": 134.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "making a prediction or forecast like that. What is  ",
      "offset": 138.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "a key piece of evidence that makes \nyou think it is plausible at all?",
      "offset": 141.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: To start, I \nthink a good place to look is,  ",
      "offset": 144.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "what is the current level of AI capabilities? \nHow close does it feel? People have obviously  ",
      "offset": 148.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "wildly different intuitions about how \nclose we are in some objective sense.",
      "offset": 153.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I think currently the situation is: the \nAIs are getting better and better at math;  ",
      "offset": 157.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "they’re sort of marching through the human \nregime in math. They’re able to do roughly  ",
      "offset": 162.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "about one hour or one and a half hours \nof sort of isolated software engineering  ",
      "offset": 170.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tasks — by that I mean a task that would \ntake a human roughly an hour and a half.  ",
      "offset": 174.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And they’re sort of getting better \nat a variety of different skills.",
      "offset": 179.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "So I think we’re at a pretty objectively \nimpressive regime — and importantly,  ",
      "offset": 182.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a much more objectively impressive \nregime than we were at two years ago,  ",
      "offset": 187.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and certainly much more impressive \nthan we were at four years ago.",
      "offset": 190.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "A naive starting point for where we will \nbe in four years is: where were we four  ",
      "offset": 193.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "years ago? Where are we now? Trying to \nqualitatively extrapolate from here to  ",
      "offset": 198.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "there. This is a bit of a sloppy perspective. \nLike, I think maybe what I would have said  ",
      "offset": 203.84,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "two years ago would have been that \nwe had GPT-3, now we have GPT-4:  ",
      "offset": 211.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "what’s the gap there? Let’s sloppily extrapolate \nthat forward and get a sense from there.",
      "offset": 216.08,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "I think now we can do better, because we have \nnot just GPT-3; we have GPT-3.5, we have GPT-4,  ",
      "offset": 222.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we’ve had the progression since GPT-4 in roughly \nthe two years since then. So over that period,  ",
      "offset": 227.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we’ve gone from the AIs just barely being \nable to do agentic tasks to having some  ",
      "offset": 232.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "reasonable amount of ability to succeed at \nthat, some ability to recover from mistakes.",
      "offset": 237.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So over this period, GPT-4 could maybe \ncomplete some agentic tasks that would  ",
      "offset": 241.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "take humans like five or 10 minutes; it \ncould sort of understand how to use the  ",
      "offset": 246.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tools. I think GPT-3.5 basically couldn’t even \nunderstand the tools in agentic scaffolds;  ",
      "offset": 251.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "GPT-4 could understand these things. And \nthen, over that period we’ve gone from like,  ",
      "offset": 256.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "“can understand the tools and can \nmaybe do it some of the time” to  ",
      "offset": 260.64,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "“can do tasks 50% of the time that takes a \nhuman software engineer an hour and a half.”",
      "offset": 263.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "There’s sort of a trend line of how fast we’ve \nbeen marching through that regime. Over 2024 at  ",
      "offset": 268.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "least it’s been pretty fast progress, starting \nfrom basically nothing at all to an hour and a  ",
      "offset": 273.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "half, with doubling times of like, I’m stealing \na bunch of content from METR, which I think maybe  ",
      "offset": 278.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "there’ll be a podcast with Beth which \ncovers a bunch of the same content — but  ",
      "offset": 284.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we’ve seen doubling times that are fast enough \nthat we’d expect the AIs doing eight-hour tasks  ",
      "offset": 290,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "or 16-hour tasks in maybe a little less than \ntwo years, which is pretty fast progress.",
      "offset": 295.76,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "And then from there, if you’re into the \nregime where they’re doing weeklong tasks,  ",
      "offset": 305.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I think you can maybe get pretty \nclose to automating the job of a  ",
      "offset": 310.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "research engineer with some schlep on top of that.",
      "offset": 313.84,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, so you would say that objectively \nthey’re pretty impressive in what they can do  ",
      "offset": 316,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "right now. Some people have a more sceptical \nreaction to that. Is there anything that you  ",
      "offset": 321.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "could point to to be clearer about what they’re \nable to do and what they’re not able to do? And  ",
      "offset": 325.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "maybe answer people who would say, “Sometimes \nI use these tools and they seem very stupid,  ",
      "offset": 330.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "or they don’t seem able to do things that \nI would expect them to be able to do,  ",
      "offset": 334.64,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "or they produce a whole lot of \nreasoning and I find errors in it”?",
      "offset": 337.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: So I’d say my kind of \nqualitative, vibesy model is: the AIs are pretty  ",
      "offset": 340.48,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "dumb, they used to be much dumber, they’re getting \nsmarter quickly, and they’re very knowledgeable.",
      "offset": 347.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "So I think a lot of what people are interfacing \nwith is that we’ve got these systems that  ",
      "offset": 352.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have got some smarts to them: they can really \nfigure out some pretty general situations OK,  ",
      "offset": 357.013,
      "duration": 5.547
    },
    {
      "lang": "en",
      "text": "and especially with reasoning models \nthey’re pretty good at thinking through  ",
      "offset": 362.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "some stuff. In addition to that, they’re \nvery knowledgeable, which means that there’s  ",
      "offset": 366.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a misleading impression of how overall general \nand how adaptable they are that people notice.",
      "offset": 370.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And this is a lot of what people are reacting to. \nThere’s sort of an overoptimistic perspective,  ",
      "offset": 376.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "which I would say is characterised by this \nchart Leopold [Aschenbrenner] had where he’s  ",
      "offset": 380.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like PhD-level intelligence, or like people \nare like PhD-level intelligence — and then  ",
      "offset": 384.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "some people respond to that being like, “PhD level \nintelligence? Come on, it can’t play tic-tac-toe.”  ",
      "offset": 388.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Maybe that’s no longer true with reasoning \nmodels, but directionally, you know, it can’t play  ",
      "offset": 394.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tic-tac-toe; it can’t respond to relatively novel \ncircumstances. It gets tripped up by this stuff.",
      "offset": 398.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Now, I think we have to discount some of \nhow it’s getting tripped up by this stuff,  ",
      "offset": 402.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "because I think a bunch of these things might \nbe better described as cognitive biases than  ",
      "offset": 407.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "lack of smarts. Like there’s a bunch of \nthings that humans systematically get wrong,  ",
      "offset": 414.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "even though they’re pretty stupid, or in \nsome sense they’re pretty dumb errors.",
      "offset": 419.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Like the conjunction fallacy. What if you’re \nlike, “What is the chance that someone is  ",
      "offset": 424.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "a librarian? What is the chance they’re \na librarian, and some property librarians  ",
      "offset": 431.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "have?” I might be getting this a bit \nwrong, but [humans will say] it’s more  ",
      "offset": 434.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "likely on the conjunction of the two, \neven though that has to be less likely.",
      "offset": 439.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And I think AI systems have \nbiases that are sort of like this,  ",
      "offset": 443.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that are shaped by the environment in which \nthey were created or by their training data.",
      "offset": 446.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "As an example, if you give AIs a riddle. Like, \n“There’s a man and a boat and a goat. The boat can  ",
      "offset": 450.8,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "carry the man and one other item. How many trips \ndoes it take for them to cross the river?” The  ",
      "offset": 458.72,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "answer is one trip: they can just cross the river. \nBut there is a similar riddle involving a man,  ",
      "offset": 465.76,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "a goat, and something like a cabbage — you know, \nthere’s some tricky approach — and the AIs are so  ",
      "offset": 472.72,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "reflexively used to doing this that maybe they \nimmediately blurt out an answer. They sort of  ",
      "offset": 480.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "have a strong heuristic towards that answer, but \nit might just be more that they feel a nudging  ",
      "offset": 486.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to that. But if you get them to be like, “Oh, \nit’s a trick question,” then they go from that.",
      "offset": 490.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And in fact, you can get humans on \nthe same sorts of trick questions,  ",
      "offset": 496.64,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "right? So if you ask humans, “What’s \nheavier: a pound of bricks or a pound  ",
      "offset": 498.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of feathers?” they’ll say the bricks, and \nthey get tripped. It’s like the language  ",
      "offset": 503.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "models have the exact opposite problem, \nwhere if you ask them, “What’s heavier:  ",
      "offset": 507.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "two pounds of bricks or a pound of feathers?” \nthen they’re like, “Same weight! Same weight!”",
      "offset": 510.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "So I worry that a lot of the tricks people \nare doing are sort of analogous to tricks  ",
      "offset": 514.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you could execute on humans, and it’s \nhard to know how much to draw from that.",
      "offset": 520,
      "duration": 4.068
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. A general challenge here \nin assessing how capable they are is, I think  ",
      "offset": 524.068,
      "duration": 4.172
    },
    {
      "lang": "en",
      "text": "Nathan Labenz uses this expression that they’re \n“human-level but not human-like” — so overall,  ",
      "offset": 528.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "maybe they’re similarly capable as human employees \nin some situations, but they have very different  ",
      "offset": 532.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "strengths and weaknesses; they can be tripped \nup in ways that seem completely baffling to us.",
      "offset": 537.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "But I guess you can imagine an AI society that \nwould look at humans and say, “How is it that  ",
      "offset": 541.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "they can’t multiply in their heads two three-digit \nnumbers? That seems crazy. These are obviously  ",
      "offset": 545.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "not general intelligences. They have almost no \nmemory of this book that they read last week. That  ",
      "offset": 549.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "doesn’t make any sense. How could \nan intelligence act that way?” It  ",
      "offset": 554.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "makes it a little bit hard to have a common \nground on which to compare humans versus AIs.",
      "offset": 558,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Is there more to say on how you would \nassess what level they’re at now?",
      "offset": 563.52,
      "duration": 3.343
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, I think that I wouldn’t \nuse the term “human-level.” Maybe this is me  ",
      "offset": 566.863,
      "duration": 6.017
    },
    {
      "lang": "en",
      "text": "being a little bit of a conservative or me \nbeing a bit pedantic, but I like reserving  ",
      "offset": 572.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the term “human-level” for “can automate \na substantial fraction of cognitive jobs.”",
      "offset": 578.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "So maybe we’re starting to get into the \nhumanish-level AIs once it can really just  ",
      "offset": 584.24,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "fully automate away a bunch of human jobs, \nor be a part of the cognitive economy in a  ",
      "offset": 590.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "way that is comparable to humans — and maybe \nnot full automation at that point, but I also  ",
      "offset": 597.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like talking about the full automation point. \nSo that’s one thing, just responding to that.",
      "offset": 601.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "More context on how good the AIs are: some things \nwe’re seeing that are maybe somewhat relevant are  ",
      "offset": 605.68,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "we’re seeing AIs sort of march through how good \nthey are at math and competitive programming. So  ",
      "offset": 612.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "we’ve seen over 2024 going from basically the \nAI is bottom 20th percentile or something on  ",
      "offset": 619.04,
      "duration": 11.04
    },
    {
      "lang": "en",
      "text": "Codeforces to being currently in the top 50, \nI think, according to what Sam Altman said.",
      "offset": 630.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Top 50 individuals?",
      "offset": 636.32,
      "duration": 1.423
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Top 50 individuals. \nLiterally the top 50 people. Or at  ",
      "offset": 637.743,
      "duration": 2.737
    },
    {
      "lang": "en",
      "text": "least people who do Codeforces; maybe there’s \nsome people who aren’t on the leaderboard,  ",
      "offset": 640.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "but roughly speaking. And then it looks \nlike they’ll get basically better than  ",
      "offset": 643.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the best human at that specific \nthing before the end of the year.",
      "offset": 648,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "And then on math, this is based on an anecdote \nfrom a colleague, but maybe they’re currently  ",
      "offset": 651.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "at the level of a very competitive eighth grader \nor something on short numerical competition math  ",
      "offset": 657.36,
      "duration": 16
    },
    {
      "lang": "en",
      "text": "problems like AIME, the top eighth graders are \ndoing as well as the AIs are doing right now.  ",
      "offset": 673.36,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "And the AIs are, I think, a \ndecent amount worse on proofs.",
      "offset": 681.44,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "But both of these things are improving \nvery rapidly — they were much, much worse  ",
      "offset": 683.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a year ago. I think basically this is \nbecause we’re RLing the AIs in these tasks.",
      "offset": 688.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I sort of expect the same trend to hit \nagentic tasks, software engineering.  ",
      "offset": 693.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "We’ve already seen that to some extent: the \nAIs are already pretty good at writing code,  ",
      "offset": 697.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pretty good at following instructions, and OK \nat noticing errors, OK at recovering from these  ",
      "offset": 701.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "things. I think with a lot of runtime compute and \na lot of scaffolding, that can be pushed further.",
      "offset": 706.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Then there’s a bunch of things in which they’re \nweaker. So they’re a bunch weaker at writing,  ",
      "offset": 710.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "they’re a bunch weaker at other stuff. But \nI sort of expect that as you’re marching  ",
      "offset": 713.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "through software engineering, you’ll get \nto a bunch of these other capabilities.",
      "offset": 717.2,
      "duration": 3.908
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. OK, so we’ve got an \narguably quite high level now. They’re  ",
      "offset": 721.108,
      "duration": 5.692
    },
    {
      "lang": "en",
      "text": "becoming capable of doing tasks that \nwould take humans longer and longer;  ",
      "offset": 726.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "they’re able to follow instructions for a \nlonger period of time, complete tasks that  ",
      "offset": 730.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "have more open-ended choices to make. And that’s \ndoubling every half a year or something like that?",
      "offset": 733.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I think \nthe doubling time on time,  ",
      "offset": 739.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "my guess is over the next year it’ll be \nsubstantially faster than every half year,  ",
      "offset": 742.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "but maybe the longer-run trend \nis roughly every half year.",
      "offset": 747.2,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "So we might expect there was basically \na spurt starting at the start of 2024  ",
      "offset": 749.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "or a little into 2024 of people doing more agency \nRL — more reinforcement learning or training the  ",
      "offset": 754.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "AIs specifically to be good at agentic software \nengineering tasks. And I think that trend  ",
      "offset": 760.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "will continue, and perhaps even accelerate, \nthrough 2025, plausibly continuing in 2026,  ",
      "offset": 765.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "plausibly later. But maybe the longer running \ntrend is more like doubling every six months.  ",
      "offset": 769.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I expect it to be more like doubling every \ntwo to four months over the next year.",
      "offset": 773.68,
      "duration": 3.668
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, so very rapid \nincrease over the coming year.",
      "offset": 777.348,
      "duration": 4.332
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Very rapid, yes.",
      "offset": 781.68,
      "duration": 0.059
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: There’s this interesting dynamic where \nwe might expect that you could virtually fully  ",
      "offset": 781.739,
      "duration": 3.461
    },
    {
      "lang": "en",
      "text": "automate an AI company, maybe substantially \nbefore you could automate almost any other  ",
      "offset": 785.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "company. Because firstly, they’re putting a lot \nof their resources towards trying to figure out  ",
      "offset": 790,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "how to automate their own stuff and their own \nprocesses, which makes sense from their point  ",
      "offset": 794.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of view — firstly because it’s the thing that \nthey understand the best, and also, these are  ",
      "offset": 798.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "among some of the highest paid knowledge workers \nin the entire world. They’re pulling an enormous  ",
      "offset": 802.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "salary. So if they can figure out how to get an AI \nto do that, then that has enormous economic value.",
      "offset": 805.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And of course, the people running the companies \nthink it’s of much greater value even than it  ",
      "offset": 810.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "looks just based on the dollar amount, because \nthey think that they’re about to trigger this  ",
      "offset": 814,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "intelligence explosion, positive reinforcement \nloop which will change everything. So to them,  ",
      "offset": 816.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this is kind of the thing that by far they’re \nmost interested in automating. They care much  ",
      "offset": 821.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "more about that than automating \nMcKinsey’s consulting reports,  ",
      "offset": 825.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "even though that is also kind of \na lucrative business. So it could  ",
      "offset": 828.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "be that we haven’t automated consulting, even \nthough that certainly would have been possible,  ",
      "offset": 831.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "mostly because they just weren’t trying. They \nwere just trying to automate their own staff.",
      "offset": 835.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I would say that \nmy guess is that it’s probably hard,  ",
      "offset": 838.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "even with a bunch of elicitation, to near fully \nautomate reasonably highly paid human knowledge  ",
      "offset": 841.84,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "workers in relatively broad fields. But I do \nexpect that there’s some jobs that if the AI  ",
      "offset": 848.88,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "companies were trying harder, they might be \nable to automate substantially more than now.",
      "offset": 856.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "And in fact, the people benefiting \nmost from AI are people close to AI,  ",
      "offset": 860.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "as you’re saying. This is true now, and I think \nwill get increasingly true in the future — again  ",
      "offset": 864.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because of this dynamic of AI company employees \nare highly paid now; at the point when AIs  ",
      "offset": 868.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "are capable enough that they can automate AI \ncompanies, they’ll be even more highly paid,  ",
      "offset": 872.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you’ll be able to drive in even more investment, \nand AI company CEOs will be even more sold on the  ",
      "offset": 877.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "possibility of AI being extremely important. \nSo I think we’ll see an even bigger gap then.",
      "offset": 881.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "One objection you might have had would be like,  ",
      "offset": 888.16,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "sure, I agree that we’ll see a lot of \nconcentration on automating the AI companies,  ",
      "offset": 889.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "but at the same time, there’s a lot of \nvaluable human knowledge work outside  ",
      "offset": 894,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that you could automate. So we’ll see that \nin parallel, we’ll see some economic impact.",
      "offset": 897.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I think this is a reasonable first \nstab, but a problem with this story  ",
      "offset": 901.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "is the price of compute will go way up as \nthe value of AI intellectual labour rises,  ",
      "offset": 904.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "at least in these short-timeline \nscenarios where compute is very scarce.",
      "offset": 909.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Just to clarify, you’re \nsaying that the companies will be  ",
      "offset": 912.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "using a lot of compute to automate their \nown work, so much compute that in fact  ",
      "offset": 915.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that they won’t have many chips available \nfor serving other customers who are doing  ",
      "offset": 919.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "things that are of less economic value, or \ncertainly less important to the company?",
      "offset": 922.64,
      "duration": 3.663
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, broadly \nspeaking. But I think it’s even  ",
      "offset": 926.303,
      "duration": 4.417
    },
    {
      "lang": "en",
      "text": "just not on just automating \nthe stuff, but on experiments.",
      "offset": 930.72,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "So let me just put a little bit of flavour \non this. So how do AI companies spend their  ",
      "offset": 933.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "compute now? I think it will depend on the \ncompany, but my sense of the breakdown for  ",
      "offset": 938.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "OpenAI is it’s very roughly something like: \na fourth on inference for external customers,  ",
      "offset": 941.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "half on experiments — so things like smaller scale \ntraining runs that don’t end up getting deployed,  ",
      "offset": 949.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "doing a little bit of testing \nof RL code, this sort of thing,  ",
      "offset": 955.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so experiment compute for researchers \n— and a fourth on big training runs.  ",
      "offset": 959.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So like three-fourths of the compute is \nalready internally directed in some sense.",
      "offset": 964.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And then if we’re seeing a regime where the \nAIs can automate AI R&D and that’s yielding  ",
      "offset": 968.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "big speedups and seems very important, \nthen you could imagine the regime might  ",
      "offset": 973.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "look more like one-fifth on doing inference \nfor your AI workers — so you’re spending a  ",
      "offset": 975.92,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "fifth of your compute just running your \nemployees — three-fifths on experiments,  ",
      "offset": 984.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and one-fifth on training or something. \nObviously it’s very speculative.",
      "offset": 988.8,
      "duration": 3.985
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So customers have \nbeen squeezed out almost entirely.",
      "offset": 992.785,
      "duration": 1.678
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, yeah. I mean, \npresumably you’ll have some customers,  ",
      "offset": 994.463,
      "duration": 2.577
    },
    {
      "lang": "en",
      "text": "but it might be squeezed out almost entirely, and \nwe’ll see the prices rise. And I think when you’re  ",
      "offset": 997.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "thinking about what customers to be serving to, \nI think you should maybe be imagining, perhaps  ",
      "offset": 1001.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "surprisingly, that the highest wage employees \nmight be who the AIs end up going for first,  ",
      "offset": 1006.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "once the AIs are capable of this automation. So \nmaybe you should be thinking like Jane Street,  ",
      "offset": 1011.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "high frequency trading, places where \nthe AIs seem particularly helpful,  ",
      "offset": 1016.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "they seem particularly high paying, and they seem \nparticularly bottlenecked on intellectual labour.",
      "offset": 1020.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Now, I think we will see automation of a \nbunch of other professions in parallel,  ",
      "offset": 1025.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "but it might be that at the point when \nthe AIs are most capable of automating,  ",
      "offset": 1028.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "much more of the attention will \nbe focused on AI R&D. And I think  ",
      "offset": 1032.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "even possibly we might see effects like some \nprofession is slowly being automated — you know,  ",
      "offset": 1036.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "mid- to low-end software engineering \nmaybe will slowly be automated more and  ",
      "offset": 1041.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "more — and we might actually see that trend \nreverse as the compute gets more valuable.",
      "offset": 1044.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Because now we’re in a regime where just everyone \nis grabbing as much inference compute as they can,  ",
      "offset": 1048.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "or at least the biggest companies or \nthe company in the lead is grabbing as  ",
      "offset": 1052.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "much inference compute as they can, and \njust outcompeting the software engineers  ",
      "offset": 1056.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "using this or the companies doing software \nengineering competing with this compute.",
      "offset": 1061.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "I don’t currently expect a trend reversal,  ",
      "offset": 1064.48,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "but I think we could see automation trends \nplateau or even reverse because of this.",
      "offset": 1067.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because people have found even \nmore valuable things to do with the AI.",
      "offset": 1072,
      "duration": 3.888
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, that’s right. \nThis is dependent on relatively short  ",
      "offset": 1075.888,
      "duration": 4.192
    },
    {
      "lang": "en",
      "text": "timelines. I think you could expect things \nare sort of smoother on longer timelines,  ",
      "offset": 1080.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "where you wouldn’t expect trend reversals \nin that case. But if things are more sudden,  ",
      "offset": 1083.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "more jumpy, then it seems at least plausible.",
      "offset": 1086.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I’m curious to turn to what \nimplications does this have for how worried we  ",
      "offset": 1089.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "ought to be, and what specifically we ought to be \nworried about? If this is the way that things go,  ",
      "offset": 1096.32,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "what stuff could we be doing now that \nwould help us to navigate this? I mean,  ",
      "offset": 1103.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this is a scenario in which things are \nbecoming quite challenging for humans to  ",
      "offset": 1108.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "track or be involved in quite quickly. So \nwe would have to have set things up well,  ",
      "offset": 1112.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I suppose, for this to pan out well for \nhumans — rather than for us to just get  ",
      "offset": 1117.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "crushed in the passage of history. \nMaybe you’ll disagree with that?",
      "offset": 1121.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I think pretty \nquickly into this regime,  ",
      "offset": 1125.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "AI takeover using a variety of mechanisms becomes \npretty plausible and potentially surprisingly easy  ",
      "offset": 1130.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "through a variety of routes. We don’t \nknow. This is another huge source of  ",
      "offset": 1136.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "uncertainty. We had many conversions between, \nhow much progress do you get? How much does  ",
      "offset": 1139.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "that progress amount to in intellectual \nlabour? And then there’s the question of  ",
      "offset": 1146,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "how much does intellectual labour help with \ntakeover? What are the physical bottlenecks  ",
      "offset": 1148.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of takeover? How much does having more \nmissiles help relative to having whatever?",
      "offset": 1151.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I think there’s a bunch of scary things that  ",
      "offset": 1156.48,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "could happen here. One thing is that once \nyou’re automating your entire AI company,  ",
      "offset": 1158.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a thing that seems very plausible to me is: \nyou’ve handed things off to the AI system,  ",
      "offset": 1162.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and you’re still trying to oversee it, you’re \nstill trying to understand what’s going on.",
      "offset": 1167.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "But it might be that the AI can run what I \nwould call a “rogue deployment” — where they can  ",
      "offset": 1170.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "basically use a bunch of the compute in an \nunmonitored way, because they’re writing so  ",
      "offset": 1177.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "much of the stuff, so much is happening so \nquickly that they can evade your safeguards  ",
      "offset": 1180.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "if they were to be misaligned. And then \nit might be the case that huge fractions  ",
      "offset": 1184.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of your compute and effort are being used \nfor things that aren’t what you wanted.",
      "offset": 1189.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "In addition to that, these AIs could be \ndoing things like backdooring the future  ",
      "offset": 1196.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "AI training runs, making sure the AIs are \naligned to them instead of aligned to you,  ",
      "offset": 1199.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and doing a huge amount of work that \nyou might just not know what’s going  ",
      "offset": 1203.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "on anymore. Even at the most basic level of \nwhat code was run, you might just be misled  ",
      "offset": 1208.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "about that specific thing because things are \ngoing so fast and so out of your control.",
      "offset": 1214,
      "duration": 4.068
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. If people are willing to \ngrant that we’ve got billions of fast superhuman  ",
      "offset": 1218.068,
      "duration": 4.972
    },
    {
      "lang": "en",
      "text": "geniuses running on this data centre and barely \nany human oversight — because the sheer quantity  ",
      "offset": 1223.04,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "of work being done would be beyond the capacity \nof all of the staff at one of these companies  ",
      "offset": 1230.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to scrutinise, and even if they did have time to \nread it, they often wouldn’t understand it because  ",
      "offset": 1234.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it would quickly be exceeding the kinds of things \nthat they’ve learned to even be able to track — I  ",
      "offset": 1237.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "think people would intuitively believe that at \nthat point, if the AIs were wanting to take over,  ",
      "offset": 1244.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that they would probably have quite a \nlot of options for doing that. Oh, no?",
      "offset": 1250.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Actually a big source of \ndisagreement is how much does cognitive labour  ",
      "offset": 1254.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "assist in takeover? I think there’s \na bunch of different routes. Man,  ",
      "offset": 1258.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "I should have brought my takeover \nplans list, my hot tips for AIs! But  ",
      "offset": 1265.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I think once the AIs are very superhuman, \nthere’s basically a route that I would  ",
      "offset": 1272.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "call the crazy nanotech route, or the crazy \nsuperhuman technologies route — where they  ",
      "offset": 1276,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just have hard military power very quickly \nvia the mechanism of being very superhuman.",
      "offset": 1280.72,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "So maybe you let them make nanotech, or maybe \nyou’re like “No, don’t make nanotech. That would  ",
      "offset": 1287.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "be super scary.” But they can do it even despite \nyou not wanting them to do that — because they  ",
      "offset": 1291.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "can use huge fractions of the compute you have \nin your data centre to work on the nanotech,  ",
      "offset": 1295.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and also exfiltrate themselves, coordinate with an \noutside version of the model, orchestrate wet lab  ",
      "offset": 1302.08,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "experiments to bootstrap nanotech from all kinds \nof things. And then you’re just totally cooked.",
      "offset": 1308.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So a possible layer of defence \nwould be, although there’s this enormous  ",
      "offset": 1314.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "capability here, if we’re able to \ndirect exactly what they’re doing,  ",
      "offset": 1318.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "such that they never get a spare moment \nto think about how they would take over  ",
      "offset": 1321.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "or to actually try to implement that, \nthen that might possibly keep us safe.",
      "offset": 1325.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "However, in this scenario — where we’re able \nto scrutinise so little, and we have so little  ",
      "offset": 1330.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "understanding of how these models operate — it \nseems likely that there would be some slippage,  ",
      "offset": 1334.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "where they might be able to figure out a way of \ngrabbing a bit of time, grabbing a bit of compute  ",
      "offset": 1338.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "here and there, and then pushing that open to use \nmore and more of the compute in the data centre  ",
      "offset": 1343.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to be pursuing their own agenda, rather than \nthe task that we think that we’re giving them.",
      "offset": 1348.32,
      "duration": 5.023
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. I’m pretty sceptical \nof being able to maintain a meaningful level  ",
      "offset": 1353.343,
      "duration": 4.417
    },
    {
      "lang": "en",
      "text": "of control if all of your AIs are coherently \nmisaligned and we’re at this level of capability.  ",
      "offset": 1357.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "By control, I mean things such that the AIs \ncouldn’t do bad stuff even if they wanted to.",
      "offset": 1365.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I think a somewhat slowed-down version of the \ninitial automation seems very plausible with  ",
      "offset": 1373.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "control, maybe even at full speed, and maybe \nyou can go even somewhat above the human range  ",
      "offset": 1378.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "while ensuring control using clever strategies — \nincluding having the AIs work on the strategies,  ",
      "offset": 1382.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and making sure their work on the \nstrategies itself is not sabotaged.",
      "offset": 1387.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "But it seems very difficult once you’re in \nthis completely insane regime, especially if  ",
      "offset": 1390.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you want to use the AIs to develop things like \na cure for cancer where you don’t understand  ",
      "offset": 1395.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "what they’re doing. They’re routing through \nbiological mechanisms you don’t understand.  ",
      "offset": 1399.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "They’re orchestrating humans who don’t know \nwhat’s going on in a wet lab. I think it’s  ",
      "offset": 1402.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "very plausible that for various reasons you’ll \nwant to have AIs directing people to do wet lab  ",
      "offset": 1406.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "experiments where you do not know what the \nfuck is going on. The AI explains something,  ",
      "offset": 1411.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and you can maybe do some checks, but you’re \nvery confused. So maintaining control of that.",
      "offset": 1416.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Maybe I’ll try to quickly run through \nthe takeover list. There’s the crazy  ",
      "offset": 1420.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "superhuman technology, which I think is \npretty plausible, for what it’s worth.",
      "offset": 1425.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "There’s what I would call superhuman \nstrategy/manipulation, including things  ",
      "offset": 1429.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "like persuasion that is getting people to \ndo things that are wildly against their  ",
      "offset": 1435.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "interests very quickly. I think this \nis not clear if this is ever possible,  ",
      "offset": 1440,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "but if it is, it does feel like it \nwould make takeover a lot easier.",
      "offset": 1442.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Then there’s a bunch of stuff that might be \nmore possible even around the human-level  ",
      "offset": 1445.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "regime. An example would be the AI escapes, \nit maybe coordinates with a copy inside,  ",
      "offset": 1448.56,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "but it also is running some independent presence \nexternally. Maybe that’s like running on the  ",
      "offset": 1454.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "GPUs internally, but it’s running code \non servers outside of the AI company.",
      "offset": 1458.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And as part of doing that, it sort of builds \nan independent industrial base, so it has like  ",
      "offset": 1462.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "all the stuff it would need to be able to recover \nindustrial civilisation and then just kills almost  ",
      "offset": 1467.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "all humans. So something like deploys bioweapons, \ndeploys mirror bacteria, deploys mirror mould,  ",
      "offset": 1472.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "whatever, and that ends up relatively \nquickly killing a huge fraction of humans.",
      "offset": 1477.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I think it seems like this is technically \nfeasible. And then from there the question is,  ",
      "offset": 1482.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "did the AI screw itself over? \nCan it maintain what it’s doing,  ",
      "offset": 1488,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "or is it just going to decay into ruin because \nthere’s no human hands to build all the things?  ",
      "offset": 1493.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So I think it needs to have some independent \nindustrial base that could either be via having  ",
      "offset": 1498.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "some human loyalists, some like humans that it’s \nenslaved and keeps alive, or via having robots.",
      "offset": 1502.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: At this point in time,  ",
      "offset": 1507.84,
      "duration": 1.2
    },
    {
      "lang": "en",
      "text": "wouldn’t there be just incredible robots \nand probably quite a large number of them?",
      "offset": 1509.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Potentially very quickly. I \nhaven’t done all the analysis of how quickly do  ",
      "offset": 1513.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you expect lots of robots. We also have to answer \nthe question of how many robots get destroyed  ",
      "offset": 1517.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "as the humans are starting to get killed by \nbioweapons and maybe suspect that it’s AI caused,  ",
      "offset": 1521.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and questions like, if there’s surviving humans, \nhow much military force is needed to handle that?",
      "offset": 1527.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "I think it’s non-obvious how the situation goes,  ",
      "offset": 1532.72,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "but this is a route for why you could get \ntakeover without needing very superhuman  ",
      "offset": 1535.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "capabilities. The thing I’m describing seems \nlike it would in principle be possible for  ",
      "offset": 1539.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "AIs to do if they’re merely fast at \nhuman level and super well coordinated.",
      "offset": 1542.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Reasons why this is hard for humans to pull off: \none, humans don’t want to do this. Another reason  ",
      "offset": 1547.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is that I think it’s the case that it’s \nhard for humans to run vast conspiracies,  ",
      "offset": 1552.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "but it might be much easier for the AIs to \nrun vast conspiracies because they don’t have  ",
      "offset": 1557.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to rely on recruiting potentially untrustworthy \nhumans. They can potentially be wildly better at  ",
      "offset": 1561.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "cybersecurity and much more meticulous. They might \nmess it up, but I just think there’s mechanisms  ",
      "offset": 1566.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "via which this could happen. So this is the “kill \neverybody via independent industrial base” story.",
      "offset": 1571.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Another story, which I think maybe \nis the most plausible story of all,  ",
      "offset": 1576.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "is the “humans give the AIs everything they \nneed” story — which is like the AIs just chill,  ",
      "offset": 1579.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "they make sure they have control of the \nsituation; maybe they’re manipulating  ",
      "offset": 1585.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "what’s going on. I talked earlier about how what \nyou see is not what’s actually going on. Like,  ",
      "offset": 1588.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you look at the experiments, they’re not what you \nexpect. The AIs are doing that. They’re sabotaging  ",
      "offset": 1593.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the alignment experiments. They’re sabotaging \nthe alignment results. They’re deluding us.",
      "offset": 1597.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "There’s a bunch of mechanisms for that, but they \ndon’t do anything very aggressive. You know,  ",
      "offset": 1602,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "they’re just chilling. We’re scaling up compute. \nThey do lots of good stuff. There’s cures to all  ",
      "offset": 1605.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "diseases, all kinds of great stuff is happening, \nindustrial development. People are like,  ",
      "offset": 1611.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "“It’s so great that AI did go well and \nwe didn’t have misalignment.” Some of  ",
      "offset": 1616.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the safety people are worryingly looking at the \nsituation, wondering if this is what’s going on.",
      "offset": 1620.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "And then at some later point, when the AI has an \ninsanely decisive advantage, and the humans are  ",
      "offset": 1625.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "starting to be an obstacle at all — which, \nthroughout this story, they might not be,  ",
      "offset": 1630.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "right? I think if the humans ended up being an \nobstacle earlier, maybe the AIs would take more  ",
      "offset": 1633.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "decisive action. But if there’s no need for \ndecisive action, maybe they just lie in wait.",
      "offset": 1638.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "And then at a point when there’s truly vast \nlevels of industry, truly vast levels of robots,  ",
      "offset": 1644.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the armies are entirely run by AIs, \nthe situation is so beyond whatever,  ",
      "offset": 1649.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "maybe even at the point of space probes \nbeing launched… It could be potentially  ",
      "offset": 1654.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that humans are deluded indefinitely. It \nmight be just like you can in principle  ",
      "offset": 1658.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Potemkin village the entire way through. But \nit could also convert at an earlier point.",
      "offset": 1661.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Now, to be clear, if humans \nare deluded indefinitely,  ",
      "offset": 1667.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "then it’s like all the stuff \non Earth might be great, right?",
      "offset": 1670.64,
      "duration": 2.388
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So you’re saying they \ncould even get to the point of saying,  ",
      "offset": 1673.028,
      "duration": 4.412
    },
    {
      "lang": "en",
      "text": "“We’re going to go off and settle space \nand do a whole lot of stuff in space,”  ",
      "offset": 1677.44,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "and they’ll tell us that they’ll do \none thing, but they’re going to do a  ",
      "offset": 1679.84,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "completely different thing while we just enjoy \nour lives on Earth, thinking that things have  ",
      "offset": 1681.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "gone better or at least very differently \nthan in fact how they’re going to play out?",
      "offset": 1685.2,
      "duration": 3.823
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. But to be clear, it could \nbe Potemkin villageised on Earth as well. It could  ",
      "offset": 1689.023,
      "duration": 4.817
    },
    {
      "lang": "en",
      "text": "be the case that you’re like, “Wow, I have so \nmany happy friends,” whatever. But steadily  ",
      "offset": 1693.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "your friends are being replaced by robots. \nAnd at some point it’s like, why doesn’t the  ",
      "offset": 1698.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "AI just suddenly take over? But in principle, \nit could go very far. So that’s another story.",
      "offset": 1704,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Another story I would call “sudden \nrobot coup.” This also doesn’t  ",
      "offset": 1709.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "require very superhuman capabilities. \nWe build vast autonomous robot armies.  ",
      "offset": 1713.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Maybe we think we have some way to turn them \noff. Either the AIs have sabotaged that, or  ",
      "offset": 1717.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "they have some way to work around that, or there’s \nvarious other mechanisms by which this could fail.",
      "offset": 1721.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "And then we built the robot armies, we saw the \nAIs building the robot armies, and we’re like,  ",
      "offset": 1726.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "“Hooray, we’re building a vast robot army \nto compete with China!” And China’s like,  ",
      "offset": 1730.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "“Hooray, we’re building a vast robot \narmy to compete with the US! Truly,  ",
      "offset": 1733.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "it’s great that we have such a big robot army.” \nAnd then it’s like, whoops. All of a sudden the  ",
      "offset": 1735.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "robot armies sweep in and do a relatively hard \npower takeover, because they control the military,  ",
      "offset": 1740.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "they can be hyper coordinated, and \nmaybe they’re in parallel using  ",
      "offset": 1744.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "things like potentially bioweapons to some \nextent, but that might not even be required.",
      "offset": 1748.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I mean, at this point they \ndon’t even need to kill us, potentially.",
      "offset": 1752.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Potentially.",
      "offset": 1755.04,
      "duration": 0.396
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because even if we all tried to \nresist, it would be pointless, basically.",
      "offset": 1755.436,
      "duration": 2.627
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Potentially. And also \nthings like they maybe are using lots  ",
      "offset": 1758.063,
      "duration": 4.897
    },
    {
      "lang": "en",
      "text": "of cyberattacks to destabilise human \nresponse. So they could potentially  ",
      "offset": 1762.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "do this at a point when we naively think that \nthe robot army is weaker than the human army.",
      "offset": 1765.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Or even we think we have shutoff switches, \nbut the shutoff switches are not reliable  ",
      "offset": 1770.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "enough. They’re not over-engineered enough, \nthey’re not shielded enough from the AI’s  ",
      "offset": 1774.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tampering. I think it’s an awkward dynamic, \nwhere if you want to have remote shutoff,  ",
      "offset": 1778.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you have to both make it so that it’s tamper-proof \nfor AIs that might be in principle very general  ",
      "offset": 1782.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and very smart and very clever, and also, it needs \nto be the case that they’re remotely triggerable.  ",
      "offset": 1788.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "So it can’t be shieldable either. It has to be the \ncase that there’s no way the AIs can shield the  ",
      "offset": 1795.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "thing from signals and avoid tampering with it — \nwhich is a very awkward combination in principle.",
      "offset": 1799.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: What determines whether you get AIs \nattempting to take over early relative to later?  ",
      "offset": 1804.48,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "I guess the reason to wait is that they’re going \nto become a bigger fraction of the economy;  ",
      "offset": 1811.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "they’ll have more hard power, so they’re just \nmore likely to trivially succeed. The reason to  ",
      "offset": 1815.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "go early is that a given set of models might \nexpect to be replaced by other models that  ",
      "offset": 1820.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "might not share their goals, so they would have \nmissed their opportunity to take over, basically.",
      "offset": 1825.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I think the strongest \nreasons to go early might be something like:  ",
      "offset": 1830.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "worries about other AIs; worries about humans \ngetting their shit together — either because we  ",
      "offset": 1836,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "get enough work out of the AIs, or because \npeople freak out and trigger some strong  ",
      "offset": 1840.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "response that could recover the situation — and \nthe third one would be impatience or something.",
      "offset": 1845.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "It might be that, by default, humans slow \nprogress down a bunch, even with AIs trying  ",
      "offset": 1849.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to manipulate things, trying to steer things \nfrom behind the scenes. And if the humans are  ",
      "offset": 1854.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "slowing things down a bunch, and the AIs are \nlike, “No, I want to get to the stars now,”  ",
      "offset": 1858.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and they’re at some sufficiently high takeover \nprobability… Or maybe the AIs are just like,  ",
      "offset": 1862.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "“I just want to do it now.” Like, I know \nhumans sometimes want things to happen  ",
      "offset": 1867.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "faster, even independent of the total \namount of stuff they end up getting.",
      "offset": 1870.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "So these are some reasons to happen earlier. These \nthings also apply to things less egregious than  ",
      "offset": 1874.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "takeover. One of my hopes for things going well \nis we can put the AIs in a position where they’re  ",
      "offset": 1879.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "forced to either take aggressive moves early or \nwe get enough work out of them. And if they take  ",
      "offset": 1885.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "aggressive moves early, then maybe we can catch \nthat and then build a case for danger. And in  ",
      "offset": 1890,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "addition to building a case for danger, we could \npotentially do stuff like study that example,  ",
      "offset": 1894,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "iterate against it — stuff I’m calling, \n“few-shot catastrophe prevention.” Like,  ",
      "offset": 1898.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we’ve caught a few examples of \nbad stuff: can we handle that?",
      "offset": 1901.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "If at multiple different levels, are the AIs \nsort of forced to take aggressive action? Or  ",
      "offset": 1904.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is it instead the case that the AIs can bide \ntheir time, because we don’t have any mechanism  ",
      "offset": 1910,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for exiting the regime using some ongoing \nbackground trend — either safety research  ",
      "offset": 1914.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "improving over time, or humanity being \nlike, “I’m sorry, what is going on here?”",
      "offset": 1919.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Are you saying a general thrust \nof the interventions that we could make  ",
      "offset": 1925.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is creating a trend where over time in fact \nthey’re going to become less and less likely  ",
      "offset": 1929.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to be able to successfully take over? Maybe \nbecause we’re successfully figuring out how  ",
      "offset": 1934.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to use AI in order to prove alignment \nand safety and to detect misalignment,  ",
      "offset": 1937.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "or we’re improving our control mechanisms. So that \nwould then force the models: they see this trend,  ",
      "offset": 1942.96,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "they see that in fact the window is closing, \nand so they have to try to strike early. And  ",
      "offset": 1950.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that gives us a chance to realise what’s \ngoing on and then actually win out.",
      "offset": 1954.16,
      "duration": 4.623
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: That’s right. But of course \nit could obviously go in the opposite direction  ",
      "offset": 1958.783,
      "duration": 3.137
    },
    {
      "lang": "en",
      "text": "where our measures can’t keep up with the rate \nof capabilities, and it keeps going up. There’s  ",
      "offset": 1961.92,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "this notion of what’s the level of capability that \nwe can handle at a given level of takeover risk?  ",
      "offset": 1969.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Maybe initially we don’t have that much going on, \nso we can handle roughly humanish level at some  ",
      "offset": 1976.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "20% takeover risk, or some very high level — which \nwill depend on various facts about misalignment:  ",
      "offset": 1982.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "which AIs are misaligned, how coherent are they, \nhow much are they unified, this sort of thing.",
      "offset": 1988.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "And then in addition to that, there’s a \nquestion of, if progress is fast enough,  ",
      "offset": 1991.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it might be that takeover risk increases over \ntime. Because as the models are getting more  ",
      "offset": 1995.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "capable, they’re more and more likely to succeed \nand more likely to be misaligned, potentially.",
      "offset": 2000,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "But also maybe we can push \nthe curve the other direction,  ",
      "offset": 2004.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "where we can make it so that the \nrange of capabilities that we can  ",
      "offset": 2008.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "hold in the n% takeover chance of \nsuccess at takeover regime is lower.",
      "offset": 2011.92,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, explain \nthat last bit. I don’t get it.",
      "offset": 2017.24,
      "duration": 2.36
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Imagine that there’s \nsome trend over time, which is:  ",
      "offset": 2019.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "for the smartest AIs we have now, what’s the \nchance of success they have? Or maybe just  ",
      "offset": 2023.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "how likely are they to take over? So initially \nour countermeasures suffice, to be like, maybe  ",
      "offset": 2028.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "at the point where they’re automating the whole \nAI company, by default, we’re at instantaneously  ",
      "offset": 2033.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "probably not even that high a chance of \ntakeover, because what are they going to do?",
      "offset": 2039.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "But then, as they get more capable, the \ninstantaneous takeover chance is going up,  ",
      "offset": 2043.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and some of our paths to success \nhave been cut off. Maybe the AIs  ",
      "offset": 2047.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "have escaped, they’re running rogue internal \ndeployments, they’re sabotaging our work.",
      "offset": 2051.573,
      "duration": 3.387
    },
    {
      "lang": "en",
      "text": "But we could make it so that the trend \nbows the other way — where over time  ",
      "offset": 2054.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the risk of takeover is going down, not up \n— because the capabilities are increasing,  ",
      "offset": 2060,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but our measures are increasing faster \nthan the capabilities. But obviously,  ",
      "offset": 2064.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "as we discussed before, the capabilities might \nbe increasing very fast. So if the measures  ",
      "offset": 2067.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "aren’t keeping up with that, if we’re not on \nsome stable regime, then that could be bad.",
      "offset": 2072.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And so I think the model I was using there is \nmost natural to think about in a regime where the  ",
      "offset": 2076.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "AIs are misaligned. But if we start at a point \nwhere they’re aligned enough that they sort of  ",
      "offset": 2080.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "maintain their own alignment, then it could \nbe that the takeover risk initially is like,  ",
      "offset": 2085.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they’re actually aligned, so we’re sort \nof in a stable attractor of alignment.",
      "offset": 2089.76,
      "duration": 3.748
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK. What effects does this picture \nhave on what your research priorities are,  ",
      "offset": 2093.508,
      "duration": 7.612
    },
    {
      "lang": "en",
      "text": "and what you think the broader AI \necosystem ought to be prioritising  ",
      "offset": 2101.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in terms of making things more likely \nto go well and less likely to go poorly?",
      "offset": 2105.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: For one thing, I’ve \nbeen talking about the timelines in  ",
      "offset": 2108.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the absence of very active intervention. \nThere’s also a question of how much do  ",
      "offset": 2112.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you pause deliberately at these \nrelevant capability milestones?",
      "offset": 2116.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I feel much more optimistic to the extent that \nwe end up pausing at a level of AI capability  ",
      "offset": 2120.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "which is around the point where you can fully \nautomate the AI company — maybe somewhat before,  ",
      "offset": 2125.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "maybe a bit after, maybe just right \naround there — for an extended period,  ",
      "offset": 2129.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "both so humans have time to study these \nsystems and also so that we have time to  ",
      "offset": 2132.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "extract a bunch of labour out of these systems \nand reduce the ongoing takeover chance. And so  ",
      "offset": 2136.24,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "there’s some question of, there’s all these \nworlds where things are slower than this.",
      "offset": 2144.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Then another source of hope is maybe you’re \njust kicking off this crazy regime with an  ",
      "offset": 2148.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "actually aligned AI: with an AI that’s not just \nnot conspiring against us, but even more strongly,  ",
      "offset": 2152.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it’s actively looking out for us, actively \nconsidering ways things might go wrong, and is  ",
      "offset": 2158.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "trying to keep us in control. This has been called \na basin of corrigibility or a basin of alignment.",
      "offset": 2163.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "So the things I’m excited for depend a bit \non the regime. I think there’s a bunch of  ",
      "offset": 2170.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "different regimes we can consider. One is a regime \nwhere, maybe especially in these short timelines,  ",
      "offset": 2175.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I expect not that much lead time between \ndifferent actors, at least in the US side.",
      "offset": 2180.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You’re saying there’ll be multiple  ",
      "offset": 2185.92,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "companies all approaching this point \nof automation roughly simultaneously.",
      "offset": 2187.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: That’s right. So I think a pretty \nnatural scenario to consider that I often think  ",
      "offset": 2190.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "about is a relatively (from my perspective) \nirresponsible company, basically their default  ",
      "offset": 2193.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "plan is scale the superintelligence as quickly \nas possible, don’t take safety concerns very  ",
      "offset": 2198.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "seriously. And that’s their explicit plan, that’s \ntheir internal thing and they’re just pursuing  ",
      "offset": 2203.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that as quickly as possible. Three months in the \nlead, let’s say. They’re maybe ahead of Chinese  ",
      "offset": 2207.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "companies and maybe some more responsible actors, \nin some mix of three to nine months behind.",
      "offset": 2213.12,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "I think in this scenario, a lot of the action is \ngoing to come from a relatively small number of  ",
      "offset": 2219.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "people inside that company. And then potentially \nthere’s a bunch of action that can come from  ",
      "offset": 2223.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "people on the outside who are enacting \nvarious influence via doing exportable  ",
      "offset": 2229.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "research and also via potentially \nchanging the policy situation. Also,  ",
      "offset": 2233.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "I think there’s definitely stuff that can \nbe done via trailing AI developers who are  ",
      "offset": 2239.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "more responsible and are putting more effort \ninto preventing bad outcomes in this case.",
      "offset": 2243.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: In that scenario, I guess \none thing that could be productive is  ",
      "offset": 2247.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that there are people inside the company \nwho say, “What we’re doing seems a little  ",
      "offset": 2250.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "bit scary. We should have better control \nmechanisms, we should have better alignment  ",
      "offset": 2255.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "mechanisms. We should take that a bit more \nseriously than our current default plan.”",
      "offset": 2258.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Another one is the people working at the more \nresponsible organisations could produce research  ",
      "offset": 2262.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that makes that easier for the company to do, \nmakes it less costly for them to have stronger  ",
      "offset": 2266.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "control measures. And then there’s also, of \ncourse, you could have a governance response:  ",
      "offset": 2269.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "maybe this will begin to take off, \nand people will be quite alarmed,  ",
      "offset": 2273.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and perhaps you’ll get broad-based support \nfor the “pause at human level” notion.",
      "offset": 2277.44,
      "duration": 5.263
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. Man, I really \nwish this notion was more memeable. An  ",
      "offset": 2282.703,
      "duration": 3.537
    },
    {
      "lang": "en",
      "text": "unfortunate thing about “pause at human level” is  ",
      "offset": 2286.24,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "all of those words are complicated and \nconfusing. And it’s just not even that,  ",
      "offset": 2288.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it’s not even that nice of a slogan. Someone \nshould think of a better slogan for this thing.",
      "offset": 2292.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "But anyway, yeah, I think there’s a bunch \nof different mechanisms for doing exportable  ",
      "offset": 2296.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "research, trying to wake up the world. There’s \nmore and less indirect things. So you can do very  ",
      "offset": 2302.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "direct policy influence, versus things more like \ndemonstrating the level of capabilities — and when  ",
      "offset": 2305.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you’re demonstrating the level of capabilities, \nthat could trigger a response from the world.",
      "offset": 2309.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So I think a lot of my hope in this sort \nof scenario is going to come especially  ",
      "offset": 2313.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in very short timelines, which I think do \nlook worse than longer timelines. Part of  ",
      "offset": 2317.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "my picture of less doom is like, it might be \neight years away instead of four years away.",
      "offset": 2322,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So in the four-year timelines, I think a mechanism \nthat we have is that the leading AI company,  ",
      "offset": 2327.92,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "we have some people who are working there \nand their goal is both prevent egregious  ",
      "offset": 2334.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "misalignment — by “egregious misalignment” \nI mean something like AI systems that are  ",
      "offset": 2338,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "deliberately engaging in subterfuge, deliberately \nundermining our tests, deliberately trying to sort  ",
      "offset": 2343.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of make our understanding of the situation \nmisleading — and then second, we can try to  ",
      "offset": 2348.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "have it so that the properties of the AI system \nare ones where we’re happy to hand things over,  ",
      "offset": 2353.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and try to aim for that kind of quickly, \nbecause we’re in a very rushed timeline.",
      "offset": 2357.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "So some of this preventing egregious \nmisalignment, or dealing with it,  ",
      "offset": 2361.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I think is in some sense it’s \nthe traditional thrust. But I  ",
      "offset": 2366.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "think there’s a bunch of stuff related to \nhandover and the AIs being wise enough,  ",
      "offset": 2369.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the AIs sort of staying faithful to our \ninterests on long horizon things, making  ",
      "offset": 2373.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sure we even just have the tests to know whether \nthis is true. These sorts of considerations.",
      "offset": 2378,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And there’s a bunch of different incremental \nsteps here. Phase zero is maybe like you’re using  ",
      "offset": 2383.36,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "mechanisms to make it so that AIs are producing \nwork for you that you think isn’t sabotaged,  ",
      "offset": 2392.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "even if those AIs were trying to sabotage it, \nand then you use this to milk a bunch of work  ",
      "offset": 2395.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "out of your AI systems. Phase one is you try \nto create an AI system that the safety people  ",
      "offset": 2399.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "are happy to hand off to, to the best extent you \ncan. And then phase two is you try to make it so  ",
      "offset": 2404.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that the AI running the organisation overall is \nan AI system that you’re happy to hand off to,  ",
      "offset": 2410.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and presumably other people are \nhappy to hand off to as well.",
      "offset": 2414.72,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "This is like a caricature of the situation, or \na simplification, because probably at a bunch of  ",
      "offset": 2418,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "points you want to be doing stuff like picking up \nlow-hanging fruit, demonstrating capabilities to  ",
      "offset": 2422.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the world, trying to consider whether something \ncan happen, trying to make it so the main AI  ",
      "offset": 2426.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "system that the organisation is training on the \nfrontier of current capabilities is less likely  ",
      "offset": 2432.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to cause problems. I’m not tremendously hopeful \nabout the situation, but I think there’s a bunch  ",
      "offset": 2437.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of different potential outs that give us some \nhope, or some saving throws, some win conditions.",
      "offset": 2442.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Things like, we catch the AIs early on is another \nhope. Maybe we catch the AIs early on, maybe that  ",
      "offset": 2448.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "convinces the world to take the situation more \nseriously. And maybe even if not, it convinces  ",
      "offset": 2453.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "company leadership that their own power is \nunder threat from the AIs. So even if they’re  ",
      "offset": 2457.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "very Machiavellian, I think the AI being a threat \nto their power, they should be worried about it.",
      "offset": 2463.28,
      "duration": 5.668
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: They might switch on.",
      "offset": 2468.948,
      "duration": 0.315
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. So that could potentially \nincrease the amount of resources spent on safety.",
      "offset": 2469.263,
      "duration": 4.245
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, so the primary story there \nwas trying to figure out a safe scenario in  ",
      "offset": 2473.508,
      "duration": 5.452
    },
    {
      "lang": "en",
      "text": "which you could hand over these processes to \nAIs that you trust. How would you do that?",
      "offset": 2478.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I guess you start with models that are \naround human level that you feel like  ",
      "offset": 2484.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you have a reasonable grasp on, \nthat probably are aligned. Like,  ",
      "offset": 2488.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "we currently think that Claude probably \nis roughly about as friendly as it seems;  ",
      "offset": 2491.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "it probably isn’t engaging in massive \nscheming against us at every turn.",
      "offset": 2498,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "You figure out clever mechanisms by which \nyou can get those kinds of models that you  ",
      "offset": 2503.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "feel OK about to do a whole bunch \nof work on control and alignment.  ",
      "offset": 2507.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "You rush through that before \nyou have had other capabilities,  ",
      "offset": 2512.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "improvements that have been allowed to happen. And \nthen you get to the point where hopefully this has  ",
      "offset": 2516.8,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "panned out that we feel OK handing off to these \nmodels that have been trained by the previous  ",
      "offset": 2523.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "generation that we kind of understood — and then \nwe’re going to cross our fingers? Is that it?",
      "offset": 2527.44,
      "duration": 3.983
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. I should be clear that \nthis is a very low political will plan. I have  ",
      "offset": 2531.423,
      "duration": 4.417
    },
    {
      "lang": "en",
      "text": "a menu of plans, and this is like the bottom \nof the barrel or whatever. So the way I would  ",
      "offset": 2535.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "have put this is like we had some early point \nwhere we had AIs that could automate a bunch of  ",
      "offset": 2541.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "stuff and we didn’t necessarily trust. We \nharness a bunch of labour from these AIs,  ",
      "offset": 2545.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "then we try to make a version of those AIs \nthat we do trust. And not just trust at the  ",
      "offset": 2548.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "level of they’re not conspiring against us; \nideally trust at the level of we’re happy to  ",
      "offset": 2552.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "hand off messy open-ended questions — like, “What \nshould our strategy be for minimising risks from  ",
      "offset": 2555.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the following things?” Including potentially \nsomewhat tricky, more philosophical topics.",
      "offset": 2560.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And then from there, we try to hand \noff to those systems and hope those  ",
      "offset": 2566.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "systems stay aligned, stay on track, \nare able. And also succeed themselves,  ",
      "offset": 2569.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "right? It could be that we hand over \nto systems and those systems are like,  ",
      "offset": 2573.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "“Wow, you guys seem like you’re in a pretty \nbad situation. I’m not sure that I can pull  ",
      "offset": 2576,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this one out of the bag.” There’s a variety \nof mechanisms by which that could occur.",
      "offset": 2579.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think you’re saying that \nthey could also mess up the next stage?",
      "offset": 2584.44,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Especially \nbecause they have very little time.",
      "offset": 2587.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because they’re also racing.",
      "offset": 2589.84,
      "duration": 1.52
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, yeah. Oh, for \nsure. So there’s an internal thing,  ",
      "offset": 2591.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "where if you’re the safety team, maybe you \nhave a version of it that you fine-tuned,  ",
      "offset": 2596.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that’s human level, that you trust, \nthat you’re happy to hand off to.  ",
      "offset": 2600.4,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "But capabilities progress is so fast \nthat maybe the cutting edge of the  ",
      "offset": 2602.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "current company is very superhuman. But you \ndon’t even want to work with that system.",
      "offset": 2606.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I think the first handover point you should try \nto do closer to the human regime — maybe slightly  ",
      "offset": 2610.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "superhuman or around the top human level of \ncapability — because the more superhuman,  ",
      "offset": 2614.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the more likely it is to be misaligned, \nso you want to start the handoff earlier.  ",
      "offset": 2619.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "But that might not stop the rest of the \norganisation from training more capable  ",
      "offset": 2622.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "AIs. So it might be that you as the safety \nteam are handing off to an AI that’s less  ",
      "offset": 2625.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "capable than the frontier of the organisation, \nand that AI needs to quickly bootstrap so the  ",
      "offset": 2629.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "trend of the handoff AIs intersects \nwith the capabilities fast enough.",
      "offset": 2634.24,
      "duration": 3.668
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. At the point that you \nhave an aligned, somewhat superhuman model,  ",
      "offset": 2637.908,
      "duration": 4.812
    },
    {
      "lang": "en",
      "text": "is there some chance that it would be \nable to persuade a broader coalition  ",
      "offset": 2642.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "of people that we need to take \nthis a little bit more seriously,  ",
      "offset": 2645.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "and it shouldn’t be forced to race against \nanother team at the same organisation?",
      "offset": 2648,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. I mean, that’s definitely \npart of the hope. Definitely part of the hope is  ",
      "offset": 2651.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that I could demonstrate empirical evidence. \nI think also, if we’re in a situation where  ",
      "offset": 2655.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we have two AIs trained by different teams, \nand both of those AIs are just throwing huge  ",
      "offset": 2660.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "amounts of shade on each other — where one \nAI is like, “That AI is obviously scheming;  ",
      "offset": 2665.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "look at all these examples,” and the other AI is \nlike, “That AI, obviously conspiring against you;  ",
      "offset": 2669.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "look at all these examples. It’s so biased. It’s \nso unreasonable. It’s clearly just a plant” — I’m  ",
      "offset": 2673.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like, OK, that’s a crazy situation. Certainly you \ncan’t come out of this regime thinking it’s fine.",
      "offset": 2677.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: One way or another, \nit’s definitely not reassuring.",
      "offset": 2683.84,
      "duration": 3.823
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: It’s not reassuring. There might \nbe ways to downplay it. So one regime is like,  ",
      "offset": 2687.663,
      "duration": 4.337
    },
    {
      "lang": "en",
      "text": "I think that’s kind of plausible \nis the safety team trains their AI,  ",
      "offset": 2692,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "tries to go to company leadership and be like, \n“The situation seems very scary. You’re training  ",
      "offset": 2695.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this AI. I think it’s misaligned. Here’s a \nbunch of evidence for that.” And the other  ",
      "offset": 2700.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "AI comes in and is like, “As we all know, that AI \nwas trained by the safety team. I think it’s just  ",
      "offset": 2703.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "very paranoid about misalignment. Of course, \nthe safety team would want us to think all the  ",
      "offset": 2708.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "AIs are misaligned. I’m not misaligned. That AI \nis not misaligned. It just has poor judgement.”",
      "offset": 2711.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "A thing that’s interesting about \nthis is that, yes, it’s concerning,  ",
      "offset": 2716.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "but it’s not like you know at least one of \nthe two AIs is misaligned. So in general,  ",
      "offset": 2719.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I think a concern with the “improve the epistemics \nat the AI company” route is that that the people  ",
      "offset": 2724.32,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "who trained the AI plausibly have a reputation \nfor being biassed on these matters relative to  ",
      "offset": 2732.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the company from the company’s perspective, \nwhich I think poses a bunch of concerns.",
      "offset": 2736.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And it might be the case that \nfor every evidence of risk,  ",
      "offset": 2741.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you can make an even more comparably compelling \nevidence that risk actually doesn’t exist.  ",
      "offset": 2746,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Because if we’re in the regime where we don’t \nreally know how to adjudicate these things,  ",
      "offset": 2751.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it could potentially just be \nthat reason doesn’t win out.",
      "offset": 2756.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So that definitely is some of the \nhope. Some of the hope is coming  ",
      "offset": 2761.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "from once you have this handover, then it’s fine.",
      "offset": 2764.72,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Now I think there’s a bunch of ways this \nstory could go wrong. Here’s an example.  ",
      "offset": 2767.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I talked about the alignment team sort of off to \nthe side in their corner, trying to like fine-tune  ",
      "offset": 2770.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "or potentially even maybe train from scratch a \nversion that they’re happy to defer to. Well,  ",
      "offset": 2774.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "one issue is that it might be the case \nthat the main AI system has already taken  ",
      "offset": 2779.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "over the AI company in a cybersecurity \nsense — as in it has a rogue deployment;  ",
      "offset": 2783.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it can use compute in an unauthorised, \nunmonitored manner; it can tamper with  ",
      "offset": 2788.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "jobs everyone is running. So you’re trying to \nalign this AI, but you need to harden yourself  ",
      "offset": 2793.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "against the rest of the company. And I’m like, \noh my god. That seems potentially hopeless.",
      "offset": 2798,
      "duration": 6.068
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Zooming out: at Redwood Research,  ",
      "offset": 2804.068,
      "duration": 3.132
    },
    {
      "lang": "en",
      "text": "you spend a bunch of time gaming out \nthese various different scenarios,  ",
      "offset": 2808.24,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "thinking about what sort of interventions might be \nhelpful in some significant fraction of them, and  ",
      "offset": 2811.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then trying to get more people working on that and \nwork on that yourself. Is that the basic picture?",
      "offset": 2815.28,
      "duration": 5.103
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, broadly.",
      "offset": 2820.383,
      "duration": 1.537
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Recently Redwood has been prioritising \ncontrol as opposed to alignment — saying,  ",
      "offset": 2821.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "let’s assume that the models don’t share \nour interests, that there is some degree  ",
      "offset": 2828.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "of misalignment: how could we make it such that \nthey are nonetheless not capable of screwing us  ",
      "offset": 2831.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "over? People used to talk about putting the AI \nin a box. That doesn’t seem so likely anymore,  ",
      "offset": 2836.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "but I guess there’s other mechanisms that \nyou could use to try to check their work:  ",
      "offset": 2842.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "put models in opposition with one another, \nforcing them to do useful stuff for you.",
      "offset": 2849.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Why focus on control? Is it just that \nthere’s so many different ways that  ",
      "offset": 2855.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this could go that AI control mechanisms seem like  ",
      "offset": 2859.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "they’re useful in a larger fraction \nof the different plausible scenarios?",
      "offset": 2862.64,
      "duration": 2.623
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. I think \nin these incredibly desperate,  ",
      "offset": 2865.263,
      "duration": 1.937
    },
    {
      "lang": "en",
      "text": "very low political will, short timelines — \nultra fast from my perspective, very fast,  ",
      "offset": 2867.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "maybe not as fast as people were speculating \nin the past — I think control looks less good.",
      "offset": 2872.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "There’s a few reasons why I think control is \nstill pretty useful in this world. For one thing,  ",
      "offset": 2876.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "if we can catch the AI early on, that \ncould potentially be very useful.  ",
      "offset": 2880.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Another thing is that getting useful work out of \nthe AIs early on in this scenario might be very  ",
      "offset": 2884.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "important and we might be able to do that prior \nto having totally lost track of the situation.",
      "offset": 2890.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And then I think also potentially you can prevent… \nSo I was talking about the situation where the  ",
      "offset": 2897.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "AIs are basically operating your own data centre. \nYou might be able to use a variety of mechanisms  ",
      "offset": 2902.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to make that much harder, so that it occurs later \nin the capabilities, so you have longer when your  ",
      "offset": 2908.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "experiments are not literally tampered with. So \nthese are why control could be good in this world.",
      "offset": 2912.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I think there’s a more complicated slew \nof interventions this doesn’t cover. I  ",
      "offset": 2917.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "do think that in these very rushed worlds \ncontrol looks less good than in worlds where  ",
      "offset": 2921.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there’s somewhat more time and somewhat more \npolitical will. The original target we were  ",
      "offset": 2925.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "thinking about when we were thinking about \ncontrol was a world where it was more like,  ",
      "offset": 2930,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we have a year of delay — like, safety \nis given a year of budget, basically,  ",
      "offset": 2932.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and we can afford to pause for \na year. We can do stuff there.",
      "offset": 2937.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "And I think control looks much, much better in \nthis regime. Or I don’t know about much better;  ",
      "offset": 2940.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I think the amount of risk reduction \ncontrol gets us is quite a bit better,  ",
      "offset": 2944.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "but these worlds are less likely. \nBut that’s not to say that the risk  ",
      "offset": 2949.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "reduction control gets us in the other worlds \nisn’t much smaller than the alignment stuff.",
      "offset": 2952.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "A different way to put this is: alignment also \nlooks really hard in the worlds where things are  ",
      "offset": 2956.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "going that quickly, and alignment looks less \ntractable to work on in advance. A lot of the  ",
      "offset": 2960.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "stuff that I’m most excited for in alignment \ninvolves doing experiments on powerful AIs,  ",
      "offset": 2965.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and you can only do that relatively late. So \nit might be that in the very rushed scenarios,  ",
      "offset": 2970.24,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "alignment is more of the picture ultimately, \nbut was harder to work on in advance.",
      "offset": 2977.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "These factors make me pretty into the control. \nThere’s also just worlds with more political  ",
      "offset": 2981.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "will where I think control looks a lot better. \nI often think about the control plus “pause  ",
      "offset": 2986.08,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "at human level” plan — which doesn’t require any \nalignment for a sustained period, potentially.  ",
      "offset": 2992.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Because you have your initial year of delay, \nyou control those AIs, and then you bootstrap  ",
      "offset": 2996.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "from there into longer delay — by building \nevidence for risk, ensuring non-proliferation,  ",
      "offset": 3001.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "developing somewhat better safeguards — and \nthen you potentially quite slowly proceed from  ",
      "offset": 3007.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that regime, at least for a while, until you’re \nactually happy to hand off, rather than doing  ",
      "offset": 3011.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "an insanely rushed desperate handoff because \nthat’s the only thing you can afford to do.",
      "offset": 3015.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess that requires some degree of  ",
      "offset": 3018.88,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "coordination between companies and \npossibly between countries as well.",
      "offset": 3020.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. At the time \nwhen I was thinking about this,  ",
      "offset": 3023.76,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "I thought a year of lead was more \nplausible, and I’ve updated against  ",
      "offset": 3026.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "that. I think it could still happen. \nThe gaps could open between companies.",
      "offset": 3028.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I think that the story where you’re working \nheavily to ensure non-proliferation and  ",
      "offset": 3033.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "potentially slowing down progress requires \na substantial level of government buy-in  ",
      "offset": 3039.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "potentially. So it might be the case that you \nhave to get it so that the US government — or  ",
      "offset": 3043.12,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "at the very least, some important governments — \nare pretty much on board with the overall plan.  ",
      "offset": 3050,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Not necessarily for the same reasons, but \nare on board with it for whatever reason.",
      "offset": 3055.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "And I’ve updated against that, so I’m now \nspending more of my time thinking about  ",
      "offset": 3060.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "relatively more desperate situations. But you \nknow, an important factor is in the relatively  ",
      "offset": 3064.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "more desperate situations, maybe it’s like \nwe’re reducing the takeover risk from like 50%  ",
      "offset": 3068.72,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "to 40%. And maybe in the worlds where control \ncould have worked and we had a year of delay,  ",
      "offset": 3076.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we’re reducing the risk from like \nwhat would have been 30% to 5%.",
      "offset": 3080.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So it could be that the total risk reduction is \nsufficiently higher in these worlds that they’re  ",
      "offset": 3084.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "actually more leveraged to work on. Because in \nthe world where we have very little influence,  ",
      "offset": 3088.88,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "because there’s so little will, it might \njust be that it’s less tractable to work on.",
      "offset": 3095.36,
      "duration": 4.788
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. In general you want to work \non medium-tractability scenarios. If things  ",
      "offset": 3100.148,
      "duration": 6.092
    },
    {
      "lang": "en",
      "text": "are too grim then it’s maybe impossible \nto save; if things are going very well,  ",
      "offset": 3106.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "then it’s hard to improve. It’s the middle case \nwhere you’re likely to get the most impact, right?",
      "offset": 3109.76,
      "duration": 5.263
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, for sure. I think a \nmistake that the safety community appears to  ",
      "offset": 3115.023,
      "duration": 3.137
    },
    {
      "lang": "en",
      "text": "have made over the past few years is too much \nfocus on overly optimistic worlds. I think  ",
      "offset": 3118.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "actually focusing on desperate, crazy, YOLO’d, \npessimistic worlds is pretty reasonable, because  ",
      "offset": 3123.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that’s where a lot of the risk lives, so getting \nfrom 50% to 5% is like a lot of the action.",
      "offset": 3129.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "And getting from 5% to 0.5%… I mean, a 5% \nlevel of risk is crazy and unacceptable, and  ",
      "offset": 3134.88,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "we don’t accept our paint companies having a 5% \nchance of massive fatalities, takeover — and all  ",
      "offset": 3141.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "this stuff is just from the takeover; I \nthink there are risks other than takeover  ",
      "offset": 3148.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is like totally unacceptable. But nonetheless, \nthat 45% has to be the bulk of the mass.",
      "offset": 3152.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: To what extent do we just \nhave to hope that there are serious  ",
      "offset": 3158.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "early warning signs? You were talking \nabout catching the AIs red-handed,  ",
      "offset": 3162.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "which is another concept that you and your \ncolleagues have tried to make more salient.",
      "offset": 3165.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "That’s a sort of warning shot, I suppose, where \nyou find the AIs inside the company have been  ",
      "offset": 3170.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sabotaging the data centre, they’ve been doing \nsomething really quite egregious. That might get  ",
      "offset": 3174.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "more buy-in to do substantially more stuff from \neither the government or the leadership of the  ",
      "offset": 3180.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "company. How much more hopeful are things if you \ndo get some sort of early warning shot like that?",
      "offset": 3185.44,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I think it helps a lot. One \nsource of scepticism I have is that it might  ",
      "offset": 3193.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "be the case that I think it’s a smoking gun \nbut the broader world does not. For example,  ",
      "offset": 3199.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the alignment faking work that we \nrecently put out, from my perspective,  ",
      "offset": 3205.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "should be quite a large update for people who \nare very sceptical about this threat model. And  ",
      "offset": 3210.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I think predictably it was a maybe no or only \na very small update for a lot of these people.",
      "offset": 3214.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "To be clear, I don’t think that evidence on \nits own should be a huge update in absolute  ",
      "offset": 3222.8,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "terms. Maybe it doesn’t move people to \nthe same level of risk of misalignment  ",
      "offset": 3230.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that I think, because a lot of that’s based \non conceptual arguments they don’t agree with.",
      "offset": 3235.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "But if I think a bunch of people who are \nlike, “There’s no way in hell this would  ",
      "offset": 3238.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "happen” — 0% chance, basically — should maybe \nmove to like 2% or 3% chance. And that’s a  ",
      "offset": 3242.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "huge difference in vibe space at least, \nand log space or whatever. Vibe space,  ",
      "offset": 3248.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "log space, same space, whatever. \nAnd people didn’t make that move.",
      "offset": 3253.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "So I can totally imagine a case where it’s like we \ncaught the AI trying to escape. Buck has a post on  ",
      "offset": 3259.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this, like, what would happen if we caught the AI \ntrying to escape? And people are like, “You know,  ",
      "offset": 3263.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "sometimes systems do that. They do weird stuff. \nAlso, is that even true? Did you train it in  ",
      "offset": 3268,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "some weird way?” Or, “Oh, that’s what happened \nat that company. But at this company we have  ",
      "offset": 3273.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "different practices.” Also people just being like, \n“Sure, maybe AIs have a weird instinct to escape.  ",
      "offset": 3276.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "But what are they even going to do?” Or things \nlike, “We’ll use various checks to prevent this.”",
      "offset": 3282.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And I think all these factors in \na relatively clear-cut scenario  ",
      "offset": 3287.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "could cut into that. And I kind \nof expect a lot of the situations  ",
      "offset": 3291.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that I think are like a huge smoking \ngun are way more confusing than this.",
      "offset": 3294.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "An example would be like, maybe it looks like the \nAIs are sandbagging on safety research — where  ",
      "offset": 3299.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "they’re performing worse than they could, \nor they’re not doing very brazen sabotage,  ",
      "offset": 3303.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "but there’s way more errors than we would \nexpect, there’s more misleading arguments.  ",
      "offset": 3308.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "It was from our perspective a big update. Because \nit seems pretty off, but it’s quite quantitative.  ",
      "offset": 3313.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "If you just had lower expectations, \nmaybe this wouldn’t be that surprising.",
      "offset": 3319.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: And I guess it’s a strategic \nsituation where the AI can to some extent  ",
      "offset": 3323.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "anticipate what the response will be. And \ninasmuch as it’s taking hostile actions,  ",
      "offset": 3328.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it can make them things that it suspects will \nnot actually trigger that larger response.  ",
      "offset": 3333.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So for things it can get away \nwith, like trying to escape,  ",
      "offset": 3339.36,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "but it couldn’t get away with mass murder. Then it \ngoes for the escape and holds off on the murder.",
      "offset": 3341.12,
      "duration": 4.783
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: At least for a while. One hope \nwith control is making it so that egregious bad  ",
      "offset": 3345.903,
      "duration": 4.897
    },
    {
      "lang": "en",
      "text": "stuff has less plausible deniability. We refer \nto this as “incrimination.” It’s really important  ",
      "offset": 3350.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to get from a position where you’re like, “Wow, \nthat was really suspicious,” to “OK, we’ve nailed  ",
      "offset": 3356.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "down exactly what was going on.” Just because \nincriminating in our lights doesn’t necessarily  ",
      "offset": 3360.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "mean it’s incriminating from the perspective of \nthe broader world, but it is moving the needle.",
      "offset": 3367.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And I think this can occur at all different \nregimes, from something looks a bit suspicious,  ",
      "offset": 3371.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to we kind of understand what’s going on and it \nlooks very suspicious; to the AI tried to escape,  ",
      "offset": 3375.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we stopped it, and we’ve done the thing where \nwe pretend it succeeded and we see what it  ",
      "offset": 3380,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "would do on the outside — and try to get to \nthe point where it’s like, “It looks like  ",
      "offset": 3384.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it’s trying to figure out how to manufacture \nbioweapons. It’s trying to run a wet lab.”",
      "offset": 3388.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Plausibly you can get, in principle at \nleast, very far into understanding what  ",
      "offset": 3392.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it would do given the affordance of having \nescaped. And to the extent that it does  ",
      "offset": 3397.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "stuff that’s much more clearly malign, I think \nthat could motivate additional action. But I  ",
      "offset": 3402.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "do worry that any sort of warning shot which \nis not causing massive amounts of damage and  ",
      "offset": 3407.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "not causing potentially human fatalities \nmaybe won’t trigger a strong response.",
      "offset": 3412.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So we’ve zoomed in on a scenario that \nis pretty vertiginous and concerning, I guess in  ",
      "offset": 3417.6,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "part because it’s remarkable that something so \ncrazy is so consistent with what we observe now.  ",
      "offset": 3425.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "That’s interesting in itself. And it’s also a \nscenario where we think there’s maybe a lot that  ",
      "offset": 3430.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "can be done now, or it’s a scenario where things \ngoing poorly is quite likely, and we should be  ",
      "offset": 3436.08,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "probably taking precautions and doing preparations \nto figure out how we would lower that risk.",
      "offset": 3442.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Just for balance, it would be good \nif we could maybe go over what are  ",
      "offset": 3447.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "some plausible scenarios that are a bit \nmore mundane, a little bit more boring,  ",
      "offset": 3450.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and perhaps hopefully a little bit less \nscary. So people don’t think this is the  ",
      "offset": 3454.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "only way that things are going to go. Because \nin fact, a key conclusion I think is just that  ",
      "offset": 3458.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we have incredibly wide error bounds around \nhow these different things could play out.",
      "offset": 3462.88,
      "duration": 3.903
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. I think maybe a good place \nto start is we talked a lot about timelines. Well,  ",
      "offset": 3466.783,
      "duration": 4.097
    },
    {
      "lang": "en",
      "text": "timelines could just be longer, right? \nSo I said my median or my 50% probability  ",
      "offset": 3470.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "was by eight years from now for the full AI lab \nautomation. But there’s a long tail after that,  ",
      "offset": 3475.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "so there’s worlds where it’s way further \nout. Potentially progress was much slower,  ",
      "offset": 3481.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "much more gradual. Things took a long time \nto integrate. Maybe the world has a better  ",
      "offset": 3485.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "understanding, more time for experiments, and \nthat could make the situation a lot nicer.",
      "offset": 3490.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "In addition to that, it could be the case \nthat on the very long timelines — or much  ",
      "offset": 3495.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "longer timelines, I should say, I don’t know \nabout very long — takeoff we should also expect  ",
      "offset": 3499.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is slower. I think short timelines are, broadly \nspeaking, correlated with fast takeoff, because it  ",
      "offset": 3503.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "indicates that the returns to more of the stuff \nwe were doing is higher. So longer timelines.",
      "offset": 3507.84,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Another thing is it could just be that AI R&D \nresearch bottlenecks extremely hard on compute.  ",
      "offset": 3514.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "And so even at the point when you have automated \nyour entire AI company and all the employees,  ",
      "offset": 3520.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you’re barely going much faster than you were \nwith human employees. In the extreme, maybe  ",
      "offset": 3526.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "you’re going only 2x faster. You could be even \nslower than that. But I think that’s pretty low to  ",
      "offset": 3533.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the low end of my error bars. I think that’s maybe \npretty implausible, but 2x or 3x is plausible.",
      "offset": 3538.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And then it could be that you have diminishing \nreturns setting in pretty quickly after that,  ",
      "offset": 3542.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "such that you’re not actually speeding up \nthat much. That would imply the situation  ",
      "offset": 3545.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "looks a lot better in terms of you \nget a much less aggressive takeoff.",
      "offset": 3550.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Another thing is that the default rate of \nAI progress is already pretty fast. So I  ",
      "offset": 3555.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "even worry — without the speedup, just on \nthe default trajectory — if we’re getting  ",
      "offset": 3559.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to this point where we have AIs capable \nenough to automate the entire AI company,  ",
      "offset": 3564.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "maybe even if it doesn’t yield much acceleration, \nand there’s some risk that comes pretty quickly  ",
      "offset": 3568.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "after, in normal government time, from \njust the default rate of progress.",
      "offset": 3572.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "But I think there are plausible arguments \nthat the rate of progress will slow down in  ",
      "offset": 3576.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the future. In particular, right now, progress is \nrelying on shovelling in more and more compute,  ",
      "offset": 3580.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "more and more investment. And if it’s the case \nthat things stall out, there’s less investment,  ",
      "offset": 3585.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "but we’re still operating at nearly pretty \nhigh capacity, we can imagine a world where  ",
      "offset": 3591.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the AI industry ends up being an industry that \njustifies maybe perhaps hundreds of billions  ",
      "offset": 3595.68,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "of dollars of compute spending per year, maybe \neven more than that, but not way more than that.",
      "offset": 3604,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "In that regime, if we’re not shovelling \nin more and more compute, then we don’t  ",
      "offset": 3609.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "get progress from hardware scaling, algorithmic \nprogress will also be slower. So maybe the pace  ",
      "offset": 3614.48,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "right now is we get 13x effective compute \nper year — where some of that is algorithm,  ",
      "offset": 3622.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some of that’s compute. I think if compute was \nliterally fixed, we might have like 2x or 3x  ",
      "offset": 3626.48,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "progress per year. And in a regime where compute \nis going slower, it could be somewhere between.",
      "offset": 3633.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, so that would be just a much \nslower takeoff. What’s the chance that alignment  ",
      "offset": 3637.44,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "is a relatively straightforward problem? Or maybe \nnot even a problem at all, and we’ve just been  ",
      "offset": 3645.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "confused, and all of these AIs that we produce \nare basically going to want to help us by default?",
      "offset": 3650.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Well, an important question \nfrom my perspective is what’s the probability  ",
      "offset": 3655.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that various AIs of different capability \nlevels are actively conspiring against us?  ",
      "offset": 3661.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I think it’s pretty plausible that you can get \ninto very superhuman capabilities and not have  ",
      "offset": 3666.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "AIs that are very actively conspiring against you \nin some strong sense. My current view is like 25%,  ",
      "offset": 3672.64,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "if you do basically no countermeasures; you \nbasically just proceed along the most efficient  ",
      "offset": 3680.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "route for capabilities. And then once we’re \nstarting to take into account countermeasures,  ",
      "offset": 3684.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "it could potentially go much lower, and that \nrisk could be lower at this early point too.",
      "offset": 3688.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "One story for hope is you have these early \nsystems that are able to automate the whole AI  ",
      "offset": 3692.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "company. Those systems aren’t conspiring against \nyou. You do the additional alignment work needed  ",
      "offset": 3697.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to make it so that they’re also trying to do \ntheir best — they’re actually trying to pursue  ",
      "offset": 3701.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "alignment research; they’re trying to anticipate \nrisks — which maybe doesn’t occur by default,  ",
      "offset": 3705.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "even if they’re not conspiring against you. It \ncould be that things aren’t conspiring against  ",
      "offset": 3709.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you, but things still go off the rails. \nBut if they’re not conspiring against  ",
      "offset": 3712,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you and you avoid things going off the \nrails, then maybe we’re just fine. We  ",
      "offset": 3714.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "hand over to the AIs. The AIs manage the \nsituation, they’re aware of the future  ",
      "offset": 3717.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "risks. They build another system with the same \nproperties, and that can be self-maintaining.",
      "offset": 3721.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Another scenario is it could be that an \neven stronger sort of thing is true by  ",
      "offset": 3725.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "default. It could be that you train the \nAIs, and with a relatively naive — what  ",
      "offset": 3728.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you would commercially do by default — training \nstrategy, they really just are, out of the box,  ",
      "offset": 3733.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "trying really hard to help you out. They’re really \nnice. Maybe even they’re just aligned in some  ",
      "offset": 3737.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "very broad sense. Not even they’re just myopic; \nthey’re really trying to pursue good outcomes for  ",
      "offset": 3742.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the world overall. I think this is possible, \nbut less likely than the previous scenario.",
      "offset": 3746.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Another thing is there’s a question, does it \nhappen by default? It could also be that it  ",
      "offset": 3750.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "just happens because of a lot of work that we \nput in, and we could just be in a much better  ",
      "offset": 3756.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "position. It’s hard to have much confidence \nabout how much alignment work or safety work  ",
      "offset": 3761.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "will happen in the next eight years. If we have \neight years, the community could build. AIs  ",
      "offset": 3766.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "will get more capable over time. More people \nwill be working on this. It seems plausible  ",
      "offset": 3770.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that there are potentially things like large \ninsights, but even putting aside large insights,  ",
      "offset": 3773.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "maybe just things like we just really advance \nour techniques, we’ve dialled in the science.",
      "offset": 3778.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So there’s timelines with, I would say, maybe \ntechnical hopes. And then there’s a broader  ",
      "offset": 3783.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "class of societal hopes. There’s some stories \nin which society just takes the situation much  ",
      "offset": 3790.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "more seriously in the future — maybe a subset of \ncountries, maybe the broader scientific community.  ",
      "offset": 3795.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "It could be because of a warning shot. It could \njust be because slowly people are exposed to AI,  ",
      "offset": 3800.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "timelines are somewhat longer maybe, \nand more people are interacting with it.",
      "offset": 3804.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "It also seems plausible that you have some big \nincidents going on that aren’t that related  ",
      "offset": 3808.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to misalignment risks, but in practice have \nhigh transfer. So maybe there’s a lot of job  ",
      "offset": 3813.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "loss that prompts a large response, and some of \nthat response goes into mitigating misalignment  ",
      "offset": 3817.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "risks — either very directly, as in people are \nlike, “AI was a big deal and a big problem in this  ",
      "offset": 3823.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "way, so maybe it’s a big deal and a big problem in \nthat way” — but also maybe much more indirectly:  ",
      "offset": 3828.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it causes slow AI progress or more cautious AI \nprogress because there’s a lot more regulation.",
      "offset": 3831.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Are there any other hopeful \ncategories, or is that mostly covering it?",
      "offset": 3836.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: So there’s things are easy on \ntimelines and takeoff, things that are easy on  ",
      "offset": 3840.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "technical, and there’s societal. A fourth category \nis we rise to the occasion. Maybe even if society  ",
      "offset": 3845.84,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "doesn’t rise to the occasion, maybe just a variety \nof heroic efforts — or hopefully not that heroic —",
      "offset": 3852.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: If one company, really the \nleading company, turns out to actually  ",
      "offset": 3858.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "be very impressively responsible and puts \na lot of effort into this and they succeed.",
      "offset": 3861.6,
      "duration": 3.743
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. I think \na lot of the risk is coming from  ",
      "offset": 3865.343,
      "duration": 5.457
    },
    {
      "lang": "en",
      "text": "worlds where you could have saved the \nworld if you just had a year of delay,  ",
      "offset": 3871.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and you were taking the situation very \ncarefully, and you were being thoughtful  ",
      "offset": 3876.48,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "about all the various considerations. Or at \nleast the risk is a lot lower. Maybe it’s not  ",
      "offset": 3878.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "minimised or it’s not eliminated. Certainly \nI wouldn’t say that results in a level of  ",
      "offset": 3884.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "risk that would be broadly acceptable from my \nperspective. But I don’t know. It could happen.",
      "offset": 3888.64,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Cool. You literally have \ncheered me up a little bit there,  ",
      "offset": 3897.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "by reminding me that there are other ways that \nthings could go. What are some lines of evidence  ",
      "offset": 3900.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that suggests that it could take substantially \nlonger than four or even eight years?",
      "offset": 3905.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: For one thing, I \nshould reiterate that I don’t think that  ",
      "offset": 3909.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we’ll see full automation or the capability \nto fully automate AI companies in four years.  ",
      "offset": 3914.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So I must think there are some lines of \nevidence that this isn’t going to happen.",
      "offset": 3918.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "I think the first place people should \nstart is just scepticism about crazy  ",
      "offset": 3921.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "shit happening in a specific time frame. \nSo I think there’s a bunch of evidence  ",
      "offset": 3927.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that suggests we might expect it soon — \nyou know, we’ve had rapid AI progress,  ",
      "offset": 3931.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "I think that has been hitting a lot of things \n— but if you’re just like, how much progress  ",
      "offset": 3934.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "have we had to date? How long has the field \nbeen going? And you’re sort of operating from  ",
      "offset": 3939.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "a very outside-view priors-y perspective, \nwhere you’re like, what is the base rate of  ",
      "offset": 3942.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "technologies, then you don’t expect in the next \nfour years — because this is a crazy technology.",
      "offset": 3946.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I even think the base rates are \nmaybe more bullish than people  ",
      "offset": 3951.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "tend to think. I think often when people \ndo this outside-view base rates thing,  ",
      "offset": 3956.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "they end up with like 200-year \ntimelines or something. But actually,  ",
      "offset": 3959.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Tom Davidson has a report from a while ago \non like if you just try to apply the outside  ",
      "offset": 3963.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "view most naively, you actually end up with \nquite a large probability in the next century.",
      "offset": 3967.76,
      "duration": 4.788
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Is that because there’s been very \nrapid increases in the amount of investment in  ",
      "offset": 3972.548,
      "duration": 4.172
    },
    {
      "lang": "en",
      "text": "the area? And we’re just increasing the orders of \nmagnitude of compute that we’re throwing in, such  ",
      "offset": 3976.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that I guess if the difficulty was distributed \nacross log space, then we’re actually —",
      "offset": 3982.8,
      "duration": 5.915
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Resources \nand compute and labour. Yeah,  ",
      "offset": 3988.715,
      "duration": 2.165
    },
    {
      "lang": "en",
      "text": "we’ve crossed a lot of that. So that’s one view.",
      "offset": 3990.88,
      "duration": 1.6
    },
    {
      "lang": "en",
      "text": "You can also just be like, even if you just \nlook at time, we’ve just only been doing  ",
      "offset": 3992.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "serious AI research for not that long. You \ncould use how long have we been doing deep  ",
      "offset": 3996.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "learning? How long has anyone been working \non AI? And if you sort of mix this together,  ",
      "offset": 4002.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it feels like powerful AI soon is in \nsome sense not that unlikely on priors.",
      "offset": 4005.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "But I still think that how unlikely \nis it on priors suggests much longer,  ",
      "offset": 4011.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and I think that pulls me towards longer.",
      "offset": 4015.68,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "Generally another perspective that \npulls me towards longer is just  ",
      "offset": 4017.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "being like, the AIs are pretty smart, \nbut they’re not that smart. And I think  ",
      "offset": 4022.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we just can’t have that much confidence \nthat the next part of progress is doable.",
      "offset": 4026.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Then there’s some more specific \nobject-level arguments.",
      "offset": 4030.88,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "I’m sorry to deflect the question a bit, but I \nalways feel tempted to go in and do some of the  ",
      "offset": 4033.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "more arguments for short timelines that you’re \nbringing up. So another perspective on short  ",
      "offset": 4038.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "timelines that I think is pretty important \nhere is we are going through a lot of orders  ",
      "offset": 4042.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of magnitude of compute and labour pretty \nquickly that we haven’t gone through before.",
      "offset": 4047.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "And in addition to this, we’re going \nthrough orders of magnitude that are  ",
      "offset": 4051.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "somewhat above our best guesses at the amount \nof human brain compute. So we have these  ",
      "offset": 4054.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "not-very-good estimates of how much compute the \nhuman brain is using that indicate maybe it’s  ",
      "offset": 4061.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like 10^24 as a central estimate of lifetime human \nbrain compute. Currently training runs are about…  ",
      "offset": 4065.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "I think Grok 3, which was just trained \nrecently, I think I saw the estimates  ",
      "offset": 4071.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "were like 3 x 10^26 — so two and a half \norders of magnitude above human lifetime.",
      "offset": 4075.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "And then we might think that you first reach \nAIs capable of basically beating humans,  ",
      "offset": 4082.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "above human lifetime compute — because generally \nwe develop algorithms that are less efficient than  ",
      "offset": 4088.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "biology first, and then they potentially pretty \nquickly become more efficient than biology.",
      "offset": 4092.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "This is in the literature, in Ajeya’s \nold bio anchors report for the people  ",
      "offset": 4099.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "who are familiar with that, what is \ncalled the “lifetime anchor.” And I  ",
      "offset": 4105.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "put a decent amount of weight on the \nlifetime anchor. It looks pretty good,  ",
      "offset": 4108.64,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "because we’re hitting these around humanish \ncapabilities around this point where we’re  ",
      "offset": 4111.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "hitting human compute. So it just feels like \nthat model has a lot of support, and that  ",
      "offset": 4116.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "suggests that we might be hitting it in \nseveral orders of magnitude more compute.",
      "offset": 4121.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And we’re just sort of burning through these \norders of magnitude of compute very quickly.  ",
      "offset": 4126.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "For context, I think training run \ncompute is increasing by about 4x  ",
      "offset": 4130.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "per year. So you go through a lot of \norders of magnitude pretty quickly.  ",
      "offset": 4134,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "It’s like a little over an order \nof magnitude per year at that rate.",
      "offset": 4138,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "And also, I just generally think we should \nput some weight on a very compute-centric  ",
      "offset": 4141.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "view on AI development — and on the \ncompute-centric view on AI development,  ",
      "offset": 4144.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "even with something less specific than lifetime \nanchor, we’re burning a lot of orders of  ",
      "offset": 4147.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "magnitude starting from a pretty competitive \nstarting point. Maybe we go pretty far.",
      "offset": 4150.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Now the bear case, relative to that, is that we \ncan only burn through orders of magnitude so far,  ",
      "offset": 4154.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "right? We’re not that far off of hitting \nthe total amount of chips you can produce.  ",
      "offset": 4161.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "I might be messing up the statistic, but a pretty \nreasonable fraction of TSMC or semiconductor  ",
      "offset": 4169.76,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "manufacturing capacity is going towards machine \nlearning chips. My cached number is 10% to 20%. I  ",
      "offset": 4178.8,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "hope I don’t get this horribly wrong, but I think \nit’s definitely above 1%. It’s well above 1%.",
      "offset": 4185.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So one thing that’s interesting about this is that \nNvidia revenue is roughly doubling every year,  ",
      "offset": 4192.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and I think that the number of wafers or \nwhatever — I hope I don’t brutalise the thing;  ",
      "offset": 4198.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I’m not a semiconductor expert here — I think \nsemiconductors for AI have been increasing a  ",
      "offset": 4203.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "little over 2x per year. Just to make it simple, \nlet’s do a slightly bullish estimate of like 3x  ",
      "offset": 4209.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "per year. Well, one thing that’s worth noting is \nif you’re starting at 20% and you’re increasing  ",
      "offset": 4215.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "at 3x per year, you do not have that long to go \nthrough before you’re just hitting limitations.",
      "offset": 4219.44,
      "duration": 4.628
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You’re using \nall of the chips for this.",
      "offset": 4224.068,
      "duration": 1.275
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, you’re using all the chips,  ",
      "offset": 4225.343,
      "duration": 0.897
    },
    {
      "lang": "en",
      "text": "and once basically like all of the fabs \nare producing AI, you can only go so fast.",
      "offset": 4226.24,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess then you’re limited by how \nquickly they can build new fabs, basically.",
      "offset": 4233.04,
      "duration": 3.503
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, limited by \nhow quickly they can build new fabs.",
      "offset": 4236.543,
      "duration": 2.497
    },
    {
      "lang": "en",
      "text": "There’s other sources of progress, to be clear. \nSo even if we’re sort of limited by building new  ",
      "offset": 4242.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "fabs, there is still potentially sources of \nprogress from hardware improving over time,  ",
      "offset": 4246.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and there are sources of progress from \nalgorithmic development. Though I think a  ",
      "offset": 4251.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "key thing that maybe people aren’t tracking \nenough is that algorithmic development is  ",
      "offset": 4254.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "also driven by dumping in more compute. So we \nshould expect algorithmic development to slow.",
      "offset": 4257.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So I think the bear case is maybe: \nyou have AI keep going. We’re already  ",
      "offset": 4261.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "hitting insane amounts of spending. As \nyou’re starting to hit the TSMC limits,  ",
      "offset": 4265.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the spending has to go even higher \nto justify building new fabs.",
      "offset": 4271.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Or maybe prior to hitting most of TSMC \nproduction, maybe people are just not  ",
      "offset": 4273.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "seeing results sufficient to justify these \nlevels of investment. I think Microsoft,  ",
      "offset": 4278.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Google, they can justify potentially like… I mean, \nMicrosoft is I think planning about $100 billion  ",
      "offset": 4284.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "in capital expenditure this year. Stargate has \nabout $100 billion committed, maybe that’s over  ",
      "offset": 4290.64,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "a year or two, and then maybe they’re hoping they \nget more money. Stargate is a project by OpenAI.",
      "offset": 4297.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "But I think once you’re starting to \nget beyond this $100 billion regime,  ",
      "offset": 4303.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it’s no longer sufficient to just have \na big tech company that’s super sold,  ",
      "offset": 4307.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right? Google does not have the ability \nto readily spend a trillion dollars,  ",
      "offset": 4311.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "especially not a trillion dollars in a year. I \nmean, I think it is not out of the question that  ",
      "offset": 4316.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "you could raise a trillion dollars, but I think \nyou would probably need very impressive results,  ",
      "offset": 4322.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and you need to be starting to pull in more \nsceptical investors, more revenue. You need  ",
      "offset": 4327.68,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "to be talking about potentially sovereign \nwealth funds. Certainly it’s possible —",
      "offset": 4334.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: The US government \ncould deliver that kind of money.",
      "offset": 4340.48,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, that’s \ntrue. Certainly it’s possible.",
      "offset": 4342.64,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Although I guess probably if \nyou try to spend a trillion dollars in a  ",
      "offset": 4344.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "year on this kind of thing, you \nstart hitting other bottlenecks.",
      "offset": 4348.24,
      "duration": 3.253
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: For sure. Yeah, yeah. I don’t \ncurrently know what the elasticity is here. I  ",
      "offset": 4351.493,
      "duration": 8.027
    },
    {
      "lang": "en",
      "text": "think Epoch did some estimates of like, what is \nthe biggest training run you could do by 2030? I  ",
      "offset": 4359.52,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "think their median, in the timeline where people \nkeep trying to spend aggressively, was maybe you  ",
      "offset": 4366.8,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "can get up to 10^30 FLOP training runs — and \nthat was taking into account data bottlenecks,  ",
      "offset": 4373.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "various considerations on bandwidth between \nchips, various considerations about how much  ",
      "offset": 4378.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you can scale up chips, chip production, how much \nTSMC is building up capacity, this sort of thing.",
      "offset": 4384,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "My guess is that that’s probably a \npretty reasonable guess independent of  ",
      "offset": 4389.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "AI acceleration. AI acceleration could make this \nfaster. But if we’re in sort of a bull timeline,  ",
      "offset": 4393.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but not a bull timeline where the AIs have started \nto speed things up, I would guess that’s a pretty  ",
      "offset": 4398,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "good estimate. But there’s a chance we fall off of \nthat because maybe the bottlenecks hit harder than  ",
      "offset": 4402,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "they were expecting, and maybe investment dries up \nfaster. Hopefully they don’t call me out on this,  ",
      "offset": 4410.72,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "but I think they weren’t taking into \naccount willingness to pay on this.",
      "offset": 4419.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "But after the 2030 point, I think things are \nstarting to get a lot harder. I think if we’re  ",
      "offset": 4422.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "getting up to 10^30 FLOP — you know, bare metal \nFLOP, just the actual computations the GPUs are  ",
      "offset": 4427.36,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "running — starting from where we are now, \nwhich is more like a little over 10^26,  ",
      "offset": 4433.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "then we’ve hit four orders of magnitude in \nthe next five years. So that’s quite fast.",
      "offset": 4441.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "I think that is enough that we could in principle \nsee the trends that we’ve seen continue, like the  ",
      "offset": 4447.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "trends that we have. I think we’ve had a bit of \na GPU lull. I think we’ll have GPUs sort of start  ",
      "offset": 4453.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "building up. I think post GPT-4 there’s a bit of a \nlull as people are trying to buy all the H100s. I  ",
      "offset": 4458.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "think we’ll see a run of models with more H100s, \nlike GPT-4.5, and then we’ll see another round  ",
      "offset": 4463.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "with Stargate, which is another big buildout. \nSemiAnalysis has speculated that Anthropic  ",
      "offset": 4469.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "has a large number of chips from Amazon, maybe \nequivalent to around 100,000 or 200,000 H100s, so  ",
      "offset": 4475.92,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "we’ll see another round of 100,000 to 200,000 H100 \nclusters. And then we’ll see maybe, I’m going to  ",
      "offset": 4483.6,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "get the dates wrong, but maybe around 2028, we’re \ngoing to see more around the million GPU range.",
      "offset": 4493.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "But if we’re not getting to really powerful \nAI by then, I think you should expect things  ",
      "offset": 4500.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to slow. There’s this longer tail of \nwe eat a bunch of compute around 2030,  ",
      "offset": 4504.56,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "2032, and then progress has to taper — \nunless we hit very powerful capabilities,  ",
      "offset": 4512.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "or we’re all wrong about how fast you can build \nfabs or how much investment you can summon.",
      "offset": 4518.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "But I think that’s a lot of the bear case \nis maybe you do a bunch of the scaling,  ",
      "offset": 4522.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "but you hit these limits. Not all of it.",
      "offset": 4527.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So the short version of that, for \npeople who didn’t follow it all, is that we’re  ",
      "offset": 4531.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "currently increasing the amount of compute that \nis going towards training AIs very quickly. And  ",
      "offset": 4535.04,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "we’re not going to be able to maintain that pace, \nbecause we’re currently doing that by grabbing  ",
      "offset": 4541.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "some chips that were being sold for other purposes \nand using them for machine learning instead.",
      "offset": 4546.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And also, these companies are going from \nspending 1% of their resources on AI  ",
      "offset": 4551.12,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "development to 10% — and then maybe they \ncan go to 100%, but they can’t really go  ",
      "offset": 4558.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "beyond that by just grabbing resources that \nwere previously going towards other stuff.",
      "offset": 4562.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "So at the point where almost all \nof the chips are going towards  ",
      "offset": 4565.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "AI training and almost all of the resources \nof these companies is going towards that,  ",
      "offset": 4569.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it levels off the rate of \nincrease that you can get there.",
      "offset": 4573.28,
      "duration": 3.583
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: For sure. Maybe a few \nminor clarifications on that. One thing is,  ",
      "offset": 4576.863,
      "duration": 3.377
    },
    {
      "lang": "en",
      "text": "when you’re thinking about repurposing the \nchips, this isn’t like they’re taking iPhone  ",
      "offset": 4580.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "chips and repurposing. It’s like there was \ncapacity to do chip manufacturing that was  ",
      "offset": 4585.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "relatively general purpose, and instead of \nmaking as many iPhones, we’re now making more  ",
      "offset": 4588.48,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "AI chips. And some of that’s coming from building \nadditional fabs, but some of it’s coming from  ",
      "offset": 4594.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "slightly increasing the prices of other chips, \nor reducing how many of them you’re getting.",
      "offset": 4598.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And when AI is in the 20% or 30% regime, \nit’s not going to have very noticeable  ",
      "offset": 4603.6,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "market effects on how much other chips \nare costing, because TSMC expanding  ",
      "offset": 4611.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "somewhat faster can keep up with that. But \nonce it’s like AI chips are 80% or 100%,  ",
      "offset": 4616.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "then we’re going to start seeing bigger \neffects and things slowing down from there.",
      "offset": 4621.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Another thing that \nmakes it a little bit difficult  ",
      "offset": 4625.2,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "to forecast all of this is that we’re \nnot just doing exactly the same thing  ",
      "offset": 4627.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "year after year; we’re not riding \nthe same trends year after year.",
      "offset": 4631.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Initially, for example, we were getting a \nlot of improvement by putting more compute  ",
      "offset": 4634.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "into the pretraining. This is the thing where \nyou dump in all the text from the internet and  ",
      "offset": 4638.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "try to get it to predict the next word. We were \ngetting enormous gains from that by throwing in  ",
      "offset": 4641.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "more data and more compute. But then that \nsort of tapered off, and we have to move  ",
      "offset": 4647.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "towards better elicitation using post-training, \nreinforcement learning from human feedback. Then  ",
      "offset": 4650.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "we’re doing a different thing, which initially \nis very effective, but then begins to level off.",
      "offset": 4656.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I guess now we’re using reinforcement \nlearning to do a sort of self-play,  ",
      "offset": 4659.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "where the models are learning to reason \nbetter, and basically we just reinforce  ",
      "offset": 4663.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "them when they get the answer right. \nWe’re on a very steep curve with that,  ",
      "offset": 4667.76,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "but presumably that at some point will level \noff and we have to do a different thing.",
      "offset": 4670.08,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "If I understand that correctly, that makes it \nmore difficult, because we don’t know exactly  ",
      "offset": 4674.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "what will be the innovation next year \nthat will be driving the improvements.",
      "offset": 4677.68,
      "duration": 2.863
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. The improvements from \nGPT-1 to GPT-2 to GPT-3 to GPT-4 — and then  ",
      "offset": 4680.543,
      "duration": 9.217
    },
    {
      "lang": "en",
      "text": "maybe even throughout there, there was \na bit of a lull in 2023 and then things  ",
      "offset": 4689.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "picking back up in 2024 — I think \na lot of the improvement there,  ",
      "offset": 4695.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "up until maybe the middle of 2024, was driven by \nscaled-up pretraining, like dumping in more data.",
      "offset": 4701.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "There’s sort of a meme going around \nthat pretraining has hit a wall. It’s  ",
      "offset": 4707.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "not so clear to me that this is what’s \ngoing on. It probably is the case that  ",
      "offset": 4711.04,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "it has relatively diminished returns, \nor the marginal returns of scaling that  ",
      "offset": 4718.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "up further are less than they were in the \npast in terms of qualitative capabilities.",
      "offset": 4723.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Based on some sense of like, we have \nGrok 3 — which is maybe a little over  ",
      "offset": 4727.84,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "10x more compute than GPT-4 and also better \nalgorithms — and how much better is it?  ",
      "offset": 4735.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "It’s somewhat better, but I think it’s also the \ncase that it’s worth noting that in previous  ",
      "offset": 4741.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "improvements — like the GPT-3 to GPT-4 gap — \nit was a lot more compute. I think that gap  ",
      "offset": 4745.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "was like roughly 100x bare metal compute. And \nit just turned out that at the time there was  ",
      "offset": 4749.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "a lot of low-hanging fruit in scaling up the \namount of FLOP you’re spending very quickly.",
      "offset": 4755.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And now we’re running into bottlenecks. Post GPT-4 \nthey were sort of waiting for H100s. The H100s  ",
      "offset": 4759.6,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "were slow to get delivered. We were getting the \nH100 clusters online kind of late, and maybe there  ",
      "offset": 4766.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "were some difficulties with getting that initially \nworking. So GPT-4.5, somewhat disappointing. I  ",
      "offset": 4771.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "think it’s rumoured that OpenAI had multiple, \nor at least one, failed training run.",
      "offset": 4776.24,
      "duration": 3.988
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Oh wow.",
      "offset": 4780.228,
      "duration": 0.572
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: So it wouldn’t be surprising \nif we see people adapting to more compute,  ",
      "offset": 4780.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "figuring out how to use it, and the returns \nfrom pretraining sort of going up from there.",
      "offset": 4784.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "That’s pretraining, and maybe even if \npretraining is diminishing, you can scale up RL,  ",
      "offset": 4788.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "reinforcement learning. We’ve seen that going on \nover 2024. We have o1, where they’re training on  ",
      "offset": 4792.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "RL. They’re training on easy-to-verify tasks, \nnot training on next token prediction. And  ",
      "offset": 4798.72,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "we’ve seen a lot of initial returns from that \nand we don’t know where that will peter out.",
      "offset": 4807.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "You know, we haven’t seen that many orders \nof magnitude on RL training. It’s speculated,  ",
      "offset": 4812.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for example, that DeepSeek-R1 was about $1 \nmillion worth of compute on the RL. So in  ",
      "offset": 4816.64,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "principle we can scale that up to be more \nlike three orders of magnitude higher with  ",
      "offset": 4824.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the clusters that people will have. Maybe \na little less than that, but ballpark.",
      "offset": 4829.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Once we’re talking about three \norders of magnitude higher,  ",
      "offset": 4834.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it could be that that yields huge returns, or it \ncould be that that doesn’t yield huge returns.",
      "offset": 4837.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "The story for yielding big returns is \nit yielded big returns from the first  ",
      "offset": 4843.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "million. Maybe you just go further, build \nmore environments, do more RL big returns.  ",
      "offset": 4847.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "The story for not is maybe there was some sort of \nlatent capacity or potential the model had, which  ",
      "offset": 4851.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "RL is bringing out. We’ve brought out most of the \npotential, and we’re hitting diminishing returns.",
      "offset": 4857.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Just to say that back, for people who \ndon’t follow this very closely and are not sure  ",
      "offset": 4861.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "exactly what you meant: one thing that is actually \nworth remembering is that I guess we used to use  ",
      "offset": 4867.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "reinforcement learning a lot, and then it somewhat \nfell off, and now it’s come to the fore again.",
      "offset": 4873.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "This is where you take the existing models, \nGPT-4o or something like that, and you give  ",
      "offset": 4876.56,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "it very challenging reasoning problems, and you \nmaybe get it to try 100 different solutions,  ",
      "offset": 4883.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "1,000 different solutions. And basically you \njust find the cases where it gets the right  ",
      "offset": 4887.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "answer at the end and you say, “Well done. \nReason more like that to try to solve other  ",
      "offset": 4890.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "problems in the same way that you just did.” \nAnd that’s turning out to be extremely powerful.",
      "offset": 4894.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I guess you’re saying that that is maybe picking \nup some low-hanging fruit, where these models had  ",
      "offset": 4899.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "more ability to do clever reasoning latent \nin the weights than was initially apparent,  ",
      "offset": 4904,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "than we were able to get out of them. And \nby using this process to get it to try all  ",
      "offset": 4909.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "kinds of different solutions and then \nfind the reasoning processes that are  ",
      "offset": 4913.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "functioning well, we’re extracting a whole \nlot of stuff that was just sitting there  ",
      "offset": 4916.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "waiting to be picked up. But that could run \nout somewhat, and at that point it’s going  ",
      "offset": 4920.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to be a heavier lift for them to basically \nlearn new superior reasoning techniques.",
      "offset": 4925.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, that’s right. People \nwere trying to get models to reason in chain of  ",
      "offset": 4930.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "thought. So as of GPT-4, people were totally aware \nthat you could do chain of thought reasoning,  ",
      "offset": 4936.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and were messing around with this. But I think \nit only worked so well. The model wasn’t that  ",
      "offset": 4940.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "good at recovering from its mistakes. It wasn’t \nthat good at carefully reasoning things through.",
      "offset": 4944.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "And I think we’re seeing with o1 and o3 and \nother reasoning models that now you can make  ",
      "offset": 4947.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "it so the model can do the reasoning \npretty well. And a bunch of that might  ",
      "offset": 4953.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have just been chain of thought, it was \nalready sort of doing it — and maybe you  ",
      "offset": 4956.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "can just make it somewhat better, but \nthere’s a question of how much better.",
      "offset": 4960.64,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Another piece of low-hanging fruit \nthat you mentioned was basically sometimes we were  ",
      "offset": 4963.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "giving these models very difficult challenges, \nbut giving them only $1 worth of compute to  ",
      "offset": 4968.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "work with. And people have found that if you \nactually just give them more like the kind of  ",
      "offset": 4972.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "resourcing that you would give to a human — $100 \nor $1,000 worth of equipment and salary — then  ",
      "offset": 4976.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "they do radically better when you’re doing \nsomething that’s a bit fairer of a comparison.",
      "offset": 4981.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "But you kind of can’t do that scaleup again. You \ncan go from $1 to $1,000, but if you’re going  ",
      "offset": 4987.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "from $1,000 to $100,000, now you’re talking real \nmoney. And there’s a question of would this ever  ",
      "offset": 4992,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually be economically useful just to give \na model so much to just solve a maths problem?",
      "offset": 4996.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. One thing \nthat’s also worth noting is,  ",
      "offset": 5000.32,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "even if it would be economical, we quickly \nrun into just the total quantity of compute.",
      "offset": 5002.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "So suppose that you’re having the model \ndo one task for $100,000. I did some  ",
      "offset": 5008.08,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "kind of crappy BOTEC [back-of-the-envelope \ncalculation], and my sense was that,  ",
      "offset": 5016.4,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "at least as of a few months ago, the total \namount of compute that OpenAI had was about  ",
      "offset": 5018.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "$500,000 per hour. So if it’s a $100,000 task, \nthen you’re using a fifth of all of OpenAI’s  ",
      "offset": 5022.56,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "compute for an hour. So you can only do so much \nof that, right? Even if it was the case that  ",
      "offset": 5029.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "there’s some really economically valuable \ntasks, you just hit bottlenecks on that.",
      "offset": 5035.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because that drives up the price.",
      "offset": 5039.36,
      "duration": 2.463
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, it’s the supply \nand demand. Even if it’s the case that  ",
      "offset": 5041.823,
      "duration": 3.857
    },
    {
      "lang": "en",
      "text": "it’s useful enough, there’s really only \nso much scaling of that that you can do.  ",
      "offset": 5047.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And, you know, you can go pretty far: people \nhave been demonstrating not just human cost,  ",
      "offset": 5051.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "but substantially above human \ncost, yielding additional returns.",
      "offset": 5058,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "And this is kind of nice because it lets us get \nsort of a sneak peek into the future. I think if  ",
      "offset": 5061.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you’re seeing the models do a task, but slowly \nand at very high cost, we should expect that  ",
      "offset": 5065.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "soon enough that will be quickly at a much lower \ncost, because the cost is rapidly dropping.",
      "offset": 5073.92,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: We can draw out the curve.",
      "offset": 5079.96,
      "duration": 1.383
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, we can draw \nout the curve. So I’m somewhat hopeful  ",
      "offset": 5081.343,
      "duration": 3.297
    },
    {
      "lang": "en",
      "text": "for people who are more sceptical about \nAIs exhibiting some capability, maybe  ",
      "offset": 5085.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "first we can exhibit it with very high runtime \ncompute, a lot of domain-specific elicitation,  ",
      "offset": 5089.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and then pretty shortly after that the need \nfor that goes away. Hopefully then we can reach  ",
      "offset": 5094.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "common knowledge somewhat before the capability \nis widespread enough to be incredibly widely used.",
      "offset": 5100.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: We were dipping in \nand out there on the bear case,  ",
      "offset": 5107.44,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "or the case where we’re thinking about \nthis is going to take a long time.",
      "offset": 5109.76,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I’m such a bad bear.",
      "offset": 5111.92,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Well, I’ll try to make one of the bear \narguments that I hear. The AIs might get very good  ",
      "offset": 5114,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "at fairly narrow tasks, and maybe they become \nreally good coders, they become really good at  ",
      "offset": 5120.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "idea generation, hypothesis generation, setting \nthem up. But there’s more to running and scaling  ",
      "offset": 5125.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "an AI company than just those things, and they’ll \nhave some serious gaps and weaknesses, and that  ",
      "offset": 5129.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is the thing that will be the limiting factor and \nslow it down. How plausible do you think that is?",
      "offset": 5133.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Some context here: historically, \nas we were saying, a lot of the returns were  ",
      "offset": 5138.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "driven by pretraining. And then starting later \nin 2023 and in 2024 there was RL — where a bunch  ",
      "offset": 5142.8,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "of the returns from GPT-4 Turbo were driven by \nRL, or was improving over time probably. This  ",
      "offset": 5150.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is speculation, but GPT-4o is a somewhat better \nbase model, but also better RL and then we had o1,  ",
      "offset": 5155.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "better RL. And this has been driving up \nbenchmark scores on programming tasks,  ",
      "offset": 5160.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "coding tasks, math tasks — tasks that \nare checkable much more than other tasks.",
      "offset": 5165.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Now, we do see some transfer. One way out of \nthis is maybe it’s just the case that you make  ",
      "offset": 5168.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the AIs very superhuman at programming, quite \nsuperhuman at software engineering — which is  ",
      "offset": 5174.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "somewhat harder to check, but doable to check, \nor at least parts of it are pretty readily  ",
      "offset": 5178.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "possible to check — and very superhuman \nat math. And then this transfers somewhat.",
      "offset": 5183.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "So maybe it’s the case that it’s very \nexpensive to label other things. You know,  ",
      "offset": 5186.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "we can assess human performance in other domains; \nyou just can’t do it automatically. So you can  ",
      "offset": 5192.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "assess how good a paper is, and we have processes \nfor doing that, which has some signal; it’s just  ",
      "offset": 5200,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "much more expensive. So in principle, if the AIs \ncan almost transfer to writing good papers and  ",
      "offset": 5204.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "need just a little bit of feedback — they’re quite \nsample efficient — then that can go pretty far.",
      "offset": 5210.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Another thing worth noting is I think there is \na “no generalisation required” story. Or nearly  ",
      "offset": 5215.28,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "no generalisation. So it might be the case \nthat you can basically make it almost work,  ",
      "offset": 5222.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "or basically work, to just do RL on things that \nare just straightforwardly part of being a really  ",
      "offset": 5226.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "good research engineer. We might be able to get \nto the point where we have nearly fully automated  ",
      "offset": 5232.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "research engineering at AI companies just via \nscaling up RL, piecing it together, and not  ",
      "offset": 5237.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "really requiring very much transfer at all. Sorry, \nwe require in-domain generalisations — we require  ",
      "offset": 5243.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the AI is improving in domain relatively quickly \nand relatively efficiently — but even if the AIs  ",
      "offset": 5249.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "don’t have very good research taste, they’re not \nvery good at these things, that can go pretty far.",
      "offset": 5254.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Now, in addition to this, you might \nbe able to get surprisingly good on  ",
      "offset": 5258.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "some of these other things of running \nan AI company even with just in domain,  ",
      "offset": 5261.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "not requiring much transfer. So we have \nthese sort of research engineer AIs,  ",
      "offset": 5266.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "for one thing: maybe that makes progress go \nfaster starting then, and then we sort of kick off  ",
      "offset": 5269.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "more progress that eventually gets us a bunch of \nother things. I can go through that story later.",
      "offset": 5274.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "But second of all, if we have these research \nengineer AIs but they’re not going to have  ",
      "offset": 5278.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "research tastes, they’re not going to have a \nbunch of other things that are harder to train…",
      "offset": 5283.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Well, one thing is a lot of what we mean \nby research taste is understanding what  ",
      "offset": 5287.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the results of experiments might be, \nhaving good predictions, being able to  ",
      "offset": 5292.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "sort of understand what’s going on based on \na smaller amount of evidence. And for this,  ",
      "offset": 5295.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you can maybe just train AIs to be amazing \nforecasters of ML research experiments.",
      "offset": 5300.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And if you’re training these AIs to \nbe superhuman ML project forecasters,  ",
      "offset": 5304.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "where they’re predicting the results based \non just generating a large number of smaller  ",
      "offset": 5310.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "scale ML projects, having some transfer to \nlarger scale ML projects — which is going  ",
      "offset": 5315.04,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "to be less easy to readily get data — then \nmaybe from there you have these AIs that  ",
      "offset": 5322,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "can predict the results of experiments as \nwell as, you know, Ilya Sutskever or Alec  ",
      "offset": 5327.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Radford. Maybe they are less relying on \ninsight and more just, mechanistically,  ",
      "offset": 5331.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "you generate a long list of ideas, you predict \nhow well they go, you proceed from here.",
      "offset": 5337.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "And I think there’s basically a bunch of \nroutes like this. Another route is maybe  ",
      "offset": 5341.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you can just do within domain, and that can go \nsurprisingly far in automating the AI company.",
      "offset": 5346.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "An important asterisk for this is, I think the \nsceptics maybe are sort of screaming to me,  ",
      "offset": 5351.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "“But what about things other than automating AI \ncompanies?!” I think maybe they’re like sure,  ",
      "offset": 5356.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "AI R&D, I guess that’s checkable enough. \nBut what about all the other things that  ",
      "offset": 5360.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "humans do which have slower feedback \nloops, like being a good strategic CEO?",
      "offset": 5365.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Fundraising?",
      "offset": 5369.36,
      "duration": 0.943
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Fundraising, all these other \nthings. So I think I want to put a pin in that,  ",
      "offset": 5370.303,
      "duration": 3.137
    },
    {
      "lang": "en",
      "text": "but maybe we’ll get back to that later.",
      "offset": 5373.44,
      "duration": 1.36
    },
    {
      "lang": "en",
      "text": "Then the second thing I want to say is, so \nthere’s the in-domain story and then there’s  ",
      "offset": 5375.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the more generalisation story — where you train \non all these narrow tasks, the AIs get smart;  ",
      "offset": 5379.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "with just a tiny bit of data, they can transfer \nto these out of domain or we didn’t have a tonne  ",
      "offset": 5384.8,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "of data things. I think it’s going to be \nsome interpolation between these probably,  ",
      "offset": 5390.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "where it’s harder to get lots and lots \nof data on long horizon SWE tasks,  ",
      "offset": 5394.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so you probably need a bit of transfer \nbut you can probably do a bunch of data.",
      "offset": 5398.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "And then there’s I think a third story, which \nis like people just figure out how to RL on  ",
      "offset": 5401.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "harder-to-check tasks using other mechanisms, \nusing things that are more like process-based  ",
      "offset": 5406.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "checks, or being able to check fuzzier tasks using \nthings more along the lines of like self-critique,  ",
      "offset": 5411.92,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "constructing more detailed checks \nthemselves, and doing RL from here.",
      "offset": 5419.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Humans, interestingly, are able to learn on lots \nof tasks that are somewhat fuzzy to check via  ",
      "offset": 5424.24,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "having some sort of notion of self-critique — \nHow well did I do? Should I do more like that,  ",
      "offset": 5430.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "less like that? And I think the \nAIs can currently do this a bit,  ",
      "offset": 5435.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "but not amazingly. And I can imagine \nthis getting better over time.",
      "offset": 5438.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And I think a thing that we’ve seen time \nafter time is, if a given thing is the limit  ",
      "offset": 5441.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to capabilities, there is just a huge amount \nof horsepower in the broader ML community,  ",
      "offset": 5446.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "but I think broadly in the AI companies, to \nsort of push on that, whatever the limit is.",
      "offset": 5453.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: There’s been \na shift over the last year  ",
      "offset": 5459.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "from almost all or a very large fraction \nof the compute being spent during training  ",
      "offset": 5462.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "runs towards a larger fraction being spent \nduring inference or runtime compute — so  ",
      "offset": 5465.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "rather than use your compute to make the \nmodel better at predicting the next word  ",
      "offset": 5470.16,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "or more likely to be positively reinforced \nduring training, you actually just give it  ",
      "offset": 5475.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "an enormous amount of time to think about \nthe specific task that you’ve given it.",
      "offset": 5479.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "How does that change the picture around \nthe time that we’re automating a lot of  ",
      "offset": 5483.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "AI R&D? I think many people think that, on the \ninference-compute-centric paradigm, this makes  ",
      "offset": 5488.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "things go a little bit slower, because you’re \ngoing to be so limited: if it turns out that  ",
      "offset": 5494.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it requires an enormous amount of compute to run \nthe equivalent of one AI researcher, then at least  ",
      "offset": 5498.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to begin with you just won’t be able to staff \nyourself up with a million equivalents of them.  ",
      "offset": 5503.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Maybe you’ll only be able to have 100 or 1,000 \nto begin with and then it will gradually go up.",
      "offset": 5506.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: For one thing, to the \nextent inference compute wasn’t priced  ",
      "offset": 5510.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "into your prior model, then it should push you \nto having relatively shorter timelines to the  ",
      "offset": 5513.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "relevant milestones. But maybe the first time we \nhit a given milestone, it’ll be very expensive.  ",
      "offset": 5518,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "So maybe we’ll first be able to be like, we have \nthis AI that could automate the job of a research  ",
      "offset": 5522.32,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "engineer at our company for the cost of maybe \n$10 million or $20 million in compute per year.",
      "offset": 5529.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And as I was noting earlier, there is just \nactually a limited supply of compute. So even if,  ",
      "offset": 5535.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in principle, you’d be happy to automate all your \nemployees — maybe you’d be happy to automate 1,000  ",
      "offset": 5540.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "employees at $10 million a year: that would be \n$100 billion per year, which is getting close  ",
      "offset": 5545.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to the amount of compute the company even has in \nthis regime, right? So we can look at how much  ",
      "offset": 5549.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "total capital expenditures have people been \nspending? That’s roughly the same ballpark.",
      "offset": 5554.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So if it was even more extreme than that \n— maybe it’s more like $100 million per  ",
      "offset": 5558.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "year — then all of a sudden, they just \nmight not have the money to do that,  ",
      "offset": 5562.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and they might not have the compute to do that. \nAnd also, it might just not be economical.",
      "offset": 5566.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "It also might be the case that with this inference \ncompute it’s too slow. It could be that maybe it  ",
      "offset": 5571.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "would work if you could make it fast enough, but \na lot of the ways of scaling up inference compute  ",
      "offset": 5576.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "might make it serially slower, which reduces \na lot of the competitive advantage AIs have.",
      "offset": 5579.28,
      "duration": 5.188
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: What do you mean by that? Just \nbecause it has to do a whole lot of things  ",
      "offset": 5584.468,
      "duration": 3.452
    },
    {
      "lang": "en",
      "text": "one after another, it just takes a very \nlong time to actually output an answer?",
      "offset": 5587.92,
      "duration": 3.103
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, so for example, I \nthink o1 often answers questions slower  ",
      "offset": 5591.023,
      "duration": 5.697
    },
    {
      "lang": "en",
      "text": "than humans would answer those questions \nbecause it spends so long thinking. Whereas  ",
      "offset": 5596.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "previously AIs were almost strictly faster \nthan humans, occasionally o1 is now slower  ",
      "offset": 5600.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "or more often in the same ballpark. And \nwe might see inference time scaling that  ",
      "offset": 5604.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "involves more serial steps having this property. \nI think there’s various ways that this could be  ",
      "offset": 5607.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "mitigated, such that I don’t know if this is \na huge obstacle. I would probably guess not,  ",
      "offset": 5612,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "at least once optimisation has been applied, \nbut it could initially be an obstacle.",
      "offset": 5615.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So in general, I think the inference \ntime compute should expect that we  ",
      "offset": 5620.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "can see capabilities exhibited \nprior to them being economical,  ",
      "offset": 5623.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and we start seeing capabilities at \nsmall scale prior to large scale.",
      "offset": 5627.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Whereas I think a surprising thing about \nprior to the pure pretraining paradigm is  ",
      "offset": 5631.36,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "that you naively expect that, sort of at the \npoint the capability first becomes available,  ",
      "offset": 5638.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you can run truly a vast number at that \ncapability level. My sense is that this  ",
      "offset": 5641.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is still broadly going to be true, basically \nbecause I think distillation of high inference  ",
      "offset": 5646.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "compute is relatively quick, and inference \ncompute I think only gets you so far.",
      "offset": 5652.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "So I think it can get you large gains. \nBut I think you have to be quantitative  ",
      "offset": 5655.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "about how large the gains are. So if that \ncan be distilled away relatively quickly,  ",
      "offset": 5660.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "which I think we’ve seen — I think we see \npretty fast distillation progress going from,  ",
      "offset": 5663.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "for example, o1 to o3 mini — I think we saw pretty \nfast gains of both just scaling up training,  ",
      "offset": 5669.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "but also being able to distil that more \neffectively down into a smaller package.",
      "offset": 5675.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So distillation is \nwhere you make the model a lot  ",
      "offset": 5679.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "smaller and it performs almost as well. So \nyou could operate more of them in parallel,  ",
      "offset": 5682.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "or it can give you more answers more quickly.",
      "offset": 5687.6,
      "duration": 2.383
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, and I should say \ndistillation is sort of a special case  ",
      "offset": 5689.983,
      "duration": 3.217
    },
    {
      "lang": "en",
      "text": "of a broader thing that sometimes \nit’s easier, at least initially,  ",
      "offset": 5693.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to make something much cheaper than it \nis to make something much better. So  ",
      "offset": 5698,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "say we have some new capability we’ve been \ndemonstrating, we sort of push the frontier,  ",
      "offset": 5701.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "then we can bring the cost of that point in the \nfrontier down pretty quickly, is what has happened  ",
      "offset": 5704.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "historically. And distillation is where you train \na smaller model on the outputs of a bigger model.",
      "offset": 5708.4,
      "duration": 9.76
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: In the inference compute paradigm, \ndon’t those two things kind of converge? Because  ",
      "offset": 5718.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you’re saying, if you can make it a tenth \nas large and it’s almost as good as thinking  ",
      "offset": 5722.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "as it was before, then you can allow it \nto think 10 times as long, effectively?",
      "offset": 5728.64,
      "duration": 4.223
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, but of course the returns \nto inference compute potentially could diminish  ",
      "offset": 5732.863,
      "duration": 3.617
    },
    {
      "lang": "en",
      "text": "pretty quickly. This is maybe not the most \nrelevant benchmark, but for example, in ARC-AGI,  ",
      "offset": 5736.48,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "what we see is they went from ballpark $3 per \ntask or $10 per task to over $1,000 per task.  ",
      "offset": 5744.8,
      "duration": 11.68
    },
    {
      "lang": "en",
      "text": "So maybe a little over two orders of magnitude of \ncost. In those two orders of magnitude of cost,  ",
      "offset": 5756.48,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "they push the performance from I think \naround 76% or 75% up to 85%. And for humans,  ",
      "offset": 5763.68,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "I think moving through the human \nregime of 75% to 85% is not that much.",
      "offset": 5771.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I think similarly we see stuff like, if you \nsample the model 64 times, you pick the most  ",
      "offset": 5776.4,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "common answer between that, you’re seeing \nrelatively marginal improvements from that.  ",
      "offset": 5784.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "So in general, I think there’s a question of how \nefficient are these inference time strategies? I  ",
      "offset": 5788.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think that it depends on the strategy, \nbut you might hit diminishing returns.",
      "offset": 5792.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: The overarching theme here has \nbeen arguments to expect AI R&D automation  ",
      "offset": 5797.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "sooner versus later. Are there any other \nkey factors that I haven’t given you a  ",
      "offset": 5803.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "chance to talk about yet that bear on \nthat question one way or the other?",
      "offset": 5808.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I think one important \nquestion is about how there were  ",
      "offset": 5811.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "people shifting towards maybe pretraining \nis hitting a wall. So I want to dive into  ",
      "offset": 5818.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "why this might be true — to the extent \nthis is true — and also how true is this.",
      "offset": 5822.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So I think one big reason why we might \nexpect this is that it might be that  ",
      "offset": 5826.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "there are issues with data quality and data \nquantity. You know, DeepSeek recently trained  ",
      "offset": 5832.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "DeepSeek-V3 with just a tiny amount of \nmoney, and they trained it on about 15  ",
      "offset": 5839.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "trillion tokens. So it might be the case \nthat you can train on lots and lots of data,  ",
      "offset": 5843.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "but it might be that there’s kind of \nsteep diminishing returns to data quality.",
      "offset": 5849.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "There’s not a lot of public evidence \nabout the extent to which this is true.  ",
      "offset": 5853.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "But if that’s the case, then it might \nbe what happens is, once you train on  ",
      "offset": 5856.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the first 15 trillion tokens, the next 15 \ntrillion tokens are a lot less valuable.",
      "offset": 5860.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "If you imagine scaling up the DeepSeek-V3 training \nrun by a factor of 10, then based on Chinchilla  ",
      "offset": 5865.68,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "scaling laws, which is just how much should you \nscale up the size of the model and the data in  ",
      "offset": 5872.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "parallel, you would scale up the data by 3x and \nyou would scale up the model size by 3x. If you  ",
      "offset": 5875.6,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "did that, you’d be at 45 trillion tokens. So it \nmight be that if you’re at 45 trillion tokens,  ",
      "offset": 5882.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the next 30 trillion tokens would be a lot worse \n— either because it’s repeated epochs on the same  ",
      "offset": 5888,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tokens or because it’s lower quality tokens. So it \nmight be the case that you can stretch things far,  ",
      "offset": 5892.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "but the returns start diminishing \naround now because of data filtering.",
      "offset": 5898.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And this might be one reason why pretraining \nscaling maybe looked somewhat more promising  ",
      "offset": 5902.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in the past than you might think it is now, \nbecause there was worse filtering. Maybe a  ",
      "offset": 5907.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "lot of the GPT-3 to GPT-4 improvement was, as \nthey were training on more tokens, they were  ",
      "offset": 5910.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "getting more of the good tokens in there. But \nnow we’re in a regime where we can really pull  ",
      "offset": 5915.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "out the good tokens and make sure to train on \nthem, and that could yield diminishing returns.",
      "offset": 5920.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "So I think this is a reason why we \nmight expect pretraining is slower,  ",
      "offset": 5924.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "but I don’t think this is an argument that \npretraining returns would totally fall away.  ",
      "offset": 5928.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "You can just train on worse tokens, train \nfor more epochs, find methods to get more  ",
      "offset": 5932.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "juice from the same tokens and get that. \nBut you’ll see a slower rate of progress.",
      "offset": 5937.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And in addition to that, there’s also the \noption of instead of doing pretraining,  ",
      "offset": 5942.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just do way more RL. We were talking a bit earlier \nabout how maybe RL has diminishing returns. But  ",
      "offset": 5945.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "even if the returns diminish, it might still be \nthat the returns are high enough that they’re  ",
      "offset": 5952,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "higher than pretraining, where you can dump in \nexponentially more compute and you get linearly  ",
      "offset": 5955.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "more performance — but qualitatively, \nlinearly more performance is a big deal.",
      "offset": 5961.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And in addition to that, I think \nwe’re just very uncertain about  ",
      "offset": 5966.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "how true all this is. And there’s \npotentially a bunch of scaling left,  ",
      "offset": 5969.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "even if the returns are weaker than \npeople were previously thinking.",
      "offset": 5973.12,
      "duration": 3.668
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, just to try \nto say some of that back:  ",
      "offset": 5976.788,
      "duration": 2.303
    },
    {
      "lang": "en",
      "text": "pretraining is when we take a huge corpus of \ninformation, of text, to try to predict the  ",
      "offset": 5979.091,
      "duration": 5.549
    },
    {
      "lang": "en",
      "text": "next token. It seemed like scaling up the \namount of data going into that process was  ",
      "offset": 5984.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "very valuable in the past. But it’s possible \nthat that is levelling off to some extent.",
      "offset": 5989.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And part of the reason for that, you’re saying,  ",
      "offset": 5994.48,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "is I guess to some extent they’ve actually run \nout of new data that they can collect. They’re  ",
      "offset": 5996.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "getting close to grabbing all of the good text \nthat humans have actually written. But also,  ",
      "offset": 6000.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "they got better over time at filtering out \nwhat is actually the high-quality tokens,  ",
      "offset": 6006,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what is the high-quality content that they want \nthe model to be training on a lot, and what  ",
      "offset": 6010.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "stuff maybe could they discard. So presumably \nthey’re keeping in published books, textbooks,  ",
      "offset": 6013.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that kind of thing, and putting extra weight on \nthat. And then just random slop taken off the  ",
      "offset": 6019.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "internet that has no particular information \nin it, they’re managing to exclude that.",
      "offset": 6023.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And I guess you’re saying, having filtered out \nthe good stuff, the only stuff that they can add  ",
      "offset": 6027.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is really quite low quality, so they’re just not \nactually getting very much juice out of that. And  ",
      "offset": 6032.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that might help to explain why data scaling hasn’t \nbeen adding as much value now as it used to.",
      "offset": 6036,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "But you’re saying they can take the effort that \nthey were putting into that and use it to improve  ",
      "offset": 6041.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the reinforcement learning process — which is \na different way of trying to improve the model:  ",
      "offset": 6045.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "rewarding it for successfully answering \nquestions and rewarding it for having good  ",
      "offset": 6049.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "thinking in the process of doing \nthat. Have I understood right?",
      "offset": 6054,
      "duration": 3.183
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, that’s right. I think also \npeople sometimes talk about synthetic data. I like  ",
      "offset": 6057.183,
      "duration": 4.977
    },
    {
      "lang": "en",
      "text": "to think about synthetic data as a sloppy version \nof RL. What you do is you get another model,  ",
      "offset": 6062.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you get it to generate some data, and you get \nthat data to be somewhat improved from what  ",
      "offset": 6067.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it was just generating by default. So maybe \nyou only select cases where it got it right;  ",
      "offset": 6072.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "maybe you have it revise its answer, or \nyou let it try to solve the math problem,  ",
      "offset": 6075.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "then you show it the correct answer, and then \nyou have it correct its chain of thought. You  ",
      "offset": 6081.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "throw that into the pretraining corpus, \nand maybe that data is somewhat valuable.",
      "offset": 6084.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "This is in some sense similar to RL, because \nyou’re finding model trajectories that are good  ",
      "offset": 6089.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and training on them, but it might have somewhat \ndifferent properties. And you can scale this up,  ",
      "offset": 6093.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "generate a lot of synthetic data, and \nI think this will yield some returns;  ",
      "offset": 6097.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "I think this will have some improvement, \nand you can scale that up further.",
      "offset": 6100.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So let me try to make the extreme bear case. The \nextreme bear case is like: DeepSeek-V3 came out  ",
      "offset": 6105.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "recently, had a $5 million training cost. In \nparallel, we saw Grok 3 come out also pretty  ",
      "offset": 6111.52,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "recently, or DeepSeek-V3 is somewhat further in \nthe past. The difference in cost is I think about  ",
      "offset": 6119.68,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "two orders of magnitude, about a factor of 100. I \nthink DeepSeek-V3 was trained on about 2,000 GPUs.  ",
      "offset": 6126.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Grok 3 was trained on about 100,000 GPUs, very \nroughly speaking. So maybe it’s closer to a factor  ",
      "offset": 6132.88,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "of like 50 or so, maybe roughly around there. So \nwe have this factor of 50 in pretraining compute.",
      "offset": 6139.28,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "And then if you look at DeepSeek-V3, \nqualitatively at least, it’s not  ",
      "offset": 6146.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that much worse than Grok 3. So what’s \ngoing on? What explains these returns?",
      "offset": 6151.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "There’s the data scaling story, which is like, \nmaybe they were already training on the juiciest  ",
      "offset": 6156.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "15 trillion tokens and the stuff that xAI was able \nto scrounge up was not as good. And so they would  ",
      "offset": 6160.24,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "scale up the model, you scale up the data and \nmaybe they didn’t get awesome returns from this.",
      "offset": 6166.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Another story is just the returns are weak, \nwhich is plausible from the evidence we  ",
      "offset": 6171.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "have. Then the question would come down to, \ncan you switch to something more like RL?",
      "offset": 6176,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And then I think another story which \nis important — and is a big part of  ",
      "offset": 6180.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "my model — is I just think DeepSeek has \nsubstantial algorithmic advantage relative to  ",
      "offset": 6183.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "xAI, right now at least. I think DeepSeek-V3 \nprobably was just actually a better optimised  ",
      "offset": 6190.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "training run, where they were using the \nFLOP more effectively — both because of  ",
      "offset": 6195.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "better hardware utilisation and training at \nlower precision, which is a technique where,  ",
      "offset": 6199.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "instead of storing the numbers with a bigger \nrepresentation, use a smaller representation.  ",
      "offset": 6203.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "It’s a bit more efficient. You just do it with \nless accuracy but you can get similar performance.",
      "offset": 6208.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And then in addition to this, I think just \nactually having a better-tuned pipeline and  ",
      "offset": 6214,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "better architecture potentially. That said, \nan important part of why DeepSeek was able  ",
      "offset": 6220.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to have a better architecture is they \nwere doing a smaller-scale training run,  ",
      "offset": 6225.12,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "which meant that they could run more experiments \nat that scale and really iron out all the kinks.  ",
      "offset": 6227.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So maybe you’ll be able to iron out the kinks, but \nevery time you go to a larger scale for the first  ",
      "offset": 6231.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "time, you’ll run into some issues. There’s \nsome rumours of this happening at OpenAI;  ",
      "offset": 6238,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it’s plausible that xAI ran into similar stuff. \nSo I think we’re seeing probably some of that.",
      "offset": 6243.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So maybe my view is that the actual amount of \neffective compute that Grok 3 is above DeepSeek  ",
      "offset": 6249.12,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "is maybe about a factor of 10, if I had to \nguess. Maybe it’s a little more than that.  ",
      "offset": 6256.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Yeah, I think about 10 because \nit’s maybe like 50 experimental,  ",
      "offset": 6263.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "but then there’s a bunch of efficiency gains that \nshrink that lead. Plausibly I’m off base here,  ",
      "offset": 6266.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "maybe someone will call me out on \nthis, but that’s kind of my guess.",
      "offset": 6271.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "And I’m like, does this 10x scaleup \nmatch what you’d qualitatively expect?  ",
      "offset": 6273.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "You know, it is actually a decent amount better,  ",
      "offset": 6278.4,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "so I don’t think it’s totally implausible that \nif this is what a 10x scale up looks like,  ",
      "offset": 6280.4,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "another 100x could plausibly be a pretty big \ndeal. So that’s a bit more of the bull case there.",
      "offset": 6287.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Another thing that I think is happening with \nAI progress that’s important to track is a lot  ",
      "offset": 6292.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of people are like, “Grok 3 is barely better \nthan GPT-4. That’s two years ago, very slow  ",
      "offset": 6296.24,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "progress. What are we doing?” But I think an \nimportant thing is people are comparing it to  ",
      "offset": 6302.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "recent releases of a model called GPT-4 versus the \noriginal release GPT-4. So I think OpenAI’s naming  ",
      "offset": 6309.12,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "convention here has caused people to get frog \nboiled or underestimate the rate of progress.",
      "offset": 6315.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So we had the original GPT-4 release back in \n2023, near the start of 2023. That model was  ",
      "offset": 6320.32,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "pretty good. And then we saw OpenAI progressively \nrelease models they were still calling GPT-4. So  ",
      "offset": 6328.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "they called the model GPT-4 Turbo and then GPT-4o, \nand these models were all somewhat better than  ",
      "offset": 6334.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "GPT-4 — on both how good the pretrained model was \nand also on RL, or just a bit better. So we had  ",
      "offset": 6340.16,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "very incremental progress while still calling \nit GPT-4, so people are sort of missing maybe  ",
      "offset": 6347.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "roughly a little over an order of magnitude of \nprogress from GPT-4 to the best version of GPT-4o,  ",
      "offset": 6354,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "and they’re doing that comparison and missing a \nbunch of progress that happened in the meanwhile.",
      "offset": 6362.16,
      "duration": 4.948
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, it’s maybe easy to forget how \nirritating the original GPT-4 was to use. I mean,  ",
      "offset": 6367.108,
      "duration": 4.732
    },
    {
      "lang": "en",
      "text": "it was incredible given our expectations at the \ntime — I was blown away — but it was very fiddly,  ",
      "offset": 6371.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it wasn’t very good at answering questions. \nIt messed up in much more obvious ways than  ",
      "offset": 6376.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "what it does now. But I guess it’s called the same \nproduct, so now you just feel it was always GPT-4,  ",
      "offset": 6381.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and you forget that people were so fussy \nabout how you had to prompt it the exact  ",
      "offset": 6386.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "right way and you had to have expert \nprompt engineers. And that has kind of  ",
      "offset": 6391.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "fallen away as they’ve just gotten better at \nfollowing instructions much more sensibly.",
      "offset": 6393.84,
      "duration": 3.663
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. As a concrete example, \noriginal GPT-4 could maybe do agentic tasks  ",
      "offset": 6397.503,
      "duration": 4.337
    },
    {
      "lang": "en",
      "text": "very poorly. Maybe it could do five- to \n10-minute agentic tasks like half the time,  ",
      "offset": 6401.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like agentic software engineering tasks. \nAnd now GPT-4, it’s much more like an hour.  ",
      "offset": 6406.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So it’s really quite a large difference \nin terms of this downstream capability.",
      "offset": 6411.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So that’s some texture there. I think it \nwould be really nice if someone did some  ",
      "offset": 6417.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "really detailed analysis trying to map \nout all the qualitative improvements and  ",
      "offset": 6421.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "how much effective compute was in. Maybe Epoch \nshould do that, or someone else should do that.",
      "offset": 6425.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Just to back up and talk about \nthe Grok vs DeepSeek comparison and spell  ",
      "offset": 6429.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "this out for people a little bit more \nclearly. You were saying people look at  ",
      "offset": 6434.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "DeepSeek and they compare it with Grok \n3 and say Grok is somewhat better,  ",
      "offset": 6438.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "but it’s not radically better. And it looks \nlike it was trained on 50 times as much compute,  ",
      "offset": 6443.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so what tiny returns we’re getting \nfrom scaling the compute input.",
      "offset": 6446.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "But you’re saying this is a bit unfair, \nbecause I suppose DeepSeek has — because  ",
      "offset": 6450.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of I guess limitations on access to \ncompute in China — been working a lot  ",
      "offset": 6454.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "on algorithmic efficiency in order to \nget the absolute maximum juice out of  ",
      "offset": 6459.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the compute that they have available. And \nI guess Grok is in the opposite situation,  ",
      "offset": 6462.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "where they’re trying to scale and train and \ngrow incredibly quickly, and they have access  ",
      "offset": 6465.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to a tonne of compute. So they’re not worried \nabout the efficient use of compute almost at all.",
      "offset": 6469.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And you’re saying if you did a more like-for-like \ncomparison, in terms of the algorithmic efficiency  ",
      "offset": 6472.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of the training, then you would say Grok \n3 was trained on maybe only 10 times the  ",
      "offset": 6476.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "effective compute, so it’s not nearly so \nlarge a scaleup. And so the fact that the  ",
      "offset": 6480.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "improvement seems on the incremental is actually \ncloser to what we would have expected anyway,  ",
      "offset": 6483.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it’s not actually a sign that \ncompute scaleup is not useful.",
      "offset": 6487.28,
      "duration": 3.823
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I mean, I don’t know. \nIt’s definitely some evidence. And  ",
      "offset": 6491.103,
      "duration": 3.057
    },
    {
      "lang": "en",
      "text": "we should put some weight on no, it’s \nactually just 50x effective compute and  ",
      "offset": 6494.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this is what you get. And that would \nbe I think lower than I would have  ",
      "offset": 6497.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "predicted. So it’s an update there. \nBut I think broadly speaking, yeah.",
      "offset": 6500.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "It’s also worth noting that it’s not just \nthat I think DeepSeek was more focused  ",
      "offset": 6504.64,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "on efficiency. For example, I think Grok \nprobably has worse algorithmic efficiency  ",
      "offset": 6507.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "than for example OpenAI and DeepSeek do. \nMy sense is they’re trailing somewhat in  ",
      "offset": 6510.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "algorithmic efficiency but are somewhat ahead \non scaling up to very large training runs,  ",
      "offset": 6514.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so they’re just in a somewhat different \nposition. Whereas my sense is DeepSeek  ",
      "offset": 6520.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is maybe pretty competitive with OpenAI on \nalgorithmic efficiency, at least at small scale.",
      "offset": 6524.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "And then another thing that’s maybe an \nimportant factor, we don’t really know,  ",
      "offset": 6528.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is that DeepSeek could practice their training \nrun a bunch of times, because they can do that  ",
      "offset": 6532.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "small-scale training run a bunch. So it’s not \njust that they’re optimising for efficiency;  ",
      "offset": 6536.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "it’s that if you can run the training run multiple \ntimes, you might have more signal — whereas if  ",
      "offset": 6539.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you’re scaling up to a new order of magnitude for \nthe first time, maybe you just mess some stuff up.",
      "offset": 6544,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Are there any other key pieces of \nevidence that bear on this timelines question?",
      "offset": 6549.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: One thing that I think is \na pretty spooky fact is that we’re sort of  ",
      "offset": 6553.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "entering this reasoning model regime where \npeople are scaling up RL and outcomes-based  ",
      "offset": 6556.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tasks. Right now people have just started \ndoing RL. I think original o1 and R1 are  ",
      "offset": 6560.96,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "probably almost entirely trained on relatively \nnarrow short tasks with not that much compute.",
      "offset": 6568,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "We have some sense of what R1 was trained on, \nfor example. It seems like it was trained on math  ",
      "offset": 6574.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "problems, like sort of trivia or questions like \nGPQA questions, which are science questions. And  ",
      "offset": 6579.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "it was trained on maybe competitive programming or \nshort programming tasks. But it was not trained on  ",
      "offset": 6584.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "things like software engineering — tasks that \nwould take multiple steps — as far as we’re  ",
      "offset": 6590.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "aware. Probably was only trained on literally \nsingle-step tasks. Plausible they didn’t ever  ",
      "offset": 6594.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "do training that involved multiple steps, \nat least not as part of their main RL phase.",
      "offset": 6598.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So to the extent that’s true, you might \nthink that there’s a bunch of low-hanging  ",
      "offset": 6602.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "fruit to just scale up the RL paradigm \nand apply it to sort of agentic tasks.",
      "offset": 6606.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "In addition to that, there’s also just scale \nup the compute way further. You can scale on  ",
      "offset": 6612.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the diversity of environments and you can also \nscale on the amount of compute. I think Epoch did  ",
      "offset": 6616.8,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "an estimate where they thought the RL on top of \nDeepSeek-V3, which was the model R1 was based on,  ",
      "offset": 6624,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "was about $1 million. A million dollars is \nchump change in the AI industry these days.",
      "offset": 6631.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So you could plausibly be scaling it up by over \ntwo orders of magnitude within the next year. Now,  ",
      "offset": 6636.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "there might be difficulties in getting \nthat scaling, I think there might be  ",
      "offset": 6642.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "infrastructural difficulties, but in principle \nit’s possible. So to the extent that we saw big  ",
      "offset": 6645.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "gains from one order of magnitude, I’m \njust like, man, algorithmic progress,  ",
      "offset": 6650.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "a bit of tuning of this stuff, we might \nbe seeing crazy stuff in the next year.",
      "offset": 6654.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Sorry, o1 and o3 are also reasoning \nmodels that went through reinforcement learning.  ",
      "offset": 6658.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And I imagine OpenAI spent a lot more than \n$1 million on the RL for those models. So  ",
      "offset": 6662.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "why does that suggest that? I mean, we don’t \nthink o1 or o3 is radically better than R1.",
      "offset": 6669.68,
      "duration": 5.103
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: So first of all, I \nactually don’t know that I think that  ",
      "offset": 6674.783,
      "duration": 2.897
    },
    {
      "lang": "en",
      "text": "o1 and o3 were trained with much more \ncompute than R1. I think we don’t know.",
      "offset": 6677.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Some reasons why you might think it wasn’t \ntrained with that much more compute:  ",
      "offset": 6681.84,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "one thing is that I think it is actually \nlegitimately hard to scale up RL on an  ",
      "offset": 6684.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "infrastructure level, and they might not have \nthe infrastructure to do that off the bat.",
      "offset": 6688.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "The second thing is it might be hard to quickly \nscale up the number of environments, but there  ",
      "offset": 6692.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "is ultimately a scalable way to do this. So it \nmight be that there’s just a bunch of returns.",
      "offset": 6696.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "The next thing is we did see a pretty big \nperformance improvement from o1 to o3,  ",
      "offset": 6700.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which is evidence that, if that trend continues, \nthat could be pretty fast. So we’re just seeing  ",
      "offset": 6705.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "they’re RLing on relatively narrow domains, \nbut within those relatively narrow domains,  ",
      "offset": 6712.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "progress appears to be very fast. So to the \nextent that they can extend the domains they’re  ",
      "offset": 6716.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "training on, that might be broader than that, \nwe might be seeing relatively fast progress.",
      "offset": 6721.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "So I think it’s unclear. To the \nextent that o3 is saturating out  ",
      "offset": 6726.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a bunch of the lower-hanging fruit on this \nparadigm — which might be true to some extent;  ",
      "offset": 6731.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "certainly it’s pulling some of the low-hanging \nfruit — then this story would go away. But to  ",
      "offset": 6735.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the extent that we have a lot of greenfield ahead \nof us on RL, I think this is probably one of the  ",
      "offset": 6740.8,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "more compelling stories for one-year \ntimelines or things going very fast.",
      "offset": 6747.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "To be clear: I don’t expect this. I think this \nis unlikely, but I think one route is that RL  ",
      "offset": 6752.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "generalises better than you expect, it can be \nextended slightly further than you expect. And  ",
      "offset": 6756.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "maybe at the end of the year you have almost \nautomated research engineer level capability,  ",
      "offset": 6760.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "maybe somewhat below that — and then \nthings could go really crazy from there.",
      "offset": 6765.36,
      "duration": 4.308
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. You’re saying that’s \nnot likely, but it’s a possibility. And  ",
      "offset": 6769.668,
      "duration": 3.132
    },
    {
      "lang": "en",
      "text": "if it does happen, this is kind of \nthe pathway by which it would occur.",
      "offset": 6772.8,
      "duration": 2.783
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, I think the \nmost foreseeable pathway would be  ",
      "offset": 6775.583,
      "duration": 4.097
    },
    {
      "lang": "en",
      "text": "this scaled-up RL in one year. And this argument  ",
      "offset": 6779.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "was brought to my attention by Josh Clymer, \na colleague of mine. So credit to him.",
      "offset": 6782.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Is there anything more to say on \nthis timelines question before we push on?",
      "offset": 6786.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I think another \nsource of scepticism is like, sure,  ",
      "offset": 6791.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "maybe you can get these LLMs to be pretty \nsmart and pretty good at these tasks,  ",
      "offset": 6794.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "but aren’t they going to need a bunch of \nother properties in order to replace humans?",
      "offset": 6798.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "You know, they need to be able to learn \non the job. Humans, when they do tasks,  ",
      "offset": 6802.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "they’re learning how to do the very task \nthat they’re doing. And AIs have worse  ",
      "offset": 6807.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "sample efficiency, so you can throw a lot \nof stuff in context and they can sort of  ",
      "offset": 6811.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "learn how to do things better based on that, but \nit’s relatively shallow, it’s relatively weak.",
      "offset": 6816,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Another thing is that AIs currently have limited \ncontext length and might have trouble tracking  ",
      "offset": 6821.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "context over very long projects. I think the \neffective context length might be much shorter  ",
      "offset": 6826.08,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "than the actual context length because they can \ndo retrieval across the entire context length,  ",
      "offset": 6833.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "but they maybe can’t do a synthesis \nacross it as easily — because  ",
      "offset": 6837.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I feel like when I do tasks, I \nget some like vibes, level sense,  ",
      "offset": 6844.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I get better intuitions. I get some overall \nvibe of where the project is going. And I  ",
      "offset": 6849.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "think AIs maybe have trouble tracking all \nthis context, even if you throw it all in.",
      "offset": 6854.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And maybe there’s routes around that, so I want \nto talk a bit about these structural factors.",
      "offset": 6857.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "For this learning-on-the-job thing: for one \nthing, we can do research on how to make  ",
      "offset": 6862.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "within-context sample efficiency \nbetter. One route to this is,  ",
      "offset": 6867.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "rather than having this more shallow architecture \nwhere you’re processing all the tokens in parallel  ",
      "offset": 6871.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and they can sort of attend to each other… \nThere’s some ways in which the transformer  ",
      "offset": 6875.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "architecture is fundamentally shallow. I \ndon’t know how much you want to get into  ",
      "offset": 6879.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the details of that, but you could change to an \narchitecture that isn’t as fundamentally shallow.",
      "offset": 6882.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "In particular, there’s been some recent \npapers on having I would say more of our  ",
      "offset": 6886.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "current architecture, where you can process \nthe activations in a more deep serial way,  ",
      "offset": 6891.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "which might allow the AI to absorb the \ncontext, and have more of a gestalt sense,  ",
      "offset": 6896.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and learn from what’s going on \nfaster over more and more context.",
      "offset": 6900,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: What do you mean \nby fundamentally shallow?",
      "offset": 6904.64,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: What I mean \nis that if you look at a token,  ",
      "offset": 6906.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the model is sort of producing some distribution \nof probabilities on the next token. It can only  ",
      "offset": 6911.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "have so many serial steps, basically because you \nrun each layer on every token. And at each layer,  ",
      "offset": 6917.44,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "you can attend to all the previous layers, but \nyou can’t attend to a previous token at a later  ",
      "offset": 6925.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "layer. So if you imagine you have 60 layers, \nlayer 10 at token 40 can attend to layers 9  ",
      "offset": 6931.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "and before on all the previous tokens, but \ncannot attend to layer 60 on token 39. So  ",
      "offset": 6937.68,
      "duration": 10.8
    },
    {
      "lang": "en",
      "text": "that means that if the AI was getting into \nsome good insights towards the end of its  ",
      "offset": 6948.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "layers, the earlier layers of later \ntokens can’t take that into account.",
      "offset": 6952.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "And I mean, the capabilities people are \non it, as always — or maybe not as always,  ",
      "offset": 6956.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but to some extent — and are \nlooking into ways to change this.",
      "offset": 6959.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I think that the way we currently have to address \nthis is that, while the AIs are shallow in this  ",
      "offset": 6963.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "way, they are not shallow with respect to \ntokens. So if you have a reasoning model,  ",
      "offset": 6969.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "yes, it’s shallow in that sense, but also \nit can produce natural language tokens  ",
      "offset": 6973.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that are sort of its updated thinking, and it can \nkeep doing relatively deep computation via that.  ",
      "offset": 6979.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "So it can solve math problems with 50 steps \nby having all the steps in natural language,  ",
      "offset": 6985.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "even though it can’t do all the steps in this \nrelatively more serially bottlenecked forward  ",
      "offset": 6989.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "pass — that’s the term for the activations \nof the transformer which have this property.",
      "offset": 6993.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "But you might be worried that natural \nlanguage is not that good of a medium  ",
      "offset": 6998.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "for doing thinking — I sort of \nhave thoughts that aren’t in  ",
      "offset": 7002.8,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "natural language — but I think you could \nin principle have a deeper architecture.",
      "offset": 7005.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And I expect people are working on this, and \nI think this poses a bunch of safety risks,  ",
      "offset": 7009.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because we have this nice property that \nwe can look at the chain of thought and  ",
      "offset": 7013.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "get some sense of what the AI is doing \nand have some confidence in that — at  ",
      "offset": 7016.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "least potentially have some confidence in \nthat — because the AI is sort of forced  ",
      "offset": 7020.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to use the chain of thought in order \nto get this serial reasoning to work.",
      "offset": 7024,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "But if it was the case that all that reasoning \nwas latent, we just lose that property,  ",
      "offset": 7027.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and now we’re in a much more dangerous regime \nwhere the AIs could be doing subversive reasoning  ",
      "offset": 7031.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that we wouldn’t even know about, or would have \nno way of knowing about, by default at least.",
      "offset": 7035.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I didn’t totally follow that, but you \nsaid you would change how a forward pass occurs,  ",
      "offset": 7040.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "such that it’s able to do more \nsophisticated reasoning within that,  ",
      "offset": 7045.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and that creates the possibility \nthat it could engage in scheming  ",
      "offset": 7049.52,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "before it’s actually outputting any \ntokens that we’re able to assess?",
      "offset": 7051.92,
      "duration": 3.103
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, that’s basically \nright. So basically you should imagine the  ",
      "offset": 7055.023,
      "duration": 3.617
    },
    {
      "lang": "en",
      "text": "transformer is not like a looped architecture, \nit’s not recurrent. So your brain is recurrent:  ",
      "offset": 7058.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you think some thoughts, you think some more \nthoughts, and it’s fully recurrent — including,  ",
      "offset": 7062.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "as far as we know, recurrent state \nthat we can’t vocalise very easily.  ",
      "offset": 7066.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Like I think people can do reasoning \nthat they’re not able to vocalise,  ",
      "offset": 7070.32,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "some of the reasoning they can vocalise. \nAnd I think right now transformers can  ",
      "offset": 7072.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do relatively limited non-vocalised reasoning, \nand then a lot of vocalised reasoning.",
      "offset": 7077.68,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "So it might be that you can change \nthe architecture — it would be a  ",
      "offset": 7084.08,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "large change to the architecture — \nin a way that makes it so they can  ",
      "offset": 7086.08,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "do a lot more of the non-vocalised \nreasoning and more dense reasoning.",
      "offset": 7088.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And there’s some papers demonstrating this,  ",
      "offset": 7092.88,
      "duration": 1.6
    },
    {
      "lang": "en",
      "text": "like the Coconut paper from Meta. I think all \nthe existing papers are relatively weak sauce,  ",
      "offset": 7094.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "not really getting that much — sorry to the \nauthors of those papers, but that’s my sense. But  ",
      "offset": 7098.8,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "it could be that you can drive this architecture \nforward, and I think people haven’t necessarily  ",
      "offset": 7105.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "really tried that hard to get this to work \nbecause there were low-hanging fruit elsewhere.",
      "offset": 7110.64,
      "duration": 3.028
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Would this be very compute intensive \nto change the architecture in this way?",
      "offset": 7113.668,
      "duration": 2.972
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Actually the thing I just \ndescribed very naively would use exactly the  ",
      "offset": 7116.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "same amount of compute on generation. Because \nright now you have to do one token at a time,  ",
      "offset": 7120.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and you could just loop the \nactivations without that.",
      "offset": 7126,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "Now, it is more compute intensive on training \nif you’re applying gradient descent all the way  ",
      "offset": 7128.32,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "through. It makes the computation graph \nfor gradient descent more annoying to  ",
      "offset": 7135.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "do gradient descent on for some structural \nreasons I don’t know if we should get into.  ",
      "offset": 7140.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "But yeah, roughly speaking, I think it is \nmore computationally expensive at training  ",
      "offset": 7144.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "time and would be more computationally \nexpensive if you’re reading something.",
      "offset": 7150.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "There’s all these cursed technical reasons \nwhy this is true, but basically if you were  ",
      "offset": 7156.32,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "to apply this sort of looping when reading, \nthen you would only be able to read one  ",
      "offset": 7158.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "token at a time. Whereas transformers can \nprocess a whole body of text in parallel,  ",
      "offset": 7162,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so transformers are extremely fast \nat reading. So you can make it so a  ",
      "offset": 7166.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "transformer reads a document that’s like a \nmillion tokens long, I think in principle,  ",
      "offset": 7169.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "if you’re willing to scale up the compute, in \nmaybe like a minute or 30 seconds — which is very,  ",
      "offset": 7174.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "very fast. And plausibly you could even \ndo faster than that, because it basically  ",
      "offset": 7179.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is like reading the whole thing in parallel \nand there’s a smaller number of serial steps.",
      "offset": 7184.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "But if it’s reading the tokens one at a \ntime, and you have to do all the layers,  ",
      "offset": 7188,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "then it would be the same as generation speed — \ngeneration speed in a single context is more like  ",
      "offset": 7191.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "100 tokens per second by default, though \npeople have exhibited faster speeds. So  ",
      "offset": 7198.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "there’s ways in which it’s costlier, but I \nthink ultimately the costs are not that high,  ",
      "offset": 7203.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and a lot of these costs are already \nbeing borne by the reasoning paradigm.",
      "offset": 7207.6,
      "duration": 3.097
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: All right. I’m curious to \nknow: at the point that we actually are  ",
      "offset": 7210.697,
      "duration": 2.423
    },
    {
      "lang": "en",
      "text": "able to largely automate AI R&D, how do \nyou think that process would play out?  ",
      "offset": 7213.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "What would it look like? What are the \ndifferent ways that it might play out?",
      "offset": 7218.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I think there’s this big \nquestion, which is: suppose that the AI company  ",
      "offset": 7221.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "has fully automated AI R&D, even the best research \nscientists don’t add much value. Maybe they add  ",
      "offset": 7224.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "a tiny bit of value, but basically the company is \nfully automated. I think there’s been a historical  ",
      "offset": 7230.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "view of people trying to do AI forecasting that \nyou’ll get very fast progress at this point,  ",
      "offset": 7236.24,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "because the AIs are automating R&D, it can run \nfaster than it was when humans were doing it.",
      "offset": 7243.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Now there’s a question of how much faster. \nIn addition to that, there’s a question of  ",
      "offset": 7247.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "does the progress slow down? So it might \nbe that the AIs are automating AI R&D,  ",
      "offset": 7250.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "but they eat up a bunch of the low-hanging fruit, \nyou have a limited labour supply, and the progress  ",
      "offset": 7255.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then slows down because you’re applying a \nlot of labour but you can only get so far.",
      "offset": 7260.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Another question is, do you run into a lot of \nbottlenecks on compute for experiments? So you  ",
      "offset": 7264.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "have all these AI researchers, maybe way more than \nyour human researchers, but maybe they don’t have  ",
      "offset": 7270,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "much compute for experiments, and so they don’t \nhave that much of an easy time yielding progress.",
      "offset": 7276.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "There’s a question of like, how fast does \nit go initially? And does it slow down?",
      "offset": 7280.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "In addition to that, it might be even not just \nthat the progress continues at the same rate,  ",
      "offset": 7284.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it could be that progress speeds up. A way \nthis could happen is you have your smart  ",
      "offset": 7290,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "AI researchers, they do a bunch of algorithmic \nprogress. You use that algorithmic progress to  ",
      "offset": 7293.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "build a smarter AI, and that AI makes progress \ngo even faster because you can do more labour  ",
      "offset": 7297.76,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "at the same amount of compute. So even \nwith a fixed amount of compute that the  ",
      "offset": 7306.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "AI company has access to, progress could, \nin principle, go even faster and faster.",
      "offset": 7309.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Tom Davidson has done a bunch of modelling on \nthis, on do we expect progress to speed up or  ",
      "offset": 7314.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "slow down? And I’ll be stealing a bunch of stuff \nfrom him while talking about this. People have  ",
      "offset": 7320,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "called this the intelligence explosion or \nthe singularity of progress is speeding up.",
      "offset": 7325.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I think it’s important to note that my view is,  ",
      "offset": 7329.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "even if progress is slowing down, it \nmight be objectively very fast. It  ",
      "offset": 7332.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "might be like you started at a high rate of \nprogress and then it slows down over time.",
      "offset": 7336.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "So one way of breaking this up is first we \nhave to talk about the question of, how fast  ",
      "offset": 7340.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is progress initially? And then maybe we should \ntalk about, does it speed up or does it slow down?  ",
      "offset": 7346.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "And then from there we can be like, how much \nprogress do we get, say, in the first year?",
      "offset": 7351.6,
      "duration": 4.788
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. What determines the \ninitial speed at the point that you  ",
      "offset": 7356.388,
      "duration": 3.612
    },
    {
      "lang": "en",
      "text": "switch it on and are automating almost everything?",
      "offset": 7360,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: The very short answer is \nno one knows. The slightly longer answer is  ",
      "offset": 7361.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we can try to get some sense based on just having \na sense of what algorithmic progress is driven by.",
      "offset": 7368.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So algorithmic progress in AI \ncompanies is driven by two main  ",
      "offset": 7373.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "factors: labour — people working on it, \npeople thinking of better algorithms,  ",
      "offset": 7377.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "people implementing experiments — and \ncompute, using compute for experiments.",
      "offset": 7382.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I’m going to separate out actually \ntraining the final model for now,  ",
      "offset": 7388.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so we’re just going to talk about this algorithmic \nprogress. Historically algorithmic progress has  ",
      "offset": 7390.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "maybe been going up about I think over 3x per \nyear, including post-training. Maybe it’s been  ",
      "offset": 7394.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "more like 4x or 5x per year. And when I say 4x \nor 5x, what do I mean? Like what units? It’s  ",
      "offset": 7400.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "in terms of effective training compute: \nit’s like every year it’s as though you  ",
      "offset": 7405.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "could train a model that’s four or five times \nbigger with the amount of compute you have.",
      "offset": 7411.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So that’s the initial rate of progress. Now \nwhat I’ll talk about is how much faster can  ",
      "offset": 7415.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the AI researchers make this happen? This is a bit \nof a tricky question to figure out because we have  ",
      "offset": 7420.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to answer the question of, if you make that so \nthat there’s way more and higher quality labour,  ",
      "offset": 7425.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "where do we go from there? \nBecause we have these two inputs  ",
      "offset": 7430.88,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "into production — labour and compute — \nand if we massively amp up the labour,  ",
      "offset": 7433.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "do things just bottleneck in the compute, \nor can you push progress much faster?",
      "offset": 7437.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "So I think a naive way to start to do \nthe modelling is we have to be like,  ",
      "offset": 7440.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "how much labour is there, how much \nAIs, how good are they, how fast?",
      "offset": 7444.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: The compute available for \nexperiments might even decline because  ",
      "offset": 7448,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you now have to use your compute \nto run your AI researchers, right?",
      "offset": 7451.6,
      "duration": 3.583
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, for sure. I \nthink this is probably a small factor,  ",
      "offset": 7455.183,
      "duration": 2.497
    },
    {
      "lang": "en",
      "text": "because — I mean, we have no idea — \nbut my guess is that the optimum is,  ",
      "offset": 7457.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of the compute on algorithmic progress: a \nfifth on AI labourers, and four-fifths on  ",
      "offset": 7462.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "running experiments. Something roughly like that. \nSo if you’re imagining this amount of compute,  ",
      "offset": 7466.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "then you have less compute to run experiments, but \nquantitatively it’s 80% as much compute, so not a  ",
      "offset": 7471.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "big deal. So I think this is not that important \nof a part of the picture. And even if you’re  ",
      "offset": 7477.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "imagining 50/50, that’s just a factor of two. So \nif you’re like, well, you’d spend all your compute  ",
      "offset": 7480.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "running AI researchers, you have no compute \nfor experiments, that’s just an unforced error.",
      "offset": 7485.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "OK, so how many AI researchers? People have done \nvarious estimates of how many AI researchers do  ",
      "offset": 7488.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you expect at the point when you can first \nautomate things? You definitely have to have  ",
      "offset": 7494.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "enough researchers to automate everything, but for \nvarious reasons I think we expect that you’ll have  ",
      "offset": 7497.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "more researchers than you needed to automate \neverything. Because at the first point you can  ",
      "offset": 7502.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "start automating everything, you probably have \nway more labour at that same level of quality.",
      "offset": 7505.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I think inference time compute could \nmake this different. It might be that  ",
      "offset": 7511.2,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "inference time compute means that at the \nfirst time you can automate everything,  ",
      "offset": 7513.68,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "you can barely automate everything. \nBut I think this is not that likely  ",
      "offset": 7516.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to be durable. So the first time you can do \nthis, probably you can radically reduce the  ",
      "offset": 7520.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "cost pretty quickly, using stuff like we \nwere talking earlier about distillation.",
      "offset": 7524.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "So overall my sense is, I don’t know, I’ve done \nsome kind of trashy estimates. I’ll try to go  ",
      "offset": 7528,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "through one of them. Maybe we have just out \nof the box like the equivalent of 100 million  ",
      "offset": 7532.64,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "human-equivalent labourers. Because we expect \nthat we’re training on about like 10^28,  ",
      "offset": 7539.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "10^29 FLOP, which is roughly what we expect \nin the 2029, 2030 period to be available. And  ",
      "offset": 7545.52,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "then if you just do that, you get a sense of \nhow many tokens you’ll be able to generate,  ",
      "offset": 7553.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and then you try to do some rough \nconversion between tokens and human labour.",
      "offset": 7556.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Then OK, maybe it’s the case that you \nhave 100 million AI labourers. That’s  ",
      "offset": 7560.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "also taking into account things like the \nAIs do not sleep, they do not get tired,  ",
      "offset": 7564.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "they can work 24/7, and the data centre can run \n24/7. So maybe you have 100 million workers.",
      "offset": 7568.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "But then maybe you got some of \nthat with inference compute,  ",
      "offset": 7574.08,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so maybe we drop an order of magnitude because \nyou got some of that on inference compute.",
      "offset": 7576.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And then to do full automation, I think you have \nto have near like Alec Radford quality — Alec  ",
      "offset": 7580.56,
      "duration": 10.402
    },
    {
      "lang": "en",
      "text": "Radford is a famous AI researcher who has many \nof the most important capabilities insights — or,  ",
      "offset": 7590.962,
      "duration": 5.838
    },
    {
      "lang": "en",
      "text": "you know, Ilya Sutskever or whatever. To get to \nthis level of quality, maybe you have to spend  ",
      "offset": 7596.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "even more inference compute. I think it’s \nuseful to denominate in terms of units of  ",
      "offset": 7600.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "top research scientists or top research \nengineers because I think that’ll make some  ",
      "offset": 7605.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of the conversions easier. Let’s say you have like \na million Alec Radford equivalents in parallel.",
      "offset": 7608.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "But then there’s another factor, \nwhich is the AIs can run faster.  ",
      "offset": 7613.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "For example, they work at night, and \nthat gives them some advantage over  ",
      "offset": 7617.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "human labourers because they can do \nserially more experiments. So humans,  ",
      "offset": 7621.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because of the serial time, just get less done \nin a year, because they’re only working maybe  ",
      "offset": 7625.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a third of the time — or for the mortals, maybe \na fourth of the time, and some people can push  ",
      "offset": 7629.84,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "up to half of the time potentially. There’s \nsome diminishing returns on focus in hours.",
      "offset": 7636.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And then they can also just run faster because \nthey just spit out tokens faster. And there’s some  ",
      "offset": 7640.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "ways to make this go further. So maybe my overall \nsense is that maybe they’re like 5x faster at each  ",
      "offset": 7644.72,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "given point in time and then 3x faster due to \nrunning at all hours. That’s a 15x speedup.",
      "offset": 7651.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "In addition to that, I think you maybe get \nanother 2x speedup because some of the time  ",
      "offset": 7657.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you can run a dumber AI that’s much faster \nfor some subtasks. And humans can’t do this  ",
      "offset": 7663.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "as easily because it requires context switching. \nSo in principle you could imagine having humans  ",
      "offset": 7667.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "use a dumber AI for some subtasks very \nquickly and switching back and forth,  ",
      "offset": 7672.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "but you can’t exchange my brain state with \nthe brain state of the weaker AI. Whereas,  ",
      "offset": 7676.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "for example with transformers, you can just \ntotally feed the context to a weaker AI. You  ",
      "offset": 7680.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "can train the weaker AI to work with the \nsmarter AI. You can even do stuff like  ",
      "offset": 7683.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "shove the activations of the smarter AI into \nthe weaker AI and do all kinds of variable  ",
      "offset": 7687.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "compute scaling things at runtime like this. \nSo maybe that gets you another factor of two.",
      "offset": 7690.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "So now we’re up to 30x speed. And to be \nclear, these speedups are going to take  ",
      "offset": 7694.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "off. They’re going to shave off the \nnumber of parallel copies we have.",
      "offset": 7698.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "And then I think you maybe get another factor \nof two from the AIs being better at coordinating  ",
      "offset": 7701.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "than humans. So I talked about maybe they can \ninterchange context with weaker AIs. Well,  ",
      "offset": 7704.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "maybe they’re also just much better \nat coordinating across parallel tasks.",
      "offset": 7708.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Let’s think about this in terms of a speedup: they \ncan take a task that maybe would be infeasible  ",
      "offset": 7711.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "for humans to parallelise. Like sometimes, when \nyou do an eight-hour software engineering task,  ",
      "offset": 7716.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you could in principle have five people work \non it all in parallel, but you lose a lot on  ",
      "offset": 7721.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "efficiency and maybe get no serial speedup \nbecause humans are so bad at coordinating.",
      "offset": 7726.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "But maybe the AIs can have all the same context \nbecause they can fork off of the same point. So  ",
      "offset": 7731.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you start with some AI, you fork off of it. \nThere’s a nice Dwarkesh article on all the  ",
      "offset": 7735.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "structural advantages AIs might have, and it goes \ninto this sort of thing. And because you can fork,  ",
      "offset": 7739.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "maybe you get more speedup. Let’s \nsay that’s another factor of two.",
      "offset": 7745.68,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Now we’re up to 60x speed, right? So we had \nour million AIs at 60x speed. Let’s make that  ",
      "offset": 7748.08,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "50x speed, sorry. So then we have 20,000 \nAIs in parallel instances, each running at  ",
      "offset": 7758.08,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "50x speed. And all of them are as good as the top \nresearch scientists, the top research engineers.",
      "offset": 7764.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Now, how much of a speedup is this \nover, let’s say, OpenAI? Maybe OpenAI,  ",
      "offset": 7769.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "at the point when they’re building \nthis AI, will have somewhere between  ",
      "offset": 7774.48,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "2,000 to 5,000 researchers. The number \nof researchers is growing over time.",
      "offset": 7777.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So naively we have 10x more parallel instances, \nbut they’re also 50x faster. So then there’s some  ",
      "offset": 7781.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "messy conversion between how much additional \nlabour you’re putting in to what overall  ",
      "offset": 7787.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "speedup you expect, taking into account the fact \nthere’s compute bottlenecks and other things,  ",
      "offset": 7791.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and also the fact that there’s penalties \nfor running in parallel. So you know,  ",
      "offset": 7795.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "nine software engineers cannot make \na thing happen that would have taken  ",
      "offset": 7801.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "nine months in one month — you \nknow, the same for the babies —",
      "offset": 7804,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You can’t have nine women \nhave one pregnancy in one month.",
      "offset": 7807.68,
      "duration": 3.183
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. I think a thing humans \nsuffer from is parallelisation penalties,  ",
      "offset": 7810.863,
      "duration": 3.857
    },
    {
      "lang": "en",
      "text": "so the fact that the AIs run much \nfaster means in some sense they suffer  ",
      "offset": 7814.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "less from this. There’s more parallel \ncopies by a factor of maybe 10 or so,  ",
      "offset": 7817.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so they’re eating some return on that, \nbut you have also just straight-up 50x  ",
      "offset": 7822,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "more speed and also more quality. And the \nquality pushes into the parallelism as well.",
      "offset": 7826.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "So then I’m like, maybe we should really think \nof the OpenAI labour force as being as good as  ",
      "offset": 7832,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "maybe like 5x or 10x fewer people that were \nbetter. So maybe it’s as though they had like  ",
      "offset": 7838.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "200 or 400 Alec Radfords or whatever. And some \npeople think it’s even more extreme than this.  ",
      "offset": 7845.2,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "And then if it’s like they have 200 or 400 Alec \nRadfords, and we have 20,000 Alec Radfords at  ",
      "offset": 7853.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "50x speed, I think intuitively it \nfeels like things could get crazy.",
      "offset": 7858.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "But the question is just how much does the \ncompute bottleneck? And people disagree a  ",
      "offset": 7862.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "lot on this. We really don’t know. No one \nhas run the experiments that we would need  ",
      "offset": 7865.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to find out how big of a deal this is. We \njust have surveys and vibes and whatever.",
      "offset": 7868.96,
      "duration": 4.388
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: What experiment would you run?",
      "offset": 7873.348,
      "duration": 1.452
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Here would be my favourite: \nGoogle is known for having a large number of  ",
      "offset": 7875.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "different teams, and I think probably, at some \npoint, someone messed up the compute allocation  ",
      "offset": 7881.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to some team, or there was some exogenous shock \ncausing the compute allocation to some team  ",
      "offset": 7886.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to be lower than it was supposed to be or to be \nhigher than it was supposed to be. And then you  ",
      "offset": 7891.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "could look at the question of when that happened, \nhow much did progress speed up or slow down? That  ",
      "offset": 7894.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "would give you some sense of what the marginal \nproduction function looks like, what the marginal  ",
      "offset": 7899.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "returns to compute look like. That would give \nus at least some sense of what’s going on.",
      "offset": 7902.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "In the AI case we’re operating very far off of the \nhuman margin, because we have so much more labour.  ",
      "offset": 7906.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So the situation might be very structurally \ndifferent, but that would give us some sense.",
      "offset": 7912.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I think my dream is that someone goes to \nGDM or whatever and scrounges up the data  ",
      "offset": 7915.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "on all the natural experiments \nthey must have been running,  ",
      "offset": 7922.32,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "and does a very economist-style analysis on \nthat and figures out what the local returns  ",
      "offset": 7924.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "look like. That only tells us so much, because \nit’s only the returns around the current regime.",
      "offset": 7930,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I think even better than that might be things \nlike having a small team of researchers who  ",
      "offset": 7933.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "you give way less compute to. You know, if \nGoogle is really into running experiments,  ",
      "offset": 7939.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "not just giving us data — I pick on \nGoogle just because that’s the example,  ",
      "offset": 7945.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but other companies could do this — they could \ntake some of their researchers and split them  ",
      "offset": 7949.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "into two groups or more groups, and have \nsome of the researchers get way less compute,  ",
      "offset": 7954,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "get more like the amount of compute we expect \nour AI researchers to have, for instance, and  ",
      "offset": 7959.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "see how much slower they operate. If it’s way, way \nslower, that would give us a sense on the regimes.",
      "offset": 7963.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I think this is a trickier thing to understand \npartially because there might be adaptation time.  ",
      "offset": 7969.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "So it might be like you put the humans in this \nregime with way less compute. Initially they’re  ",
      "offset": 7975.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "way slower, but they sort of learn to work \nwithin those limits. And I think the AIs will  ",
      "offset": 7979.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have lots of time to learn to work within those \nlimits because they’re running so much faster.",
      "offset": 7982.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Regardless, my sense is that the initial speedup, \nthe instantaneous speedup of your AI researchers  ",
      "offset": 7989.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "will be… When I take all these things into account \nand try to do the math on the production function,  ",
      "offset": 7995.92,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "maybe I do something like a Cobb-Douglas \nproduction function with some factors and  ",
      "offset": 8005.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we try to have a parallelism penalty that \nwe apply both to the humans and the AIs,  ",
      "offset": 8008.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and we normalise the labour force. \nThere’s a bunch of messy stuff here.",
      "offset": 8011.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "I think that the inside view, fully extrapolating \nfrom the current frontier econ model,  ",
      "offset": 8014.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "spits out numbers like, depending \non exactly how you do the estimate,  ",
      "offset": 8020.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "I think maybe my favourite picks of the \nconstants are around 50x faster. I think  ",
      "offset": 8023.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "this is probably overestimating the speed. \nThat’s 50x faster than the current rate of  ",
      "offset": 8028.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "progress. The current rate of algorithm \nprogress is somewhat over half an OoM per  ",
      "offset": 8032.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "year. So naively that would get you some truly \nungodly instantaneous rate of 25 OoMs per year.",
      "offset": 8035.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "I think now I think people might be \nlike, “Come on, the thing you’re saying,  ",
      "offset": 8042.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that’s ridiculous.” I think I’m like, yeah, \nthe thing I’m saying, it’s a bit ridiculous.  ",
      "offset": 8046,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "So maybe we want to discount this view of the \ninstantaneous speedup a lot. So rather than having  ",
      "offset": 8049.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the equivalent of 50 years of progress, or \none year of progress in one week, I’m like,  ",
      "offset": 8055.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "maybe that’s too crazy, and then I end up dividing \ndown to maybe it’s more like 20x rate of progress,  ",
      "offset": 8060.88,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "maybe even a bit lower than that at the \ninstantaneous speed, as sort of my median guess.",
      "offset": 8069.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "And again, I think this is like wild \nspeculation; we’re extrapolating from  ",
      "offset": 8073.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "a regime that we don’t even understand to a \nwildly different regime. No one knows. So it  ",
      "offset": 8079.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "could be much faster; it could be much slower. \nOr it can’t be that much faster, I guess.",
      "offset": 8085.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So at the point that you fully \nautomate it, it sounds like it could be  ",
      "offset": 8088.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "blisteringly fast at that moment. But I \nguess one way of making this sound less  ",
      "offset": 8095.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "crazy is you say it starts out incredibly fast \nand then starts flattening out quite quickly,  ",
      "offset": 8098,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so you only have one week of this level of \nblistering progress. I guess the alternative  ",
      "offset": 8102.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is it could go even faster — you were \nsaying that is also a live possibility.",
      "offset": 8107.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Do you want to explain what evidence would bear on \nwhether we expect it to slow down versus speed up?",
      "offset": 8111.04,
      "duration": 4.543
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. Another thing on that is \nI’ve been doing this instantaneous analysis, and I  ",
      "offset": 8115.583,
      "duration": 4.257
    },
    {
      "lang": "en",
      "text": "think people might be like, “Sure, maybe you would \nget that if you drop these in. But it’ll be more  ",
      "offset": 8119.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "gradual in the leadup to this.” So for one thing, \nin short timelines, I think we should expect the  ",
      "offset": 8124,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "gap between substantial acceleration (but not \nfull automation) and full automation is small  ",
      "offset": 8127.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "in calendar time. And if you’re expecting that the \nsubstantial automation speeds things up, then it’s  ",
      "offset": 8132.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "even smaller in calendar time. So I think this \ninstantaneous analysis is at least non-crazy.",
      "offset": 8136.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Regardless, there’s a question of, does it \nspeed up or does it slow down? So if we had  ",
      "offset": 8140.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this 10x or 20x progress rate, then we’d be \ntalking like the instantaneous rate is like  ",
      "offset": 8145.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "five or 10 OoMs in one year — orders of \nmagnitude of effective compute progress.",
      "offset": 8150.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Now, does it speed up or does it slow down? \nThis analysis is even trickier. There’s a lot  ",
      "offset": 8155.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "more factors. The basic story is: you have your \nAIs, they do a bunch of algorithmic research,  ",
      "offset": 8159.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "they train a new AI, that new AI is \nsmarter and better and more efficient  ",
      "offset": 8165.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "(or some mixture of those attributes), that \nnew AI does even faster algorithmic research.  ",
      "offset": 8171.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "But the returns have also diminished, \nright? So returns have diminished,  ",
      "offset": 8177.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but also you have smarter AIs and you \ncan get either superexponential progress,  ",
      "offset": 8180.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "exactly exponential progress — exactly exponential \nprogress is it continues at the same rate, so the  ",
      "offset": 8186.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "progress was already exponential in effective \ncompute — or you can have decaying progress.",
      "offset": 8192.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So the way that we try to get an estimate for \nthis is we try to have a sense of… We’ve been  ",
      "offset": 8198.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "dumping more and more human labour over \ntime into things like computer vision,  ",
      "offset": 8203.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "LLMs — and we can try to get a vague sense of, \nwhen we’ve been dumping in all those researchers,  ",
      "offset": 8209.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "how much has that accelerated progress? And \nthen we do a bunch of adjustments for the AI  ",
      "offset": 8213.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "case. So maybe we have a conversion from dumping \nin AI labour to how much more effective compute  ",
      "offset": 8218.88,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "that keeps getting you — which we also needed the \nsame sort of analysis to get the initial speedup.",
      "offset": 8227.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "And if we have that, then the question is,  ",
      "offset": 8232.56,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "how much more labour does each effective \ncompute get us? So each 10x of effective  ",
      "offset": 8234.72,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "compute gets us more labour. It also gets us more \ncapable labour, and then that can loop back in.",
      "offset": 8239.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So there’s a bunch of math here. And again, I \nthink we have plausibly even more uncertainty  ",
      "offset": 8244.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "on this component than the previous component. \nBut I think the best estimates indicate that at  ",
      "offset": 8249.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "least initially, progress will speed \nup rather than slow down. Probably.",
      "offset": 8253.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I mean, you can roll to disbelieve on this or \nwhatever, but I think if you just do the naive  ",
      "offset": 8260,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "analysis, you try to account for the factors — \nyou try to account for the compute bottlenecks,  ",
      "offset": 8264.399,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you try to account for parallelism issues, you \ntry to account for all this stuff — it turns  ",
      "offset": 8267.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "out that it just makes the AIs more capable \nand smarter fast enough that — very roughly,  ",
      "offset": 8272.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "on our very trashy models — we expect \nprogress to speed up reasonably quickly.",
      "offset": 8279.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So if this is right,  ",
      "offset": 8284.24,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "we’re blowing past human level incredibly \nquickly into a totally superhuman regime  ",
      "offset": 8286.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in terms of just how capable these models \nare in general. Am I understanding right?",
      "offset": 8292.56,
      "duration": 4.463
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Well, it’s kind of \ncomplicated. So there’s a question of  ",
      "offset": 8297.023,
      "duration": 2.737
    },
    {
      "lang": "en",
      "text": "how many orders of magnitude of progress you get, \nand there’s a question of how much does it matter?",
      "offset": 8299.76,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "So I’m throwing around this effective \ncompute unit, and I think this has a  ",
      "offset": 8308.24,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "problem of being a very econ-brain unit of \nanalysis. People are like, “OK, come on,  ",
      "offset": 8311.439,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "how much is an order of magnitude of effective \ncompute even? How much does that matter?”",
      "offset": 8315.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "We were talking about that earlier in \nthe discussion about how much is an  ",
      "offset": 8320.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "order of magnitude of effective compute between \nDeepSeek-V3 and Grok. And we also care about,  ",
      "offset": 8325.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "does the qualitative trend continue? What is the  ",
      "offset": 8330.24,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "right qualitative trend? How superhuman \ncan things even get? This sort of thing.",
      "offset": 8332.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I want to spend a bit more time on one \nthing about the accelerating progress,  ",
      "offset": 8337.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which is that you should expect that the returns  ",
      "offset": 8341.6,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "eventually must diminish. So another key factor \nis limits: progress can only go on for so long,  ",
      "offset": 8346.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "right? You cannot get 100 OoMs of \nprogress, because at some point —",
      "offset": 8353.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: The laws of physics bite.",
      "offset": 8357.8,
      "duration": 2.502
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, the laws of \nphysics bite. And also more importantly,  ",
      "offset": 8360.302,
      "duration": 2.658
    },
    {
      "lang": "en",
      "text": "perhaps the amount of compute you have \nbites, right? So you only had so much  ",
      "offset": 8362.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "compute. I’ve been talking about all \nthis analysis on a fixed compute base.",
      "offset": 8366.88,
      "duration": 10.245
    },
    {
      "lang": "en",
      "text": "So here’s a naive bear case for efficiency: \nimagine that you got 10 OoMs of progress on  ",
      "offset": 8377.125,
      "duration": 1.274
    },
    {
      "lang": "en",
      "text": "algorithmic efficiency. As of now, that \nwould naively imply you could train  ",
      "offset": 8378.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "DeepSeek-V3 for… So 10 OoMs is a factor of 10 \nbillion, and it was trained for $5 million, so  ",
      "offset": 8384.72,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "that’s like less than a cent. Well, whatever. It’s \nvery little, right? Yeah, a bit less than a cent.",
      "offset": 8394.08,
      "duration": 12.08
    },
    {
      "lang": "en",
      "text": "So like, OK, come on, are you going to be \nable to train DeepSeek-V3 for less than  ",
      "offset": 8407.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "a cent? It’s like you’re doing seconds on an \nH100, less than seconds on an H100. I’m like,  ",
      "offset": 8410.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "come on guys. How many parameters can \nthat be? So in that same ballpark,  ",
      "offset": 8415.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it’s like, how many numbers can you even \nmultiply? You can only have touched so many  ",
      "offset": 8420,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "parameters, right? If you just do all \nthis, I think you should be very sceptical.",
      "offset": 8425.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Now, one thing that’s worth noting is I \nthink limits up might be different than  ",
      "offset": 8430.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "limits down. So it might be that you can \nonly make things so much more efficient,  ",
      "offset": 8434.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "but you can make things scale \nbetter. So DeepSeek-V3 is maybe  ",
      "offset": 8439.359,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "five orders of magnitude more efficient, \nfour orders of magnitude more efficient,  ",
      "offset": 8446.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "even in the limits. I think probably a little \nmore than that, but somewhere around there.",
      "offset": 8449.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "But maybe you can make DeepSeek-V3… You know, \nthere’s some scaling trends. There’s a question  ",
      "offset": 8456.08,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "of how good would DeepSeek-V3 be if we scaled \nit up by five orders of magnitude? Maybe we can,  ",
      "offset": 8462.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "for DeepSeek-V3-level compute, go up five orders \nof magnitude on the DeepSeek-V3 scaling law.",
      "offset": 8468.319,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "Does this make sense? This is a bit tricky.",
      "offset": 8473.84,
      "duration": 1.268
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: No. Maybe explain \nthat like I’m a bit of an idiot.",
      "offset": 8475.108,
      "duration": 3.434
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: OK, OK. So for every model, \nthere’s some way to naively scale this up  ",
      "offset": 8478.543,
      "duration": 4.816
    },
    {
      "lang": "en",
      "text": "both on RL and data and whatever. Now, \nthere’s a bit of complexity around this,  ",
      "offset": 8483.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and it’s a bit of a tricky analysis, but \nwe could say, how good would we have been  ",
      "offset": 8486.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "if we took the DeepSeek-V3 algorithms, and \nwe scaled them up five orders of magnitude,  ",
      "offset": 8491.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and adapted to that amount of compute, \nand didn’t mess up that training run?",
      "offset": 8495.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "It might be the case that it’s much easier \nto replicate what we would have been able to  ",
      "offset": 8499.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "do with five orders of magnitude more compute \nthan DeepSeek-V3 than to make it five orders  ",
      "offset": 8504.64,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "of magnitude more efficient. It’s a bit tricky \nto do anchoring, because a lot of the limits  ",
      "offset": 8512,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I’m defining in different ways, but minimally I \nthink 10 orders of magnitude up on the DeepSeek-V3  ",
      "offset": 8516.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "efficiency — as in you’re doing with DeepSeek-V3 \ntraining compute as well as if you were doing  ",
      "offset": 8521.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "10 orders of magnitude more compute on those \nsame algorithms — seems very plausible to me.",
      "offset": 8528.319,
      "duration": 4.308
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, what does that imply?",
      "offset": 8532.628,
      "duration": 1.691
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I think there’s a bunch \nof ways of doing the analysis. I’ll try  ",
      "offset": 8534.319,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to do a quick version of the analysis that’s \nvery quick and dirty, but gets us something.",
      "offset": 8537.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So we trained our human-level AIs, or \nour AIs that are broadly at the level  ",
      "offset": 8541.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "of top human research scientists, \nlet’s say in 2029 or 2030. Maybe  ",
      "offset": 8547.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the training run was around like 10^28 FLOP, \nand we produce something at the level of humans.",
      "offset": 8555.6,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "We have some very trashy estimates of human brain \nlifetime compute, like how much compute the human  ",
      "offset": 8562.479,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "brain is using in a lifetime. Like if you had \nthe algorithms of the human brain, and you were  ",
      "offset": 8567.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "able to do that, how long would it take to train a \nhuman who’s as good as the best human scientists?  ",
      "offset": 8573.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "And our sense is it’s around 10^24. So that’s four \norders of magnitude of efficiency right there,  ",
      "offset": 8578.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "because we trained something that was competitive \nwith humans for more compute than humans. So four  ",
      "offset": 8583.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "orders of magnitude on that, maybe it’s \na bit less, but ballpark. Makes sense?",
      "offset": 8587.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Not completely. It’s four \norders of magnitude of what exactly?",
      "offset": 8592.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: So we were able to train \nsomething that’s as good as a human,  ",
      "offset": 8597.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but it required us to use four orders of \nmagnitude more compute. And so you might think,  ",
      "offset": 8602,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "at the very least, we can get to the point \nwhere we can train a human for 10^24 FLOP,  ",
      "offset": 8609.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "or a human-level model. And then we have four \norders of magnitude of room above that to expand.",
      "offset": 8614.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Four orders of \nmagnitude more to expand?",
      "offset": 8618.4,
      "duration": 2.863
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: As in, imagine that we advanced \nthe algorithm so we can now train a human for  ",
      "offset": 8621.263,
      "duration": 3.777
    },
    {
      "lang": "en",
      "text": "10^24 FLOP. Now we have an additional \nfour OoMs of scaling available to us.",
      "offset": 8625.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK. Because originally \nthe training was so inefficient.",
      "offset": 8629.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, that’s \nright. And an important thing  ",
      "offset": 8633.2,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "here is that in shorter timelines \nwe must be imagining we have more  ",
      "offset": 8635.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "efficient algorithms — whereas in longer \ntimelines, where more compute is required,  ",
      "offset": 8639.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "presumably we have less efficient algorithms. \nThere’s some interesting dynamics here.",
      "offset": 8642.56,
      "duration": 3.588
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Why is that?",
      "offset": 8646.148,
      "duration": 0.812
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Imagine that we produce \nfull automation of an AI company in 2028,  ",
      "offset": 8646.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "2029, 2030: then we must be \noperating around like 10^28,  ",
      "offset": 8652.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "10^30 training runs. On the other hand, \nimagine we’re doing it in 2040 or 2045:  ",
      "offset": 8657.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "plausibly we could be having quite a \nfew more orders of magnitude of compute.",
      "offset": 8663.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "I haven’t done the math, but it could \nbe like four more orders of magnitude.  ",
      "offset": 8667.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I think like 2050 at least maybe you \ncould get four more orders of magnitude  ",
      "offset": 8673.84,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "of compute by you’ve scaled the fabs, you \nmade them cheaper, you have new techniques,  ",
      "offset": 8676.479,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "maybe you’re using optical computing \nand more speculative approaches. So if  ",
      "offset": 8679.76,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you’re training the human-level AIs for like \n10^36 FLOP, then you have way more headroom.",
      "offset": 8684.479,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. So we know that we can achieve \nthe level of efficiency that the human brain has.  ",
      "offset": 8691.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "And if it was taking 12 orders of magnitude more \ncompute to reach the equivalent performance as a  ",
      "offset": 8696.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "human, well then you have an enormous amount \nof potential algorithmic efficiency gain.",
      "offset": 8703.84,
      "duration": 3.718
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: In the limit.\nRob Wiblin: In the limit. OK, yeah.",
      "offset": 8707.558,
      "duration": 1.864
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Anyway, I think \na reasonable objection here is:  ",
      "offset": 8709.422,
      "duration": 2.977
    },
    {
      "lang": "en",
      "text": "we don’t know what the human brain is doing; \ncan we even produce that level of compute? Also,  ",
      "offset": 8713.359,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "isn’t it the case that evolution did a huge \namount of optimisation? Maybe that required a  ",
      "offset": 8719.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "bunch of compute. And so even if in principle \nyou could have the human algorithm — which  ",
      "offset": 8723.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is like the human genome — then finding the \nhuman genome itself would take a huge amount  ",
      "offset": 8727.359,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "of research compute, because we have to run \nsimulations equivalent to what evolution was.",
      "offset": 8732.08,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "So this is some scepticism. I’m \nbasically going to put this aside  ",
      "offset": 8735.359,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "and not address it, and just be like, \nI’m sceptical. But I think it’s going  ",
      "offset": 8737.92,
      "duration": 8.248
    },
    {
      "lang": "en",
      "text": "to be somewhere in between this picture. \nBut I don’t think that’s a huge discount.",
      "offset": 8746.168,
      "duration": 0.086
    },
    {
      "lang": "en",
      "text": "And there’s another thing, which is that we \ndon’t think that humans are at the limit of  ",
      "offset": 8746.254,
      "duration": 1.185
    },
    {
      "lang": "en",
      "text": "efficiency. There’s many reasons why humans \nare inefficient: they have physical brains  ",
      "offset": 8747.439,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "under a bunch of constraints, they can only \ndo local training algorithms for structural  ",
      "offset": 8752,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "reasons about propagation of information \nbackward — so human brains basically can’t  ",
      "offset": 8757.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "do backprop very directly and they can \nonly do more local learning algorithms. So  ",
      "offset": 8762.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "our current best local learning algorithms are \nmuch worse than SGD [stochastic gradient descent].",
      "offset": 8766.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Of course, evolution had more time \noptimising these local learning algorithms,  ",
      "offset": 8771.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so maybe that’s a big factor. Maybe \nthat’s even like two orders of magnitude.",
      "offset": 8775.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "And then there’s a bunch of other factors. \nAnother thing is that, within humans,  ",
      "offset": 8778.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "performance on the task of AI R&D varies wildly. \nThere’s a huge variation between the median human  ",
      "offset": 8784.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and the best human on ability to do this. Some \nof that’s training; some of that’s genetics;  ",
      "offset": 8790.08,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "some of that’s upbringing from things other \nthan direct training, like training on other  ",
      "offset": 8797.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "tasks. So maybe that gives us another bunch of \nheadroom. So you can imagine making 300-IQ humans  ",
      "offset": 8803.52,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "without having much bigger brains, but just by \nhaving more efficient brains — like with more  ",
      "offset": 8810.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of the mutations removed, possibly more \nthan that. So that gets you some more.",
      "offset": 8814.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "And there’s a long list of considerations \nlike this. Things like, maybe the AIs are  ",
      "offset": 8818.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "able to sync mind states more effectively, \nwhich gives them more coordination. Maybe  ",
      "offset": 8821.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "they can generate much better training \ndata. I’m going to miss some of these.",
      "offset": 8826.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "But anyway, I think when we add all \nthese up, my guess is a median of like  ",
      "offset": 8830.24,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "nine OoMs up. We talked about the distinction \nbetween up and down. That’s also going to apply  ",
      "offset": 8838.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "on humans. So maybe you can’t train a human \nfor nine OoMs less FLOP — you can’t train a  ",
      "offset": 8842.399,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "human for 10^15 FLOP, which would be like a \nsecond on an H100. But maybe you can train  ",
      "offset": 8847.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "something that’s like nine OoMs better \nthan a human with human-level compute.",
      "offset": 8853.84,
      "duration": 3.828
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. OK, that might be the \nmost technical or challenging to follow  ",
      "offset": 8857.668,
      "duration": 5.212
    },
    {
      "lang": "en",
      "text": "half hour of the show. I was very happy to \nlet you go, so people can get a sense of  ",
      "offset": 8862.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just how many moving pieces there are, and \nalso how much thought has gone into this.",
      "offset": 8867.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "It sounds like there are quite a lot of people \ntrying to forecast this time and trying to sketch  ",
      "offset": 8871.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "out the different plausible trajectories \nand the different factors that weigh on it.",
      "offset": 8876.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Is it possible to bring it back to something \nthat someone with less technical understanding  ",
      "offset": 8880,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "can grasp? Is the bottom line that people \nthought about it a lot, it’s quite hazy,  ",
      "offset": 8885.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "there’s a lot of factors at play? It’s possible \nthat at peak AI R&D, things could be moving very  ",
      "offset": 8889.92,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "fast, and it is plausible that it could even \nspeed up as the AIs get better. It’s also  ",
      "offset": 8896.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "possible it could slow down. We should just \nbe open to all of these different options?",
      "offset": 8903.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I would have said surprisingly \nlittle time has been spent thinking about this,  ",
      "offset": 8906.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually. As far as I can tell, maybe \naround four full-time-equivalent years  ",
      "offset": 8910.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "have been spent very directly on trying \nto build these models to forecast takeoff  ",
      "offset": 8916.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and applying those models to forecast \ntimelines — maybe even less than this.",
      "offset": 8921.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "And there’s a bunch more work that Epoch has done \non trends and other analysis that I’m pulling in,  ",
      "offset": 8927.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but this type of analysis I’m talking \nabout, these takeoff dynamic analyses,  ",
      "offset": 8931.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "I think maybe at this point it’s more like \neight equivalent years. I was not pricing in  ",
      "offset": 8937.68,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "a few Epoch papers. Maybe the Epoch people are \ngoing to call me out for underrating their hard  ",
      "offset": 8943.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "work. But they’ve done a bunch of the background \nwork of the statistics I’m pulling in and a lot  ",
      "offset": 8949.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of the trends I’m pulling in. But I think there \nhasn’t been that much work on the analysis here.",
      "offset": 8953.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So I’m like, come on, eight person years? This is \nmaybe the most important question, one of the most  ",
      "offset": 8957.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "important questions. I don’t expect us to get that \nmuch signal on it, but it does have a huge effect,  ",
      "offset": 8964,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and it is a very big disagreement. I think \na lot of people are sort of expecting that  ",
      "offset": 8968.479,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "progress peters out around human level, or it just \nis relatively slow or it’s mostly bottlenecked on  ",
      "offset": 8972.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "compute. And the question of whether this \nis true or not makes a huge difference.",
      "offset": 8978.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "One argument also that I didn’t mention \nthere, which I sort of just brought up,  ",
      "offset": 8984,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "was I was sort of imagining we just are just \nlike flying through this human regime with  ",
      "offset": 8986.479,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "no important discontinuity or kink around \nhuman level. But it could in principle be  ",
      "offset": 8990.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that we were able to get to the human level \nvia sort of piggybacking or fast following  ",
      "offset": 8995.04,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "on human behaviour. My guess is this isn’t \nthat big of a factor, and it’s just like  ",
      "offset": 9000.479,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a one-time cost that’s not that big. But I \nthink we shouldn’t get too much into that.",
      "offset": 9005.439,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Anyway, we had how fast is the initial speedup? \nDoes it speed up or does it slow down? And we had  ",
      "offset": 9010.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "what are the limits? Eventually it must \nslow down, right? So we have this model  ",
      "offset": 9015.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "in which it maybe even initially is speeding \nup and it’s like continuing to speed up and  ",
      "offset": 9021.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it’s following this sort of hyperbolic \ntrajectory where it’s going to infinity  ",
      "offset": 9025.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "in finite time. Eventually that must end \nas you’re starting to near the limits. We  ",
      "offset": 9029.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "don’t know when it starts to slow down. \nIt’s going to slow down at some point.",
      "offset": 9032.399,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "But I think the all-considered \nmodel is: things might be very fast,  ",
      "offset": 9036.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "it could happen quite quickly. I think the \nestimates imply my median is maybe we’re  ",
      "offset": 9041.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "hitting about five or six orders of magnitude \nof progress in a year of algorithmic progress.",
      "offset": 9046.319,
      "duration": 6.629
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: And it’s a bit hard \nto know exactly what qualitative  ",
      "offset": 9052.948,
      "duration": 2.491
    },
    {
      "lang": "en",
      "text": "impact that will have on how smart \nthe models will actually feel to us.",
      "offset": 9055.439,
      "duration": 3.744
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, for sure. That is another \nbig source of uncertainty. I’ve been doing this  ",
      "offset": 9059.183,
      "duration": 5.617
    },
    {
      "lang": "en",
      "text": "very econ-brain analysis, where I put everything \nin these effective compute units, and I’m doing  ",
      "offset": 9064.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "a bunch of quick conversions back and forth \nto labour supply to get a bunch of things.",
      "offset": 9068.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "There’s a bunch of different ways of \nvisualising this progress. I should also  ",
      "offset": 9073.439,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "say there’s a few factors I’m neglecting, like \nyou’re scaling up compute during this period  ",
      "offset": 9078.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and a bunch of other minor considerations. \nThese are priced into my five or six  ",
      "offset": 9082.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "OoMs of progress in a year. But I don’t \nthink we should get too much into that.",
      "offset": 9086.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Regardless, I don’t know… I have this sort \nof intuitive-to-me model of initial rate,  ",
      "offset": 9091.04,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "speedup/slowdown limits, and then limits \naffect, even if it’s initially speeding up,  ",
      "offset": 9100.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "when it starts slowing down again. Does \nthis model sort of make sense to you?",
      "offset": 9106.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, I think that \nmakes sense. Those are kind of  ",
      "offset": 9109.359,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "the three big stylistic factors \nthat you’re playing around with.",
      "offset": 9112.08,
      "duration": 2.543
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. And then there’s a bunch \nof tricky details about, suppose the limit is this  ",
      "offset": 9114.623,
      "duration": 4.097
    },
    {
      "lang": "en",
      "text": "many OoMs away. The factor of is it speeding \nup or is it slowing down, and how does that  ",
      "offset": 9118.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "change over time, you might think it’s initially \nspeeding up and the time at which that stops is  ",
      "offset": 9124.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "very close to the end of the limits. Or could \nbe that it’s more continuous across the limits,  ",
      "offset": 9128.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and this will have a big effect on \nhow many orders of magnitude you get.",
      "offset": 9132.479,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "But regardless, I think that’s the sort of \nintuitive model. I think people should play  ",
      "offset": 9135.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with this. I think playing with this sort of \nmodel is interesting. It’s pretty clear that  ",
      "offset": 9139.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "this is both a simplified model and also has an \ninsane number of moving parts that we have very  ",
      "offset": 9146.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "little data to estimate. We’re sort of fitting \nthis model in a massively extrapolated regime  ",
      "offset": 9151.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "from trashy data. You know, what can \nwe do? And including data as trashy as  ",
      "offset": 9156.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "guessing how much more efficient \nyou can be than the human brain.",
      "offset": 9160.8,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "So as you were saying, we’re very uncertain and \nwe have huge error bars. My view is you’re going  ",
      "offset": 9163.2,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "to get some initial speedup and you’re also going \nto be able to pile in more compute. So maybe the  ",
      "offset": 9172.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "25th percentile is like you get somewhat \nfaster than previous years of progress,  ",
      "offset": 9178.08,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "or the 25th percentile is plausibly just \nbarely faster than preexisting progress.  ",
      "offset": 9184.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And I think that like 80th or 75th \npercentile might be like completely insane.",
      "offset": 9189.76,
      "duration": 6.868
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So this is the question of, at \nthe point that we are able to automate things,  ",
      "offset": 9196.628,
      "duration": 2.892
    },
    {
      "lang": "en",
      "text": "how much does it actually speed up what the \ncompany was doing? And you’re saying the 25th  ",
      "offset": 9199.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "percentile of this is maybe it’s kind of just \nat roughly the same rate as it was before — but  ",
      "offset": 9203.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the 75th percentile, which is not even an extreme \noutcome, it’s radically speeding up the research.",
      "offset": 9208.479,
      "duration": 4.863
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. At least quickly. It might \nbe that the initial speedup is not that high,  ",
      "offset": 9213.342,
      "duration": 3.137
    },
    {
      "lang": "en",
      "text": "but the speedup increases over time \nand diminishes relatively slowly.",
      "offset": 9216.479,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "And also, I’ve been talking \nabout this one-year timescale,  ",
      "offset": 9220.56,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "but I think on a lot of the modelling most \nof the progress might happen in the first six  ",
      "offset": 9222.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "months — because you’ve already started to hit \nthis diminishing returns regime kind of quickly.",
      "offset": 9226.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It’s like the faster you \ngo, the sooner you start hitting limits.",
      "offset": 9230.399,
      "duration": 4.143
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, that’s right. And you \nknow, it could go pretty different ways. Anyway,  ",
      "offset": 9234.543,
      "duration": 4.658
    },
    {
      "lang": "en",
      "text": "I’ve been saying six OoMs of progress: what \ndoes that even mean? What does this look like?",
      "offset": 9240.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “OoM” is “order \nof magnitude” for anyone who  ",
      "offset": 9244.24,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "didn’t pick that up but is still with us.",
      "offset": 9246.08,
      "duration": 1.583
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I’m sorry. I love OoM. \nWhat a good term. It’s one of my favourites.",
      "offset": 9247.663,
      "duration": 4.656
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Onomatopoeia.",
      "offset": 9252.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, yeah, it’s great. \nAnyway, so six OoMs, how much is that? So  ",
      "offset": 9256.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "it’s roughly two GPTs: there was broadly \nan OoM between GPT-2 and GPT-3 in terms of  ",
      "offset": 9260.24,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "roughly 10x algorithmic progress and \naround 100x compute. Very roughly speaking,  ",
      "offset": 9269.439,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "maybe a bit less than this. And something \nbroadly similar between GPT-3 and GPT-4.",
      "offset": 9275.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So the naive qualitative model \nwe can do is we can be like,  ",
      "offset": 9280,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "how big was the GPT-3 to GPT-4 gap? And then \nwe can be like, we have two of those gaps:  ",
      "offset": 9283.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "two more GPTs. And then I’m like, what does \nthat mean? I think the two GPTs analysis  ",
      "offset": 9288.8,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "makes me feel more reassured. I’m like, two \nGPTs, is that even that bad? I mean, come on.",
      "offset": 9294.479,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I think another framing is how many years of AI \nprogress is this? I think six OoMs is about five  ",
      "offset": 9299.6,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "years of AI progress, very roughly speaking, \nmaybe four. So it’s like going from, in 2020,  ",
      "offset": 9306.399,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "we had just gotten GPT-2 XL to \nnow. So it’s like the gap between —",
      "offset": 9312.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: But I guess it’s hard \nto know intuitively what that means,  ",
      "offset": 9318.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "because GPT-2 was pretty useless for anything.",
      "offset": 9320.88,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Or GPT-3 was pretty close. Yeah, \nso that’s pretty useless. I think perhaps the  ",
      "offset": 9323.439,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "thing that we even have less grounding on is how \nmuch does progress above the human range mean?  ",
      "offset": 9329.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Like, at the point we’re starting this, the \nAIs are matching the best human professionals.  ",
      "offset": 9336.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Maybe they’re not quite as efficient, not \nquite as smart, but via various tricks  ",
      "offset": 9339.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and whatever, they can basically match human \nprofessionals. Now, how much further do you go?",
      "offset": 9343.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So there’s the GPTs. I think another notion \nis trying to convert from the GPTs to some IQ  ",
      "offset": 9348.08,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "or some notion of that. People have \nwildly different intuitions here,  ",
      "offset": 9357.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but if we imagine that we were \nstarting at maybe 150-IQ AIs,  ",
      "offset": 9361.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "because they were able to automate everything… \nAgain, IQ is kind of a trashy unit.",
      "offset": 9366.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It feels like it wasn’t \ndesigned for this purpose too.",
      "offset": 9370.399,
      "duration": 2.143
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Oh no. Nothing was designed \n[for it]. We’re abusing the shit out of these  ",
      "offset": 9372.543,
      "duration": 6.816
    },
    {
      "lang": "en",
      "text": "econ models also. I’ve been doing all this \necon-style analysis on econ models that  ",
      "offset": 9379.359,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "definitely weren’t designed with this \nregime in mind. And growth economics,  ",
      "offset": 9385.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "which is the field that we’re pulling from, \nis just not that good of a field — sorry,  ",
      "offset": 9390.319,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "no offence to the growth economists \nout there, but there’s just not that  ",
      "offset": 9394.64,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "many people working on it, and we have a lot \nof uncertainty over a lot of things there.",
      "offset": 9396.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Anyway, so there’s the two GPTs. How many IQ \npoints is that? This intuition makes me think  ",
      "offset": 9401.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "maybe a GPT is a bit over 50 IQ points or \nsomething. And so we go from 150 to 250,  ",
      "offset": 9407.439,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and also we have many more parallel copies and \nthey can run faster. These are some intuitions.",
      "offset": 9411.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Another intuition is how much better are \nthey in terms of human professionals?  ",
      "offset": 9418.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Here’s a trend that I think is good to track: \nif you look at programming competitions,  ",
      "offset": 9423.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "we’ve been seeing progress in terms of \nranking on those programming competitions  ",
      "offset": 9428.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "over 2024. At the start, maybe the AIs \nwere like 20th percentile, roughly. And  ",
      "offset": 9431.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "then they were at 50th percentile, \nand then I think o1 was like 75th,  ",
      "offset": 9437.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "o1-preview was a bit over 90th percentile, and \nthen o3 was 99.8th percentile or something.",
      "offset": 9441.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "So there’s some relationship between orders of \nmagnitude of compute or algorithmic progress  ",
      "offset": 9449.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and what rank ordering you have among \nhuman professionals. At the point when  ",
      "offset": 9455.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we’re starting this crazy stuff, maybe the AIs \nare broadly like hundredth or tenth best human  ",
      "offset": 9460.24,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "professional rank ordering, and then we have \nthese six orders of magnitude of progress.",
      "offset": 9467.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I think that there’s some conversion we could \ntry to have between orders of magnitude and  ",
      "offset": 9472.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "ranking — where it’s like every order of magnitude \nmaybe means that you’re like 10x better on this  ",
      "offset": 9476.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "ranking. So instead of being the thousandth \nbest, you’re the hundredth best. My guess is  ",
      "offset": 9480.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "it’s a bit over. Like an OoM of effective compute \nis somewhat more than an OoM of this sort of rank  ",
      "offset": 9486.399,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "ordering. I think no one has done this analysis \nvery carefully. Someone should do it. Suppose  ",
      "offset": 9491.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that it’s a little over an OoM, then maybe with \nour six OoMs, we get eight OOMs of rank ordering.",
      "offset": 9495.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: And so pretty \nsoon you’re below one, right?",
      "offset": 9501.52,
      "duration": 2.847
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, you’re below \none, so now we’re extrapolating this  ",
      "offset": 9504.367,
      "duration": 1.633
    },
    {
      "lang": "en",
      "text": "thing. One way to put this is we sort \nof quickly get to just human parity,  ",
      "offset": 9506,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and then maybe we have a bit more than six OoMs \nleft still. Or say the literal best human parity,  ",
      "offset": 9509.76,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "and then we have another six OoMs of progress. So \nit’s as big of a gap as going from the millionth  ",
      "offset": 9518.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "best human at a thing — because million is six \nOoMs — to the best human at a thing. So it’s like  ",
      "offset": 9523.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we took the best human and we did the equivalent \nof going from the millionth best to the best.",
      "offset": 9528.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And that’s another qualitative intuition. \nI don’t know how much that tells you,  ",
      "offset": 9531.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but there’s some extrapolation there \nyou can do. This is brazenly ripped  ",
      "offset": 9535.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "from Daniel Kokotajlo’s way \nof thinking about the OoMs.",
      "offset": 9540.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Now, we also have uncertainty on this point. So I \nthink if it’s more like each OoM is two OoMs, then  ",
      "offset": 9544.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "it’s more like you’re over a billion x \nbetter than the best human professionals.",
      "offset": 9549.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You’ve gone from the \nbillionth best to the very best,  ",
      "offset": 9557.28,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "and then you’ve made that leap again.",
      "offset": 9559.52,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, that’s right. Which \nis quite a large gap. Importantly, I think  ",
      "offset": 9561.84,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "that you can’t understand the billionth best in \nthe human range because it doesn’t make sense  ",
      "offset": 9570.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to generalise out of a career. Like, who’s the \nbillionth best person at software engineering?",
      "offset": 9574.8,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: This is a silly question.",
      "offset": 9581.6,
      "duration": 1.695
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: This is a silly question. I \nthink the millionth best person at software  ",
      "offset": 9583.295,
      "duration": 4.625
    },
    {
      "lang": "en",
      "text": "engineering is now at least somewhat meaningful. \nWe can start working with that. And more niche  ",
      "offset": 9587.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "human professions, it’s less meaningful. So I \nthink we have this kind of insane gap from that.",
      "offset": 9594.08,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "Another intuition I like is thinking about \nhow big is the labour supply. So in a lot  ",
      "offset": 9600.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of the econ analysis I was doing earlier \nabout does progress speed up or slow down,  ",
      "offset": 9606.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "an important question was how much does each \norder of magnitude of effective compute get you  ",
      "offset": 9611.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "in terms of more cognitive juice to throw \nat problems in terms of how much can you  ",
      "offset": 9615.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "feed into the labour part \nof the production function?",
      "offset": 9620.24,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "One way to do it is we can just be like, how good \nis an order of magnitude of compute relative to  ",
      "offset": 9626.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "how many orders of magnitude of parallel workers \nis that equivalent to? My understanding is that  ",
      "offset": 9631.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "our best available estimates are like \nevery order of magnitude of effective  ",
      "offset": 9637.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "compute is like two orders of \nmagnitude of parallel workers.",
      "offset": 9639.92,
      "duration": 3.828
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: And this is because having lots of  ",
      "offset": 9643.748,
      "duration": 1.132
    },
    {
      "lang": "en",
      "text": "people work in parallel is \nactually quite inefficient?",
      "offset": 9644.88,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, the AIs are faster, \nmore capable, and you get more parallel  ",
      "offset": 9649.12,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "copies. So when you scale up effective \ncompute, at least in the current paradigm,  ",
      "offset": 9656,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you have more efficient AIs that are smarter, \npotentially. So you can basically be scaling  ",
      "offset": 9658.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "all these factors in parallel and you can be \nscaling whichever factor is most effective.",
      "offset": 9664,
      "duration": 4.068
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. So you get to allocate \nyour compute budget between having more of  ",
      "offset": 9668.068,
      "duration": 6.172
    },
    {
      "lang": "en",
      "text": "them and having smarter ones in \nthe most efficient combination.",
      "offset": 9674.24,
      "duration": 3.983
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. And you can sort \nof gear your training runs to be like,  ",
      "offset": 9678.223,
      "duration": 2.256
    },
    {
      "lang": "en",
      "text": "are we training a bigger model, or are we training \na smaller model? There’s some tradeoffs between  ",
      "offset": 9680.479,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "all these things that’s kind of complicated. \nThere’s ways to trade off inference compute and  ",
      "offset": 9686,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "training compute as well. But all considered, like \nI’m going to do denomination and parallel copies.",
      "offset": 9689.359,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So we started with like 20,000 geniuses running \nat 50x speed, and then we had six orders of  ",
      "offset": 9693.439,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "magnitude — but we’re actually doubling that, so \nwe have 12 orders of magnitude. That’s a trillion.  ",
      "offset": 9700.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "So now we’re going to 20 quadrillion running \nat 50x speed. Now, I think this is perhaps a  ",
      "offset": 9704.479,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "bit misleading because an important component is \nthe parallelism bottlenecks. But if you are used  ",
      "offset": 9710.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to thinking in terms of human organisations, then \nI think you should think of 20 quadrillion humans  ",
      "offset": 9714.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "running at 50x speed is right, and like the amount \nof stepping on toes is sort of analogous to that.",
      "offset": 9720.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "And then in practice, maybe the thing \nI actually more expect is maybe it’s  ",
      "offset": 9725.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "qualitatively closer to like a billion or \n2 billion humans that are way smarter than  ",
      "offset": 9730.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "humans. So like 250-IQ humans running at 100x \nspeed. Probably my numbers are a bit sloppy,  ",
      "offset": 9736.8,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "but I think that’s more \nlike the intuition I expect.",
      "offset": 9743.439,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "And then you could do the same \nin terms of the professionals.  ",
      "offset": 9745.52,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "You have to be careful not to overcount. Part \nof the mechanism via which they’re much better  ",
      "offset": 9752.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "at human professions is having more of them, \nso all these things are going to funge across,  ",
      "offset": 9755.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "but maybe it’s as though you go from millionth \nbest human to best human and then million above  ",
      "offset": 9759.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "that. We sort of do the same extrapolation. \nMaybe it’s as though we have millions of them  ",
      "offset": 9764.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "at least running at 100x speed, which is \nlike, OK, this is fucking insane, right?",
      "offset": 9770.56,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "For example, very quickly the AIs will \ndo more cognitive progress on problems  ",
      "offset": 9777.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "than has been applied in human history \nby huge margins. And very naively they’re  ",
      "offset": 9782.08,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "running at 100x speed. So it’s like if \nthere’s something that you could have  ",
      "offset": 9787.439,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "done purely in the domain of cognition, \nlike purely without access to the world,  ",
      "offset": 9790.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that would have taken humans 10 years — it \nwould have taken a team of 100 humans 10 years:  ",
      "offset": 9794.479,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "OK, boom, happens in a tenth of a year with \njust a tiny fraction of the labour supply.",
      "offset": 9799.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "So I think we should start being like, \nwhat kinds of crazy technologies will be  ",
      "offset": 9805.28,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "spit out of this process? There’s \na bunch of things that I think  ",
      "offset": 9809.439,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "could in principle be accelerated massively \nthat we haven’t even tried that hard at.",
      "offset": 9812.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Effort has been spent on atomically precise \nmanufacturing. Not that much effort has  ",
      "offset": 9817.76,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "been spent on nanobots, nanosystems, whatever. I \nthink Drexler, who originally thought about this,  ",
      "offset": 9821.439,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "thought it was going to be very little labour, so \nthought that it might be very easy for humans to  ",
      "offset": 9827.439,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "do, but very little effort has been applied. \nSo it seems very plausible that you come out  ",
      "offset": 9831.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "of this regime very quickly with like, you \nknow, atomically precise manufacturing that  ",
      "offset": 9834.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "allows for massively increasing the compute \nsupply and all kinds of other crazy things.",
      "offset": 9840.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "That’s like one example. I \nthink emulated minds and a  ",
      "offset": 9843.359,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "tonne of other things could happen pretty quickly.",
      "offset": 9845.92,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: To wrap up, it’d be good to \ndo a bit of discussion of what you think  ",
      "offset": 9847.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "are the highest priority things for the sorts of \npeople who listen to this show to be working on,  ",
      "offset": 9853.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "given your enormous distribution of predictions \nabout different ways that things could run.",
      "offset": 9858.399,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "On the technical side, what are \nsome of the things that stand  ",
      "offset": 9865.6,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "out to you as particularly neglected and useful?",
      "offset": 9867.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: I think more people should \ndo control work relative to what’s going  ",
      "offset": 9870.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on. My colleague Buck is probably going to \ntalk more about what that would look like,  ",
      "offset": 9874.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so I won’t go into too much detail \nthere. I think that’s now a lot less  ",
      "offset": 9877.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "neglected than it was, but still seems \ngood to have more people working on.",
      "offset": 9880.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I think more people should spend their time \nthinking about and working on how would you  ",
      "offset": 9884,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "train AIs that are wise and are able to make \ndecisions that are better than the decisions  ",
      "offset": 9889.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we would make. Basically how would you get to \na point where you have ruled out the models  ",
      "offset": 9895.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "plotting against you? How would you make \nthem be the AIs that you are happy to hand  ",
      "offset": 9901.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "over to? This is a much more conceptually \nthorny area, and I’m planning on spending  ",
      "offset": 9904.319,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "more time thinking about what research \nprojects should be spun out of that.",
      "offset": 9908.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "There’s a decent amount of work on what I would \ncall “model internals” that people could work on.  ",
      "offset": 9911.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Maybe this is falling somewhat \nunder control, but things like  ",
      "offset": 9916.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "probing to make it so that we can detect if \nthe models are taking misaligned actions. How  ",
      "offset": 9919.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "would we do that? How would we know \nif it worked? This sort of thing.",
      "offset": 9924.24,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "There’s some work on maybe \ndecoding uninterpretable  ",
      "offset": 9927.439,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "reasoning. Suppose models learn to reason \nsteganographically in their chain of thought —",
      "offset": 9930.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So this is that \nthey’re scheming against you,  ",
      "offset": 9935.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "but you can’t tell. It’s kind of encoded.",
      "offset": 9938,
      "duration": 1.743
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. Steganographically is \nthey’re using different symbols and you don’t  ",
      "offset": 9939.743,
      "duration": 2.897
    },
    {
      "lang": "en",
      "text": "understand what’s going on. Or maybe it looks like \ngibberish to you. Maybe it looks like one thing,  ",
      "offset": 9942.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "but actually is another thing. Or maybe it’s \nlike they’re doing a lot of latent reasoning.  ",
      "offset": 9945.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "We talked earlier about maybe models doing a \nlot of reasoning in a latent way rather than  ",
      "offset": 9950,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in natural language, and being able \nto decode that reasoning in some way,  ",
      "offset": 9953.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and trying to figure out some methods for training \ndecoders on that that work somewhat and give  ",
      "offset": 9958.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "us some sense of what the AI is actually \nthinking, I think could be pretty helpful.",
      "offset": 9962.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "In addition to this, there’s a bunch \nof different work on demonstrating  ",
      "offset": 9967.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "AIs are very capable now. I talked some \nabout how I think there’s overhang in the  ",
      "offset": 9971.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "level of capability that has been demonstrated. \nI think demonstrating that current systems are  ",
      "offset": 9976,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "capable and future systems are very capable \nseems probably somewhat good at the margin,  ",
      "offset": 9980.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "because I’m worried about situations where the \nworld is not very prepared for what’s going on.",
      "offset": 9984.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So things like demonstrating high levels of \nautonomous cybercapability, which I think is  ",
      "offset": 9988.64,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "a sweet spot of both being directly relevant \nto a lot of threat models people are already  ",
      "offset": 9995.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "considering, and also is not that far from \nthe scenarios that we’re worried about,  ",
      "offset": 9999.439,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "which do involve a lot of autonomous cyber \nactivity, and that is actually a key part  ",
      "offset": 10004.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "of the threat model. So it maybe bridges \nthis divide in a nice way. And especially  ",
      "offset": 10009.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "focusing on what is the best demo that we \nwill ever be able to achieve in this realm.",
      "offset": 10013.52,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "Another big area that people should work \non is what I would call model organisms:  ",
      "offset": 10020.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "trying to produce empirical examples \nof a misaligned model to study how  ",
      "offset": 10027.68,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "likely this is to arise and present \nevidence about that. So things like,  ",
      "offset": 10035.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "does misalignment arise in XYZ circumstance? \nDoes reward hacking emerge and how does it  ",
      "offset": 10039.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "generalise? Things like the alignment faking \npaper and various continuations of that.",
      "offset": 10044.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I think part of the hope here is gathering \nevidence. Part of the hope here is just having  ",
      "offset": 10048.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "something to iterate on with techniques. \nEven model organisms which aren’t very  ",
      "offset": 10053.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "convincing to the world or maybe don’t produce any \nevidence about misalignment one way or another,  ",
      "offset": 10057.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "if they’re analogous enough that we can experiment \non them, that could be potentially very useful.",
      "offset": 10061.84,
      "duration": 5.828
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because you can try to \ndevelop countermeasures that work  ",
      "offset": 10067.668,
      "duration": 2.332
    },
    {
      "lang": "en",
      "text": "in the model organism case that \nthen hopefully will transfer?",
      "offset": 10070,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. I think a key \ndifficulty with alignment overall is  ",
      "offset": 10072.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "normally we solve problems with empirical \niteration. And to the extent that a lot  ",
      "offset": 10075.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of our alignment failures make our tests \ndeceptive, then if we can build some way  ",
      "offset": 10079.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to get around that in advance — or just \nbe ready to build it in the last minute,  ",
      "offset": 10083.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and then do a bunch of iterations in those kinds \nof cases — I think that could be pretty helpful.",
      "offset": 10088,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, that was what seems \nmost promising on the technical side.  ",
      "offset": 10093.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Are there things that stand out \non governance or other angles?",
      "offset": 10097.12,
      "duration": 3.823
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah, I think there’s a variety \nof different room for non-technical interventions  ",
      "offset": 10100.943,
      "duration": 4.577
    },
    {
      "lang": "en",
      "text": "that seem pretty good. It’s hard for me \nto have very strong views on these things,  ",
      "offset": 10105.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "because I don’t spend that long thinking \nabout it. There’s a bunch of work.",
      "offset": 10109.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "We’ve gone through a lot of conceptual points \nhere, and I think there’s room for people working  ",
      "offset": 10112.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "on just figuring out all these details, trying to \nhave a better understanding of takeoff dynamics,  ",
      "offset": 10117.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "trying to have a better understanding \nof different considerations other than  ",
      "offset": 10122.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "misalignment that might come up. Things \nlike how worried should we be about human  ",
      "offset": 10125.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "power grabs? How worried should we be about \nother issues? I think there’s some of that.",
      "offset": 10128.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I think there’s a decent amount of work on just  ",
      "offset": 10134.16,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "acting as an intermediary between the \nvery in-the-weeds technical AI safety  ",
      "offset": 10137.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and the world of policy, and trying \nto translate that to some extent.",
      "offset": 10142.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "There’s a bunch of specific regulation that could \npotentially be good. I think making the EU Code  ",
      "offset": 10146.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of Practice better seems good. The EU AI Office \nis hiring, so you could work on that. I think  ",
      "offset": 10151.52,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "there’s maybe other strategies for \nregulation that could actually be good.",
      "offset": 10160.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I think there’s some stuff related \nto making coordination more likely  ",
      "offset": 10164.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "or assisting with coordination that \ncould be pretty helpful. Things like  ",
      "offset": 10168.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "improving the compute governance regime so that \nthe US and China can verify various statements  ",
      "offset": 10174.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "made about the current training process. I don’t \nhave a strong view on how promising that is,  ",
      "offset": 10178.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "but I think surprisingly few people are working \non that, and that’s surprisingly uncoordinated.  ",
      "offset": 10182.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So maybe someone should get on that, because \nit could potentially be a pretty big deal.",
      "offset": 10186.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "In addition to that, I think just having a lot \nof people in positions where they’re just trying  ",
      "offset": 10192,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to provide technical expertise; they’re in a \nposition where they’re currently building skills,  ",
      "offset": 10197.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "they’re currently getting ready to have \nmore direct impact; and will later,  ",
      "offset": 10203.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "as stuff gets crazier, be \nready to do something then.",
      "offset": 10207.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Another one is just generic defence. So we \ntalked earlier about AI takeover scenarios.  ",
      "offset": 10213.359,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "A bunch of the AI takeover scenarios \nI was saying involve, for example,  ",
      "offset": 10218.479,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "bioweapons. Just generically improving \nrobustness to bioweapons seems like it  ",
      "offset": 10224.319,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "helps some. It’s complicated the extent to \nwhich it helps, but I think it helps some.",
      "offset": 10229.439,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Similar for making the world more robust to \nAIs hacking stuff. I think it helps some.  ",
      "offset": 10234,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I think it’s probably less \nleveraged than other things,  ",
      "offset": 10238.88,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "but interventions that steer more resources \nto those things seem good from a wide variety  ",
      "offset": 10240.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of perspectives and potentially different \nassumptions about misalignment. I think those  ",
      "offset": 10246.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "things maybe make a lot of sense even with \nno misalignment risks at all, for example.",
      "offset": 10250.8,
      "duration": 3.428
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, I guess \nbecause misuse is also an issue.",
      "offset": 10254.228,
      "duration": 2.715
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Yeah. And in addition to that, \nthere’s a bunch of different work on security  ",
      "offset": 10256.943,
      "duration": 4.577
    },
    {
      "lang": "en",
      "text": "that could be good. So some of the threat models \nI was discussing involve various outcomes, like  ",
      "offset": 10261.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the model exfiltrating itself. They involve the \nmodel being internally deployed in a rogue way,  ",
      "offset": 10266.399,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "where it’s bypassing your security and potentially \nusing a bunch of compute it’s not supposed to.",
      "offset": 10272.24,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "I think pushing back the time at which these \nthings happen via security mechanisms seems  ",
      "offset": 10278.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "good. Also security to prevent human actors from \nstealing the model could potentially increase the  ",
      "offset": 10282.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "probability that there is delay and increase \nthe probability of less racing, more caution.",
      "offset": 10289.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: What are the priorities for your \nresearch over the next couple of months?",
      "offset": 10294.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Right now I’m doing a decent \namount of planning and conceptual work, and then  ",
      "offset": 10299.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the plan for that is to then spin off a bunch of \nprojects. So I’m thinking through questions like:  ",
      "offset": 10303.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "What should you do in this scenario where your \nresponsible AI company is three months in the  ",
      "offset": 10309.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "lead and there’s very low political will — what’s \nthe full list of potential alignment measures that  ",
      "offset": 10313.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "might be promising? What is the route you \nshould take? How should people prioritise?",
      "offset": 10319.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And then trying to maybe make both \nconcrete recommendations and also  ",
      "offset": 10323.2,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "just figure out things at the margin, basically  ",
      "offset": 10326.479,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "with the aim being that it just seems like \nRedwood has had reasonable luck just trying  ",
      "offset": 10330.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to make overall plans and then spinning off \nsome insights from this. I think control came  ",
      "offset": 10334.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "out of this. I’ve had some updates based on \nthinking this through more. That’s one thing.",
      "offset": 10338.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Then I’m working on some demonstrations, or \ntrying to look into how big of a deal reward  ",
      "offset": 10343.76,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "hacking is right now. Just recently we’ve seen RL \nwork when it wasn’t really doing as much before,  ",
      "offset": 10350.399,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "and wasn’t being scaled as far. So one natural \nquestion is: how much reward hacking are  ",
      "offset": 10356.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we getting? How egregious might that be? In \nwhat situations is it more or less egregious?",
      "offset": 10361.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "There’s been some prior work on this, but I \nthink now that this is really going quite far,  ",
      "offset": 10366.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we might expect that we potentially see very \negregious reward hacking, and we might see  ",
      "offset": 10370.399,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "threat models that are just driven purely by \nreward hacking all the way to very egregious  ",
      "offset": 10376.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "outcomes. In principle, doing things like \nmassively deluding humans or potentially  ",
      "offset": 10380.479,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "trying to seize control of assets could \npotentially be generalised from reward hacking.",
      "offset": 10385.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "There’s also stories via which reward hacking \nleads to very pernicious misalignment,  ",
      "offset": 10389.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "because you started with an AI that was \ndisobeying your instructions, and that sort  ",
      "offset": 10394,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of crystallised in some way that involves the \nAI conspiring against you, even if it’s not as  ",
      "offset": 10397.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "directly for something that we potentially have \nas much control over as the oversight signal.",
      "offset": 10402.96,
      "duration": 3.908
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: All right, well, you \nand your colleagues write quite a  ",
      "offset": 10406.868,
      "duration": 3.292
    },
    {
      "lang": "en",
      "text": "lot on the Alignment Forum and you have \na Substack. What’s the address for that?",
      "offset": 10410.16,
      "duration": 4.382
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: The Substack? \nredwoodresearch.substack.com. Our  ",
      "offset": 10414.543,
      "duration": 13.297
    },
    {
      "lang": "en",
      "text": "Substack does not have a short URL, I’m afraid.",
      "offset": 10427.84,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So if you want to pull on some of the \nthreads of the things you’ve talked about here,  ",
      "offset": 10429.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "there’s a pretty high chance that \nthere’s some article or blog post  ",
      "offset": 10433.2,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "out there that you or a colleague have \nwritten that could elaborate a bit more.",
      "offset": 10435.359,
      "duration": 3.74
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: For sure.",
      "offset": 10439.099,
      "duration": 0.248
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess it’s a huge to-do list that \nyou laid out there. If there’s people in the  ",
      "offset": 10439.347,
      "duration": 3.853
    },
    {
      "lang": "en",
      "text": "audience who are able to help out with that, \nthen I guess time is short. We could use all  ",
      "offset": 10443.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "hands on deck to help push forward all of these \nagendas and hopefully make things go better.",
      "offset": 10448.399,
      "duration": 3.903
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: For sure.",
      "offset": 10452.302,
      "duration": 0.578
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: My guest today has \nbeen Ryan Greenblatt. Thanks so  ",
      "offset": 10452.88,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "much for coming on The 80,000 Hours Podcast, Ryan.",
      "offset": 10454.88,
      "duration": 1.903
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt: Thanks for having me.",
      "offset": 10456.783,
      "duration": 4.977
    }
  ],
  "cleanText": "Rob Wiblin: Today I have the pleasure of speaking with Ryan Greenblatt. Ryan is the chief scientist at Redwood Research and the lead author on the paper “Alignment faking in large language models” — which has been described as probably the most important empirical result ever on the topic of loss of control of artificial intelligence. Thanks so much for coming on the show, Ryan.\n\nRyan Greenblatt: Yeah, it’s great to be here.\n\nRob Wiblin: Let’s start by talking about the best arguments for and against a software-based intelligence explosion in the relatively near future — by which I’m thinking maybe under four years, approximately. What do you think is the chance that we’ll be able to largely automate AI R&D, or maybe roughly automate a full AI company as it exists today, within the next four years or so?\n\nRyan Greenblatt: I think the probability of that capability existing — not necessarily being used, not necessarily being fully deployed — is about 25%, and then maybe about 50% if we extend it to like eight years.\n\nRob Wiblin: I think many people hear that and they would wonder, is this just pure speculation? How can we form realistic or grounded expectations about this kind of thing? I’d be interested to know what are the key pieces of evidence that bear on you making a prediction or forecast like that. What is a key piece of evidence that makes you think it is plausible at all?\n\nRyan Greenblatt: To start, I think a good place to look is, what is the current level of AI capabilities? How close does it feel? People have obviously wildly different intuitions about how close we are in some objective sense. I think currently the situation is: the AIs are getting better and better at math; they’re sort of marching through the human regime in math. They’re able to do roughly about one hour or one and a half hours of sort of isolated software engineering tasks — by that I mean a task that would take a human roughly an hour and a half. And they’re sort of getting better at a variety of different skills. So I think we’re at a pretty objectively impressive regime — and importantly, a much more objectively impressive regime than we were at two years ago, and certainly much more impressive than we were at four years ago. A naive starting point for where we will be in four years is: where were we four years ago? Where are we now? Trying to qualitatively extrapolate from here to there. This is a bit of a sloppy perspective. Like, I think maybe what I would have said two years ago would have been that we had GPT-3, now we have GPT-4: what’s the gap there? Let’s sloppily extrapolate that forward and get a sense from there. I think now we can do better, because we have not just GPT-3; we have GPT-3.5, we have GPT-4, we’ve had the progression since GPT-4 in roughly the two years since then. So over that period, we’ve gone from the AIs just barely being able to do agentic tasks to having some reasonable amount of ability to succeed at that, some ability to recover from mistakes. So over this period, GPT-4 could maybe complete some agentic tasks that would take humans like five or 10 minutes; it could sort of understand how to use the tools. I think GPT-3.5 basically couldn’t even understand the tools in agentic scaffolds; GPT-4 could understand these things. And then, over that period we’ve gone from like, “can understand the tools and can maybe do it some of the time” to “can do tasks 50% of the time that takes a human software engineer an hour and a half.” There’s sort of a trend line of how fast we’ve been marching through that regime. Over 2024 at least it’s been pretty fast progress, starting from basically nothing at all to an hour and a half, with doubling times of like, I’m stealing a bunch of content from METR, which I think maybe there’ll be a podcast with Beth which covers a bunch of the same content — but we’ve seen doubling times that are fast enough that we’d expect the AIs doing eight-hour tasks or 16-hour tasks in maybe a little less than two years, which is pretty fast progress. And then from there, if you’re into the regime where they’re doing weeklong tasks, I think you can maybe get pretty close to automating the job of a research engineer with some schlep on top of that.\n\nRob Wiblin: OK, so you would say that objectively they’re pretty impressive in what they can do right now. Some people have a more sceptical reaction to that. Is there anything that you could point to to be clearer about what they’re able to do and what they’re not able to do? And maybe answer people who would say, “Sometimes I use these tools and they seem very stupid, or they don’t seem able to do things that I would expect them to be able to do, or they produce a whole lot of reasoning and I find errors in it”?\n\nRyan Greenblatt: So I’d say my kind of qualitative, vibesy model is: the AIs are pretty dumb, they used to be much dumber, they’re getting smarter quickly, and they’re very knowledgeable. So I think a lot of what people are interfacing with is that we’ve got these systems that have got some smarts to them: they can really figure out some pretty general situations OK, and especially with reasoning models they’re pretty good at thinking through some stuff. In addition to that, they’re very knowledgeable, which means that there’s a misleading impression of how overall general and how adaptable they are that people notice. And this is a lot of what people are reacting to. There’s sort of an overoptimistic perspective, which I would say is characterised by this chart Leopold [Aschenbrenner] had where he’s like PhD-level intelligence, or like people are like PhD-level intelligence — and then some people respond to that being like, “PhD level intelligence? Come on, it can’t play tic-tac-toe.” Maybe that’s no longer true with reasoning models, but directionally, you know, it can’t play tic-tac-toe; it can’t respond to relatively novel circumstances. It gets tripped up by this stuff. Now, I think we have to discount some of how it’s getting tripped up by this stuff, because I think a bunch of these things might be better described as cognitive biases than lack of smarts. Like there’s a bunch of things that humans systematically get wrong, even though they’re pretty stupid, or in some sense they’re pretty dumb errors. Like the conjunction fallacy. What if you’re like, “What is the chance that someone is a librarian? What is the chance they’re a librarian, and some property librarians have?” I might be getting this a bit wrong, but [humans will say] it’s more likely on the conjunction of the two, even though that has to be less likely. And I think AI systems have biases that are sort of like this, that are shaped by the environment in which they were created or by their training data. As an example, if you give AIs a riddle. Like, “There’s a man and a boat and a goat. The boat can carry the man and one other item. How many trips does it take for them to cross the river?” The answer is one trip: they can just cross the river. But there is a similar riddle involving a man, a goat, and something like a cabbage — you know, there’s some tricky approach — and the AIs are so reflexively used to doing this that maybe they immediately blurt out an answer. They sort of have a strong heuristic towards that answer, but it might just be more that they feel a nudging to that. But if you get them to be like, “Oh, it’s a trick question,” then they go from that. And in fact, you can get humans on the same sorts of trick questions, right? So if you ask humans, “What’s heavier: a pound of bricks or a pound of feathers?” they’ll say the bricks, and they get tripped. It’s like the language models have the exact opposite problem, where if you ask them, “What’s heavier: two pounds of bricks or a pound of feathers?” then they’re like, “Same weight! Same weight!” So I worry that a lot of the tricks people are doing are sort of analogous to tricks you could execute on humans, and it’s hard to know how much to draw from that.\n\nRob Wiblin: Yeah. A general challenge here in assessing how capable they are is, I think Nathan Labenz uses this expression that they’re “human-level but not human-like” — so overall, maybe they’re similarly capable as human employees in some situations, but they have very different strengths and weaknesses; they can be tripped up in ways that seem completely baffling to us. But I guess you can imagine an AI society that would look at humans and say, “How is it that they can’t multiply in their heads two three-digit numbers? That seems crazy. These are obviously not general intelligences. They have almost no memory of this book that they read last week. That doesn’t make any sense. How could an intelligence act that way?” It makes it a little bit hard to have a common ground on which to compare humans versus AIs. Is there more to say on how you would assess what level they’re at now?\n\nRyan Greenblatt: Yeah, I think that I wouldn’t use the term “human-level.” Maybe this is me being a little bit of a conservative or me being a bit pedantic, but I like reserving the term “human-level” for “can automate a substantial fraction of cognitive jobs.” So maybe we’re starting to get into the humanish-level AIs once it can really just fully automate away a bunch of human jobs, or be a part of the cognitive economy in a way that is comparable to humans — and maybe not full automation at that point, but I also like talking about the full automation point. So that’s one thing, just responding to that. More context on how good the AIs are: some things we’re seeing that are maybe somewhat relevant are we’re seeing AIs sort of march through how good they are at math and competitive programming. So we’ve seen over 2024 going from basically the AI is bottom 20th percentile or something on Codeforces to being currently in the top 50, I think, according to what Sam Altman said.\n\nRob Wiblin: Top 50 individuals?\n\nRyan Greenblatt: Top 50 individuals. Literally the top 50 people. Or at least people who do Codeforces; maybe there’s some people who aren’t on the leaderboard, but roughly speaking. And then it looks like they’ll get basically better than the best human at that specific thing before the end of the year. And then on math, this is based on an anecdote from a colleague, but maybe they’re currently at the level of a very competitive eighth grader or something on short numerical competition math problems like AIME, the top eighth graders are doing as well as the AIs are doing right now. And the AIs are, I think, a decent amount worse on proofs. But both of these things are improving very rapidly — they were much, much worse a year ago. I think basically this is because we’re RLing the AIs in these tasks. I sort of expect the same trend to hit agentic tasks, software engineering. We’ve already seen that to some extent: the AIs are already pretty good at writing code, pretty good at following instructions, and OK at noticing errors, OK at recovering from these things. I think with a lot of runtime compute and a lot of scaffolding, that can be pushed further. Then there’s a bunch of things in which they’re weaker. So they’re a bunch weaker at writing, they’re a bunch weaker at other stuff. But I sort of expect that as you’re marching through software engineering, you’ll get to a bunch of these other capabilities.\n\nRob Wiblin: Yeah. OK, so we’ve got an arguably quite high level now. They’re becoming capable of doing tasks that would take humans longer and longer; they’re able to follow instructions for a longer period of time, complete tasks that have more open-ended choices to make. And that’s doubling every half a year or something like that?\n\nRyan Greenblatt: I think the doubling time on time, my guess is over the next year it’ll be substantially faster than every half year, but maybe the longer-run trend is roughly every half year. So we might expect there was basically a spurt starting at the start of 2024 or a little into 2024 of people doing more agency RL — more reinforcement learning or training the AIs specifically to be good at agentic software engineering tasks. And I think that trend will continue, and perhaps even accelerate, through 2025, plausibly continuing in 2026, plausibly later. But maybe the longer running trend is more like doubling every six months. I expect it to be more like doubling every two to four months over the next year.\n\nRob Wiblin: OK, so very rapid increase over the coming year.\n\nRyan Greenblatt: Very rapid, yes.\n\nRob Wiblin: There’s this interesting dynamic where we might expect that you could virtually fully automate an AI company, maybe substantially before you could automate almost any other company. Because firstly, they’re putting a lot of their resources towards trying to figure out how to automate their own\n\n\nStuff and their own processes, which makes sense from their point of view — firstly because it’s the thing that they understand the best, and also, these are among some of the highest paid knowledge workers in the entire world. They’re pulling an enormous salary. So if they can figure out how to get an AI to do that, then that has enormous economic value.\nAnd of course, the people running the companies think it’s of much greater value even than it looks just based on the dollar amount, because they think that they’re about to trigger this intelligence explosion, positive reinforcement loop which will change everything. So to them, this is kind of the thing that by far they’re most interested in automating. They care much more about that than automating McKinsey’s consulting reports, even though that is also kind of a lucrative business. So it could be that we haven’t automated consulting, even though that certainly would have been possible, mostly because they just weren’t trying. They were just trying to automate their own staff.\nRyan Greenblatt: I would say that my guess is that it’s probably hard, even with a bunch of elicitation, to near fully automate reasonably highly paid human knowledge workers in relatively broad fields. But I do expect that there’s some jobs that if the AI companies were trying harder, they might be able to automate substantially more than now.\nAnd in fact, the people benefiting most from AI are people close to AI, as you’re saying. This is true now, and I think will get increasingly true in the future — again because of this dynamic of AI company employees are highly paid now; at the point when AIs are capable enough that they can automate AI companies, they’ll be even more highly paid, you’ll be able to drive in even more investment, and AI company CEOs will be even more sold on the possibility of AI being extremely important. So I think we’ll see an even bigger gap then.\nOne objection you might have had would be like, sure, I agree that we’ll see a lot of concentration on automating the AI companies, but at the same time, there’s a lot of valuable human knowledge work outside that you could automate. So we’ll see that in parallel, we’ll see some economic impact.\nI think this is a reasonable first stab, but a problem with this story is the price of compute will go way up as the value of AI intellectual labour rises, at least in these short-timeline scenarios where compute is very scarce.\nRob Wiblin: Just to clarify, you’re saying that the companies will be using a lot of compute to automate their own work, so much compute that in fact that they won’t have many chips available for serving other customers who are doing things that are of less economic value, or certainly less important to the company?\nRyan Greenblatt: Yeah, broadly speaking. But I think it’s even just not on just automating the stuff, but on experiments.\nSo let me just put a little bit of flavour on this. So how do AI companies spend their compute now? I think it will depend on the company, but my sense of the breakdown for OpenAI is it’s very roughly something like: a fourth on inference for external customers, half on experiments — so things like smaller scale training runs that don’t end up getting deployed, doing a little bit of testing of RL code, this sort of thing, so experiment compute for researchers — and a fourth on big training runs. So like three-fourths of the compute is already internally directed in some sense.\nAnd then if we’re seeing a regime where the AIs can automate AI R&D and that’s yielding big speedups and seems very important, then you could imagine the regime might look more like one-fifth on doing inference for your AI workers — so you’re spending a fifth of your compute just running your employees — three-fifths on experiments, and one-fifth on training or something. Obviously it’s very speculative.\nRob Wiblin: So customers have been squeezed out almost entirely.\nRyan Greenblatt: Yeah, yeah. I mean, presumably you’ll have some customers, but it might be squeezed out almost entirely, and we’ll see the prices rise. And I think when you’re thinking about what customers to be serving to, I think you should maybe be imagining, perhaps surprisingly, that the highest wage employees might be who the AIs end up going for first, once the AIs are capable of this automation. So maybe you should be thinking like Jane Street, high frequency trading, places where the AIs seem particularly helpful, they seem particularly high paying, and they seem particularly bottlenecked on intellectual labour.\nNow, I think we will see automation of a bunch of other professions in parallel, but it might be that at the point when the AIs are most capable of automating, much more of the attention will be focused on AI R&D. And I think even possibly we might see effects like some profession is slowly being automated — you know, mid- to low-end software engineering maybe will slowly be automated more and more — and we might actually see that trend reverse as the compute gets more valuable.\nBecause now we’re in a regime where just everyone is grabbing as much inference compute as they can, or at least the biggest companies or the company in the lead is grabbing as much inference compute as they can, and just outcompeting the software engineers using this or the companies doing software engineering competing with this compute.\nI don’t currently expect a trend reversal, but I think we could see automation trends plateau or even reverse because of this.\nRob Wiblin: Because people have found even more valuable things to do with the AI.\nRyan Greenblatt: Yeah, that’s right. This is dependent on relatively short timelines. I think you could expect things are sort of smoother on longer timelines, where you wouldn’t expect trend reversals in that case. But if things are more sudden, more jumpy, then it seems at least plausible.\nRob Wiblin: I’m curious to turn to what implications does this have for how worried we ought to be, and what specifically we ought to be worried about? If this is the way that things go, what stuff could we be doing now that would help us to navigate this? I mean, this is a scenario in which things are becoming quite challenging for humans to track or be involved in quite quickly. So we would have to have set things up well, I suppose, for this to pan out well for humans — rather than for us to just get crushed in the passage of history. Maybe you’ll disagree with that?\nRyan Greenblatt: I think pretty quickly into this regime, AI takeover using a variety of mechanisms becomes pretty plausible and potentially surprisingly easy through a variety of routes. We don’t know. This is another huge source of uncertainty. We had many conversions between, how much progress do you get? How much does that progress amount to in intellectual labour? And then there’s the question of how much does intellectual labour help with takeover? What are the physical bottlenecks of takeover? How much does having more missiles help relative to having whatever?\nI think there’s a bunch of scary things that could happen here. One thing is that once you’re automating your entire AI company, a thing that seems very plausible to me is: you’ve handed things off to the AI system, and you’re still trying to oversee it, you’re still trying to understand what’s going on.\nBut it might be that the AI can run what I would call a “rogue deployment” — where they can basically use a bunch of the compute in an unmonitored way, because they’re writing so much of the stuff, so much is happening so quickly that they can evade your safeguards if they were to be misaligned. And then it might be the case that huge fractions of your compute and effort are being used for things that aren’t what you wanted.\nIn addition to that, these AIs could be doing things like backdooring the future AI training runs, making sure the AIs are aligned to them instead of aligned to you, and doing a huge amount of work that you might just not know what’s going on anymore. Even at the most basic level of what code was run, you might just be misled about that specific thing because things are going so fast and so out of your control.\nRob Wiblin: Yeah. If people are willing to grant that we’ve got billions of fast superhuman geniuses running on this data centre and barely any human oversight — because the sheer quantity of work being done would be beyond the capacity of all of the staff at one of these companies to scrutinise, and even if they did have time to read it, they often wouldn’t understand it because it would quickly be exceeding the kinds of things that they’ve learned to even be able to track — I think people would intuitively believe that at that point, if the AIs were wanting to take over, that they would probably have quite a lot of options for doing that. Oh, no?\nRyan Greenblatt: Actually a big source of disagreement is how much does cognitive labour assist in takeover? I think there’s a bunch of different routes. Man, I should have brought my takeover plans list, my hot tips for AIs! But I think once the AIs are very superhuman, there’s basically a route that I would call the crazy nanotech route, or the crazy superhuman technologies route — where they just have hard military power very quickly via the mechanism of being very superhuman.\nSo maybe you let them make nanotech, or maybe you’re like “No, don’t make nanotech. That would be super scary.” But they can do it even despite you not wanting them to do that — because they can use huge fractions of the compute you have in your data centre to work on the nanotech, and also exfiltrate themselves, coordinate with an outside version of the model, orchestrate wet lab experiments to bootstrap nanotech from all kinds of things. And then you’re just totally cooked.\nRob Wiblin: So a possible layer of defence would be, although there’s this enormous capability here, if we’re able to direct exactly what they’re doing, such that they never get a spare moment to think about how they would take over or to actually try to implement that, then that might possibly keep us safe.\nHowever, in this scenario — where we’re able to scrutinise so little, and we have so little understanding of how these models operate — it seems likely that there would be some slippage, where they might be able to figure out a way of grabbing a bit of time, grabbing a bit of compute here and there, and then pushing that open to use more and more of the compute in the data centre to be pursuing their own agenda, rather than the task that we think that we’re giving them.\nRyan Greenblatt: Yeah. I’m pretty sceptical of being able to maintain a meaningful level of control if all of your AIs are coherently misaligned and we’re at this level of capability. By control, I mean things such that the AIs couldn’t do bad stuff even if they wanted to.\nI think a somewhat slowed-down version of the initial automation seems very plausible with control, maybe even at full speed, and maybe you can go even somewhat above the human range while ensuring control using clever strategies — including having the AIs work on the strategies, and making sure their work on the strategies itself is not sabotaged.\nBut it seems very difficult once you’re in this completely insane regime, especially if you want to use the AIs to develop things like a cure for cancer where you don’t understand what they’re doing. They’re routing through biological mechanisms you don’t understand. They’re orchestrating humans who don’t know what’s going on in a wet lab. I think it’s very plausible that for various reasons you’ll want to have AIs directing people to do wet lab experiments where you do not know what the fuck is going on. The AI explains something, and you can maybe do some checks, but you’re very confused. So maintaining control of that.\nMaybe I’ll try to quickly run through the takeover list. There’s the crazy superhuman technology, which I think is pretty plausible, for what it’s worth.\nThere’s what I would call superhuman strategy/manipulation, including things like persuasion that is getting people to do things that are wildly against their interests very quickly. I think this is not clear if this is ever possible, but if it is, it does feel like it would make takeover a lot easier.\nThen there’s a bunch of stuff that might be more possible even around the human-level regime. An example would be the AI escapes, it maybe coordinates with a copy inside, but it also is running some independent presence externally. Maybe that’s like running on the GPUs internally, but it’s running code on servers outside of the AI company.\nAnd as part of doing that, it sort of builds an independent industrial base, so it has like all the stuff it would need to be able to recover industrial civilisation and then just kills almost all humans. So something like deploys bioweapons, deploys mirror bacteria, deploys mirror mould, whatever, and that ends up relatively quickly killing a huge fraction of humans.\nI think it seems like this is technically feasible. And then from there the question is, did the AI screw itself over? Can it maintain what it’s doing, or is it just going to decay into ruin because there’s no human hands to build all the things? So I think it needs to have some independent industrial base that could either be via having some human loyalists, some like humans that it’s enslaved and keeps alive, or via having robots.\nRob Wiblin: At this point in time, wouldn’t there be just incredible robots and probably quite a large number of them?\n\n\nRyan Greenblatt: Potentially very quickly.\nI haven’t done all the analysis of how quickly do you expect lots of robots.\nWe also have to answer the question of how many robots get destroyed as the humans are starting to get killed by bioweapons and maybe suspect that it’s AI caused, and questions like, if there’s surviving humans, how much military force is needed to handle that?\nI think it’s non-obvious how the situation goes, but this is a route for why you could get takeover without needing very superhuman capabilities.\nThe thing I’m describing seems like it would in principle be possible for AIs to do if they’re merely fast at human level and super well coordinated.\nReasons why this is hard for humans to pull off: one, humans don’t want to do this.\nAnother reason is that I think it’s the case that it’s hard for humans to run vast conspiracies, but it might be much easier for the AIs to run vast conspiracies because they don’t have to rely on recruiting potentially untrustworthy humans.\nThey can potentially be wildly better at cybersecurity and much more meticulous.\nThey might mess it up, but I just think there’s mechanisms via which this could happen.\nSo this is the “kill everybody via independent industrial base” story.\nAnother story, which I think maybe is the most plausible story of all, is the “humans give the AIs everything they need” story — which is like the AIs just chill, they make sure they have control of the situation; maybe they’re manipulating what’s going on.\nI talked earlier about how what you see is not what’s actually going on.\nLike, you look at the experiments, they’re not what you expect.\nThe AIs are doing that.\nThey’re sabotaging the alignment experiments.\nThey’re sabotaging the alignment results.\nThey’re deluding us.\nThere’s a bunch of mechanisms for that, but they don’t do anything very aggressive.\nYou know, they’re just chilling.\nWe’re scaling up compute.\nThey do lots of good stuff.\nThere’s cures to all diseases, all kinds of great stuff is happening, industrial development.\nPeople are like, “It’s so great that AI did go well and we didn’t have misalignment.”\nSome of the safety people are worryingly looking at the situation, wondering if this is what’s going on.\nAnd then at some later point, when the AI has an insanely decisive advantage, and the humans are starting to be an obstacle at all — which, throughout this story, they might not be, right?\nI think if the humans ended up being an obstacle earlier, maybe the AIs would take more decisive action.\nBut if there’s no need for decisive action, maybe they just lie in wait.\nAnd then at a point when there’s truly vast levels of industry, truly vast levels of robots, the armies are entirely run by AIs, the situation is so beyond whatever, maybe even at the point of space probes being launched… It could be potentially that humans are deluded indefinitely.\nIt might be just like you can in principle Potemkin village the entire way through.\nBut it could also convert at an earlier point.\nNow, to be clear, if humans are deluded indefinitely, then it’s like all the stuff on Earth might be great, right?\nRob Wiblin: So you’re saying they could even get to the point of saying, “We’re going to go off and settle space and do a whole lot of stuff in space,” and they’ll tell us that they’ll do one thing, but they’re going to do a completely different thing while we just enjoy our lives on Earth, thinking that things have gone better or at least very differently than in fact how they’re going to play out?\nRyan Greenblatt: Yeah.\nBut to be clear, it could be Potemkin villageised on Earth as well.\nIt could be the case that you’re like, “Wow, I have so many happy friends,” whatever.\nBut steadily your friends are being replaced by robots.\nAnd at some point it’s like, why doesn’t the AI just suddenly take over?\nBut in principle, it could go very far.\nSo that’s another story.\nAnother story I would call “sudden robot coup.”\nThis also doesn’t require very superhuman capabilities.\nWe build vast autonomous robot armies.\nMaybe we think we have some way to turn them off.\nEither the AIs have sabotaged that, or they have some way to work around that, or there’s various other mechanisms by which this could fail.\nAnd then we built the robot armies, we saw the AIs building the robot armies, and we’re like, “Hooray, we’re building a vast robot army to compete with China!”\nAnd China’s like, “Hooray, we’re building a vast robot army to compete with the US!\nTruly, it’s great that we have such a big robot army.”\nAnd then it’s like, whoops.\nAll of a sudden the robot armies sweep in and do a relatively hard power takeover, because they control the military, they can be hyper coordinated, and maybe they’re in parallel using things like potentially bioweapons to some extent, but that might not even be required.\nRob Wiblin: I mean, at this point they don’t even need to kill us, potentially.\nRyan Greenblatt: Potentially.\nRob Wiblin: Because even if we all tried to resist, it would be pointless, basically.\nRyan Greenblatt: Potentially.\nAnd also things like they maybe are using lots of cyberattacks to destabilise human response.\nSo they could potentially do this at a point when we naively think that the robot army is weaker than the human army.\nOr even we think we have shutoff switches, but the shutoff switches are not reliable enough.\nThey’re not over-engineered enough, they’re not shielded enough from the AI’s tampering.\nI think it’s an awkward dynamic, where if you want to have remote shutoff, you have to both make it so that it’s tamper-proof for AIs that might be in principle very general and very smart and very clever, and also, it needs to be the case that they’re remotely triggerable.\nSo it can’t be shieldable either.\nIt has to be the case that there’s no way the AIs can shield the thing from signals and avoid tampering with it — which is a very awkward combination in principle.\nRob Wiblin: What determines whether you get AIs attempting to take over early relative to later?\nI guess the reason to wait is that they’re going to become a bigger fraction of the economy; they’ll have more hard power, so they’re just more likely to trivially succeed.\nThe reason to go early is that a given set of models might expect to be replaced by other models that might not share their goals, so they would have missed their opportunity to take over, basically.\nRyan Greenblatt: I think the strongest reasons to go early might be something like: worries about other AIs; worries about humans getting their shit together — either because we get enough work out of the AIs, or because people freak out and trigger some strong response that could recover the situation — and the third one would be impatience or something.\nIt might be that, by default, humans slow progress down a bunch, even with AIs trying to manipulate things, trying to steer things from behind the scenes.\nAnd if the humans are slowing things down a bunch, and the AIs are like, “No, I want to get to the stars now,” and they’re at some sufficiently high takeover probability… Or maybe the AIs are just like, “I just want to do it now.”\nLike, I know humans sometimes want things to happen faster, even independent of the total amount of stuff they end up getting.\nSo these are some reasons to happen earlier.\nThese things also apply to things less egregious than takeover.\nOne of my hopes for things going well is we can put the AIs in a position where they’re forced to either take aggressive moves early or we get enough work out of them.\nAnd if they take aggressive moves early, then maybe we can catch that and then build a case for danger.\nAnd in addition to building a case for danger, we could potentially do stuff like study that example, iterate against it — stuff I’m calling, “few-shot catastrophe prevention.”\nLike, we’ve caught a few examples of bad stuff: can we handle that?\nIf at multiple different levels, are the AIs sort of forced to take aggressive action?\nOr is it instead the case that the AIs can bide their time, because we don’t have any mechanism for exiting the regime using some ongoing background trend — either safety research improving over time, or humanity being like, “I’m sorry, what is going on here?”\nRob Wiblin: Are you saying a general thrust of the interventions that we could make is creating a trend where over time in fact they’re going to become less and less likely to be able to successfully take over?\nMaybe because we’re successfully figuring out how to use AI in order to prove alignment and safety and to detect misalignment, or we’re improving our control mechanisms.\nSo that would then force the models: they see this trend, they see that in fact the window is closing, and so they have to try to strike early.\nAnd that gives us a chance to realise what’s going on and then actually win out.\nRyan Greenblatt: That’s right.\nBut of course it could obviously go in the opposite direction where our measures can’t keep up with the rate of capabilities, and it keeps going up.\nThere’s this notion of what’s the level of capability that we can handle at a given level of takeover risk?\nMaybe initially we don’t have that much going on, so we can handle roughly humanish level at some 20% takeover risk, or some very high level — which will depend on various facts about misalignment: which AIs are misaligned, how coherent are they, how much are they unified, this sort of thing.\nAnd then in addition to that, there’s a question of, if progress is fast enough, it might be that takeover risk increases over time.\nBecause as the models are getting more capable, they’re more and more likely to succeed and more likely to be misaligned, potentially.\nBut also maybe we can push the curve the other direction, where we can make it so that the range of capabilities that we can hold in the n% takeover chance of success at takeover regime is lower.\nRob Wiblin: Yeah, explain that last bit.\nI don’t get it.\nRyan Greenblatt: Imagine that there’s some trend over time, which is: for the smartest AIs we have now, what’s the chance of success they have?\nOr maybe just how likely are they to take over?\nSo initially our countermeasures suffice, to be like, maybe at the point where they’re automating the whole AI company, by default, we’re at instantaneously probably not even that high a chance of takeover, because what are they going to do?\nBut then, as they get more capable, the instantaneous takeover chance is going up, and some of our paths to success have been cut off.\nMaybe the AIs have escaped, they’re running rogue internal deployments, they’re sabotaging our work.\nBut we could make it so that the trend bows the other way — where over time the risk of takeover is going down, not up — because the capabilities are increasing, but our measures are increasing faster than the capabilities.\nBut obviously, as we discussed before, the capabilities might be increasing very fast.\nSo if the measures aren’t keeping up with that, if we’re not on some stable regime, then that could be bad.\nAnd so I think the model I was using there is most natural to think about in a regime where the AIs are misaligned.\nBut if we start at a point where they’re aligned enough that they sort of maintain their own alignment, then it could be that the takeover risk initially is like, they’re actually aligned, so we’re sort of in a stable attractor of alignment.\nRob Wiblin: OK.\nWhat effects does this picture have on what your research priorities are, and what you think the broader AI ecosystem ought to be prioritising in terms of making things more likely to go well and less likely to go poorly?\nRyan Greenblatt: For one thing, I’ve been talking about the timelines in the absence of very active intervention.\nThere’s also a question of how much do you pause deliberately at these relevant capability milestones?\nI feel much more optimistic to the extent that we end up pausing at a level of AI capability which is around the point where you can fully automate the AI company — maybe somewhat before, maybe a bit after, maybe just right around there — for an extended period, both so humans have time to study these systems and also so that we have time to extract a bunch of labour out of these systems and reduce the ongoing takeover chance.\nAnd so there’s some question of, there’s all these worlds where things are slower than this.\nThen another source of hope is maybe you’re just kicking off this crazy regime with an actually aligned AI: with an AI that’s not just not conspiring against us, but even more strongly, it’s actively looking out for us, actively considering ways things might go wrong, and is trying to keep us in control.\nThis has been called a basin of corrigibility or a basin of alignment.\nSo the things I’m excited for depend a bit on the regime.\nI think there’s a bunch of different regimes we can consider.\nOne is a regime where, maybe especially in these short timelines, I expect not that much lead time between different actors, at least in the US side.\nRob Wiblin: You\n\n\nWe’re saying there’ll be multiple companies all approaching this point of automation roughly simultaneously.\nRyan Greenblatt: That’s right. So I think a pretty natural scenario to consider that I often think about is a relatively (from my perspective) irresponsible company, basically their default plan is scale the superintelligence as quickly as possible, don’t take safety concerns very seriously. And that’s their explicit plan, that’s their internal thing and they’re just pursuing that as quickly as possible. Three months in the lead, let’s say. They’re maybe ahead of Chinese companies and maybe some more responsible actors, in some mix of three to nine months behind.\nI think in this scenario, a lot of the action is going to come from a relatively small number of people inside that company. And then potentially there’s a bunch of action that can come from people on the outside who are enacting various influence via doing exportable research and also via potentially changing the policy situation. Also, I think there’s definitely stuff that can be done via trailing AI developers who are more responsible and are putting more effort into preventing bad outcomes in this case.\nRob Wiblin: In that scenario, I guess one thing that could be productive is that there are people inside the company who say, “What we’re doing seems a little bit scary. We should have better control mechanisms, we should have better alignment mechanisms. We should take that a bit more seriously than our current default plan.”\nAnother one is the people working at the more responsible organisations could produce research that makes that easier for the company to do, makes it less costly for them to have stronger control measures. And then there’s also, of course, you could have a governance response: maybe this will begin to take off, and people will be quite alarmed, and perhaps you’ll get broad-based support for the “pause at human level” notion.\nRyan Greenblatt: Yeah. Man, I really wish this notion was more memeable. An unfortunate thing about “pause at human level” is all of those words are complicated and confusing. And it’s just not even that, it’s not even that nice of a slogan. Someone should think of a better slogan for this thing.\nBut anyway, yeah, I think there’s a bunch of different mechanisms for doing exportable research, trying to wake up the world. There’s more and less indirect things. So you can do very direct policy influence, versus things more like demonstrating the level of capabilities — and when you’re demonstrating the level of capabilities, that could trigger a response from the world.\nSo I think a lot of my hope in this sort of scenario is going to come especially in very short timelines, which I think do look worse than longer timelines. Part of my picture of less doom is like, it might be eight years away instead of four years away.\nSo in the four-year timelines, I think a mechanism that we have is that the leading AI company, we have some people who are working there and their goal is both prevent egregious misalignment — by “egregious misalignment” I mean something like AI systems that are deliberately engaging in subterfuge, deliberately undermining our tests, deliberately trying to sort of make our understanding of the situation misleading — and then second, we can try to have it so that the properties of the AI system are ones where we’re happy to hand things over, and try to aim for that kind of quickly, because we’re in a very rushed timeline.\nSo some of this preventing egregious misalignment, or dealing with it, I think is in some sense it’s the traditional thrust. But I think there’s a bunch of stuff related to handover and the AIs being wise enough, the AIs sort of staying faithful to our interests on long horizon things, making sure we even just have the tests to know whether this is true. These sorts of considerations.\nAnd there’s a bunch of different incremental steps here. Phase zero is maybe like you’re using mechanisms to make it so that AIs are producing work for you that you think isn’t sabotaged, even if those AIs were trying to sabotage it, and then you use this to milk a bunch of work out of your AI systems. Phase one is you try to create an AI system that the safety people are happy to hand off to, to the best extent you can. And then phase two is you try to make it so that the AI running the organisation overall is an AI system that you’re happy to hand off to, and presumably other people are happy to hand off to as well.\nThis is like a caricature of the situation, or a simplification, because probably at a bunch of points you want to be doing stuff like picking up low-hanging fruit, demonstrating capabilities to the world, trying to consider whether something can happen, trying to make it so the main AI system that the organisation is training on the frontier of current capabilities is less likely to cause problems. I’m not tremendously hopeful about the situation, but I think there’s a bunch of different potential outs that give us some hope, or some saving throws, some win conditions.\nThings like, we catch the AIs early on is another hope. Maybe we catch the AIs early on, maybe that convinces the world to take the situation more seriously. And maybe even if not, it convinces company leadership that their own power is under threat from the AIs. So even if they’re very Machiavellian, I think the AI being a threat to their power, they should be worried about it.\nRob Wiblin: They might switch on.\nRyan Greenblatt: Yeah. So that could potentially increase the amount of resources spent on safety.\nRob Wiblin: OK, so the primary story there was trying to figure out a safe scenario in which you could hand over these processes to AIs that you trust. How would you do that?\nI guess you start with models that are around human level that you feel like you have a reasonable grasp on, that probably are aligned. Like, we currently think that Claude probably is roughly about as friendly as it seems; it probably isn’t engaging in massive scheming against us at every turn.\nYou figure out clever mechanisms by which you can get those kinds of models that you feel OK about to do a whole bunch of work on control and alignment. You rush through that before you have had other capabilities, improvements that have been allowed to happen. And then you get to the point where hopefully this has panned out that we feel OK handing off to these models that have been trained by the previous generation that we kind of understood — and then we’re going to cross our fingers? Is that it?\nRyan Greenblatt: Yeah. I should be clear that this is a very low political will plan. I have a menu of plans, and this is like the bottom of the barrel or whatever. So the way I would have put this is like we had some early point where we had AIs that could automate a bunch of stuff and we didn’t necessarily trust. We harness a bunch of labour from these AIs, then we try to make a version of those AIs that we do trust. And not just trust at the level of they’re not conspiring against us; ideally trust at the level of we’re happy to hand off messy open-ended questions — like, “What should our strategy be for minimising risks from the following things?” Including potentially somewhat tricky, more philosophical topics.\nAnd then from there, we try to hand off to those systems and hope those systems stay aligned, stay on track, are able. And also succeed themselves, right? It could be that we hand over to systems and those systems are like, “Wow, you guys seem like you’re in a pretty bad situation. I’m not sure that I can pull this one out of the bag.” There’s a variety of mechanisms by which that could occur.\nRob Wiblin: I think you’re saying that they could also mess up the next stage?\nRyan Greenblatt: Especially because they have very little time.\nRob Wiblin: Because they’re also racing.\nRyan Greenblatt: Yeah, yeah. Oh, for sure. So there’s an internal thing, where if you’re the safety team, maybe you have a version of it that you fine-tuned, that’s human level, that you trust, that you’re happy to hand off to. But capabilities progress is so fast that maybe the cutting edge of the current company is very superhuman. But you don’t even want to work with that system.\nI think the first handover point you should try to do closer to the human regime — maybe slightly superhuman or around the top human level of capability — because the more superhuman, the more likely it is to be misaligned, so you want to start the handoff earlier. But that might not stop the rest of the organisation from training more capable AIs. So it might be that you as the safety team are handing off to an AI that’s less capable than the frontier of the organisation, and that AI needs to quickly bootstrap so the trend of the handoff AIs intersects with the capabilities fast enough.\nRob Wiblin: Yeah. At the point that you have an aligned, somewhat superhuman model, is there some chance that it would be able to persuade a broader coalition of people that we need to take this a little bit more seriously, and it shouldn’t be forced to race against another team at the same organisation?\nRyan Greenblatt: Yeah. I mean, that’s definitely part of the hope. Definitely part of the hope is that I could demonstrate empirical evidence. I think also, if we’re in a situation where we have two AIs trained by different teams, and both of those AIs are just throwing huge amounts of shade on each other — where one AI is like, “That AI is obviously scheming; look at all these examples,” and the other AI is like, “That AI, obviously conspiring against you; look at all these examples. It’s so biased. It’s so unreasonable. It’s clearly just a plant” — I’m like, OK, that’s a crazy situation. Certainly you can’t come out of this regime thinking it’s fine.\nRob Wiblin: One way or another, it’s definitely not reassuring.\nRyan Greenblatt: It’s not reassuring. There might be ways to downplay it. So one regime is like, I think that’s kind of plausible is the safety team trains their AI, tries to go to company leadership and be like, “The situation seems very scary. You’re training this AI. I think it’s misaligned. Here’s a bunch of evidence for that.” And the other AI comes in and is like, “As we all know, that AI was trained by the safety team. I think it’s just very paranoid about misalignment. Of course, the safety team would want us to think all the AIs are misaligned. I’m not misaligned. That AI is not misaligned. It just has poor judgement.”\nA thing that’s interesting about this is that, yes, it’s concerning, but it’s not like you know at least one of the two AIs is misaligned. So in general, I think a concern with the “improve the epistemics at the AI company” route is that that the people who trained the AI plausibly have a reputation for being biassed on these matters relative to the company from the company’s perspective, which I think poses a bunch of concerns.\nAnd it might be the case that for every evidence of risk, you can make an even more comparably compelling evidence that risk actually doesn’t exist. Because if we’re in the regime where we don’t really know how to adjudicate these things, it could potentially just be that reason doesn’t win out.\nSo that definitely is some of the hope. Some of the hope is coming from once you have this handover, then it’s fine.\nNow I think there’s a bunch of ways this story could go wrong. Here’s an example. I talked about the alignment team sort of off to the side in their corner, trying to like fine-tune or potentially even maybe train from scratch a version that they’re happy to defer to. Well, one issue is that it might be the case that the main AI system has already taken over the AI company in a cybersecurity sense — as in it has a rogue deployment; it can use compute in an unauthorised, unmonitored manner; it can tamper with jobs everyone is running. So you’re trying to align this AI, but you need to harden yourself against the rest of the company. And I’m like, oh my god. That seems potentially hopeless.\nRob Wiblin: Zooming out: at Redwood Research, you spend a bunch of time gaming out these various different scenarios, thinking about what sort of interventions might be helpful in some significant fraction of them, and then trying to get more people working on that and work on that yourself. Is that the basic picture?\nRyan Greenblatt: Yeah, broadly.\nRob Wiblin: Recently Redwood has been prioritising control as opposed to alignment — saying, let’s assume that the models don’t share our interests, that there is some degree of misalignment: how could we make it such that they are nonetheless not capable of screwing us over? People used to talk about putting the AI in a box. That doesn’t seem so likely anymore, but I guess there’s other mechanisms that you could use to try to check their work: put models in opposition with one another, forcing them to do useful stuff for you.\nWhy focus on control? Is it just that there’s so many different ways that this could go that AI control mechanisms seem like they’re useful in a larger fraction of the different plausible scenarios?\nRyan Greenblatt: Yeah. I think in these incredibly desperate, very low political will, short timelines — ultra fast from my perspective, very fast,\n\n\nMaybe not as fast as people were speculating in the past — I think control looks less good. There’s a few reasons why I think control is still pretty useful in this world. For one thing, if we can catch the AI early on, that could potentially be very useful. Another thing is that getting useful work out of the AIs early on in this scenario might be very important, and we might be able to do that prior to having totally lost track of the situation. And then I think also potentially you can prevent… So I was talking about the situation where the AIs are basically operating your own data centre. You might be able to use a variety of mechanisms to make that much harder, so that it occurs later in the capabilities, so you have longer when your experiments are not literally tampered with. So these are why control could be good in this world.\n\nI think there’s a more complicated slew of interventions this doesn’t cover. I do think that in these very rushed worlds control looks less good than in worlds where there’s somewhat more time and somewhat more political will. The original target we were thinking about when we were thinking about control was a world where it was more like, we have a year of delay — like, safety is given a year of budget, basically, and we can afford to pause for a year. We can do stuff there. And I think control looks much, much better in this regime. Or I don’t know about much better; I think the amount of risk reduction control gets us is quite a bit better, but these worlds are less likely. But that’s not to say that the risk reduction control gets us in the other worlds isn’t much smaller than the alignment stuff.\n\nA different way to put this is: alignment also looks really hard in the worlds where things are going that quickly, and alignment looks less tractable to work on in advance. A lot of the stuff that I’m most excited for in alignment involves doing experiments on powerful AIs, and you can only do that relatively late. So it might be that in the very rushed scenarios, alignment is more of the picture ultimately, but was harder to work on in advance.\n\nThese factors make me pretty into the control. There’s also just worlds with more political will where I think control looks a lot better. I often think about the control plus “pause at human level” plan — which doesn’t require any alignment for a sustained period, potentially. Because you have your initial year of delay, you control those AIs, and then you bootstrap from there into longer delay — by building evidence for risk, ensuring non-proliferation, developing somewhat better safeguards — and then you potentially quite slowly proceed from that regime, at least for a while, until you’re actually happy to hand off, rather than doing an insanely rushed desperate handoff because that’s the only thing you can afford to do.\n\nRob Wiblin: I guess that requires some degree of coordination between companies and possibly between countries as well.\n\nRyan Greenblatt: Yeah. At the time when I was thinking about this, I thought a year of lead was more plausible, and I’ve updated against that. I think it could still happen. The gaps could open between companies. I think that the story where you’re working heavily to ensure non-proliferation and potentially slowing down progress requires a substantial level of government buy-in potentially. So it might be the case that you have to get it so that the US government — or at the very least, some important governments — are pretty much on board with the overall plan. Not necessarily for the same reasons, but are on board with it for whatever reason. And I’ve updated against that, so I’m now spending more of my time thinking about relatively more desperate situations. But you know, an important factor is in the relatively more desperate situations, maybe it’s like we’re reducing the takeover risk from like 50% to 40%. And maybe in the worlds where control could have worked and we had a year of delay, we’re reducing the risk from like what would have been 30% to 5%. So it could be that the total risk reduction is sufficiently higher in these worlds that they’re actually more leveraged to work on. Because in the world where we have very little influence, because there’s so little will, it might just be that it’s less tractable to work on.\n\nRob Wiblin: I see. In general you want to work on medium-tractability scenarios. If things are too grim then it’s maybe impossible to save; if things are going very well, then it’s hard to improve. It’s the middle case where you’re likely to get the most impact, right?\n\nRyan Greenblatt: Yeah, for sure. I think a mistake that the safety community appears to have made over the past few years is too much focus on overly optimistic worlds. I think actually focusing on desperate, crazy, YOLO’d, pessimistic worlds is pretty reasonable, because that’s where a lot of the risk lives, so getting from 50% to 5% is like a lot of the action. And getting from 5% to 0.5%… I mean, a 5% level of risk is crazy and unacceptable, and we don’t accept our paint companies having a 5% chance of massive fatalities, takeover — and all this stuff is just from the takeover; I think there are risks other than takeover is like totally unacceptable. But nonetheless, that 45% has to be the bulk of the mass.\n\nRob Wiblin: To what extent do we just have to hope that there are serious early warning signs? You were talking about catching the AIs red-handed, which is another concept that you and your colleagues have tried to make more salient. That’s a sort of warning shot, I suppose, where you find the AIs inside the company have been sabotaging the data centre, they’ve been doing something really quite egregious. That might get more buy-in to do substantially more stuff from either the government or the leadership of the company. How much more hopeful are things if you do get some sort of early warning shot like that?\n\nRyan Greenblatt: I think it helps a lot. One source of scepticism I have is that it might be the case that I think it’s a smoking gun but the broader world does not. For example, the alignment faking work that we recently put out, from my perspective, should be quite a large update for people who are very sceptical about this threat model. And I think predictably it was a maybe no or only a very small update for a lot of these people. To be clear, I don’t think that evidence on its own should be a huge update in absolute terms. Maybe it doesn’t move people to the same level of risk of misalignment that I think, because a lot of that’s based on conceptual arguments they don’t agree with. But if I think a bunch of people who are like, “There’s no way in hell this would happen” — 0% chance, basically — should maybe move to like 2% or 3% chance. And that’s a huge difference in vibe space at least, and log space or whatever. Vibe space, log space, same space, whatever. And people didn’t make that move. So I can totally imagine a case where it’s like we caught the AI trying to escape. Buck has a post on this, like, what would happen if we caught the AI trying to escape? And people are like, “You know, sometimes systems do that. They do weird stuff. Also, is that even true? Did you train it in some weird way?” Or, “Oh, that’s what happened at that company. But at this company we have different practices.” Also people just being like, “Sure, maybe AIs have a weird instinct to escape. But what are they even going to do?” Or things like, “We’ll use various checks to prevent this.” And I think all these factors in a relatively clear-cut scenario could cut into that. And I kind of expect a lot of the situations that I think are like a huge smoking gun are way more confusing than this. An example would be like, maybe it looks like the AIs are sandbagging on safety research — where they’re performing worse than they could, or they’re not doing very brazen sabotage, but there’s way more errors than we would expect, there’s more misleading arguments. It was from our perspective a big update. Because it seems pretty off, but it’s quite quantitative. If you just had lower expectations, maybe this wouldn’t be that surprising.\n\nRob Wiblin: And I guess it’s a strategic situation where the AI can to some extent anticipate what the response will be. And inasmuch as it’s taking hostile actions, it can make them things that it suspects will not actually trigger that larger response. So for things it can get away with, like trying to escape, but it couldn’t get away with mass murder. Then it goes for the escape and holds off on the murder.\n\nRyan Greenblatt: At least for a while. One hope with control is making it so that egregious bad stuff has less plausible deniability. We refer to this as “incrimination.” It’s really important to get from a position where you’re like, “Wow, that was really suspicious,” to “OK, we’ve nailed down exactly what was going on.” Just because incriminating in our lights doesn’t necessarily mean it’s incriminating from the perspective of the broader world, but it is moving the needle. And I think this can occur at all different regimes, from something looks a bit suspicious, to we kind of understand what’s going on and it looks very suspicious; to the AI tried to escape, we stopped it, and we’ve done the thing where we pretend it succeeded and we see what it would do on the outside — and try to get to the point where it’s like, “It looks like it’s trying to figure out how to manufacture bioweapons. It’s trying to run a wet lab.” Plausibly you can get, in principle at least, very far into understanding what it would do given the affordance of having escaped. And to the extent that it does stuff that’s much more clearly malign, I think that could motivate additional action. But I do worry that any sort of warning shot which is not causing massive amounts of damage and not causing potentially human fatalities maybe won’t trigger a strong response.\n\nRob Wiblin: So we’ve zoomed in on a scenario that is pretty vertiginous and concerning, I guess in part because it’s remarkable that something so crazy is so consistent with what we observe now. That’s interesting in itself. And it’s also a scenario where we think there’s maybe a lot that can be done now, or it’s a scenario where things going poorly is quite likely, and we should be probably taking precautions and doing preparations to figure out how we would lower that risk. Just for balance, it would be good if we could maybe go over what are some plausible scenarios that are a bit more mundane, a little bit more boring, and perhaps hopefully a little bit less scary. So people don’t think this is the only way that things are going to go. Because in fact, a key conclusion I think is just that we have incredibly wide error bounds around how these different things could play out.\n\nRyan Greenblatt: Yeah. I think maybe a good place to start is we talked a lot about timelines. Well, timelines could just be longer, right? So I said my median or my 50% probability was by eight years from now for the full AI lab automation. But there’s a long tail after that, so there’s worlds where it’s way further out. Potentially progress was much slower, much more gradual. Things took a long time to integrate. Maybe the world has a better understanding, more time for experiments, and that could make the situation a lot nicer. In addition to that, it could be the case that on the very long timelines — or much longer timelines, I should say, I don’t know about very long — takeoff we should also expect is slower. I think short timelines are, broadly speaking, correlated with fast takeoff, because it indicates that the returns to more of the stuff we were doing is higher. So longer timelines. Another thing is it could just be that AI R&D research bottlenecks extremely hard on compute. And so even at the point when you have automated your entire AI company and all the employees, you’re barely going much faster than you were with human employees. In the extreme, maybe you’re going only 2x faster. You could be even slower than that. But I think that’s pretty low to the low end of my error bars. I think that’s maybe pretty implausible, but 2x or 3x is plausible. And then it could be that you have diminishing returns setting in pretty quickly after that, such that you’re not actually speeding up that much. That would imply the situation looks a lot better in terms of you get a much less aggressive takeoff. Another thing is that the default rate of AI progress is already pretty fast. So I even worry — without the speedup, just on the default trajectory — if we’re getting to this point where we have AIs capable enough to automate the entire AI company, maybe even if it doesn’t yield much acceleration, and there’s some risk that comes pretty quickly after, in normal government time, from just the default rate of progress. But I think there are plausible arguments that the rate of progress will slow down in the future. In particular, right now, progress is relying on shovelling in more and more compute, more and more investment. And if it’s the case that things stall out, there’s less investment, but we’re still operating at nearly pretty high capacity, we can imagine a world where the AI industry ends up being an industry that justifies maybe perhaps hundreds of billions of dollars\n\n\nof compute spending per year, maybe even more than that, but not way more than that. In that regime, if we’re not shovelling in more and more compute, then we don’t get progress from hardware scaling, algorithmic progress will also be slower. So maybe the pace right now is we get 13x effective compute per year — where some of that is algorithm, some of that’s compute. I think if compute was literally fixed, we might have like 2x or 3x progress per year. And in a regime where compute is going slower, it could be somewhere between.\n\nRob Wiblin: Okay, so that would be just a much slower takeoff. What’s the chance that alignment is a relatively straightforward problem? Or maybe not even a problem at all, and we’ve just been confused, and all of these AIs that we produce are basically going to want to help us by default?\n\nRyan Greenblatt: Well, an important question from my perspective is what’s the probability that various AIs of different capability levels are actively conspiring against us? I think it’s pretty plausible that you can get into very superhuman capabilities and not have AIs that are very actively conspiring against you in some strong sense. My current view is like 25%, if you do basically no countermeasures; you basically just proceed along the most efficient route for capabilities. And then once we’re starting to take into account countermeasures, it could potentially go much lower, and that risk could be lower at this early point too.\n\nOne story for hope is you have these early systems that are able to automate the whole AI company. Those systems aren’t conspiring against you. You do the additional alignment work needed to make it so that they’re also trying to do their best — they’re actually trying to pursue alignment research; they’re trying to anticipate risks — which maybe doesn’t occur by default, even if they’re not conspiring against you. It could be that things aren’t conspiring against you, but things still go off the rails. But if they’re not conspiring against you and you avoid things going off the rails, then maybe we’re just fine. We hand over to the AIs. The AIs manage the situation, they’re aware of the future risks. They build another system with the same properties, and that can be self-maintaining.\n\nAnother scenario is it could be that an even stronger sort of thing is true by default. It could be that you train the AIs, and with a relatively naive — what you would commercially do by default — training strategy, they really just are, out of the box, trying really hard to help you out. They’re really nice. Maybe even they’re just aligned in some very broad sense. Not even they’re just myopic; they’re really trying to pursue good outcomes for the world overall. I think this is possible, but less likely than the previous scenario.\n\nAnother thing is there’s a question, does it happen by default? It could also be that it just happens because of a lot of work that we put in, and we could just be in a much better position. It’s hard to have much confidence about how much alignment work or safety work will happen in the next eight years. If we have eight years, the community could build. AIs will get more capable over time. More people will be working on this. It seems plausible that there are potentially things like large insights, but even putting aside large insights, maybe just things like we just really advance our techniques, we’ve dialled in the science.\n\nSo there’s timelines with, I would say, maybe technical hopes. And then there’s a broader class of societal hopes. There’s some stories in which society just takes the situation much more seriously in the future — maybe a subset of countries, maybe the broader scientific community. It could be because of a warning shot. It could just be because slowly people are exposed to AI, timelines are somewhat longer maybe, and more people are interacting with it.\n\nIt also seems plausible that you have some big incidents going on that aren’t that related to misalignment risks, but in practice have high transfer. So maybe there’s a lot of job loss that prompts a large response, and some of that response goes into mitigating misalignment risks — either very directly, as in people are like, “AI was a big deal and a big problem in this way, so maybe it’s a big deal and a big problem in that way” — but also maybe much more indirectly: it causes slow AI progress or more cautious AI progress because there’s a lot more regulation.\n\nRob Wiblin: Are there any other hopeful categories, or is that mostly covering it?\n\nRyan Greenblatt: So there’s things are easy on timelines and takeoff, things that are easy on technical, and there’s societal. A fourth category is we rise to the occasion. Maybe even if society doesn’t rise to the occasion, maybe just a variety of heroic efforts — or hopefully not that heroic —\n\nRob Wiblin: If one company, really the leading company, turns out to actually be very impressively responsible and puts a lot of effort into this and they succeed.\n\nRyan Greenblatt: Yeah. I think a lot of the risk is coming from worlds where you could have saved the world if you just had a year of delay, and you were taking the situation very carefully, and you were being thoughtful about all the various considerations. Or at least the risk is a lot lower. Maybe it’s not minimised or it’s not eliminated. Certainly I wouldn’t say that results in a level of risk that would be broadly acceptable from my perspective. But I don’t know. It could happen.\n\nRob Wiblin: Cool. You literally have cheered me up a little bit there, by reminding me that there are other ways that things could go. What are some lines of evidence that suggests that it could take substantially longer than four or even eight years?\n\nRyan Greenblatt: For one thing, I should reiterate that I don’t think that we’ll see full automation or the capability to fully automate AI companies in four years. So I must think there are some lines of evidence that this isn’t going to happen.\n\nI think the first place people should start is just scepticism about crazy shit happening in a specific time frame. So I think there’s a bunch of evidence that suggests we might expect it soon — you know, we’ve had rapid AI progress, I think that has been hitting a lot of things — but if you’re just like, how much progress have we had to date? How long has the field been going? And you’re sort of operating from a very outside-view priors-y perspective, where you’re like, what is the base rate of technologies, then you don’t expect in the next four years — because this is a crazy technology.\n\nI even think the base rates are maybe more bullish than people tend to think. I think often when people do this outside-view base rates thing, they end up with like 200-year timelines or something. But actually, Tom Davidson has a report from a while ago on like if you just try to apply the outside view most naively, you actually end up with quite a large probability in the next century.\n\nRob Wiblin: Is that because there’s been very rapid increases in the amount of investment in the area? And we’re just increasing the orders of magnitude of compute that we’re throwing in, such that I guess if the difficulty was distributed across log space, then we’re actually —\n\nRyan Greenblatt: Resources and compute and labour. Yeah, we’ve crossed a lot of that. So that’s one view.\n\nYou can also just be like, even if you just look at time, we’ve just only been doing serious AI research for not that long. You could use how long have we been doing deep learning? How long has anyone been working on AI? And if you sort of mix this together, it feels like powerful AI soon is in some sense not that unlikely on priors.\n\nBut I still think that how unlikely is it on priors suggests much longer, and I think that pulls me towards longer.\n\nGenerally another perspective that pulls me towards longer is just being like, the AIs are pretty smart, but they’re not that smart. And I think we just can’t have that much confidence that the next part of progress is doable.\n\nThen there’s some more specific object-level arguments.\n\nI’m sorry to deflect the question a bit, but I always feel tempted to go in and do some of the more arguments for short timelines that you’re bringing up. So another perspective on short timelines that I think is pretty important here is we are going through a lot of orders of magnitude of compute and labour pretty quickly that we haven’t gone through before.\n\nAnd in addition to this, we’re going through orders of magnitude that are somewhat above our best guesses at the amount of human brain compute. So we have these not-very-good estimates of how much compute the human brain is using that indicate maybe it’s like 10^24 as a central estimate of lifetime human brain compute. Currently training runs are about… I think Grok 3, which was just trained recently, I think I saw the estimates were like 3 x 10^26 — so two and a half orders of magnitude above human lifetime.\n\nAnd then we might think that you first reach AIs capable of basically beating humans, above human lifetime compute — because generally we develop algorithms that are less efficient than biology first, and then they potentially pretty quickly become more efficient than biology.\n\nThis is in the literature, in Ajeya’s old bio anchors report for the people who are familiar with that, what is called the “lifetime anchor.” And I put a decent amount of weight on the lifetime anchor. It looks pretty good, because we’re hitting these around humanish capabilities around this point where we’re hitting human compute. So it just feels like that model has a lot of support, and that suggests that we might be hitting it in several orders of magnitude more compute.\n\nAnd we’re just sort of burning through these orders of magnitude of compute very quickly. For context, I think training run compute is increasing by about 4x per year. So you go through a lot of orders of magnitude pretty quickly. It’s like a little over an order of magnitude per year at that rate.\n\nAnd also, I just generally think we should put some weight on a very compute-centric view on AI development — and on the compute-centric view on AI development, even with something less specific than lifetime anchor, we’re burning a lot of orders of magnitude starting from a pretty competitive starting point. Maybe we go pretty far.\n\nNow the bear case, relative to that, is that we can only burn through orders of magnitude so far, right? We’re not that far off of hitting the total amount of chips you can produce. I might be messing up the statistic, but a pretty reasonable fraction of TSMC or semiconductor manufacturing capacity is going towards machine learning chips. My cached number is 10% to 20%. I hope I don’t get this horribly wrong, but I think it’s definitely above 1%. It’s well above 1%.\n\nSo one thing that’s interesting about this is that Nvidia revenue is roughly doubling every year, and I think that the number of wafers or whatever — I hope I don’t brutalise the thing; I’m not a semiconductor expert here — I think semiconductors for AI have been increasing a little over 2x per year. Just to make it simple, let’s do a slightly bullish estimate of like 3x per year. Well, one thing that’s worth noting is if you’re starting at 20% and you’re increasing at 3x per year, you do not have that long to go through before you’re just hitting limitations.\n\nRob Wiblin: You’re using all of the chips for this.\n\nRyan Greenblatt: Yeah, you’re using all the chips, and once basically like all of the fabs are producing AI, you can only go so fast.\n\nRob Wiblin: I guess then you’re limited by how quickly they can build new fabs, basically.\n\nRyan Greenblatt: Yeah, limited by how quickly they can build new fabs.\n\nThere’s other sources of progress, to be clear. So even if we’re sort of limited by building new fabs, there is still potentially sources of progress from hardware improving over time, and there are sources of progress from algorithmic development. Though I think a key thing that maybe people aren’t tracking enough is that algorithmic development is also driven by dumping in more compute. So we should expect algorithmic development to slow.\n\nSo I think the bear case is maybe: you have AI keep going. We’re already hitting insane amounts of spending. As you’re starting to hit the TSMC limits, the spending has to go even higher to justify building new fabs.\n\nOr maybe prior to hitting most of TSMC production, maybe people are just not seeing results sufficient to justify these levels of investment. I think Microsoft, Google, they can justify potentially like… I mean, Microsoft is I think planning about $100 billion in capital expenditure this year. Stargate has about $100 billion committed, maybe that’s over a year or two, and then maybe they’re hoping they get more money. Stargate is a project by OpenAI.\n\nBut I think once you’re starting to get beyond this $100 billion regime, it’s no longer sufficient to just have a big tech company that’s super sold, right? Google does not have the ability to readily spend a trillion dollars, especially not a trillion dollars in a year. I mean, I think it is not out of the question that you could raise a trillion dollars, but I think you would probably need very impressive results, and you need to be starting to pull in more sceptical investors, more revenue. You need to be talking about potentially sovereign wealth funds. Certainly it\n\n\nIt’s possible —\n\nRob Wiblin: The US government could deliver that kind of money.\n\nRyan Greenblatt: Yeah, that’s true. Certainly it’s possible.\n\nRob Wiblin: Although I guess probably if you try to spend a trillion dollars in a year on this kind of thing, you start hitting other bottlenecks.\n\nRyan Greenblatt: For sure. Yeah, yeah. I don’t currently know what the elasticity is here. I think Epoch did some estimates of like, what is the biggest training run you could do by 2030? I think their median, in the timeline where people keep trying to spend aggressively, was maybe you can get up to 10^30 FLOP training runs — and that was taking into account data bottlenecks, various considerations on bandwidth between chips, various considerations about how much you can scale up chips, chip production, how much TSMC is building up capacity, this sort of thing.\nMy guess is that that’s probably a pretty reasonable guess independent of AI acceleration. AI acceleration could make this faster. But if we’re in sort of a bull timeline, but not a bull timeline where the AIs have started to speed things up, I would guess that’s a pretty good estimate. But there’s a chance we fall off of that because maybe the bottlenecks hit harder than they were expecting, and maybe investment dries up faster. Hopefully they don’t call me out on this, but I think they weren’t taking into account willingness to pay on this.\nBut after the 2030 point, I think things are starting to get a lot harder. I think if we’re getting up to 10^30 FLOP — you know, bare metal FLOP, just the actual computations the GPUs are running — starting from where we are now, which is more like a little over 10^26, then we’ve hit four orders of magnitude in the next five years. So that’s quite fast.\nI think that is enough that we could in principle see the trends that we’ve seen continue, like the trends that we have. I think we’ve had a bit of a GPU lull. I think we’ll have GPUs sort of start building up. I think post GPT-4 there’s a bit of a lull as people are trying to buy all the H100s. I think we’ll see a run of models with more H100s, like GPT-4.5, and then we’ll see another round with Stargate, which is another big buildout. SemiAnalysis has speculated that Anthropic has a large number of chips from Amazon, maybe equivalent to around 100,000 or 200,000 H100s, so we’ll see another round of 100,000 to 200,000 H100 clusters. And then we’ll see maybe, I’m going to get the dates wrong, but maybe around 2028, we’re going to see more around the million GPU range.\nBut if we’re not getting to really powerful AI by then, I think you should expect things to slow. There’s this longer tail of we eat a bunch of compute around 2030, 2032, and then progress has to taper — unless we hit very powerful capabilities, or we’re all wrong about how fast you can build fabs or how much investment you can summon.\nBut I think that’s a lot of the bear case is maybe you do a bunch of the scaling, but you hit these limits. Not all of it.\n\nRob Wiblin: So the short version of that, for people who didn’t follow it all, is that we’re currently increasing the amount of compute that is going towards training AIs very quickly. And we’re not going to be able to maintain that pace, because we’re currently doing that by grabbing some chips that were being sold for other purposes and using them for machine learning instead.\nAnd also, these companies are going from spending 1% of their resources on AI development to 10% — and then maybe they can go to 100%, but they can’t really go beyond that by just grabbing resources that were previously going towards other stuff.\nSo at the point where almost all of the chips are going towards AI training and almost all of the resources of these companies is going towards that, it levels off the rate of increase that you can get there.\n\nRyan Greenblatt: For sure. Maybe a few minor clarifications on that. One thing is, when you’re thinking about repurposing the chips, this isn’t like they’re taking iPhone chips and repurposing. It’s like there was capacity to do chip manufacturing that was relatively general purpose, and instead of making as many iPhones, we’re now making more AI chips. And some of that’s coming from building additional fabs, but some of it’s coming from slightly increasing the prices of other chips, or reducing how many of them you’re getting.\nAnd when AI is in the 20% or 30% regime, it’s not going to have very noticeable market effects on how much other chips are costing, because TSMC expanding somewhat faster can keep up with that. But once it’s like AI chips are 80% or 100%, then we’re going to start seeing bigger effects and things slowing down from there.\n\nRob Wiblin: Another thing that makes it a little bit difficult to forecast all of this is that we’re not just doing exactly the same thing year after year; we’re not riding the same trends year after year.\nInitially, for example, we were getting a lot of improvement by putting more compute into the pretraining. This is the thing where you dump in all the text from the internet and try to get it to predict the next word. We were getting enormous gains from that by throwing in more data and more compute. But then that sort of tapered off, and we have to move towards better elicitation using post-training, reinforcement learning from human feedback. Then we’re doing a different thing, which initially is very effective, but then begins to level off.\nI guess now we’re using reinforcement learning to do a sort of self-play, where the models are learning to reason better, and basically we just reinforce them when they get the answer right. We’re on a very steep curve with that, but presumably that at some point will level off and we have to do a different thing.\nIf I understand that correctly, that makes it more difficult, because we don’t know exactly what will be the innovation next year that will be driving the improvements.\n\nRyan Greenblatt: Yeah. The improvements from GPT-1 to GPT-2 to GPT-3 to GPT-4 — and then maybe even throughout there, there was a bit of a lull in 2023 and then things picking back up in 2024 — I think a lot of the improvement there, up until maybe the middle of 2024, was driven by scaled-up pretraining, like dumping in more data.\nThere’s sort of a meme going around that pretraining has hit a wall. It’s not so clear to me that this is what’s going on. It probably is the case that it has relatively diminished returns, or the marginal returns of scaling that up further are less than they were in the past in terms of qualitative capabilities.\nBased on some sense of like, we have Grok 3 — which is maybe a little over 10x more compute than GPT-4 and also better algorithms — and how much better is it? It’s somewhat better, but I think it’s also the case that it’s worth noting that in previous improvements — like the GPT-3 to GPT-4 gap — it was a lot more compute. I think that gap was like roughly 100x bare metal compute. And it just turned out that at the time there was a lot of low-hanging fruit in scaling up the amount of FLOP you’re spending very quickly.\nAnd now we’re running into bottlenecks. Post GPT-4 they were sort of waiting for H100s. The H100s were slow to get delivered. We were getting the H100 clusters online kind of late, and maybe there were some difficulties with getting that initially working. So GPT-4.5, somewhat disappointing. I think it’s rumoured that OpenAI had multiple, or at least one, failed training run.\n\nRob Wiblin: Oh wow.\n\nRyan Greenblatt: So it wouldn’t be surprising if we see people adapting to more compute, figuring out how to use it, and the returns from pretraining sort of going up from there.\nThat’s pretraining, and maybe even if pretraining is diminishing, you can scale up RL, reinforcement learning. We’ve seen that going on over 2024. We have o1, where they’re training on RL. They’re training on easy-to-verify tasks, not training on next token prediction. And we’ve seen a lot of initial returns from that and we don’t know where that will peter out.\nYou know, we haven’t seen that many orders of magnitude on RL training. It’s speculated, for example, that DeepSeek-R1 was about $1 million worth of compute on the RL. So in principle we can scale that up to be more like three orders of magnitude higher with the clusters that people will have. Maybe a little less than that, but ballpark.\nOnce we’re talking about three orders of magnitude higher, it could be that that yields huge returns, or it could be that that doesn’t yield huge returns.\nThe story for yielding big returns is it yielded big returns from the first million. Maybe you just go further, build more environments, do more RL big returns. The story for not is maybe there was some sort of latent capacity or potential the model had, which RL is bringing out. We’ve brought out most of the potential, and we’re hitting diminishing returns.\n\nRob Wiblin: Just to say that back, for people who don’t follow this very closely and are not sure exactly what you meant: one thing that is actually worth remembering is that I guess we used to use reinforcement learning a lot, and then it somewhat fell off, and now it’s come to the fore again.\nThis is where you take the existing models, GPT-4o or something like that, and you give it very challenging reasoning problems, and you maybe get it to try 100 different solutions, 1,000 different solutions. And basically you just find the cases where it gets the right answer at the end and you say, “Well done. Reason more like that to try to solve other problems in the same way that you just did.” And that’s turning out to be extremely powerful.\nI guess you’re saying that that is maybe picking up some low-hanging fruit, where these models had more ability to do clever reasoning latent in the weights than was initially apparent, than we were able to get out of them. And by using this process to get it to try all kinds of different solutions and then find the reasoning processes that are functioning well, we’re extracting a whole lot of stuff that was just sitting there waiting to be picked up. But that could run out somewhat, and at that point it’s going to be a heavier lift for them to basically learn new superior reasoning techniques.\n\nRyan Greenblatt: Yeah, that’s right. People were trying to get models to reason in chain of thought. So as of GPT-4, people were totally aware that you could do chain of thought reasoning, and were messing around with this. But I think it only worked so well. The model wasn’t that good at recovering from its mistakes. It wasn’t that good at carefully reasoning things through.\nAnd I think we’re seeing with o1 and o3 and other reasoning models that now you can make it so the model can do the reasoning pretty well. And a bunch of that might have just been chain of thought, it was already sort of doing it — and maybe you can just make it somewhat better, but there’s a question of how much better.\n\nRob Wiblin: Another piece of low-hanging fruit that you mentioned was basically sometimes we were giving these models very difficult challenges, but giving them only $1 worth of compute to work with. And people have found that if you actually just give them more like the kind of resourcing that you would give to a human — $100 or $1,000 worth of equipment and salary — then they do radically better when you’re doing something that’s a bit fairer of a comparison.\nBut you kind of can’t do that scaleup again. You can go from $1 to $1,000, but if you’re going from $1,000 to $100,000, now you’re talking real money. And there’s a question of would this ever actually be economically useful just to give a model so much to just solve a maths problem?\n\nRyan Greenblatt: Yeah. One thing that’s also worth noting is, even if it would be economical, we quickly run into just the total quantity of compute.\nSo suppose that you’re having the model do one task for $100,000. I did some kind of crappy BOTEC [back-of-the-envelope calculation], and my sense was that, at least as of a few months ago, the total amount of compute that OpenAI had was about $500,000 per hour. So if it’s a $100,000 task, then you’re using a fifth of all of OpenAI’s compute for an hour. So you can only do so much of that, right? Even if it was the case that there’s some really economically valuable tasks, you just hit bottlenecks on that.\n\nRob Wiblin: Because that drives up the price.\n\nRyan Greenblatt: Yeah, it’s the supply and demand. Even if it’s the case that it’s useful enough, there’s really only so much scaling of that that you can do. And, you know, you can go pretty far: people have been demonstrating not just human cost, but substantially above human cost, yielding additional returns.\nAnd this is kind of nice because it lets us get sort of\n\n\nA sneak peek into the future.\nI think if you’re seeing the models do a task, but slowly and at very high cost, we should expect that soon enough that will be quickly at a much lower cost, because the cost is rapidly dropping.\nRob Wiblin: We can draw out the curve.\nRyan Greenblatt: Yeah, we can draw out the curve.\nSo I’m somewhat hopeful for people who are more sceptical about AIs exhibiting some capability, maybe first we can exhibit it with very high runtime compute, a lot of domain-specific elicitation, and then pretty shortly after that the need for that goes away.\nHopefully then we can reach common knowledge somewhat before the capability is widespread enough to be incredibly widely used.\nRob Wiblin: We were dipping in and out there on the bear case, or the case where we’re thinking about this is going to take a long time.\nRyan Greenblatt: I’m such a bad bear.\nRob Wiblin: Well, I’ll try to make one of the bear arguments that I hear.\nThe AIs might get very good at fairly narrow tasks, and maybe they become really good coders, they become really good at idea generation, hypothesis generation, setting them up.\nBut there’s more to running and scaling an AI company than just those things, and they’ll have some serious gaps and weaknesses, and that is the thing that will be the limiting factor and slow it down.\nHow plausible do you think that is?\nRyan Greenblatt: Some context here: historically, as we were saying, a lot of the returns were driven by pretraining.\nAnd then starting later in 2023 and in 2024 there was RL — where a bunch of the returns from GPT-4 Turbo were driven by RL, or was improving over time probably.\nThis is speculation, but GPT-4o is a somewhat better base model, but also better RL and then we had o1, better RL.\nAnd this has been driving up benchmark scores on programming tasks, coding tasks, math tasks — tasks that are checkable much more than other tasks.\nNow, we do see some transfer.\nOne way out of this is maybe it’s just the case that you make the AIs very superhuman at programming, quite superhuman at software engineering — which is somewhat harder to check, but doable to check, or at least parts of it are pretty readily possible to check — and very superhuman at math.\nAnd then this transfers somewhat.\nSo maybe it’s the case that it’s very expensive to label other things.\nYou know, we can assess human performance in other domains; you just can’t do it automatically.\nSo you can assess how good a paper is, and we have processes for doing that, which has some signal; it’s just much more expensive.\nSo in principle, if the AIs can almost transfer to writing good papers and need just a little bit of feedback — they’re quite sample efficient — then that can go pretty far.\nAnother thing worth noting is I think there is a “no generalisation required” story.\nOr nearly no generalisation.\nSo it might be the case that you can basically make it almost work, or basically work, to just do RL on things that are just straightforwardly part of being a really good research engineer.\nWe might be able to get to the point where we have nearly fully automated research engineering at AI companies just via scaling up RL, piecing it together, and not really requiring very much transfer at all.\nSorry, we require in-domain generalisations — we require the AI is improving in domain relatively quickly and relatively efficiently — but even if the AIs don’t have very good research taste, they’re not very good at these things, that can go pretty far.\nNow, in addition to this, you might be able to get surprisingly good on some of these other things of running an AI company even with just in domain, not requiring much transfer.\nSo we have these sort of research engineer AIs, for one thing: maybe that makes progress go faster starting then, and then we sort of kick off more progress that eventually gets us a bunch of other things.\nI can go through that story later.\nBut second of all, if we have these research engineer AIs but they’re not going to have research tastes, they’re not going to have a bunch of other things that are harder to train…\nWell, one thing is a lot of what we mean by research taste is understanding what the results of experiments might be, having good predictions, being able to sort of understand what’s going on based on a smaller amount of evidence.\nAnd for this, you can maybe just train AIs to be amazing forecasters of ML research experiments.\nAnd if you’re training these AIs to be superhuman ML project forecasters, where they’re predicting the results based on just generating a large number of smaller scale ML projects, having some transfer to larger scale ML projects — which is going to be less easy to readily get data — then maybe from there you have these AIs that can predict the results of experiments as well as, you know, Ilya Sutskever or Alec Radford.\nMaybe they are less relying on insight and more just, mechanistically, you generate a long list of ideas, you predict how well they go, you proceed from here.\nAnd I think there’s basically a bunch of routes like this.\nAnother route is maybe you can just do within domain, and that can go surprisingly far in automating the AI company.\nAn important asterisk for this is, I think the sceptics maybe are sort of screaming to me, “But what about things other than automating AI companies?!”\nI think maybe they’re like sure, AI R&D, I guess that’s checkable enough.\nBut what about all the other things that humans do which have slower feedback loops, like being a good strategic CEO?\nRob Wiblin: Fundraising?\nRyan Greenblatt: Fundraising, all these other things.\nSo I think I want to put a pin in that, but maybe we’ll get back to that later.\nThen the second thing I want to say is, so there’s the in-domain story and then there’s the more generalisation story — where you train on all these narrow tasks, the AIs get smart; with just a tiny bit of data, they can transfer to these out of domain or we didn’t have a tonne of data things.\nI think it’s going to be some interpolation between these probably, where it’s harder to get lots and lots of data on long horizon SWE tasks, so you probably need a bit of transfer but you can probably do a bunch of data.\nAnd then there’s I think a third story, which is like people just figure out how to RL on harder-to-check tasks using other mechanisms, using things that are more like process-based checks, or being able to check fuzzier tasks using things more along the lines of like self-critique, constructing more detailed checks themselves, and doing RL from here.\nHumans, interestingly, are able to learn on lots of tasks that are somewhat fuzzy to check via having some sort of notion of self-critique — How well did I do?\nShould I do more like that, less like that?\nAnd I think the AIs can currently do this a bit, but not amazingly.\nAnd I can imagine this getting better over time.\nAnd I think a thing that we’ve seen time after time is, if a given thing is the limit to capabilities, there is just a huge amount of horsepower in the broader ML community, but I think broadly in the AI companies, to sort of push on that, whatever the limit is.\nRob Wiblin: There’s been a shift over the last year from almost all or a very large fraction of the compute being spent during training runs towards a larger fraction being spent during inference or runtime compute — so rather than use your compute to make the model better at predicting the next word or more likely to be positively reinforced during training, you actually just give it an enormous amount of time to think about the specific task that you’ve given it.\nHow does that change the picture around the time that we’re automating a lot of AI R&D?\nI think many people think that, on the inference-compute-centric paradigm, this makes things go a little bit slower, because you’re going to be so limited: if it turns out that it requires an enormous amount of compute to run the equivalent of one AI researcher, then at least to begin with you just won’t be able to staff yourself up with a million equivalents of them.\nMaybe you’ll only be able to have 100 or 1,000 to begin with and then it will gradually go up.\nRyan Greenblatt: For one thing, to the extent inference compute wasn’t priced into your prior model, then it should push you to having relatively shorter timelines to the relevant milestones.\nBut maybe the first time we hit a given milestone, it’ll be very expensive.\nSo maybe we’ll first be able to be like, we have this AI that could automate the job of a research engineer at our company for the cost of maybe $10 million or $20 million in compute per year.\nAnd as I was noting earlier, there is just actually a limited supply of compute.\nSo even if, in principle, you’d be happy to automate all your employees — maybe you’d be happy to automate 1,000 employees at $10 million a year: that would be $100 billion per year, which is getting close to the amount of compute the company even has in this regime, right?\nSo we can look at how much total capital expenditures have people been spending?\nThat’s roughly the same ballpark.\nSo if it was even more extreme than that — maybe it’s more like $100 million per year — then all of a sudden, they just might not have the money to do that, and they might not have the compute to do that.\nAnd also, it might just not be economical.\nIt also might be the case that with this inference compute it’s too slow.\nIt could be that maybe it would work if you could make it fast enough, but a lot of the ways of scaling up inference compute might make it serially slower, which reduces a lot of the competitive advantage AIs have.\nRob Wiblin: What do you mean by that?\nJust because it has to do a whole lot of things one after another, it just takes a very long time to actually output an answer?\nRyan Greenblatt: Yeah, so for example, I think o1 often answers questions slower than humans would answer those questions because it spends so long thinking.\nWhereas previously AIs were almost strictly faster than humans, occasionally o1 is now slower or more often in the same ballpark.\nAnd we might see inference time scaling that involves more serial steps having this property.\nI think there’s various ways that this could be mitigated, such that I don’t know if this is a huge obstacle.\nI would probably guess not, at least once optimisation has been applied, but it could initially be an obstacle.\nSo in general, I think the inference time compute should expect that we can see capabilities exhibited prior to them being economical, and we start seeing capabilities at small scale prior to large scale.\nWhereas I think a surprising thing about prior to the pure pretraining paradigm is that you naively expect that, sort of at the point the capability first becomes available, you can run truly a vast number at that capability level.\nMy sense is that this is still broadly going to be true, basically because I think distillation of high inference compute is relatively quick, and inference compute I think only gets you so far.\nSo I think it can get you large gains.\nBut I think you have to be quantitative about how large the gains are.\nSo if that can be distilled away relatively quickly, which I think we’ve seen — I think we see pretty fast distillation progress going from, for example, o1 to o3 mini — I think we saw pretty fast gains of both just scaling up training, but also being able to distil that more effectively down into a smaller package.\nRob Wiblin: So distillation is where you make the model a lot smaller and it performs almost as well.\nSo you could operate more of them in parallel, or it can give you more answers more quickly.\nRyan Greenblatt: Yeah, and I should say distillation is sort of a special case of a broader thing that sometimes it’s easier, at least initially, to make something much cheaper than it is to make something much better.\nSo say we have some new capability we’ve been demonstrating, we sort of push the frontier, then we can bring the cost of that point in the frontier down pretty quickly, is what has happened historically.\nAnd distillation is where you train a smaller model on the outputs of a bigger model.\nRob Wiblin: In the inference compute paradigm, don’t those two things kind of converge?\nBecause you’re saying, if you can make it a tenth as large and it’s almost as good as thinking as it was before, then you can allow it to think 10 times as long, effectively?\nRyan Greenblatt: Yeah, but of course the returns to inference compute potentially could diminish pretty quickly.\nThis is maybe not the most relevant benchmark, but for example, in ARC-AGI, what we see is they went from ballpark $3 per task or $10 per task to over $1,000 per task.\nSo maybe a little over two orders of magnitude of cost.\nIn those two orders of magnitude of cost, they push the performance from I think around 76% or 75% up to 85%.\nAnd for humans, I think moving through the human regime of 75% to 85% is not that much.\nI think similarly we see stuff like, if you sample the model 64 times, you pick the most common answer between that, you’re seeing relatively marginal improvements from that.\nSo in general, I think there’s a question of how efficient are these inference time strategies?\nI think that it depends on the strategy, but you might hit diminishing returns.\nRob Wiblin: The overarching theme here has been arguments to expect AI R&D automation sooner versus later.\nAre there any other key factors that I haven’t given you a chance to talk about yet that bear on that question one way or the other?\n\n\nRyan Greenblatt: I think one important question is about how there were people shifting towards maybe pretraining is hitting a wall. So I want to dive into why this might be true — to the extent this is true — and also how true is this.\n\nSo I think one big reason why we might expect this is that it might be that there are issues with data quality and data quantity. You know, DeepSeek recently trained DeepSeek-V3 with just a tiny amount of money, and they trained it on about 15 trillion tokens. So it might be the case that you can train on lots and lots of data, but it might be that there’s kind of steep diminishing returns to data quality.\n\nThere’s not a lot of public evidence about the extent to which this is true. But if that’s the case, then it might be what happens is, once you train on the first 15 trillion tokens, the next 15 trillion tokens are a lot less valuable.\n\nIf you imagine scaling up the DeepSeek-V3 training run by a factor of 10, then based on Chinchilla scaling laws, which is just how much should you scale up the size of the model and the data in parallel, you would scale up the data by 3x and you would scale up the model size by 3x. If you did that, you’d be at 45 trillion tokens. So it might be that if you’re at 45 trillion tokens, the next 30 trillion tokens would be a lot worse — either because it’s repeated epochs on the same tokens or because it’s lower quality tokens. So it might be the case that you can stretch things far, but the returns start diminishing around now because of data filtering.\n\nAnd this might be one reason why pretraining scaling maybe looked somewhat more promising in the past than you might think it is now, because there was worse filtering. Maybe a lot of the GPT-3 to GPT-4 improvement was, as they were training on more tokens, they were getting more of the good tokens in there. But now we’re in a regime where we can really pull out the good tokens and make sure to train on them, and that could yield diminishing returns.\n\nSo I think this is a reason why we might expect pretraining is slower, but I don’t think this is an argument that pretraining returns would totally fall away. You can just train on worse tokens, train for more epochs, find methods to get more juice from the same tokens and get that. But you’ll see a slower rate of progress.\n\nAnd in addition to that, there’s also the option of instead of doing pretraining, just do way more RL. We were talking a bit earlier about how maybe RL has diminishing returns. But even if the returns diminish, it might still be that the returns are high enough that they’re higher than pretraining, where you can dump in exponentially more compute and you get linearly more performance — but qualitatively, linearly more performance is a big deal.\n\nAnd in addition to that, I think we’re just very uncertain about how true all this is. And there’s potentially a bunch of scaling left, even if the returns are weaker than people were previously thinking.\n\nRob Wiblin: OK, just to try to say some of that back: pretraining is when we take a huge corpus of information, of text, to try to predict the next token. It seemed like scaling up the amount of data going into that process was very valuable in the past. But it’s possible that that is levelling off to some extent.\n\nAnd part of the reason for that, you’re saying, is I guess to some extent they’ve actually run out of new data that they can collect. They’re getting close to grabbing all of the good text that humans have actually written. But also, they got better over time at filtering out what is actually the high-quality tokens, what is the high-quality content that they want the model to be training on a lot, and what stuff maybe could they discard. So presumably they’re keeping in published books, textbooks, that kind of thing, and putting extra weight on that. And then just random slop taken off the internet that has no particular information in it, they’re managing to exclude that.\n\nAnd I guess you’re saying, having filtered out the good stuff, the only stuff that they can add is really quite low quality, so they’re just not actually getting very much juice out of that. And that might help to explain why data scaling hasn’t been adding as much value now as it used to.\n\nBut you’re saying they can take the effort that they were putting into that and use it to improve the reinforcement learning process — which is a different way of trying to improve the model: rewarding it for successfully answering questions and rewarding it for having good thinking in the process of doing that. Have I understood right?\n\nRyan Greenblatt: Yeah, that’s right. I think also people sometimes talk about synthetic data. I like to think about synthetic data as a sloppy version of RL. What you do is you get another model, you get it to generate some data, and you get that data to be somewhat improved from what it was just generating by default. So maybe you only select cases where it got it right; maybe you have it revise its answer, or you let it try to solve the math problem, then you show it the correct answer, and then you have it correct its chain of thought. You throw that into the pretraining corpus, and maybe that data is somewhat valuable.\n\nThis is in some sense similar to RL, because you’re finding model trajectories that are good and training on them, but it might have somewhat different properties. And you can scale this up, generate a lot of synthetic data, and I think this will yield some returns; I think this will have some improvement, and you can scale that up further.\n\nSo let me try to make the extreme bear case. The extreme bear case is like: DeepSeek-V3 came out recently, had a $5 million training cost. In parallel, we saw Grok 3 come out also pretty recently, or DeepSeek-V3 is somewhat further in the past. The difference in cost is I think about two orders of magnitude, about a factor of 100. I think DeepSeek-V3 was trained on about 2,000 GPUs. Grok 3 was trained on about 100,000 GPUs, very roughly speaking. So maybe it’s closer to a factor of like 50 or so, maybe roughly around there. So we have this factor of 50 in pretraining compute.\n\nAnd then if you look at DeepSeek-V3, qualitatively at least, it’s not that much worse than Grok 3. So what’s going on? What explains these returns?\n\nThere’s the data scaling story, which is like, maybe they were already training on the juiciest 15 trillion tokens and the stuff that xAI was able to scrounge up was not as good. And so they would scale up the model, you scale up the data and maybe they didn’t get awesome returns from this.\n\nAnother story is just the returns are weak, which is plausible from the evidence we have. Then the question would come down to, can you switch to something more like RL?\n\nAnd then I think another story which is important — and is a big part of my model — is I just think DeepSeek has substantial algorithmic advantage relative to xAI, right now at least. I think DeepSeek-V3 probably was just actually a better optimised training run, where they were using the FLOP more effectively — both because of better hardware utilisation and training at lower precision, which is a technique where, instead of storing the numbers with a bigger representation, use a smaller representation. It’s a bit more efficient. You just do it with less accuracy but you can get similar performance.\n\nAnd then in addition to this, I think just actually having a better-tuned pipeline and better architecture potentially. That said, an important part of why DeepSeek was able to have a better architecture is they were doing a smaller-scale training run, which meant that they could run more experiments at that scale and really iron out all the kinks. So maybe you’ll be able to iron out the kinks, but every time you go to a larger scale for the first time, you’ll run into some issues. There’s some rumours of this happening at OpenAI; it’s plausible that xAI ran into similar stuff. So I think we’re seeing probably some of that.\n\nSo maybe my view is that the actual amount of effective compute that Grok 3 is above DeepSeek is maybe about a factor of 10, if I had to guess. Maybe it’s a little more than that. Yeah, I think about 10 because it’s maybe like 50 experimental, but then there’s a bunch of efficiency gains that shrink that lead. Plausibly I’m off base here, maybe someone will call me out on this, but that’s kind of my guess.\n\nAnd I’m like, does this 10x scaleup match what you’d qualitatively expect? You know, it is actually a decent amount better, so I don’t think it’s totally implausible that if this is what a 10x scale up looks like, another 100x could plausibly be a pretty big deal. So that’s a bit more of the bull case there.\n\nAnother thing that I think is happening with AI progress that’s important to track is a lot of people are like, “Grok 3 is barely better than GPT-4. That’s two years ago, very slow progress. What are we doing?” But I think an important thing is people are comparing it to recent releases of a model called GPT-4 versus the original release GPT-4. So I think OpenAI’s naming convention here has caused people to get frog boiled or underestimate the rate of progress.\n\nSo we had the original GPT-4 release back in 2023, near the start of 2023. That model was pretty good. And then we saw OpenAI progressively release models they were still calling GPT-4. So they called the model GPT-4 Turbo and then GPT-4o, and these models were all somewhat better than GPT-4 — on both how good the pretrained model was and also on RL, or just a bit better. So we had very incremental progress while still calling it GPT-4, so people are sort of missing maybe roughly a little over an order of magnitude of progress from GPT-4 to the best version of GPT-4o, and they’re doing that comparison and missing a bunch of progress that happened in the meanwhile.\n\nRob Wiblin: Yeah, it’s maybe easy to forget how irritating the original GPT-4 was to use. I mean, it was incredible given our expectations at the time — I was blown away — but it was very fiddly, it wasn’t very good at answering questions. It messed up in much more obvious ways than what it does now. But I guess it’s called the same product, so now you just feel it was always GPT-4, and you forget that people were so fussy about how you had to prompt it the exact right way and you had to have expert prompt engineers. And that has kind of fallen away as they’ve just gotten better at following instructions much more sensibly.\n\nRyan Greenblatt: Yeah. As a concrete example, original GPT-4 could maybe do agentic tasks very poorly. Maybe it could do five- to 10-minute agentic tasks like half the time, like agentic software engineering tasks. And now GPT-4, it’s much more like an hour. So it’s really quite a large difference in terms of this downstream capability.\n\nSo that’s some texture there. I think it would be really nice if someone did some really detailed analysis trying to map out all the qualitative improvements and how much effective compute was in. Maybe Epoch should do that, or someone else should do that.\n\nRob Wiblin: Just to back up and talk about the Grok vs DeepSeek comparison and spell this out for people a little bit more clearly. You were saying people look at DeepSeek and they compare it with Grok 3 and say Grok is somewhat better, but it’s not radically better. And it looks like it was trained on 50 times as much compute, so what tiny returns we’re getting from scaling the compute input.\n\nBut you’re saying this is a bit unfair, because I suppose DeepSeek has — because of I guess limitations on access to compute in China — been working a lot on algorithmic efficiency in order to get the absolute maximum juice out of the compute that they have available. And I guess Grok is in the opposite situation, where they’re trying to scale and train and grow incredibly quickly, and they have access to a tonne of compute. So they’re not worried about the efficient use of compute almost at all.\n\nAnd you’re saying if you did a more like-for-like comparison, in terms of the algorithmic efficiency of the training, then you would say Grok 3 was trained on maybe only 10 times the effective compute, so it’s not nearly so large a scaleup. And so the fact that the improvement seems on the incremental is actually closer to what we would have expected anyway, it’s not actually a sign that compute scaleup is not useful.\n\nRyan Greenblatt: I mean, I don’t know. It’s definitely some evidence. And we should put some weight on no, it’s actually just 50x effective compute and this is what you get. And that would be I think lower than I would have predicted. So it’s an update there. But I think broadly speaking, yeah.\n\nIt’s also worth noting that it’s not just that I think DeepSeek was more focused on efficiency. For example, I think Grok probably has worse algorithmic efficiency than for example OpenAI and DeepSeek do. My sense is they’re trailing somewhat in algorithmic efficiency but are somewhat ahead on scaling up to very large training runs, so they’re just in a somewhat different position. Whereas\n\n\nMy sense is DeepSeek is maybe pretty competitive with OpenAI on algorithmic efficiency, at least at a small scale. And then another thing that’s maybe an important factor, we don’t really know, is that DeepSeek could practice their training run a bunch of times, because they can do that small-scale training run a bunch. So it’s not just that they’re optimising for efficiency; it’s that if you can run the training run multiple times, you might have more signal — whereas if you’re scaling up to a new order of magnitude for the first time, maybe you just mess some stuff up.\n\nRob Wiblin: Are there any other key pieces of evidence that bear on this timelines question?\n\nRyan Greenblatt: One thing that I think is a pretty spooky fact is that we’re sort of entering this reasoning model regime where people are scaling up RL and outcomes-based tasks. Right now people have just started doing RL. I think original o1 and R1 are probably almost entirely trained on relatively narrow short tasks with not that much compute. We have some sense of what R1 was trained on, for example. It seems like it was trained on math problems, like sort of trivia or questions like GPQA questions, which are science questions. And it was trained on maybe competitive programming or short programming tasks. But it was not trained on things like software engineering — tasks that would take multiple steps — as far as we’re aware. Probably was only trained on literally single-step tasks. Plausible they didn’t ever do training that involved multiple steps, at least not as part of their main RL phase. So to the extent that’s true, you might think that there’s a bunch of low-hanging fruit to just scale up the RL paradigm and apply it to sort of agentic tasks. In addition to that, there’s also just scale up the compute way further. You can scale on the diversity of environments and you can also scale on the amount of compute. I think Epoch did an estimate where they thought the RL on top of DeepSeek-V3, which was the model R1 was based on, was about $1 million. A million dollars is chump change in the AI industry these days. So you could plausibly be scaling it up by over two orders of magnitude within the next year. Now, there might be difficulties in getting that scaling, I think there might be infrastructural difficulties, but in principle it’s possible. So to the extent that we saw big gains from one order of magnitude, I’m just like, man, algorithmic progress, a bit of tuning of this stuff, we might be seeing crazy stuff in the next year.\n\nRob Wiblin: Sorry, o1 and o3 are also reasoning models that went through reinforcement learning. And I imagine OpenAI spent a lot more than $1 million on the RL for those models. So why does that suggest that? I mean, we don’t think o1 or o3 is radically better than R1.\n\nRyan Greenblatt: So first of all, I actually don’t know that I think that o1 and o3 were trained with much more compute than R1. I think we don’t know. Some reasons why you might think it wasn’t trained with that much more compute: one thing is that I think it is actually legitimately hard to scale up RL on an infrastructure level, and they might not have the infrastructure to do that off the bat. The second thing is it might be hard to quickly scale up the number of environments, but there is ultimately a scalable way to do this. So it might be that there’s just a bunch of returns. The next thing is we did see a pretty big performance improvement from o1 to o3, which is evidence that, if that trend continues, that could be pretty fast. So we’re just seeing they’re RLing on relatively narrow domains, but within those relatively narrow domains, progress appears to be very fast. So to the extent that they can extend the domains they’re training on, that might be broader than that, we might be seeing relatively fast progress. So I think it’s unclear. To the extent that o3 is saturating out a bunch of the lower-hanging fruit on this paradigm — which might be true to some extent; certainly it’s pulling some of the low-hanging fruit — then this story would go away. But to the extent that we have a lot of greenfield ahead of us on RL, I think this is probably one of the more compelling stories for one-year timelines or things going very fast. To be clear: I don’t expect this. I think this is unlikely, but I think one route is that RL generalises better than you expect, it can be extended slightly further than you expect. And maybe at the end of the year you have almost automated research engineer level capability, maybe somewhat below that — and then things could go really crazy from there.\n\nRob Wiblin: I see. You’re saying that’s not likely, but it’s a possibility. And if it does happen, this is kind of the pathway by which it would occur.\n\nRyan Greenblatt: Yeah, I think the most foreseeable pathway would be this scaled-up RL in one year. And this argument was brought to my attention by Josh Clymer, a colleague of mine. So credit to him.\n\nRob Wiblin: Is there anything more to say on this timelines question before we push on?\n\nRyan Greenblatt: I think another source of scepticism is like, sure, maybe you can get these LLMs to be pretty smart and pretty good at these tasks, but aren’t they going to need a bunch of other properties in order to replace humans? You know, they need to be able to learn on the job. Humans, when they do tasks, they’re learning how to do the very task that they’re doing. And AIs have worse sample efficiency, so you can throw a lot of stuff in context and they can sort of learn how to do things better based on that, but it’s relatively shallow, it’s relatively weak. Another thing is that AIs currently have limited context length and might have trouble tracking context over very long projects. I think the effective context length might be much shorter than the actual context length because they can do retrieval across the entire context length, but they maybe can’t do a synthesis across it as easily — because I feel like when I do tasks, I get some like vibes, level sense, I get better intuitions. I get some overall vibe of where the project is going. And I think AIs maybe have trouble tracking all this context, even if you throw it all in. And maybe there’s routes around that, so I want to talk a bit about these structural factors. For this learning-on-the-job thing: for one thing, we can do research on how to make within-context sample efficiency better. One route to this is, rather than having this more shallow architecture where you’re processing all the tokens in parallel and they can sort of attend to each other… There’s some ways in which the transformer architecture is fundamentally shallow. I don’t know how much you want to get into the details of that, but you could change to an architecture that isn’t as fundamentally shallow. In particular, there’s been some recent papers on having I would say more of our current architecture, where you can process the activations in a more deep serial way, which might allow the AI to absorb the context, and have more of a gestalt sense, and learn from what’s going on faster over more and more context.\n\nRob Wiblin: What do you mean by fundamentally shallow?\n\nRyan Greenblatt: What I mean is that if you look at a token, the model is sort of producing some distribution of probabilities on the next token. It can only have so many serial steps, basically because you run each layer on every token. And at each layer, you can attend to all the previous layers, but you can’t attend to a previous token at a later layer. So if you imagine you have 60 layers, layer 10 at token 40 can attend to layers 9 and before on all the previous tokens, but cannot attend to layer 60 on token 39. So that means that if the AI was getting into some good insights towards the end of its layers, the earlier layers of later tokens can’t take that into account. And I mean, the capabilities people are on it, as always — or maybe not as always, but to some extent — and are looking into ways to change this. I think that the way we currently have to address this is that, while the AIs are shallow in this way, they are not shallow with respect to tokens. So if you have a reasoning model, yes, it’s shallow in that sense, but also it can produce natural language tokens that are sort of its updated thinking, and it can keep doing relatively deep computation via that. So it can solve math problems with 50 steps by having all the steps in natural language, even though it can’t do all the steps in this relatively more serially bottlenecked forward pass — that’s the term for the activations of the transformer which have this property. But you might be worried that natural language is not that good of a medium for doing thinking — I sort of have thoughts that aren’t in natural language — but I think you could in principle have a deeper architecture. And I expect people are working on this, and I think this poses a bunch of safety risks, because we have this nice property that we can look at the chain of thought and get some sense of what the AI is doing and have some confidence in that — at least potentially have some confidence in that — because the AI is sort of forced to use the chain of thought in order to get this serial reasoning to work. But if it was the case that all that reasoning was latent, we just lose that property, and now we’re in a much more dangerous regime where the AIs could be doing subversive reasoning that we wouldn’t even know about, or would have no way of knowing about, by default at least.\n\nRob Wiblin: I didn’t totally follow that, but you said you would change how a forward pass occurs, such that it’s able to do more sophisticated reasoning within that, and that creates the possibility that it could engage in scheming before it’s actually outputting any tokens that we’re able to assess?\n\nRyan Greenblatt: Yeah, that’s basically right. So basically you should imagine the transformer is not like a looped architecture, it’s not recurrent. So your brain is recurrent: you think some thoughts, you think some more thoughts, and it’s fully recurrent — including, as far as we know, recurrent state that we can’t vocalise very easily. Like I think people can do reasoning that they’re not able to vocalise, some of the reasoning they can vocalise. And I think right now transformers can do relatively limited non-vocalised reasoning, and then a lot of vocalised reasoning. So it might be that you can change the architecture — it would be a large change to the architecture — in a way that makes it so they can do a lot more of the non-vocalised reasoning and more dense reasoning. And there’s some papers demonstrating this, like the Coconut paper from Meta. I think all the existing papers are relatively weak sauce, not really getting that much — sorry to the authors of those papers, but that’s my sense. But it could be that you can drive this architecture forward, and I think people haven’t necessarily really tried that hard to get this to work because there were low-hanging fruit elsewhere.\n\nRob Wiblin: Would this be very compute intensive to change the architecture in this way?\n\nRyan Greenblatt: Actually the thing I just described very naively would use exactly the same amount of compute on generation. Because right now you have to do one token at a time, and you could just loop the activations without that. Now, it is more compute intensive on training if you’re applying gradient descent all the way through. It makes the computation graph for gradient descent more annoying to do gradient descent on for some structural reasons I don’t know if we should get into. But yeah, roughly speaking, I think it is more computationally expensive at training time and would be more computationally expensive if you’re reading something. There’s all these cursed technical reasons why this is true, but basically if you were to apply this sort of looping when reading, then you would only be able to read one token at a time. Whereas transformers can process a whole body of text in parallel, so transformers are extremely fast at reading. So you can make it so a transformer reads a document that’s like a million tokens long, I think in principle, if you’re willing to scale up the compute, in maybe like a minute or 30 seconds — which is very, very fast. And plausibly you could even do faster than that, because it basically is like reading the whole thing in parallel and there’s a smaller number of serial steps. But if it’s reading the tokens one at a time, and you have to do all the layers, then it would be the same as generation speed — generation speed in a single context is more like 100 tokens per second by default, though people have exhibited faster speeds. So there’s ways in which it’s costlier, but I think ultimately the costs are not that high, and a lot of these costs are already being borne by the reasoning paradigm.\n\nRob Wiblin: All right. I’m curious to know: at the point that we actually are able to largely automate AI R&D, how do you think that process would play out? What would it look like? What are the different ways that it might play out?\n\nRyan Greenblatt: I think there’s this big question, which is: suppose that the AI company has fully automated AI R&D, even the best research scientists don’t add much value. Maybe they add a tiny bit of value, but basically the company is fully automated. I think there\n\n\nIt's been a historical view of people trying to do AI forecasting that you’ll get very fast progress at this point, because the AIs are automating R&D; it can run faster than it was when humans were doing it. Now there’s a question of how much faster. In addition to that, there’s a question of does the progress slow down? So it might be that the AIs are automating AI R&D, but they eat up a bunch of the low-hanging fruit, you have a limited labour supply, and the progress then slows down because you’re applying a lot of labour but you can only get so far. Another question is, do you run into a lot of bottlenecks on compute for experiments? So you have all these AI researchers, maybe way more than your human researchers, but maybe they don’t have much compute for experiments, and so they don’t have that much of an easy time yielding progress. There’s a question of like, how fast does it go initially? And does it slow down? In addition to that, it might be even not just that the progress continues at the same rate, it could be that progress speeds up. A way this could happen is you have your smart AI researchers, they do a bunch of algorithmic progress. You use that algorithmic progress to build a smarter AI, and that AI makes progress go even faster because you can do more labour at the same amount of compute. So even with a fixed amount of compute that the AI company has access to, progress could, in principle, go even faster and faster. Tom Davidson has done a bunch of modelling on this, on do we expect progress to speed up or slow down? And I’ll be stealing a bunch of stuff from him while talking about this. People have called this the intelligence explosion or the singularity of progress is speeding up. I think it’s important to note that my view is, even if progress is slowing down, it might be objectively very fast. It might be like you started at a high rate of progress and then it slows down over time. So one way of breaking this up is first we have to talk about the question of, how fast is progress initially? And then maybe we should talk about, does it speed up or does it slow down? And then from there we can be like, how much progress do we get, say, in the first year?\nRob Wiblin: Yeah. What determines the initial speed at the point that you switch it on and are automating almost everything?\nRyan Greenblatt: The very short answer is no one knows. The slightly longer answer is we can try to get some sense based on just having a sense of what algorithmic progress is driven by. So algorithmic progress in AI companies is driven by two main factors: labour — people working on it, people thinking of better algorithms, people implementing experiments — and compute, using compute for experiments. I’m going to separate out actually training the final model for now, so we’re just going to talk about this algorithmic progress. Historically algorithmic progress has maybe been going up about I think over 3x per year, including post-training. Maybe it’s been more like 4x or 5x per year. And when I say 4x or 5x, what do I mean? Like what units? It’s in terms of effective training compute: it’s like every year it’s as though you could train a model that’s four or five times bigger with the amount of compute you have. So that’s the initial rate of progress. Now what I’ll talk about is how much faster can the AI researchers make this happen? This is a bit of a tricky question to figure out because we have to answer the question of, if you make that so that there’s way more and higher quality labour, where do we go from there? Because we have these two inputs into production — labour and compute — and if we massively amp up the labour, do things just bottleneck in the compute, or can you push progress much faster? So I think a naive way to start to do the modelling is we have to be like, how much labour is there, how much AIs, how good are they, how fast?\nRob Wiblin: The compute available for experiments might even decline because you now have to use your compute to run your AI researchers, right?\nRyan Greenblatt: Yeah, for sure. I think this is probably a small factor, because — I mean, we have no idea — but my guess is that the optimum is, of the compute on algorithmic progress: a fifth on AI labourers, and four-fifths on running experiments. Something roughly like that. So if you’re imagining this amount of compute, then you have less compute to run experiments, but quantitatively it’s 80% as much compute, so not a big deal. So I think this is not that important of a part of the picture. And even if you’re imagining 50/50, that’s just a factor of two. So if you’re like, well, you’d spend all your compute running AI researchers, you have no compute for experiments, that’s just an unforced error. OK, so how many AI researchers? People have done various estimates of how many AI researchers do you expect at the point when you can first automate things? You definitely have to have enough researchers to automate everything, but for various reasons I think we expect that you’ll have more researchers than you needed to automate everything. Because at the first point you can start automating everything, you probably have way more labour at that same level of quality. I think inference time compute could make this different. It might be that inference time compute means that at the first time you can automate everything, you can barely automate everything. But I think this is not that likely to be durable. So the first time you can do this, probably you can radically reduce the cost pretty quickly, using stuff like we were talking earlier about distillation. So overall my sense is, I don’t know, I’ve done some kind of trashy estimates. I’ll try to go through one of them. Maybe we have just out of the box like the equivalent of 100 million human-equivalent labourers. Because we expect that we’re training on about like 10^28, 10^29 FLOP, which is roughly what we expect in the 2029, 2030 period to be available. And then if you just do that, you get a sense of how many tokens you’ll be able to generate, and then you try to do some rough conversion between tokens and human labour. Then OK, maybe it’s the case that you have 100 million AI labourers. That’s also taking into account things like the AIs do not sleep, they do not get tired, they can work 24/7, and the data centre can run 24/7. So maybe you have 100 million workers. But then maybe you got some of that with inference compute, so maybe we drop an order of magnitude because you got some of that on inference compute. And then to do full automation, I think you have to have near like Alec Radford quality — Alec Radford is a famous AI researcher who has many of the most important capabilities insights — or, you know, Ilya Sutskever or whatever. To get to this level of quality, maybe you have to spend even more inference compute. I think it’s useful to denominate in terms of units of top research scientists or top research engineers because I think that’ll make some of the conversions easier. Let’s say you have like a million Alec Radford equivalents in parallel. But then there’s another factor, which is the AIs can run faster. For example, they work at night, and that gives them some advantage over human labourers because they can do serially more experiments. So humans, because of the serial time, just get less done in a year, because they’re only working maybe a third of the time — or for the mortals, maybe a fourth of the time, and some people can push up to half of the time potentially. There’s some diminishing returns on focus in hours. And then they can also just run faster because they just spit out tokens faster. And there’s some ways to make this go further. So maybe my overall sense is that maybe they’re like 5x faster at each given point in time and then 3x faster due to running at all hours. That’s a 15x speedup. In addition to that, I think you maybe get another 2x speedup because some of the time you can run a dumber AI that’s much faster for some subtasks. And humans can’t do this as easily because it requires context switching. So in principle you could imagine having humans use a dumber AI for some subtasks very quickly and switching back and forth, but you can’t exchange my brain state with the brain state of the weaker AI. Whereas, for example with transformers, you can just totally feed the context to a weaker AI. You can train the weaker AI to work with the smarter AI. You can even do stuff like shove the activations of the smarter AI into the weaker AI and do all kinds of variable compute scaling things at runtime like this. So maybe that gets you another factor of two. So now we’re up to 30x speed. And to be clear, these speedups are going to take off. They’re going to shave off the number of parallel copies we have. And then I think you maybe get another factor of two from the AIs being better at coordinating than humans. So I talked about maybe they can interchange context with weaker AIs. Well, maybe they’re also just much better at coordinating across parallel tasks. Let’s think about this in terms of a speedup: they can take a task that maybe would be infeasible for humans to parallelise. Like sometimes, when you do an eight-hour software engineering task, you could in principle have five people work on it all in parallel, but you lose a lot on efficiency and maybe get no serial speedup because humans are so bad at coordinating. But maybe the AIs can have all the same context because they can fork off of the same point. So you start with some AI, you fork off of it. There’s a nice Dwarkesh article on all the structural advantages AIs might have, and it goes into this sort of thing. And because you can fork, maybe you get more speedup. Let’s say that’s another factor of two. Now we’re up to 60x speed, right? So we had our million AIs at 60x speed. Let’s make that 50x speed, sorry. So then we have 20,000 AIs in parallel instances, each running at 50x speed. And all of them are as good as the top research scientists, the top research engineers. Now, how much of a speedup is this over, let’s say, OpenAI? Maybe OpenAI, at the point when they’re building this AI, will have somewhere between 2,000 to 5,000 researchers. The number of researchers is growing over time. So naively we have 10x more parallel instances, but they’re also 50x faster. So then there’s some messy conversion between how much additional labour you’re putting in to what overall speedup you expect, taking into account the fact there’s compute bottlenecks and other things, and also the fact that there’s penalties for running in parallel. So you know, nine software engineers cannot make a thing happen that would have taken nine months in one month — you know, the same for the babies —\nRob Wiblin: You can’t have nine women have one pregnancy in one month.\nRyan Greenblatt: Yeah. I think a thing humans suffer from is parallelisation penalties, so the fact that the AIs run much faster means in some sense they suffer less from this. There’s more parallel copies by a factor of maybe 10 or so, so they’re eating some return on that, but you have also just straight-up 50x more speed and also more quality. And the quality pushes into the parallelism as well. So then I’m like, maybe we should really think of the OpenAI labour force as being as good as maybe like 5x or 10x fewer people that were better. So maybe it’s as though they had like 200 or 400 Alec Radfords or whatever. And some people think it’s even more extreme than this. And then if it’s like they have 200 or 400 Alec Radfords, and we have 20,000 Alec Radfords at 50x speed, I think intuitively it feels like things could get crazy. But the question is just how much does the compute bottleneck? And people disagree a lot on this. We really don’t know. No one has run the experiments that we would need to find out how big of a deal this is. We just have surveys and vibes and whatever.\nRob Wiblin: What experiment would you run?\nRyan Greenblatt: Here would be my favourite: Google is known for having a large number of different teams, and I think probably, at some point, someone messed up the compute allocation to some team, or there was some exogenous shock causing the compute allocation to some team to be lower than it was supposed to be or to be higher than it was supposed to be. And then you could look at the question of when that happened, how much did progress speed up or slow down? That would give you some sense of what the marginal production function looks like, what the marginal returns to compute look like. That would give us at least some sense of what’s going on. In the AI case we’re operating very far off of the human margin, because we have so much more labour. So the situation might be very structurally different, but that would give us some sense. I think my dream is that someone goes to GDM or whatever and scrounges up the data on all the natural experiments they must have been running, and does a very economist-style analysis on that and figures out what the local returns look like. That only tells us so much, because it’s only the returns around the current regime. I think even better than that might be things like having a small\n\n\nA team of researchers who you give way less compute to.\nYou know, if Google is really into running experiments, not just giving us data — I pick on Google just because that’s the example, but other companies could do this — they could take some of their researchers and split them into two groups or more groups, and have some of the researchers get way less compute, get more like the amount of compute we expect our AI researchers to have, for instance, and see how much slower they operate.\nIf it’s way, way slower, that would give us a sense on the regimes.\nI think this is a trickier thing to understand partially because there might be adaptation time.\nSo it might be like you put the humans in this regime with way less compute.\nInitially they’re way slower, but they sort of learn to work within those limits.\nAnd I think the AIs will have lots of time to learn to work within those limits because they’re running so much faster.\nRegardless, my sense is that the initial speedup, the instantaneous speedup of your AI researchers will be… When I take all these things into account and try to do the math on the production function, maybe I do something like a Cobb-Douglas production function with some factors and we try to have a parallelism penalty that we apply both to the humans and the AIs, and we normalise the labour force.\nThere’s a bunch of messy stuff here.\nI think that the inside view, fully extrapolating from the current frontier econ model, spits out numbers like, depending on exactly how you do the estimate, I think maybe my favourite picks of the constants are around 50x faster.\nI think this is probably overestimating the speed.\nThat’s 50x faster than the current rate of progress.\nThe current rate of algorithm progress is somewhat over half an OoM per year.\nSo naively that would get you some truly ungodly instantaneous rate of 25 OoMs per year.\nI think now I think people might be like, “Come on, the thing you’re saying, that’s ridiculous.”\nI think I’m like, yeah, the thing I’m saying, it’s a bit ridiculous.\nSo maybe we want to discount this view of the instantaneous speedup a lot.\nSo rather than having the equivalent of 50 years of progress, or one year of progress in one week, I’m like, maybe that’s too crazy, and then I end up dividing down to maybe it’s more like 20x rate of progress, maybe even a bit lower than that at the instantaneous speed, as sort of my median guess.\nAnd again, I think this is like wild speculation; we’re extrapolating from a regime that we don’t even understand to a wildly different regime.\nNo one knows.\nSo it could be much faster; it could be much slower.\nOr it can’t be that much faster, I guess.\nRob Wiblin: So at the point that you fully automate it, it sounds like it could be blisteringly fast at that moment.\nBut I guess one way of making this sound less crazy is you say it starts out incredibly fast and then starts flattening out quite quickly, so you only have one week of this level of blistering progress.\nI guess the alternative is it could go even faster — you were saying that is also a live possibility.\nDo you want to explain what evidence would bear on whether we expect it to slow down versus speed up?\nRyan Greenblatt: Yeah.\nAnother thing on that is I’ve been doing this instantaneous analysis, and I think people might be like, “Sure, maybe you would get that if you drop these in.\nBut it’ll be more gradual in the leadup to this.”\nSo for one thing, in short timelines, I think we should expect the gap between substantial acceleration (but not full automation) and full automation is small in calendar time.\nAnd if you’re expecting that the substantial automation speeds things up, then it’s even smaller in calendar time.\nSo I think this instantaneous analysis is at least non-crazy.\nRegardless, there’s a question of, does it speed up or does it slow down?\nSo if we had this 10x or 20x progress rate, then we’d be talking like the instantaneous rate is like five or 10 OoMs in one year — orders of magnitude of effective compute progress.\nNow, does it speed up or does it slow down?\nThis analysis is even trickier.\nThere’s a lot more factors.\nThe basic story is: you have your AIs, they do a bunch of algorithmic research, they train a new AI, that new AI is smarter and better and more efficient (or some mixture of those attributes), that new AI does even faster algorithmic research.\nBut the returns have also diminished, right?\nSo returns have diminished, but also you have smarter AIs and you can get either superexponential progress, exactly exponential progress — exactly exponential progress is it continues at the same rate, so the progress was already exponential in effective compute — or you can have decaying progress.\nSo the way that we try to get an estimate for this is we try to have a sense of… We’ve been dumping more and more human labour over time into things like computer vision, LLMs — and we can try to get a vague sense of, when we’ve been dumping in all those researchers, how much has that accelerated progress?\nAnd then we do a bunch of adjustments for the AI case.\nSo maybe we have a conversion from dumping in AI labour to how much more effective compute that keeps getting you — which we also needed the same sort of analysis to get the initial speedup.\nAnd if we have that, then the question is, how much more labour does each effective compute get us?\nSo each 10x of effective compute gets us more labour.\nIt also gets us more capable labour, and then that can loop back in.\nSo there’s a bunch of math here.\nAnd again, I think we have plausibly even more uncertainty on this component than the previous component.\nBut I think the best estimates indicate that at least initially, progress will speed up rather than slow down.\nProbably.\nI mean, you can roll to disbelieve on this or whatever, but I think if you just do the naive analysis, you try to account for the factors — you try to account for the compute bottlenecks, you try to account for parallelism issues, you try to account for all this stuff — it turns out that it just makes the AIs more capable and smarter fast enough that — very roughly, on our very trashy models — we expect progress to speed up reasonably quickly.\nRob Wiblin: So if this is right, we’re blowing past human level incredibly quickly into a totally superhuman regime in terms of just how capable these models are in general.\nAm I understanding right?\nRyan Greenblatt: Well, it’s kind of complicated.\nSo there’s a question of how many orders of magnitude of progress you get, and there’s a question of how much does it matter?\nSo I’m throwing around this effective compute unit, and I think this has a problem of being a very econ-brain unit of analysis.\nPeople are like, “OK, come on, how much is an order of magnitude of effective compute even?\nHow much does that matter?”\nWe were talking about that earlier in the discussion about how much is an order of magnitude of effective compute between DeepSeek-V3 and Grok.\nAnd we also care about, does the qualitative trend continue?\nWhat is the right qualitative trend?\nHow superhuman can things even get?\nThis sort of thing.\nI want to spend a bit more time on one thing about the accelerating progress, which is that you should expect that the returns eventually must diminish.\nSo another key factor is limits: progress can only go on for so long, right?\nYou cannot get 100 OoMs of progress, because at some point —\nRob Wiblin: The laws of physics bite.\nRyan Greenblatt: Yeah, the laws of physics bite.\nAnd also more importantly, perhaps the amount of compute you have bites, right?\nSo you only had so much compute.\nI’ve been talking about all this analysis on a fixed compute base.\nSo here’s a naive bear case for efficiency: imagine that you got 10 OoMs of progress on algorithmic efficiency.\nAs of now, that would naively imply you could train DeepSeek-V3 for… So 10 OoMs is a factor of 10 billion, and it was trained for $5 million, so that’s like less than a cent.\nWell, whatever.\nIt’s very little, right?\nYeah, a bit less than a cent.\nSo like, OK, come on, are you going to be able to train DeepSeek-V3 for less than a cent?\nIt’s like you’re doing seconds on an H100, less than seconds on an H100.\nI’m like, come on guys.\nHow many parameters can that be?\nSo in that same ballpark, it’s like, how many numbers can you even multiply?\nYou can only have touched so many parameters, right?\nIf you just do all this, I think you should be very sceptical.\nNow, one thing that’s worth noting is I think limits up might be different than limits down.\nSo it might be that you can only make things so much more efficient, but you can make things scale better.\nSo DeepSeek-V3 is maybe five orders of magnitude more efficient, four orders of magnitude more efficient, even in the limits.\nI think probably a little more than that, but somewhere around there.\nBut maybe you can make DeepSeek-V3… You know, there’s some scaling trends.\nThere’s a question of how good would DeepSeek-V3 be if we scaled it up by five orders of magnitude?\nMaybe we can, for DeepSeek-V3-level compute, go up five orders of magnitude on the DeepSeek-V3 scaling law.\nDoes this make sense?\nThis is a bit tricky.\nRob Wiblin: No.\nMaybe explain that like I’m a bit of an idiot.\nRyan Greenblatt: OK, OK.\nSo for every model, there’s some way to naively scale this up both on RL and data and whatever.\nNow, there’s a bit of complexity around this, and it’s a bit of a tricky analysis, but we could say, how good would we have been if we took the DeepSeek-V3 algorithms, and we scaled them up five orders of magnitude, and adapted to that amount of compute, and didn’t mess up that training run?\nIt might be the case that it’s much easier to replicate what we would have been able to do with five orders of magnitude more compute than DeepSeek-V3 than to make it five orders of magnitude more efficient.\nIt’s a bit tricky to do anchoring, because a lot of the limits I’m defining in different ways, but minimally I think 10 orders of magnitude up on the DeepSeek-V3 efficiency — as in you’re doing with DeepSeek-V3 training compute as well as if you were doing 10 orders of magnitude more compute on those same algorithms — seems very plausible to me.\nRob Wiblin: OK, what does that imply?\nRyan Greenblatt: I think there’s a bunch of ways of doing the analysis.\nI’ll try to do a quick version of the analysis that’s very quick and dirty, but gets us something.\nSo we trained our human-level AIs, or our AIs that are broadly at the level of top human research scientists, let’s say in 2029 or 2030.\nMaybe the training run was around like 10^28 FLOP, and we produce something at the level of humans.\nWe have some very trashy estimates of human brain lifetime compute, like how much compute the human brain is using in a lifetime.\nLike if you had the algorithms of the human brain, and you were able to do that, how long would it take to train a human who’s as good as the best human scientists?\nAnd our sense is it’s around 10^24.\nSo that’s four orders of magnitude of efficiency right there, because we trained something that was competitive with humans for more compute than humans.\nSo four orders of magnitude on that, maybe it’s a bit less, but ballpark.\nMakes sense?\nRob Wiblin: Not completely.\nIt’s four orders of magnitude of what exactly?\nRyan Greenblatt: So we were able to train something that’s as good as a human, but it required us to use four orders of magnitude more compute.\nAnd so you might think, at the very least, we can get to the point where we can train a human for 10^24 FLOP, or a human-level model.\nAnd then we have four orders of magnitude of room above that to expand.\nRob Wiblin: Four orders of magnitude more to expand?\nRyan Greenblatt: As in, imagine that we advanced the algorithm so we can now train a human for 10^24 FLOP.\nNow we have an additional four OoMs of scaling available to us.\nRob Wiblin: OK.\nBecause originally the training was so inefficient.\nRyan Greenblatt: Yeah, that’s right.\nAnd an important thing here is that in shorter timelines we must be imagining we have more efficient algorithms — whereas in longer timelines, where more compute is required, presumably we have less efficient algorithms.\nThere’s some interesting dynamics here.\nRob Wiblin: Why is that?\nRyan Greenblatt: Imagine that we produce full automation of an AI company in 2028, 2029, 2030: then we must be operating around like 10^28, 10^30 training runs.\nOn the other hand, imagine we’re doing it in 2040 or 2045: plausibly we could be having quite a few more orders of magnitude of compute.\nI haven’t done the math, but it could be like four more orders of magnitude.\nI think like 2050 at least maybe you could get four more orders of magnitude of compute by you’ve scaled the fabs, you made them cheaper, you have new techniques, maybe you’re using optical computing and more speculative approaches.\nSo if you’re training the human-level AIs for like 10^36 FL\n\n\nOP, then you have way more headroom.\nRob Wiblin: I see. So we know that we can achieve the level of efficiency that the human brain has. And if it was taking 12 orders of magnitude more compute to reach the equivalent performance as a human, well then you have an enormous amount of potential algorithmic efficiency gain.\nRyan Greenblatt: In the limit.\nRob Wiblin: In the limit. OK, yeah.\nRyan Greenblatt: Anyway, I think a reasonable objection here is: we don’t know what the human brain is doing; can we even produce that level of compute? Also, isn’t it the case that evolution did a huge amount of optimisation? Maybe that required a bunch of compute. And so even if in principle you could have the human algorithm — which is like the human genome — then finding the human genome itself would take a huge amount of research compute, because we have to run simulations equivalent to what evolution was.\nSo this is some scepticism. I’m basically going to put this aside and not address it, and just be like, I’m sceptical. But I think it’s going to be somewhere in between this picture. But I don’t think that’s a huge discount.\nAnd there’s another thing, which is that we don’t think that humans are at the limit of efficiency. There’s many reasons why humans are inefficient: they have physical brains under a bunch of constraints, they can only do local training algorithms for structural reasons about propagation of information backward — so human brains basically can’t do backprop very directly and they can only do more local learning algorithms. So our current best local learning algorithms are much worse than SGD [stochastic gradient descent].\nOf course, evolution had more time optimising these local learning algorithms, so maybe that’s a big factor. Maybe that’s even like two orders of magnitude.\nAnd then there’s a bunch of other factors. Another thing is that, within humans, performance on the task of AI R&D varies wildly. There’s a huge variation between the median human and the best human on ability to do this. Some of that’s training; some of that’s genetics; some of that’s upbringing from things other than direct training, like training on other tasks. So maybe that gives us another bunch of headroom. So you can imagine making 300-IQ humans without having much bigger brains, but just by having more efficient brains — like with more of the mutations removed, possibly more than that. So that gets you some more.\nAnd there’s a long list of considerations like this. Things like, maybe the AIs are able to sync mind states more effectively, which gives them more coordination. Maybe they can generate much better training data. I’m going to miss some of these.\nBut anyway, I think when we add all these up, my guess is a median of like nine OoMs up. We talked about the distinction between up and down. That’s also going to apply on humans. So maybe you can’t train a human for nine OoMs less FLOP — you can’t train a human for 10^15 FLOP, which would be like a second on an H100. But maybe you can train something that’s like nine OoMs better than a human with human-level compute.\nRob Wiblin: I see. OK, that might be the most technical or challenging to follow half hour of the show. I was very happy to let you go, so people can get a sense of just how many moving pieces there are, and also how much thought has gone into this.\nIt sounds like there are quite a lot of people trying to forecast this time and trying to sketch out the different plausible trajectories and the different factors that weigh on it.\nIs it possible to bring it back to something that someone with less technical understanding can grasp? Is the bottom line that people thought about it a lot, it’s quite hazy, there’s a lot of factors at play? It’s possible that at peak AI R&D, things could be moving very fast, and it is plausible that it could even speed up as the AIs get better. It’s also possible it could slow down. We should just be open to all of these different options?\nRyan Greenblatt: I would have said surprisingly little time has been spent thinking about this, actually. As far as I can tell, maybe around four full-time-equivalent years have been spent very directly on trying to build these models to forecast takeoff and applying those models to forecast timelines — maybe even less than this.\nAnd there’s a bunch more work that Epoch has done on trends and other analysis that I’m pulling in, but this type of analysis I’m talking about, these takeoff dynamic analyses, I think maybe at this point it’s more like eight equivalent years. I was not pricing in a few Epoch papers. Maybe the Epoch people are going to call me out for underrating their hard work. But they’ve done a bunch of the background work of the statistics I’m pulling in and a lot of the trends I’m pulling in. But I think there hasn’t been that much work on the analysis here.\nSo I’m like, come on, eight person years? This is maybe the most important question, one of the most important questions. I don’t expect us to get that much signal on it, but it does have a huge effect, and it is a very big disagreement. I think a lot of people are sort of expecting that progress peters out around human level, or it just is relatively slow or it’s mostly bottlenecked on compute. And the question of whether this is true or not makes a huge difference.\nOne argument also that I didn’t mention there, which I sort of just brought up, was I was sort of imagining we just are just like flying through this human regime with no important discontinuity or kink around human level. But it could in principle be that we were able to get to the human level via sort of piggybacking or fast following on human behaviour. My guess is this isn’t that big of a factor, and it’s just like a one-time cost that’s not that big. But I think we shouldn’t get too much into that.\nAnyway, we had how fast is the initial speedup? Does it speed up or does it slow down? And we had what are the limits? Eventually it must slow down, right? So we have this model in which it maybe even initially is speeding up and it’s like continuing to speed up and it’s following this sort of hyperbolic trajectory where it’s going to infinity in finite time. Eventually that must end as you’re starting to near the limits. We don’t know when it starts to slow down. It’s going to slow down at some point.\nBut I think the all-considered model is: things might be very fast, it could happen quite quickly. I think the estimates imply my median is maybe we’re hitting about five or six orders of magnitude of progress in a year of algorithmic progress.\nRob Wiblin: And it’s a bit hard to know exactly what qualitative impact that will have on how smart the models will actually feel to us.\nRyan Greenblatt: Yeah, for sure. That is another big source of uncertainty. I’ve been doing this very econ-brain analysis, where I put everything in these effective compute units, and I’m doing a bunch of quick conversions back and forth to labour supply to get a bunch of things.\nThere’s a bunch of different ways of visualising this progress. I should also say there’s a few factors I’m neglecting, like you’re scaling up compute during this period and a bunch of other minor considerations. These are priced into my five or six OoMs of progress in a year. But I don’t think we should get too much into that.\nRegardless, I don’t know… I have this sort of intuitive-to-me model of initial rate, speedup/slowdown limits, and then limits affect, even if it’s initially speeding up, when it starts slowing down again. Does this model sort of make sense to you?\nRob Wiblin: Yeah, I think that makes sense. Those are kind of the three big stylistic factors that you’re playing around with.\nRyan Greenblatt: Yeah. And then there’s a bunch of tricky details about, suppose the limit is this many OoMs away. The factor of is it speeding up or is it slowing down, and how does that change over time, you might think it’s initially speeding up and the time at which that stops is very close to the end of the limits. Or could be that it’s more continuous across the limits, and this will have a big effect on how many orders of magnitude you get.\nBut regardless, I think that’s the sort of intuitive model. I think people should play with this. I think playing with this sort of model is interesting. It’s pretty clear that this is both a simplified model and also has an insane number of moving parts that we have very little data to estimate. We’re sort of fitting this model in a massively extrapolated regime from trashy data. You know, what can we do? And including data as trashy as guessing how much more efficient you can be than the human brain.\nSo as you were saying, we’re very uncertain and we have huge error bars. My view is you’re going to get some initial speedup and you’re also going to be able to pile in more compute. So maybe the 25th percentile is like you get somewhat faster than previous years of progress, or the 25th percentile is plausibly just barely faster than preexisting progress. And I think that like 80th or 75th percentile might be like completely insane.\nRob Wiblin: So this is the question of, at the point that we are able to automate things, how much does it actually speed up what the company was doing? And you’re saying the 25th percentile of this is maybe it’s kind of just at roughly the same rate as it was before — but the 75th percentile, which is not even an extreme outcome, it’s radically speeding up the research.\nRyan Greenblatt: Yeah. At least quickly. It might be that the initial speedup is not that high, but the speedup increases over time and diminishes relatively slowly.\nAnd also, I’ve been talking about this one-year timescale, but I think on a lot of the modelling most of the progress might happen in the first six months — because you’ve already started to hit this diminishing returns regime kind of quickly.\nRob Wiblin: It’s like the faster you go, the sooner you start hitting limits.\nRyan Greenblatt: Yeah, that’s right. And you know, it could go pretty different ways. Anyway, I’ve been saying six OoMs of progress: what does that even mean? What does this look like?\nRob Wiblin: “OoM” is “order of magnitude” for anyone who didn’t pick that up but is still with us.\nRyan Greenblatt: I’m sorry. I love OoM. What a good term. It’s one of my favourites.\nRob Wiblin: Onomatopoeia.\nRyan Greenblatt: Yeah, yeah, it’s great. Anyway, so six OoMs, how much is that? So it’s roughly two GPTs: there was broadly an OoM between GPT-2 and GPT-3 in terms of roughly 10x algorithmic progress and around 100x compute. Very roughly speaking, maybe a bit less than this. And something broadly similar between GPT-3 and GPT-4.\nSo the naive qualitative model we can do is we can be like, how big was the GPT-3 to GPT-4 gap? And then we can be like, we have two of those gaps: two more GPTs. And then I’m like, what does that mean? I think the two GPTs analysis makes me feel more reassured. I’m like, two GPTs, is that even that bad? I mean, come on.\nI think another framing is how many years of AI progress is this? I think six OoMs is about five years of AI progress, very roughly speaking, maybe four. So it’s like going from, in 2020, we had just gotten GPT-2 XL to now. So it’s like the gap between —\nRob Wiblin: But I guess it’s hard to know intuitively what that means, because GPT-2 was pretty useless for anything.\nRyan Greenblatt: Or GPT-3 was pretty close. Yeah, so that’s pretty useless. I think perhaps the thing that we even have less grounding on is how much does progress above the human range mean? Like, at the point we’re starting this, the AIs are matching the best human professionals. Maybe they’re not quite as efficient, not quite as smart, but via various tricks and whatever, they can basically match human professionals. Now, how much further do you go?\nSo there’s the GPTs. I think another notion is trying to convert from the GPTs to some IQ or some notion of that. People have wildly different intuitions here, but if we imagine that we were starting at maybe 150-IQ AIs, because they were able to automate everything… Again, IQ is kind of a trashy unit.\nRob Wiblin: It feels like it wasn’t designed for this purpose too.\nRyan Greenblatt: Oh no. Nothing was designed [for it]. We’re abusing the shit out of these econ models also. I’ve been doing all this econ-style analysis on econ models that definitely weren’t designed with this regime in mind. And growth economics, which is the field that we’re pulling from, is just not that good of a field — sorry, no offence to the growth economists out there, but there’s just not that many people working on it, and we have a lot of uncertainty over a lot of things there.\n\n\nAnyway, so there’s the two GPTs. How many IQ points is that? This intuition makes me think maybe a GPT is a bit over 50 IQ points or something. And so we go from 150 to 250, and also we have many more parallel copies and they can run faster. These are some intuitions.\n\nAnother intuition is how much better are they in terms of human professionals? Here’s a trend that I think is good to track: if you look at programming competitions, we’ve been seeing progress in terms of ranking on those programming competitions over 2024. At the start, maybe the AIs were like 20th percentile, roughly. And then they were at 50th percentile, and then I think o1 was like 75th, o1-preview was a bit over 90th percentile, and then o3 was 99.8th percentile or something.\n\nSo there’s some relationship between orders of magnitude of compute or algorithmic progress and what rank ordering you have among human professionals. At the point when we’re starting this crazy stuff, maybe the AIs are broadly like hundredth or tenth best human professional rank ordering, and then we have these six orders of magnitude of progress.\n\nI think that there’s some conversion we could try to have between orders of magnitude and ranking — where it’s like every order of magnitude maybe means that you’re like 10x better on this ranking. So instead of being the thousandth best, you’re the hundredth best. My guess is it’s a bit over. Like an OoM of effective compute is somewhat more than an OoM of this sort of rank ordering. I think no one has done this analysis very carefully. Someone should do it. Suppose that it’s a little over an OoM, then maybe with our six OoMs, we get eight OOMs of rank ordering.\n\nRob Wiblin: And so pretty soon you’re below one, right?\n\nRyan Greenblatt: Yeah, you’re below one, so now we’re extrapolating this thing. One way to put this is we sort of quickly get to just human parity, and then maybe we have a bit more than six OoMs left still. Or say the literal best human parity, and then we have another six OoMs of progress. So it’s as big of a gap as going from the millionth best human at a thing — because million is six OoMs — to the best human at a thing. So it’s like we took the best human and we did the equivalent of going from the millionth best to the best.\n\nAnd that’s another qualitative intuition. I don’t know how much that tells you, but there’s some extrapolation there you can do. This is brazenly ripped from Daniel Kokotajlo’s way of thinking about the OoMs.\n\nNow, we also have uncertainty on this point. So I think if it’s more like each OoM is two OoMs, then it’s more like you’re over a billion x better than the best human professionals.\n\nRob Wiblin: You’ve gone from the billionth best to the very best, and then you’ve made that leap again.\n\nRyan Greenblatt: Yeah, that’s right. Which is quite a large gap. Importantly, I think that you can’t understand the billionth best in the human range because it doesn’t make sense to generalise out of a career. Like, who’s the billionth best person at software engineering?\n\nRob Wiblin: This is a silly question.\n\nRyan Greenblatt: This is a silly question. I think the millionth best person at software engineering is now at least somewhat meaningful. We can start working with that. And more niche human professions, it’s less meaningful. So I think we have this kind of insane gap from that.\n\nAnother intuition I like is thinking about how big is the labour supply. So in a lot of the econ analysis I was doing earlier about does progress speed up or slow down, an important question was how much does each order of magnitude of effective compute get you in terms of more cognitive juice to throw at problems in terms of how much can you feed into the labour part of the production function?\n\nOne way to do it is we can just be like, how good is an order of magnitude of compute relative to how many orders of magnitude of parallel workers is that equivalent to? My understanding is that our best available estimates are like every order of magnitude of effective compute is like two orders of magnitude of parallel workers.\n\nRob Wiblin: And this is because having lots of people work in parallel is actually quite inefficient?\n\nRyan Greenblatt: Yeah, the AIs are faster, more capable, and you get more parallel copies. So when you scale up effective compute, at least in the current paradigm, you have more efficient AIs that are smarter, potentially. So you can basically be scaling all these factors in parallel and you can be scaling whichever factor is most effective.\n\nRob Wiblin: I see. So you get to allocate your compute budget between having more of them and having smarter ones in the most efficient combination.\n\nRyan Greenblatt: Yeah. And you can sort of gear your training runs to be like, are we training a bigger model, or are we training a smaller model? There’s some tradeoffs between all these things that’s kind of complicated. There’s ways to trade off inference compute and training compute as well. But all considered, like I’m going to do denomination and parallel copies.\n\nSo we started with like 20,000 geniuses running at 50x speed, and then we had six orders of magnitude — but we’re actually doubling that, so we have 12 orders of magnitude. That’s a trillion. So now we’re going to 20 quadrillion running at 50x speed. Now, I think this is perhaps a bit misleading because an important component is the parallelism bottlenecks. But if you are used to thinking in terms of human organisations, then I think you should think of 20 quadrillion humans running at 50x speed is right, and like the amount of stepping on toes is sort of analogous to that.\n\nAnd then in practice, maybe the thing I actually more expect is maybe it’s qualitatively closer to like a billion or 2 billion humans that are way smarter than humans. So like 250-IQ humans running at 100x speed. Probably my numbers are a bit sloppy, but I think that’s more like the intuition I expect.\n\nAnd then you could do the same in terms of the professionals. You have to be careful not to overcount. Part of the mechanism via which they’re much better at human professions is having more of them, so all these things are going to funge across, but maybe it’s as though you go from millionth best human to best human and then million above that. We sort of do the same extrapolation. Maybe it’s as though we have millions of them at least running at 100x speed, which is like, OK, this is fucking insane, right?\n\nFor example, very quickly the AIs will do more cognitive progress on problems than has been applied in human history by huge margins. And very naively they’re running at 100x speed. So it’s like if there’s something that you could have done purely in the domain of cognition, like purely without access to the world, that would have taken humans 10 years — it would have taken a team of 100 humans 10 years: OK, boom, happens in a tenth of a year with just a tiny fraction of the labour supply.\n\nSo I think we should start being like, what kinds of crazy technologies will be spit out of this process? There’s a bunch of things that I think could in principle be accelerated massively that we haven’t even tried that hard at.\n\nEffort has been spent on atomically precise manufacturing. Not that much effort has been spent on nanobots, nanosystems, whatever. I think Drexler, who originally thought about this, thought it was going to be very little labour, so thought that it might be very easy for humans to do, but very little effort has been applied. So it seems very plausible that you come out of this regime very quickly with like, you know, atomically precise manufacturing that allows for massively increasing the compute supply and all kinds of other crazy things.\n\nThat’s like one example. I think emulated minds and a tonne of other things could happen pretty quickly.\n\nRob Wiblin: To wrap up, it’d be good to do a bit of discussion of what you think are the highest priority things for the sorts of people who listen to this show to be working on, given your enormous distribution of predictions about different ways that things could run. On the technical side, what are some of the things that stand out to you as particularly neglected and useful?\n\nRyan Greenblatt: I think more people should do control work relative to what’s going on. My colleague Buck is probably going to talk more about what that would look like, so I won’t go into too much detail there. I think that’s now a lot less neglected than it was, but still seems good to have more people working on.\n\nI think more people should spend their time thinking about and working on how would you train AIs that are wise and are able to make decisions that are better than the decisions we would make. Basically how would you get to a point where you have ruled out the models plotting against you? How would you make them be the AIs that you are happy to hand over to? This is a much more conceptually thorny area, and I’m planning on spending more time thinking about what research projects should be spun out of that.\n\nThere’s a decent amount of work on what I would call “model internals” that people could work on. Maybe this is falling somewhat under control, but things like probing to make it so that we can detect if the models are taking misaligned actions. How would we do that? How would we know if it worked? This sort of thing.\n\nThere’s some work on maybe decoding uninterpretable reasoning. Suppose models learn to reason steganographically in their chain of thought —\n\nRob Wiblin: So this is that they’re scheming against you, but you can’t tell. It’s kind of encoded.\n\nRyan Greenblatt: Yeah. Steganographically is they’re using different symbols and you don’t understand what’s going on. Or maybe it looks like gibberish to you. Maybe it looks like one thing, but actually is another thing. Or maybe it’s like they’re doing a lot of latent reasoning. We talked earlier about maybe models doing a lot of reasoning in a latent way rather than in natural language, and being able to decode that reasoning in some way, and trying to figure out some methods for training decoders on that that work somewhat and give us some sense of what the AI is actually thinking, I think could be pretty helpful.\n\nIn addition to this, there’s a bunch of different work on demonstrating AIs are very capable now. I talked some about how I think there’s overhang in the level of capability that has been demonstrated. I think demonstrating that current systems are capable and future systems are very capable seems probably somewhat good at the margin, because I’m worried about situations where the world is not very prepared for what’s going on.\n\nSo things like demonstrating high levels of autonomous cybercapability, which I think is a sweet spot of both being directly relevant to a lot of threat models people are already considering, and also is not that far from the scenarios that we’re worried about, which do involve a lot of autonomous cyber activity, and that is actually a key part of the threat model. So it maybe bridges this divide in a nice way. And especially focusing on what is the best demo that we will ever be able to achieve in this realm.\n\nAnother big area that people should work on is what I would call model organisms: trying to produce empirical examples of a misaligned model to study how likely this is to arise and present evidence about that. So things like, does misalignment arise in XYZ circumstance? Does reward hacking emerge and how does it generalise? Things like the alignment faking paper and various continuations of that.\n\nI think part of the hope here is gathering evidence. Part of the hope here is just having something to iterate on with techniques. Even model organisms which aren’t very convincing to the world or maybe don’t produce any evidence about misalignment one way or another, if they’re analogous enough that we can experiment on them, that could be potentially very useful.\n\nRob Wiblin: Because you can try to develop countermeasures that work in the model organism case that then hopefully will transfer?\n\nRyan Greenblatt: Yeah. I think a key difficulty with alignment overall is normally we solve problems with empirical iteration. And to the extent that a lot of our alignment failures make our tests deceptive, then if we can build some way to get around that in advance — or just be ready to build it in the last minute, and then do a bunch of iterations in those kinds of cases — I think that could be pretty helpful.\n\nRob Wiblin: OK, that was what seems most promising on the technical side. Are there things that stand out on governance or other angles?\n\nRyan Greenblatt: Yeah, I think there’s a variety of different room for non-technical interventions that seem pretty good. It’s hard for me to have very strong views on these things, because I don’t spend that long thinking about it. There’s a bunch of work.\n\nWe’ve gone through a lot of conceptual points here, and I think there’s room for people working on just figuring out all these details, trying to have a better understanding of takeoff dynamics, trying to have a better understanding of different considerations other than misalignment that might come up. Things like how worried should we be about human power grabs? How worried should we be about other issues? I think there’s some of that.\n\nI think there’s a decent amount of work on just acting as an intermediary between the very in-the-weeds technical AI safety and the world of\n\n\nPolicy, and trying to translate that to some extent.\nThere’s a bunch of specific regulation that could potentially be good.\nI think making the EU Code of Practice better seems good.\nThe EU AI Office is hiring, so you could work on that.\nI think there’s maybe other strategies for regulation that could actually be good.\nI think there’s some stuff related to making coordination more likely or assisting with coordination that could be pretty helpful.\nThings like improving the compute governance regime so that the US and China can verify various statements made about the current training process.\nI don’t have a strong view on how promising that is, but I think surprisingly few people are working on that, and that’s surprisingly uncoordinated.\nSo maybe someone should get on that, because it could potentially be a pretty big deal.\nIn addition to that, I think just having a lot of people in positions where they’re just trying to provide technical expertise; they’re in a position where they’re currently building skills, they’re currently getting ready to have more direct impact; and will later, as stuff gets crazier, be ready to do something then.\nAnother one is just generic defence.\nSo we talked earlier about AI takeover scenarios.\nA bunch of the AI takeover scenarios I was saying involve, for example, bioweapons.\nJust generically improving robustness to bioweapons seems like it helps some.\nIt’s complicated the extent to which it helps, but I think it helps some.\nSimilar for making the world more robust to AIs hacking stuff.\nI think it helps some.\nI think it’s probably less leveraged than other things, but interventions that steer more resources to those things seem good from a wide variety of perspectives and potentially different assumptions about misalignment.\nI think those things maybe make a lot of sense even with no misalignment risks at all, for example.\nRob Wiblin: Yeah, I guess because misuse is also an issue.\nRyan Greenblatt: Yeah.\nAnd in addition to that, there’s a bunch of different work on security that could be good.\nSo some of the threat models I was discussing involve various outcomes, like the model exfiltrating itself.\nThey involve the model being internally deployed in a rogue way, where it’s bypassing your security and potentially using a bunch of compute it’s not supposed to.\nI think pushing back the time at which these things happen via security mechanisms seems good.\nAlso security to prevent human actors from stealing the model could potentially increase the probability that there is delay and increase the probability of less racing, more caution.\nRob Wiblin: What are the priorities for your research over the next couple of months?\nRyan Greenblatt: Right now I’m doing a decent amount of planning and conceptual work, and then the plan for that is to then spin off a bunch of projects.\nSo I’m thinking through questions like: What should you do in this scenario where your responsible AI company is three months in the lead and there’s very low political will — what’s the full list of potential alignment measures that might be promising?\nWhat is the route you should take?\nHow should people prioritise?\nAnd then trying to maybe make both concrete recommendations and also just figure out things at the margin, basically with the aim being that it just seems like Redwood has had reasonable luck just trying to make overall plans and then spinning off some insights from this.\nI think control came out of this.\nI’ve had some updates based on thinking this through more.\nThat’s one thing.\nThen I’m working on some demonstrations, or trying to look into how big of a deal reward hacking is right now.\nJust recently we’ve seen RL work when it wasn’t really doing as much before, and wasn’t being scaled as far.\nSo one natural question is: how much reward hacking are we getting?\nHow egregious might that be?\nIn what situations is it more or less egregious?\nThere’s been some prior work on this, but I think now that this is really going quite far, we might expect that we potentially see very egregious reward hacking, and we might see threat models that are just driven purely by reward hacking all the way to very egregious outcomes.\nIn principle, doing things like massively deluding humans or potentially trying to seize control of assets could potentially be generalised from reward hacking.\nThere’s also stories via which reward hacking leads to very pernicious misalignment, because you started with an AI that was disobeying your instructions, and that sort of crystallised in some way that involves the AI conspiring against you, even if it’s not as directly for something that we potentially have as much control over as the oversight signal.\nRob Wiblin: All right, well, you and your colleagues write quite a lot on the Alignment Forum and you have a Substack.\nWhat’s the address for that?\nRyan Greenblatt: The Substack?\nredwoodresearch.substack.com.\nOur Substack does not have a short URL, I’m afraid.\nRob Wiblin: So if you want to pull on some of the threads of the things you’ve talked about here, there’s a pretty high chance that there’s some article or blog post out there that you or a colleague have written that could elaborate a bit more.\nRyan Greenblatt: For sure.\nRob Wiblin: I guess it’s a huge to-do list that you laid out there.\nIf there’s people in the audience who are able to help out with that, then I guess time is short.\nWe could use all hands on deck to help push forward all of these agendas and hopefully make things go better.\nRyan Greenblatt: For sure.\nRob Wiblin: My guest today has been Ryan Greenblatt.\nThanks so much for coming on The 80,000 Hours Podcast, Ryan.\nRyan Greenblatt: Thanks for having me.\n",
  "dumpedAt": "2025-07-21T18:43:24.544Z"
}